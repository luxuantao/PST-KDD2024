<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenyong</forename><surname>Dong</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Mengchu</forename><surname>Zhou</surname></persName>
							<email>zhou@njit.edu</email>
						</author>
						<author>
							<persName><surname>Dong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Supervised Learning and Control Method to Improve Particle Swarm Optimization Algorithms</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Software Enginering and Computer School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Systems Engineering</orgName>
								<orgName type="institution">Macau University of Science and Technology</orgName>
								<address>
									<postCode>999078</postCode>
									<settlement>Macau</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">New Jersey Institute of Technology</orgName>
								<address>
									<postCode>07102</postCode>
									<settlement>Newark</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">472627BA2247EE5841C96E5037AD5C96</idno>
					<idno type="DOI">10.1109/TSMC.2016.2560128</idno>
					<note type="submission">Manuscript received June 6, 2015; accepted March 25, 2016.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dynamic quadratic programming (DQP)</term>
					<term>intelligent optimization</term>
					<term>particle swarm optimization (PSO)</term>
					<term>supervised learning and control (SLC)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an adaptive particle swarm optimization with supervised learning and control (APSO-SLC) for the parameter settings and diversity maintenance of particle swarm optimization (PSO) to adaptively choose parameters, while improving its exploration competence. Although PSO is a powerful optimization method, it faces such issues as difficult parameter setting and premature convergence. Inspired by supervised learning and predictive control strategies from machine learning and control fields, we propose APSO-SLC that employs several strategies to address these issues. First, we treat PSO with its optimization problem as a system to be controlled and model it as a dynamic quadratic programming model with box constraints. Its parameters are estimated by the recursive least squares with a dynamic forgetting factor to enhance better parameter setting and weaken worse ones. Its optimal parameters are calculated by this model to feed back to PSO. Second, a progress vector is proposed to monitor the progress rate for judging whether premature convergence happens. By studying the reason of premature convergence, this work proposes the strategies of back diffusion and new attractor learning to extend swam diversity, and speed up the convergence. Experiments are performed on many benchmark functions to compare APSO-SLC with the state-of-the-art PSOs. The results show that it is simple to program and understand, and can provide excellent and consistent performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>problems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b66">[67]</ref>- <ref type="bibr" target="#b72">[73]</ref>. Its concept was initially proposed in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and <ref type="bibr" target="#b31">[32]</ref>. PSO and its variants use a simple kinematic equation inspired by biological mechanisms in birds flocking and fish schooling to update the particles to collaboratively search for globally optimal solutions. PSO makes iterative progress, and during the process, every particle adjusts its position according to its own historical best solution (called local attractor) as well as the globally best solution (called global attractor). In order to balance exploration and exploitation, PSO uses three parameters-inertia weight, cognitive learning factor, and social learning factor-to combine local search through the first two with global one through the third factor <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p><p>Due to its effectiveness and simple implementation in solving multidimensional optimization problems, PSO and its variants have been successfully applied to obtain best-to-date results on optimizing some difficult nonconvex functions, multimodal functions, and multiobjective functions. They help solve many real-world problems, e.g., control system design, decision-making, job scheduling, power system optimization, and structural analysis <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b61">[62]</ref>- <ref type="bibr" target="#b69">[70]</ref>. However, they often suffer from three drawbacks.</p><p>1) The performance of PSO is heavily dependent on its parameters. Desired parameter settings differ remarkably from a problem to another. To address it, researchers seek their theoretical stability analysis and parameter adaptation. The former aims to give some advices for parameter setting. Although some theoretical results give the guidelines to choose these parameters, they tend to give the scopes but not accurate values <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b64">[65]</ref>.</p><p>The latter attempts to develop some tuning rules for parameters to reduce their impact on PSO performance <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. 2) They suffer from premature convergence and slow convergence rate. Four types of methods are proposed to address it. The first one restricts the social learning of particles within a specific range via the concept of a swam neighborhood topology, also called sociometric structures <ref type="bibr" target="#b24">[25]</ref>.</p><p>The effects of various neighborhood topologies, from a linear structure (the degree is 2) to all interconnected structure (the degree is the number of particles), on PSO are systematically investigated. The second one organizes the swarm into a hierarchical structure, especially 2168-2216 c 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>Fig. <ref type="figure">1</ref>. APSO-SLC consists of a plant to be controlled and SLC module.</p><p>The former is PSO with its optimized problem. The latter plays a role of an expert performing real-time supervision of PSO and guiding its search.</p><p>in a tree structure. A particle is influenced by its own best position so far and by the best position of the particle that is directly above it in the hierarchy <ref type="bibr" target="#b15">[16]</ref>. The third one is called parallel or multiswarms in which several small swarms are organized dynamically or statically, and different swarms mutually interchange information <ref type="bibr" target="#b29">[30]</ref>.</p><p>The last one is hybrid strategies in which other optimization techniques, such as a gradient-based algorithm, genetic algorithm, and simplex algorithm, are combined into PSO. However, the proper time to trigger them remains unsolved <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b43">[44]</ref>.</p><p>3) The original PSO kinematic equation for the current best particle has small possibility to improve PSO performance and sometimes slows down the convergence rate.</p><p>On the other hand, improper attractors as determined by cognitive and social learning results may lead to some useless searches. In this paper, we aim to solve the above problems from a viewpoint of an adaptive control system. Inspired by the supervised learning and predictive control strategies, we propose a new framework based on supervised learning and control for adaptation of PSO (APSO-SLC) as shown in Fig. <ref type="figure">1</ref>. A supervised learning and control (SLC) module serves as a sophisticated expert who supervises a PSO system in realtime, and gives the following important advice regarding: 1) what PSO parameters can accelerate PSO convergence; 2) does it trap into stagnation; and 3) what strategies can be used to break the stagnation. First, we treat PSO with its optimization problem as a plant to be controlled and model it as a dynamic quadratic programming (DQP) model with box constraints. We estimate its parameters by the recursive least squares with a dynamic forgetting factor. Then the optimal parameters are fed back to the algorithm. Second, we propose a concept called a progress vector that can be used to monitor the progress rate of PSO and help us judge whether premature convergence happens. If so, we propose two strategies to handle it, i.e., back diffusion (BD) and social attractor renewal. They can promote swam diversity, and thus overcome premature convergence. Finally, we introduce a local search strategy to speed up the convergence of the best particles. We have verified the performance of APSO-SLC through its comparisons with several PSO variants, and shown that it is effective to optimize benchmark functions with different properties.</p><p>An intuitive interpretation of our method, as shown in Fig. <ref type="figure">1</ref>, is given by analogy as follows: as you drive a car (like PSOs) in different road conditions (like optimal problems), the control parameters are the speed, gas, and steering wheel (like PSO parameters). The driving strategies to tune the parameters depend on a driver's experience (like metamodels), and the methods (like back diffusion in this paper) to handle emergence are also essential.</p><p>Section II reviews some parameter adaptation strategies for PSO. Section III presents the proposed APSO-SLC. Section IV discusses its implementation issue and criteria for comparisons. Section V gives and analyzes experimental results. Section VI gives the conclusion and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REVIEW OF PARAMETER ADAPTATION FOR PSO</head><p>To improve PSO performance, many strategies for adaptive setting of parameters have been proposed since dozen years ago. Researchers deeply analyze the dynamic behavior, and give some guidelines to select PSO parameters. Practical strategies for parameter adaptation fall into two types depending on if we adapt and apply three parameters to all particles uniformly or do so for each individual particle, called type-I and type-II adaptation, respectively. The latter has 3N parameters to adjust where N is the number of particles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Parameter Selection Theory</head><p>In a standard PSO, each particle is represented by two vectors standing for its position and velocity, and each vector's dimension is identical to that of an optimization problem to be solved. They are updated through its historical cognitive and social information from its neighbors along with each iterative process. The update rules with inertial weight for particle i at iteration k + 1 is</p><formula xml:id="formula_0">v k+1 i = ωv k i + c 1 • r 1 • l k i -x k i + c 2 • r 2 • g k -x k i x k+1 i = x k i + v k+1 i (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where v k i velocity of the ith particle at the kth iteration; ω inertial weight; c 1 cognitive learning factor; r 1 and r 2 random number whose distribution is uniform over [0, 1]; l k i the ith particle's best position found so far; x k i position of the ith particle at the kth iteration; c 2 social learning factor; g k best position found so far by the entire swarm. PSO theory concentrates on the understanding of particle trajectories, and gives some guidelines for parameter selection. Many factors influence whether PSO converges and how fast it converges, but three parameters, i.e., inertial weight, cognitive learning factor, and social learning factor, have the most significant impact on its efficiency and reliability. Thus most work provides deep insights into how these parameters take effects on deterministic or stochastic trajectories of particles. Stochastic analysis of the trajectories <ref type="bibr" target="#b5">[6]</ref> show that given a certain parameter range, when the global and ith particle's best positions do not change with time, all the particles approach their corresponding stationary points as</p><formula xml:id="formula_2">x s i = c 1 • l s i + c 2 • g s c 1 + c 2<label>(2)</label></formula><p>where x s i stationary points of the ith particle; l s i the ith particles best position found so far when PSO is trapped into stagnation; g s the best position found so far when PSO stagnates. The work <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b34">[35]</ref> gives a deep analysis through deterministic dynamics. As a result of stability analysis of the particle swarm dynamics, a sufficient condition for the parameter selection is established as</p><formula xml:id="formula_3">(ω, φ) : |ω| &lt; 1, ω = 0, φ &lt; 1 -2|ω| + ω 2 1 + ω (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where</p><formula xml:id="formula_5">φ = (c 1 + c 2 )/2.</formula><p>Following it, Martínez and Gonzalo <ref type="bibr" target="#b34">[35]</ref> extend PSO into generalized PSO (GPSO), and obtain the following condition:</p><formula xml:id="formula_6">(ω, φ) : 1 - 2 t &lt; ω &lt; 1, 0 &lt; φ &lt; 2(ω t -t + 2) t 2 (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where t is the time step of GPSO.</p><p>Based on <ref type="bibr" target="#b5">[6]</ref>, Van Den Bergh <ref type="bibr" target="#b54">[55]</ref> gives a simple form for these PSO parameters as follows:</p><formula xml:id="formula_8">{(ω, φ) : 0 ≤ ω &lt; 1, ω &gt; φ -1}.<label>(5)</label></formula><p>Jiang et al. <ref type="bibr" target="#b16">[17]</ref> employ a stochastic process to analyze the particle's trajectories, and derive the following sufficient condition for PSO stability:</p><formula xml:id="formula_9">{(ω, φ) : 0 ≤ ω &lt; 1, 0 &lt; φ/2 &lt; ω + 1}.<label>(6)</label></formula><p>Some researchers give instructive parameter setting. For instance, it is suggested that ω should be 0.6 or 0.715 and both c 1 and c 2 equal 1.7 <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b64">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptive Parameter Selection in PSO</head><p>Except for theoretical work for PSO parameter selection, researchers have also figured out some effective strategies. They can be divided into two categories: 1) type-I and 2) type-II adaptation as mentioned before. The former requires all the particles in one swam to employ the same parameters termed as global parameters; while the latter requires each particle to employ its own ones termed as private parameters.</p><p>1) Type-I Parameter Adaptation: With this strategy, all particles take the same values for ω, c 1 , and c 2 . It is typically accomplished via a linear or nonlinear function, and fuzzy rules.</p><p>With linear function adaptation <ref type="bibr" target="#b48">[49]</ref>, only ω is set to linearly decrease with the iteration count</p><formula xml:id="formula_10">ω(i) = ω max - i • (ω max -ω min ) I (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where ω(i) inertia weight at the ith iteration; ω max maximum inertial weight, and usually set to 0.9; ω min minimum inertial weight, and usually set to 0.4; I maximum iteration count. Many nonlinear function adaptation strategies are in use <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Jiao et al. <ref type="bibr" target="#b17">[18]</ref> propose to change ω as follows:</p><formula xml:id="formula_12">ω(i) = ω 0 u -i<label>(8)</label></formula><p>where ω 0 = 0.9, and u ∈ [1.001, 1.005]. Chatterjee and Siarry <ref type="bibr" target="#b4">[5]</ref> extend <ref type="bibr" target="#b6">(7)</ref> to</p><formula xml:id="formula_13">ω(i) = ω max + (I -i) ρ • (ω max -ω min ) I ρ (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>where ρ is a parameter ranging from 0.1 to 2. The above mentioned strategies, regardless linear or nonlinear function adaptation, decrease ω from a relatively large value to a small one as PSO proceeds. The idea behind them is, at the early stages, the search range of PSO should be larger, while at the latter stages, it should tend to search the vicinity of the global optimum. Unfortunately, such simple adaptive strategies cannot meet the "truly" adaptive requirement of inertia weight existing in a search process. By recognizing this, Shi and Eberhart <ref type="bibr" target="#b49">[50]</ref> propose fuzzy rules to adjust ω and these fuzzy rules are designed to tune it according to the current best performance evaluation result, which measures the performance of the best candidate solution found so far by PSO. The experimental results verify that this method is more effective than the other two strategies to adaptively tune ω. Except ω, both c 1 and c 2 can be adjusted by a chaotic sequence generated through the chaos theory <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>2) Type-II Parameter Adaptation: With this strategy, each particle possesses its own parameters, i.e., ω, c 1 , and c 2 , which are tuned at each iteration according to its own performance with respect to the best particle found so far.</p><p>Yamaguchi and Yasuda <ref type="bibr" target="#b58">[59]</ref> propose the following adaptive equation for c 1 and c 2 :</p><formula xml:id="formula_15">⎧ ⎨ ⎩ c k+1 1,i = c k 1,i + α k i c k 1,best -c k 1,i c k+1 2,i = c k 2,i + α k i c k 2,best -c k 2,i<label>(10)</label></formula><p>where c k 1,i and c k 1,best cognitive learning factor of the ith particle and best one at the kth generation; α k i selected from 0 or 2/I depending on whether the best particle improves or not; c k 2,i and c k 2,best social learning factor of the ith particle and best one at the kth generation. Hu et al. <ref type="bibr" target="#b13">[14]</ref> propose an intelligent augmentation of PSO with multiple adaptive methods and then its enhanced version named APSO-MAN <ref type="bibr" target="#b14">[15]</ref>. A parameter control mechanism, based on the relative position between each particle and the global best one, adaptively tunes ω, c 1 , and c 2 of each particle IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS as follows:</p><formula xml:id="formula_16">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ω k+1 i = ω k i + σ k i × 2 g k -x k+1 i T × v k i c k+1 1,i = c k 1,i + σ k i • r k 1,i × 2 g k -x k+1 i T × l k i -x k i c k+1 2,i = c k 2,i + σ k i • r k 2,i × 2 g k -x k+1 i T × g k -x k i (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>where ω k i inertia weight of the ith particle at the kth generation; σ k i step size, also known as Polyak's step size; r k 1,i and r k 2,i random numbers whose distribution are uniform over [0, 1]. Type-II adaptive strategies improve PSO performance much more than type-I ones in term of solution quality and convergence rate according to <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SUPERVISED LEARNING AND CONTROL ADAPTATION</head><p>This work proposes SLC strategies to enhance PSO performance by supervising PSO behaviors on-line so as to select proper parameters and change its search behavior. SLC is composed of two parts: 1) a DQP model with box constraints (DQP-BC), which is used to estimate optimal control parameters and 2) a mechanism that triggers proper strategies to force PSO out of stagnation once it is detected via our proposed progress vector. The main reasons for choosing a DQP model to model the relationship between PSO parameters and its performance along with the optimization process are as follows.</p><p>1) Linear models cannot well capture the interactive relationship among PSO parameters; nor that between them and PSO performance. 2) During the searching process of PSO, a proper combination of parameters that can speed PSO convergence is different. They may be adaptively changed via feedback control strategies and the related model is dynamic. 3) More sophisticated models than DQP need more computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DQP-BC for Parameter Setting</head><p>To reduce the sensitivity of PSO to its parameter settings, many strategies are proposed to adaptively tune them. But none is based on supervising PSO online performance and thus they largely fail to give the most proper parameter settings. Similar to SLC in the cognitive learning and control system field, we introduce DQP-BC to learn the relationship between PSO performance and its three parameters and derive their optimal values.</p><p>We view PSO with its optimization problem as a plant to be controlled, and model them as</p><formula xml:id="formula_18">y(t + 1) = f (u(t), t)<label>(12)</label></formula><p>where y(t + 1) output of PSO, which equals the optimum so far found by step t+1 for the optimization problem; f (u(t), t)</p><p>to-be-found model of the plant; u(t) = (ω(t), c 1 (t), vector of PSO parameters used at c 2 (t)) T step t, viewed as the input. In the field of optimization via simulation, ( <ref type="formula" target="#formula_18">12</ref>) is called a meta-model, which means a model of models, and is often constructed to approximate a "true" system model <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b26">[27]</ref>.</p><p>In order to properly model the relationship between the parameters of PSO and its performance (optimum) along with the PSO process, both linear and nonlinear models are candidate solutions. But the relationship cannot be modeled with a linear model because of its complexity and stochastic effect, while a simple but nonlinear model seems an appropriate choice. Thus a DQP model for f (u(t), t) is selected i.e.,</p><formula xml:id="formula_19">y(t + 1) = (u(t)) T Q(u(t)) + c T u(t) + b (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>where c ∈ R n is a vector, Q ∈ R n×n is a symmetric matrix, and b ∈ R is a constant. They are all unknown and to be estimated by such methods as least square estimation or maximum likelihood estimation. As the iterations of PSO proceed, the data are generated and then used to estimate them. We use a recursive least square scheme with a dynamic forgetting factor.</p><p>For convenience, ( <ref type="formula" target="#formula_19">13</ref>) can be presented in a linear form as</p><formula xml:id="formula_21">y(t + 1) = f T θ = θ 1 f 1 (u(t)) + θ 2 f 2 (u(t)) + • • • + θ p f p (u(t))<label>(14)</label></formula><p>where</p><formula xml:id="formula_22">( f 1 (u(t)), f 2 (u(t)), . . . , f p (u(t))) corresponds to</formula><p>(1, u 1 (t), . . . , u n (t), (u 1 (t)) 2 , . . . , (u n (t)) 2 ), and (θ 1 , θ 2 , . . . , θ p ) corresponds to all the parameters of b, c, and Q. The total number of parameters are p = 1 + n + n(n + 1)/2 by noting that Q is symmetric.</p><p>Along with a PSO process, {(u(t), y(t+1))}, t = 1, 2, . . . , m, is gathered to construct a training data set. The data are substituted into <ref type="bibr" target="#b13">(14)</ref>, yielding the following linear equations:</p><formula xml:id="formula_23">⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ f 1 (u(1))θ 1 + • • • + f p (u(1))θ p = y(2) f 1 (u(2))θ 1 + • • • + f p (u(2))θ p = y(3) . . . f 1 (u(m))θ 1 + • • • + f p (u(m))θ p = y(m + 1). (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>Equation ( <ref type="formula" target="#formula_23">15</ref>) can be written in a matrix form as</p><formula xml:id="formula_25">Aθ = y<label>(16)</label></formula><p>where</p><formula xml:id="formula_26">A = ⎡ ⎢ ⎢ ⎢ ⎣ f 1 (u(1)) f 2 (u(1)) • • • f p (u(1)) f 1 (u(2)) f 2 (u(2)) • • • f p (u(2)) . . . . . . . . . . . . f 1 (u(m)) f 2 (u(m)) • • • f p (u(m)) ⎤ ⎥ ⎥ ⎥ ⎦ θ T = (θ 1 , θ 2 , . . . , θ p ), and y T = (y(2), y(3), . . . , y(m + 1)).</formula><p>The least square estimation involves minimizing the following cost function:</p><formula xml:id="formula_27">J = 1 2 m t=1 y(t + 1) -( f (t)) T θ 2 . (<label>17</label></formula><formula xml:id="formula_28">)</formula><p>If A T A is a nonsingular matrix, (17) has a unique minimal solution</p><formula xml:id="formula_29">θ = A T A -1 Ay.<label>(18)</label></formula><p>Sometimes, the assessment is needed to be done whenever the new information about input/output samples arrives, i.e., in each sampling period. Because data are collected in a PSO process and the model is a nonlinear time-varying system, we can formulate the following recursive least square estimation with a dynamic forgetting factor:</p><formula xml:id="formula_30">θ(t + 1) = ⎡ ⎢ ⎢ ⎢ ⎣ A a T ⎡ ⎢ ⎢ ⎢ ⎣ η(t) . . . η(t) 1 ⎤ ⎥ ⎥ ⎥ ⎦ A a ⎤ ⎥ ⎥ ⎥ ⎦ -1 A a T ⎡ ⎢ ⎢ ⎢ ⎣ η(t) . . . η(t) 1 ⎤ ⎥ ⎥ ⎥ ⎦ Y y . (<label>19</label></formula><formula xml:id="formula_31">)</formula><p>Or in a matrix form</p><formula xml:id="formula_32">θ(t + 1) = η(t)A T A + a T a -1 η(t)A T Y + a T y<label>(20)</label></formula><p>where A∈ R m×p old design matrix; a∈ R 1×p new design matrix based on the new coming sample, and η(t) ≤ 1 is a dynamic forgetting factor. The above formulation allows one to gradually forget older data in favor of more recent data. The scheme in <ref type="bibr" target="#b19">(20)</ref> is known as least-square estimate with a dynamic forgetting factor.</p><p>Let P t = (A T A) -1 . By using "matrix inversion lemma" <ref type="bibr" target="#b52">[53]</ref>, we have</p><formula xml:id="formula_33">P t+1 = η(t)A T A + a T a -1 . (<label>21</label></formula><formula xml:id="formula_34">)</formula><p>Then θ(t + 1) can be calculated recursively as follows:</p><formula xml:id="formula_35">θ t = P t A T y<label>(22)</label></formula><formula xml:id="formula_36">θ t+1 = P t+1 η(t)P -1 t θ t + a T y = P t+1 P -1 t+1 θ t -a T aθ t + a T y = θ t + P t+1 a T y -aθ g (<label>23</label></formula><formula xml:id="formula_37">)</formula><formula xml:id="formula_38">P t+1 = η(t)A T A + a T a -1 = 1 η(t) P t - 1 η(t) P t a T I + a 1 η(t) P t a T -1 a 1 η(t) P t = 1 η(t) P t - 1 η(t) P t a T η(t)I + aP t a T -1 aP t . (<label>24</label></formula><formula xml:id="formula_39">)</formula><p>The above method is called a recursive least square estimation method, and its application demands the supposition of initial values of system parameters θ(0) and covariance matrix P 0 . There are many ways to choose the values of initial system parameters. We set θ(0) = (1, 1, . . . , 1). A standard choice of P 0 can be in the following form:</p><formula xml:id="formula_40">P 0 = rI (<label>25</label></formula><formula xml:id="formula_41">)</formula><p>where I is a unit matrix, and r is usually set at 100 to 10000 to ensure true f (u(t), t) to be learned fast.</p><p>The method to update dynamic forgetting factor η(t) will be discussed after we introduce the concept of a progress vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Solve DQP-BC</head><p>After estimating the supervised model, in order to control PSO parameters, we change (13) into the version with box constraints as</p><formula xml:id="formula_42">y(t + 1) = (w(t), c 1 (t), c 2 (t)) T Q(w(t), c 1 (t), c 2 (t)) + c T (w(t), c 1 (t), c 2 (t)) + b s.t. ⎧ ⎨ ⎩ 0.1 ≤ w(t) ≤ 0.9 0.2 ≤ c 1 (t) ≤ 2.7 0.2 ≤ c 2 (t) ≤ 2.7. (<label>26</label></formula><formula xml:id="formula_43">)</formula><p>There are many algorithms to solve <ref type="bibr" target="#b25">(26)</ref>, and a survey of research on DQP with box constraints up to 1997 was given in <ref type="bibr" target="#b6">[7]</ref>. More recent papers include <ref type="bibr" target="#b55">[56]</ref> and <ref type="bibr" target="#b57">[58]</ref>. Because there are only three parameters in <ref type="bibr" target="#b25">(26)</ref>, we can simply employ a standard active set strategy with the gradient projection method <ref type="bibr" target="#b36">[37]</ref> to solve it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Progress Vector</head><p>To learn the performance of PSO in time, we utilize a progress vector motivated by the evolutionary path in covariance matrix adaptation evolutionary strategy (CMA-ES) <ref type="bibr" target="#b12">[13]</ref>, i.e., a cumulated sum of difference vectors within successive best positions at the tth and (t + 1)th iterations, and is denoted by t at the tth iteration. It is then applied to judge whether PSO stagnates, adaptively update DQPM-BC, and update the movement of current particles in a PSO swarm.</p><p>Initialized with 0 = 0, t+1 is updated as</p><formula xml:id="formula_44">t+1 = (1 -c) t + c g t+1 -g t (<label>27</label></formula><formula xml:id="formula_45">)</formula><p>where t progress vector at iteration t; 0&lt;c≤1 update factor for the progress vector, and its reciprocal, 1/c, means the backward time horizon of a progress vector <ref type="bibr" target="#b12">[13]</ref>; g t the best position found so far by the swarm at t. The update factor determines which one is important between the previous and current progress vectors. Suppose that the best position from iterations t 0 to t does not change any more. From <ref type="bibr" target="#b26">(27)</ref>, we have</p><formula xml:id="formula_46">t+1 = (1 -c) t-t 0 t 0 . (<label>28</label></formula><formula xml:id="formula_47">)</formula><p>An algorithm can make progress well when it does not stagnate. Quantitatively, we assume that it is equivalent to that || t || &gt; || t 0 ||/e from generation t 0 to t, where || t || is a vector norm and e is the base of the natural logarithm. In other words, we define that the progress stagnates if</p><formula xml:id="formula_48">|| t || = || t 0 ||/e, that is (1 -c) t-t 0 = 1 e . (<label>29</label></formula><formula xml:id="formula_49">)</formula><p>Solving ( <ref type="formula" target="#formula_48">29</ref>) and using the Taylor approximation of ln(1-c) yield</p><formula xml:id="formula_50">t -t 0 = ln(1 e) ln(1 -c) ≈ 1 c . (<label>30</label></formula><formula xml:id="formula_51">)</formula><p>Equation <ref type="bibr" target="#b29">(30)</ref> implies that the progress vector norm decreases at most 37% in 1/c iterations. Hence, we can choose c between 1/d to 1/ √ d, where d is the dimension, i.e., the number of variables of the optimization problem.</p><p>Based on ( <ref type="formula" target="#formula_46">28</ref>)-( <ref type="formula" target="#formula_50">30</ref>), the condition of PSO stagnation can be judged as follows. In successive 1/c iterations, the progress vector norm decreases to 1/e of the original one, we conclude that PSO is trapped into stagnation.</p><p>The progress vector can be used to update the dynamic forgetting factor η(t), i.e., if its norm at iteration t is larger than the one at t-1, the current PSO parameters should increase the search speed. Thus, we stress the current parameters by increasing η(t), and otherwise we should decrease it. We should restrict η(t) within [η min , η max ], where η min and η max are the minimum and maximum forgetting factors. A projection strategy is adopted to ensure that the particles always stay inside the bound. In other words, if η(t) is outside its range, we perform the following operation:</p><formula xml:id="formula_52">η(t + 1) = H η min , η 0 e c η t - t+1 , η max (<label>31</label></formula><formula xml:id="formula_53">)</formula><p>where η 0 is a given reference value of the forgetting factor, and c η is the significance coefficient.</p><p>H is a projection operator on R defined as follows:</p><formula xml:id="formula_54">H(x min , x, x max ) = ⎧ ⎨ ⎩ x min , x &lt; x min x, x min ≤ x ≤ x max x max , x &gt; x max . (<label>32</label></formula><formula xml:id="formula_55">)</formula><p>From <ref type="bibr" target="#b30">(31)</ref> we can see that if a progress vector in the current iteration improves its previous value, then the forgetting factor is set to be small such that "better parameters" are deeply remembered; otherwise, we set it to a large value such that the "worse parameters" are quickly forgotten.</p><p>When PSO stagnates, the reasons behind it should be diagnosed. We introduce a covariance matrix of all particles in the current swarm:</p><formula xml:id="formula_56">C k = 1 N N i=1 x k i -xk x k i -xk T (<label>33</label></formula><formula xml:id="formula_57">)</formula><p>where C k covariance matrix of all particles at iteration k; xk mean of all particles at iteration k; N number of particles. Because C k is a non-negative matrix, if its maximum eigenvalue is smaller than a given value σ , we reason that all particles in a swarm have gathered together. Otherwise, they may be trapped into their own stationary points as given by <ref type="bibr" target="#b1">(2)</ref> or enter some unknown conditions under which they lose their searching ability. This work sets σ as</p><formula xml:id="formula_58">σ = max 1≤i≤d b u i -b l i 100 (<label>34</label></formula><formula xml:id="formula_59">)</formula><p>where d is the dimension of the optimization problem, b u i is the upper bound of the ith variable, and b l i is the lower bound. We next present two strategies to handle them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Back Diffusion (BD)</head><p>PSO is an elitist reserving algorithm, i.e., the best position found so far is always kept in the swarm. The elitist strategy has its own advantage, such as fast convergence. However, it leads PSO to lose its diversity of population and be trapped into local optima. As discussed in Section III-B, a progress vector is proposed to detect such stagnation. The most possible reason behind stagnation is that PSO is trapped into local optima and thus loses its further search ability. This requires some strategies to solve the problem.</p><p>The first one we propose is called BD, which can efficiently maintain the diversity of the population without losing the best position found by each particle. It transforms all the particle positions to their new mirror positions without changing their control parameters. The behavior is similar to the scatter behavior when the birds are scared. Two issues need to be resolved for BD: 1) what are the new positions for all particles and 2) how to change the speed of each particle. The method to determine the new position of each particle is based on the concept of opposition-based learning (OL) <ref type="bibr" target="#b56">[57]</ref>. Although OL is effective to enhance PSO performance, it is a deterministic approach to determine the new position of each particle. To further increase PSO's search ability, we change OL to random OL (ROL).</p><p>1) Opposite Number: Suppose that x ∈ [a, b] is a real number. Its opposite number is defined as the mirror of x</p><formula xml:id="formula_60">x = a + b -x. (<label>35</label></formula><formula xml:id="formula_61">)</formula><p>2) Opposite Point:</p><formula xml:id="formula_62">Let x = (x 1 , x 2 , . . . , x d ) ∈ R d be a point in a d-dimensional space, subject to box constraints, i.e., x i ∈ [a i , b i ]. The opposite point x of x is defined as x = a + b -x<label>(36)</label></formula><p>where a T = (a 1 , a 2 , . . . , a d ) is the lower bound vector, and <ref type="figure">b d</ref> ) is the upper bound vector.</p><formula xml:id="formula_63">b T = (b 1 , b 2 , . . . ,</formula><p>3) Random Opposite Point: Let x and x be original and opposite points. The random opposite point is their random combination</p><formula xml:id="formula_64">x = rx + (1 -r)x = r(a + b) + (1 -2r)x (<label>37</label></formula><formula xml:id="formula_65">)</formula><p>where r is a random number following the uniform distribution over [0, 1]. By considering the behavior that the birds fly separately when they get scared, we set the velocity of each particle to be the average of its original velocity and a randomly generated one, i.e.,</p><formula xml:id="formula_66">ṽi = H v min , (1 -c v )v i + c v v r i , v max<label>(38)</label></formula><p>Fig. <ref type="figure">2</ref>. There are three particles in the swarm, the best one found so far is marked by a red square, the local best ones found so far by the other two particles are marked by green circles, and attractors calculated by ( <ref type="formula" target="#formula_2">2</ref>) are marked by blue diamonds. Except the best one, the other two particles approach their corresponding attractors that are both poor for this maximization problem. When in stagnation, PSO loses its search ability.</p><p>where ṽi new velocity of the ith particle; H vector projection operator, and each of its components is calculated via (32); v min and v max lower and upper bounds of velocity; c v coefficient in (0, 1); v r i randomly generated velocity, that is</p><formula xml:id="formula_67">v r i = v min + λ • (v max -v min ) (<label>39</label></formula><formula xml:id="formula_68">)</formula><p>where λ is a random number following the uniform distribution over [0, 1]. By applying the definition of a random opposite point and random diffusion velocity, BD can be defined as follows. For each particle, the new position and velocity are calculated via <ref type="bibr" target="#b37">(38)</ref> and <ref type="bibr" target="#b38">(39)</ref>, and the other parameters, i.e., ω, c 1 , and c 2 , do not change. In order to accelerate the convergence, the elitist strategy is adopted. If a random opposite point is better than the original one, update the position with the random opposite point; and otherwise keep the current one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Social Attractor Renewal</head><p>When stagnation happens, a "bad" attractor for each particle may be another reason, as depicted in Fig. <ref type="figure">2</ref>. Each particle has its own attractor calculated by <ref type="bibr" target="#b1">(2)</ref>. There are three particles in the swarm, the best one found so far is marked by the red square, the local best positions found so far by the other two particles are marked by green circles, and attractors are marked by blue diamonds. Except the best one, the other two particles approach their corresponding attractors, but both attractors are very poor in performance for this maximization problem. When PSO stagnates, the swarm loses its search ability. In order to overcome this drawback, once PSO is detected to be in stagnation, changing their attractors may be another good strategy. Hence, we introduce social attractor renewal (SAR) to break the stagnation. It is defined as a weighted average of parts in the current particles with their best particles found so far</p><formula xml:id="formula_69">m k+1 == N/2 i=1 w i x k i:2N (40) N/2 i=1 w i = 1, w 1 ≥ w 2 ≥ • • • ≥ w N/2 &gt; 0 (41)</formula><p>where m k+1 new attractor for all particles; N number of particles; w i ∈ R + positive utility weight coefficient for calculating the attractor. The weights satisfy (41). Each particle x k i and its local best position l k i are evaluated via an objective function. There are 2N positions, which are ranked as x k i:2N according to their function values (FVs), where i : 2N stands for the ith best particle in the swarm. Then only N/2 best positions are chosen to calculate the attractor.</p><p>After introducing the social attractor, the kinematic equation in ( <ref type="formula" target="#formula_0">1</ref>) should be changed into</p><formula xml:id="formula_70">v k+1 i = ωv k i + c • r • m k -x k i x k+1 i = x k i + v k+1 i (<label>42</label></formula><formula xml:id="formula_71">)</formula><p>where c social cognitive learning factor; r random number whose distribution is uniform over [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION OF APSO-SLC</head><p>SLC is the core of APSO-SLC. Because PSO is a stochastic algorithm, it has very large contingence only running one generation. When measuring PSO performance, we should let PSO run enough iterations to show its search ability. If d is the dimension of the search space, we run PSO for d times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modules of APSO-SLC</head><p>As shown in Fig. <ref type="figure" target="#fig_0">3</ref>, the proposed APSO-SLC has four modules: 1) PSO; 2) SLC; 3) back diffusion; and 4) SAR.</p><p>1) PSO: This module performs a standard PSO algorithm with inertia weight. In each iteration, each particle updates its velocity and position by ( <ref type="formula" target="#formula_0">1</ref>), and at the same time guarantees each new position to be feasible. The local and global best positions found so far are updated. 2) SLC: This module plays a role of a supervisor, which in real-time supervises PSO performance, and gives proper advice to guide the search. Its three functions are: a) to calculate three PSO parameters; b) to employ progress vectors to judge whether PSO stagnates; and c) to judge whether all the particles gather together. If so, decide which strategy should be used between BD and SAR. 3) Back Diffusion: When SLC finds that PSO stagnation is caused by all the particles gathering together, BD is triggered. It is similar to the standard PSO, but adopts <ref type="bibr" target="#b37">(38)</ref> to update the velocities and positions of all particles.</p><p>Because it scatters all the particles, it should run only when necessary. This is accomplished by setting d 1 to a small integer.  An intuitive interpretation of this method as shown in Fig. <ref type="figure" target="#fig_1">4</ref> is given by the following analogy: as one drives a car (like PSO) in different road conditions (like optimization problems), the control parameters that are the speed, fuel amount, and steering wheel angel (like PSO parameters), the driving strategies to tune the parameters depend on the driver experiences (like the meta-models), and the methods (like BD and SCR) to handle any emergence and those never-experienced conditions are also essential.</p><p>In general, the algorithm should be stopped if it starts to waste CPU-time. Hence, we recommend the following termination criteria mostly related to numerical stability <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>.</p><p>1) Stop if the global best solution found so far at the current generation is within the required precision when compared with optima if the latter are known. 2) Stop if the norm of a progress vector approaches zero within tolerance δ = 10 -30 . 3) Stop if the maximum number of function evaluations (MFEs) is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Measures and Experiments</head><p>1) Performance Measures: to compare the algorithm performance, qualitative and quantitative analyses are often adopted. The former employs graphs or charts to intuitively show which algorithm is faster. The latter employs some numerical criteria to judge which algorithm is better. We choose performance metrics to measure the solution quality and convergence speed, such as FV, expected FV (EFV), expected success rate (ESR), running time (RT), and expected running time (ERT) <ref type="bibr" target="#b33">[34]</ref>.</p><p>1) FV and EFV: FV is the objective FV when the algorithm stops in one run. EFV is the average objective FV of all runs. 2) ESR: The average of the percentage of all runs in which the global optima are successfully located, which depends on given target accuracy ε. Let ε = 10 -5 . 3) RT and ERT: They depend on a given target FV, f t = f opt + f . RT is the number of function evaluations executed in one run. If the best FV has not reached f opt , we set it to MFE. ERT is computed over all relevant trials as the number of function evaluations executed during each trial while the best FV has not reached f opt , summed over all trials divided by the number of trials that have actually reached f opt . 2) Experiments: In order to compare the performance of different algorithms, two categories of experiments are conducted. The first one is to compare different adaptive strategies with our proposed method, and the second one is to compare the performance of different PSO algorithms for global optimization.</p><p>In the following experiments I and II, there are 31 test functions used as benchmarks. Their details can be found in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and <ref type="bibr" target="#b50">[51]</ref>. They are divided into six groups: 1) six unimodal nonrotated; 2) six uni-modal rotated; 3) eleven multimodal nonrotated; 4) four multimodal rotated; 5) two noisy; and 6) two mis-scaled functions.</p><p>1) Experiment I: We aim to compare different parameter adaptive strategies, and in order to compare them fairly, we choose the standard PSO as the basic deployment for all adaptive strategies except their adaptive strategies for the changing parameters. There are five kinds of adaptive strategies: a) linear control in <ref type="bibr" target="#b6">(7)</ref>; b) nonlinear control (9); c) individual-level adaptive strategy shown in <ref type="bibr" target="#b9">(10)</ref>; d) type-II in <ref type="bibr" target="#b10">(11)</ref>; and e) SLC. We denote the first four methods as AS1-AS4. 2) Experiment II: We aim to compare different PSO algorithms for global optimization, that is: a) PSO-w, which is the PSO with adaptive inertia weight <ref type="bibr" target="#b47">[48]</ref>; b) PSO-cf, which is PSO with constriction factor <ref type="bibr" target="#b5">[6]</ref>; c) UPSO, which is unified PSO <ref type="bibr" target="#b41">[42]</ref>; d) wFIPS, which is weighted fully informed particle swarm <ref type="bibr" target="#b35">[36]</ref>; e) FDR-PSO, which is fitness-distance-ratio based PSO <ref type="bibr" target="#b42">[43]</ref>; f) CPSO-H, which is cooperative PSO <ref type="bibr" target="#b53">[54]</ref>; g) CLPSO, which is comprehensive learning PSO <ref type="bibr" target="#b30">[31]</ref>; h) DMS-PSO, which is dynamic multiswarm PSO <ref type="bibr" target="#b29">[30]</ref>; i) APSO-MAN <ref type="bibr" target="#b14">[15]</ref>; and j) APSO-SLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULT ANALYSIS AND DISCUSSION</head><p>This section presents our numerical experiments and results. We focus on the adaptive ability to find global optima, and computational efficiency of the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Validity Test on Parameter Adaptation Strategies</head><p>In experiment I, in order to test the validity of our proposed algorithm, two statistic test methods are employed: one side t-test and one side Wilcoxon rank sum test. We simply call ttest2 and ranksum that are MATLAB's functions to perform the tests <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The null hypothesis is that the mean of two compared data sets is equal, and alternative is that the mean of first data set is less than that of the second one (left tail test).</p><p>If the return result of a test function is 1, we should accept the alternative hypothesis, and otherwise we should exchange the compared sequence to perform the second test. If both results are zeros, we conclude that their means are equal. Because we only compare different parameter adaptation strategies, we must set them in the same environments, i.e., other mechanisms in APSO-MAM and APSO-SLC are temporarily disabled. Except a basic PSO loop, the intelligent multiple search methods (labeled by M2) and mutation method (labeled by M3) in APSO-MAM are not considered <ref type="bibr" target="#b14">[15]</ref>, and the stagnation judgment, back diffusion, and SAR in APSO-SLC are neither considered. The stop criteria are set as follows.</p><p>1) The accuracy ε = 10 -5 is reached or 2) MFE = 3 × 10 5 is reached.</p><p>Because the stop criteria are different from those used in the proposed APSO-SLC, only two situations happen if both algorithms stop: either the best solution reaches a given accuracy or MFE is reached. In order to save space, we only record the number of pair-wise comparison results, as shown in Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_0">II</ref>.</p><p>Tables I and II list the results of t-test and Wilcoxon test on FV and RT for all 31 functions. For all the statistic tests, we set the level of significance is 0.05. In both tables, the symbols "+," "=," and "-" indicate that APSO-SLC performs significantly better than, almost the same as, and significantly worse than the compared method, respectively. From Table <ref type="table" target="#tab_0">I</ref>, by t-test, we can conclude that the parameter adaptation strategy of APSO-SLC is better than the other four adaptation strategies denoted by AS1-AS4. In total 31 functions according to t-test, the number of times that APSO-SLC is better than AS1-AS4 is 12, 10, 9, and 12, respectively; meanwhile, for only 2, 3, 2, and 6 times, it is worse than them. The similar statistical conclusion can be obtained from Wilcoxon tests shown in Table <ref type="table" target="#tab_0">II</ref>. From these results, we conclude that APSO-SLC significantly outperforms the others both in terms of solution quality (FV) and the number of function evaluations (RT) via both tests. They are owing to different adaptation strategies. The strategies in AS1 and AS2 only adaptively tune the inertia weight. But cognitive learning and social learning factors may also have significant impact on PSO performance. Strategies AS3 and AS4, although tuning all three parameters, tend to tune them locally, which leads all particles to search in the same directions and thus be easily trapped into local optima. The strategies in APSO-SLC comprehensively consider historical information and choose the optimal parameters to impose on a search process. They also tune in real-time the "plant" model to capture the new characteristics. In other words, a real-time control mechanism leads to its better performance.</p><p>Although APSO-SLC needs fewer function evaluations, it needs more time to choose the parameters for PSO. The computational time for the compared algorithms is shown in Table <ref type="table" target="#tab_0">III</ref>. From it, in total 31 functions, APSO-SLC is faster than AS1-AS4 for 13, 10, 5, and 0 times only, respectively; but slower for 18, 21, 26, and 31 times. Clearly, its computational efficiency is the worst among them. As the "No free lunch" states, one aspect improvement may lead to performance reduction in other aspects. How to reduce the time complexity is one of our future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison With Other PSO Methods</head><p>To play fairly, we set the same execution conditions as those in <ref type="bibr" target="#b14">[15]</ref>, i.e., APSO-SLC runs 30 times independently for all functions, the population size is set to 30, MFE is set to 3×10 5  and ε = 10 -5 . We only run the proposed algorithm, while all the other experimental data are collected from <ref type="bibr" target="#b14">[15]</ref>, and   <ref type="table" target="#tab_0">III</ref> that lists only the last columns of all the detailed tables as presented in <ref type="bibr" target="#b7">[8]</ref> to save space. In all experiments, if none of 30 runs is successful (SR = 0), we set ERT to be blank implying that ERT = MFE.</p><p>The optimization results on six uni-modal nonrotated functions are summarized in Table IV-1. APSO-SLC outperforms the other eight algorithms, except APSO-MAM, in terms of solution quality (EFV) and convergence rate (ERT). By EFV, the proposed algorithm is similar to APSO-MAM, and APSO-SLC reaches the same accuracy in four functions among all the six ones, except Shifted Schwefel P1.2 (f3) and Shifted Rosenbrock (f5). But APSO-SLC needs more function evaluations than APSO-MAM.</p><p>The optimization results on six uni-modal rotated functions are summarized in Table IV-2. The similar conclusion can be drawn. The proposed algorithm outperforms the other eight algorithms, except APSO-MAM, in terms of solution quality (EFV) and convergence rate (ERT). APSO-MAM outperforms the proposed algorithm on five functions among all six ones in terms of solution quality, except shifted rotated ellipse (f11).</p><p>The optimization results on eleven multimodal nonrotatedfunctions are summarized in Table IV-3. For eight out of 11 multimodal nonrotated functions except functions Shifted Ackley (f17), Shifted Salomon (f20), and Schwefel P2.13 (f21), APSO-SLC is listed as the best algorithm in terms of solution quality, and for Weierstrass (f19), the proposed algorithm exceeds APSO-MAM by solution quality (EFV).</p><p>The optimization results on four multimodal rotated functions are summarized in Table IV-4. For multimodal functions, the proposed algorithm shows excellent performance in terms of solution quality. It outperforms the other algorithms on three multimodal rotated functions except for Shifted Rotated Salomon (f27). However, the cost of APSO-SLC does not show huge superiority. It exhausts all function evaluations on Rotated 2-D minima (f24) and Shifted Rotated Salomon (f27).</p><p>The optimization results on two noisy functions are summarized in Table IV-5. The proposed algorithm is the best among all the algorithms. Note that the success rates for all algorithms are zero. Maybe, the noise in these functions has an impact on all algorithms and misleads a search process.</p><p>The optimization results on two mis-scaled functions are given in Table IV-6. By solution quality, APSO-SLC is better than or equal to all the others. But it needs more computation than APSO-MAM does.</p><p>In the experiments on 31 functions, we can conclude that the proposed algorithm performs better than the eight PSO algorithms. But it is worse than APSO-MAM for most functions, better than APSO-MAM for three functions only among all 31 functions, and its computing cost is larger than APSO-MAM's. The reason may be that the derivation yields excellent effects in APSO-MAM, while the proposed algorithm does not employ any derivation information. As a conclusion, APSO-SLC can take a significant place in thestate-of-art PSO algorithms and SLC makes it insensitive to the initial parameter settings. It should be noted that all finally tuned parameters via the proposed method differ from a run to another and a problem to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Aiming to solve some existing problems in a standard PSO, a new framework based on APSO-SLC is proposed. It regards PSO with its optimization problem as a nonlinear control system, whose inputs are PSO parameters, and output is PSO performance, i.e., the best FVs obtained in each iterative process. A DQP model, which is estimated by the least square methods with a dynamic forgetting factor, is used to describe the relationship between the inputs and output. Because PSO parameters have box constraints, the model is changed to the   DQP model with box constraints (DPM-BC). A standard active set strategy with the gradient projection method is employed to solve its optima for PSO. Besides DPM-BC, the concept of progress vectors is proposed to diagnose whether the algorithm stagnates, and once stagnation is detected, the strategies of back diffusion and SAR are triggered to break it. The proposed APSO-SLC has four parts: 1) PSO; 2) SLC; 3) back diffusion; and 4) social attractor renewal.</p><p>Two kinds of experiments are conducted to test its properties: 1) the effectiveness of the strategies to adapt parameters and 2) the ability to locate all global optima. The results show that: 1) the parameter adaptation strategy based on SLC is effective in comparison with the others and 2) its performance is superior to eight out of nine benchmark algorithms, but slightly weaker than APSO-MAM. Note that for the noise and mis-scale functions, it is better than APSO-MAM.</p><p>Future research directions are as follows: 1) As shown in the experiments, APSO-SLC does not show overwhelming superiority over all the other benchmark algorithms. Especially it is worse than the algorithm employing (estimated) gradient information, i.e., APSO-MAM. Hence, using such information in it should be tried. 2) Dynamic optimization problems are interesting <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b65">[66]</ref>. To solve them, adaptation strategies are particularly crucial for the population-based algorithms. Because DQP-BC employs a dynamic updating strategy, it can trace the performance variation, and dynamically tune parameters. However, whether it can be an excellent choice to control the search of population-based algorithms remains open. 3) Because the roles of the four proposed mechanisms are different, we do not consider the single contribution/importance of each of them. In our unpublished study, we have found that each of them alone cannot lead significant performance improvement of PSO. Thus a deeper study needs to be conducted in the future. 4) In our method, we use a DQP model to capture the relationship between PSO parameters and performance. Thus, we think that the proposed approach regarding parameter learning is similar to but not same as the surrogate modeling method. Their comparison is an interesting topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. APSO-SLC has four modules: PSO, SLC, back diffusion, and social attractor renewal. 4) Social Attractor Renewal: When SLC finds that the stagnation is caused by all the particles trapped into their own attractors by (2), SAR is triggered. It is also similar</figDesc><graphic coords="8,88.99,52.88,433.68,628.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Intuitive interpretation of APSO-SLC.</figDesc><graphic coords="9,74.49,53.24,200.40,318.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ONE</head><label>I</label><figDesc>SIDE t-TEST COMPARISON BETWEEN APSO-SLC AND OTHER PARAMETERS ADAPTIVE STRATEGIES</figDesc><table><row><cell>TABLE II</cell></row><row><cell>ONE SIDE WILCOXON TEST COMPARISON BETWEEN APSO-SLC AND OTHER PARAMETERS ADAPTIVE STRATEGIES</cell></row><row><cell>TABLE III</cell></row><row><cell>COMPUTATIONAL TIME COMPARISON BETWEEN APSO-SLC</cell></row><row><cell>AND OTHER PARAMETERS ADAPTIVE STRATEGIES</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell>-1</cell></row><row><cell>OPTIMIZATION RESULTS FOR UNIMODAL NONROTATED FUNCTIONS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV -</head><label>IV</label><figDesc>2 OPTIMIZATION RESULTS FOR UNIMODAL ROTATED FUNCTIONS the comparison criteria, e.g., EFV, ESR, and ERT defined in Section IV-B1, are the average values of the 30 runs. The results are shown in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell>-3</cell></row><row><cell>OPTIMIZATION RESULTS FOR MULTIMODAL NONROTATED FUNCTIONS</cell></row><row><cell>TABLE IV-4</cell></row><row><cell>OPTIMIZATION RESULTS FOR MULTIMODAL FUNCTIONS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell>-5</cell></row><row><cell>OPTIMIZATION RESULTS FOR NOISE FUNCTIONS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV -</head><label>IV</label><figDesc>6 OPTIMIZATION RESULTS FOR MIS-SCALED FUNCTIONS</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61170305 and Grant 61672024, and in part by FDCT (Fundo para o Desenvolvimento das Ciencias e da Tecnologia) under Grant 119/2014/A3. This paper was recommended by Associate Editor S. Tong.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chaos embedded particle swarm optimization algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alatas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Ozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Soliton. Fract</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1715" to="1734" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic optimization of nonlinear bioreactors</title>
		<author>
			<persName><forename type="first">M.-S</forename><forename type="middle">G</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Balsa-Canto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Banga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Taming Heterogeneity and Complexity of Embedded Control</title>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>ISTE</publisher>
			<date type="published" when="2013-03">Mar. 2013</date>
			<biblScope unit="page" from="307" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review of particle swarm optimization. Part I: Background and development</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anyakoha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="484" />
			<date type="published" when="2007-07">Jul. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Issues in development of simultaneous forward-inverse metamodels</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Winter Simulat. Conf</title>
		<meeting>Winter Simulat. Conf<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonlinear inertia weight variation for dynamic adaptation in particle swarm optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siarry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="859" to="871" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The particle swarm-explosion, stability, and convergence in a multidimensional complex space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="73" />
			<date type="published" when="2002-02">Feb. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quadratic programming with box constraints</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>De Angelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Pardalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toraldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Developments in Global Optimization</title>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="73" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Experiment Results for APSO-SLC</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://www.dropbox.com/s/qbxytqitgh92oik/T-SMC-Sys-WDong-MZhou" />
		<imprint>
			<date type="published" when="2014-01">Jan. 2014. Jan2014</date>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new optimizer using particle swarm theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Symp. Micro Mach</title>
		<meeting>6th Int. Symp. Micro Mach<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tracking and optimizing dynamic systems with particle swarms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Congr</title>
		<meeting>Congr</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="94" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fundamentals of Computational Swarm Intelligence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Engelbrecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Wiley</publisher>
			<pubPlace>Chichester, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hybrid simplex search and particle swarm optimization for unconstrained optimization</title>
		<author>
			<persName><forename type="first">S.-K</forename><forename type="middle">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="527" to="548" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Completely derandomized selfadaptation in evolution strategies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ostermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="195" />
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An intelligent augmentation of particle swarm optimization with multiple adaptive methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<biblScope unit="page" from="68" to="83" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An adaptive particle swarm optimization with multiple adaptive methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="705" to="720" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hierarchical particle swarm optimizer and its adaptive variant</title>
		<author>
			<persName><forename type="first">S</forename><surname>Janson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Middendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1272" to="1282" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic convergence analysis and parameter selection of the standard particle swarm optimization algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="16" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dynamic inertia weight particle swarm optimization algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Soliton. Fract</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="698" to="705" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive fuzzy particle swarm optimization for global optimization of multimodal functions</title>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="4539" to="4549" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stability analysis of the particle dynamics in particle swarm optimizer</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kadirkamanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Selvarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Fleming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="255" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new hybrid approach for dynamic continuous optimization problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nobahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pourtakdoust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1158" to="1167" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Swarm intelligence approaches to optimal power flow problem with distributed generator failures in power networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="353" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Centralized charging strategy and scheduling algorithm for electric vehicles under a battery swapping scenario</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Ammari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="659" to="669" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Neural Netw</title>
		<meeting>IEEE Int. Conf. Neural Netw<address><addrLine>Perth, WA, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Small worlds and mega-minds: Effects of neighborhood topology on particle swarm performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Congr</title>
		<meeting>Congr<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1931" to="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Population structure and particle swarm performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Congr</title>
		<meeting>Congr<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1671" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Kelton</surname></persName>
		</author>
		<title level="m">Simulation Modeling and Analysis</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw Hill</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Composite particle swarm optimizer with historical memory for function optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2350" to="2363" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A hybrid PSO-BFGS strategy for global optimization of multimodal functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-Y</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1003" to="1014" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic multi-swarm particle swarm optimizer</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Swarm Intell. Symp. (SIS)</title>
		<meeting>IEEE Swarm Intell. Symp. (SIS)<address><addrLine>Pasadena, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Comprehensive learning particle swarm optimizer for global optimization of multimodal functions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="295" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recent advances in particle swarm optimization via population structuring and individual behavior control</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE Int. Conf. Netw. Sens. Control (ICNSC)</title>
		<meeting>10th IEEE Int. Conf. Netw. Sens. Control (ICNSC)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="503" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An adaptive particle swarm optimization method based on clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="448" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chaos adaptive improved particle swarm algorithm for solving multi-objective optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TELKOMNIKA Indonesian J. Elect. Eng</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="703" to="710" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The generalized PSO: A new door to PSO evolution</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L F</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Gonzalo</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1453686" />
	</analytic>
	<monogr>
		<title level="j">J. Artif. Evol. Appl</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008-01">2008. Jan. 2008</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The fully informed particle swarm: Simpler, maybe better</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="204" to="210" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the solution of large quadratic programming problems with bound constraints</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toraldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="113" />
			<date type="published" when="1991-02">Feb. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic optimization model for planning of integrated logistical system functioning</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Morozova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Postan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dashkovskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamics in Logistics</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evolutionary dynamic optimization: A survey of the state of the art</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Analysis of a simple particle swarm optimization system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ozcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Eng. Syst. Through Artif. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="253" to="258" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Particle swarm optimization: Surfing the waves</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ozcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Congr</title>
		<meeting>Congr<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1939" to="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">UPSO: A Unified Particle Swarm Optimization Scheme</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Series on Computer and Computational Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="868" to="873" />
			<date type="published" when="2004-01">Jan. 2004</date>
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fitness-distanceratio based particle swarm optimization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Peram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Swarm Intell. Symp. (SIS)</title>
		<meeting>IEEE Swarm Intell. Symp. (SIS)<address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A hybrid particle swarm-gradient algorithm for global structural optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Plevris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papadrakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput.-Aided Civil Infrastruct. Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="68" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blackwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="57" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ratnaweera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Halgamuge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="240" to="255" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-objective particle swarm optimizers: A survey of the state-of-the-art</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes-Sierra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C A</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="308" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A modified particle swarm optimizer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf</title>
		<meeting>IEEE Int. Conf<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parameter selection in particle swarm optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Evol. Program. VII</title>
		<meeting>7th Int. Conf. Evol. Program. VII<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fuzzy adaptive particle swarm optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Congr</title>
		<meeting>Congr</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="101" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Benchmark functions for the CEC&apos;2008 special session and competition on large scale global optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="http://nical.ustc.edu.cn/cec08ss.php" />
	</analytic>
	<monogr>
		<title level="j">Nat. Inspired Comput. Appl. Lab., Univ. Sci. Technol. China</title>
		<imprint>
			<biblScope unit="page" from="153" to="177" />
			<date type="published" when="2007">2007</date>
			<pubPlace>Hefei, China</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dual-objective scheduling of rescue vehicles to distinguish forest fires via differential evolution and particle swarm optimization combined algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2015.2505323</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generalization of the matrix inversion lemma</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Tylavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R L</forename><surname>Sohie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1986-07">Jul. 1986</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1050" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A cooperative approach to particle swarm optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Engelbrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="239" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An analysis of particle swarm optimizers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Bergh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Univ. Pretoria</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Pretoria, South Africa</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A branch-and-cut algorithm for nonconvex quadratic programs with box constraints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vandenbussche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="559" to="575" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Enhancing particle swarm optimization using generalized opposition-based learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahnamayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ventresca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="4699" to="4714" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A polyhedral approach for nonconvex quadratic programming problems with box constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fujie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Global Optim</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="170" />
			<date type="published" when="1998-09">Sep. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adaptive particle swarm optimization; self-coordinating mechanism with updating information</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yasuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Syst. Man Cybern. (SMC)</title>
		<meeting>IEEE Int. Conf. Syst. Man Cybern. (SMC)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2303" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adaptive particle swarm optimization</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1362" to="1381" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stair-like generalized predictive control for improving the output power of proton exchange membrane fuel cell system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE Int. Conf. Control Autom. (ICCA)</title>
		<meeting>10th IEEE Int. Conf. Control Autom. (ICCA)<address><addrLine>Hangzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="67" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Incorporation of optimal computing budget allocation for ordinal optimization into learning automata</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1008" to="1017" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fast and epsilon-optimal discretized pursuit learning automata</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2089" to="2099" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Last-position eliminationbased learning automata</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2484" to="2492" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Guidelines for parameter selection in particle swarm optimization according to control theory</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Nat. Comput. (ICNC)</title>
		<meeting>5th Int. Conf. Nat. Comput. (ICNC)<address><addrLine>Tianjin, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="520" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Robust tracking control of uncertain MIMO nonlinear systems with application to UAVs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA J. Automat. Sinica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">ε-constraint and fuzzy logicbased optimization of hazardous material transportation via lane reservation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="847" to="857" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An efficient outpatient scheduling approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="701" to="709" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Efficient role transfer based on Kuhn-Munkres algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst., Humans</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="491" to="496" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Role transfer problems and algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst., Humans</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1442" to="1450" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Group role assignment via a Kuhn-Munkres algorithm-based solution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst., Humans</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="739" to="750" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Optimal scheduling of complex multi-cluster tools based on timed resource-oriented Petri nets</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2096" to="2109" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Scheduling of single-arm multi-cluster tools with wafer residency time constraints in semiconductor manufacturing</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Semicond. Manuf</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="125" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
