<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A face recognition system based on automatically determined facial fiducial points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Arca</surname></persName>
							<email>arca@dsi.unimi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Scienze dell&apos;Informazione</orgName>
								<orgName type="institution">Università degli Studi di Milano</orgName>
								<address>
									<addrLine>Via Comelico</addrLine>
									<postCode>39/41 20135</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paola</forename><surname>Campadelli</surname></persName>
							<email>campadelli@dsi.unimi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Scienze dell&apos;Informazione</orgName>
								<orgName type="institution">Università degli Studi di Milano</orgName>
								<address>
									<addrLine>Via Comelico</addrLine>
									<postCode>39/41 20135</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raffaella</forename><surname>Lanzarotti</surname></persName>
							<email>lanzarotti@dsi.unimi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Scienze dell&apos;Informazione</orgName>
								<orgName type="institution">Università degli Studi di Milano</orgName>
								<address>
									<addrLine>Via Comelico</addrLine>
									<postCode>39/41 20135</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A face recognition system based on automatically determined facial fiducial points</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">14A5A90CA66A3ADE4485A46906421044</idno>
					<idno type="DOI">10.1016/j.patcog.2005.06.015</idno>
					<note type="submission">Received 21 November 2003; received in revised form 27 June 2005; accepted 27 June 2005</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face and facial feature localization</term>
					<term>Face analysis, recognition, and normalization</term>
					<term>Facial fiducial points</term>
					<term>Gabor wavelet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, a completely automatic face recognition system is presented. The method works on color images: after having localized the face and the facial features, it determines 24 facial fiducial points, and characterizes them applying a bank of Gabor filters which extract the peculiar texture around them (jets). Recognition is realized measuring the similarity between the different jets. The system is inspired by the elastic bunch graph method, while it does no assumption on the scale, pose, and the background. Comparison with standard algorithms is presented and discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of recognizing faces from still or video images has been largely investigated in the last decade, due to its wide application field ranging from the commercial to the security one. All these applications aim at non-intrusive systems, able to recognize a person with no collaboration, the less possible restrictions, and a match process which is computationally efficient.</p><p>Face recognition techniques (FRTs) can be classified according to several criteria <ref type="bibr" target="#b0">[1]</ref>; for our purpose we focus on these two categories: holistic and feature-based matching methods.</p><p>In holistic approaches face recognition is obtained using a single feature vector that represents the whole face image. Examples of this category are eigenfaces <ref type="bibr" target="#b1">[2]</ref>, linear discriminant analysis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, discriminative common vector <ref type="bibr" target="#b4">[5]</ref>, bayesian intrapersonal classifier <ref type="bibr" target="#b5">[6]</ref>, and classifiers trained by examples <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. These techniques have shown to be powerful for classifying frontal views of faces, with no occlusions, and acquired at a fixed scale. Such restrictions are due to the fact that global features are highly sensitive to translations, rotations, scale, and appearance variations. To overcome these problems the most pursued strategy consists in adding a pre-processing step for aligning and normalizing the different faces <ref type="bibr" target="#b8">[9]</ref>, which is itself still an open problem.</p><p>Because of the restrictions imposed by global methods, in the literature there has been a shift to feature-based approaches which have proven to perform better <ref type="bibr" target="#b9">[10]</ref>. These approaches require to localize some features such as eyes, mouth and nose, in order to apply a local-based classification. Within this category, we can distinguish between the approaches which require the rough facial feature localization only, and the ones which need the precise fiducial point estimation. Examples of the first class are the template-based approaches <ref type="bibr" target="#b10">[11]</ref>, the FRT based on support vector machines presented by Heisele et al. <ref type="bibr" target="#b9">[10]</ref>, the eigenfeatures <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, and the LFA <ref type="bibr" target="#b13">[14]</ref> techniques, while the elastic bunch graph matching (EBGM) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> is an example of the second class.</p><p>In this paper, we present a feature-based approach derived from the idea presented in Ref. <ref type="bibr" target="#b15">[16]</ref>, but with a new method to localize the facial fiducial points and a different way to measure the similarity between images. We consider a set of 24 points: the eyebrow and chin vertices, the nose tip, its lateral extremes and its vertical mid point, the eye and lip corners and their upper and lower middle points, the midpoint between the two eyes, and four points on the cheeks. This set is smaller than the one used in Ref. <ref type="bibr" target="#b15">[16]</ref>, allowing to reduce the computational costs, although maintaining high effectiveness.</p><p>The system we propose consists of three modules: a face and features detector, a fiducial points estimator and a face recognizer. The first two exploit both color and shape information, while the recognition step characterizes each face with the texture information only. In particular, the method searches for skin regions on the basis of their peculiar color, and then validate as "face" the one which has at least one eye within it. The other facial features are then localized and processed in order to identify the corresponding fiducial points. Subsequently, to characterize, compare and recognize the faces, we apply a bank of Gabor filters in correspondence to each fiducial point. We notice that this step requires to process faces with approximately the same expression and the same size. Regarding the former request, we assume the faces have almost neutral expression, while for the latter, we automatically normalize the images on the basis of the fiducial point positions.</p><p>When images of people in different pose are available, we build different galleries, each one containing one image per person. This choice has been driven by the observation that the recognition is more robust when the angular disparity between the gallery images and the test ones is at most of 15 • <ref type="bibr" target="#b16">[17]</ref>.</p><p>Thus, given a test image, the system computes its face characterization (the jets vector J), determines its pose (frontal, right or left rotated) on the basis of the eyes and nose fiducial points, selects the reference gallery, G, and measures the similarity between J and the jets vectors of the images in G. The face is recognized to be the one in the gallery which maximizes a similarity measure that we defined in Ref. <ref type="bibr" target="#b17">[18]</ref>.</p><p>Tests on the whole system and comparisons of it with standard algorithms have been carried out on two databases: a public and standard one, the XM2VTS <ref type="bibr" target="#b18">[19]</ref>, and ours, the UniMiDb.</p><p>The XM2VTS database consists of high quality color images of frontal people with neutral expression acquired over a homogeneous dark background; the illumination is always uniform and good, and the image scale is fixed (about (230 × 300) pixels). The data set contains four recordings of 295 subjects, taken over a period of four months.</p><p>To test the system with respect to scale and pose variations, we acquired 400 color images (UniMiDb database) of very different scales ranging between (50 × 60) and (500 × 650) pixels, with homogeneous and light-colored background, under frontal and diffuse illumination. Faces can be either in frontal position (100 images), rotated around the head vertical axis of 30 • at most, or tilted laterally of about 10 • (300 images).</p><p>The remaining of this paper is organized as follows: in Sections 2 and 3 a synthetic overview of the facial feature localization and the fiducial point estimation techniques is reported; in Section 4 a method for checking the fiducial points reliability is described; in Section 5 the face local characterization and the adopted similarity measure are introduced. Finally, in Sections 6 and 7 the obtained results are reported and discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Face and facial features localization</head><p>The first fundamental step consists in localizing the face in the image and the corresponding facial features (eyes, nose, mouth, and chin). In Ref. <ref type="bibr" target="#b19">[20]</ref>, we proposed a completely automatic face localization system, which does not require any manual setting or normalization like in Refs. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>, and that works on color images representing faces standing on generic backgrounds. This face locator consists of two subsequent modules. First, it identifies the regions (skin-map) whose color is similar to the color of the skin; this step does not pretend to detect only the face region, while it restricts significantly the search area for the subsequent module; in fact we expect that each skin-map region may represent either a true skin region (the face, an arm, a shoulder, etc.) or a skin false positive (a pink wall, etc.). To validate the one corresponding to a face and discard the others, we scan the skin-map with a support vector machine (SVM) trained to recognize eyes: the skin region that includes the two positions with the highest confidence to be eyes, is recognized to be the face. The notion of confidence depends both on the mutual position of the points and on their margins, that is the strength of the classification response on them. However, we fix an absolute threshold on margins in order to make sure that what we select is really relevant (it can happen to find only one eye if it is the only strong candidate within the image). In this way, we come to discard all nonfaces (regions with no eyes) and to locate the face as the only region containing eyes. The method, which has been extensively experimented also on outdoor images, is robust to scale, illumination, and pose variations, and deals with partial occlusions (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>After having detected the eyes, the mouth is localized exploiting its peculiar color and shape, and the nose, eyebrows and chin are detected on the basis of the eyes and mouth positions <ref type="bibr" target="#b21">[22]</ref>.</p><p>We experimented this method on the 1180 color images of the XM2VTS and on the 400 images of the UniMiDb  databases, reporting correct localization of all the features, respectively, in 97.7% and 98.8% of the cases (Fig. <ref type="figure" target="#fig_1">2</ref>). The performance obtained on the XM2VTS is slightly worse since the database includes 430 images of people wearing spectacles; making explicit the behavior on the two subsets, we obtain 96.7% and 98.3%, respectively, in case of subjects with and without glasses. These results are comparable with those obtained by Smeraldi and Bigun in Ref. <ref type="bibr" target="#b22">[23]</ref> on the XM2VTS database, but their method is scale and pose dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fiducial point estimation</head><p>In this section, we describe the fundamental steps followed to determine the facial fiducial points. For this and the subsequent processing we consider only subjects without glasses, that is, all the images of the UniMiDb database and 750 images of the XM2VTS for a total of 1150 images.</p><p>We process each feature sub-image separately, adopting different techniques according to the feature peculiarities.</p><p>This method allows to determine 16 fiducial points per image: the eyebrow and chin vertices, the nose tip, and the eye and lip corners and upper and lower middle points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Eyes</head><p>The eye is described by a parametric model which is inspired by the deformable template proposed by Yuille and others <ref type="bibr" target="#b23">[24]</ref> but with significant simplifications (6 parameters instead of 11) and variations: we have made a great deal to improve the template initialization, and to exploit the color information.</p><p>Given an eye sub-image, at first the iris is localized, allowing to initialize the eye deformable template in a plausible position and scale, which are two required conditions to obtain correct results; to this end we adopt the Hough transform for circumferences (Fig. <ref type="figure" target="#fig_2">3</ref> (left) and (center)); afterwards the eventual presence of a reflection is determined and removed (Fig. <ref type="figure" target="#fig_2">3</ref> (right)), since otherwise the strong reflection edges would make the template converge to them, bringing to a wrong eye description.</p><p>The eye model is made of two parabolas, representing the upper and lower eye arcs, and intersecting at the eye corners (Fig. <ref type="figure" target="#fig_3">4</ref>); the model is described by the parameters p = {x w , y w , a, b, c, } that is, the eye center coordinates (x w , y w ), the eye upper and lower semi-heights a and c, the eye semi-width b, and the rotation angle expressed with respect to the eye center. In order to adapt the generic template to a specific eye, we minimize an energy function E t which is a function of the template parameters and of the image characteristics (prior information on the eye shape, edges, and 'white' of the eye). The characteristics are evaluated on the u plane of the CIE-Luv 3 space, since in this color plane the information we are looking for (edges and 'white' of the eye) are strengthened and clearer (Fig. <ref type="figure" target="#fig_4">5</ref>).</p><p>More precisely:</p><formula xml:id="formula_0">E t = E prior + E e + E i ,</formula><p>where</p><formula xml:id="formula_1">E prior = k 1 /2((x w -x i ) 2 + (y w -y i ) 2 ) + k 2 /2(b -2r) 2 + k 3 /2((b -2a) 2 + (a -2c) 2 ),<label>(1)</label></formula><p>being (x i , y i ) the iris center and r the iris radius obtained previously by the Hough transform;</p><formula xml:id="formula_2">E e = -c 1 /|jR w | • jR w e ( x) ds,<label>(2)</label></formula><p>being jR w the upper and lower parabolas, and e the edge image;</p><formula xml:id="formula_3">E i = -c 2 R w i ( x) ds,<label>(3)</label></formula><p>being R w the region enclosed between the two parabolas, and i the weighted image obtained with an adaptive threshold method.</p><p>The function is optimized adopting a search strategy based on the steepest descent, as suggested in Yuille's work; once obtained the eye contour description, we derive the two eye corners and the upper and lower middle points straightforwardly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Eyebrows</head><p>A good description of the eyebrow is given by the best parabola which approximates it. In order to find it, we calculate the vertical derivative of the eyebrow gray-level image, 3 Uniform color space introduced by the CIE (Commission Internationale de l'Eclairage) to represent properly distances between colors <ref type="bibr" target="#b24">[25]</ref>.</p><p>we binarize it keeping the 10% of the highest derivative values, and we extract the upper border of the obtained regions. We then proceed fitting these points to a parabola and taking the corresponding vertex as the eyebrow fiducial point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Nose</head><p>The nose is characterized by very simple and generic properties which are true independently of the pose: the nose has a 'base' which gray-levels contrast significantly with the neighbor regions; moreover, the nose profile can be characterized as the set of points with the highest symmetry and high luminance values; finally we can say that the nose tip lies on the nose profile, above the nose base line, and corresponds to bright gray-levels. These considerations allow to localize the nose tip robustly (Fig. <ref type="figure" target="#fig_5">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pose estimation and face normalization</head><p>The procedures described so far do not make any assumption on either face pose or scale. However, the subsequent steps require this information: the head pose estimation is useful both to determine the mouth fiducial points and to refer to the proper gallery constituted by either frontal, left or right rotated faces; the estimated face scale allows to normalize it in order to make all the faces comparable in the face recognition phase.</p><p>To this end, we consider the triangle T defined by the nose tip, N , and the two external eye corners, E sx and E dx ; the face pose is estimated comparing the length of the segments NE sx and NE dx : in case of frontal faces, they are approximately the same, while when the face is left rotated the side NE dx is longer than NE sx , and vice versa when the face is right rotated. Thus, we calculate the ratio r = NE dx /NE sx , concluding that the face is frontal if 0.8 r 1.2, it is left rotated if r &lt; 0.8 and right rotated if r &gt; 1.2 (Fig. <ref type="figure">7</ref>).</p><p>To both infer robustly the image scale and reduce it to a fixed size we refer to the area of the triangle T : we scale all the faces so that their triangle area is of 2000 pixels, making all the images comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Mouth</head><p>Our goal for the mouth is the determination of its corners and of its upper and lower middle points. To this aim, we determine the entire contour adopting a parametric model derived from the deformable template proposed by Yuille et al. <ref type="bibr" target="#b23">[24]</ref>.</p><p>Two different models (Fig. <ref type="figure">8</ref>) have been defined according to the face position: frontal and laterally rotated. In both cases the upper lip is modelled by two cubics, while the lower lip is modelled by a parabola for frontal images, and by a cubic for rotated faces. The choice of the proper template is based on the estimated pose.     To make the parametric model converge, the template initialization is fundamental; therefore, the algorithm estimates the mouth corners and hooks the template to them. To this end, we determine the 'lip cut', and we take its extremes. The lip cut is characterized by having high vertical derivative values, and low gray-level values; combining this information we obtain a robust localization <ref type="bibr" target="#b25">[26]</ref>.</p><p>Once determined the mouth corners (x 1 , y 1 ) and (x 2 , y 2 ), we adapt the generic template to the shape of a specific mouth, minimizing two energy functions. Both of them are functions of the template parameters, but the first, E i , depends on the image colors, while the second, E e , depends on the image edges. More precisely:</p><formula xml:id="formula_4">E i = -c 1 /|R c | R c i ( x) dA,<label>(4)</label></formula><p>where |R c | denotes the area included in the three curves composing the template and i is a binary image called mouth-map determined as follows:</p><p>• determination of the mouth-map, by means of a transformation in the Y C b C r color space mouth-map = (255</p><formula xml:id="formula_5">-(C r -C b ))C 2 r</formula><p>• binarization of the mouth-map, using a clustering algorithm.</p><p>• for every pixel p i (p) = 255 if p is white, -80 if p is black.</p><formula xml:id="formula_6">E e = -c 2 /|jR c | jR c e ( x) ds,<label>(5)</label></formula><p>where |jR c | gives the length of the three curves and e is the gradient image obtained applying the Sobel filter to the mouth gray-level sub-image.</p><p>Once determined the mouth contour, the fiducial points estimation is an easy task. The mouth corners (x 1 , y 1 ) and (x 2 , y 2 ) have already been determined.</p><p>The mouth lower extremum (x low , y low ) corresponds to the lower extremum of the parabola/cubic describing the lower lip. The upper middle point (x up , y up ) is identified as the intersection point between the two cubics describing the upper lip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Chin description</head><p>To localize and describe the chin, we determine the best parabola which approximates it and its vertex. The chin shape information is extracted applying to the gray-level image a non-linear edge detector <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. At this stage, knowing the positions and the dimensions of the facial features, we can restrict significantly the parabola search area, and in particular the position of its vertex, obtaining a good chin approximation (Fig. <ref type="figure" target="#fig_7">9</ref>).</p><p>In Fig. <ref type="figure" target="#fig_8">10</ref> we report some outputs obtained running all the steps described above. In particular, we remark the robustness of the method to the presence of either beard, moustache or fringe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Selection of the reliable fiducial points and error correction</head><p>Both the feature localization and the fiducial point estimation introduce some errors. However, it happens very seldom that, given a face image, all its features are wrongly localized or described. To detect and correct any error, we determined some rules based on simple geometrical constraints satisfied by faces of a fixed size. In particular, to statistically estimate both the typical feature dimensions and their relative positions, we considered 200 normalized images whose features had been correctly localized and the fiducial points well determined (Table <ref type="table" target="#tab_0">1</ref>), considering the distances drawn in Fig. <ref type="figure" target="#fig_9">11</ref>.</p><p>The control method starts verifying the correctness of the feature positions, checking if the eyes are vertically aligned (their vertical distance, Dist Y, is lower than twice the calculated variance 4 ) and the mouth is centered with respect to the eye positions (its mid-point abscissa is within the abscissae of the two eye mid-points). If one of the features does not satisfy these conditions, the method redetermines its position, inferring it on the basis of both the calculated measures (Table <ref type="table" target="#tab_0">1</ref>) and the information on the other two features, and then the corresponding fiducial points are recalculated.</p><p>This step lowers the feature localization error from 1.5% (the mean feature localization error on all the 1150 images without glasses) to 0.3%.  Afterwards, the method deals with fiducial point errors, applying to each feature the following steps:</p><p>• Eye: Evaluate if its area is in the range ( (eye area) ±2• (eye area)) and the ratio between its height and width is greater than 0.7. If these conditions are not satisfied, the eye fiducial points are recalculated modelling the iris with a different circumference, and thus changing the initialization of the deformable template. Besides, the point of the corresponding eyebrow is recalculated too. • Mouth: evaluate if its area, A, is in the range ( (mouth area) ±2 • (mouth area)). If this condition is not satisfied, the deformable template is forced to shrink or to enlarge (depending on the over or under-estimation of the area A) with respect to the mean value of the mouth area. After having changed the mouth fiducial points, the chin vertex is searched for again, but in a more precisely defined area.</p><p>Finally, the control method evaluates the new calculated fiducial points: regarding the eyes and mouth it checks if they verify the conditions stated above and, if not, it discards them definitely. Regarding the nose and chin fiducial points, it discards them if their abscissae are not within the mouth corners. Finally, regarding the eyebrow fiducial points, they are discarded if they are either too distant from the centroid of the corresponding eye (A &gt; (A) + 2 • (A)) or too unaligned (the abscissae are not within the eye corners).</p><p>In Table <ref type="table" target="#tab_1">2</ref>, we report the percentage of detected errors and the ones that the system did not correct bringing to the elimination of the corresponding fiducial points. The decision to eliminate erroneous points is based on the observation that the recognition performance is much more degraded by erroneous than by missing points. Fig. <ref type="figure" target="#fig_1">12</ref>. Big red points: fiducial points inferred from the calculated ones (small white points).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Inference of additional fiducial points</head><p>Once determined the reliable fiducial points, we infer from them eight additional ones (Fig. <ref type="figure" target="#fig_1">12</ref>), in order to gather more information for the recognition. Their derivation is obtained defining simple geometric rules on the basis of the reliable fiducial points.</p><p>We notice that, in case of rotated faces, one of the lateral extreme of the nose is completely hidden and the points on the corresponding cheek are unreliable, considering they often fall in correspondence to the face border. Thus, on the basis on the estimated pose, we decide whether to consider the whole set of points or to exclude the three on the most hidden cheek.</p><p>Experimentally we have observed that, for neutral expression faces, the additional fiducial points allow to increase the recognition rate systematically; on the contrary, in case of non-neutral expression, they cause a loss of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Face characterization</head><p>Once the fiducial points have been extracted, the pose determined, and the face re-scaled, we proceed characterizing each fiducial point in terms of the surrounding gray-level portion of image. Two techniques have been experimented: the steerable Gaussian first derivative basis filters and the Gabor wavelet transform. Both of them are biologically motivated and in both cases a bank of filters is adopted, varying the orientations and the frequencies/scales.</p><p>The steerable Gaussian first derivative basis filters behaved well only referring to a small gallery, and dealing with small head rotations; on the contrary the Gabor wavelet transform proved to be more scalable and robust; for these reasons we introduce the latter technique only.</p><p>Following the idea of Wiskott <ref type="bibr" target="#b15">[16]</ref>, to characterize a fiducial point, we convolve the portion of gray-image around it with the following bank of Gabor kernels:</p><formula xml:id="formula_7">j ( x) = k 2 j 2 exp - k 2 j x 2 2 2 • exp i k j x -exp - 2 2 ,</formula><p>in the shape of plane waves with wave vector k j , restricted by a Gaussian envelope function. We employ a discrete set of five different frequencies and eight orientations, indexed by v = 0, . . . , 4 and = 0, . . . , 7, respectively,</p><formula xml:id="formula_8">k j = k jx k jy = k v cos k v sin ,</formula><p>where</p><formula xml:id="formula_9">k v = 2 -(v+2)/2 , = 8 ,</formula><p>with index j = + 8v. The width /k of the Gaussian is controlled by the parameter = 2 . We observe that the kernels are DC-free, that is j ( x) d 2 x = 0 allowing to deal with different illumination conditions.</p><p>The obtained 40 coefficients are complex numbers. A jet <ref type="bibr" target="#b27">[28]</ref> J is obtained considering the magnitude parts only.</p><p>Applying the Gabor wavelet transform to all the facial fiducial points, we obtain the face characterization, consisting in a jets vector of (40 × |M|) real coefficients, where M is the set of maintained fiducial points.</p><p>To recognize a test image T , we compute for each image k in the gallery G a score, representing the closeness of k to the test image T ; the face in T is recognized as the one in the gallery which obtained the highest score.</p><p>In particular, we proceed as follows:</p><p>• for each image k ∈ G and each fiducial point i ∈ M, compute the similarity measure between pairs of corresponding Jets:</p><formula xml:id="formula_10">S k,i = S(J T ,i , J k,i ) = z J T ,i z J k,i z z J T ,i z 2 z J k,i z 2 ,</formula><p>where z = 0, . . . , 39, and J T ,i is the Jet in the test image corresponding to the ith fiducial point.</p><p>• for each fiducial point i, order the values {S k,i } in descending order, and assign to each of them a weight w k,i as a function of its ordered position p k,i .</p><p>The weight w k,i is determined by the function f (p k,i ):</p><formula xml:id="formula_11">f (p k,i ) = c • ln(x + y) -ln(x + p k,i ) ,</formula><p>where y = |G| 4 , x = e -1/2 , and c is a normalization factor.</p><p>• for each gallery image k, consider the set, BestPoints, of the 13 fiducial points 5 which have the highest weights, and determine the score:</p><formula xml:id="formula_12">Score(k) = i∈BestP oints w k,i S k,i .</formula><p>This technique gives better results than the computation of the average over all the similarities <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref>, since it allows to discard wrong matches on single points: if some fiducial points are not precisely localized either in the test or in the gallery images, they will have low similarity measures so that they will not belong to the set BestPoints, and they will not be used for the recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head><p>To evaluate the performance of our algorithm, different experiments on the XM2VTS and the UniMiDb databases have been carried out. In Section 6.1, we report the results obtained running our algorithm, while in Section 6.2 we compare our performance with the ones given by standard algorithm (PCA, LDA, Bayesian classifier) as implemented in Ref. <ref type="bibr" target="#b28">[29]</ref>.</p><p>The performances are evaluated according to the cumulative match characteristic CMC metric presented in Ref. <ref type="bibr" target="#b29">[30]</ref> and defined as</p><formula xml:id="formula_13">P R (r) = |C(r)| |T | • 100,</formula><p>where C(r) is the set of images in the test set T that are recognized at rank r or better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Performance of our method</head><p>We set three experiments, in order to test our method with respect to different kind of difficulties. In these experiments, the algorithm determines the fiducial points as described so far, and it constructs the galleries and the test sets according to the acquisition sessions. 5 The cardinality of the set BestPoints has been set to the half of the total number of available fiducial points (24) plus 1, that is 13. This number is a trade-off between the necessity to both maintain enough information and discard the less unreliable fiducial point contribution.</p><p>Experiment 1 refers to the images of subjects without glasses of the XM2VTS database. Aim of this experiment is to test the robustness of the system on images taken over a relatively long period (four months). To this end, we consider as gallery the images acquired in the first session, and we test the system with the remaining images organized according to the acquisition sessions T1, T2, and T3. With this setting, we obtain a gallery with 185 subjects, that is, all the ones acquired in the first session which have at least one correspondence in the subsequent ones.</p><p>The obtained results are reported in Table <ref type="table" target="#tab_2">3</ref>-(Experiment 1); they display that the system works well independently of the session, showing a robustness to the time variation. Analyzing the errors, we notice that most of the wrong matches take place on test images representing faces whose eyes look either up, down or laterally, or when the mouth shape is not neutral.</p><p>Experiment 2 has been done to evaluate if a greater gallery cardinality makes the system performance decrease. To this end we constructed a gallery which includes both the 50 frontal face images of the UniMiDb acquired in the first session, and all the first-session images of the XM2VTS database (294 images) including the ones representing people wearing glasses; in this case the eye fiducial points have been manually determined. The test sets T1, T2 and T3 correspond to the three last acquisition sessions of the XM2VTS, including the subjects wearing glasses; the test set T f consists of the remaining frontal face images of the UniMiDb. The obtained results are reported in Table <ref type="table" target="#tab_2">3</ref>-(Experiment 2).</p><p>Comparing the results obtained referring, respectively, to a 185 and a 344-subject gallery (Experiment 1 and 2), we observe a loss of about the 2% of recognition against an increment of the gallery cardinality of 159 subjects, that is almost a doubling of gallery size. This result is consistent with the best ones presented in the literature <ref type="bibr" target="#b0">[1]</ref>.</p><p>Experiment 3 refers to the UniMiDb, which consists of both frontal and rotated faces, acquired at very different scales. Aim of this experiment is to test the system robustness to pose and scale variations. According to the head pose, we automatically construct three 50-subject galleries (G f , G r , and G l ), and three test sets (T f , T r , and T l ): given all the images of a subject, we cluster them according to the head pose (frontal, right and left rotated), and, for each pose cluster, we put the image acquired in the first session in the corresponding gallery and the remaining ones in the proper test set.</p><p>Running the recognition experiment on the three test sets, we obtained the results reported in Table <ref type="table" target="#tab_2">3</ref>-(Experiment 3), showing the system performs equally well both on frontal and rotated faces. The high robustness to pose and scale variations is due to the procedure described in Section 3.4 that allows us to compare faces which are approximately in the same pose and are normalized to the same size.</p><p>As far as the computational time is concerned, all the steps required to determine and characterize all the fiducial points, on an image of (500 × 650) pixels, is on average 30 s on a Pentium 4 with clock 3.2 GHz, where the feature template adaptation is the most demanding step; we notice that the code is written in an interpreted language and it is not optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison with standard method</head><p>Most of the FRTs presented in the literature <ref type="bibr" target="#b0">[1]</ref> have been tested on gray-level images (e.g. the FERET standard database <ref type="bibr" target="#b30">[31]</ref>), preventing a straightforward comparison with our system, which requires color images as input. Thus, in order to make possible a direct comparison, we run the CSU face identification evaluation system (Version 5.0) <ref type="bibr" target="#b28">[29]</ref> with the three experimental settings defined in Section 6.1. The algorithms provided in the CSU system <ref type="bibr" target="#b31">[32]</ref> are the principal component analysis (PCA), a combined principal component analysis and linear discriminant analysis (LDA), a Bayesian intrapersonal/extrapersonal classifier (BIC), and the elastic bunch graph matching (EBGM). We observe that we did not test the EBGM algorithm, since it requires as input 80 fiducial points that we do not have, and we thought it unreasonable to set them manually.</p><p>These methods consist of four stages: preprocessing, training, testing, and analysis. The first step aims at aligning and normalizing the faces to a given scale on the basis of the location of four fiducial points (the centroids of eyes, nose, and mouth); in our tests we computed them exploiting the ones determined by our method described in Section 3. Regarding the training stage, we realized it on a set of 100 images, corresponding to 50 subjects; we notice that the training and test sets are not disjoint because of the requirement to feed the training with at least two images per subject. Finally, the test and analysis stages have been realized setting the gallery and the test images according to the specifications given in Section 6.1, with the only difference that at this stage we do not discriminate the results in the different test sets, thus considering all the test images of the same experiment together. In Table <ref type="table" target="#tab_3">4</ref>, we report the obtained results, and in Figs. <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> we graphically summarize them.  On the basis of these experiments, we can conclude that in case of frontal faces (Comparisons 1 and 2) our method behaves slightly worse than the two best ones (the PCA with the Euclidean distance and the LDA soft methods), while it performs better than the remaining methods especially when the gallery cardinality is higher (Comparison 2).</p><p>We observe that our method outperforms all the others when referring to rotated faces (Comparison 3), showing high robustness to pose variation.</p><p>Besides, we observe that the results we obtained with the Bayesian algorithm are strongly dependent on the images used in the training set; this behavior seems to show the method is very unstable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>In this paper, we have presented a new face recognition method which is inspired by the EBGM <ref type="bibr" target="#b15">[16]</ref>, but it has a completely different feature localization step, a different way to measure the similarity between images, and it works on a smaller set of points thus reducing the computational costs of their characterization.</p><p>The method works on color images and its main characteristics are the scale and pose independency. A comparison with standard algorithms <ref type="bibr" target="#b28">[29]</ref> shows that on frontal faces it behaves slightly worse than the PCA with the Euclidean distance and the LDA, being better than the others, and on rotated ones it outperforms all the algorithms. This is due to the fact that we are able to determine the subject pose and exploit this relevant information both in the gallery construction and in test phase. Besides, we must notice that the standard algorithms we referred to, require the location of four fiducial points to align and normalize the faces; nothing is said about the way to derive them from images of arbitrary scale. In our tests, we computed them automatically from the points determined by our method.</p><p>According to Ref. <ref type="bibr" target="#b0">[1]</ref>, the best system of the face recognition vendor test 2002 (FRVT2002) reached a recognition rate of the 85% on a database of 800 people. We do not have results on such a gallery cardinality, but, given that our system performance is about 95% on a gallery of 344 subjects, we do not expect the system performance to decrease more than the 10% doubling the gallery size.</p><p>Open problems are to deal with images of people wearing glasses and with partial occlusions. Regarding the former, we think an ad hoc algorithm is necessary; thus, we have developed a first module which detects the glasses presence. Afterwards, we intend to base the eye fiducial point determination on the coordinates given by the SVM, since the image portions corresponding to the eyes are often corrupted by either reflections on the lens or the glasses frame itself, which both mislead the deformable template. In this context, we are doing a statistical analysis of the relation between the intra-ocular distance derived from the SVM data and the eye template parameters. With regards to faces with partial occlusions, our method is potentially suitable to deal with them, being component-based; however, we have to develop a module for recognizing their presence.</p><p>A last consideration refers to the current challenge in face recognition <ref type="bibr" target="#b32">[33]</ref>, that is to work on high resolution images captured outdoors and with generic background. The method for face and feature localization, we presented can deal successfully with these situations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Some results of the eye localization step, showing both the detected skin-maps, and the localized eyes.</figDesc><graphic coords="3,120.94,70.90,360.00,74.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Some subdivision results on frontal, tilted, and rotated faces.</figDesc><graphic coords="3,120.94,187.84,360.00,113.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Iris and reflection localization: input to the Hough transform; iris description; detected reflection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Deformable template of the eye.</figDesc><graphic coords="4,72.86,71.49,180.00,120.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Eye image processing (from left to right, top to bottom): original image; u plane; gradient of the u plane; binarization of the u plane; model initialization; final result.</figDesc><graphic coords="5,120.94,86.62,360.00,80.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of nose image process. The black horizontal line indicates the nose base; the black dots along the nose are the symmetry points for the corresponding row; the red line is the vertical axis approximating the symmetry points; the green marker indicates the nose tip.</figDesc><graphic coords="5,120.94,382.52,360.00,165.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Examples of three different poses and the corresponding segments which join the eye external corners with the nose tip.</figDesc><graphic coords="5,119.94,622.39,362.40,95.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Some chin descriptions on both frontal and rotated faces.</figDesc><graphic coords="6,115.04,71.78,312.12,125.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Some results on the two databases.</figDesc><graphic coords="6,115.04,260.27,339.12,124.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Distances considered for the fiducial point selection. We observe that the distance A is defined in correspondence to both eyes, even if in the figure we have shown it only once.</figDesc><graphic coords="7,348.45,71.37,168.12,171.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Fig. 13. CMC curves of the results reported inTable 4-(Comparison 1).</figDesc><graphic coords="11,86.93,323.79,187.97,134.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. CMC curves of the results reported inTable 4-(Comparison 3).</figDesc><graphic coords="11,349.62,82.43,186.21,129.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Means and variances of the parameters taken into account for the feature selection</figDesc><table><row><cell>Measure</cell><cell>Mean</cell><cell>Variance</cell></row><row><cell>Mouth area</cell><cell>940</cell><cell>91</cell></row><row><cell>Eye area</cell><cell>296</cell><cell>28</cell></row><row><cell>Dist X</cell><cell>63</cell><cell>5</cell></row><row><cell>Dist Y</cell><cell>0</cell><cell>3</cell></row><row><cell>Dist A</cell><cell>18</cell><cell>3</cell></row><row><cell>Dist B</cell><cell>81</cell><cell>8</cell></row><row><cell>Dist C</cell><cell>80</cell><cell>8</cell></row></table><note><p><p>4 </p>If the two eyes are unaligned the method considers wrong the one whose distance from the mouth (Dist B or Dist C) is further from the corresponding mean value.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Percentage of detected errors before and after the corrections</figDesc><table><row><cell>Kind of error</cell><cell>Error detected (%)</cell><cell>Feature discarded (%)</cell></row><row><cell>Feature localization</cell><cell>1.5</cell><cell>0.3</cell></row><row><cell>Two eyes</cell><cell>0.5</cell><cell>0.0</cell></row><row><cell>One eye</cell><cell>2.2</cell><cell>0.4</cell></row><row><cell>Mouth</cell><cell>3.5</cell><cell>2.5</cell></row><row><cell>Eyebrows, nose, chin</cell><cell>6.5</cell><cell>1.9</cell></row><row><cell>No error</cell><cell>85.8</cell><cell>94.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Recognition results obtained referring, respectively, to a 185-frontal subject gallery, a 344-frontal subject gallery, and to a 50-subject gallery which includes both frontal and rotated faces</figDesc><table><row><cell>Test set</cell><cell>P R (1)</cell><cell>P R (2)</cell><cell>P R (3)</cell><cell>P R (4)</cell><cell>P R (5)</cell><cell></cell></row><row><cell>Experiment 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T1</cell><cell>96.7</cell><cell>98.9</cell><cell>98.9</cell><cell>99.5</cell><cell>100</cell><cell></cell></row><row><cell>T2</cell><cell>97.7</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell></cell></row><row><cell>T3</cell><cell>97.8</cell><cell>98.3</cell><cell>98.9</cell><cell>99.4</cell><cell>99.4</cell><cell></cell></row><row><cell>Experiment 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T1</cell><cell>95.6</cell><cell>98.3</cell><cell>99.0</cell><cell>99.7</cell><cell>100</cell><cell></cell></row><row><cell>T2</cell><cell>95.2</cell><cell>96.9</cell><cell>97.2</cell><cell>98.3</cell><cell>98.7</cell><cell></cell></row><row><cell>T3</cell><cell>95.6</cell><cell>95.6</cell><cell>96.3</cell><cell>96.9</cell><cell>97.3</cell><cell></cell></row><row><cell>T f</cell><cell>96.0</cell><cell>98.0</cell><cell>98.0</cell><cell>98.0</cell><cell>98.0</cell><cell></cell></row><row><cell>Experiment 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T f</cell><cell>96</cell><cell>98</cell><cell>98</cell><cell>100</cell><cell>100</cell><cell></cell></row><row><cell>T r</cell><cell>96</cell><cell>98</cell><cell>98</cell><cell>98</cell><cell>98</cell><cell></cell></row><row><cell>T l</cell><cell>96</cell><cell>98</cell><cell>98</cell><cell>98</cell><cell>98</cell><cell></cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Comparison of our method with the CSU ones, referring respectively to the experiment 1, 2, and 3</cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell></cell><cell>P R (1)</cell><cell>P R (2)</cell><cell>P R (3)</cell><cell>P R (4)</cell><cell>P R (5)</cell></row><row><cell cols="2">Comparison 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our</cell><cell></cell><cell></cell><cell></cell><cell>97.4</cell><cell>98.9</cell><cell>99.3</cell><cell>99.4</cell><cell>99.8</cell></row><row><cell>Bayesian</cell><cell></cell><cell>MAP</cell><cell></cell><cell>95.38</cell><cell>99.8</cell><cell>99.8</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell>ML</cell><cell></cell><cell>95.38</cell><cell>99.8</cell><cell>99.8</cell><cell>100</cell><cell>100</cell></row><row><cell>LDA</cell><cell></cell><cell>Soft</cell><cell></cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>PCA</cell><cell></cell><cell>Euclidean</cell><cell></cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell>Mah angle</cell><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Comparison 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our</cell><cell></cell><cell></cell><cell></cell><cell>95.4</cell><cell>97.0</cell><cell>97.5</cell><cell>98.27</cell><cell>98.59</cell></row><row><cell>Bayesian</cell><cell></cell><cell>MAP</cell><cell></cell><cell>72.2</cell><cell>79.6</cell><cell>85.5</cell><cell>88.6</cell><cell>90.5</cell></row><row><cell></cell><cell></cell><cell>ML</cell><cell></cell><cell>72.2</cell><cell>79.6</cell><cell>85.5</cell><cell>88.6</cell><cell>90.5</cell></row><row><cell>LDA</cell><cell></cell><cell>Soft</cell><cell></cell><cell>97.1</cell><cell>97.3</cell><cell>97.5</cell><cell>97.5</cell><cell>97.5</cell></row><row><cell>PCA</cell><cell></cell><cell>Euclidean</cell><cell></cell><cell>99.6</cell><cell>99.8</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell>Mah angle</cell><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Comparison 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our</cell><cell></cell><cell></cell><cell></cell><cell>96</cell><cell>98</cell><cell>98</cell><cell>98</cell><cell>98</cell></row><row><cell>Bayesian</cell><cell></cell><cell>M A P</cell><cell></cell><cell>1 2</cell><cell>1 2</cell><cell>1 2</cell><cell>1 2</cell><cell>1 2</cell></row><row><cell></cell><cell></cell><cell>ML</cell><cell></cell><cell>12</cell><cell>12</cell><cell>12</cell><cell>12</cell><cell>12</cell></row><row><cell>LDA</cell><cell></cell><cell>Soft</cell><cell></cell><cell>29.3</cell><cell>33.3</cell><cell>38.7</cell><cell>44.7</cell><cell>47.3</cell></row><row><cell>PCA</cell><cell></cell><cell>Euclidean</cell><cell></cell><cell>53.3</cell><cell>61.7</cell><cell>65.7</cell><cell>67.7</cell><cell>72.3</cell></row><row><cell></cell><cell></cell><cell>Mah angle</cell><cell></cell><cell>38.7</cell><cell>49.3</cell><cell>54.7</cell><cell>59.3</cell><cell>62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note><p>-(Comparison 2).</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b0">1</ref> <p>Work partially supported by project "Acquisizione e compressione di Range Data e tecniche di modellazione 3D di volti da immagini", COFIN 2003. <ref type="bibr" target="#b1">2</ref> Work partially supported by the PASCAL Network of Excellence under EC Grant no. 506778. This publication only reflects the authors' view.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition: a literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sur</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>ACM</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognitive Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminant analysis of principal components for face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face Recognition: From Theory to Applications</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Fogelman-Soulie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="73" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative common vectors for face recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cevikalp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neamtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barkaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="13" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A bayesian similarity measure for direct image matching</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nastar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training support vector machines: an application to face detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision and Pattern Recognition, CVPR&apos;97</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition, CVPR&apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast face localisation and verification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="575" to="581" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face recognition: component-based versus global approaches</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heisele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="6" to="21" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face recognition: features versus templates</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brunelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1042" to="1062" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">View-based and modular eigenspaces for face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Comput. Vision Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic visual learning for object representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="696" to="710" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Local feature analysis: a general statistical theory for object representation network</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Penev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Atick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="500" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimation of eye, eyebrow and nose features in videophone sequences, International Workshop on Very Low Bitrate Video Coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kampmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLBV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="101" to="104" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Biometric Techniques in Fingerprints and Face Recognition</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</editor>
		<meeting><address><addrLine>Boca Raton, FL</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="355" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A feature-based face recognition system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Savazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Analysis and Processing</title>
		<meeting>the IEEE International Conference on Image Analysis and Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003, Mantova, 2003</date>
			<biblScope unit="page" from="68" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An efficient method to detect facial fiducial points for face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Pattern Recognition, ICPR 2004</title>
		<meeting>the 17th International Conference on Pattern Recognition, ICPR 2004<address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="http://www.ee.surrey.ac.uk/Research/VSSP/xm2vtsdb/" />
		<title level="m">The XM2VTS Database</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Face localization in color images with complex background, Proceedings of the IEEE International Workshop on Computer Architecture for Machine Perception</title>
		<author>
			<persName><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lipori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="243" to="248" />
			<pubPlace>Palermo, Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing imprecisely localized, partially occluded and expression variant faces from a single sample per class</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fiducial point localization in color images of face foregrounds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput. J</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="863" to="872" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Retinal vision applied to facial features detection and face authentication</title>
		<author>
			<persName><forename type="first">F</forename><surname>Smeraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="463" to="475" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature extraction from faces using deformable templates</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Color Science: Concepts and Methods, Quantitative Data and Formulae</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wyszecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Stiles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Facial feature detection and description</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Università degli Studi di Milano</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature extraction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zamperoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Feature Processing</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Maitre</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zim-Justin</surname></persName>
		</editor>
		<imprint>
			<publisher>North Holland</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Representation of local geometry in the visual system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="367" to="375" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The CSU face identification evaluation system</title>
		<idno>Version 5.0</idno>
		<ptr target="http://www.cs.colostate.edu/evalfacerec/algorithms5.html" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face recognition vendor test 2002 performance metrics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Micheals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings AVBPA</title>
		<imprint>
			<biblScope unit="page" from="937" to="945" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<ptr target="http://www.itl.nist.gov/iad/humanid/feret/" />
		<title level="m">The FERET database</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The csu face identification evaluation system: its purpose, features, and structure</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICVS</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Piater</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Paletta</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2626</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The face recognition vendor test, FRVT</title>
		<ptr target="http://www.frvt.org/FRVT2005/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
