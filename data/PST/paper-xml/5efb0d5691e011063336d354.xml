<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-27">27 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<email>yuxdong@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
							<email>kuansanw@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
							<email>kwchang@cs.ucla.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-27">27 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403237</idno>
					<idno type="arXiv">arXiv:2006.15437v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies ‚Üí Unsupervised learning</term>
					<term>Neural networks</term>
					<term>Learning latent representations Generative Pre-Training</term>
					<term>Graph Neural Networks</term>
					<term>Graph Representation Learning</term>
					<term>Network Embedding</term>
					<term>GNN Pre-Training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN * framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The breakthroughs in graph neural networks (GNNs) have revolutionized graph mining from structural feature engineering to representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17]</ref>. Recent GNN developments have been demonstrated to benefit various graph applications and network tasks, such as semi-supervised node classification <ref type="bibr" target="#b16">[17]</ref>, recommendation systems <ref type="bibr" target="#b41">[42]</ref>, and knowledge graph inference <ref type="bibr" target="#b27">[28]</ref>.</p><p>Commonly, GNNs take a graph with attributes as input and apply convolutional filters to generate node-level representations layer by layer. Often, a GNN model is trained with supervised information in an end-to-end manner for one task on the input graph. That said, for different tasks on the same graph, it is required to have enough and different sets of labeled data to train dedicated GNNs corresponding to each task. Usually, it is arduously expensive and sometimes infeasible to access sufficient labeled data for those tasks, particularly for large-scale graphs. Take, for example, the author disambiguation task in academic graphs <ref type="bibr" target="#b33">[34]</ref>, it has still faced the challenge of the lack of ground-truth to date.</p><p>Similar issues had also been experienced in natural language processing (NLP). Recent advances in NLP address them by training a model from a large unlabeled corpus and transferring the learned model to downstream tasks with only a few labels-the idea of pretraining. For example, the pre-trained BERT language model <ref type="bibr" target="#b3">[4]</ref> is able to learn expressive contextualized word representations by reconstructing the input text-next sentence and masked language predictions, and thus it can significantly improve the performance of various downstream tasks. Additionally, similar observations have also been demonstrated in computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Inspired by these developments, we propose to pre-train graph neural networks for graph mining. The goal of the pre-training is to empower GNNs to capture the structural and semantic properties of a input graph, so that it can easily generalize to any downstream tasks with a few fine-tuning steps on the graphs within the same domain. To achieve this goal, we propose to model the graph distribution by learning to reconstruct the input attributed graph.</p><p>To pre-train GNNs based on graph reconstruction, one straightforward option could be to directly adopt the neural graph generation techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">43]</ref>. However, they are not suitable for pre-training GNNs by design. First, most of them focus only on generating graph structure without attributes, which does not capture the underlying patterns between node attributes and graph structure-the core of convolutional aggregation in GNNs. Second, they are designed to handle small graphs to date, limiting their potential to pre-train on large-scale graphs.  First, a GNN is pre-trained with the self-supervised learning task-attribute and structure generations. Second, the pretrained model and its parameters are then used to initialize models for downstream tasks on the input graph or graphs of the same domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-GNN (ùúÉ)</head><p>Contributions. In this work, we design a self-supervised attributed graph generation task for GNN pre-training, with which both the structure and attributes of the graph are modeled. Based on this task, we present the GPT-GNN framework for generative pretraining of graph neural networks (Cf. Figure <ref type="figure" target="#fig_1">1</ref>). The pre-trained GNN on the input graph can be then used as the initialization of models for different downstream tasks on the same type of graphs. Specifically, our contributions are illustrated below.</p><p>First, we design an attributed graph generation task to model both node attributes and graph structure. We decompose the graph generation objective into two components: Attribute Generation and Edge Generation, whose joint optimization is equivalent to maximizing the probability likelihood of the whole attributed graph. In doing this, the pre-trained model can capture the inherent dependency between node attributes and graph structure.</p><p>Second, we propose an efficient framework GPT-GNN to conduct generative pre-training with the aforementioned task. GPT-GNN can calculate the attribute and edge generation losses of each node simultaneously, and thus only need to run the GNN once for the graph. Additionally, GPT-GNN can handle large-scale graphs with sub-graph sampling and mitigate the inaccurate loss brought by negative sampling with an adaptive embedding queue.</p><p>Finally, we pre-train GNNs on two large-scale graphs-the Open Academic Graph (OAG) of 179 million nodes &amp; 2 billion edges and Amazon recommendation data of 113 million nodes. Extensive experiments show that the GPT-GNN pre-training framework can significantly benefit various downstream tasks. For example, by applying the pre-trained model on OAG, the node classification and link prediction performance is on average lifted by 9.1% over the state-of-the-art GNN models without pre-training. In addition, we show that GPT-GNN can consistently improve the performance of different base GNNs under various settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES AND RELATED WORK</head><p>The goal of pre-training is to allow a model (usually neural networks) to initialize its parameters with pre-trained weights. In this way, the model can leverage the commonality between the pretraining and downstream tasks. Recently pre-training has shown superiority in boosting the performance of many downstream applications in computer vision and natural language processing. In the following, we first introduce the preliminaries about GNNs and then review pre-training approaches in graphs and other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries of Graph Neural Networks</head><p>Recent years have witnessed the success of GNNs for modeling graph data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>. A GNN can be regarded as using the input graph structure as the computation graph for message passing <ref type="bibr" target="#b8">[9]</ref>, during which the local neighborhood information is aggregated to get a more contextual representation. Formally, suppose H (l ) t is the node representation of node t at the (l)-th GNN layer, the update procedure from the (l-1)-th layer to the (l)-th layer is:</p><formula xml:id="formula_0">H (l ) t ‚Üê Aggregate ‚àÄs ‚ààN (t ),‚àÄe ‚ààE(s,t ) Extract H (l ‚àí1) s ; H (l ‚àí1) t , e ,<label>(1)</label></formula><p>where N (t) denotes all the source nodes of node t and E(s, t) denotes all the edges from node s to t.</p><p>There are two basic operators for GNNs, which are Extract(‚Ä¢) and Aggregate(‚Ä¢). Among them, Extract(‚Ä¢) represents the neighbor information extractor. It uses the target node's representation H (l ‚àí1) t and the edge e between the two nodes as query, and extract useful information from source node H (l ‚àí1) s . Aggregate(‚Ä¢) serves as the aggregation function of the neighborhood information. The mean, sum, and max functions are often considered as the basic aggregation operators, while sophisticated pooling and normalization functions can also be designed. Under this framework, various GNN architectures have been proposed. For example, the graph convolutional network (GCN) proposed by Kipf et al. <ref type="bibr" target="#b16">[17]</ref> averages the one-hop neighbor of each node in the graph, followed by a linear projection and then a non-linear activation. Hamilton et al. <ref type="bibr" target="#b11">[12]</ref> propose GraphSAGE that generalizes GCN's aggregation operation from average to sum, max and a RNN unit.</p><p>Also, there are a bunch of works incorporating the attention mechanism into GNNs. In general, the attention-based models implement the Extract(‚Ä¢) operation by estimating the importance of each source node, based on which a weighted aggregation is applied. For example, Velickovi et al. <ref type="bibr" target="#b35">[36]</ref> propose the graph attention network (GAT), which adopts an additive mechanism to calculate attention and uses the same weight for calculating messages. Recently, Hu et al. propose the heterogeneous graph transformer (HGT) <ref type="bibr" target="#b14">[15]</ref> that leverages multi-head attentions for different relation types to get type-dependent attentions. The proposed pre-training framework GPT-GNN can apply to all of these GNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-Training for Graphs</head><p>Previous studies have proposed to utilize pre-training to learn node representations, which largely belong to two categories. The first category is usually termed as network/graph embedding, which directly parameterizes the node embedding vectors and optimizes them by preserving some similarity measures, such as the network proximity <ref type="bibr" target="#b32">[33]</ref> or statistics derived from random walks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>. However, the embeddings learned in this way cannot be used to initialize other models for fine-tuning over other tasks. In contrast, we consider a transfer learning setting, where the goal is to pre-train a generic GNN that can deal with different tasks.</p><p>With the increasing focus on GNNs, researchers have explored the direction of pre-training GNNs on unannotated data. <ref type="bibr">Kipf et al.</ref> propose Variational Graph Auto-Encoders <ref type="bibr" target="#b15">[16]</ref> to reconstruct the graph structure. <ref type="bibr">Hamilton et al.</ref> propose GraphSAGE <ref type="bibr" target="#b11">[12]</ref>, which can optimize via an unsupervised loss by using random walk based similarity metric. Velickovic et al. introduce Graph Infomax <ref type="bibr" target="#b36">[37]</ref>, which maximizes the mutual information between node representations obtained from GNNs and a pooled graph representation. Although these methods show enhancements over purely-supervised learning settings, the learning tasks can be achieved by forcing nearby nodes to have similar embeddings, ignoring the rich semantics and higher-order structure of the graph. Our work proposes to pre-train GNNs by the permutated generative objective, which is a harder graph task and thus can guide the model to learn more complex semantics and structure of the input graph.</p><p>In addition, there are attempts to pre-train GNNs to extract graphlevel representations. Sun et al. present InfoGraph <ref type="bibr" target="#b28">[29]</ref>, which maximizes the mutual information between graph-level representations obtained from GNNs and the representations of sub-structures. Hu et al. <ref type="bibr" target="#b13">[14]</ref> introduce different strategies to pre-train GNNs at both node and graph levels and show that combining them together can improve the performance on graph classification tasks. Our work is different with them as our goal is to pre-train GNNs over a single (large-scale) graph and conduct the node-level transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-Training for Vision and Language</head><p>Pre-training has been widely used in computer vision (CV) and natural language processing (NLP). In CV, early pre-training techniques <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref> mostly follow the paradigm of first pre-training a model on large-scale supervised datasets (such as ImageNet <ref type="bibr" target="#b2">[3]</ref>) and then fine-tuning the pre-trained model on downstream tasks <ref type="bibr" target="#b9">[10]</ref> or directly extracting the representations as features <ref type="bibr" target="#b4">[5]</ref>. Recently, some self-supervised tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35]</ref> have also been utilized to pre-train vision models. In NLP, Early works have been focused on learning (shallow) word embeddings <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref> by leveraging the cooccurrence statistics on the text corpus. More recently, significant progresses have been made on contextualized word embeddings, such as BERT <ref type="bibr" target="#b3">[4]</ref>, XLNET <ref type="bibr" target="#b40">[41]</ref> and GPT <ref type="bibr" target="#b26">[27]</ref>. Take BERT as an example, it pre-trains a text encoder with two self-supervised tasks in order to better encode words and their contexts. These pre-training approaches have been shown to yield state-of-the-art performance in a wide range of NLP tasks and thus used as a fundamental component in many NLP systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GENERATIVE PRE-TRAINING OF GNNS</head><p>In this section, we formalize the attributed graph generation task and introduce the generative pre-training framework (GPT-GNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The GNN Pre-Training Problem</head><p>The input to GNNs is usually an attributed graph G = (V, E, X), where V and E denote its node and edge sets, and X represents the node feature matrix. A GNN model learns to output node representations under the supervision of a specific downstream task, such as node classification. Sometimes there exist multiple tasks on a single graph, and most GNNs require sufficient dedicated labeled data for each task. However, it is often challenging to obtain sufficient annotations, in particular for large-scale graphs, hindering the training of a well-generalized GNN. Therefore it is desirable to have a pre-trained GNN model that can generalize with few labels. Conceptually, this model should 1) capture the intrinsic structure and attribute patterns underlying the graph and 2) thus benefit various downstream tasks on this graph.</p><p>GNN Pre-Training. Formally, our goal of GNN pre-training concerns the learning of a general GNN model f Œ∏ purely based on single (large-scale) graph G = (V, E, X) without labeled data such that f Œ∏ is a good initialization for various (unseen) downstream tasks on the same graph or graphs of the same domain. To learn such a general GNN model without labeled data on the graph, a natural question arises here is: how to design an unsupervised learning task over the graph for pre-training the GNN model?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Generative Pre-Training Framework</head><p>Recent advances in self-supervised learning for NLP <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref> and CV <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35]</ref> have shown that unlabeled data itself contains rich semantic knowledge, and thus a model that can capture the data distribution is able to transfer onto various downstream tasks. Inspired by this, we propose GPT-GNN, which pre-trains a GNN by reconstructing/generating the input graph's structure and attributes.</p><p>Formally, given an input graph G = (V, E, X) and a GNN model f Œ∏ , we model the likelihood over this graph by this GNN as p(G; Œ∏ )representing how the nodes in G are attributed and connected. GPT-GNN aims to pre-train the GNN model by maximizing the graph likelihood, i.e., Œ∏ * = max Œ∏ p(G; Œ∏ ).</p><p>Then, the first question becomes how to properly model p(G; Œ∏ ). Note that most existing graph generation methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref> follow the auto-regressive manner to factorize the probability objective, i.e., the nodes in the graph come in an order, and the edges are generated by connecting each new arriving node to existing nodes. Similarly, we denote a permutation vector œÄ to determine the node ordering, where i œÄ denotes the node id of i-th position in permutation œÄ . Consequently, the graph distribution p(G; Œ∏ ) is equivalent to the expected likelihood over all possible permutations, i.e.,</p><formula xml:id="formula_1">p(G; Œ∏ ) = E œÄ p Œ∏ (X œÄ , E œÄ ) ,</formula><p>where X œÄ ‚àà R |V |√ód denotes permutated node attributes and E is a set of edges, while E œÄ i denotes all edges connected with node i œÄ . For simplicity, we assume that observing any node ordering œÄ has an equal probability and also omit the subscript œÄ when illustrating the generative process for one permutation in the following sections. Given a permutated order, we can factorize the log likelihood autoregressively-generating one node per iteration-as:</p><formula xml:id="formula_2">log p Œ∏ (X , E) = |V | i=1 log p Œ∏ (X i , E i | X &lt;i , E &lt;i ).<label>(2)</label></formula><p>At each step i, we use all nodes that are generated before i, their attributes X &lt;i , and the structure (edges) between these nodes E &lt;i to generate a new node i, including both its attribute X i and its connections with existing nodes E i . Essentially, the objective in Eq. 2 describes the autoregressive generative process of an attributed graph. The question becomes: how to model the conditional probability p Œ∏ (X i , E i |X &lt;i , E &lt;i )? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Factorizing Attributed Graph Generation</head><p>To compute p Œ∏ (X i , E i |X &lt;i , E &lt;i ), one naive solution could be to simply assume that X i and E i are independent, that is,</p><formula xml:id="formula_3">p Œ∏ (X i , E i |X &lt;i , E &lt;i ) = p Œ∏ (X i |X &lt;i , E &lt;i ) ‚Ä¢ p Œ∏ (E i |X &lt;i , E &lt;i )</formula><p>With such decomposition, for each node, the dependency between its attributes and connections are completely neglected. However, the ignored dependency is the core property of attributed graphs and also the foundation of convolutional aggregation in GNNs. Therefore, such a naive decomposition cannot provide informative guidance for pre-training GNNs.</p><p>To address this issue, we present the dependency-aware factorization mechanism for the attributed graph generation process. Specifically, when estimating a new node's attributes, we are given its structure information, and vice versa. During the process, a part of the edges has already been observed (or generated). Then the generation can be decomposed into two coupled parts:</p><p>‚Ä¢ given the observed edges, generate node attributes; ‚Ä¢ given the observed edges and generated node attributes, generate the remaining edges.</p><p>In this way, the model can capture the dependency between the attributes and structure for each node. Formally, we define a variable o to denote the index vector of all the observed edges within E i . Thus, E i,o denotes the observed edges. Similarly, ¬¨o denotes the index of all the masked edges, which are to be generated. With this, we can rewrite the conditional probability as an expected likelihood over all observed edges:</p><formula xml:id="formula_4">p Œ∏ (X i , E i | X &lt;i , E &lt;i ) = o p Œ∏ (X i , E i,¬¨o | E i,o , X &lt;i , E &lt;i ) ‚Ä¢ p Œ∏ (E i,o | X &lt;i , E &lt;i ) =E o p Œ∏ (X i , E i,¬¨o | E i,o , X &lt;i , E &lt;i ) =E o p Œ∏ (X i | E i,o , X &lt;i , E &lt;i ) 1) generate attributes ‚Ä¢ p Œ∏ (E i,¬¨o | E i,o , X ‚â§i , E &lt;i ) 2) generate edges .<label>(3)</label></formula><p>This factorization design is able to model the dependency between node i's attributes X i and its associated connections E i . The first term p Œ∏ (X i | E i,o , X &lt;i , E &lt;i ) denotes the generation of attributes for node i. Based on the observed edges E i,o , we gather the target node i's neighborhood information to generate its attributes X i . The second term p Œ∏ (E i,¬¨o | E i,o , X ‚â§i , E &lt;i ) denotes the generation of masked edges. Based on both the observed edges E i,o and the generated attributes X i , we generate the representation of the target node i and predict whether each edge within E i,¬¨o exists.</p><p>A graph generation example. We intuitively show how the proposed factorization-based graph generation process works. Take, for example, an academic graph, if we would like to generate one paper node, whose title is considered as its attribute, while this paper node is connected to its authors, published venue, and cited papers. Based on some observed edges between this paper and some of its authors, our generation process first generates its title. Then, based on both the observed edges and generated title, we predict its remaining authors, published venue, and references. In this way, this process models the interaction between the paper's attribute (title) and structure (observed and remaining edges) to complete the generation task, bringing in informative signals for pre-training GNNs over the academic graph.</p><p>So far, we factorize the attributed graph generation process into a node attribute generation step and an edge generation step. The question we need to answer here is: How to efficiently pre-train GNNs by optimizing both attribute and edge generation tasks?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Efficient Attribute and Edge Generation</head><p>For the sake of efficiency, it is desired to compute the loss of attribute and edge generations by running the GNN only once for the input graph. In addition, we expect to conduct attribute generation and edge generation simultaneously. However, edge generation requires node attributes as input, which can be leaked to attribute generation. To avoid information leakage, we design to separate each node into two types:</p><p>‚Ä¢ Attribute Generation Nodes. We mask out the attributes of these nodes by replacing their attributes with a dummy token and learn a shared vector X init to represent it ‚Ä† . This is equivalent to the trick of using the [Mask] token in the masked language model <ref type="bibr" target="#b3">[4]</ref>. ‚Ä¢ Edge Generation Nodes. For these nodes, we keep their attributes and put them as input to the GNN. ‚Ä† X ini t has the same dimension as X i and can be learned during pre-training.</p><p>We then input the modified graph to the GNN model and generate the output representations. We use h At t r and h Ed–¥e to represent the output embeddings of Attribute Generation and Edge Generation Nodes, respectively. As the attributes of Attribute Generation Nodes are masked out, h At t r in general contains less information than h Ed–¥e . Therefore, when conduct the GNN message passing, we only use Edge Generation Nodes' output h Ed–¥e as outward messages. The representations of the two sets of nodes are then used to generate attributes and edges with different decoders.</p><p>For Attribute Generation, we denote its decoder as Dec At t r (‚Ä¢), which takes h At t r as input and generates the masked attributes. The modeling choice depends on the type of attributes. For example, if the input attribute of a node is text, we can use the text generator model (e.g., LSTM) to generate it. If the input attribute is a standard vector, we can apply a multi-layer Perceptron to generate it. Also, we define a distance function as a metric between the generated attributes and the real ones, such as perplexity for text or L2-distance for vectors. Thus, we calculate the attribute generation loss via:</p><formula xml:id="formula_5">L At t r i = Distance Dec At t r (h At t r i ), X i .<label>(4)</label></formula><p>By minimizing the distance between the generated and masked attributes, it is equivalent to maximize the likelihood to observe each node attribute, i.e., p Œ∏ (X i | E i,o , X &lt;i , E &lt;i ), and thus the pretrained model can capture the semantic of this graph.</p><p>For Edge Generation, we assume that the generation of each edge is independent with others, so that we can factorize the likelihood:</p><formula xml:id="formula_6">p Œ∏ (E i,¬¨o | E i,o , X ‚â§i , E &lt;i ) = j + ‚ààE i, ¬¨o p Œ∏ (j + | E i,o , X ‚â§i , E &lt;i ). (5)</formula><p>Next, after getting the Edge Generation node representation h Ed–¥e , we model the likelihood that node i is connected with node j by Dec Ed–¥e (h</p><formula xml:id="formula_7">Ed–¥e i , h Ed–¥e j</formula><p>), where Dec Ed–¥e is a pairwise score function. Finally, we adopt the negative contrastive estimation to calculate the likelihood for each linked node j + . We prepare all the unconnected nodes as S ‚àí i and calculate the contrastive loss via</p><formula xml:id="formula_8">L Ed–¥e i = ‚àí j + ‚ààE i, ¬¨o log exp Dec Ed–¥e (h Ed–¥e i , h Ed–¥e j + ) j ‚ààS ‚àí i ‚à™{j + } exp Dec Ed–¥e (h Ed–¥e i , h Ed–¥e j )<label>(6)</label></formula><p>By optimizing L Ed–¥e , it is equivalent to maximizing the likelihood of generating all the edges, and thus the pre-trained model is able to capture the intrinsic structure of the graph.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> illustrates the attributed graph generation process. Specifically: (a) We determine the node permutation order œÄ for the input graph. (b) We randomly select a portion of the target node's edges as observed edges E i,o and the remaining as masked edges E i,¬¨o (grey dashed lines with cross). We delete masked edges in the graph. (c) We separate each node into the Attribute Generation and Edge Generation nodes to avoid information leakage. (d) After the preprocessing, we use the modified adjacency matrix to calculate the representations of node 3,4 and 5, including both their Attribute and Edge Generation Nodes. Finally, as illustrated in (d)-(e), we train the GNN model via the attribute prediction and masked edge prediction task for each node in parallel. The overall pipeline of GPT-GNN is illustrated in Algo. 1 (See Appendix B for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The GPT-GNN Pre-Training Framework</head><p>Require: Input Attributed Graph G, Graph Sampler Sampler (‚Ä¢). Ensure:</p><p>1: Initialize the GNN model as f Œ∏ , the attribute generation decoder as Dec At t r , and the edge generation decoder as Dec Ed–¥e . 2: Initialize the adaptive node embedding queue Q = {} and the attribute vector h init . 3: for each sampled graph ƒú ‚àà Sampler (G) do 4:</p><p>For each node, sample the observed edge index o and masked edges ¬¨o, and delete masked edges E i,¬¨o accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Separate each node into the Attribute Generation and Edge Generation nodes. Replace the input to Attribute Generation node as h init . Apply GNN f Œ∏ to get two sets of node embeddings h At t r and h Ed–¥e for each node in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for node i with attributes X i and masked edges E i,¬¨o do 7:</p><p>Calculate the attribute generation loss L At t r by Eq. 4 8:</p><p>Prepare negative samples S ‚àí i for edge generation by concatenating unconnected nodes and adaptive queue Q.   Update Q by adding in h Ed–¥e and popping out most outdated embeddings. 13: end for 14: return Pre-trained model parameters Œ∏ * for downstream tasks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">GPT-GNN for Heterogeneous &amp; Large Graphs</head><p>In this section, we discuss how to apply GPT-GNN to pre-train for large-scale and heterogeneous graphs, which can be of practical use for modeling real-world complex systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>, such as academic graphs, product graphs, IoT networks, and knowledge graphs.</p><p>Heterogeneous graphs. Many real-world graphs are heterogeneous, meaning that they contain different types of nodes and edges. For heterogeneous graphs, the proposed GPT-GNN framework can be straightforwardly applied to pre-train heterogeneous GNNs. The only difference is that each type of nodes and edges may have its own decoder, which is specified by the heterogeneous GNNs rather than the pre-training framework. All the other components remain exactly the same.</p><p>Large-scale graphs. To pre-train GNNs on graphs that are too large to fit into the hardware, we sample subgraphs for training.</p><p>In particular, we propose to sample a dense subgraph from homogeneous and heterogeneous graphs by using the LADIES algorithm <ref type="bibr" target="#b44">[45]</ref> and its heterogeneous version HGSampling <ref type="bibr" target="#b14">[15]</ref>, respectively. Both methods theoretically guarantee that the sampled nodes are highly interconnected with each other and maximally preserve the structural information.</p><p>To estimate the contrastive loss in Eq. 6, it is required to go over all nodes of the input graph. However, we only have access to the sampled nodes in a subgraph for estimating this loss, making the (self-)supervision only focus on local signals. To alleviate this issue, we propose the Adaptive Queue, which stores node representations in previously-sampled subgraphs as negative samples. Each time we process a new subgraph, we progressively update this queue by adding the latest node representations and remove the oldest ones. As the model parameters will not be updated rigorously, the negative samples stored in the queue are consistent and accurate. The Adaptive Queue enables us to use much larger negative sample pools S ‚àí i . Moreover, the nodes across different sampled sub-graphs can bring in the global structural guidance for contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>To evaluate the performance of GPT-GNN, we conduct experiments on the Open Academic Graph (OAG) and Amazon Recommendation datasets. To evaluate the generalizability of GPT-GNN, we consider different transfer settings-time transfer and field transfer-which are of practical importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets and Tasks. We conduct experiments on both heterogeneous and homogeneous graphs. For heterogeneous graphs, we use the Open Academic Graph and Amazon Review Recommendation data. For homogeneous graphs, we use the Reddit dataset <ref type="bibr" target="#b11">[12]</ref> and the paper citation network extracted from OAG. All datasets are publicly available and the details can be found in Appendix A.</p><p>Open Academic Graph (OAG) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref> contains more than 178 million nodes and 2.236 billion edges. It is the largest publicly available heterogeneous academic dataset to date. Each paper is labeled with a set of research topics/fields (e.g., Physics and Medicine) and the publication date ranges from 1900 to 2019. We consider the prediction of Paper-Field, Paper-Venue, and Author Name Disambiguation (Author ND) as three downstream tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>. The performance is evaluated by MRR-a widely adopted ranking metric <ref type="bibr" target="#b18">[19]</ref>.</p><p>Amazon Review Recommendation Dataset (Amazon) <ref type="bibr" target="#b22">[23]</ref> contains 82.8 million reviews, 20.9 million users, and 9.3 million products. The reviews are published from 1996 to 2018. Each review consists of a discrete rating score from 1 to 5 and a specific field, including book, fashion, etc. For downstream tasks, we predict the rating score as a five-class classification task within the Fashion, Beauty, and Luxury fields. We use micro F1-score as the evaluation metric.</p><p>The base GNN model. On the OAG and Amazon datasets, we use the state-of-the-art heterogeneous GNN-Heterogeneous Graph Transformer (HGT) <ref type="bibr" target="#b14">[15]</ref>-as the base model for GPT-GNN. Furthermore, we also use other (heterogeneous) GNNs as the base model to test our generative pre-training framework.</p><p>Implementation details. For all base models, we set the hidden dimension as 400, the head number as 8, and the number of GNN layers as 3. All of them are implemented using the PyTorch Geometric (PyG) package <ref type="bibr" target="#b7">[8]</ref>.</p><p>We optimize the model via the AdamW optimizer <ref type="bibr" target="#b20">[21]</ref> with the Cosine Annealing Learning Rate Scheduler <ref type="bibr" target="#b19">[20]</ref> with 500 epochs and select the one with the lowest validation loss as the pre-trained model. We set the adaptive queue size to be 256.</p><p>During downstream evaluation, we fine-tune the model using the same optimization setting for 200 epochs as that in pre-training. We train the model on the downstream tasks for five times and report the mean and standard deviation of test performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training baselines.</head><p>There exist several works that propose unsupervised objectives over graphs, which can potentially be used to pre-train GNNs. We thus compare the proposed GPT-GNN framework with these baselines:</p><p>‚Ä¢ GAE <ref type="bibr" target="#b15">[16]</ref>, which denotes graph auto-encoders, focuses on a traditional link prediction task. It randomly masks out a fixed proportion of the edges and asks the model to reconstruct these masked edges. ‚Ä¢ GraphSAGE (unsp.) <ref type="bibr" target="#b11">[12]</ref> forces connected nodes to have similar output node embeddings. Its main difference with GAE lies in that it does not mask out the edges during pre-training. ‚Ä¢ Graph Infomax <ref type="bibr" target="#b36">[37]</ref> tries to maximize the local node embeddings with global graph summary embeddings. Following its setting for a large-scale graph, for each sampled subgraph, we shuffle the graph to construct negative samples. In addition, we also evaluate the two pre-training tasks in GPT-GNN by using each one of them alone, that is, attribute generation-GPT-GNN (Attr)-and edge generation-GPT-GNN (Edge).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-Training and Fine-Tuning Setup</head><p>The goal of pre-training is to transfer knowledge learned from numerous unlabeled nodes of a large graph to facilitate the downstream tasks with a few labels. Specifically, we first pre-train a GNN and use the pre-trained model weights to initialize models for downstream tasks. We then fine-tune the models with the downstream task specific decoder on the training (fine-tuning) set and evaluate the performance on the test set.</p><p>Broadly, there are two different setups. The first one is to pretrain and fine-tune on exactly the same graph. The second one is relatively more practical, which is to pre-train on one graph and fine-tune on unseen graphs of the same type as the pre-training one. Specifically, we consider the following three graph transfer settings between the pre-training and fine-tuning stages:</p><p>‚Ä¢ Time Transfer, where we use data from different time spans for pre-training and fine-tuning. For both OAG and Amazon, we use data before 2014 for pre-training and data since 2014 for fine-tuning. ‚Ä¢ Field Transfer, where we use data from different fields for pre-training and evaluating. In OAG, we use papers in the field of computer science (CS) for downstream fine-tuning and use all papers in the remaining fields (e.g., Medicine) for pre-training. In Amazon, we pre-train on products in Arts, Crafts, and Sewing, and fine-tune on products in Fashion, Beauty, and Luxury.  <ref type="bibr" target="#b14">[15]</ref> as the base model. 10% of labeled data is used for fine-tuning.</p><p>During pre-training, we randomly select a subset of the data as the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>We summarize the performance of downstream tasks with different pre-training methods on OAG and Amazon in Table <ref type="table" target="#tab_0">1</ref>. As discussed above, we setup three different transfer settings between pre-training and fine-tuning stages: Field Transfer, Time Transfer, and Field + Time Combined Transfer, as organized in three different blocks in the Table <ref type="table">.</ref> Overall, the proposed GPT-GNN framework significantly enhances the performance for all downstream tasks on both datasets. On average, GPT-GNN achieves relative performance gains of 13.3% and 5.7% over the base model without pre-training on OAG and Amazon, respectively. Moreover, it consistently outperforms other pre-training frameworks, such as Graph Infomax, across different downstream tasks for all three transfer settings on both datasets. Different transfer settings. Observed from Table <ref type="table" target="#tab_0">1</ref>, the performance gain lifted by pre-training under the field transfer is higher than that under the time transfer, and the time + field combined transfer is the most challenging setting as evident in the least performance gain brought by pre-training. Nonetheless, under the combined transfer, GPT-GNN still achieves 11.7% and 4.6% performance gains on both datasets, respectively. Altogether, the results suggest that the proposed generative pre-training strategy enables the GNN model to capture the generic structural and semantic knowledge of the input graph, which can be used to fine-tune on the unseen part of the graph data.</p><p>Ablation studies on pre-training tasks. We analyze the effectiveness of the two pre-training tasks in GPT-GNN-attribute generation and edge generation-by examining which of them is more beneficial for the pre-training framework and, by extension, downstream tasks. In Table <ref type="table" target="#tab_0">1</ref>, we report the performance of GPT-GNN by using attribute generation and edge generation alone, that is, GPT-GNN (Attr) and GPT-GNN (Edge). On OAG, the average performance gains by GPT-GNN (Attr) and GPT-GNN (Edge) are 7.4% and 10.3%, respectively, suggesting that Edge Generation is a more informative pre-training task than Attribute Generation in GPT-GNN. However, we have an opposite observation for Amazon, on which the performance improved by Attribute Generation is 5.2% in contrast to the 4.1% improvement lifted by Edge Generation. This suggests that the GPT-GNN framework benefits differently from attribute and edge generations on different datasets. However, combining the two pre-training tasks together produces the best performance on both cases.</p><p>We further compare the Edge Generation task against other edge-based pre-training methods-GAE and GraphSage (unsp.)-in Table <ref type="table" target="#tab_0">1</ref>. On OAG, the performance improvements brought by GPT-GNN's edge generation, GAE, and GraphSage over no pre-training are 10.3%, 7.4%, and 4.0%, respectively. On Amazon, the gains are 5.2%, 3.1%, and 1.3%, respectively. From the comparisons, we have the following observations. First, both GAE and GPT-GNN's edge generation offer better results than GraphSage on both datasets, demonstrating that masking on edges is an effective strategy for self-supervised graph representation learning. Without edge masking, the model simply retains a similar embedding for connected nodes, as the label we would like to predict (whether two nodes are linked) has already been encoded in the input graph structure. Such information leakage will downgrade the edge prediction task to a trivial problem. Second, the proposed Edge Generation task consistently outperforms GAE. The main advantage of GPT-GNN's edge generation comes from that it learns to generate missing edges autoregressively and thus can capture the dependencies between the masked edges, which are discarded by GAE. In summary, the results suggest that the proposed graph generation tasks can give informative self-supervision for GNN pre-training.</p><p>Ablation studies on the base GNN. We investigate whether the other GNN architectures can benefit from the proposed pretraining framework. Therefore, in addition to HGT <ref type="bibr" target="#b14">[15]</ref>, we try GCN <ref type="bibr" target="#b16">[17]</ref>, GAT <ref type="bibr" target="#b35">[36]</ref>, RGCN <ref type="bibr" target="#b27">[28]</ref>, and HAN <ref type="bibr" target="#b38">[39]</ref> as the base model. Specifically, we pre-train them on OAG and then use the paper-field prediction task under the combined transfer setting with 10% of training data for fine-tuning and testing. Model-independent hyperparameters, such as the hidden dimension size and optimization, are set the same. The results are reported in in Table <ref type="table" target="#tab_1">2</ref>. We can observe that 1) HGT achieves the best performance among all non pre-trained GNN models; 2) GPT-GNN with HGT generates the most promising results for the concerned downstream task; and 3) more importantly, the proposed GPT-GNN pre-training framework can enhance the downstream performance for all the GNN architectures.</p><p>Ablation studies on the node separation and adaptive queue. Finally, we examine the effectiveness of the two design choices of GPT-GNN, i.e., node separation and adaptive queue. The node separation is designed for alleviating the information leakage problem for the Attribute Generation task. Without this component, the attributes would appear in the input and thus the pre-training method would only need to maintain the input features for output. In other words, it cannot learn any knowledge of the input graph that could be transferred to downstream tasks and thus affect the results negatively. From Table <ref type="table" target="#tab_0">1</ref>, we can see that the attribute generation based pre-training model suffers from the removal of the node separation component (w/o node separation), and in many cases, its performance is even worse than the ones without pre-training. This demonstrates the significance of this node separation design in avoiding attribute information leakage.  The adaptive queue is designed for alleviating the gap between the sampled graphs and the full graph. Similarly, we conduct the ablation study by removing it from the Edge Generation based pre-training model and from Table <ref type="table" target="#tab_0">1</ref>, we witness the consistent performance drops for all tasks-GPT-GNN (Edge) vs. (w/o adaptive queue). This indicates that adding more negative samples by using the adaptive queue is indeed helpful to the pre-training framework.</p><p>Training data size. In Figure <ref type="figure" target="#fig_6">3</ref>, we examine whether the proposed GPT-GNN method can generalize well with different training data size during fine-tuning, i.e., from 10% to 100%. First, we can observe that GPT-GNN and other pre-training frameworks consistently improve the downstream task performance with more labeled training data. Second, it is clear that GPT-GNN performs the best among all pre-training tasks/frameworks. Finally, we can see that with the pre-trained model, fine-tuning with only 10-20% of data (the two leftmost blue circles) generates comparative performance to the supervised learning with all 100% of training data (the rightmost purple diamond), demonstrating the superiority of GNN pre-training, especially when the label is scarce.</p><p>Results for homogeneous graphs. In addition to heterogeneous graphs, we also test whether the GPT-GNN pre-training framework can benefit downstream tasks on homogeneous graphs. Specifically, we pre-train and fine-tune on two homogeneous graphs: 1) the paper citation network extracted from the field of CS in OAG, on which the topic of each paper is predicted; 2) the Reddit network consisting of Reddit posts, on which the community of each post is inferred. As there is only one type of nodes and edges in homogeneous graphs, we require one single set of edge and attribute decoders for pre-training. HGT is used as the base pre-training model by ignoring its heterogeneous components. The fine-tuned results with 10% labeled data are presented in Table <ref type="table" target="#tab_2">3</ref>. We can observe that the downstream tasks on both homogeneous graphs can benefit from all pre-training frameworks, among which the proposed GPT-GNN offers the largest performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we study the problem of graph neural network pretraining. We present GPT-GNN-a generative GNN pre-training framework. In GPT-GNN, we design the graph generation factorization to guide the base GNN model to autoregressively reconstruct both the attributes and structure of the input graph. Furthermore, we propose to separate the attribute and edge generation nodes to avoid information leakage. In addition, we introduce the adaptive node representation queue to mitigate the gap between the likelihoods over the sampled graph and the full graph. The pretrained GNNs with fine-tuning over few labeled data can achieve significant performance gains on various downstream tasks across different datasets. Also, GPT-GNN is robust to different transfer settings between pre-training and fine-tuning. Finally, we find that fine-tuning the generative pre-trained GNN model with 10-20% of labeled data offers comparative performance for downstream tasks to the supervised GNN model with 100% of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DATASET DETAILS</head><p>We mainly use Open Academic Graph (OAG) and Amazon Review Recommendation Dataset (Amazon) for evaluation. Both are widely used heterogeneous graph <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Here we introduce their statistics, schema and how we prepare the attributes and tasks in detail.</p><p>Open Academic Graph (OAG) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref> consists of five types of nodes: 'Paper', 'Author', 'Field', 'Venue', and 'Institute', and 14 types of edges between these nodes. The schema and meta relations are illustrated in Figure <ref type="figure" target="#fig_7">4</ref>(a). For example, the 'Field' nodes in the OAG are categorized into six levels from L 0 to L 5 , which are organized with a hierarchical tree (We use 'is_organized_in' to represent this hierarchy). Therefore, we differentiate the 'Paper-Field' edges in the corresponding field levels. Besides, we differentiate the different author orders (i.e., the first author, the last one, and others) and venue types (i.e., journal, conference, and preprint) as well. Finally, the 'Self' type corresponds to the self-loop connection, which is widely added in GNN architectures. Despite 'Self' and 'CoAuthor' edge relationships, which are symmetric, all other edge types X have a reverse edge type X ‚àí1 .</p><p>For downstream tasks, we choose the following three tasks: the prediction of Paper-Field (L2), Paper-Venue, and Author Disambiguation. In the first two tasks, we give a model a paper and want it to predict the correct fields it belongs to or the venue it is published at. We model these three tasks as node classification problem, where we use GNNs to get the contextual node representation of the paper and use a softmax output layer to get its classification results. For author disambiguation, we pick all the authors with the same name, and the papers that link to one of these same-name authors. The task is to conduct link prediction between papers and candidate authors. After getting the paper and author node representations from GNNs, we use a Neural Tensor Network to get the probability of each author-paper pair to be linked.</p><p>For input attributes of heterogeneous graph, as we don't assume the attribute of each data type belongs to the same distribution, and we are free to use the most appropriate attributes to represent each type of node. For paper and author nodes, the node numbers are extremely large. Therefore, traditional node embedding algorithms are not suitable for extracting attributes for them. We, therefore, resort to the paper titles for extracting attributes. For each paper, we get its title text and use a pre-trained XLNet <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> to get the representation of each word in the title. We then average them weighted by each word's attention to get the title representation for each paper. The initial attribute of each author is simply an average of his/her published papers' embeddings. For field, venue and institute nodes, the node numbers are small and we use the metap-ath2vec model <ref type="bibr" target="#b5">[6]</ref> to train their node embeddings by reflecting the heterogeneous network structures.</p><p>Amazon Review Recommendation Dataset (Amazon) <ref type="bibr" target="#b22">[23]</ref> consists of three types of nodes, including reviews (ratings and text), users and products, and some other meta-types of the product, including color, size, style and quantity. The schema and meta relations are illustrated in Figure <ref type="figure" target="#fig_7">4(b)</ref>. Compared to a general user-item bipartite graph, this dataset have review in between, each is associated with text information and a rating from 1 to 5. The reviews are also associated with those product meta-type descriptions. For simplicity, we only consider these review-type link as 'categorize_in' type. Thus, there are totally three types of relations in this graph.</p><p>For downstream task, we choose the rating classification for each new review. Since the problem is a node-level multi-class classification, we use GNNs to get the contextual node representation of the review and use a softmax output layer to get 5-class prediction.</p><p>For input attributes of Amazon, we also use a pre-trained XLNet to get each review embedding, and the attributes for all the other nodes are simple an average of its associated review's embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B OVERALL PIPELINE OF GPT-GNN</head><p>The overall pipeline of GPT-GNN is illustrated by Algorithm 1. Given an attributed graph G, we each time sample a subgraph ƒú as training instance for generative pre-training. The first step is to determine the node permutation order œÄ . To support parallel training, we want to conduct the forward pass for a single run and get the representation of the whole graph, so that we can simultaneously calculate the loss for each node, instead of processing each node recursively. Therefore, we remove all the edges from nodes with higher order to those with lower order according to œÄ , which means each node can only receive information from nodes with lower order. In this way, they won't leak the information to the autoregressive generative objective, and thus we only need one single round to get node embeddings of the whole graph, which can directly be utilized to conduct generative pre-training.</p><p>Afterwards, we need to determine the edges to be masked out. For each node, we get all its outward edges, randomly select a set of edges to be masked out. This corresponds to line 4. Next, we conduct node separation and get contextualized node embeddings for the whole graph in line 5, which will be utilized to calculate generative loss. line 7-9. For both OAG and Amazon, the main nodes that contain meaningful attributes are paper and review nodes, which both have text feature as input. Thus, we only consider them for Attribute Generation, with a 2-layer LSTM as decoder. Note that in line 8, we prepare the negative samples by aggregating both the unconnected nodes within this sampled graph and the previously calculated embeddings stored in the adaptive queue Q. This can mitigate the gap between optimizing over sampled graph with over the whole graph. Finally, we optimize the model and update the adaptaive queue in line 11-12. Afterwards, we can use the pretrained model as initialization, to fine-tune on other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPLEMENTATION DETAILS AND CONVERGENCE CURVES</head><p>We use a Tesla K80 to run both pre-training and downstream tasks. For graph sampling, we follow the HGSampling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45]</ref> to sample subgraph over large-scale heterogeneous graph. For each type of node, we sample 128 nodes per layer. We repeat sampling for 6 times for OAG and average sampled nodes in the sub-graph is 3561 nodes. We repeat for 8 times for Amazon, and average sampled nodes is 1478. For each batch, we sample 32 graphs to conduct generative pre-training. During GPU training, we conduct multiprocess sampling to prepare the pre-training data. Such CPU-GPU cooperation can help us save the sampling time.</p><p>We here illustrate the convergence curves for pre-training and fine-tuning. For pre-training, as illustrated in Figure <ref type="figure" target="#fig_10">6</ref>, we show the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predict Paper Title</head><p>Groundtruth Paper Title person recognition system using automatic probabilistic classification person re-identification by probabilistic relative distance comparison a novel framework using spectrum sensing in wireless systems a secure collaborative spectrum sensing strategy in cyber physical systems a efficient evaluation of a distributed data storage service storage an empirical analysis of a large scale mobile cloud storage service parameter control in wireless sensor networks networks networks optimal parameter estimation under controlled communication over sensor networks a experimental system for for to the analysis of graphics an interactive computer graphics approach to surface representation Table <ref type="table">4</ref>: Generated paper title samples. The left column is generated by GPT-GNN, and the right column is the groundtruth.   For downstream tasks, we show the convergence curve utilizing our GPT-GNN with no-pretrain, with different data percentage. As is illustrated in Figure <ref type="figure" target="#fig_9">5</ref>, GPT-GNN can always get a more generalized model than no-pretrain, and is more robust to over-fitting since a good initialization from pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PAPER TITLE GENERATION EXAMPLES</head><p>For OAG, since our attribute generation task is oriented on the paper title, we'd like to see how well our GPT-GNN can learn to generate the title. The results are shown in table <ref type="bibr" target="#b3">4</ref>. We can see that the model can capture the main meaning of each paper to be predicted, only by looking at partial neighborhoods (note that we use Attribute Generation Node for this task, which replace the input attribute as a share vector). For example, for the first sentence, our model successfully predict the key words for this paper, including 'person recognition', 'probabilistic', etc. This shows that the graph itself contains rich semantic information, and explains why a pretrained model can generalize well to downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The pre-training and fine-tuning flow of GPT-GNN:First, a GNN is pre-trained with the self-supervised learning task-attribute and structure generations. Second, the pretrained model and its parameters are then used to initialize models for downstream tasks on the input graph or graphs of the same domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustrative example of the proposed attributed graph generation procedure.</figDesc><graphic url="image-1.png" coords="4,53.80,83.69,504.39,133.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>9 :</head><label>9</label><figDesc>Calculate the edge generation loss L Ed–¥e by Eq. 6 10: end for 11:Optimize Œ∏ by minimizing L At t r and L Ed–¥e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Compare pre-training tasks with different training data size. Evaluated by the paper-field prediction task on OAG under the field transfer setting.</figDesc><graphic url="image-2.png" coords="8,318.28,79.45,243.83,139.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The schema and meta relations of Open Academic Graph and Amazon Review Recommendation Dataset.</figDesc><graphic url="image-3.png" coords="11,53.80,83.69,508.24,83.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fine-tuning convergence comparison of GPT-GNN with no-pretrain, under different training data percentage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The pre-training convergence curve of GPT-GNN on OAG. It took about 10 hours (400 epochs) to converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>‚Ä¢ Time + Field Transfer, where we use the graph of particular fields before 2014 to pre-train the model and use the data from other fields since 2014 for fine-tuning. Intuitively, this combined transfer setting is more challenging than the transfer of time or field alone. For example, we pre-train on the OAG graph except CS field before 2014 and fine-tune on the CS graph since 2014. During fine-tuning, for both datasets, we choose nodes from 2014 to 2016 for training, 2017 for validation, and since 2018 for testing. To meet the assumption that training data is usually scarce, we only provide 10% of the labels for training (fine-tuning) by default, while the ablation study over different data percentages is also conducted. Performance of different downstream tasks on OAG and Amazon by using different pre-training frameworks with the heterogeneous graph transformer (HGT)</figDesc><table><row><cell></cell><cell>Downstream Dataset</cell><cell></cell><cell>OAG</cell><cell></cell><cell></cell><cell>Amazon</cell></row><row><cell></cell><cell>Evaluation Task</cell><cell cols="3">Paper-Field Paper-Venue Author ND</cell><cell>Fashion</cell><cell>Beauty</cell><cell>Luxury</cell></row><row><cell></cell><cell>No Pre-train</cell><cell>.336¬±.149</cell><cell>.365¬±.122</cell><cell>.794¬±.105</cell><cell cols="2">.586¬±.074 .546¬±.071 .494¬±.067</cell></row><row><cell>Field Transfer</cell><cell>GAE GraphSAGE (unsp.) Graph Infomax GPT-GNN (Attr) GPT-GNN (Edge)</cell><cell>.403¬±.114 .368¬±.125 .387¬±.112 .396¬±.118 .401¬±.109</cell><cell>.418¬±.093 .401¬±.096 .404¬±.097 .423¬±.105 .428¬±.096</cell><cell>.816¬±.084 .803¬±.092 .810¬±.084 .818¬±.086 .826¬±.093</cell><cell cols="2">.610¬±.070 .568¬±.066 .516¬±.071 .597¬±.065 .554¬±.061 .509¬±.052 .604¬±.063 .561¬±.063 .506¬±.074 .621¬±.053 .576¬±.056 .528¬±.061 .616¬±.060 .570¬±.059 .520¬±.047</cell></row><row><cell></cell><cell>GPT-GNN</cell><cell>.407¬±.107</cell><cell>.432¬±.098</cell><cell>.831¬±.102</cell><cell cols="2">.625¬±.055 .577¬±.054 .531¬±.043</cell></row><row><cell>Time Transfer</cell><cell>GAE GraphSAGE (unsp.) Graph Infomax GPT-GNN (Attr) GPT-GNN (Edge)</cell><cell>.384¬±.117 .352¬±.121 .369¬±.116 .382¬±.114 .392¬±.105</cell><cell>.412¬±.101 .394¬±.105 .398¬±.102 .414¬±.098 .421¬±.102</cell><cell>.812¬±.095 .799¬±.093 .805¬±.089 .811¬±.089 .821¬±.088</cell><cell cols="2">.603¬±.065 .562¬±.063 .510¬±.071 .594¬±.067 .553¬±.069 .501¬±.064 .599¬±.063 .558¬±.060 .503¬±.063 .614¬±.057 .573¬±.053 .522¬±.051 .608¬±.055 .567¬±.038 .513¬±.058</cell></row><row><cell></cell><cell>GPT-GNN</cell><cell>.400¬±.108</cell><cell>.429¬±.101</cell><cell>.825¬±.093</cell><cell cols="2">.617¬±.059 .572¬±.059 .525¬±.057</cell></row><row><cell>Time + Field Transfer</cell><cell>GAE GraphSAGE (unsp.) Graph Infomax GPT-GNN (Attr) -(w/o node separation) GPT-GNN (Edge) -(w/o adaptive queue) GPT-GNN</cell><cell>.371¬±.124 .349¬±.130 .360¬±.121 .364¬±.115 .347¬±.128 .386¬±.116 .376¬±.121 .393¬±.112</cell><cell>.403¬±.108 .393¬±.118 .391¬±.102 .391¬±.102 .414¬±.104 .410¬±.115 .420¬±.108</cell><cell>.806¬±.102 .797¬±.097 .800¬±.093 .809¬±.094 .791¬±.108 .815¬±.105 .808¬±.104 .818¬±.102</cell><cell cols="2">.596¬±.065 .554¬±.063 .505¬±.061 .589¬±.071 .545¬±.068 .498¬±.064 .591¬±.068 .550¬±.058 .501¬±.063 .608¬±.062 .569¬±.057 .517¬±.057 .585¬±.068 .546¬±.062 .497¬±.062 .604¬±.058 .565¬±.057 .514¬±.047 .599¬±.068 .562¬±.065 .509¬±.062 .610¬±.054 .572¬±.063 .521¬±.049</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Compare the pre-training Gain with different GNN architectures. Evaluate on OAG, Paper-Field Task, under Combined Transfer setting with 10% training data.</figDesc><table><row><cell>Model</cell><cell cols="5">HGT GCN GAT RGCN HAN</cell></row><row><cell>No Pre-train</cell><cell>.336</cell><cell>.317</cell><cell>.308</cell><cell>.296</cell><cell>.322</cell></row><row><cell>GPT-GNN</cell><cell>.407</cell><cell>.349</cell><cell>.362</cell><cell>.351</cell><cell>.384</cell></row><row><cell cols="6">Relative Gain 21.1% 10.1% 17.5% 18.6% 19.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Downstream performance on homogeneous graphs, including the paper citation network in OAG and Reddit.</figDesc><table><row><cell cols="2">Downstream Dataset OAG (citation)</cell><cell>Reddit</cell></row><row><cell>No Pre-train</cell><cell>.281¬±.087</cell><cell>.873¬±.036</cell></row><row><cell>GAE</cell><cell>.296¬±.095</cell><cell>.885¬±.039</cell></row><row><cell>GraphSAGE (unsp.)</cell><cell>.287¬±.093</cell><cell>.880¬±.042</cell></row><row><cell>Graph Infomax</cell><cell>.291¬±.086</cell><cell>.877¬±.034</cell></row><row><cell>GPT-GNN</cell><cell>.309¬±.081</cell><cell>.896¬±.028</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is partially supported by NSF III-1705169, NSF CAREER Award 1741634, NSF 1937599, DARPA HR00112090027, DARPA N660011924032, Okawa Foundation Grant, and Amazon Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>arxiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Heterogeneous Network Representation Learning</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational Graph Auto-Encoders</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient Graph Generation with Graph Recurrent Attention Networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to Rank for Information Retrieval</title>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic Gradient Descent with Warm Restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Kr√§henb√ºhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mining Heterogeneous Information Networks: Principles and Methodologies</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integrating meta-path selection with user-guided object clustering in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">A√§ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft Academic Graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Attention Network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R√£ƒæmi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>arXiv:cs.CL/1910.03771</idno>
		<title level="m">Transformers: State-of-the-art Natural Language Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">OAG: Toward Linking Large-scale Heterogeneous Entity Graphs</title>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
