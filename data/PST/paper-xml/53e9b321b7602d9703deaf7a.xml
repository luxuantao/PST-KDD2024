<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Worst-case Optimal Join Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hung</forename><forename type="middle">Q</forename><surname>Ngo</surname></persName>
							<email>hungngo@buffalo.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ely</forename><surname>Porat</surname></persName>
							<email>porately@cs.biu.ac.il</email>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Worst-case Optimal Join Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C2298B0F91B09E13561F7CAB02F9870</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.4 [Database Management]: Systems-Relational databases Algorithms</term>
					<term>Theory Join Algorithms</term>
					<term>fractional cover bound</term>
					<term>Loomis-Whitney inequality</term>
					<term>Bollobás-Thomason inequality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficient join processing is one of the most fundamental and wellstudied tasks in database research. In this work, we examine algorithms for natural join queries over many relations and describe a novel algorithm to process these queries optimally in terms of worst-case data complexity. Our result builds on recent work by Atserias, Grohe, and Marx, who gave bounds on the size of a full conjunctive query in terms of the sizes of the individual relations in the body of the query. These bounds, however, are not constructive: they rely on Shearer's entropy inequality which is informationtheoretic. Thus, the previous results leave open the question of whether there exist algorithms whose running time achieve these optimal bounds. An answer to this question may be interesting to database practice, as we show in this paper that any projectjoin plan is polynomially slower than the optimal bound for some queries. We construct an algorithm whose running time is worstcase optimal for all natural join queries. Our result may be of independent interest, as our algorithm also yields a constructive proof of the general fractional cover bound by Atserias, Grohe, and Marx without using Shearer's inequality. In addition, we show that this bound is equivalent to a geometric inequality by Bollobás and Thomason, one of whose special cases is the famous Loomis-Whitney inequality. Hence, our results algorithmically prove these inequalities as well. Finally, we discuss how our algorithm can be used to compute a relaxed notion of joins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recently, Grohe and Marx <ref type="bibr" target="#b17">[17]</ref> and Atserias, Grohe, and Marx <ref type="bibr" target="#b5">[5]</ref> (AGM's results henceforth) derived tight bounds on the number of output tuples of a full conjunctive query <ref type="foot" target="#foot_0">1</ref> in terms of the sizes of the relations mentioned in the query's body. As query output size estimation is fundamentally important for efficient query processing, these results have generated a great deal of excitement.</p><p>To understand the spirit of AGM's results, consider the following example where we have a schema with three attributes, A, B, and C, and three relations, R(A, B), S (B,C) and T (A, C), defined over those attributes. Consider the following natural join query:</p><formula xml:id="formula_0">q = R S T<label>(1)</label></formula><p>Let q(I) denote the set of tuples that is output from applying q to a database instance I, i.e. q(I) is the set of triples of constants (a, b, c) such that R(ab), S (bc), and T (ac) are in I. Our goal is to bound the number of tuples returned by q on I, One can obtain a better bound by noticing that the output of any pair-wise join (say R S ) will be a superset of q(I), since the union of the attributes in R and S together contain (or "cover") all attributes. This leads to the bound |q(I)| ≤ N 2 . AGM showed that one can get a better upper bound of |q(I)| ≤ N 3/2 by generalizing the notion of cover to a socalled "fractional cover" (see <ref type="bibr">Section 2)</ref>. Moreover, this estimate is tight in the sense that for infinitely many values of N, one can find a database instance I for which |R| = |S | = |T | = N and |q(I)| = N 3/2 . These non-trivial estimates are exciting to database researchers as they offer previously unknown, nontrivial methods to estimate the cardinality of a query result -a fundamental problem to support efficient query processing. More generally, given an arbitrary natural-join query q and given the sizes of input relations, the AGM method can generate an upper bound U such that |q(I)| ≤ U, where U depends on the "best" fractional cover of the attributes. This "best" fractional cover can be computed by a linear program (see Section 2 for more details). Henceforth, we refer to this inequality as the AGM's fractional cover inequality, and the bound U as the AGM's fractional cover bound. They also show that the bound is essentially optimal in the sense that for infinitely many sizes of input relations, there exists an instance I such that each relation in I is of the prescribed size and |q(I)| = U.</p><formula xml:id="formula_1">denoted</formula><p>AGM's results leave open whether one can compute the actual set q(I) in time O(U). In fact, AGM observed this issue and presented an algorithm that computes q(I) with a running time of O(|q| 2 • U • N) where N is the cardinality of the largest input relation and |q| denotes the size of the query q. AGM established that their joinproject plan can in some cases be super-polynomially better than any join-only plan. However, AGM's join algorithm is not optimal. Even on query <ref type="bibr" target="#b1">(1)</ref>, we can construct a family of database instances I 1 , I 2 , . . . , I N , . . . , such that in the Nth instance I N we have |R| = |S | = |T | = N and any join-project plan (which includes AGM's algorithm) takes Ω(N<ref type="foot" target="#foot_1">2</ref> )-time even though from AGM's bound we know that |q(I)| ≤ U = N 3/2 , which is the best worst-case run-time that one can hope for.</p><p>The √ N-gap on a small example motivates our central question. In what follows, natural join queries are defined as the join of a set of relations R 1 , . . . , R m .</p><p>Optimal Worst-case Join Evaluation Problem (Optimal Join Problem). Given a fixed database schema</p><formula xml:id="formula_2">R = R i ( Āi ) m i=1</formula><p>and an m-tuple of integers N = (N 1 , . . . , N m ). Let q be the natural join query joining the relations in R and let I( N) be the set of all instances such that |R I i | = N i for i = 1, . . . , m. Define U = sup I∈I( N) |q(I)|. Then, the optimal worst-case join evaluation problem is to evaluate q in time O(U + m i=1 N i ). Since any algorithm to produce q(I) requires time at least |q(I)|, an algorithm that solves the above problem would have an optimal worst-case data-complexity. 2 (Note that we are mainly concerned with data complexity and thus the O(U) bound above ignores the dependence on |q|. Our results have a small O(|q|) factor.)</p><p>Implicitly, this problem has been studied for over three decades: a modern RDBMS uses decades of highly tuned algorithms to efficiently produce query results. Nevertheless, as we described above, such systems are asymptotically suboptimal -even for query <ref type="bibr" target="#b1">(1)</ref>. Our main result is an algorithm that achieves asymptotically optimal worst-case running times for all join queries.</p><p>We begin by describing connections between AGM's inequality and a family of inequalities in geometry. In particular, we show that the AGM's inequality is equivalent to the discrete version of a geometric inequality proved by Bollobás and Thomason <ref type="bibr" target="#b8">[8,</ref><ref type="bibr">Theorem 2]</ref>. This equivalence is shown in Section 2.2.</p><p>Our ideas for an algorithm solving the optimal join problem begin by examining a special case of the Bollobás-Thomason (BT) inequality: the classic Loomis-Whitney (LW) inequality <ref type="bibr" target="#b26">[26]</ref>. The LW inequality bounds the measure of an n-dimensional set in terms of the measures of its (n -1)-dimensional projections onto the coordinate hyperplanes. The bound |q(I)| ≤ √ |R||S ||T | for query (1) is exactly the LW inequality with n = 3 applied to the discrete measure. Our algorithmic development begins with a slight generalization of query <ref type="bibr" target="#b1">(1)</ref>. We describe an algorithm for join queries which have the same format as in the LW inequality setup with n ≥ 3. In particular, we consider "LW instances" of the optimal join problem, where the query is to join n relations whose attribute sets are all the distinct (n -1)-subsets of a universe of n attributes. Since the LW inequality is tight, and our join algorithm has running time that is asymptotically data-optimal for this class of queries (e.g., O(N 3/2 ) in our motivating example), our algorithm is worst-case data-complexity optimal for LW instances.</p><p>Our algorithm for LW instances exhibits a key twist compared to a conventional join algorithm. The twist is that our algorithm partitions the values of the join key on each side of the join into two sets: those values that are heavy and those values that are light. Intuitively, a value of a join key is heavy if its fanout is high enough so that joining all such join keys could violate the size bound (e.g., N 3/2 above). The art is selecting the precise fanout threshold for when a join key is heavy. This per-tuple choice of join strategy is not typically done in standard RDBMS join processing.</p><p>Building on the algorithm for LW instances, we next describe our main result: an algorithm to solve the optimal join problem for all join queries. In particular, we design an algorithm for evaluating join queries which not only proves AGM's fractional cover inequality without using the information-theoretic Shearer's inequality, but also has a running time that is linear in the bound (modulo pre-processing time). As AGM's inequality is equivalent to BT inequality and thus implies LW inequality, our result is the first algorithmic proof of these geometric inequalities as well. To do this, we must carefully select which projections of relations to join and in which order our algorithm joins relations on a "per tuple" basis as in the LW-instance case. Our algorithm computes these orderings, and then at each stage it performs a heavy/light tuple check that is similar to the strategy used for the LW instances earlier.</p><p>It is easy to show that any join-only plan is suboptimal for some queries. A natural question is, when do classical RDBMS algorithms have higher worst-case run-time than our proposed approach? AGM's analysis of their join-project algorithm leads to a worst case run-time complexity that is a factor of the largest relation worse than the AGM's bound. To investigate whether AGM's analysis is tight, we ask a sharper variant of this question: Given a query q does there exist a family of instances I such that our algorithm runs asymptotically faster than a standard binary-join-based plan or AGM's join-project plan? We give a partial answer to this question by describing a sufficient syntactic condition for the query q such that for each k ≥ 2, we can construct a family of instances where each relation is of size N such that any project-join plan (which as a special case includes AGM's algorithm) will need time Ω(N 2 /k 2 ), while the fractional cover bound is O(N 1+1/(k-1) ) -an asymptotic gap. We then show through a more detailed analysis that our algorithm on these instances takes O(k 2 N)-time.</p><p>We consider several extensions and improvements of our main result. In terms of the dependence on query size, our algorithms are also efficient (at most linear in |q|, which is better than the quadratic dependence in AGM) for full queries, but they are not necessarily optimal. In particular, if each relation in the schema has arity 2, we are able to give an algorithm with better query complexity than our general algorithm. This shows that in general our algorithm's dependence on the factors of the query is not the best possible. We also consider computing a relaxed notion of joins and give worstcase optimal algorithms for this problem as well.</p><p>Outline. The remainder of the paper is organized as follows: in the rest of this section, we describe related work. In Section 2 we describe our notation, formulate the main problem, and prove the connection between AGM's inequality and BT inequality. Our main results are in Section 3. We first present a data-optimal join algorithm for LW instances, and then present the optimal algorithm for arbitrary join queries. We also discuss the limits of performance of prior approaches and our approach in more detail. In Section 4, we describe several extensions. We conclude in Section 5. Due to space constraints some proofs are deferred to the full version <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Grohe and Marx <ref type="bibr" target="#b17">[17]</ref> made the first (implicit) connection between fractional edge cover and the output size of a conjunctive query. (Their results were stated for constraint satisfaction problems.) At-serias, Grohe, and Marx <ref type="bibr" target="#b5">[5]</ref> extended Grohe and Marx's results in the database setting.</p><p>The first relevant result of AGM is the following inequality. Consider a join query over relations R e , e ∈ E, where E is a collection of subsets of an attribute "universe" V, and relation R e is on attribute set e. Then, the number of output tuples is bounded above by e∈E |R e | xe , where x = (x e ) e∈E is an arbitrary fractional cover of the hypergraph H = (V, E).</p><p>They also showed that this bound is tight. In particular, for infinitely many positive integers N there is a database instance with |R e | = N, ∀e ∈ E, and the upper bound gives the actual number of output tuples. When the sizes |R e | were given as inputs to the (output size estimation) problem, obviously the best upper bound is obtained by picking the fractional cover x which minimizes the linear objective function e∈E (log |R e |)• x e . In this "size constrained" case, however, their lower bound is off from the upper bound by a factor of 2 n , where n is the total number of attributes. AGM also presented an inapproximability result which justifies this gap. Note, however, that the gap is only dependent on the query size and the bound is still asymptotically optimal in the data-complexity sense.</p><p>The second relevant result from AGM is a join-project plan with running time O |q| 2 N 1+ xe max , where N max is the maximum size of input relations and |q| = |V| • |E| is the query size.</p><p>The AGM's inequality contains as a special case the discrete versions of two well-known inequalities in geometry: the Loomis-Whitney (LW) inequality <ref type="bibr" target="#b26">[26]</ref> and its generalization the Bollobás-Thomason (BT) inequality <ref type="bibr" target="#b8">[8]</ref>. There are two typical proofs of the discrete LW and BT inequalities. The first proof is by induction using Hölder's inequality <ref type="bibr" target="#b8">[8]</ref>. The second proof (see Lyons and Peres <ref type="bibr" target="#b27">[27]</ref>) essentially uses "equivalent" entropy inequalities by Han <ref type="bibr" target="#b19">[19]</ref> and its generalization by Shearer <ref type="bibr" target="#b9">[9]</ref>, which was also the route Grohe and Marx <ref type="bibr" target="#b17">[17]</ref> took to prove AGM's bound. All of these proofs are non-constructive.</p><p>There are many applications of the discrete LW and BT inequalities. The n = 3 case of the LW inequality was used to prove communication lower bounds for matrix multiplication on distributed memory parallel computers <ref type="bibr" target="#b22">[22]</ref>. The inequality was used to prove submultiplicativity inequalities regarding sums of sets of integers <ref type="bibr" target="#b18">[18]</ref>. In <ref type="bibr" target="#b25">[25]</ref>, a special case of BT inequality was used to prove a networkcoding bound. Recently, some of the authors of this paper have used our algorithmic version of the LW inequality to design a new sub-linear time decodable compressed sensing matrices <ref type="bibr" target="#b12">[12]</ref> and efficient pattern matching algorithms <ref type="bibr" target="#b31">[31]</ref>.</p><p>Inspired by AGM's results, Gottlob, Lee, and Valiant <ref type="bibr" target="#b13">[13]</ref> generalized AGM's results to conjunctive queries with functional dependencies. Their key idea was a new notion, the "coloring number", which is derived from the dual linear program of the fractional cover linear program.</p><p>Join processing is one of the most studied problems in database research. On the theoretical side, that acyclic queries can be computed in polynomial time is one of the classic results in database theory <ref type="bibr" target="#b1">[1,</ref><ref type="bibr">Ch. 6.4]</ref>. When the join graph is acyclic, there are several known results which achieve (near) optimal run time with respect to the output size <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b38">38]</ref>. One direction to extend the reach of these positive results is using hypertree decompositions that capture the idea that many queries are nearly acyclic <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>; This work has culminated in efficient algorithms for broad classes of conjunctive queries -a more general class of queries than we consider here. The algorithms in this work are complementary: our algorithms are most interesting when the queries are cyclic. In practice, a staggering number of variants have been considered, we list a few: Block-Nested loop join, Hash-Join, Grace, Sort-merge (see Grafe <ref type="bibr" target="#b16">[16]</ref> for a survey). Conceptually, it is interesting that none of the classical algorithms consider performing a per-tuple cardinality estimation as our algorithm does. It is interesting future work to implement our algorithm to better understand its performance.</p><p>Related to the problem of estimating the size of an output is cardinality estimation. A large number of structures have been proposed for cardinality estimation <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b33">33]</ref>. Often, deriving estimates for arbitrary query expressions involves making statistical assumptions, such as the independence or containment assumptions, which may result in large estimation errors <ref type="bibr" target="#b21">[21]</ref>. Follow-up work has considered sophisticated probability models, entropy-based models <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b35">35]</ref> and graphical models <ref type="bibr" target="#b36">[36]</ref>. In contrast, in this work we examine the worst case behavior of algorithms in terms of its cardinality estimates.</p><p>On a technical level, the work adaptive query processing is related, e.g., Eddies <ref type="bibr" target="#b6">[6]</ref> and RIO <ref type="bibr">[7]</ref>. The main idea is that to compensate for erroneous statistics, the query plan may adaptively be changed (as it better understands the properties of the data). While both our method and the methods proposed here are adaptive in some sense, our focus is different: this body of work focuses on heuristic optimization methods, while our focus is on provable worstcase running time bounds. A related idea has been considered in practice: heuristics that split tuples based on their fanout have been deployed in modern parallel databases to handle skew <ref type="bibr" target="#b39">[39]</ref>. This idea was not used to theoretically improve the running time of join algorithms. We are excited that a key mechanism used by our algorithm is implemented in a modern commercial system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>We first describe our notation and formal problem statement. Then, we describe the connection between AGM's result and the BT inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation and Formal Problem Statement</head><p>We assume the existence of a set of attribute names A = A 1 , . . . , A n with associated domains D 1 , . . . , D n and infinite set of relational symbols R 1 , R 2 , . . . . A relational schema for the symbol R i of arity k is a tuple Āi = (A i 1 , . . . , A i k ) of distinct attributes that defines the attributes of the relation. A relational database schema is a set of relational symbols and associated schemas denoted by R 1 ( Ā1 ), . . . , R m ( Ām ). A relational instance for R(A i 1 , . . . ,</p><formula xml:id="formula_3">A i k ) is a subset of D i 1 × • • • × D i k .</formula><p>A relational database I is an instance for each relational symbol in schema, denoted by R I i . A natural join query (or simply query) q is specified by a finite subset of relational symbols q ⊆ N, denoted by i∈q R i . Let Ā(q) denote the set of all attributes that appear in some relation in q, that is Ā(q) = {A | A ∈ Āi for some i ∈ q}. Given a tuple t we will write t Ā to emphasize that its support is the attribute set Ā. Further, for any S ⊂ Ā we let t S denote t restricted to S . Given a database instance I, the output of the query q on I is denoted q(I) and is defined as</p><formula xml:id="formula_4">q(I) def = t ∈ D Ā(q) | t Āi ∈ R I i for each i ∈ q</formula><p>where D Ā(q) is a shorthand for × i:A i ∈ Ā(q) D i . We also use the notion of a semijoin: Given two relations R( Ā) and S ( B) their semijoin R S is defined by</p><formula xml:id="formula_5">R S def = {t ∈ R : ∃u ∈ S s.t. t Ā∩ B = u Ā∩ B} .</formula><p>For any relation R( Ā), and any subset S ⊆ Ā of its attributes, let π S (R) denote the projection of R onto S , i.e.</p><formula xml:id="formula_6">π S (R) = t S | ∃t Ā\ S , (t S , t Ā\ S ) ∈ R .</formula><p>For any tuple t S , define the t S -section of R as</p><formula xml:id="formula_7">R[t S ] = π Ā\ S (R {t S }).</formula><p>From Join Queries to Hypergraphs. A query q = i∈q R i on attributes Ā(q) can be viewed as a hypergraph H = (V, E) where V = Ā(q) and there is an edge e i = Āi for each i ∈ q. Let N e = |R e | be the number of tuples in R e . From now on we use the hypergraph and the original notation for the query interchangeably.</p><p>We use this hypergraph to introduce the fractional edge cover polytope that plays a central role in our technical developments. The fractional edge cover polytope defined by H is the set of all points x = (x e ) e∈E ∈ R E such that e:v∈e x e ≥ 1, for any v ∈ V x e ≥ 0, for any e ∈ E Note that the solution x e = 1 for e ∈ E is always a feasible solution for hypergraphs representing join queries (since each vertex appears in some edge, e:v∈e x e ≥ 1). A point x in the polytope is also called a fractional (edge) cover solution of the hypergraph H.</p><p>Atserias, Grohe, and Marx <ref type="bibr" target="#b5">[5]</ref> establish that for any point x = (x e ) e∈E in the fractional edge cover polytope</p><formula xml:id="formula_8">| e∈E R e | ≤ e∈E N xe e . (<label>2</label></formula><formula xml:id="formula_9">)</formula><p>The bound is proved nonconstructively using Shearer's entropy inequality <ref type="bibr" target="#b9">[9]</ref>. However, AGM provide an algorithm based on join-project plans that runs in time O(|q| 2 • N 1+ e xe max ) where N max = max e∈E N e . They observed that for a fixed hypergraph H and given sizes N e the bound (2) can be minimized by solving the linear program which minimizes the linear objective e (log N e )• x e over fractional edge cover solutions x. (Since in linear time we can determine if we have an empty relation, and hence an empty output, for the rest of the paper we are always going to assume that N e ≥ 1.) We recast our problem using the above language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.1 (OJ Problem -Optimal Join Problem). With the notation above, design an algorithm to compute e∈E R e with running time</head><formula xml:id="formula_10">O        f (|V|, |E|) • e∈E N xe e + g(|V|, |E|) e∈E N e        .</formula><p>Here f (|V|, |E|) and g(|V|, |E|) are ideally polynomials with (small) constant degrees, which only depend on the query size. The linear term e∈E N e is to read and index the input (in a specific way). Such an algorithm would be data-optimal in the worst case. 3   We recast our motivating example from the introduction in our notation. Recall that we are given as input, <ref type="figure">R(A,</ref><ref type="figure">B),</ref><ref type="figure">S (B,</ref><ref type="figure">C),</ref><ref type="figure">T (A,</ref><ref type="figure">C</ref>). The resulting hypergraph (V, E) is such that V = {A, B, C} and E contains three edges corresponding to each of R, S , and T . More explicitly, we have E = {{A, B}, {B, C}, {A, C}}. Thus, |V| = 3 and |E| = 3. If N e = N, one can check that the optimal solution to the LP is x e = 1 2 for e ∈ E which has the objective value 3 2 log N; 3 As shall be seen later, the worst-case preprocessing time is linear in the RAM model using the "lazy array" technique of Lemma A.3 of Flum, Frick, and Grohe <ref type="bibr" target="#b11">[11]</ref>, at the expense of potentially huge space overhead. To remove this excess space, we can build a set of hash indices in expected linear time using any perfect hashing scheme with worst-case constant time, e.g., Cuckoo hashing. Also, one can build a search tree for each relation to ensure a worst-case guarantee but with an extra log factor in the running time. Example 1. Given an odd integer N, we construct an instance I N such that (1)</p><formula xml:id="formula_11">|R I N | = |S I N | = |T I N | = N, (2) |R S | = |R T | = |S T | = (N +1) 2 /4+(N -1)/2, and (3) |R S T | = (3N -1)/2.</formula><p>The following instance satisfies all three properties:</p><formula xml:id="formula_12">R I N = S I N = T I N = {(0, j)} (N-1)/2 j=0 ∪ {( j, 0)} (N-1)/2 j=0 . For example, R S = {(i, 0, j)} (N-1)/2 i, j=0 ∪ {(0, i, 0)} i=1,...,(N-1)/2</formula><p>and R S T = {(0, 0, j)} (N-1)/2 j=0 ∪{(0, j, 0)} (N-1)/2 j=1 ∪{( j, 0, 0)} (N-1)/2 j=1 . Thus, any standard join-based algorithm takes time Ω(N 2 ). We show later that any project-join plan (which includes AGM's algorithm) takes Ω(N 2 )-time too. Recall that the AGM bound for this instance is O(N 3/2 ), and our algorithm thus takes time O(N 3/2 ). In fact, as shall be shown later, on this particular family of instances our algorithm takes only O(N) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Connections to Geometric Inequalities</head><p>We describe the Bollobás-Thomason (BT) inequality from discrete geometry and prove that the BT inequality is equivalent to the AGM inequality. We then look at a special case of the BT inequality called the Loomis-Whitney (LW) inequality, from which our algorithmic development starts in the next section. The BT inequality can be stated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2.2 (Discrete Bollobás-Thomason (BT) Inequality). Let S ⊂ Z n be a finite set of n-dimensional grid points. Let F be a collection of subsets of [n] in which every i ∈ [n] occurs in exactly d members of F . Let S F be the set of projections</head><formula xml:id="formula_13">Z n → Z F of points in S onto the coordinates in F. Then, |S | d ≤ F∈F |S F |.</formula><p>To prove the equivalence between BT inequality and the AGM bound, we first need a simple observation, whose proof can be found in the full version <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.3. Consider an instance of the OJ problem consisting of a hypergraph H = (V, E), a fractional cover x = (x e ) e∈E of H, and relations R e for e ∈ E. Then, in linear time we can transform the instance into another instance H</head><formula xml:id="formula_14">= (V, E ), x = (x e ) e∈E ,</formula><p>(R e ) e∈E , such that the following properties hold: (a) x is a "tight" fractional edge cover of the hypergraph H , namely x ≥ 0 and e∈E :v∈e</p><formula xml:id="formula_15">x e = 1, for every v ∈ V.</formula><p>(b) The two problems have the same answer:</p><p>e∈E R e = e∈E R e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c) AGM's bound on the transformed instance is at least as good as that of the original instance:</head><p>e∈E</p><formula xml:id="formula_16">|R e | x e ≤ e∈E |R e | xe .</formula><p>With this technical observation, we can now connect the two families of inequalities: Proposition 2.4. BT inequality and AGM's fractional cover bound are equivalent.</p><p>Proof. To see that AGM's inequality implies BT inequality, we think of each coordinate as an attribute, and the projections S F as the input relations. Set</p><formula xml:id="formula_17">x F = 1/d for each F ∈ F . It follows that x = (x F ) F∈F is a fractional cover for the hypergraph H = ([n], F ). AGM's bound then implies that |S | ≤ F∈F |S F | 1/d .</formula><p>Conversely, consider an instance of the OJ problem with hypergraph H = (V, E) and a rational fractional cover x = (x e ) e∈E of H. First, by Lemma 2.3, we can assume that all cover constraints are tight, i.e., e:v∈e x e = 1, for any v ∈ V. Second, when all variables x e are rational we can write x e as d e /d for a positive common denominator d. Consequently, When some of the x e are not rational, we can replace each irrational x e by a rational x e &gt; x e with a sufficiently small difference and apply the above analysis.</p><p>Loomis-Whitney. We now consider a special case of the BT inequality, the discrete version of a classic geometric inequality called the Loomis-Whitney inequality <ref type="bibr" target="#b26">[26]</ref>. The setting is that for 4 where in this case x e = 1/(|V| -1), ∀e ∈ E is a fractional cover solution for (V, E). LW showed the following: Theorem 2.5 (Discrete Loomis-Whitney (LW) inequality). Let S ⊂ Z n be a finite set of n-dimensional grid points. For each di-</p><formula xml:id="formula_18">n ≥ 2, V = [n] and E = V |V|-1 ,</formula><formula xml:id="formula_19">mension i ∈ [n], let S [n]\{i} denote the (n-1)-dimensional projection of S onto the coordinates [n] \ {i}. Then, |S | n-1 ≤ n i=1 |S [n]\{i} |.</formula><p>The LW inequality is a special case of the BT inequality (and so the AGM inequality), and it is with this special case that we begin our algorithmic development in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MAIN RESULTS</head><p>We first describe our algorithm for the LW inequality. We then describe our main algorithmic result, which is an algorithm that proves the AGM bound and whose running time matches the bound. Finally, we observe some limitations of project-join plans, which include as special cases both standard binary join-based algorithms and AGM's join algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithm for Loomis-Whitney Instances</head><p>We first consider queries whose forms are slightly more general than that in our motivating example <ref type="bibr" target="#b1">(1)</ref>. This class of queries has the same setup as in LW inequality of Theorem 2.5. In this spirit, we define a Loomis-Whitney (LW) instance of the OJ problem to be a hypergraph H = (V, E) such that E is the collection of all subsets of V of size |V| -1. When the LW inequality is applied to this setting, it guarantees that | e∈E R e | ≤ ( e∈E N e ) 1/(n-1) , and the bound is tight in the worst case. The main result of this section is the following: Theorem 3.1 (Loomis-Whitney instance). Let n ≥ 2 be an integer. Consider a Loomis-Whitney instance H = (V = [n], E) of 4 We use E = V k to denote the set of all undirected hyperedges (subsets of nodes) of size exactly k.</p><p>the OJ problem with input relations R e , where |R e | = N e for e ∈ E. Then the join e∈E R e can be computed in time</p><formula xml:id="formula_20">O         n 2 •        e∈E N e        1/(n-1) + n 2 e∈E N e         .</formula><p>Before describing our algorithm, we give an example that illustrates the intuition behind our algorithm and solves the motivating example (1) from the introduction.</p><p>Example 2. Recall that our input has three relations R(A, B), S (B, C), T (A, C) and an instance I such that</p><formula xml:id="formula_21">|R I | = |S I | = |T I | = N. Let J = R S</formula><p>T . Our goal is to construct J in time O(N 3/2 ). For exposition, define a parameter τ ≥ 0 that we will choose below. We use τ to define two sets that effectively partition the tuples in R I .</p><formula xml:id="formula_22">D = {t B ∈ π B (R) : |R I [t B ]| &gt; τ} and G = {(t A , t B ) ∈ R I : t B D}</formula><p>Intuitively, D contains the heavy join keys in R. Note that |D| &lt; N/τ. Observe that J ⊆ (D × T ) ∪ (G S ) (also note that this union is disjoint). Our algorithm will construct D × T (resp. G S ) in time O(N 3/2 ), then it will filter out those tuples in both S and R (resp. T ) using the hash tables on S and R (resp. T ); this process produces exactly J. Since our running time is linear in the above sets, the key question is how big are these two sets?</p><p>Observe that To describe the general algorithm underlying Theorem 3.1, we need to introduce some data structures and notation.</p><formula xml:id="formula_23">|D × T | ≤ (N/τ)N = N 2 /τ while |G S | = t B ∈π B (G) |R[t B ]||S [t B ]| ≤ τN. Setting τ = √ N makes both terms at most N 3/2 ,</formula><p>Data Structures and Notation. Let H = (V, E) be an LW instance. Algorithm 1 begins by constructing a labeled, binary tree T whose set of leaves is exactly V and each internal node has exactly two children. Any binary tree over this leaf set can be used. We denote the left child of any internal node x as lc(x) and its right child as rc(x). Each node x ∈ T is labeled by a function label, where label(x) ⊆ V are defined inductively as follows: label(x) = V \ {x} for a leaf node x ∈ V, and label(x) = label(lc(x)) ∩ label(rc(x)) if x is an internal node of the tree. It is immediate that for any internal node x we have label(lc(x)) ∪ label(rc(x)) = V and that label(x) = ∅ if and only if x is the root of the tree. Let J denote the output set of tuples of the join, i.e. J = e∈E R e . For any node x ∈ T , let T (x) denote the subtree of T rooted at x, and L(T (x)) denote the set of leaves under this subtree. For any three relations R, S , and T , define R S T = (R T ) S .</p><p>Algorithm for LW instances. Algorithm 1 works in two stages.</p><p>Let u be the root of the tree T . First we compute a tuple set C(u) containing the output J such that C(u) has a relatively small size (at most the size bound times n). Second, we prune those tuples that cannot participate in the join (which takes only linear time in the size of C(u)). The interesting part is how we compute C(u). Inductively, we compute a set C(x) that at each stage contains candidate tuples and an auxiliary set D(x), which is a superset of the projection π label(x) (J \ C(x)). The set D(x) intuitively allows us to deal with those tuples that would blow up the size of an intermediate relation. The key novelty in Algorithm 1 is the construction of the set G that contains all those tuples (join keys) that are in some </p><formula xml:id="formula_24">1: if x is a leaf then 2: return (∅, R label(x) ) 3: (C L , D L ) ← LW(lc(x)) and (C R , D R ) ← LW(rc(x)) 4: F ← π label(x) (D L ) ∩ π label(x) (D R ) 5: G ← {t ∈ F : |D L [t]| + 1 ≤ P/|D R | } // F = G = ∅ if |D R | = 0 6: if x is the root of T then 7: C ← (D L D R ) ∪ C L ∪ C R 8: D ← ∅ 9: else 10: C ← (D L G D R ) ∪ C L ∪ C R 11: D ← F \ G. 12: return (C, D)</formula><p>sense light, i.e., joining over them would not exceed the size/time bound P by much. The elements that are not light are postponed to be processed later by pushing them to the set D(x). This is in full analogy to the sets G and D defined in Example 2.</p><p>By induction on each step of the algorithm, we establish in the full version of this paper that the following three properties hold for every node x ∈ T : (1)</p><formula xml:id="formula_25">π label(x) (J \ C(x)) ⊆ D(x); (2) |C(x)| ≤ (|L(T (x))| -1) • P; and (3) |D(x)| ≤ min min l∈L(T (x)) {N [n]\{l} }, l∈L(T (x)) N [n]\{l} P |L(T (x))|-1 .</formula><p>Assuming the above three properties, we next prove that our algorithm correctly computes the join J. Let u denote the root of the tree T . By property <ref type="bibr" target="#b1">(1)</ref>,</p><formula xml:id="formula_26">π label(lc(u)) (J \ C(lc(u))) ⊆ D(lc(u)) π label(rc(u)) (J \ C(rc(u))) ⊆ D(rc(u))</formula><p>Hence,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J\(C(lc(u))∪C(rc(u))) ⊆ D(lc(u))×D(rc(u)) = D(lc(u)) D(rc(u)).</head><p>This implies J ⊆ C(u). Thus, from C(u) we can compute J by keeping only tuples in C(u) whose projection on any attribute set e ∈ E = [n]  n-1 is contained in R e (the "pruning" step).</p><p>Running Time. For the run time complexity of the above algorithm, we claim that for every node x, we need time O(n|C(x)| + n|D(x)|). To see this note that for each node x, the lines 4, 5, 7, 10, and 11 of Algorithm 1 can be computed within the time bound using hashing. Using property (3) above, we have a (loose) upper bound of O nP + n min l∈L(T (x)) N [n]\{l} on the run time for node x. Summing the run time over all the nodes in the tree gives the claimed run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An Algorithm for All Join Queries</head><p>This section presents our algorithm for proving the AGM inequality that has a running time that matches the bound. (3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Furthermore, given the projections S F we can compute S in time</head><formula xml:id="formula_27">O          |F |n         F∈F |S F |         1/d + n 2 F∈F |S F | + |F | 2 n         </formula><p>Recall that the LW inequality is a special case of the BT inequality. Hence, our algorithm proves the LW inequality as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The Algorithm and Terminology</head><p>Algorithm 2 has three main phases: (1) We first construct a labeled binary tree that we call a query plan tree or QP tree. Then, we construct a total order of attributes to be used in the next step.</p><p>(2) Using the total order from phase (1), we construct a set of hash indices for various probing operations in the next step. In step (3), we give a recursive algorithm to compute the required join (whose recursion is based on the QP tree). The algorithm in (3) is similar to our LW algorithm: it uses a notion of heavy and light join keys, it computes a superset of the join and uses hash tables to filter this set. It does have some key technical differences: the structure of the recursion is different and the handling of heavy/light join keys is more general. To make this section self-contained, we repeat some terminology and notation. For each tuple t on attribute set A, we will write t as t A to emphasize the support of t: t A = (t a ) a∈A . Consider any relation R with attribute set S . Let A ⊂ S and t A be a fixed tuple. Then, π A (R) denotes the projection of R on to attributes in A and,</p><formula xml:id="formula_28">R[t A ] := π S \A (R {t A }) = {t S \A | (t A , t S \A ) ∈ R}.</formula><p>In particular, R[t ∅ ] = R. There is a complete worked example of our algorithm in the full version of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Step (1): Build a query plan tree</head><p>Given a query H = (V, E), fix an arbitrary order e 1 , e 2 , . . . , e m of all the hyperedges in E. We construct a labeled binary tree (T , lc, rc) where lc (resp. rc) maps an internal node to their left child (resp. right child) and to a special constant nil if no such child exists. Each node x ∈ T is equipped with a pair of functions label(x) ∈ [m] and univ(x) ⊆ V. Very roughly, each node x and the sub-tree below it forms the "skeleton" of a sub-problem. There will be many sub-problems that correspond to each skeleton. The value label(x) points to an "anchor" relation for the sub-problem and univ(x) is the set of attributes that the sub-problem is joining on. The anchor relation divides the universe univ(x) into two parts to further sub-divide the recursion structure. rc(u) ← build-tree(U ∩ e k , k -1) 7: return u Algorithm 3 builds the query plan tree T . Note that line 5 and 6 will not be executed if U ⊆ e i , ∀i ∈ [k], in which case u is a leaf node. When u is not a leaf node, if U ⊆ e k then u will not have a left child (lc(u) = nil). If e i ∩ U ∩ e k = ∅ for all i ∈ [k -1] then u will not have a right child (rc(u) = nil). The running time for this pre-processing step is O(m 2 n). Figure <ref type="figure" target="#fig_4">1</ref> shows a query plan tree produced by Algorithm 3 on an example query.</p><p>From T , we compute a total order on V in two steps. First, we define a partial order of all attributes by traversing the tree T in post-order. If a node u is visited before a node v, then all elements of univ(u) precede elements univ(v) \ univ(u) in this partial order. Second, we take an arbitrary linear extension of this partial order and call it the total order. A complete pseudocode listing of this routine can be found in the full version of this paper, along with a few properties of the total order. In the example of Figure <ref type="figure" target="#fig_4">1</ref>, the total order is 1, 4, 2, 5, 3, 6. </p><formula xml:id="formula_29">q = R 1 (A 1 , A 2 , A 4 , A 5 ) R 2 (A 1 , A 3 , A 4 , A 6 ) R 3 (A 1 , A 2 , A 3 ) R 4 (A 2 , A 4 , A 6 ) R 5 (A 3 , A 5 , A 6 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Step (2): Build a Family of Indexes</head><p>We describe the form of the hash tables constructed by our algorithm. Each hash table is described by a triple (i, K, Ā) where i ∈ [m], K ⊆ e i is the search key, and Ā ⊆ e i \ K are the value attributes. For each such triple, our algorithm builds three hash tables that map hash keys t ∈ D K to one of three data types below. (HT3) A hash table that returns all tuples u ∈ π Ā(R e [t]) in time linear in the output size (if the output is not empty).</p><p>The hash tables for relation R e can be built in total time O(N e ). We denote this hash table by HTw(i, K, Ā) for w ∈ {1, 2, 3}. We abuse notation slightly and write HTw(i, U, Ā) for w ∈ {1, 2, 3} when U \ e i ∅ by defining HTw(i, U, Ā) = HTw(i, U ∩ e i , ( Ā \ U) ∩ e i ).</p><p>We will describe later how the total order allows us to reduce the total number hash indices down to O(n 2 m). We will only need to build 3 hash tables for every triple (i, K, Ā) such that K precede Ā in the total order. Thus, for R 4 in Figure <ref type="figure" target="#fig_4">1</ref> we need to build at most 21 indexes, i.e., three indexes for each of the following nine different key pairs: [(), (A 4 )], [(), (A 4 , A 2 )], [(), (A 4 , A 2 , A 6 )], [(A 4 ), (A 2 )], [(A 4 ), (A 2 , A 6 )], [(A 4 , A 2 ), (A 6 )], and [(A 4 , A 2 , A 6 ), ()]. (It is less than 21 because some indices are trivial or not defined, e.g. HT1 when t = ().) We group the pairs by brackets to make them easier to visually parse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Step (3): Compute the Join Recursively</head><p>We are ready to present the heart of Algorithm 2 which computes the join recursively in a nested-loop-like fashion. The input to the algorithm consists of the hypergraph H = (V, E) with |V| = n, |E| = m, and a point x = (x e ) e∈E in the fractional cover polytope e:i∈e x e ≥ 1, for any i ∈ V x e ≥ 0, for any e ∈ E Throughout this section, we denote the final output by J which is defined to be J = e∈E R e .</p><p>The crux of Algorithm 2 is a procedure called Recursive-Join (Procedure 4) that takes as inputs three arguments: (1) a node u from the QP-tree T whose label is k for some k ∈ [m]. (2) A tuple t S ∈ D S where S is the set of all attributes preceding univ(u) in the total order, and (3) a fractional cover solution y E k = (y e 1 , . . . , y e k ) of the hypergraph instance (univ(u), E k ). <ref type="bibr">(</ref> return Ret 10: if lc(u) = nil then // u is not a leaf node of T 11:</p><p>L ← {t S } 12:</p><p>// note that L ∅ and t S could be nil (when S = ∅) 13: else 14:</p><p>L ← Recursive-Join(lc(u), (y 1 , . . . , y k-1 ), t S ) 15:</p><formula xml:id="formula_30">W ← U \ e k , W -← e k ∩ U 16: if W -= ∅ then 17: return L 18: for each tuple t S ∪W = (t S , t W ) ∈ L do 19:</formula><p>if y e k ≥ 1 then 20:</p><p>go to line 27</p><formula xml:id="formula_31">21: if        k-1 i=1 |π e i ∩W -(R e i [t (S ∪W)∩e i ])| ye i 1-ye k &lt; |π W -(R e k [t S ∩e k ])|        then 22: Z ← Recursive-Join rc(u), ye i 1-ye k k-1 i=1 , t S ∪W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>23:</head><p>for each tuple (t</p><formula xml:id="formula_32">S , t W , t W -) ∈ Z do 24: if t W -∈ π W -(R e k [t S ∩e k ]) then 25: Ret ← Ret ∪ {(t S , t W , t W -)} 26: else 27:</formula><p>for</p><formula xml:id="formula_33">each tuple t W -∈ π W -(R e k [t S ∩e k ]) do 28: if t e i ∩W -∈ π e i ∩W -(R e i [t (S ∪W)∩e i ])</formula><p>for all e i such that i &lt; k and e i ∩ W -∅ then 29:</p><p>Ret ← Ret ∪ {(t S , t W , t W -)} 30: return Ret we only take the restrictions of hyperedges in E k onto the universe univ(u)). More precisely, y E k is a point in the following polytope: The goal of Recursive-Join is to compute a superset of the relation {t S } × π univ(u) (J[t S ]), i.e., a superset of the output tuples that start with t S on the attributes S ∪ univ(u). This intermediate output is analogous to the set C in Algorithm 1 for LW instances. A second similarity to Algorithm 1 is that our algorithm makes a choice per tuple based on the output's estimated size.</p><p>Theorem 3.2 is a special case of the following lemma where we set u to be the root of the QP-tree T , y = x, and S = ∅ (t S = nil). Finally, we observe that we need only O(n 2 ) number of hash indices per input relation, which completes the proof. (For the sake of presentation, we use the convention that when U ∩ e i = ∅ we set |π U∩e i (R e i [t S ∩e i ])| = 1 so that the factor does not contribute anything to the product.) (b) Furthermore, the procedure runs in time O(mn • B(u, y, t S )).</p><p>The lemma is proved by induction on the height of the sub-tree of T rooted at u. We include a full formal proof in the full version of this paper, but give the main ideas here.</p><p>Base Case. In the base case, the node u is a leaf, and univ(u</p><formula xml:id="formula_34">) ⊆ e i , ∀i ∈ [label(u)]. Observe that min i=1,...,k |π U (R e i [t S ])| ≤ k i=1 |π U (R e i [t S ])| y i = B(u, y, t S ).</formula><p>since k i=1 y i ≥ 1 (y is a fractional cover solution). Because the left-hand-side of the above inequality is clearly an upper bound on the number of output tuples, so is the right-hand-side. Hence, (a) holds. To control the running time and prove (b), a nestedloop strategy works: we first find the j that achieves the left-hand side of the inequality, i.e., for which π U (R e j [t S ]) is smallest among j ∈ [k]. To find this minimum, we probe HT2(i, S , U) with search key t S for i ∈ [k] (i.e., once for each relation). Since u is a leaf node, univ(u) ⊆ e i for each i ∈ [k] and hence U = U ∩ e i . Thus, we can query HT3( j, S , U) to find all tuples in relation π</p><formula xml:id="formula_35">U (R e j [t S ]) in time O(|π U (R e j [t])|). Then, for each such tuple v ∈ π U (R e j [t]</formula><p>), and for each relation R e i with i ∈ [k] \ { j} we probe into R e i with HT1(i, S , U); the tuple v is returned iff this probe returns true for all i ∈ [k] \ { j}. This procedure takes O(kn + kn|π U (R e j [t S ])|) where j is the minimum as above.</p><p>Induction Step. In this case, u is an internal node. The key challenge is that we have to make a cost-based decision about which lower subproblems to solve. The interesting case is when there are both a left-and a right-child problem. We recursively solve the left subproblem, from which we get back a relation on S ∪ U \ e k (each of whose tuples has values t S on attributes S ), which we then store in a variable L (formally, L ⊇ {t S } × π U\e k (J[t S ])). For example, consider the highlighted node in Figure <ref type="figure" target="#fig_8">1(b</ref>). We will refer to this node throughout this section as our example. Here, S = {1} and so we have a fixed tuple t S as input. The left-subproblem is the leaf that computes the tuples in {t S } × π {4} (J[t S ]), which have support {1, 4}.</p><p>Next, for each tuple t = t S ∪(U\e k ) ∈ L, we will make a decision on whether to solve an associated "right subproblem." There are |L| such subproblems and thus |L| decisions to make. Each decision is based on our estimation of the running time if we were to solve the subproblem. The run-time estimation is the AGM's bound on the output size of the sub-problem. To obtain the estimation, we define for each right subproblem a fractional cover solution. The relation R e k is used as an "anchor" for the entire process.</p><p>Specifically, we construct a hypergraph (univ(rc(u)), E k-1 ) with an associated fractional cover y E k-1 where y e i = y e i /(1y e k ) for i ∈ [k -1]. (When y e k ≥ 1 we will not solve this subproblem and directly take option (1) below.) For each t ∈ L, the input relation sizes for this sub-problem are</p><formula xml:id="formula_36">|π e i ∩U∩e k (R e i [t])| for i ∈ [k -1].</formula><p>For each t S ∪W = (t S , t W ) ∈ L, where W = U \ e k , our algorithm considers two options, and we use the estimated run-time of the projected subproblem to choose between these options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Option (1) Our algorithm loops over each tuple in π</head><formula xml:id="formula_37">U∩e k (R e k [t S ∩e k ])</formula><p>and filters it against all projections that are below it (lines <ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref>. In this case our running time is O(|π U∩e k (R e k )[t S ∩e k ]|). In our running example, given the tuple t {1,4} = (t 1 , t 4 ) ∈ L, we would loop over each tuple</p><formula xml:id="formula_38">t {2} = (t 2 ) ∈ π {2} (R 3 [t 1 ]).</formula><p>For each such tuple, we add (t 1 , t 4 , t 2 ) to the output Ret if</p><formula xml:id="formula_39">t 2 ∈ π {2} (R 1 [(t 1 , t 4 )]</formula><p>). This check can be done by probing R 1 using HT1(1, (A 1 , A 4 , A 2 ), ()).</p><p>Option (2) Our algorithm solves the right subproblem recursively and filters the result with π U∩e k (R e k [t S ∩e k ]) (lines 22-25). In our running example, given (t 1 , t 4 ) ∈ L the right subproblem will compute those tuples (t 2 ) in π {2} (R 1 [t {1,4} ]) and then filter them with HT1(3, (A 1 , A 2 ), ()). The important property of option (2) is that its running time does not depend on</p><formula xml:id="formula_40">|π U∩e k (R e k [t S ∩e k ])|.</formula><p>In particular, option (2)'s running time only depends on the output size of the right subproblem.</p><p>To decide between these two options, we compare the following two quantities:</p><formula xml:id="formula_41">LHS = |π U∩e k (R e k [t S ∩e k ])| versus RHS = k-1 i=1 π e i ∩U∩e k (R e i [t (S ∪W)∩e i ]</formula><p>)</p><formula xml:id="formula_42">y e i</formula><p>We choose option (1) if either y e k ≥ 1 or the LHS is less than the RHS and option (2) otherwise. Observe that we can compute both quantities given our indices in time proportional to O(kn). Our overall running time is proportional to the minimum of these two quantities (plus the inconsequential term O(kn)). Summing over all tuples t ∈ L the minimum of the above two quantities and "unroll" the sum by applying a generalized Hölder's inequality many times we can then prove both the output size and the running time.</p><p>Used Search Keys. Finally, we need to understand which search keys are used in the hash table. Observe that whenever an attribute v is used in a search key (e.g., HTw(i, S , U) for w ∈ {1, 2, 3}), all attributes that come before v in the total order and are in e i are bound. Thus, if e i = (v i 1 , . . . , v i |e i | ) and the attributes are ordered as above, then the search key and the returned keys is always a prefix of v i 1 , . . . , v i k . Hence, we only need to have 3n e∈E |e| indices. In the full version of this paper, we describe a slightly more complex data structure that combines all hash tables for one relation into a single "search tree" structure. The search trees have the advantage that their building time is deterministic (unlike a typically perfect hashing scheme which has a constant worst-case lookup time but only expected linear building time). However, the search trees necessitate a log-factor blow up in the total run time of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Limits of Standard Approaches</head><p>For a given join query q, we describe a sufficient syntactic condition for q so that when computed by any join-project plan is asymptotically slower than the worst-case bound. Our algorithm runs within this bound, and so for such q there is an asymptotic running-time gap.</p><p>Recall that an LW instance of the OJ problem is a join query q represented by the hypergraph (V, E), where V = [n], and E =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[n]</head><p>n-1 for some integer n ≥ 2. Our main result in this section is the following lemma. <ref type="foot" target="#foot_2">5</ref>Lemma 3.6. Let n ≥ 2 be an arbitrary integer. Given any LWquery q represented by a hypergraph ([n], [n]  n-1 ), and any positive integer N ≥ 2, there exist relations R i , i ∈</p><formula xml:id="formula_43">[n], such that |R i | = N, ∀i ∈ [n]</formula><p>, the attribute set for R i is [n] \ {i}, and that any joinproject plan for q on these relations runs in time Ω(N 2 /n 2 ).</p><p>Before proving the lemma, we note that both the traditional jointree algorithm and AGM's algorithm are join-project plans, and thus their running times are asymptotically worse than the best AGM bound for this instance which is 1) . On the other hand, both Algorithm 1 and Algorithm 2 take O(N 1+1/(n-1) )-time as we have analyzed. In fact, for Algorithm 2, we are able to demonstrate a stronger result: its run-time on this instance is O(n 2 N) which is better than what we can analyze for a general instance of this type. In particular, the run-time gap between Algorithm 2 and AGM's algorithm is Ω(N) for constant n.</p><formula xml:id="formula_44">| n i=1 R i | ≤ n i=1 |R i | 1/(n-1) = N 1+1/(n-</formula><p>Proof of Lemma 3.6. In the instances below the domain of any attribute will be D = {0, 1, . . . , (N -1)/(n -1)} For the sake of clarity, we ignore the integrality issue. For any i ∈ [n], let R i be the set of all tuples in D [n]-{i} each of which has at most one nonzero value. Then, it is not hard to see that For an arbitrary join-project plan starting from the simple relations R i , we eventually must join two relations whose attribute sets are not contained in one another and this step alone requires Ω(N 2 /n 2 ) run time. The first thing Algorithm 2 does is that it computes the join L n = n-1 i=1 π {n} (R i ), in time O(nN). Note that L n = D, the domain. Next, Algorithm 2 goes through each value a ∈ L n and decides whether to solve a subproblem or not. First, consider the case a &gt; 0. Here Algorithm 2 estimates a bound for the join n-1 j=1 π</p><formula xml:id="formula_45">|R i | = (n -1)[(N - 1)/(n -1) + 1] -(n -2) = N, for all i ∈ [n]; and, | n i=1 R i | = n[(N -1)/(n -1) + 1] -(n -1) = N + (N -1)/(n -1). A relation R on attribute set Ā ⊆ [n] is called simple if R</formula><formula xml:id="formula_46">[n-1] (R j [a]</formula><p>). The estimate is 1 because |π [n-1] (R j [a])| = 1 for all a &gt; 0. Hence, the algorithm will recursively compute this join which takes time O(n 2 ) and filter the result against R n . Overall, solving the sub problems for a &gt; 0 takes O(n 2 N) time. Second, consider the case when a = 0. In this case |π</p><formula xml:id="formula_47">[n-1] (R j [0])| = (n-2)N-1 (n-1) . The subproblem's estimated size bound is n-1 i=1 |π [n-1] (R j [0])| 1/(n-1) 1-1/(n-1) = (n -2)N -1 (n -1) (n-1)/(n-2)</formula><p>&gt; N</p><p>if N ≥ 4 and n ≥ 4. Hence, in this case R n will be filtered against the π [n-1] (R j [0]), which takes O(n 2 N) time.</p><p>Extending beyond LW instances. Using the above results, we give a sufficient condition for when there exist a family of instances I = I 1 , . . . , I N , . . . , such that on instance I N every binary join strategy takes time at least Ω(N 2 ), but our algorithm takes o(N 2 ). Given a hypergraph H = (V, E). We first define some notation. Fix U ⊆ V then call an attribute v ∈ V \ U U-relevant if for all e such that v ∈ e then e ∩ U ∅; call v U-troublesome if for all e ∈ E, if v ∈ e then U ⊆ e. Now we can state our result: Lemma 3.8. Given a join query H = (V, E) and some U ⊆ V where |U| ≥ 2, then if there exists F ⊆ E such that |F| = |U| that satisfies the following three properties: (1) each u ∈ U occurs in exactly |U| -1 elements in F, (2) each v ∈ V that is U-relevant appears in at least |U| -1 edges in F, <ref type="bibr" target="#b3">(3)</ref> there are no U-troublesome attributes. Then, there is some family of instances I such that (a) computing the join query represented by H with a join tree takes time Ω(N 2 /|U| 2 ) while (b) the algorithm from Section 3.2 takes time O(N 1+1/(|U|-1) ).</p><p>Given a (U, F) as in the lemma, the idea is to simply to set all those edges in f ∈ F to be the instances from Lemma 3.6 and extend all attributes with a single value, say c 0 . Since there are no U-troublesome attributes, to construct the result set at least one of the relations in F must be joined. Since any pair F must take time Ω(N 2 /|U| 2 ) by the above construction, this establishes (a). To establish (b), we need to describe a particular feasible solution to the cover LP whose objective value is N 1+1/(|U|-1) , implying that the running time of our proposed algorithm is upper bounded by this value. To do this, we first observe that any attribute not in U takes the value only c 0 . Then, we observe that any node v ∈ V that is not U-relevant is covered by some edge e whose size is exactly 1 (and so we can set x e = 1). Thus, we may assume that all nodes are U-relevant. Then, observe that all relevant attributes can be set by the cover x e = 1/(|U| -1) for e ∈ F. This is a feasible solution to the LP and establishes our claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXTENSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Combined Complexity</head><p>Given that our algorithms are data optimal for worst-case inputs it is tempting to wonder if one can obtain an join algorithm whose run time is both query and data optimal in the worst-case. We show that in the special case when each input relation has arity at most 2 we can attain a data-optimal algorithm that is simpler than Algorithm 2 with an asymptotically better query complexity.</p><p>Further, given promising results in the worst case, it is natural to wonder whether or not one can obtain a join algorithm whose run time is polynomial in both the size of the query as well as the size of the output. More precisely, given a join query q and an instance I, can one compute the result of query q on instance I in time poly(|q|, |q(I)|, |I|). Unfortunately, this is not possible unless NP = RP. We briefly present a proof of this fact below.</p><p>Each relation has at most 2 attributes. As is mentioned in the introduction, our algorithm in Theorem 3.2 not only has better data complexity than AGM's algorithm (in fact we showed our algorithm has optimal worst-case data complexity), it has better query complexity. In this section, we show that for the special case when the join query q is on relations with at most two attributes (i.e., the corresponding hypergraph H is a graph), we can obtain better query complexity compared to the algorithm in Theorem 3.2 (while retaining the same (optimal) data complexity).</p><p>Without loss of generality, we can assume that each relation contains exactly 2 attributes because a 1-attribute relation R e needs to have x e = 1 in the corresponding LP and thus, contributes a separate factor N e to the final product. Thus, R e can be joined with the rest of the query with any join algorithm (including the naive Cartesian product based algorithm). In this case, the hypergraph H is a graph which can be assumed to be simple. We assume that all relations are indexed in advanced, which takes O( e N e ) time.</p><p>In what follows we will not include this preprocessing time in the analysis.</p><p>We first state a lemma for the case when H is a cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.1 (Cycle Lemma). If H is a cycle, then e∈E R e can be computed in time O(m</head><p>e∈H N e ).</p><p>The proof of the lemma shows that we can reduce the computation of the case when H is a cycle to our previous algorithm for Loomis-Whitney instances with n = 3.</p><p>With the help of Lemma 4.1, we can now derive a solution for the case when H is an arbitrary graph. Consider any basic feasible solution x = (x e ) e∈E of the fractional cover polyhedron e:v∈e x e ≥ 1, for any v ∈ V x e ≥ 0, for any e ∈ E.</p><p>It is known that x is half-integral, i.e., x e ∈ {0, 1/2, 1} for all e ∈ E (see Schrijver's book <ref type="bibr" target="#b34">[34]</ref>, Theorem 30.10). However, we will also need a graph structure associated with the half-integral solution; hence, we adapt a known proof of the half-integrality property with a slightly more specific analysis <ref type="bibr" target="#b34">[34]</ref>. Lemma 4.2. For any basic feasible solution x = (x e ) e∈E of the fractional cover polyhedron above, x e ∈ {0, 1/2, 1} for all e ∈ E. Furthermore, the collection of edges e for which x e = 1 is a union S of stars. And, the collection of edges e for which x e = 1/2 form a set C of vertex-disjoint odd-length cycles that are also vertex disjoint from the union S of stars. Now, let x * be an optimal basic feasible solution to the following linear program. Consequently, we can apply Lemma 4.1 to each cycle C ∈ C and take a cross product of all the resulting relations with the relations R e for e ∈ S . We summarize the above discussion in the following theorem. Impossibility of Instance Optimality. We use the standard reduction of 3SAT to conjunctive queries but with two simple specializations: (i) We reduce from the 3UniqueSAT, where the input formula is either unsatisfiable or has exactly one satisfying assignment, and (ii) q is a full join query instead of a general conjunctive query. It is known that 3UniqueSAT cannot be solved in deterministic polynomial time unless NP = RP <ref type="bibr" target="#b37">[37]</ref>. We sketch the reduction here. Let φ = C 1 ∧ C 2 ∧ . . . C m be a 3UniqueSAT CNF formula on n variables a 1 , . . . , a n . (W.l.o.g. assume that a clause does not contain both a variable and its negation.) For each clause C j for j ∈ [m], create a relation R j on the variables that occur in C j . The query q is j∈[m] R j . Now define the database I as follows: for each j ∈ [m], R I j contains the seven assignments to the variables in C j that makes it true. Note that q(I) contains all the satisfying assignments for φ: in other words, q(I) has one element if φ is satisfiable otherwise q(I) = ∅. In other words, we have |q(I)| ≤ 1, |q| = O(m + n) and |I| = O(m). Thus an instance optimal algorithm with time complexity poly(|q|, |q(I)|, |I|) for q would be able to determine if φ is satisfiable or not in time poly(n, m), which would imply NP = RP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relaxed Joins</head><p>We observe that our algorithm can actually evaluate a relaxed notion of join queries. Say we are given a query q represented by a hypergraph H = (V, E) where V = [n] and |E| = m. The m input relations are R e , e ∈ E. We are also given a "relaxation" number 0 ≤ r ≤ m. Our goal is to output all tuples that agree with at least mr input relations. In other words, we want to compute ∪ S ⊆E,|S |≥m-r e∈S R e . However, we need to modify the problem to avoid the case that the set of attributes of relations indexed by S does not cover all the attributes in the universe V. Towards this end, define the set</p><formula xml:id="formula_48">C(q, r) =        S ⊆ E | |S | ≥ m -r and e∈S e = V        .</formula><p>With the notations established above, we are now ready to define the relaxed join problem. Definition 4.4 (Relaxed join problem). Given a query q represented by the hypergraph H = (V = [n], E), and an integer 0 ≤ r ≤ m, evaluate q r de f = S ∈C(q,r)</p><formula xml:id="formula_49">( e∈S R e ) .</formula><p>Before we proceed, we first make the following simple observation: given any two sets S , T ∈ C(q, r) such that S ⊆ T , we have e∈T R e ⊆ e∈S R e . This means in the relaxed join problem we only need to consider subsets of relations that are not contained in any other subset. In particular, define Ĉ(q, r) ⊆ C(q, r) to be the largest subset of C(q, r) such that for any S T ∈ Ĉ(q, r) neither S ⊂ T nor T ⊂ S . We only need to evaluate q r = S ∈ Ĉ(q,r) ( e∈S R e ) .</p><p>Given an S ∈ Ĉ(q, r), let LPOpt(S ) denote the size bound given by the AGM fractional cover inequality (2) on the join query represented by the hypergraph (V, S ), so that LPOpt(S ) = e∈S |R e | x * e where x * S = (x * e ) e∈S is an optimal solution to the following linear program called LP(S ):</p><formula xml:id="formula_50">min e∈S (log |R e |) • x e subject to e∈S :v∈e x e ≥ 1 foranyv ∈ V (4)</formula><p>x e ≥ 0 f o ra n ye ∈ S .</p><p>Upper bounds. We start with a straightforward upper bound.</p><p>Proposition 4.5. Let q be a join query on m relations and let 0 ≤ r ≤ m be an integer. Then given sizes of the input relations, the number of output tuples for query q r is upper bounded by S ∈ Ĉ(q,r) LPOpt(S ). Further, Algorithm 2 evaluates q r with data complexity linear in the bound above. The next natural question is to determine how good the upper bound is. Before we answer the question, we prove a stronger upper bound.</p><p>Given a subset of hyperedges S ⊆ E that "covers" V, i.e. ∪ e∈S e = V, let BFS(S ) ⊆ S be the subset of hyperedges in S that gets a positive x * e value in an optimal basic feasible solution to the linear program LP(S ) defined in <ref type="bibr" target="#b4">(4)</ref>. (If there are multiple such solutions, pick any one in a consistent manner.) Call two subsets S , T ⊆ E bfsequivalent if BFS(S ) = BFS(T ). Finally, define C * (q, r) ⊆ Ĉ(q, r) as the collection of sets from Ĉ(q, r) which contains exactly one arbitrary representative from each bfs-equivalence class. Theorem 4.6. Let q be a join query represented by H = (V, E), and let 0 ≤ r ≤ m be an integer. The number of output tuples of q r is upper bounded by S ∈C * (q,r) LPOpt(S ). Further, the query q r can be evaluated in time O         S ∈C * (q,r) mn • LPOpt(S ) + poly(n, m)         plus the time needed to compute C * (q, r) from q.</p><p>Note that since C * (q, r) ⊆ Ĉ(q, r), the bound in Theorem 4.6 is no worse than that in Proposition 4.5. We will show later that the bound in Theorem 4.6 is indeed tight.</p><p>We defer the proof of Theorem 4.6 to the full version and mention the main idea here. Let S S ∈ Ĉ(q, r) be two different sets of hyperedges with the following property. Define T de f = BFS(S ) = BFS(S ) and let x * T = (x * i ) i∈T be the projection of the corresponding optimal basic feasible solution to the (V, S ) and the (V, S ) problems projected down to T . (The two projections result in the same vector x * T .) The outputs of the joins on S and on S are both subsets of the output of the join on T . We can simply run Algorithm 2 on inputs (V, T ) and x * T , then prune the output against relations R e with e ∈ S \ T or S \ T . In particular, we only need to compute e∈T R e once for both S and S .</p><p>Lower bound. We now show that the bound in Theorem 4.6 is (almost) tight for some query and some database instance I.</p><p>We first define the query q. The hypergraph is H = (V = Finally, we argue that C * (q, r) = {{n + 1}, [n]}. Towards this end, consider any T ∈ Ĉ(q, r). Note that if (n + 1) T , we have T = [n] and since BFS(T ) = T (and we will see soon that for any other T ∈ Ĉ(q, r), we have BFS(T ) [n]), which implies that [n] ∈ C * (q, r). Now consider the case when (n + 1) ∈ T . Note that in this case T = {n + 1} ∪ T for some T ⊂ [n] such that |T | ≥ nr. Now note that all the relations in T cannot cover the n attributes but R n+1 by itself does include all the n attributes. This implies that BFS(T ) = {n + 1} in this case. This proves that {n + 1} is the other element in C * (q, r), as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>We establish optimal algorithms for the worst-case behavior of join algorithms. We also demonstrate that the join algorithms employed in RDBMSs do not achieve these optimal bounds. Moreover, we demonstrate families of instances where join-project algorithms are asymptotically worse by factors close to the size of the largest relation. It is interesting to ask similar questions for average case complexity. Our work offers a different way to approach join optimization rather than the traditional binary-join/dynamicprogramming-based approach. Thus, our immediate future work is to implement these ideas to see how they compare in real RDBMS settings to the algorithms in a modern RDBMS.</p><p>Another interesting direction is to extend these results to a larger classes of queries and to database schemata that have constraints. We include in the full version some preliminary results on full conjunctive queries and simple functional dependencies (FDs). Not surprisingly, using dependency information one can obtain tighter bounds compared to the (FD-unaware) fractional cover technique.</p><p>There are potentially interesting connections between our work and several inter-related topics. We algorithmically prove that the AGM inequality is equivalent to the BT inequality; in turn both inequalities are essentially equivalent to Shearer's entropy inequality. There are known combinatorial interpretations of entropy inequalities (which include Shearer's as a special case); for example, Alon et al. <ref type="bibr" target="#b3">[3]</ref> derived some such connections using a notion of "sections" similar to what we used in this paper. An analogous partitioning procedure is used by Marx <ref type="bibr" target="#b29">[29]</ref> to compute joins by relating the number of solutions to submodular functions. Query (1) is essentially equivalent to the problem of enumerating all triangles in a tri-partite graph, which can be solved in time O(N 3/2 ) <ref type="bibr" target="#b4">[4]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>by |q(I)|, in terms of |R|, |S |, and |T |. For simplicity, let us consider the case when |R| = |S | = |T | = N. A trivial bound is |q(I)| ≤ N 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>in turn, this gives sup I∈I( N) |q(I)| ≤ N 3/2 (recall I( N) = {I : |R I e | = N e for e ∈ E}).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e:v∈e d e = d, for any v ∈ V. Now, create d e copies of each relation R e . Call the new relations R e . We obtain a new hypergraph H = (V, E ) where every attribute v occurs in exactly d hyperedges. This is precisely the Bollóbas-Thomason's setting of Theorem 2.2. Hence, the size of the join is bounded above by e∈E |R e | 1/d = e∈E |R e | de/d = e∈E |R e | xe .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>establishing the running time of our algorithm. One can check that if the relations are of different cardinalities, then we can still use the same algorithm; moreover, by setting τ = |R||T | |S | , we achieve a running time of O( √ |R||S ||T | + |R| + |S | + |T |).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Algorithm for Loomis-Whitney Instances 1: An LW instance: R e for e ∈ V |V|-1 and N e = |R e |. 2: P = e∈E N 1/(n-1) e (the size bound from LW inequality) 3: u ← root(T ); (C(u), D(u)) ← LW(u) 4: "Prune" C(u) and return LW(x) : x ∈ T returns (C, D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 3 . 2 .Remark 3 . 3 .</head><label>3233</label><figDesc>Let H = (V, E) be a hypergraph representing a natural join query. Let n = |V| and m = |E|. Let x = (x e ) e∈E be an arbitrary point in the fractional cover polytope e:v∈e x e ≥ 1, for any v ∈ V x e ≥ 0, for any e ∈ E For each e ∈ E, let R e be a relation of size N e = |R e | (number of tuples in the relation). Then, (a) The join e∈E R e has size (number of tuples) bounded by | e∈E R e | ≤ e∈E N xe e .(b) Furthermore, the join e∈E R e can be computed in time O In the running time above, m 2 n is the query preprocessing time, n2  e∈E N e is the data preprocessing time, and mn e∈E N xe e is the query evaluation time. If all relations in the database are indexed in advance to satisfy three conditions (HT1), (HT2), and (HT3) from Section 3.2.3, then we can remove the term n2  e∈E N e from the running time. To make the bound tight, the fractional cover solution x should be the best fractional cover in terms of the linear objective e (log N e )•x e . The data-preprocessing time of O(n 2e N e ) is for a single known query. If we were to index all relations in advance without knowing which queries to be evaluated, then the advance-indexing takes O(n • n! e N e )-time. This price is paid once, up-front, for an arbitrary number of future queries.Before turning to our algorithm and proof of this theorem, we observe that a consequence of this theorem is the following algorithmic version of the discrete version of BT inequality. Corollary 3.4. Let S ⊂ Z n be a finite set of n-dimensional grid points. Let F be a collection of subsets of [n] in which every i ∈ [n] occurs in exactly d members of F . Let S F be the set of projections Z n → Z F of points in S onto the coordinates in F. Then, |S | d ≤ F∈F |S F |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 2</head><label>2</label><figDesc>Computing the join e∈E R e Input: Hypergraph H = (V, E), |V| = n, |E| = m Input: Fractional cover solution x = (x e ) e∈E Input: Relations R e , e ∈ E 1: Compute the query plan tree T , let u be T 's root node 2: Compute a total order of attributes 3: Compute a collection of hash indices for all relations 4: return Recursive-Join(u, x, nil)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 3</head><label>3</label><figDesc>Constructing the query plan tree T 1: Fix an arbitrary order e 1 , e 2 , . . . , e m of all the hyperedges in E.2: T ← build-tree(V, m) build-tree(U, k) 1: if e i ∩ U = ∅, ∀i ∈ [k] then 2:return nil 3: Create a node u with label(u) ← k and univ(u) = U 4: if k &gt; 1 and ∃i ∈ [k] such that U e i then 5: lc(u) ← build-tree(U \ e k , k -1) 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) A query q and (b) a sample QP tree for q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>e∈E k :i∈e y e ≥ 1, for any i ∈ univ(u) y e ≥ 0, for any e ∈ E k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma 3 . 5 .</head><label>35</label><figDesc>Consider a call Recursive-Join(u, y, t S ) to Procedure 4. Let k = label(u) and U = univ(u). Then, (a) The procedure outputs a relation Ret on attributes S ∪U with at most the following number of tuples B(u, y, t S ) := k i=1 |π U∩e i (R e i [t S ∩e i ])| y i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>is the set of all tuples in D Ā each of which has at most one non-zero value. Then, we observe the following properties. (a) The input relations R i are simple. (b) An arbitrary projection of a simple relation is simple. (c) Let S and T be any two simple relations on attribute sets ĀS and ĀT , respectively. If ĀS is contained in ĀT or vice versa, then S T is simple. If neither ĀS nor ĀT is contained in the other, then |S T | ≥ (1 + (N -1)/(n -1)) 2 = Ω(N 2 /n 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Finally, we analyzeLemma 3 . 7 .</head><label>37</label><figDesc>the run-time of Algorithm 2 directly on this instance without resorting to Theorem 3.2. On the collection of instances from the previous lemma, Algorithm 2 runs in time O(n 2 N).Proof. Without loss of generality, assume the hyperedge order Algorithm 2 considers is [n] -{1}, . . . , [n] -{n}. In this case, the universe of the left-child of the root of the QP-tree is {n}, and the universe of the right-child of the root is [n -1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>min e (log N e ) • x e s.t. e:v∈e x e ≥ 1, for any v ∈ V x e ≥ 0, for any e ∈ E.Then e∈E Nx * e e ≤ e∈E N xe e for any feasible fractional cover x. Let S be the set of edges on the stars and C be the collection of disjoint cycles as shown in the above lemma, applied to x * . Then,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Theorem 4 . 3 .</head><label>43</label><figDesc>When each relation has at most two attributes, we can compute the join e∈E R e in time O(m e∈E N xe e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>[n], E) where m = |E| = n + 1. The hyperedges are E = {e 1 , . . . , e n+1 } where e i = {i} for i ∈ [n] and e n+1 = [n]. The database instance I consists of relations R e , e ∈ E, all of which are of size N. For each i ∈ [n], R e i = [N]. And, R e n+1 = N i=1 {N + i} n . It is easy to check that for any r ≥ n, q r (I) is the set R e n+1 ∪ [N] n , i.e. |q r (I)| = N + N n . (For 0 &lt; r &lt; n, we have |q r (I)| = N n .) Next, we claim that for this query instance for any r &gt; 0, C * (q, r) = {{n + 1}, [n]}. Note that BFS({n + 1}) = {n + 1} and BFS([n]) = [n], which implies that LPOpt({n + 1}) = N and LPOpt([n]) = N n . This along with Theorem 4.6 implies that |q r (I)| ≤ N + N n , which proves the tightness of the size bound in Theorem 4.6 for (r ≥ n), as desired. (For 0 &lt; r &lt; n, the bound is almost tight.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>(</head><label></label><figDesc>HT1) A hash table that maps t to a Boolean that is true if t ∈ π</figDesc><table /><note><p><p><p><p>K </p>(R e ). Thus, we can decide for any fixed tuple t ∈ D</p>K whether t ∈ π K (R e ) in time O(| K|).</p>(HT2) A hash table that maps each t to |π Ā(R e [t])|, i.e., the number of tuples u ∈ π K∪ Ā(R e ) such that t K = u K . For a fixed t ∈ D K , this can be queried in time O(| K|).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Here, E k = {e 1 , . . . , e k } and Procedure 4 Recursive-Join(u, y, t S ) 1: Let U = univ(u), k = label(u) 2: Ret ← ∅ // Ret is the returned tuple set 3: if u is a leaf node of T then // note that U ⊆ e i , ∀i ≤ k 4: j ← argmin i∈[k] |π U (R e i [t S ∩e i ])| 5: // By convention, R e [nil] = R e and R e [t ∅ ] = R e 6: for each tuple t U ∈ π U (R e j [t S ∩e j ]) do 7: if t U ∈ π U (R e i [t S ∩e i ]), for all i ∈ [k] \ { j} then</figDesc><table><row><cell>8:</cell><cell>Ret ← Ret ∪ {(t S , t U )}</cell></row><row><cell>9:</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A full conjunctive query is a conjunctive query where every variable in the body appears in the head.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In an RDBMS, one computes information, e.g., indexes, offline that may obviate the need to read the entire input relations to produce the output. In a similar spirit, we can extend our results to evaluate any query q in time O(U), removing the term i N i by precomputing some indices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>We thank an anonymous PODS'12 referee for showing us that our example works for all join-project plans rather than just the AGM algorithm and arbitrary join-only algorithms.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>We thank Georg Gottlob for sending us a full version of his work <ref type="bibr" target="#b13">[13]</ref> and XuanLong Nguyen for introducing us to the Loomis-Whitney inequality. We thank the anonymous referees for many helpful comments that greatly improved the presentation of the paper. In particular, we thank a reviewer for pointing out the current proof (and statement) of Lemma 3.6 and an error in previous lower bound argument in Section 4.2. AR's work on this project is supported the NSF CAREER Award under CCF-0844796. CR's work on this project is generously supported by the NSF CAREER Award under IIS-1054009, the ONR under N000141210041, and gifts from Google, Greenplum, LogicBlox, and Oracle.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Foundations of Databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abiteboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vianu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tracking join and self-join sizes in limited storage</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="10" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Partitioning multi-dimensional sets in a small number of &quot;uniform&quot; parts</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Vereshchagin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Comb</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="144" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding and counting given length cycles</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Zwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="223" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Size bounds and query plans for relational joins</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atserias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="739" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eddies: Continuously adaptive query processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Avnur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proactive re-optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bizarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Projections of bodies and hereditary properties of hypergraphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thomason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. London Math. Soc</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Some intersection theorems for ordered sets and graphs</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frankl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Shearer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Combin. Theory Ser. A</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="37" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extended wavelets for multiple measures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deligiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Garofalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roussopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TODS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Query evaluation via tree-decompositions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Flum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="716" to="752" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficiently decodable 2 / 2 for each compressed sensing with tiny failure probability</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Porat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Strauss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-11">November 2011</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Size and treewidth bounds for conjunctive queries</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gottlob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypertree decompositions: A survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gottlob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarcello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MFCS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized hypertree decompositions: np-hardness and tractable variants</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gottlob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Miklós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schwentick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Query evaluation techniques for large databases</title>
		<author>
			<persName><forename type="first">G</forename><surname>Graefe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="170" />
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constraint solving via fractional edge covers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A superadditivity and submultiplicativity property for cardinalities of sumsets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gyarmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matolcsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Z</forename><surname>Ruzsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="174" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonnegative entropy measures of multivariate symmetric correlations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="156" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Ioannidis</surname></persName>
		</author>
		<title level="m">The history of histograms (abridged). In VLDB</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the propagation of errors in the size of join results</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christodoulakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Communication lower bounds for distributed-memory matrix multiplication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Irony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tiskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1017" to="1026" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimal Histograms with Quality Guarantees</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Poosala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sevcik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining Histograms and Parametric Curve Fitting for Feedback-Driven Query Result-size Estimation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Network coding: does the model need tuning?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="499" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An inequality related to the isoperimetric inequality</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Loomis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="961" to="962" />
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Probability on trees and networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lyons</surname></persName>
		</author>
		<ptr target="http://php.indiana.edu/rdlyons/prbtree/prbtree.html" />
		<imprint>
			<date type="published" when="2011-06">jun 2011</date>
		</imprint>
	</monogr>
	<note>with Yuval Peres url</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Consistently estimating the selectivity of conjuncts of predicates</title>
		<author>
			<persName><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="373" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tractable hypergraph properties for constraint satisfaction and conjunctive queries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="735" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Worst-case optimal join algorithms</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Porat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.1952</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>cs.DB</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Porat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<imprint>
			<publisher>Personal Communciation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable computation of acyclic joins</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved histograms for selectivity estimation of range predicates</title>
		<author>
			<persName><forename type="first">V</forename><surname>Poosala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Shekita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="294" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combinatorial optimization. Polyhedra and efficiency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms and Combinatorics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2003">2003</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Isomer: Consistent histogram construction using query feedback</title>
		<author>
			<persName><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lightweight graphical models for selectivity estimation without independence assumptions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tzoumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="852" to="863" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">NP is as easy as detecting unique solutions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="85" to="93" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Applications of range query theory to relational data base join and selection operations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Willard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Handling data skew in parallel joins in shared-nothing systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kostamaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
