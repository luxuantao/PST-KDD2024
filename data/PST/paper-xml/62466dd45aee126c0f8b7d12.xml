<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analyzing Wrap-Up Effects through an Information-Theoretic Lens</title>
				<funder ref="#_2kPd2Rz">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_ZrnuVwx">
					<orgName type="full">Swiss National Science Foundation</orgName>
					<orgName type="abbreviated">SNSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
							<email>clara.meister@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich University of Cambridge Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich University of Cambridge Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">Hikaru</forename><surname>Clark</surname></persName>
							<email>thclark@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich University of Cambridge Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<email>ryan.cotterell@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich University of Cambridge Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
							<email>rplevy@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich University of Cambridge Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Analyzing Wrap-Up Effects through an Information-Theoretic Lens</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Numerous analyses of reading time (RT) data have been implemented-all in an effort to better understand the cognitive processes driving reading comprehension. However, data measured on words at the end of a sentence-or even at the end of a clause-is often omitted due to the confounding factors introduced by so-called "wrap-up effects," which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these wrap-up effects is limited. In this work, we attempt to learn more about these processes by examining the relationship between wrap-up effects and information-theoretic quantities, such as word and context surprisals. We find that the distribution of information in prior contexts is often predictive of sentence-and clause-final RTs (while not of sentence-medial RTs). This lends support to several prior hypotheses about the processes involved in wrap-up effects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reading puts the unfolding of linguistic input in the hands-or, really, the eyes-of the reader. Consequently, it presents a unique opportunity to gain a better understanding of how humans comprehend written language. The rate at which humans choose to read text (and process its information) should be determined by their goal of understanding it. Ergo, examining where a reader spends their time should help us to understand the nature of language comprehension processes themselves. Indeed, studies analyzing reading times have been employed to explore a number of psycholinguistic theories (e.g., <ref type="bibr" target="#b31">Smith and Levy, 2013;</ref><ref type="bibr" target="#b9">Futrell et al., 2020;</ref><ref type="bibr" target="#b33">Van Schijndel and Linzen, 2021)</ref>.</p><p>One behavior revealed by such studies is the tendency for humans to spend more time<ref type="foot" target="#foot_0">1</ref> on the last word of a sentence or clause. While the existence of such wrap-up effects is well-known <ref type="bibr" target="#b19">(Just et al., 1982;</ref><ref type="bibr" target="#b15">Hill and Murray, 2000;</ref><ref type="bibr" target="#b28">Rayner et al., 2000;</ref><ref type="bibr" target="#b2">Camblin et al., 2007)</ref>, the cognitive processes giving rise to them are still not fully understood. This is likely (at least in part) due to the dearth of analyses targeting naturalistic sentence-final reading behavior. First, most studies of online processing omit data from these words to explicitly control for the confounding factors wrap-up effects introduce (e.g., <ref type="bibr" target="#b31">Smith and Levy, 2013;</ref><ref type="bibr" target="#b11">Goodkind and Bicknell, 2018)</ref>. Second, the few studies on wrap-up effects rely on small datasets, none of which analyze naturalistic text <ref type="bibr" target="#b18">(Just and Carpenter, 1980;</ref><ref type="bibr" target="#b28">Rayner et al., 2000;</ref><ref type="bibr" target="#b21">Kuperberg et al., 2011)</ref>. This work addresses this gap, using several large corpora of reading time data. Specifically, we study whether informationtheoretic concepts (such as surprisal) provide insights into the cognitive processes that occur at a sentence's boundary. Notedly, informationtheoretic approaches have been proven effective for analyzing sentence-medial reading time behavior.</p><p>We follow the long line of work that has connected information-theoretic measures and psychometric data <ref type="bibr" target="#b8">(Frank et al., 2015;</ref><ref type="bibr" target="#b11">Goodkind and Bicknell, 2018;</ref><ref type="bibr" target="#b34">Wilcox et al., 2020;</ref><ref type="bibr">Meister et al., 2021 , inter alia)</ref>, employing similar methods to build models of sentence-and clause-final RTs. Using surprisal estimates from state-of-the-art language models, we search for a link between wrapup effects and the information content within a sentence. We find that the distribution of surprisals of prior context is often predictive of sentence-and clause-final reading times (RTs), while not adding significant predictive power to models of sentencemedial RTs. This result suggests that the nature of cognitive processes involved during the reading of these boundary words may indeed be different than those at other positions. Such findings lend support to several prior hypotheses regarding which processes may underlie wrap-up effects (e.g., the resolution of prior ambiguities), while providing evidence against other speculations (e.g., that the time spent at sentence boundaries can be quantified with a constant factor, independent of the processing difficulty of the text itself).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Process of Reading</head><p>Decades of research on reading behavior have improved our understanding of the cognitive processes involved in reading comprehension <ref type="bibr" target="#b18">(Just and Carpenter, 1980;</ref><ref type="bibr">Rayner and Clifton, 2009 , inter alia)</ref>. Here, we will briefly describe overarching themes that are relevant for understanding wrap-up effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Incrementality and its Implications</head><p>It is widely accepted that language processing is incremental in nature, i.e., readers process text one word at a time <ref type="bibr" target="#b12">(Hale, 2001</ref><ref type="bibr" target="#b13">(Hale, , 2006;;</ref><ref type="bibr" target="#b27">Rayner and Clifton, 2009;</ref><ref type="bibr">Boston et al., 2011 , inter alia)</ref>. Consequently, much can be uncovered about reading comprehension via studies that analyze cognitive processing at the word-level. Many pyscholinguistic studies make use of this notion, taking per-word RTs in self-paced reading (SPR) or eyetracking studies to be a direct reflection of the processing load of that word (e.g., <ref type="bibr" target="#b31">Smith and Levy, 2013;</ref><ref type="bibr" target="#b33">Van Schijndel and Linzen, 2021)</ref>. This RT-processing effort relationship then allows us to identify relationships between a word's processing load and its attributes (e.g., surprisal or length)-which in turn hints at the underlying cognitive processes involved in comprehension. One prominently studied attribute is word predictability; a notion naturally quantified by surprisal (also known as <ref type="bibr" target="#b30">Shannon's (1948)</ref> information content). Formally, the surprisal of a word w is defined as s(w) def = -log p(w | w &lt;t ), i.e., a unit's negative log-probability given the prior sentential context w &lt;t . Notedly, this operationalization provides a way of quantifying how our prior expectations can affect our ability to process a linguistic signal.</p><p>There are several hypothesis about the mathematical nature of the relationship between perword surprisal and processing load. <ref type="foot" target="#foot_1">2</ref> While there has been much empirical proof that surprisal estimates serve as a good predictor of word-level RTs <ref type="bibr" target="#b31">(Smith and Levy, 2013;</ref><ref type="bibr">Goodkind and Bick-nell, 2018;</ref><ref type="bibr" target="#b34">Wilcox et al., 2020)</ref>, the data observed from sentence-final words appears not to follow the same relationship. Specifically, in comparison to sentence-medial words, sentence-or clause-final words are associated with increased RTs in selfpaced studies <ref type="bibr" target="#b19">(Just et al., 1982;</ref><ref type="bibr" target="#b15">Hill and Murray, 2000)</ref> and both increased fixation and regression times in eye-tracking studies <ref type="bibr" target="#b28">(Rayner et al., 2000;</ref><ref type="bibr" target="#b2">Camblin et al., 2007)</ref>. Such behavior has also been observed in controlled settings-for example, <ref type="bibr" target="#b29">Rayner et al. (1989)</ref> found that readers fixated longer on a word when it ended a clause than when the same word did not end a clause.</p><p>Such wide-spread experimental evidence suggests sentence-final and sentence-medial reading behaviors differ from each other, and that other cognitive processes (besides standard word-level processing) effort may be at play. Yet unfortunately, these wrap-up effects have received relatively little attention in the psycholinguistic community: Most reading time studies simply exclude sentence-final (or even clause-final) words from their analyses, claiming that the (poorly-understood) effects are confounding factors in understanding the reading process (e.g., <ref type="bibr" target="#b7">Frank et al., 2013</ref><ref type="bibr" target="#b8">Frank et al., , 2015;;</ref><ref type="bibr" target="#b34">Wilcox et al., 2020)</ref>. Rather, we believe this data can potentially provide new insights in their own right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Wrap-up Effects</head><p>It remains unclear what exactly occurs in the mind of the reader at the end of a sentence or clause. Which cognitive processes are encompassed by the term wrap-up effects? Several theories have been posited. First, <ref type="bibr" target="#b18">Just and Carpenter (1980)</ref> hypothesize that wrap-up effects include actions such as "the constructions of inter-clause relations." Second, <ref type="bibr" target="#b28">Rayner et al. (2000)</ref> suggest they might involve attempts to resolve previously postponed comprehension problems, which could have been deferred in the hope that upcoming words would resolve the problem. Third, <ref type="bibr" target="#b16">Hirotani et al. (2006)</ref> posit the hesitation when crossing clause boundaries is out of efficiency <ref type="bibr" target="#b17">(Jarvella, 1971)</ref>; readers do not want to have to return to the clause later, so they take the extra time to make sure there are no inconsistencies in the prior text.</p><p>While some prior hypotheses have been largely dismissed (see <ref type="bibr" target="#b32">Stowe et al., 2018</ref> for a more detailed summary) due to, e.g., the wide-spread support of theories of incremental processing, most others lack formal testing in naturalistic reading studies. We attempt to address this gap. Concretely, we posit the relationship between text's information-theoretic attributes and its observed wrap-up times can provide an indication of the presence (or lack) of several cognitive processes that are potentially a part of sentence wrap-up. For example, high-surprisal words in the preceding context may correlate with the presence of ambiguities in the text; they may also correlate with complex linguistic relationships of the current text with prior sentences-which are two driving forces in the theories given above. Consequently, in this work, we ask whether the reading behavior observed at the end of a sentence or clause can be described (at least partially) by the distribution of information content in the preceding context,<ref type="foot" target="#foot_2">3</ref> as this may give insights for several prior hypotheses about wrap-up effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language Models as Predictors of Psychometric Data</head><p>Formally, a language model p is a probability distribution over natural language sentences. In the case when p is locally normalized, which is the predominant case for today's neural language models, p is defined as the product of conditional probability distributions: p(y) = |y| t=1 p(y t | y &lt;t ), where each p(? | y &lt;t ) is a distribution with support over linguistic units y (typically words) from a set vocabulary V, which includes a special end-of-sequence token. Consequently, we can use p to estimate individual word probabilities. Model parameters are typically estimated by minimizing the negative loglikelihood of a corpus of natural language strings C, i.e., minimizing L( p) = -y?C log p(y).</p><p>One widely embraced technique in informationtheoretic psycholinguistics is the use of these language models to estimate the probabilities required for computing surprisal <ref type="bibr" target="#b12">(Hale, 2001;</ref><ref type="bibr" target="#b4">Demberg and Keller, 2008;</ref><ref type="bibr" target="#b24">Mitchell et al., 2010;</ref><ref type="bibr" target="#b5">Fernandez Monsalve et al., 2012)</ref>. It has even been observed that a language model's perplexity<ref type="foot" target="#foot_3">4</ref> correlates negatively with the psychometric predictive power provided by its surprisal estimates <ref type="bibr" target="#b6">(Frank and Bod, 2011;</ref><ref type="bibr" target="#b11">Goodkind and Bicknell, 2018;</ref><ref type="bibr" target="#b34">Wilcox et al., 2020)</ref>. If these language models keep improving at their current fast pace <ref type="bibr" target="#b26">(Radford et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al., 2020)</ref>, exciting new results in computational psycholinguistics may follow, connecting reading behavior to the statistics of natural language.</p><p>Predicting Reading Times. In the computational psycholinguistics literature, the RT-surprisal relationship is typically studied using predictive models: RTs are predicted using surprisal estimates (along with other attributes such as number of characters) for the current word. The predictive power of these models, together with the structure of the model itself (which defines a specific relationship between RTs and surprisal), is then used as evidence of the studied effect. While this paradigm is successful in modeling sentence-medial RTs <ref type="bibr" target="#b31">(Smith and Levy, 2013;</ref><ref type="bibr" target="#b11">Goodkind and Bicknell, 2018;</ref><ref type="bibr" target="#b34">Wilcox et al., 2020)</ref>, its effectiveness for modeling sentence-and clause-final times is largely unknown due to the omission of this data from the majority of RT analyses.</p><p>A priori, we might expect per-word surprisal to be a similarly powerful predictor of sentence and clause-final RTs.<ref type="foot" target="#foot_4">5</ref> Yet in Fig. <ref type="figure" target="#fig_0">1</ref>, we see that when our baseline linear model (described more precisely in ?4) is fit to sentence-medial RTs, the residuals for predictions of clause-final RTs appear to be neither normally distributed nor centered around 0. Further, these trends appear to be different for eyetracking and SPR data, where the latter are skewed towards lower values for all datasets. 6 These re-sults provide further confirmation that clause-final data does not adhere to the same relationship with RT as sentence-medial data, a phenomenon that may perhaps be accounted for by additional factors at play in the comprehension of clause-final words. Thus, we ask whether taking into account information from the entire prior context can give us a better model of these clause-final RTs.</p><p>To this end, we operationalize the information content INF in text w (of length T ) as: 7</p><formula xml:id="formula_0">INF (k) (w) def = T t=1 s(w t ) k (k ? 0) (1)</formula><p>where w may be an entire sentence, or only its first T words. Notably, the case of k = 0 returns T ; under k = 1, we get the total information content of w. For k &gt; 1, moments of high-surprisal will disproportionately drive up the value of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Data. We use reading time data from 5 corpora over 2 modalities: the Natural Stories <ref type="bibr" target="#b10">(Futrell et al., 2018)</ref>, Brown <ref type="bibr" target="#b31">(Smith and Levy, 2013)</ref>, and UCL (SP) <ref type="bibr" target="#b7">(Frank et al., 2013)</ref> Corpora, which contain SPR data, as well as the Provo <ref type="bibr" target="#b22">(Luke and Christianson, 2018)</ref>, Dundee <ref type="bibr" target="#b20">(Kennedy et al., 2003)</ref> and UCL (ET) <ref type="bibr" target="#b7">(Frank et al., 2013)</ref> Corpora, which contain eye movements during reading. All corpora are in English. For eye-tracking data, we take reading time to be the sum over all fixation times on that word. We provide an analysis of regression (a.k.a. go-past) time in App. B. We provide further details regarding pre-processing in App. A.</p><p>Estimating Surprisal. We obtain surprisal estimates from three language models: GPT-2 <ref type="bibr" target="#b26">(Radford et al., 2019)</ref>, TransformerXL <ref type="bibr" target="#b3">(Dai et al., 2019)</ref> and a 5-gram model, estimated using Modified Kneser-Essen-Ney Smoothing <ref type="bibr" target="#b25">(Ney et al., 1994)</ref>.</p><p>We compute per-word surprisal as the sum of subword surprisals, when applicable. Additionally, punctuation is included in these estimates, although data; see App. B. 7 We note <ref type="bibr" target="#b23">Meister et al. (2021)</ref> used similar operationalizations to test for evidence in support of the uniform information density hypothesis. see App. B for results omitting punctuation, which are qualitatively the same. More details are given in App. A.</p><p>Evaluation. Following <ref type="bibr" target="#b34">Wilcox et al. (2020)</ref> and <ref type="bibr" target="#b23">Meister et al. (2021)</ref>, we quantify the predictive power of a variable of interest (INF (k) (w) here) as the mean difference in log-likelihood ?LogLik of a (held-out) data point when using a model with and without that predictor. In other words, we train two models to predict RTs-one with and one without access to INF (k) (w)-the difference in their predictive power is ?LogLik. A positive ?LogLik value indicates the model with this predictor fits the observed data more closely than a model without this predictor. We use 10-fold cross-validation to compute ?LogLik values so as to avoid overfitting, taking the mean across the held-out folds as our final metric. Our baseline model for predicting perword RTs contains predictors for surprisal, unigram log-frequency, character length, and the interaction of the latter two. These values, albeit computed on the previous word, are also included to account for spill-over effects <ref type="bibr" target="#b31">(Smith and Levy, 2013)</ref>. Surprisal from two words back is included for SPR datasets. Unless otherwise stated, GPT-2 estimates are used for baseline surprisal estimates in all models.</p><p>Results. Here we explore the additional predictive power that INF (k) gives us when modeling clause-final RTs. In Fig. <ref type="figure" target="#fig_1">2</ref>, we observe that often the additional information provided by INF (k) (w) indeed leads to better models of clause-final RTs. In most cases, INF (k) at some value of k &gt; 0 leads to larger gains in predictive power than k = 0. Ergo, the information content of the preceding text is more indicative of wrap-up behavior than length alone. Further, while often within standard error, INF (k) (w) at k &gt; 1 provides more predictive power than at k = 1 across the majority of datasets. This indicates that unevenness in the distribution of surprisal is stronger than the total surprisal content alone as a predictor of clause-final RTs. The same experiments for sentence-medial words show these quantities are less helpful when modeling their RTs. Note that these effects hold above and beyond the spill-over effects from the window immediately preceding the sentence boundary. The effect of the distribution of surprisal throughout the sentence is stronger for eye-tracking data than for SPR; further, the trends are even more pronounced when measuring regression times for eye-tracking  (k) for models of sentence and clause-final (top row) and sentence-medial (bottom row) RTs using surprisal estimates from different language models. Shaded region connects standard error estimates. Vertical intercepts at k = 0, 1 are for reference. We see that our informationtheoretic predictors contribute much less modeling power to the prediction of sentence-medial RTs in comparison to sentence-and clause-final RTs.</p><p>data (see App. B).</p><p>Notably, we see some variation in trends across datasets. Due to the nature of psycholinguistic studies, it is natural to expect some variation due to, e.g., data collection procedures or inaccuracies from measurement devices. Another (perhaps more influential) factor in the difference in trends comes from the variation in dataset sizes. We see that with the smaller datasets (e.g., UCL and Provo), there may not be enough data to learn accurate model parameters. This artifact may manifest as the noisiness or a lack of a significant increase in log-likelihood (on a held-out test set) over the baseline that we observe in some cases.</p><p>When considering prior theories of wrap-up processes, these results have several implications. For example, they can be interpreted as supporting and extending <ref type="bibr">Rayner et al.'s (2000)</ref> hypothesis, which suggests the extra time at sentence boundaries is spent resolving prior ambiguities. In this case, the observed correlation between wrap-up times and INF (k) (w) may potentially be linked to two factors: (1) contextual ambiguities increasing variation in per-word information content; and (2) contextual ambiguities being resolved at clause ends. On the other hand, these results provide evidence against the hypothesis that the cognitive processes occurring during the comprehension of sentence-medial and clause-final words are the same. Further, it also goes against <ref type="bibr">Hirotani et al.'s (2006)</ref> hypothesis (discussed in ?2.2), as the differences in sentence-medial and clause-final times cannot be purely quantified by a constant factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We attempt to shed light on the nature of wrap-up effects by exploring the relationship between clause-final RTs and information-theoretic attributes of text. We find that operationalizations of the information contained in preceding context lead to better predictions of these RTs, while not adding significant predictive power for sentence-medial RTs. This suggests that information-theoretic attributes of text can shed light on the cognitive processes happening during the comprehension of clause-final words. Further, these processes may indeed be different in nature than those required for sentence-medial words. In short, our results provide evidence (either in support or against) about several theories of the nature of wrap-up processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setup</head><p>A.1 Data Pre-processing</p><p>We use the Moses decoder 8 tokenizer and punctuation normalizer to pre-process all text data. Some of the Hugging Face tokenizers for respective neural models performed additional tokenization; we refer the reader to the library documentation for more details. We determine clause-final words as all those ending in punctuation. Capitalization was kept intact albeit the lowercase version of words were used in unigram probability estimates. We estimate unigram log-probabilities on WikiText-103 using the KenLM <ref type="bibr" target="#b14">(Heafield, 2011)</ref> library with default hyperparameters. We removed outlier wordlevel reading times (specifically those with a zscore &gt; 3 when the distribution was modeled as log-linear).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Surprisal Estimates</head><p>We use pre-trained neural language models to compute most surprisal estimates.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Regression Times Analysis</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distributions of residuals when predicting either clause-final or non clause-final times using our baseline linear models. Models are fit to (the logtransform of) non clause-final average RTs. Outlier times (according to log-normal distribution) are excluded. The top level datasets contain eye-tracking data while the bottom contain SPR data. Full distributions of RTs are shown in App. B, where we also show models fit to regression times, rather than full reading times.</figDesc><graphic url="image-9.png" coords="3,313.62,70.87,203.32,109.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mean ?LogLik as a function of the exponent k in INF(k) for models of sentence and clause-final (top row) and sentence-medial (bottom row) RTs using surprisal estimates from different language models. Shaded region connects standard error estimates. Vertical intercepts at k = 0, 1 are for reference. We see that our informationtheoretic predictors contribute much less modeling power to the prediction of sentence-medial RTs in comparison to sentence-and clause-final RTs.</figDesc><graphic url="image-11.png" coords="5,83.75,156.66,439.93,84.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For reproducibility, we employ the model checkpoints provided by Hugging Face (Wolf et al., 2020). Specifically, for GPT-2, we use the default OpenAI version (gpt2); for TransformerXL, we use a version of the model (architecture described in Dai et al. (2019)) that has been fine-tuned on WikiText-103 (transfo-xl-wt103); for BERT, we use the bert-base-cased version. Notably, BERT models the probability of a word given both prior and later context, which means it can only give us pseudo estimates of surprisal. Both GPT-2 and BERT use sub-word tokenization. We additionally use surprisal estimates from a 5-gram model trained on WikiText-103 using the KenLM (Heafield, 2011) library with default hyperparameters for Kneser-Essen-Ney smoothing. 8 http://www.statmt.org/moses/ B Additional Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distributions of average RTs for clause-final and non-clause-final words. Outlier times (according to log-normal distribution) are excluded from averages for both graphs. The top level datasets contain eye-tracking data while the bottom contain SPR data.</figDesc><graphic url="image-12.png" coords="8,317.94,101.29,194.67,109.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Version of Fig. 1 where surprisal estimates do not include the surprisal assigned to punctuation, which is often a large contributor to clause-final surprisal estimates. We see very little qualitative difference with Fig. 1.</figDesc><graphic url="image-13.png" coords="8,306.14,295.10,218.26,113.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Version of (a) Fig. 3 and (b) Fig. 1 for regression times for clause-final and non-clause-final words. Only applicable for eye-tracking datasets</figDesc><graphic url="image-15.png" coords="8,313.23,577.35,204.10,53.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Same setup as Fig. 2 albeit using respective model estimates for the baseline per-word surprisal estimate. (a) shows results for predicting clause-final words, while (b) shows results for predicting sentence-medial words. Results follow similar trends to those seen in Fig. 2.</figDesc><graphic url="image-19.png" coords="9,99.99,346.06,408.19,77.33" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Longer reading times in self-paced reading studies and longer fixation times in eye-tracking studies.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Surprisal theory<ref type="bibr" target="#b12">(Hale, 2001)</ref>, for instance, posits a linear relation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Importantly, the research questions we ask are not concerned with describing the full set of cognitive processes that occur at the end of a clause or sentence-or even whether there is a causal relationship between information content and sentence-and clause-final RTs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>  4  Perplexity is a monotonic function of the average surprisal of linguistic units in-context under a model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Several works (e.g.,<ref type="bibr" target="#b32">Stowe et al., 2018)</ref> have argued the cognitive processes involved in comprehension of clause-final words are exactly the same as those for sentence-medial words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>The opposite is true for regression times in eye-tracking</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>RC acknowledges support from the <rs type="funder">Swiss National Science Foundation (SNSF)</rs> as part of the "The <rs type="projectName">Forgotten Role of Inductive Bias in Interpretability</rs>" project. TP is supported by a <rs type="grantName">Facebook Fellowship Award</rs>. RPL acknowledges support from <rs type="funder">NSF</rs> grant <rs type="grantNumber">2121074</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ZrnuVwx">
					<orgName type="grant-name">Facebook Fellowship Award</orgName>
					<orgName type="project" subtype="full">Forgotten Role of Inductive Bias in Interpretability</orgName>
				</org>
				<org type="funding" xml:id="_2kPd2Rz">
					<idno type="grant-number">2121074</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>All studies involving human evaluations were conducted outside of the scope of this paper. The authors foresee no ethical concerns with the work presented in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parallel processing and sentence comprehension difficulty</title>
		<author>
			<persName><forename type="first">Marisa</forename><surname>Ferrara Boston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shravan</forename><surname>Vasishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhold</forename><surname>Kliegl</surname></persName>
		</author>
		<idno type="DOI">10.1080/01690965.2010.492228</idno>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="349" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The interplay of discourse congruence and lexical association during sentence processing: Evidence from ERPs and eye tracking</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Camblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">C</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">Y</forename><surname>Swaab</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2006.07.005</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="128" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1285</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data from eyetracking corpora as evidence for theories of syntactic processing complexity</title>
		<author>
			<persName><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2008.07.008</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="210" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexical surprisal as a general predictor of reading time</title>
		<author>
			<persName><forename type="first">Irene</forename><surname>Fernandez Monsalve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter</title>
		<meeting>the 13th Conference of the European Chapter<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="398" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Insensitivity of the human sentence-processing system to hierarchical structure</title>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611409589</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="829" to="834" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading time data for evaluating broad-coverage models of English sentence processing</title>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Fernandez Monsalve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">L</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-012-0313-y</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1182" to="1190" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The ERP response to the amount of information conveyed by words in sentences</title>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leun</forename><forename type="middle">J</forename><surname>Otten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Galli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bandl.2014.10.006</idno>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lossy-context surprisal: An informationtheoretic model of memory effects in sentence processing</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page">12814</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Natural Stories Corpus</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><forename type="middle">J</forename><surname>Tily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Vishnevetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Piantadosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Fedorenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predictive power of word surprisal for reading times is a linear function of language model quality</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Goodkind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klinton</forename><surname>Bicknell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the 8th Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A probabilistic Earley parser as a psycholinguistic model</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Meeting of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uncertainty about the rest of the sentence</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15516709cog0000_64</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="672" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">KenLM: Faster and smaller language model queries</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Commas and Spaces: Effects of Punctuation on Eye Movements and Sentence Parsing</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Murray</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-008043642-5/50027-9</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="565" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Punctuation and intonation effects on clause and sentence wrap-up: Evidence from eye movements</title>
		<author>
			<persName><forename type="first">Masako</forename><surname>Hirotani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyn</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2005.12.001</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="443" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Syntactic processing of connected speech</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Jarvella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="416" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A theory of reading: From eye fixations to comprehension</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">A</forename><surname>Just</surname></persName>
		</author>
		<author>
			<persName><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="329" to="354" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Paradigms and processes in reading comprehension</title>
		<author>
			<persName><forename type="first">Marcel</forename><forename type="middle">Adam</forename><surname>Just</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><forename type="middle">D</forename><surname>Woolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="228" to="238" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Dundee Corpus</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Pynte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Eye Movements</title>
		<meeting>the 12th European Conference on Eye Movements</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Establishing Causal Coherence across Sentences: An ERP Study</title>
		<author>
			<persName><forename type="first">Gina</forename><forename type="middle">R</forename><surname>Kuperberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Paczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Ditman</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn.2010.21452</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1230" to="1246" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Provo Corpus: A large eye-tracking corpus with predictability norms</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">G</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiel</forename><surname>Christianson</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-017-0908-4</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="826" to="833" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting the uniform information density hypothesis</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>J?ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Syntactic and semantic factors in processing difficulty: An integrated measure</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="196" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On structuring probabilistic dependences in stochastic language modelling</title>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ute</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<idno type="DOI">10.1006/csla.1994.1001</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language processing in reading and speech perception is fast and incremental: Implications for event-related potential research</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Clifton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biopsycho.2008.05.002</idno>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="9" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The effect of clause wrap-up on eye movements during reading</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Kambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">A</forename><surname>Duffy</surname></persName>
		</author>
		<idno type="DOI">10.1080/713755934</idno>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology Section A</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1061" to="1080" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Eye movements and on-line language comprehension processes</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">C</forename><surname>Sereno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">K</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R?ne</forename><surname>Schmauder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Clifton</surname><genName>Jr</genName></persName>
		</author>
		<idno type="DOI">10.1080/01690968908406362</idno>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="21" to="I49" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The effect of word predictability on reading time is logarithmic</title>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2013.02.013</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="319" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The sentence wrap-up dogma</title>
		<author>
			<persName><forename type="first">Laurie</forename><forename type="middle">A</forename><surname>Stowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edith</forename><surname>Kaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2018.03.011</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="232" to="247" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Singlestage prediction models do not explain the magnitude of syntactic disambiguation difficulty</title>
		<author>
			<persName><forename type="first">Marten</forename><surname>Van Schijndel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12988</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the predictive power of neural language models for human realtime comprehension behavior</title>
		<author>
			<persName><forename type="first">Ethan Gotlieb</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</editor>
		<editor>
			<persName><surname>Rush</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Proceedings of the 42nd Annual Meeting of the Cognitive Science Society</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
