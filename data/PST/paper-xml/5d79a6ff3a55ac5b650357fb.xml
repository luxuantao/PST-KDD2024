<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Prefetch Control and Cache Partitioning to Improve Multicore Performance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gongjin</forename><surname>Sun</surname></persName>
							<email>gongjins@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Shen</surname></persName>
							<email>junjies1@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Prefetch Control and Cache Partitioning to Improve Multicore Performance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/IPDPS.2019.00103</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern commercial multi-core processors are equipped with multiple hardware prefetchers on each core. The prefetchers can significantly improve application performance. However, shared resources, such as last-level cache (LLC) and off-chip memory bandwidth and controller, can lead to prefetch interference. Multiple techniques have been proposed to reduce such interference and improve the performance isolation across cores, such as coordinated control among prefetchers and cache partitioning (CP). Each of them has its advantages and disadvantages. This paper proposes combining these two techniques in a coordinated way. Prefetchers and LLC are treated as separate resources and a multi-resource management mechanism is proposed to control prefetching and cache partitioning. This control mechanism is implemented as a Linux kernel module and can be applied to a wide variety of prefetch architectures. An implementation on Intel Xeon E5 v4 processor shows that combining LLC partitioning and prefetch throttling provides a significant improvement in performance and fairness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sharing LLC and off-chip memory bandwidth can lead to destructive inter-core interference as more cores are deployed in modern commercial multicore processors. This interference may be further exacerbated by prefetch traffic from multiple hardware prefetchers deployed in each core. For example, a modern Intel Xeon server processor <ref type="bibr" target="#b0">[1]</ref> has four hardware data prefetchers per core, two at the L1 cache and two at the L2 cache. These prefetchers may significantly improve core performance by hiding the access latency. At the same time, they may increase LLC and memory traffic and contention. This, in turn, may reduce throughput and fairness, as when a process consumes more than its fair share of LLC space or memory bandwidth due to prefetching. Performance isolation and fairness across processes/cores are still important problems and this work proposes to address them via a combination of cache partitioning (CP) and prefetch throttling. It targets Intel Server processors, but can be applied to any processor with similar capabilities.</p><p>Extensive prior work <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b10">[11]</ref> proposed reducing prefetchercaused inter-core interference by controlling prefetch aggressiveness. These included hardware-and software-based schemes. The former are mostly microarchitectural techniques requiring additional hardware support. Key metrics used in these techniques, e.g. prefetch accuracy, cannot be collected for software use on today's processors. The latter work on existing processors require no hardware modifications, e.g. Jimenez et al. <ref type="bibr" target="#b5">[6]</ref> designed for IBM POWER7 processor <ref type="bibr" target="#b11">[12]</ref>.</p><p>CP has also been researched intensively. Early work <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> proposed theory and michroarchitectural techniques. Recently, Intel server processors incorporated software controllable, way-based cache partitioning of LLC called Cache Allocation Technology (CAT) <ref type="bibr" target="#b14">[15]</ref>. Recent work <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> utilized CAT for performance isolation. However, it did not consider prefetching directly, which left potential for further improvement.</p><p>Both prefetch throttling and partitioning can be used to improve performance isolation on multicore processors. This raises the following research questions: 1) What are their respective pros and cons? 2) Is there a performance difference between them? 3) Is it possible to combine them for additional improvement?</p><p>This paper aims to answer the above questions. It develops CMM, a multi-resource management framework for combining prefetch throttling and CP. CMM is a low-overhead, software-based framework running on Intel multicore processors. Several prefetch throttling and/or cache partitioning mechanisms were implemented using the framework to compare the two techniques, study their interaction and to explore the design space. The framework is implemented as a Linux kernel module and monitors system behavior via hardware Performance Monitoring Unit (PMU). An integrated throttling/partitioning mechanism is proposed based on this implementation.It targets multi-programmed workloads and is transparent to application software. It is evaluated on an Intel processor, but is easily portable to any processor with similar capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head><p>Intel Prefetchers. Today's Intel processor has four data prefetchers per physical core. The L1 data cache has IP (stride) and next-line prefetchers, and the private L2 cache has stream and adjacent line prefetchers <ref type="bibr" target="#b14">[15]</ref>. By default, all prefetchers in a processor are turned on to improve application performance. Intel provides a mechanism to turn on/off each of the four prefetchers independently by programming a special MSR in the software. Additionally, Intel exposes several hardware performance counters (PMU events) that are related to prefetch, e.g. L2 prefetch misses that count how many prefetch requests arrive at LLC. These event counts can be used for dynamic prefetch control (Pref Ctrl).</p><p>Demand requests of a core may trigger L1 prefetchers to issue requests. Both demand and prefetch requests first check the local L1 cache and only on a miss continue to the next cache level (L2). Requests arriving at L2 will trigger L2's prefetchers to issue new prefetch requests, which are checked in L2 before issuing to LLC.</p><p>Cache Partitioning. Intel CAT allows users to create shared and overlapping way-based partitions on a commercial processor. Such partitioning is much more flexible and can be used on larger multi-core systems with a low associativity LLC <ref type="bibr" target="#b18">[19]</ref>. Prior work Selfa et al. <ref type="bibr" target="#b16">[17]</ref> used CAT to improve fairness of multi-programmed workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prefetch-related Terms</head><p>Many similar terms have been used to describe prefetching in prior work, such as "prefetch sensitive/effective/aggressive/useful/friendly." This section defines the terms used in this paper to avoid confusion.</p><p>A demand intensive<ref type="foot" target="#foot_0">1</ref> program has a large working set and generates many demand requests. A demand intensive program may or may not generate a large number of prefetch requests. This depends on whether its access patterns will trigger a prefetcher. A prefetch aggressive program has a high ratio of prefetch to demand requests. A prefetch aggressive program brings many prefetched lines into the cache hierarchy and may interfere with other programs or itself. A program has a high prefetch accuracy if most of the prefetched data are used for a given prefetcher<ref type="foot" target="#foot_1">2</ref> and is called prefetch useful. As in prior work Srinath et al. <ref type="bibr" target="#b19">[20]</ref>, prefetch accuracy is defined as the percentage of prefetched data used.</p><p>Memory bandwidth required by a program is an indication of demand intensity. Fig. <ref type="figure" target="#fig_0">1</ref> shows the measured average memory bandwidth (BW) of SPEC CPU2006 benchmarks (on an Intel E5-2620 v4 processor and for high BW benchmarks) with and without prefetching. The increase from prefetching is shown on the top part of each bar. Benchmarks, such as 437.leslie3d, 459.GemsFDTD and 410.bwaves, use approximately 4GB/sec demand BW (the blue part in the bar). Thus they are demand intensive compared with other ones. Their BW use increases by more than 80% with prefetching. Therefore, they are also prefetch aggressive.</p><p>Prefetch aggressive AND useful programs usually benefit significantly from prefetching, and are called prefetch friendly. A demand intensive but not prefetch aggressive (i.e. generating few prefetch requests) program can also be prefetch useful. However, performance improvement brought by its prefetching is very limited. Finally, a non demand intensive program usually will not cause prefetch-caused interference to others. 1 2  Today's processors have no PMU counters to directly measure prefetch accuracy. This work uses its approximation to identify prefetch friendly applications (See Sec. III-B1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pros and Cons of Prefetch Throttling and Cache Partitioning</head><p>Prefetch throttling may improve system performance if applied periodically to select a different prefetcher configuration on each core to reduce interference. However, this may lead to performance loss for applications on throttled cores. These are usually prefetch friendly and benefit significantly from prefetching. They can yield LLC space and memory bandwidth to other applications and still have reasonable performance. Thus, there is a trade-off between degrading performance of such applications and reducing their interference.</p><p>Exclusive CP creates a physical LLC isolation among cores and hence prevents inter-core interference (regardless of prefetchers). However, when multiple cores share a partition, interference is created within a partition. LLC partitioning does not necessarily reduce prefetch traffic arriving at LLC and even memory. This is because prefetchers have little feedback from LLC to indicate that throttling may be beneficial, let alone coordinating across cores. Thus, a prefetch friendly but aggressive application can generate a large number of prefetch requests to memory. Memory BW contention may also be significant if prefetch aggressive programs occupy the same partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Coordinating Prefetching and Cache Partitioning</head><p>Let us make two observations vis a vis performance based on SPEC CPU2006 benchmarks.</p><p>Fig. <ref type="figure">2</ref> shows the IPC speedup from prefetching for SPEC CPU2006 <ref type="bibr" target="#b20">[21]</ref> benchmarks. Several benchmarks, such as 462.libquantum, 410.bwaves, 481.wrf, 459.GemsFDTD,etc Fig. <ref type="figure">3</ref>: IPC distribution across LLC ways (all prefetchers on).</p><p>have 50+% improvement from prefetchers (are prefetch friendly).</p><p>Fig. <ref type="figure">3</ref> shows each benchmark's performance as a function of the number of LLC ways with prefetching enabled. Many benchmarks, especially prefetch aggressive and friendly, need no more than 2 ways to achieve 90% of their highest performance. Selfa et al. <ref type="bibr" target="#b16">[17]</ref> had a similar observation. This observation-that applications whose high performance is mainly from prefetching may not need many cache ways to achieve its highest performance-leads us to rethink how to utilize both prefetching and CP in a coordinated way. Prefetchers and LLC can be viewed as two different resources. An application's performance can be from either of them or both. Yielding one resource and getting another makes it possible to improve overall system performance and fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COORDINATED MULTI-RESOURCE MANAGEMENT</head><p>(CMM) Our overall goal is to reduce prefetcher-caused inter-core interference. This requires identification of prefetch aggressive (Pref Agg) applications. One can then throttle their resource usage by applying prefetcher control or CP, or both periodically, to maximize system performance and fairness. Targeting this approach, the CMM framework is designed as a decoupled structure: a Front-and a Back-end. The frontend is responsible for detection and the back-end for resource allocation. This flexible design allows either the front-or backend to be modified independently as needed without affecting the other.</p><p>CMM performs resource allocation periodically using sampling. Fig. <ref type="figure" target="#fig_1">4</ref> shows a high-level diagram. The execution of a process is divided into a sequence of "Execution Epochs", each followed by a "Profiling Epoch." The latter consists of multiple "Sampling Intervals". At the end of each execution epoch, CMM's front-end collects the necessary runtime statistics and identifies the prefetch aggressive applications. Then, the back-end profiles an application by trying different resource configurations and chooses a good candidate for the next execution epoch (more detail is provided in Sec. III-B1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pref Agg Application Detection</head><p>Assume N applications are running on N different cores concurrently. Intel's PMU provides the following relevant events per logical cpu:  Specific metrics derived from these events are listed in Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Execution Epoch</head><note type="other">Profiling Epoch Sampling Interval</note><p>These metrics are used for identifying Pref Agg cores. Panda et al. <ref type="bibr" target="#b7">[8]</ref> uses L2 PPM (M-6) to classify cores into two categories: aggressive and meek, and applied a coarse-grained (5 configurations) group-level prefetch throttling. Their throttling applies to all cores and was designed for a totally different prefetcher hierarchy (the per-core LLC prefetcher). Using this metric on the Intel L2 cache side cannot accurately identify the Pref Agg cores.</p><p>This work uses a different detection mechanism for Pref Agg cores. First, PGA (M-4) is used to detect cores whose access patterns cause prefetchers to generate prefetch requests. A core whose PGA is above the average value Fig. <ref type="figure">5</ref>: Detection of Prefetch Aggressive cores across all cores is viewed as potentially prefetch aggressive. Second, L2 PMR (M-5) is used to filter out cores with high prefetch locality, i.e. whose most prefetch requests hit the L2 cache. A threshold (say 70%) will be considered. Third, L2 PTR (M-3) is used to evaluate the prefetch pressure on the LLC. It measures the bandwidth pressure caused by prefetch (instead of an absolute prefetch number) between L2 and LLC. Figure <ref type="figure">5</ref> shows a high level diagram of this mechanism.</p><p>One can also use metric LLC PT (M-7) to identify cores that issue a large number of prefetch requests to memory. However, we observed that cores with high L2 pref miss traffic rate usually also have a high LLC PT. The identified Agg set by Figure <ref type="figure">5</ref> basically stays the same as when using LLC PT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Back end Design</head><p>Once the Agg set is determined by the front end, the back end of CMM will conduct resource allocation across cores to improve performance isolation. Let us consider the following three approaches.</p><p>? Prefetch Throttling: Throttle the prefetchers of one or more cores in Agg set to reduce their interference ? Cache Partitioning: Place cores in Agg set into a relatively small partition (called Agg partition) and let the cores in neutral set share the whole cache ? Coordinated-throttling: First perform Cache Partitoning, then throttle the prefetchers within Agg partition 1) Prefetch Throttling (PT): This approach throttles prefetchers of cores in the Agg set. All four prefetchers per core are either on or off. For example, if Agg set contains two cores, there are four prefetch combinations: {on, on}, {on, off}, ..., {off, off}. PT applies them for a very short sampling interval each and chooses the "best" one. "Best" refers to the combination that produces the lowest system average normalized turnaround time (ANTT), which is the reciprocal of harmonic speedup (HS) (Eyerman et al. <ref type="bibr" target="#b21">[22]</ref>). The calculation of HS requires the knowledge of an application's IPC when running alone. Though prior work Subramanian et al. <ref type="bibr" target="#b22">[23]</ref> tried to estimate the running-alone IPC in multicore processors; it requires additional hardware support.</p><p>This work instead uses a proxy metric, the harmonic mean of all cores' IPC (called hm ipc), to estimate the performance and fairness of a prefetch combination. The evaluation (see Sec. V) shows the system fairness and performance are indeed improved over the baseline (no prefetch control) using this metric. This "best" prefetch combination is applied to next execution epoch.</p><p>The first sampling interval is always {on, on} because some cores' prefetchers could have been turned off in the last execution epoch if their L2 PTR or PGA was 0. So a short sampling period with all cores' prefetchers on is needed to collect the PMU statistics before trying other sampling intervals.</p><p>As mentioned above, disabling prefetching for prefetch friendly applications will hurt their performance even though doing so can reduce the interference with others (Actually, not all cores in Agg set are prefetch friendly. This is discussed in more detail below). The experiments show that some applications (410.bwaves, 462.libquantum) could get performance degradation of more than 50% if their prefetching is turned off most of the time.</p><p>A large Agg set (say 10 cores) makes the search space of all available prefetch settings large. With the four per-core prefetchers viewed as a single entity and only "turned on/off", there are still 2 10 settings to search. In this case, a practical and scalable<ref type="foot" target="#foot_2">1</ref> solution is to use group-level throttling. The cores in Agg set are clustered into a limited number of groups (say 3) and all prefetchers in each group are viewed as a single entity. The limited group-level settings (2 3 = 8) are then tried. Prior work Panda et al. <ref type="bibr" target="#b7">[8]</ref> only uses a 2-group clustering which is coarse grained. This paper uses the K-Means Algorithm <ref type="bibr" target="#b23">[24]</ref> to cluster the cores in the Agg set into limited groups by their L2 PTRs (M-3), which evaluates the prefetch pressure to LLC brought by each core. The cores with similar L2 PTRs are placed into the same group.</p><p>Useful and Useless Prefetching: Cores in Agg set can be further categorized into two classes: prefetch unfriendly and prefetch friendly. The former usually prefetches more useless data because of its inaccurate prediction and thus benefits little from prefetching (or even is hurt) while polluting the cache severely. The latter needs to be treated with care: turning off their prefetching will significantly hurt their performance, but keeping their prefetching on will probably hurt other programs' performance due to interference. Prior work Liu et al. <ref type="bibr" target="#b24">[25]</ref> (Paragraph 5 in the introduction) shows that maximum weighted speedup can be achieved when a core with very useful prefetching yields some bandwidth to cores with less useful prefetching. With only prefetch throttling, this might be a good solution. However, as shown in the next section, it is possible to avoid reducing performance of applications with aggressive AND useful prefetching.</p><p>Detect prefetch friendliness: This paper uses an indirect method to identify prefetch friendly applications in lieu of prefetch accuracy. Prefetchers of cores in Agg set are turned off in the second sampling interval. A per-core IPC speedup from prefetching is calculated (from data of the first sampling A core with a predefined speedup over a threshold (say 50%) is considered prefetch friendly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Cache Partitioning (CP):</head><p>There are several proposed CP implementations. Selfa et al. <ref type="bibr" target="#b16">[17]</ref> groups cores into several clusters and each cluster occupies a separate partition. However, it did not consider the effect of prefetching (see Sec. V-B for more discussion).</p><p>This work studies two CP plans: 1) place the Agg set into a small partition, and 2) split Agg set into two subsets: prefetch friendly and unfriendly, then place each subset into a separate partition. They are compared with the best known algorithm, Dunn in <ref type="bibr" target="#b16">[17]</ref>, in Sec. V-B. Note that CP just needs the first two sampling intervals for the detection of prefetch usefulness.</p><p>While CP isolates prefetch interference from cores in the Agg set, the local interference within the partition still exists. Also the number of prefetch requests leaving the LLC may not be reduced and could still cause memory bandwidth contention with other programs. Additionally, if Agg set contains many prefetch-unfriendly cores, it is beneficial to turn off their prefetching.</p><p>3) Coordinated Throttling: A coordinated solution combines prefetch throttling and CP. First, all prefetch-friendly cores are placed into a small shared partition while keeping their prefetchers enabled. They are typically not LLC sensitive and a small partition results in pretty high performance. Meanwhile, the remaining cores share the whole cache capacity (note that we are using overlapping partitioning). Second, prefetch-unfriendly cores are identified and a group-level throttling is applied as discussed above. Other implementations are possible. Figure <ref type="figure" target="#fig_2">6</ref> shows the available options.</p><p>? (a) puts Agg set into a small partition. Agg set contains both prefetch-friendly and/or unfriendly cores. ? (b) only puts prefetch-friendly cores into a partition. ? (c) allows prefetch-unfriendly cores to stay in a separate small partition. ? (d) indicates a special scenario: Agg set is empty. This usually happens when there are no prefetch aggressive applications in the multi-programmed workload or current program phase. If so, it will be meaningless to throttle the prefetchers. In this case, CMM will use the "Dunn" algorithm in prior work <ref type="bibr" target="#b16">[17]</ref> for cache partitioning. This paper evaluates the first three options. Note that only the prefetchers of prefetch-unfriendly cores can be throttled. If no such cores are found, only CP will be applied. Such coordinated throttling trades off the use of two resource types across cores and tries to maximize each core's performance and the whole system performance and fairness as well. It should be noted that prefetch-useless cores could lose either one of them (prefetching) or both depending on the implementation.</p><p>Partition Size: Per Fig. <ref type="figure">3</ref>, most prefetch aggressive applications just need no more than 2 ways to achieve 90% of their highest performance. It was experimentally determined that a partition size of 1.5 times the size of the Agg set works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Processor used</head><p>All measurements were performed on an Intel Broadwell-EP Xeon server processor E5-2620 V4 <ref type="bibr" target="#b25">[26]</ref>. It contains 8 physical cores and 16 hardware threads and each physical core contains private 32KB L1 I/D-Cache and 256 KB L2 cache. All cores share a 20 MB L3 cache. This processor has a 2.1 GHz base frequency and supports 68.3 GB/s Maximum Memory Bandwidth. The memory is DDR-2400 and OS is Ubuntu 16.04 with the kernel 4.4.24. We disable the "Turbo boost" to keep each core frequency the same. Performance counters used in this work exist on almost all Intel mainstream processors from the 2rd generation Intel Core to the latest 8th one <ref type="bibr" target="#b14">[15]</ref>.</p><p>Baseline system The baseline used in the evaluation has all four prefetchers in each core enabled and no prefetch control and no cache partitioning are applied. Each workload was executed three times and the median value was used in the results.</p><p>Implementation details CMM is implemented as a loadable kernel module. We implement our own PMI (Performance Monitoring Interrupt) and IPI (Inter-Processor Interrupt) handlers to collect necessary PMU statistics and execute the algorithms of prefetch throttling and/or cache partitioning. In order to estimate the module-related overhead, we used two counters to collect cycles: PMU and TSC (Timestamp Counter). The former does not count handler-related cycles, and the latter does. Then, we compared them and found the overhead to be less than 0.1%.</p><p>The decoupled design allows the algorithm(s) in either the front-or the back-end to be changed individually. This work provides a flexible and open design framework for exploration. For example, if Intel or other processors expose more PMU events in the future, Table <ref type="table" target="#tab_0">I</ref> and related detection algorithm can be easily explored and updated to achieve better results. <ref type="bibr" target="#b20">[21]</ref> and an micro-benchmark (see below) are used to create multicore mixed workloads. Each N-core workload contains N benchmarks. Each benchmark was compiled with Intel ICC 17.0.0 or Intel Ifort 17.0.0 with the -O3 option. We classify the benchmarks into different categories: (1) perfetch aggressive (if their demand BW is more than 1500 MB/s AND BW increase brought by prefetching is more than 50% in Fig. <ref type="figure" target="#fig_0">1</ref>) or not; (2) prefetch friendly 2 (if their IPC speedup is more than 30% in Fig. <ref type="figure">2</ref>) or not; (3) LLC sensitive (if they need at least 8 ways to achieve 80% of its highest performance in Fig. <ref type="figure">3</ref>) or not. LLC sensitive applications are more easily affected by the interference in LLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mixed Workloads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPEC CPU2006 benchmarks</head><p>Since there is no SPEC benchmark whose performance is significantly reduced by prefetch (471.omnetpp is reduced only slightly), we manually created a micro-benchmark called "Rand Access" as a typical prefetch unfriendly application. It is strongly prefetch aggressive and conducts random access in a large memory region. Its performance slowdown with prefetching over no-prefetching is 25% when running alone because its access pattern is irregular.</p><p>It is not feasible to evaluate our mechanism with all possible combinations of 8 benchmarks. Instead, typical workload types are created by mixing benchmarks from different classes. The number in parenthesis below indicates how many benchmarks of a given type are in a workload.</p><p>? Pref Fri: Prefetch-friendly benchmarks (4) + Non Pref Agg benchmarks (4) ? Pref Agg: Prefetch-friendly benchmarks (2) + unfriendly ones (2) + Non Pref Agg ones (4) ? Pref Unfri: Prefetch-unfriendly benchmarks (4) + Non Pref Agg ones (4) ? Pref No Agg: Non Pref Agg benchmarks (8) 3  Each category contains 10 workloads and the benchmarks are chosen randomly from their respective class (prefetch friendly/unfriendly). Note that four non Pref Agg benchmarks include at least two LLC-sensitive benchmarks. Each workload runs for 2.5 minutes for both baseline and CMM-based mechanisms. If an application finishes earlier, it restarts from the beginning.</p><p>The length of an execution epoch and sampling period are 5 billion and 100 million core cycles respectively, which are determined to be a good set up experimentally. In fact, Jim?nez et al. <ref type="bibr" target="#b2">[3]</ref> shows a 50:1 of "execution epoch vs sampling period" is a reasonable choice. Other lengths (2 billion, 50 million) or (1 billion, 40 million) show similar results. One reason could be that Intel has a different prefetcher design from IBM's POWER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>Prior work <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref> explored the "fairness" and "performance" on multi-core systems from various perspectives. Like recent architecture work ( <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>), we use harmonic speedup (HS) and normalized weighted speedup over baseline (WS) ( <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref>) to evaluate the system performance and fairness of the above multiprogrammed workloads. They are defined as follows: 2 In this paper, a "prefetch friendly" application is also prefetch aggressive unless otherwise specified. 3 In some program phases, the Agg set may not be empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HS = N</head><formula xml:id="formula_0">N i=1 IP Ci alone IP Ci together W S = N i=1 IP C i Algorithm-x IP C i baseline</formula><p>where IP C i is the IPC of core i and N is the number of cores. IP C alone i represents program i 's IPC when it runs alone on a single core of a multicore processor. And IP C together i represents program i 's IPC when it runs together with other programs on different cores in a multicore processor. HS considers both system performance and fairness. According to Eyerman et al. <ref type="bibr" target="#b21">[22]</ref>, 1/HS is equal to the average turnaround time, which is a key performance metric in a multicore system. IP C Algorithm-x i and IP C baseline i represent program i 's IPC when Algorithm-x and baseline are applied, respectively. Algorithm-x could be PT, CP or CMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>The proposed mechanisms are evaluated and compared in this section. In all graphs, the first 10 workloads are Pref Fri, the second 10 are Pref Agg, followed by 10 Pref Unfri and 10 Pref No Agg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prefetch Throttling (PT)</head><p>Fig. <ref type="figure">7</ref> shows the normalized HS and WS vs baseline for PT. Most workloads gain from PT. Four grey bars show the average value for each workload category. Pref Unfri and Pref Agg have the highest and second-highest performance improvement, respectively. Pref Fri achieves a relatively lower improvement and Pref No Agg sees no improvement, as Pref Unfri and Pref Agg contain prefetch unfriendly applications. Turning on their prefetchers improves both their own and other programs' performance. However, turning off the prefetchers in Pref Fri reduces the interference to others at the cost of hurting their own performance. As a result, the overall performance improvement is not very significant.</p><p>Recall that some prefetch-friendly applications lose performance when their prefetching is disabled. Fig. <ref type="figure">8</ref> shows this. The per-application IPC speedup with PT over baseline is calculated. The lowest speedup is called the worst-case speedup in a workload. The figure shows that at least one application's performance is significantly reduced for 80% of the workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cache Partitioning (CP)</head><p>Fig. <ref type="figure">9</ref> shows the normalized HS and WS to baseline of Dunn, Pref-CP and Pref-CP2. Pref-CP puts Agg set into a small partition and Pref-CP2 splits Agg set into two subsets (prefetch friendly or not) to assign each a separate partition. It shows Pref-CP and Pref-CP2 significantly improve the system performance over Dunn in Selfa et al. <ref type="bibr" target="#b16">[17]</ref>. Dunn uses the PMU event "STALLS L2 PENDING" to cluster cores into several different groups based on the similarity of each core's measurement. Each group is then assigned a certain number Fig. <ref type="figure">7</ref>: Normalized HS and WS Fig. <ref type="figure">8</ref>: Lowest Normalized IPC in each workload of LLC ways as its partition. A group with a higher average "STALLS L2 PENDING" gets more ways. The partitions partially overlap with each other; in fact they are nested.</p><p>Dunn did not consider the effect of prefetching. For example, a prefetch aggressive program that has similar stalls l2 pending to other programs will be clustered into the same partition. As a result, the prefetch interference happens within this partition. Fig. <ref type="figure" target="#fig_0">10</ref> shows our algorithms have a higher worst-case speedup than Dunn. It is worth noting that Pref-CP has higher performance than Pref-CP2 for workload category Pref Agg and Pref Unfri in Fig. <ref type="figure">9</ref>. One reason is that some prefetch unfriendly applications (like 471.omnetpp) still need many LLC ways to get more performance as shown in Fig. <ref type="figure">3</ref>. So Pref-CP2, which puts the unfriendly ones into a small partition, can hurt these applications' performance. Workloads Pref Fri and Pref No Agg show the same performance. The reason is that the three mechanisms degenerate into CP-based throttling for these two workload categories because no prefetch unfriendly cores are throttled. For Pref Agg and Pref Unfri categories, CMM-a and CMM-c show better performance than CMM-b. In CMM-b, prefetch unfriendly cores still cause demand interference to the cores that share the whole LLC with them even though throttling their prefetching can reduce the prefetch-caused interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Coordinated Multi-resource Management (CMM)</head><p>Additionally, Fig. <ref type="figure" target="#fig_0">12</ref> shows that CMM-a/b/c gives all workloads a 80%+ worst-case speedup and most of them get 90%+. This indicates no individual application is hurt significantly by the mechanisms.</p><p>Finally, putting everything together, Fig. <ref type="figure" target="#fig_0">13</ref> shows the comparison among all 7 throttling mechanisms. Workload category Pref Agg and Pref Unfri benefit the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Effect of Memory Traffic</head><p>Fig. <ref type="figure" target="#fig_5">14</ref> shows the normalized BW to baseline of 7 throttling mechanisms. PT has the lowest bandwidth consumption because it frequently disables some cores' prefetching. By  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. The Effect of L2 stall cycles</head><p>The PMU event "STALLS L2 PENDING" is used for counting the number of cycles during which the execution of an application is stalled due to L2 cache misses <ref type="bibr" target="#b14">[15]</ref>. Prior work Selfa et al. <ref type="bibr" target="#b16">[17]</ref> shows there is a strong positive correlation between its count and an application's slowdown. This event's count is affected by the interference on shared resources such as LLC, memory bandwidth and controller, on-chip interconnects. As <ref type="bibr" target="#b16">[17]</ref> pointed out, as the number of concurrently running applications grows, the stall cycles caused by interference will dominate in total stall cycles experienced by the cores. Therefore, by observing this event, we know how effectively our mechanisms improve the system performance isolation.</p><p>Fig. <ref type="figure" target="#fig_0">15</ref> shows the normalized "STALLS L2 PENDING" for each workload (we sum per-core's number to get a single value for each workload). Again, CMM-a/c has the lowest number for most workloads (the lower the better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Local/Global Prefetch Control</head><p>Wu et al. <ref type="bibr" target="#b1">[2]</ref> proposes a dynamic control of prefetching to mitigate intra-application prefetch interference on Intel architectures. Liao et al. <ref type="bibr" target="#b3">[4]</ref> proposes a machine learning-based model to adjust the prefetch configuration for the individual application in Intel processors. Kang et al. <ref type="bibr" target="#b30">[31]</ref> studies the effect of hardware prefetching in virtualized environments. Jimenez et al. <ref type="bibr" target="#b2">[3]</ref> proposed an adaptive prefetch control to independently adjust each core's prefetch aggressiveness on Fig. <ref type="figure" target="#fig_0">15</ref>: Normalized L2 stalls per workload IBM POWER7 architecture. Intel prefetch architecture does not provide a fine granularity control like the POWER7 architecture.</p><p>Ebrahimi et al. <ref type="bibr" target="#b4">[5]</ref> proposes a hierarchical prefetcher aggressiveness controller (HPAC) to throttle multiple prefetchers on multi-core processors. HPAC dynamically identifies applications that cause inter-core interference and throttle their prefetchers to reduce the interference. Albericio et al. <ref type="bibr" target="#b31">[32]</ref> proposes an adaptive controller (ABS) to manage the aggressive prefetcher of each core on the multicore platform. Panda et al. <ref type="bibr" target="#b7">[8]</ref> proposes a synergistic prefetcher aggressiveness controller (SPAC) to improve the system fair-speedup. Jimenez et al. <ref type="bibr" target="#b5">[6]</ref> proposes a memory bandwidth based approach to throttle the prefetchers on IBM POWER7 architecture. This approach makes throttling decisions based on each core's bandwidth consumption and prefetch usefulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Other Prefetch-related research</head><p>Unlike the control of multiple prefetchers, the goals of <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b37">[38]</ref> are to design a single prefetcher. They are orthogonal to the work in this paper, which does not propose a new prefetcher architecture. Instead, the global control described in this paper can be applied to a variety of prefetchers. In addition, Liu et al. <ref type="bibr" target="#b24">[25]</ref> studies the interaction of prefetching and BW partitioning. <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref> propose various filters to reduce the prefetch requests that can cause interference to other applications or be useless. <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b45">[46]</ref> explores how to manage shared resources on multi-core platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cache Partitioning</head><p>Qureshi et al. <ref type="bibr" target="#b13">[14]</ref> proposes a utility-based cache partitioning (UCP) mechanism to partition the shared LLC among multiple applications. A way-based partitioning algorithm uses the utility to allocate various amounts of cache ways to each application. It should be noted that different partitions cannot overlap with each other. Cook et al. <ref type="bibr" target="#b15">[16]</ref> evaluates the effect of cache partitioning on real Intel multi-core processors, including an optimal static LLC partitioning and a dynamic algorithm. They find that measurements on real machines provide different observations than past simulation-based work. Wang et al. <ref type="bibr" target="#b46">[47]</ref> proposes a combined cache partition method (SWAP) to take into account both of set and way partitioning. This method can cooperatively manage cache sets and ways and provide many fine-grained partitions. Selfa et al. <ref type="bibr" target="#b16">[17]</ref> proposes a clustering-based cache partitioning mechanism to improve the fairness of multi-core processors. The allocations are grouped into clusters according to their l2 cache stalls, and different clusters get different ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>This paper proposes CMM, a coordinated multi-resource management mechanism, to reduce prefetch caused intercore interference on mainstream Intel multi-core processors. It does not require additional hardware support and manages the use of hardware prefetchers and LLC in a dynamic and coordinated manner by monitoring the applications' prefetching/cache behavior. Several CMM implementations were evaluated for diverse multiprogrammed workloads. The results show that using shared cache partitions to isolate applications with different prefetching behavior, combined with prefetch throttling, significantly improves system performance and fairness (performance isolation) for multiprogrammed workloads. The results are based on Intel architecture but are generally applicable to other processors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Memory Bandwidth Consumption of Part of SPEC CPU2006 benchmarks</figDesc><graphic url="image-3.png" coords="2,313.62,203.04,236.45,109.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The Execution and Sampling Mechanism</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: Partition Options interval with prefetchers on). A core with a predefined speedup over a threshold (say 50%) is considered prefetch friendly.2) Cache Partitioning (CP): There are several proposed CP implementations. Selfa et al.<ref type="bibr" target="#b16">[17]</ref> groups cores into several clusters and each cluster occupies a separate partition. However, it did not consider the effect of prefetching (see Sec. V-B for more discussion).This work studies two CP plans: 1) place the Agg set into a small partition, and 2) split Agg set into two subsets: prefetch friendly and unfriendly, then place each subset into a separate partition. They are compared with the best known algorithm, Dunn in<ref type="bibr" target="#b16">[17]</ref>, in Sec. V-B. Note that CP just needs the first two sampling intervals for the detection of prefetch usefulness.While CP isolates prefetch interference from cores in the Agg set, the local interference within the partition still exists. Also the number of prefetch requests leaving the LLC may not be reduced and could still cause memory bandwidth contention with other programs. Additionally, if Agg set contains many prefetch-unfriendly cores, it is beneficial to turn off their prefetching.3) Coordinated Throttling: A coordinated solution combines prefetch throttling and CP. First, all prefetch-friendly cores are placed into a small shared partition while keeping their prefetchers enabled. They are typically not LLC sensitive and a small partition results in pretty high performance. Meanwhile, the remaining cores share the whole cache capacity (note that we are using overlapping partitioning). Second, prefetch-unfriendly cores are identified and a group-level throttling is applied as discussed above. Other implementations are possible. Figure6shows the available options.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 11 Fig. 9 :Fig. 10 :</head><label>11910</label><figDesc>Fig. 11 shows the normalized HS and WS to baseline of three different coordinatedly throttling mechanisms: CMMa, CMM-b and CMM-c, which correspond to (a),(b) and (c) in Fig. 6. CMM-a puts Agg set into a small partition and applies PT to the prefetch unfriendly cores in the set. CMM-b puts only the prefetch friendly cores in Agg set into a small</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 :Fig. 12 :Fig. 13 :</head><label>111213</label><figDesc>Fig. 11: Normalized HS and WS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: Normalized Bandwidth Consumption for workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Multiple Metrics</figDesc><table><row><cell>No.</cell><cell>Metric</cell><cell>Definition</cell><cell>Description</cell></row><row><cell>M-1</cell><cell>L2-LLC-traffic</cell><cell>L2 pref miss</cell><cell>The sum of both demand</cell></row><row><cell></cell><cell></cell><cell>+ L2 dm miss</cell><cell>and prefetch requests</cell></row><row><cell></cell><cell></cell><cell></cell><cell>between L2 and LLC</cell></row><row><cell>M-2</cell><cell>L2 pref miss</cell><cell>L2 pref miss</cell><cell>The fraction of the prefetch</cell></row><row><cell></cell><cell>frac</cell><cell cols="2">/L2-LLC-traffic requests</cell></row><row><cell>M-3</cell><cell>L2 pref miss</cell><cell>L2 pref miss</cell><cell>L2 prefetch requests</cell></row><row><cell></cell><cell>traffic rate</cell><cell>per second</cell><cell>arriving at LLC per sec</cell></row><row><cell></cell><cell>(L2 PTR)</cell><cell></cell><cell></cell></row><row><cell>M-4</cell><cell>pref gen ablity</cell><cell>L2 pref req</cell><cell>Measures the ability of an</cell></row><row><cell></cell><cell>(PGA)</cell><cell>/L2 dm req</cell><cell>application to generate L2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>prefetches</cell></row><row><cell>M-5</cell><cell>L2 pref miss</cell><cell>L2 pref miss</cell><cell>The fraction of prefetches</cell></row><row><cell></cell><cell>rate (L2 PMR)</cell><cell>/L2 pref req</cell><cell>missing L2</cell></row><row><cell>M-6</cell><cell>L2 PPM</cell><cell>L2 pref req</cell><cell>Prefetches issued per</cell></row><row><cell></cell><cell></cell><cell>/L2 dm miss</cell><cell>demand miss</cell></row><row><cell>M-7</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>LLC-Mem-pref-BW -L3 load An approximation of LLC traffic (LLC PT) miss*64 to memory prefetch traffic ? L2 pref req Number of prefetch requests generated by the adjacent prefetcher and streamer prefetcher in an L2 cache. ? L2 pref miss Number of L2 prefetch requests that miss the local L2 cache. Only these missed requests will arrive at LLC. ? L2 dm req Number of demand requests that arrive at L2 cache. ? L2 dm miss Number of L2 demand requests that miss L2 cache. ? L3 load miss Number of load micro-ops that miss LLC and issue to memory.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The term "memory intensive" is not used because it may not reflect the real working set of a program in the presence of prefetching.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The same program may show a low prefetch accuracy for a different prefetcher.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>By "scalable", we mean it can be applied to more cores.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">E5</forename><surname>Intel</surname></persName>
		</author>
		<ptr target="https://ark.intel.com/products/series/91287/Intel-Xeon-Processor-E5" />
		<title level="m">Intel? xeon? processor e5-2620 v4</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Characterization and dynamic mitigation of intra-application cache interference</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intl. Symposium on Performance Analysis of Systems and Software</title>
		<meeting>the IEEE Intl. Symposium on Performance Analysis of Systems and Software</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Making data prefetch smarter: Adaptive prefetching on power7</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gioiosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>O'connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intl. Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the IEEE Intl. Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Machine learning-based prefetch optimization for data center applications</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Proceedings of the Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coordinated control of multiple prefetchers in systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Intl. Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>in in Proc. of the</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Increasing multicore system efficiency through intelligent bandwidth shifting</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Symposium on High Performance Computer Architecture</title>
		<meeting>of the IEEE Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Caffeine: A utility-driven prefetcher aggressiveness engine for multicores</title>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spac: A synergistic prefetcher aggressiveness controller for multi-core systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3740" to="3753" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Techniques for bandwidthefficient prefetching of linked data structures in hybrid prefetching systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA 2009. IEEE 15th International Symposium on</title>
		<title level="s">High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="7" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Band-pass prefetching: an effective prefetch management mechanism using prefetch-fraction metric in multi-core systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arep: Adaptive resource efficient prefetching for maximizing multicore performance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Laurenzanoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Parallel Architecture and Compilation (PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="367" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ibm power7 multicore server processor</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sinharoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cargnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Norstrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stuecheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leenstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Guthrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal partitioning of cache memory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1054" to="1068" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Utility-based cache partitioning: A low-overhead, high-performance, runtime mechanism to partition shared caches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<idno>MICRO-39. 39</idno>
	</analytic>
	<monogr>
		<title level="m">th Annual IEEE/ACM International Symposium on</title>
		<editor>
			<persName><surname>Microarchitecture</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Intel 3B, Intel? 64 and IA-32 Architectures Software Developer&apos;s</title>
		<imprint>
			<date type="published" when="2016-06">June, 2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="253669" to="253059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hardware evaluation of cache partitioning to improve utilization and energy-efficiency while preserving responsiveness</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="308" to="319" />
			<date type="published" when="2013">2013</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Application clustering policies to address system fairness with intel&apos;s cache allocation technology</title>
		<author>
			<persName><forename type="first">V</forename><surname>Selfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sahuquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>G?mez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kpart: A hybrid cache partitioning-sharing technique for commodity multicores</title>
		<author>
			<persName><forename type="first">N</forename><surname>El-Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on</title>
		<title level="s">High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="104" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Intel? xeon? scalable processors</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://ark.intel.com/products/series/125191/Intel-Xeon-Scalable-Processors" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feedback directed prefetching: Improving the performance and bandwidth-efficiency of hardware prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Symposium on High Performance Computer Architecture</title>
		<meeting>of the IEEE Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SPEC CPU 2006 benchmark Suite</title>
		<author>
			<persName><surname>Spec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPEC.org</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">System-level performance metrics for multiprogram workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The application slowdown model: Quantifying and controlling the impact of inter-application interference at shared caches and main memory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Symposium on Microarchitecture</title>
		<meeting>the 48th International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="62" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Studying the impact of hardware prefetching and bandwidth partitioning in chip-multiprocessors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM Intl. Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>of the ACM Intl. Conference on Measurement and Modeling of Computer Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">E5</forename><surname>Intel</surname></persName>
		</author>
		<ptr target="https://ark.intel.com/products/" />
		<title level="m">Intel? xeon? processor e5-2620 v4</title>
		<imprint>
			<date type="published" when="0210">92986/Intel-Xeon-Processor-E5-2620v4-20M-Cache-2 1 0 -GHz, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fairness enforcement in switch-on event multithreading</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ACM Trans. Architecture and Code Optimization</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Balancing thoughput and fairness in smt processors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gummaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Performance Analysis of Systems and Software</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001. 2001</date>
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
	<note>IEEE International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fairness metrics for multi-threaded processors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vandierendonck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="7" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fairness-aware scheduling on single-isa heterogeneous multi-cores</title>
		<author>
			<persName><forename type="first">K</forename><surname>Van Craeynest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Architectures and Compilation Techniques (PACT), 2013 22nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">To hardware prefetch or not to prefetch? a virtualized environment study and core binding approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM Conference on Architectural Support for Programming Languages and Systems</title>
		<meeting>of the ACM Conference on Architectural Support for Programming Languages and Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abs: A low-cost adaptive controller for prefetching in a banked shared last-level cache</title>
		<author>
			<persName><forename type="first">J</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ib?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vi?als</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Llaber?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ac/dc: An adaptive data cache prefetcher</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Dhodapkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the 13th International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="135" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Memory prefetching using adaptive stream detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<idno>MICRO-39. 39</idno>
	</analytic>
	<monogr>
		<title level="m">th Annual IEEE/ACM International Symposium on</title>
		<editor>
			<persName><surname>Microarchitecture</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="397" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Access map pattern matching for data cache prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Supercomputing</title>
		<meeting>the 23rd international conference on Supercomputing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="499" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiple stream tracker: a new hardware stride prefetcher</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM Intl.Conference on Computing Frontiers</title>
		<meeting>of the ACM Intl.Conference on Computing Frontiers</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sandbox prefetching: Safe run-time evaluation of aggressive prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/ACM Symposium on High Performance Computer Architecture</title>
		<meeting>of the IEEE/ACM Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2016 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reducing cache pollution via dynamic data prefetch filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A thread-aware adaptive data prefetcher</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Design (ICCD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="278" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A pab-based multi-prefetcher mechanism</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Birk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="188" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Courteous cache sharing: Being nice to others in capacity management</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srikantaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation Conference (DAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coordinated management of multiple interacting resources in chip multiprocessors: A machine learning approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bitirgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">st IEEE/ACM International Symposium on</title>
		<editor>
			<persName><surname>Microarchitecture</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
	<note>MICRO-41</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Prefetch-aware shared resource management for multi-core systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="141" to="152" />
			<date type="published" when="2011">2011</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Qos policies and architecture for cache/memory in cmp platforms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Illikkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM Intl</title>
		<meeting>of the ACM Intl</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Conference on Measurement and Modeling of Computer Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Managing contention for shared resources on multicore processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuravlev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Swap: Effective finegrain management of shared last-level caches with minimum hardware support</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
