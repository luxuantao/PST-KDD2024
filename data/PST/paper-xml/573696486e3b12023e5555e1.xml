<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Petuum: A New Platform for Distributed Machine Learning on Big Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qirong</forename><surname>Ho</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><forename type="middle">Kyu</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinliang</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Seunghak</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abhimanu</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
						</author>
						<title level="a" type="main">Petuum: A New Platform for Distributed Machine Learning on Big Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TBDATA.2015.2472014</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2015.2472014, IEEE Transactions on Big Data This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2015.2472014, IEEE Transactions on Big Data This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2015.2472014, IEEE Transactions on Big Data This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2015.2472014, IEEE Transactions on Big Data</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Big Data</term>
					<term>Big Model</term>
					<term>Distributed Systems</term>
					<term>Theory</term>
					<term>Data-Parallelism</term>
					<term>Model-Parallelism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>What is a systematic way to efficiently apply a wide spectrum of advanced ML programs to industrial scale problems, using Big Models (up to 100s of billions of parameters) on Big Data (up to terabytes or petabytes)? Modern parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce, or even specialized graph-based execution that relies on graph representations of ML programs. The variety of approaches tends to pull systems and algorithms design in different directions, and it remains difficult to find a universal platform applicable to a wide range of ML programs at scale. We propose a general-purpose framework, Petuum, that systematically addresses data-and model-parallel challenges in large-scale ML, by observing that many ML programs are fundamentally optimization-centric and admit error-tolerant, iterative-convergent algorithmic solutions. This presents unique opportunities for an integrative system design, such as bounded-error network synchronization and dynamic scheduling based on ML program structure. We demonstrate the efficacy of these system designs versus well-known implementations of modern ML algorithms, showing that Petuum allows ML programs to run in much less time and at considerably larger model sizes, even on modestly-sized compute clusters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>M ACHINE Learning (ML) is becoming a primary mech- anism for extracting information from data. However, the surging volume of Big Data from Internet activities and sensory advancements, and the increasing needs for Big Models for ultra high-dimensional problems have put tremendous pressure on ML methods to scale beyond a single machine, due to both space and time bottlenecks. For example, on the Big Data front, the Clueweb 2012 web crawl 1 contains over 700 million web pages as 27TB of text data; while photo-sharing sites such as Flickr, Instagram and Facebook are anecdotally known to possess 10s of billions of images, again taking up TBs of storage. It is highly inefficient, if possible, to use such big data sequentially in a batch or scholastic fashion in a typical iterative ML algorithm. On the Big Model front, state-of-the-art image recognition systems have now embraced large-scale deep learning models with billions of parameters <ref type="bibr" target="#b0">[1]</ref>; topic models with up to 10 6 topics can cover long-tail semantic word sets for substantially improved online advertising <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>; and very-high-rank matrix factorization yields improved prediction on collaborative filtering problems <ref type="bibr" target="#b3">[4]</ref>. Training such big models with a single machine can be prohibitively slow, if not impossible. While careful model design and feature engineering can certainly reduce the size of the model, they require domain-specific expertise and are fairly labor-intensive, hence the recent appeal (as seen in the above papers) of building high-capacity Big Models in order to substitute computation cost for labor cost.</p><p>• Qirong Ho is with Institute of Infocomm Research, A*STAR Singapore. Despite the recent rapid development of many new ML models and algorithms aiming at scalable applications <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, adoption of these technologies remains generally unseen in the wider data mining, NLP, vision, and other application communities for big problems, especially those built on advanced probabilistic or optimization programs. A likely reason for such a gap, at least from the scalable execution point of view, that prevents many state-ofthe-art ML models and algorithms from being more widely applied at Big-Learning scales is the difficult migration from an academic implementation, often specialized for a small, well-controlled computer platform such as desktop PCs and small lab-clusters, to a big, less predictable platform such as a corporate cluster or the cloud, where correct execution of the original programs require careful control and mastery of low-level details of the distributed environment and resources through highly nontrivial distributed programming.</p><p>Many programmable platforms have provided partial solutions to bridge this research-to-production gap: while Hadoop <ref type="bibr" target="#b10">[11]</ref> is a popular and easy to program platform, its implementation of MapReduce requires the program state to be written to disk every iteration, thus its performance on many ML programs has been surpassed by in-memory alternatives <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. One such alternative is Spark <ref type="bibr" target="#b11">[12]</ref>, which improves upon Hadoop by keeping ML program state in memory -resulting in large performance gains over Hadoop -whilst preserving the easy-to-use MapReduce programming interface. However, Spark ML implementations are often still slower than specially-designed ML implementations, in part because Spark does not offer flexible and fine-grained scheduling of computation and communication, which has been shown to be hugely advantageous, if not outright necessary, for fast and correct execution of advanced ML algorithms <ref type="bibr" target="#b13">[14]</ref>. Graph-centric Fig. <ref type="figure">1</ref>. The scale of Big ML efforts in recent literature. A key goal of Petuum is to enable larger ML models to be run on fewer resources, even relative to highly-specialized implementations.</p><p>platforms such as GraphLab <ref type="bibr" target="#b12">[13]</ref> and Pregel <ref type="bibr" target="#b14">[15]</ref> efficiently partition graph-based models with built-in scheduling and consistency mechanisms, but due to limited theoretical work, it is unclear whether asynchronous graph-based consistency models and scheduling will always yield correct execution of ML programs. Other systems provide lowlevel programming interfaces <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, that, while powerful and versatile, do not yet offer higher-level general-purpose building blocks such as scheduling, model partitioning strategies, and managed communication that are key to simplifying the adoption of a wide range of ML methods. In summary, existing systems supporting distributed ML each manifest a unique tradeoff on efficiency, correctness, programmability, and generality.</p><p>In this paper, we explore the problem of building a distributed machine learning framework with a new angle toward the efficiency, correctness, programmability, and generality tradeoff. We observe that, a hallmark of most (if not all) ML programs is that they are defined by an explicit objective function over data (e.g., likelihood, errorloss, graph cut), and the goal is to attain optimality of this function, in the space defined by the model parameters and other intermediate variables. Moreover, these algorithms all bear a common style, in that they resort to an iterativeconvergent procedure (see Eq. 1). It is noteworthy that iterative-convergent computing tasks are vastly different from conventional programmatic computing tasks (such as database queries and keyword extraction), which reach correct solutions only if every deterministic operation is correctly executed, and strong consistency is guaranteed on the intermediate program state -thus, operational objectives such as fault tolerance and strong consistency are absolutely necessary. However, an ML program's true goal is fast, efficient convergence to an optimal solution, and we argue that fine-grained fault tolerance and strong consistency are but one vehicle to achieve this goal, and might not even be the most efficient one.</p><p>We present a new distributed ML framework, Petuum, built on an ML-centric optimization-theoretic principle, as opposed to various operational objectives explored earlier. We begin by formalizing ML algorithms as iterative-convergent programs, which encompass a large space of modern ML, such as stochastic gradient descent <ref type="bibr" target="#b17">[18]</ref> and coordinate descent <ref type="bibr" target="#b9">[10]</ref> for convex optimization problems, proximal methods <ref type="bibr" target="#b18">[19]</ref> for more complex constrained optimization, as well as MCMC <ref type="bibr" target="#b19">[20]</ref> and variational inference <ref type="bibr" target="#b6">[7]</ref> for inference in probabilistic models. To our knowledge, no existing programmable 2 platform has considered such a wide spectrum of ML algorithms, which exhibit diverse representation abstractions, model and data access patterns, and synchronization and scheduling requirements. So what are the shared properties across such a "zoo of ML algorithms"? We believe that the key lies in the recognition of a clear dichotomy between data (which is conditionally independent and persistent throughout the algorithm) and model (which is internally coupled, and is transient before converging to an optimum). This inspires a simple yet statistically-rooted bimodal approach to parallelism: data parallel and model parallel distribution and execution of a big ML program over a cluster of machines. This data parallel, model parallel approach keenly exploits the unique statistical nature of ML algorithms, particularly the following three properties: (1) Error tolerance -iterative-convergent algorithms are often robust against limited errors in intermediate calculations;</p><p>(2) Dynamic structural dependency -during execution, the changing correlation strengths between model parameters are critical to efficient parallelization; (3) Non-uniform convergence -the number of steps required for a parameter to converge can be highly skewed across parameters. The core goal of Petuum is to execute these iterative updates in a manner that quickly converges to an optimum of the ML program's objective function, by exploiting these three statistical properties of ML, which we argue are fundamental to efficient large-scale ML in cluster environments.</p><p>This design principle contrasts that of several existing programmable frameworks discussed earlier. For example, central to the Spark framework <ref type="bibr" target="#b11">[12]</ref> is the principle of perfect fault tolerance and recovery, supported by a persistent memory architecture (Resilient Distributed Datasets); whereas central to the GraphLab framework is the principle of local and global consistency, supported by a vertex programming model (the Gather-Apply-Scatter abstraction). While these design principles reflect important aspects of correct ML algorithm execution -e.g., atomic recoverability of each computing step (Spark), or consistency satisfaction for all subsets of model variables (GraphLab)some other important aspects, such as the three statistical properties discussed above, or perhaps ones that could be more fundamental and general, and which could open more room for efficient system designs, remain unexplored.</p><p>To exploit these properties, Petuum introduces three novel system objectives grounded in the aforementioned key properties of ML programs, in order to accelerate their convergence at scale: (1) Petuum synchronizes the parameter states with bounded staleness guarantees, thereby achieves provably correct outcomes due to the error-tolerant 2. Our discussion is focused on platforms which provide libraries and tools for writing new ML algorithms. Because programmability is an important criteria for writing new ML algorithms, we exclude ML software that does not allow new algorithms to be implemented on top of them, such as AzureML and Mahout. nature of ML, but at a much cheaper communication cost than conventional per-iteration bulk synchronization;</p><p>(2) Petuum offers dynamic scheduling policies that take into account the changing structural dependencies between model parameters, so as to minimize parallelization error and synchronization costs; and (3) Since parameters in ML programs exhibit non-uniform convergence costs (i.e. different numbers of updates required), Petuum prioritizes computation towards non-converged model parameters, so as to achieve faster convergence.</p><p>To demonstrate this approach, we show how dataparallel and model-parallel algorithms can be implemented on Petuum, allowing them to scale to large data/model sizes with improved algorithm convergence times. The experiments section provides detailed benchmarks on a range of ML programs: topic modeling, matrix factorization, deep learning, Lasso regression, and distance metric learning. These algorithms are only a subset of the full open-source Petuum ML library 3 -the PMLlib, which we will briefly discuss in this paper. As illustrated in Figure <ref type="figure">1</ref>, Petuum PMLlib covers a rich collection of advanced ML methods not usually seen in existing ML libraries; the Petuum platform enables PMLlib to solve a range of ML problems at large scales -scales that have only been previously attempted in a case-specific manner with corporate-scale efforts and resources -but using relatively modest clusters (10-100 machines) that are within reach of most ML practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES: ON DATA PARALLELISM AND MODEL PARALLELISM</head><p>We begin with a principled formulation of iterativeconvergent ML programs, which exposes a dichotomy of data and model, that inspires the parallel system architecture ( §3), algorithm design ( §4), and theoretical analysis ( §6) of Petuum. Consider the following programmatic view of ML as iterative-convergent programs, driven by an objective function.</p><p>Iterative-Convergent ML Algorithm: Given data D and a model objective function L (e.g. mean-squared loss, likelihood, margin), a typical ML problem can be grounded as executing the following update equation iteratively, until the model state (i.e., parameters and/or latent variables) A reaches some stopping criteria:</p><formula xml:id="formula_0">A (t) = F (A (t−1) , ∆ L (A (t−1) , D))<label>(1)</label></formula><p>where superscript (t) denotes the iteration counter. The update function ∆ L () (which improves the loss L) performs computation on data D and model state A, and outputs intermediate results to be aggregated with the current estimate of A by F () to produce the new estimate of A.</p><p>For simplicity, in the rest of the paper we omit L in the subscript with the understanding that all ML programs of our interest here bear an explicit loss function that can be used to monitor the quality of convergence to a solution, as opposed to heuristics or procedures not associated such a loss function. Also for simplicity, we focus on iterativeconvergent equations with an additive form:</p><formula xml:id="formula_1">A (t) = A (t−1) + ∆(A (t−1) , D),<label>(2)</label></formula><p>3. Petuum is available as open source at http://petuum.org. i.e. the aggregation F () is replaced with a simple addition.</p><p>The approaches we propose can also be applied to this general F ().</p><p>In large-scale ML, both data D and model A can be very large. Data-parallelism, in which data is divided across machines, is a common strategy for solving Big Data problems, while model-parallelism, which divides the ML model, is common for Big Models. Both strategies are not exclusive, and can be combined to tackle challenging problems with large data D and large model A. Hence, every Petuum ML program is either data-parallel, model-parallel, or dataand-model-parallel, depending on problem needs. Below, we discuss the (different) mathematical implications of each parallelism (see Figure <ref type="figure">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Parallelism</head><p>In data-parallel ML, the data D is partitioned and assigned to computational workers (indexed by p = 1..P ); we denote the p-th data partition by D p . The function ∆() can be applied to each data partition independently, and the results combined additively, yielding a data-parallel equation (left panel of Figure <ref type="figure">2</ref>):</p><formula xml:id="formula_2">A (t) = A (t−1) + P p=1 ∆(A (t−1) , D p ). (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>This form is commonly seen in stochastic gradient descent or sampling-based algorithms. For example, in distance metric learning optimized via stochastic gradient descent (SGD), the data pairs are partitioned over different workers, and the intermediate results (subgradients) are computed on each partition, before being summed and applied to the model parameters. A slightly modified form, A (t) = P p=1 ∆(A (t−1) , D p ), is used by some algorithms, such as variational EM.</p><p>Importantly, this additive updates property allows the updates ∆() to be computed at each local worker before transmission over the network, which is crucial because CPUs can produce updates ∆() much faster than they can be (individually) transmitted over the network. Additive updates are the foundation for a host of techniques to speed up dataparallel execution, such as minibatch, asynchronous and bounded-asynchronous execution, and parameter servers. Key to the validity of additivity of updates from different workers is the notion of independent and identically distributed (iid) data, which is assumed for many ML programs, and implies that each parallel worker contributes "equally" (in a statistical sense) to the ML algorithm's progress via ∆(), no matter which data subset D p it uses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Parallelism</head><p>In model-parallel ML, the model A is partitioned and assigned to workers p = 1..P and updated therein in parallel, running update functions ∆(). Because the outputs from each ∆() affect different elements of A (hence denoted now by ∆ p () to make explicit the parameter subset affected at worker p), they are first concatenated into a full vector of updates (i.e., the full ∆()), before aggregated with model parameter vector A (see right panel of Figure <ref type="figure">2</ref>):</p><formula xml:id="formula_4">A (t) = A (t−1) + Con ∆ p (A (t−1) , S (t−1) p (A (t−1) )) P p=1 ,<label>(4)</label></formula><p>where we have omitted the data D for brevity and clarity. Coordinate descent algorithms for regression and matrix factorization are a canonical example of model-parallelism. Each update function ∆ p () also takes a scheduling function S (t−1) p (A), which restricts ∆ p () to modify only a carefullychosen subset of the model parameters A. S (t−1) p (A) outputs a set of indices {j 1 , j 2 , . . . , }, so that ∆ p () only performs updates on A j1 , A j2 , . . . -we refer to such selection of model parameters as scheduling. In some cases, the additive update formula above can be replaced by a replace operator that directly replaces corresponding elements in A with ones in the concatenated update vector.</p><p>Unlike data-parallelism which enjoys iid data properties, the model parameters A j are not, in general, independent of each other (Figure <ref type="figure">2</ref>), and it has been established that modelparallel algorithms can only be effective if the parallel updates are restricted to independent (or weakly-correlated) parameters <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Hence, our definition of model-parallelism includes the global scheduling mechanism S p () that can select carefully-chosen parameters for parallel updating.</p><p>The scheduling function S() opens up a large design space, such as fixed, randomized, or even dynamicallychanging scheduling on the whole space, or a subset of, the model parameters. S() not only can provide safety and correctness (e.g., by selecting independent parameters and thus minimize parallelization error), but can offer substantial speed-up (e.g., by prioritizing computation onto nonconverged parameters). In the Lasso example, Petuum uses S() to select coefficients that are weakly correlated (thus preventing divergence), while at the same time prioritizing coefficients far from zero (which are more likely to be nonconverged).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementing Data-and Model-Parallel Programs</head><p>Data-and model-parallel programs exhibit a certain programming and systems desiderata: they are stateful, in that they continually update shared model parameters A. Thus, an ML platform needs to synchronize A across all running threads and processes, and this should be done via a highperformance, non-blocking asynchronous strategy that still guarantees convergence. If the program is model-parallel, it may require fine control over the order of parameter updates, in order to avoid non-convergence due to dependency violations -thus, the ML platform needs to provide fine-grained scheduling capability. We discuss some of the difficulties associated with achieving these desiderata.</p><p>Data-and model-parallel programs can certainly be written in a message-passing style, in which the programmer explicitly writes code to send and receive parameters over the network. However, we believe it is more desirable to provide a Distributed Shared Memory (DSM) abstraction, in which the programmer simply treats A like a global program variable, accessible from any thread/process in a manner similar to single-machine programming -no explict network code is required from the user, and all synchronization is done in the background. While DSM-like interfaces could be added to alternative ML platforms like Hadoop, Spark and GraphLab, these systems usually operate in either a bulk synchronous (prone to stragglers and blocking due to the high rate of update ∆() generation) or asynchronous (having no parameter consistency guarantee, and hence no convergence guarantee) fashion.</p><p>Model-parallel programs pose an additional challenge, in that they require fine-grained control over the parallel ordering of variable updates. Again, while it is completely possible to achieve such control via message-passing programming style, there is nevertheless an opportunity to provide a simpler abstraction, in which the user merely has to define the model scheduling function S (t−1) p (A). In such an abstraction, networking and synchronization code is again hidden from the user. While Hadoop and Spark provide easy-to-use abstractions, their design does not give users fine-grained control over the ordering of updatesfor example, MapReduce provides no control over the order in which mappers or reducers are executed. We note that GraphLab has a priority-based scheduler specialized for some model-parallel applications, but still does not expose a dedicated scheduling function S (t−1) p (A). One could certainly modify Hadoop's or Spark's built-in schedulers to expose the required level of control, but we do not consider this reasonable for the average ML practitioner without strong systems expertise.</p><p>These considerations make effective data-and modelparallel programming challenging, and there is an opportunity to abstract them away via a platform that is specifically focused on data/model-parallel ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PETUUM -A PLATFORM FOR DISTRIBUTED ML</head><p>A core goal of Petuum is to allow practitioners to easily implement data-parallel and model-parallel ML algorithms. Petuum provides APIs to key systems that make dataand model-parallel programming easier: (1) a parameter server system, which allows programmers to access global model state A from any machine via a convenient distributed shared-memory interface that resembles single-machine programming, and adopts a bounded-asychronous consistency model that preserves data-parallel convergence guarantees, thus freeing users from explicit network synchronization; (2) a scheduler, which allows fine-grained control over the parallel ordering of model-parallel updates ∆() -in essence, the scheduler allows users to define their own ML application consistency rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Programming Interface</head><p>Figure <ref type="figure">4</ref> shows pseudocode for a generic Petuum program, consisting of three user-written functions (in either C++ or Java): a central scheduler function schedule(), a parallel update function push() (analogous to Map in MapReduce), and a central aggregation function pull() (analogous to Reduce). Data-parallel programs can be written with just push(), while model-parallel programs are written with all three functions schedule(), push() and pull().</p><p>The model variables A are held in the parameter server, which can be accessed at any time from any function via the PS object. The PS object can be accessed from any function, and has 3 functions: PS.get() to read a parameter, PS.inc() to add to a parameter, and PS.put() to overwrite a parameter. With just these operations, the parameter server automatically ensures parameter consistency between all Petuum components; no additional user programming is necessary. In the example pseudocode, DATA is a placeholder for data D; this can be any 3rd-party data structure, database, or distributed file system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Petuum System Design</head><p>ML algorithms exhibit several principles that can be exploited to speed up distributed ML programs: dependency </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Staleness s=3</head><p>Updates eagerly pushed out to Worker 1, but not guaranteed to be visible Worker 1 is automatically blocked by the PS system until worker 2 reaches iter 4</p><p>Worker progress Fig. <ref type="figure">6</ref>. ESSP consistency model, used by the Parameter Server. Workers are allowed to run at different speeds, but are prevented from being more than s iterations apart. Updates from the most recent s iterations are "eagerly" pushed out to workers, but are not guaranteed to be visible.</p><p>structures between parameters, non-uniform convergence of parameters, and a limited degree of error tolerance <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Through schedule(), push() and pull(), Petuum allows practitioners to write dataparallel and model-parallel ML programs that exploit these principles, and can be scaled to Big Data and Big Model applications. The Petuum system comprises three components (Fig. <ref type="figure" target="#fig_2">5</ref>): parameter server, scheduler, and workers.</p><p>Parameter Server: The parameter server (PS) enables data-parallelism, by providing users with global read/write access to model parameters A, via a convenient distributed shared memory API that is similar to table-based or key-value stores. The PS API consists of three functions: PS.get(), PS.inc() and PS.put(). As the names suggest, the first function reads a part of the global A into local memory, while the latter two add or overwrite local changes into the global A.</p><p>To take advantage of ML error tolerance, the PS implements the Eager Stale Synchronous Parallel (ESSP) consistency model <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, which reduces network synchronization and communication costs, while maintaining boundedstaleness convergence guarantees implied by ESSP. The ESSP consistency model ensures that, if a worker reads from parameter server at iteration c, it will definitely receive all updates from all workers computed at and before iteration c − s − 1, where s is a staleness threshold -see Figure <ref type="figure">6</ref> for an illustration. In Section 6, we will cover theoretical guarantees enjoyed by ESSP consistency.</p><p>Scheduler:</p><p>The scheduler system enables modelparallelism, by allowing users to control which model parameters are updated by worker machines. This is performed through a user-defined scheduling function schedule() (corresponding to S (t−1) p ()), which outputs a set of parameters for each worker. The scheduler sends the identities of these parameters to workers via the scheduling control channel (Fig. <ref type="figure" target="#fig_2">5</ref>), while the actual parameter values are delivered through the parameter server system. In Sec- Using algorithm or model-specific criteria, the Petuum scheduler prioritizes a small subset of parameters from the full model A. This is followed by a dependency analysis on the prioritized subset: parameters are further subdivided into groups, where a parameter Ai in group gu is must be independent of all other parameters Aj in all other groups gv. This is illustrated as a graph partitioning, although the implementation need not actually construct a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scheduler System All model parameters Prioritize parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-specific</head><note type="other">Dependency Analysis</note><p>tion 6, we will discuss the theoretical guarantees enjoyed by model-parallel schedules.</p><p>Several common patterns for schedule design are worth highlighting: fixed-scheduling (schedule_fix()) dispatches model parameters A in a pre-determined order; static, round-robin schedules (e.g. repeatedly loop over all parameters) fit the schedule_fix() model. Dependencyaware (schedule_dep()) scheduling allows re-ordering of variable/parameter updates to accelerate model-parallel ML algorithms, e.g. in Lasso regression, by analyzing the dependency structure over model parameters A. Finally, prioritized scheduling (schedule_pri()) exploits uneven convergence in ML, by prioritizing subsets of variables U sub ⊂ A according to algorithm-specific criteria, such as the magnitude of each parameter, or boundary conditions such as KKT. These common schedules are provided as preimplemented software libraries, or users can opt to write their own schedule().</p><p>Workers: Each worker p receives parameters to be updated from schedule(), and then runs parallel update functions push() (corresponding to ∆()) on data D. While push() is being executed, the model state A can be easily synchronized with the parameter server, using the PS.get() and PS.inc() API. After the workers finish push(), the scheduler may use the new model state to generate future scheduling decisions.</p><p>Petuum intentionally does not enforce a data abstraction, so that any data storage system may be used -workers may read from data loaded into memory, or from disk, or over a distributed file system or database such as HDFS. Furthermore, workers may touch the data in any order desired by the programmer: in data-parallel stochastic algorithms, workers might sample one data point at a time, while in batch algorithms, workers might instead pass through all data points in one iteration. Through pseudocode, it can be seen that Petuum allows these algorithms to be easily realized on distributed clusters, without dwelling on low level system programming, or nontrivial recasting of our ML problems into representations such as RDDs or vertex programs. Instead our ML problems can be coded at a high level, more akin to Matlab or R. We round off with a brief description of how we used Petuum implement a couple of other ML algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PETUUM PARALLEL ALGORITHMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data-Parallel Distance Metric Learning</head><p>Let us first consider a large-scale Distance Metric Learning (DML) problem. DML improves the performance of other ML programs such as clustering, by allowing domain experts to incorporate prior knowledge of the form "data points x, y are similar (or dissimilar)" <ref type="bibr" target="#b24">[25]</ref> -for example, we could enforce that "books about science are different from books about art". The output is a distance function d(x, y) that captures the aforementioned prior knowledge. Learning a proper distance metric <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>  </p><formula xml:id="formula_5">(x − y) T M (x − y) s.t. (x − y) T M (x − y) ≥ 1, ∀(x, y) ∈ D M 0<label>(5)</label></formula><p>where M 0 denotes that M is required to be positive semidefinite. This optimization problem minimizes the Mahalanobis distances between all pairs labeled as similar, while separating dissimilar pairs with a margin of 1.</p><p>In its original form, this optimization problem is difficult to parallelize due to the constraint set. To create a data-parallel optimization algorithm and implement it on Petuum, we shall relax the constraints via slack variables (similar to SVMs). First, we replace M with L T L, and introduce slack variables ξ to relax the hard constraint in Eq.( <ref type="formula" target="#formula_5">5</ref>), yielding Using hinge loss, the constraint in Eq.( <ref type="formula" target="#formula_6">6</ref>) can be eliminated, yielding an unconstrained optimization problem:</p><formula xml:id="formula_6">min L (x,y)∈S L(x − y) 2 + λ (x,y)∈D ξ x,y s.t. L(x − y) 2 ≥ 1 − ξ x,y , ξ x,y ≥ 0, ∀(x, y) ∈ D<label>(6)</label></formula><formula xml:id="formula_7">min L (x,y)∈S L(x − y) 2 +λ (x,y)∈D max(0, 1 − L(x − y) 2 )<label>(7)</label></formula><p>Unlike the original constrained DML problem, this relaxation is fully data-parallel, because it now treats the dissimilar pairs as iid data to the loss function (just like the similar pairs); hence, it can be solved via data-parallel Stochastic Gradient Descent (SGD). SGD can be naturally parallelized over data, and we partition the data pairs onto P machines. Every iteration, each machine p randomly samples a minibatch of similar pairs S p and dissimilar pairs D p from its data shard, and computes the following update to L:</p><formula xml:id="formula_8">L p = (x,y)∈Sp 2L(x − y)(x − y) T − (a,b)∈Dp 2L(a − b)(a − b) T • I( L(a − b) 2 ≤ 1)<label>(8)</label></formula><p>where I(•) is the indicator function.</p><p>Figure <ref type="figure">8</ref> shows pseudocode for Petuum DML, which is simple to implement because the parameter server system PS abstracts away complex networking code under a simple get()/read() API. Moreover, the PS automatically ensures high-throughput execution, via a boundedasynchronous consistency model (Stale Synchronous Parallel) that can provide workers with stale local copies of the parameters L, instead of forcing workers to wait for network communication. Later, we will review the strong consistency and convergence guarantees provided by the SSP model.</p><p>Since DML is a data-parallel algorithm, only the parallel update push() needs to be implemented (Figure <ref type="figure">8</ref>). The scheduling function schedule() is empty (because every worker touches every model parameter L), and we do not need aggregation push() for this SGD algorithm. In our next example, we will show how schedule() and push() can be used to implement model-parallel execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model-Parallel Lasso</head><p>Lasso is a widely used model to select features in highdimensional problems, such as gene-disease association studies, or in online advertising via 1 -penalized regression <ref type="bibr" target="#b26">[27]</ref>. Lasso takes the form of an optimization problem:</p><formula xml:id="formula_9">min β (X, y, β) + λ j |β j |,<label>(9)</label></formula><p>where λ denotes a regularization parameter that determines the sparsity of β, and (•) is a non-negative convex loss function such as squared-loss or logistic-loss; we assume that X and y are standardized and consider (9) without  an intercept. For simplicity but without loss of generality, we let (X, y, β) = 1 2 y − Xβ 2 2 ; other loss functions (e.g. logistic) are straightforward and can be solved using the same approach <ref type="bibr" target="#b9">[10]</ref>. We shall solve this via a coordinate descent (CD) model-parallel approach, similar but not identical to <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>The simplest parallel CD Lasso , shotgun <ref type="bibr" target="#b9">[10]</ref>, selects a random subset of parameters to be updated in parallel. We now present a scheduled model-parallel Lasso that improves upon shotgun: the Petuum scheduler chooses parameters that are nearly independent with each other <ref type="foot" target="#foot_1">4</ref> , thus guaranteeing convergence of the Lasso objective. In addition, it prioritizes these parameters based on their distance to convergence, thus speeding up optimization.</p><p>Why is it important to choose independent parameters via scheduling? Parameter dependencies affect the CD update equation in the following manner: by taking the gradient of (9), we obtain the CD update for β j :  </p><formula xml:id="formula_10">δ (t) ij ← x ij y i − k =j x ij x ik β (t−1) k ,<label>(10)</label></formula><formula xml:id="formula_11">β (t) j ← S( N i=1 δ (t) ij , λ),<label>(11)</label></formula><p>where S(•, λ) is a soft-thresholding operator, defined by S(β j , λ) ≡ sign(β) (|β| − λ). In <ref type="bibr" target="#b10">(11)</ref>, if x T j x k = 0 (i.e., nonzero correlation) and β (t−1) j = 0 and β (t−1) k = 0, then a coupling effect is created between the two features β j and β k . Hence, they are no longer conditionally independent given the data: β j ⊥ β k |X, y. If the j-th and the k-th coefficients are updated concurrently, parallelization error may occur, causing the Lasso problem to converge slowly (or even diverge outright).</p><p>Petuum's schedule(), push() and pull() interface is readily suited to implementing scheduled modelparallel Lasso. We use schedule() to choose parameters with low dependency, and to prioritize non-converged parameters. Petuum pipelines schedule() and push(); thus schedule() does not slow down workers running push(). Furthermore, by separating the scheduling code schedule() from the core optimization code push() and pull(), Petuum makes it easy to experiment with complex scheduling policies that involve prioritization and dependency checking, thus facilitating the implementation of new model-parallel algorithms -for example, one could use schedule() to prioritize according to KKT conditions in a constrained optimization problem, or to perform graphbased dependency checking like in Graphlab <ref type="bibr" target="#b12">[13]</ref>. Later, we will show that the above Lasso schedule schedule() is guaranteed to converge, and gives us near optimal solutions by controlling errors from parallel execution. The pseudocode for scheduled model parallel Lasso under Petuum is shown in Figure <ref type="figure" target="#fig_7">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Topic Model (LDA):</head><p>Topic Modeling uncovers semantically-coherent topics from unstructured document corpora, and is widely used in industry -e.g. Yahoo's YahooLDA <ref type="bibr" target="#b27">[28]</ref>, and Google's Rephil <ref type="bibr" target="#b28">[29]</ref>. The most well-known member of the topic modeling family is Latent Dirichlet Allocation (LDA): given a corpus of N documents and a pre-specified K for number of topics, the objective of LDA inference is to output K "topics" (discrete distributions over V unique words in the corpus), as well as the topic distribution of each document (a discrete distribution over topics).</p><p>One popular LDA inference technique is collapsed Gibbs sampling, a Markov Chain Monte Carlo algorithm that samples the topic assignments for each "token" (word position) in each document until convergence. This is an iterativeconvergent algorithm that repeatedly updates three types of model state parameters: an M -by-K "word-topic table" V , an N -by-K "doc-topic" table U , and the token topic assignments z ij . The LDA Gibbs sampler update is</p><formula xml:id="formula_12">P (z ij = k | U, V ) ∝ α+U ik Kα+ K =1 U i + β+V w ij ,k M β+ M m=1 V mk ,<label>(12)</label></formula><p>where z ij are topic assignments to each word "token" w ij in document i. The document word tokens w ij , topic assignments z ij and doc-topic table rows U i are partitioned across worker machines and kept fixed, as is common practice with Big Data. Although there are multiple parameters, the only one that is read and updated by all parallel worker (and hence needs to be globally-stored) is the word-topic table <ref type="table">V</ref> . We adopt a model-parallel approach to LDA, and use a schedule() (Algorithm 10) that cycles rows of the wordtopic table (rows correspond to different words, e.g. "machine" or "learning") across machines, to be updated via push() and pull(); data is kept fixed at each machine. This schedule() ensures that no two workers will ever touch the same rows of V in the same iteration 5 , unlike previous LDA implementations <ref type="bibr" target="#b27">[28]</ref> which allow workers to update any parameter, resulting in dependency violations.</p><p>Note that the function LDAGibbsSample() in push() can be replaced with any recent state-of-the art Gibbs sampling algorithm, such as the fast Metropolis-Hastings algorithm in LightLDA <ref type="bibr" target="#b2">[3]</ref>. Our experiments use the SparseLDA algorithm <ref type="bibr" target="#b29">[30]</ref>, to ensure a fair comparison with Ya-hooLDA <ref type="bibr" target="#b27">[28]</ref> (which also uses SparseLDA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Matrix Factorization (MF):</head><p>MF is used in recommendation, where users' item preferences are predicted based on other users' partially observed ratings. The MF algorithm decomposes an incomplete observation matrix X N ×M into two smaller matrices W ∈ R K×N and H ∈ R K×M such that W T H ≈ X, where K min{M, N } is a user-specified rank:</p><formula xml:id="formula_13">min W,H (i,j)∈Ω ||X ij − w T i h j || 2 + Reg(W, H),<label>(13)</label></formula><p>where Reg(W, H) is a regularizer such as the Frobenius norm, and Ω indexes the observed entries in X. High-rank decompositions of large matrices provide improved accuracy <ref type="bibr" target="#b3">[4]</ref>, and can be solved by a model-parallel stochastic gradient approach that ensures workers never touch the 5. Petuum LDA's "cyclic" schedule differs from the model streaming in <ref type="bibr" target="#b2">[3]</ref>; the latter has workers touch the same set of parameters, one set at a time. Model streaming can easily be implemented in Petuum, by changing schedule() to output the same word range for every jp.  same elements of W, H in the same iteration. There are two update equations, for W, H respectively:</p><formula xml:id="formula_14">δW ik = (a,b)∈Ω I(a = i) [−2X ab H kb + 2W a• H •b H kb ] , (<label>14</label></formula><formula xml:id="formula_15">)</formula><formula xml:id="formula_16">δH kj = (a,b)∈Ω I(b = j) [−2X ab W ak + 2W a• H •b W ak ] ,<label>(15)</label></formula><p>where I() is the indicator function.</p><p>Previous systems using this approach <ref type="bibr" target="#b17">[18]</ref> exhibited a load-balancing issue, because the rows of X exhibit a power-law distribution of non-zero entries; this was theoretically solved by the Fugue algorithm implemented on Hadoop <ref type="bibr" target="#b30">[31]</ref>, which essentially repeats the available data x ij at each worker to restore load balance. Petuum can implement Fugue SGD MF as Algorithm 11; we also provide an Alternating Least Squares implementation for comparison against other ALS-using systems like Spark and GraphLab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Other Algorithms</head><p>We have implemented other data-and model-parallel algorithms on Petuum as well. Here, we briefly mention a few algorithms whose data/model-parallel implementation on Petuum substantially differs from existing software. Many other ML algorithms are included in the Petuum opensource code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning (DL):</head><p>We implemented two types on Petuum: a general-purpose fully-connected Deep Neural Network (DNN) using the cross-entropy loss, and a Convolutional Neural Network (CNN) for image classification based off the open-source Caffe project. We adopt a dataparallel strategy schedule_fix(), where each worker uses its data subset to perform updates push() to the full model A. While this data-parallel strategy could be amenable to MapReduce, Spark and GraphLab, we are not aware of DL implementations on those platforms.</p><p>Logstic Regression (LR) and Support Vector Machines (SVM): Petuum implements LR and SVM using the same dependency-checking, prioritized model-parallel strategy as the Lasso Algorithm 9. Dependency checking and prioritization are not easily implemented on MapReduce and Spark. While GraphLab has support for these features; the key difference with Petuum is that Petuum's scheduler performs dependency checking on small subsets of parameters at a time, whereas GraphLab performs graph partitioning on all parameters at once (which can be costly).</p><p>Maximum Entropy Discrimination LDA (MedLDA): MedLDA <ref type="bibr" target="#b31">[32]</ref> is a constrained variant of LDA, that uses side information to constrain and improve the recovered topics. Petuum implements MedLDA using a data-parallel strategy schedule_fix(), where each worker uses push() to alternate between Gibbs sampling (like regular LDA) and solving for Lagrange multiplers associated with the constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BIG DATA ECOSYSTEM SUPPORT</head><p>To support ML at scale in production, academic, or cloudcompute clusters, Petuum provides a ready-to-run ML library, called PMLlib; Table <ref type="table">1</ref> shows the current list of ML applications, with more applications are actively being developed for future releases. Petuum also integrates with Hadoop ecosystem modules, thus reducing the engineering effort required to deploy Petuum in academic and realworld settings.</p><p>Many industrial and academic clusters run Hadoop, which provides, in addition to the MapReduce programming interface, a job scheduler that allows multiple programs to run on the same cluster (YARN) and a distributed filesystem for storing Big Data (HDFS). However, programs that are written for stand-alone clusters are not compatible with YARN/HDFS, and vice versa, applications written for YARN/HDFS are not compatible with stand alone clusters.</p><p>Petuum solves this issue by providing common libraries that work on either Hadoop or non-Hadoop clusters. Hence, all Petuum PMLlib applications (and new user-written applications) can be run in stand-alone mode or deployed as YARN jobs to be scheduled alongside other MapReduce jobs, and PMLlib applications can also read/write input data and output results from both the local machine filesystem as well as HDFS. More specifically, Petuum provides a YARN launcher that will deploy any Petuum application (including user-written ones) onto a Hadoop cluster; the YARN launcher will automatically restart failed Petuum jobs and ensure that they always complete. Petuum also provides a data access library with C++ iostreams (or Java file streams) for HDFS access, which allows users to write Petuum ML Library (PMLlib): ML applications and achievable problem scale for a given cluster size. Petuum's goal is to solve large model and data problems using medium-sized clusters with only 10s of machines (100-1000 cores, 1TB+ memory). Running time varies between 10s of minutes to several days, depending on the application. generic file stream code that works on both HDFS files the local filesystem. The data access library also provides preimplemented routines to load common data formats, such as CSV, libSVM, and sparse matrix. While Petuum PMLlib applications are written in C++ for maximum performance, new Petuum applications can be written in either Java or C++; Java has the advantages of easier deployment and a wider user base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">PRINCIPLES AND THEORY</head><p>Our iterative-convergent formulation of ML programs, and the explicit notion of data and model parallelism, make it convenient to explore three key properties of ML programs -error-tolerant convergence, non-uniform convergence, dependency structures (Fig. <ref type="figure" target="#fig_11">12</ref>) -and to analyze how Petuum exploits these properties in a theoretically-sound manner to speed up ML program completion at Big Learning scales.</p><p>Some of these properties have previously been successfully exploited by a number of bespoke, large-scale implementations of popular ML algorithms: e.g. topic models <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b16">[17]</ref>, matrix factorization <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, and deep learning <ref type="bibr" target="#b0">[1]</ref>. It is notable that MapReduce-style systems (such as Hadoop <ref type="bibr" target="#b10">[11]</ref> and Spark <ref type="bibr" target="#b11">[12]</ref>) often do not fare competitively against these custom-built ML implementations, and one of the reasons is that these key ML properties are difficult to exploit under a MapReduce-like abstraction. Other abstractions may offer a limited degree of opportunity -for example, vertex programming <ref type="bibr" target="#b12">[13]</ref> permits graph dependencies to influence model-parallel execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Error tolerant convergence</head><p>Data-parallel ML algorithms are often robust against minor errors in intermediate calculations; as a consequence, they still execute correctly even when their model parameters A experience synchronization delays (i.e. the P workers only see old or stale parameters), provided those delays are strictly bounded <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Petuum exploits this error-tolerance to reduce network communication/synchronization overheads substantially, by implementing the Stale Synchronous Parallel (SSP) consistency model <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref> on top of the parameter server system, which provides all machines with access to parameters A.</p><p>The SSP consistency model guarantees that if a worker reads from parameter server at iteration c, it is guaranteed to receive all updates from all workers computed at and before iteration c − s − 1, where s is the staleness threshold. If this is impossible because some straggling worker is more than s iterations behind, the reader will stop until the straggler catches up and sends its updates. For stochastic gradient descent algorithms (such as the DML program), SSP has very attractive theoretical properties <ref type="bibr" target="#b13">[14]</ref>, which we partially re-state here:</p><p>Theorem 1 (adapted from <ref type="bibr" target="#b13">[14]</ref>). SGD under SSP, convergence in probability: Let f (x) = T t=1 f t (x) be a convex function, where the f t are also convex. We search for a minimizer x * via stochastic gradient descent on each component ∇f t under SSP, with staleness parameter s and P workers. Let u t := −η t ∇ t f t (x t ) with η t = η √ t . Under suitable conditions (f t are L-Lipschitz and bounded divergence D(x||x ) ≤ F 2 ), we have</p><formula xml:id="formula_17">P R [X] T − 1 √ T ηL 2 + F 2 η + 2ηL 2 µ γ ≥ τ ≤ exp −T τ 2 2η T σ γ + 2 3 ηL 2 (2s + 1)P τ where R[X] := T t=1 f t (x t ) − f (x * ), and ηT = η 2 L 4 (ln T +1) T = o(1) as T → ∞.</formula><p>This theorem has two implications: (1) learning under the SSP model is correct (like Bulk Synchronous Parallel learning), because</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R[X] T</head><p>-which is the difference between the SSP parameter estimate and the true optimum -converges to O(T −1/2 ) in probability with an exponential tail-bound;</p><p>(2) keeping staleness (i.e. asynchrony) as low as possible improves per-iteration convergence -specifically, the bound becomes tighter with lower maximum staleness s, and lower average µ γ and variance σ γ of the staleness experienced by the workers. Conversely, naive asynchronous systems (e.g. Hogwild! <ref type="bibr" target="#b34">[35]</ref> and YahooLDA <ref type="bibr" target="#b27">[28]</ref>) may experience poor convergence, particularly in production environments where machines may slow down due to other tasks or users. Such slowdown can cause the maximum staleness s and staleness variance σ γ to become arbitrarily large, leading to poor convergence rates. In addition to the above theorem (which bounds the distribution of x), Dai et al. also showed that the variance of x can be bounded, ensuring reliability and stability near an optimum <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dependency structures</head><p>Naive parallelization of model-parallel algorithms (e.g. coordinate descent) may lead to uncontrolled parallelization error and non-convergence, caused by inter-parameter dependencies in the model. The mathematical definition of dependency differs between algorithms and models; examples include the Markov Blanket structure of graphical models (explored in GraphLab <ref type="bibr" target="#b12">[13]</ref>) and deep neural networks (partially considered in <ref type="bibr" target="#b4">[5]</ref>), or the correlation between data dimensions in regression problems (explored in <ref type="bibr" target="#b21">[22]</ref>).</p><p>Such dependencies have been thoroughly analyzed under fixed execution schedules (where each worker updates the same set of parameters every iteration) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b35">[36]</ref>, but there has been little research on dynamic schedules that can react to changing model dependencies or model state A. Petuum's scheduler allows users to write dynamic scheduling functions S (t) p (A (t) ) -whose output is a set of model indices {j 1 , j 2 0, . . . }, telling worker p to update A j1 , A j2 , . . . -as per their application's needs. This enables ML programs to analyze dependencies at run time (implemented via schedule()), and select subsets of independent (or nearly-independent) parameters for parallel updates.</p><p>To motivate this, we consider a generic optimization problem, which many regularized regression problemsincluding the Petuum Lasso example -fit into: Definition: Regularized Regression Problem (RRP)</p><formula xml:id="formula_18">min w∈R d f (w) + r(w),<label>(16)</label></formula><p>where w is the parameter vector, r(w) = i r(w i ) is separable and f has β-Lipschitz continuous gradient in the following sense:</p><formula xml:id="formula_19">f (w + z) ≤ f (w) + z ∇f (w) + β 2 z X Xz,<label>(17)</label></formula><p>where X = [x 1 , . . . , x d ] are d feature vectors. W.l.o.g., we assume that each feature vector x i is normalized, i.e.,</p><formula xml:id="formula_20">x i 2 = 1, i = 1, . . . , d. Therefore |x i x j | ≤ 1 for all i, j.</formula><p>In the regression setting, f (w) represents a leastsquares loss, r(w) represents a separable regularizer (e.g.</p><p>1 penalty), and x i represents the i-th feature column of the design (data) matrix, each element in x i is a separate data sample. In particular, |x i x j | is the correlation between the i-th and j-th feature columns. The parameters w are simply the regression coefficients.</p><p>In the context of the model-parallel equation ( <ref type="formula" target="#formula_4">4</ref>), we can map the model A = w, the data D = X, and the update equation ∆(A, S p (A)) to</p><formula xml:id="formula_21">w + jp ← arg min z∈R β 2 [z − (w jp − 1 β g jp )] 2 + r(z),<label>(18) where S (t)</label></formula><p>p (A) has selected a single coordinate j p to be updated by worker p -thus, P coordinates are updated in every iteration. The aggregation function F () simply allows each update w jp to pass through without change.</p><p>The effectiveness of parallel coordinate descent depends on how the schedule S (t) p () selects the coordinates j p . In particular, naive random selection can lead to poor convergence rate or even divergence, with error proportional to the correlation |x ja x j b | between the randomly-selected coordinates j a , j b <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>. An effective and cheaply-computable schedule S (t) RRP,p () involves randomly proposing a small set of Q &gt; P features {j 1 , . . . , j Q }, and then finding P features in this set such that |x ja x j b | ≤ θ for some threshold θ, where j a , j b are any two features in the set of P . This requires at most O(B 2 ) evaluations of |x ja x j b | ≤ θ (if we cannot find P features that meet the criteria, we simply reduce the degree of parallelism). We have the following convergence theorem:</p><formula xml:id="formula_22">Theorem 2. S RRP () convergence: Let := d(E[P 2 ]/E[P −1])(ρ−1) d 2 ≈ (E[P −1])(ρ−1) d &lt; 1</formula><p>, where ρ is a constant that depends on the input data x and the scheduler S RRP (). After t steps, we have</p><formula xml:id="formula_23">E[F(w (t) ) − F (w )] ≤ Cdβ E[P(1 − )] 1 t ,<label>(19)</label></formula><p>where F (w) := f (w) + r(w) and w is a minimizer of</p><formula xml:id="formula_24">F . E[P]</formula><p>is the average degree of parallelization over all iterations -we say "average" to account for situations where the scheduler cannot select P nearly-independent parameters (due to high correlation in the data). The proof for this theorem can be found in the Appendix.</p><p>For most real-world data sets, this is not a problem, and E[P] is equal to the number of workers.</p><p>This theorem states that S RRP ()-scheduling (which is used by Petuum Lasso) achieves close to P -fold (linear) improvement in per-iteration convergence (where P is the number of workers). This comes from the 1/E[P (1 − )] factor on the RHS of Eq. ( <ref type="formula" target="#formula_23">19</ref>); for input data x that is sparse and high-dimensional, the S RRP () scheduler will cause ρ − 1 to become close to zero, and therefore will also be close to zero -thus the per-iteration convergence rate is improved by nearly P -fold. We contrast this against a naive system that selects coordinates at random; such a system will have far larger ρ − 1, thus degrading per-iteration convergence.</p><p>In addition to asymptotic convergence, we show that S RRP 's trajectory is close to ideal parallel execution: Theorem 3. S RRP () is close to ideal execution: Let S ideal () be an oracle schedule that always proposes P random features with zero correlation. Let w</p><formula xml:id="formula_25">(t)</formula><p>ideal be its parameter trajectory, and let w (t) RRP be the parameter trajectory of S RRP (). Then,</p><formula xml:id="formula_26">E[|w (t) ideal − w (t) RRP |] ≤ 2dP m (t + 1) 2 P L 2 X XC,<label>(20)</label></formula><p>for constants C, m, L, P . The proof for this theorem can be found in the Appendix.</p><p>This theorem says that the difference between the S RRP () parameter estimate w RRP and the ideal oracle estimate w ideal rapidly vanishes, at a fast 1/(t + 1) 2 rate. In other words, one cannot do much better than S RRP () scheduling -it is near-optimal. We close this section by noting that S RRP () is different from Scherrer et al. <ref type="bibr" target="#b21">[22]</ref>, who pre-cluster all M features before starting coordinate descent, in order to find "blocks" of nearly-independent parameters. In the Big Data and especially Big Model setting, feature clustering can be prohibitive -fundamentally, it requires O(M 2 ) evaluations of |x i x j | for all M 2 feature combinations (i, j), and although greedy clustering algorithms can mitigate this to some extent, feature clustering is still impractical when M is very large, as seen in some regression problems <ref type="bibr" target="#b26">[27]</ref>. The proposed S RRP () only needs to evaluate a small number of |x i x j | every iteration. Furthermore, the random selection in S RRP () can be replaced with prioritization to exploit nonuniform convergence in ML problems, as explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Non-uniform convergence</head><p>In model-parallel ML programs, it has been empirically observed that some parameters A j can converge in much fewer/more updates than other parameters <ref type="bibr" target="#b20">[21]</ref>. For instance, this happens in Lasso regression because the model enforces sparsity, so most parameters remain at zero throughout the algorithm, with low probability of becoming non-zero again. Prioritizing Lasso parameters according to their magnitude greatly improves convergence per iteration, by avoiding frequent (and wasteful) updates to zero parameters <ref type="bibr" target="#b20">[21]</ref>.</p><p>We call this non-uniform ML convergence, which can be exploited via a dynamic scheduling function S (t) p (A (t) ) whose output changes according to the iteration t -for instance, we can write a scheduler S mag () that proposes parameters with probability proportional to their current magnitude (A (t) j ) 2 . This S mag () can be combined with the earlier dependency structure checking, leading to a dependency-aware, prioritizing scheduler. Unlike the dependency structure issue, prioritization has not received as much attention in the ML literature, though it has been used to speed up the PageRank algorithm, which is iterative-convergent <ref type="bibr" target="#b36">[37]</ref>.</p><p>The prioritizing schedule S mag () can be analyzed in the context of the Lasso problem. First, we rewrite it by duplicating the original J features with opposite sign, as in <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_27">F (β) := min β 1 2 y − Xβ 2 2 + λ 2J j=1 β j .</formula><p>Here, X contains 2J features and β j ≥ 0, for all j = 1, . . . , 2J. Theorem 4. [Adapted from <ref type="bibr" target="#b20">[21]</ref>] Optimality of Lasso priority scheduler: Suppose B is the set of indices of coefficients updated in parallel at the t-th iteration, and ρ is sufficiently small constant such that ρδβ</p><formula xml:id="formula_28">(t) j δβ (t)</formula><p>k ≈ 0, for all j = k ∈ B. Then, the sampling distribution p(j) ∝ (δβ</p><formula xml:id="formula_29">(t) j ) 2 approximately maximizes a lower bound on E B [F (β (t) ) − F (β (t) + δβ (t) )].</formula><p>This theorem shows that a prioritizing scheduler will speed up Lasso convergence, by decreasing the objective as much as is theoretically possible every iteration.</p><p>In practice, the Petuum scheduler system approximates p(j) ∝ (δβ (t) j ) 2 with p (j) ∝ (β (t−1) j ) 2 + η, in order to allow pipelining of multiple iterations for faster real-time convergence 6 . The constant η ensures that all β j 's have a non-zero probability of being updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">PERFORMANCE</head><p>Petuum's ML-centric system design supports a variety of ML programs, and improves their performance on Big Data in the following senses: (1) Petuum ML implementations achieve significantly faster convergence rate than welloptimized single-machine baselines (i.e., DML implemented on single machine, and Shotgun <ref type="bibr" target="#b9">[10]</ref>); (2) Petuum ML implementations can run faster than other programmable platforms (e.g. Spark, GraphLab 7 ), because Petuum can exploit model dependencies, uneven convergence and error tolerance; (3) Petuum ML implementations can reach larger model sizes than other programmable platforms, because Petuum stores ML program variables in a lightweight fashion (on the parameter server and scheduler); (4) for ML programs without distributed implementations, we can implement them on Petuum and show good scaling with an increasing number of machines. We emphasize that Petuum is, for the moment, primarily about allowing ML practitioners to implement and experiment with new data/modelparallel ML algorithms on small-to-medium clusters. Our experiments are therefore focused on clusters with 10-100 machines, in accordance with our target users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Hardware Configuration</head><p>To demonstrate that Petuum is adaptable to different hardware generations, our experiments used 3 clusters with varying specifications: Cluster-1 has up to 128 machines with 2 AMD cores, 8GB RAM, 1Gbps Ethernet; Cluster-2 has up to 16 machines with 64 AMD cores, 128GB RAM, 40Gbps Infiniband; Cluster-3 has up to 64 machines with 16 Intel cores, 128GB RAM, 10Gbps Ethernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Parameter Server and Scheduler Performance</head><p>Petuum's Parameter Server (PS) and Scheduler speed up existing ML algorithms by improving iteration throughput and iteration quality respectively. We measure iteration throughput as "iterations executed per second", and we quantify iteration quality by plotting the ML objective function L against iteration number -"objective progress per iteration". In either case, the goal is to improve the ML algorithm's real-time convergence rate, quantified by plotting the objective function L against real time ("objective progress per second").</p><p>Parameter Server (PS): We consider how the PS improves iteration throughput (through the Eager SSP consistency model), evaluated using PLMlib's Matrix Factorization with the schedule() function disabled (in order to remove the beneficial effect of scheduling, so we may focus on the PS). This experiment was conducted using 64 Cluster-3 machines on a 332GB sparse matrix (7.7m by 288k entries, 26b nonzeros, created by duplicating the Netflix dataset 16 times horizontally and 16 times vertically). We compare 6. Without this approximation, pipelining is impossible because δβ (t) j is unavailable until all computations on β (t) j have finished. 7. We omit Hadoop and Mahout, as it is already well-established that Spark and GraphLab significantly outperform it <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. the performance of MF running under Petuum PS's Eager SSP mode (using staleness s = 2; higher staleness values did not produce additional benefit), versus running under MapReduce-style Bulk Synchronous Parallel (BSP) mode. Figure <ref type="figure" target="#fig_12">13</ref> shows that ESSP provides a 30% improvement to iteration throughput (top left), without a significantly affecting iteration quality (top right). Consequently, the MF application converges 30% faster in real time (middle left).</p><p>The iteration throughput improvement occurs because ESSP allows both gradient computation and inter-worker communication to occur at the same time, whereas classic BSP execution requires computation and communication to alternate in a mutually exclusive manner. Because the maximum staleness s = 2 is small, and because ESSP eagerly pushes parameter updates as soon as they are available, there is almost no penalty to iteration quality despite allowing staleness.</p><p>Scheduler: We examine how the Scheduler improves iteration quality (through a well-designed schedule() function), evaluated using PMLlib's Lasso application. This experiment was conducted using 16 Cluster-2 on a simulated 150GB sparse dataset (50m features); adjacent features in the dataset are highly correlated in order to simulate the effects of realistic feature engineering. We compare the original PMLlib Lasso (whose schedule() performs pri- oritization and dependency checking) to a simpler version whose schedule() selects parameters at random (the shotgun algorithm <ref type="bibr" target="#b9">[10]</ref>). Figure <ref type="figure" target="#fig_12">13</ref> shows that PMLlib Lasso's schedule() slightly decreases iteration throughput (middle right), but greatly improves iteration quality (bottom left), resulting in several orders of magnitude improvement to real-time convergence (bottom right).</p><p>The iteration quality improvement is mostly due to prioritization; we note that without prioritization, 85% of the parameters would converge within 5 iterations, but the remaining 15% would take over 100 iterations. Moreover, prioritization alone is not enough to achieve fast convergence speed -when we repeated the experiment with a prioritization-only schedule() (not shown), the parameters became unstable, which caused the objective function to diverged. This is because dependency checking is necessary to avoid correlation effects in Lasso (discussed in the proof to Theorem 2), which we observed were greatly amplified under the prioritization-only schedule().</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison to Programmable Platforms</head><p>Figure <ref type="figure" target="#fig_13">14</ref> (left) compares Petuum to popular platforms for writing new ML programs (Spark v1.2 and GraphLab), as well as a well-known cluster implementation of LDA (YahooLDA). We compared Petuum to Spark, GraphLab and YahooLDA on two applications: LDA and MF. We ran LDA on 128 Cluster-1 machines, using 3.9m English Wikipedia abstracts with unigram (V = 2.5m) and bigram (V = 21.8m) vocabularies; the bigram vocabulary is an example of feature engineering to improve performance at the cost of additional computation. The MF comparison was performed on 10 Cluster-2 machines using the original Netflix dataset.</p><p>Speed: For MF and LDA, Petuum is between 2-6 times faster than other platforms (Figures <ref type="figure" target="#fig_13">14, 15</ref>). For MF, Petuum uses the same model-parallel approach as Spark and GraphLab, but it performs twice as fast as Spark, while GraphLab ran out of memory (due to the need to construct an explicit graph representation, which consumes significant memory). On the other hand, Petuum LDA is nearly 6 times faster than YahooLDA; the speedup mostly comes from the Petuum LDA schedule() (Figure <ref type="figure" target="#fig_8">10</ref>), which performs correct model-parallel execution by only allowing each worker to operate on disjoint parts of the vocabulary. This is similar to GraphLab's implementation, but is far more memory-efficient because Petuum does not need to construct a full graph representation of the problem. Model Size: We show that Petuum supports larger ML models for the same amount of cluster memory. Figure <ref type="figure" target="#fig_2">15</ref> shows ML program running time versus model size, given a fixed number of machines -the left panel compares Petuum LDA and YahooLDA; PetuumLDA converges faster and supports LDA models that are &gt; 10 times larger 8 , allowing long-tail topics to be captured. The right panels compare Petuum MF versus Spark and GraphLab; again Petuum is faster and supports much larger MF models (higher rank) than either baseline. Petuum's model scalability comes from two factors: (1) model-parallelism, which divides the model across machines; (2) a lightweight parameter server system with minimal storage overhead. In contrast, Spark and GraphLab have additional overheads that may not be necessary in an ML context -Spark needs to construct a "lineage graph" in order to preserve its strict fault recovery guarantees, while GraphLab needs to represent the ML problem in graph form. Because ML applications are error-tolerant, fault recovery can be performed with lower overhead through periodic checkpointing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Fast Cluster Implementations of New ML Programs</head><p>Petuum facilitates the development of new ML programs without existing cluster implementations; here we present two case studies. The first is a cluster version of the opensource Caffe CNN toolkit, created by adding ∼ 600 lines of Petuum code. The basic data-parallel strategy in Caffe was left unchanged, so the Petuum port directly tests Petuum's efficiency. We tested on 4 Cluster-3 machines, using a 250k  Second, we compare the Petuum DML program against the original DML algorithm proposed in <ref type="bibr" target="#b24">[25]</ref> (denoted by Xing2002), implemented using SGD on a single machine (with parallelization over matrix operations). The intent is to show that, even for ML algorithms that have received less research attention towards scalability (such as DML), one can still implement a reasonably simple data-parallel SGD algorithm on Petuum, and enjoy the benefits of parallelization over a cluster. The DML experiment was run on 4 Cluster-2 machines, using the 1-million-sample Imagenet <ref type="bibr" target="#b37">[38]</ref> dataset with 1000 classes (21.5k-by-21.5k matrix with 220m model parameters), and 200m similar/dissimilar statements. The Petuum DML implementation converges 3.8 times faster than Xing2002 on 4 machines (Figure <ref type="figure" target="#fig_13">14</ref>, right plot). We also evaluated Petuum DML's convergence speed on 1-4 machines (Figure <ref type="figure">16</ref>) -compared to using 1 machine, Petuum DML achieves 3.8 times speedup with 4 machines and 1.9 times speedup with 2 machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">SUMMARY AND FUTURE WORK</head><p>Petuum provides ML practitioners with an ML library and ML programming platform, capable of handling Big Data and Big ML Models with performance that is competitive with specialized implementations, while running on reasonable cluster sizes (10-100 machines). This is made possible by systematically exploiting the unique properties of iterative-convergent ML algorithms -error tolerance, dependency structures and uneven convergence; these properties have yet to be thoroughly explored in general-purpose Big Data platforms such as Hadoop and Spark.</p><p>In terms of feature set, Petuum is still relatively immature compared to Hadoop and Spark, and lacks the following: fault recovery from partial program state (critical for scaling to 1000+ machines), ability to adjust resource usage on-the-fly in running jobs, scheduling jobs for multiple users (multi-tenancy), a unified data interface that closely integrates with databases and distributed file systems, and support for interactive scripting languages such as Python and R. The lack of these features imposes a barrier to entry for new users, and future work on Petuum will address these issues -but in a manner consistent with Petuum's focus on iterative-convergent ML properties. For example, fault recovery in ML does not require perfect, synchronous checkpoints (used in Hadoop and Spark); instead, checkpoints with ESSP-style bounded error consistency can be used. This in turn opens up new ways to achieve on-the-fly resource adjustment and multi-tenancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX PROOF OF THEOREM 2</head><p>We prove that the Petuum S RRP () scheduler makes the Regularized Regression Problem converge. We note that S RRP () has the following properties: (1) the scheduler uniformly randomly selects Q out of d coordinates (where d is the number of features); (2) the scheduler performs dependency checking and retains P out of Q coordinates; (3) in parallel, each of the P workers is assigned one coordinate, and performs coordinate descent on it:</p><formula xml:id="formula_30">w + jp ← arg min z∈R β 2 [z − (w jp − 1 β g jp )] 2 + r(z),<label>(21)</label></formula><p>where g j = ∇ j f (w) is the j-th partial derivative, and the coordinate j p is assigned to the p-th worker. Note that ( <ref type="formula" target="#formula_30">21</ref>) is simply the gradient update: w ← w − 1 β g, followed by applying the proximity operator of r.</p><p>As we just noted, S RRP () scheduling selects P coordinates out of Q by performing dependency checking: effectively, the scheduler will put coordinates i and j into the same "block" iff |x i x j | ≤ θ for some "correlation threshold" θ ∈ (0, 1). The idea is that coordinates in the same block will never be updated in parallel; the algorithm must choose the P coordinates from P distinct blocks. In order to analyze the effectiveness of this procedure, we will consider the following matrix:</p><formula xml:id="formula_31">∀i, A ii = 1, ∀i = j, A ij = x i x j , if |x i x j | ≤ θ 0, otherwise .<label>(22)</label></formula><p>This matrix A captures the impact of grouping coordinates into blocks, and its spectral radius ρ = ρ(A) will be used to show that scheduling entails a nearly P -fold improvement in convergence with P processors. A simple bound for the spectral radius ρ(A) is:</p><formula xml:id="formula_32">|ρ − 1| ≤ j =i |A ij | ≤ (d − 1)θ.<label>(23)</label></formula><p>S RRP () scheduling sets the correlation threshold θ to a small constant, causing the spectral radius ρ to also be small (which will lead to a nearly P -fold improvement in per-iteration convergence rate). We contrast S RRP () with random shotgun-style <ref type="bibr" target="#b9">[10]</ref> scheduling, which is equivalent to setting θ = 1; this causes ρ to become large, which will degrade the per-iteration convergence rate. Finally, let N denote the number of pairs (i, j) that pass the dependency check |x i x j | ≤ θ. In high-dimensional problems with over 100 million dimensions, it is often the case that N ≈ d 2 , because each coordinate i is unlikely to be correlated with more than a few other coordinates j. We therefore assume N ≈ d 2 for our analysis. We note that P , the number of coordinates selected for parallel update by the scheduler, is a random variable (because it may not always be possible to select P independent coordinates).</p><p>Our analysis therefore considers the expected value E[P].</p><p>We are now ready to prove Theorem 2: Theorem 2: Let := d(E[P 2 ]/E[P −1])(ρ−1) N ≈ (E[P −1])(ρ−1) d &lt; 1, then after t steps, we have</p><formula xml:id="formula_33">E[F(w (t) ) − F (w )] ≤ Cdβ E[P(1 − )] 1 t ,<label>(24)</label></formula><p>where F (w) := f (w) + r(w) and w denotes a (global) minimizer of F (whose existence is assumed for simplicity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 2:</head><p>We first bound the algorithm's progress at step t. To avoid cumbersome double indices, let w = w t and z = w t+1 . Then, by applying ( <ref type="formula" target="#formula_19">17</ref> where we define = d(E[P 2 ]/E[P −1])(ρ−1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>, and the second inequality follows from the optimality of w + as defined in <ref type="bibr" target="#b20">(21)</ref>. Therefore as long as &lt; 1, the algorithm is decreasing the objective. This in turn puts a limit on the maximum number of parallel workers, which is inversely proportional to the spectral radius ρ.</p><p>The rest of the proof follows the same line as the shotgun paper <ref type="bibr" target="#b9">[10]</ref>. Briefly, consider the case where 0 ∈ ∂r(w t ), then </p><p>Using induction it follows that E(δ t ) ≤ Cdβ E[P (1− )] 1 t for some universal constant C.</p><p>The theorem confirms two intuitions: The larger the number of selected coordinates E[P] (i.e. more parallel workers), the faster the algorithm converges per-iteration; however, this also increases , demonstrating a tradeoff between parallelization and correctness. Also, the smaller the variance E[P 2 ], the faster the algorithm converges (since is proportional to it). Remark: We compare Theorem 2 with Shotgun <ref type="bibr" target="#b9">[10]</ref> and the Block greedy algorithm in <ref type="bibr" target="#b21">[22]</ref>. The convergence rate we get is similar to shotgun, but with a significant difference: Our spectral radius ρ = ρ(A) is potentially much smaller than shotgun's ρ(X X), since by partitioning we zero out all entries in the correlation matrix X X that are bigger than the threshold θ. In other words, we get to control the spectral radius while shotgun is totally passive.</p><p>The convergence rate in <ref type="bibr" target="#b21">[22]</ref>  . Compared with ours, we have a bigger (hence worse) numerator (d vs. B) but the denominator ( vs. ) are not directly comparable: we have a bigger spectral radius ρ and bigger d while <ref type="bibr" target="#b21">[22]</ref> has a smaller spectral radius ρ (essentially taking a submatrix of our A) and smaller B − 1. Nevertheless, we note that <ref type="bibr" target="#b21">[22]</ref> may have a higher per-step complexity: each worker needs to check all of its assigned τ coordinates just to update one "optimal" coordinate. In contrast, we simply pick a random coordinate, and hence can be much cheaper per-step.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig.2. The difference between data and model parallelism: data samples are always conditionally independent given the model, but some model parameters are not independent of each other.Data-ParallelismModel-Parallelism</figDesc><graphic url="image-18.png" coords="3,318.30,230.02,240.77,103.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 4. Petuum Program Structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Petuum system: scheduler, workers, parameter servers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Scheduler system. Using algorithm or model-specific criteria, the Petuum scheduler prioritizes a small subset of parameters from the full model A. This is followed by a dependency analysis on the prioritized subset: parameters are further subdivided into groups, where a parameter Ai in group gu is must be independent of all other parameters Aj in all other groups gv. This is illustrated as a graph partitioning, although the implementation need not actually construct a graph.</figDesc><graphic url="image-40.png" coords="6,48.56,44.84,249.95,80.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Now we turn to development of parallel algorithms for large-scale distributed ML problems, in light of the data and model parallel principles underlying Petuum. As examples, we explain how to use Petuum's programming interface to implement novel or state-of-the-art versions of the following 4 algorithms: (1) data-parallel Distance Metric Learning, (2) model-parallel Lasso regression, (3) model-parallel topic modeling (LDA), and (4) model-parallel Matrix Factorization. These algorithms all enjoy significant performance advantages over the previous state-of-the-art and existing open-source software, as we will show.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>is essential for many distance based data mining and machine learning algorithms, such as retrieval, k-means clustering and knearest neighbor (k-NN) classification. DML has not received much attention in the Big Data setting, and we are not aware of any distributed implementations of DML. The most popular version of DML tries to learn a Mahalanobis distance matrix M (symmetric and positivesemidefinite), which can then be used to measure the distance between two samples D(x, y) = (x − y) T M (x − y). Given a set of "similar" sample pairs S = {(x i , y i )} |S| i=1 , and a set of "dissimilar" pairs D = {(x i , y i )} |D| i=1 , DML learns the Mahalanobis distance by optimizing min M (x,y)∈S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>/</head><label></label><figDesc>Fig. 8. Petuum DML data-parallel pseudocode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Petuum Lasso model-parallel pseudocode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Petuum Topic Model (LDA) model-parallel pseudocode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Petuum MF model-parallel pseudocode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>on given cluster size Topic Model (LDA) 220b params (22m unique words, 10k topics) on 256 cores and 1TB memory Constrained Topic Model (MedLDA) 610m params (61k unique words, 10k topics, 20 classes) on 16 cores and 128GB memory Convolutional Neural Networks (CNN) 1b params on 1024 CPU cores and 2TB memory Fully-connected Deep Neural Networks (DNN) 24m params on 96 CPU cores and 768GB memory Matrix Factorization (MF) 20m-by-20k input matrix, rank 400 (8b params) on 128 cores and 1TB memory Non-negative Matrix Factorization (NMF) 20m-by-20k input matrix, rank 50 (1b params) on 128 cores and 1TB memory Sparse Coding (SC) 1b params on 128 cores and 1TB memory Logistic Regression (LR) 100m params (50k samples, 2b nonzeros) on 512 cores and 1TB memory Multi-class Logistic Regression (MLR) 10m params (10 classes, 1m features) on 512 cores and 1TB memory Lasso Regression 100m params (50k samples, 2b nonzeros) on 512 cores and 1TB memory Support Vector Machines (SVM) 100m params (50k samples, 2b nonzeros) on 512 cores and 1TB memory Distance Metric Learning (DML) 441m params (63k samples, 21k feature dimension) on 64 cores and 512GB memory K-means clustering 1m params (10m samples, 1k feature dimension, 1k clusters) on 32 cores and 256GB memory Random Forest 8000 trees (2m samples) on 80 cores and 640 GB memory TABLE 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Key properties of ML algorithms: (a) Non-uniform convergence; (b) Error-tolerant convergence; (c) Dependency structures amongst variables.</figDesc><graphic url="image-60.png" coords="10,59.27,230.61,226.80,85.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Performance increase in ML applications due to the Petuum Parameter Server (PS) and Scheduler. The Eager Stale Synchronous Parallel (ESSP) consistency model (on the PS) improves the number of iterations executed per second (throughput) while maintaining per-iteration quality. Prioritized, dependency-aware scheduling allows the Scheduler to improve the quality of each iteration, while maintaining iteration throughput. In both cases, overall real-time convergence rate is improved -30% improvement for the PS Matrix Factorization example, and several orders of magnitude for the Scheduler Lasso example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig.14. Left: Petuum relative speedup vs popular platforms (larger is better). Across ML programs, Petuum is at least 2-10 times faster than popular implementations. Right: Petuum allows single-machine algorithms to be transformed into cluster versions, while still achieving near-linear speedup with increasing number of machines (Caffe CNN and DML).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .Fig. 16 .</head><label>1516</label><figDesc>Fig. 15. Left: LDA convergence time: Petuum vs YahooLDA (lower is better). Petuum's data-and-model-parallel LDA converges faster than YahooLDA's data-parallel-only implementation, and scales to more LDA parameters (larger vocab size, number of topics). Right panels: Matrix Factorization convergence time: Petuum vs GraphLab vs Spark. Petuum is fastest and the most memory-efficient, and is the only platform that could handle Big MF models with rank K ≥ 1000 on the given hardware budget.</figDesc><graphic url="image-70.png" coords="14,48.00,44.27,85.68,73.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>subset of Imagenet with 200 classes, and 1.3m model parameters. Compared to the original single-machine Caffe (which does not have the overhead of network communication), Petuum approaches linear speedup (3.1-times speedup on 4 machines, Figure 14 right plot) due to the parameter server's ESSP consistency for managing network communication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>g 2 w + − w 2 2 +</head><label>22</label><figDesc>), we haveE[F(z) − F (w)] jp (w + jp − w jp ) + r(w + jp ) − r(w jp ) + β 2 (w + jp − w jp ) 2 + β 2 p =q (w + jp − w jp )(w + jq − w jq )x jp x jq = E[P] d g (w + − w) + r(w + ) − r(w) + β βE[P (P − 1)] 2N (w + − w) (A − I)(w + − w)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>F 2 2 = g 2 2 /β 2 . 2 Ew t+1 − w 2 2 [</head><label>22222</label><figDesc>(w t+1 )−F (w ) ≤ (w t+1 −w ) g ≤ w t+1 −w 2 • g 2 ,and w t+1 − w t Thus, defining δ t = F (w t ) − F (w ), we haveE(δ t+1 − δ t ) ≤ − E[P(1 − )] 2dβ w t+1 − w 2 E(δ t+1 )] 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>is CB P (1− ) 1 t , where = (P −1)(ρ −1) B−1</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">IEEE TRANSACTIONS ON BIG DATA, VOL. XX, NO. X, MARCH 2015</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">. In the context of Lasso, this means the data columns x •j corresponding to the chosen parameters j have very small pair-wise dot product, below a threshold τ .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported in part by DARPA FA87501220324, and NSF IIS1447676 grants to Eric P. Xing.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF OF THEOREM 3</head><p>For the Regularized Regression Problem, we prove that the Petuum S RRP () scheduler produces a solution trajectory w (t) RRP that is close to ideal execution: Theorem 3: (S RRP () is close to ideal execution) Let S ideal () be an oracle schedule that always proposes P random features with zero correlation. Let w (t) ideal be its parameter trajectory, and let w (t) RRP be the parameter trajectory of S RRP (). Then,</p><p>C is a data dependent constant, m is the strong convexity constant, L is the domain width of A j , and P is the expected number of indexes that S RRP () can actually parallelize in each iteration (since it may not be possible to find P nearlyindependent parameters). We assume that the objective function F (w) = f (w) + r(w) is strongly convex -for certain problems, this can be achieved through parameter replication, e.g. min w 1 2 ||y − Xw|| 2 2 + λ 2M j=1 w j is the replicated form of Lasso regression seen in Shotgun <ref type="bibr" target="#b9">[10]</ref>. Lemma 1: The difference between successive updates is:</p><p>(∆w) T X T X∆w (28) Proof of Lemma 1: The Taylor expansion of F (w + ∆w) around w coupled with the fact that F (w) (3rd-order) and higher order derivatives are zero leads to the above result.</p><p>Proof of Theorem 3: By using Lemma 1, and telescoping sum:</p><p>Since S ideal chooses P features with 0 correlation,</p><p>Again using Lemma 1, and telescoping sum:</p><p>Taking the difference of the two sequences, we have:</p><p>Taking expectations w.r.t. the randomness in iteration, indices chosen at each iteration, and the inherent randomness in the two sequences, we have:</p><p>where C data is a data dependent constant. Here, the difference between (∆w</p><p>ideal and (∆w</p><p>can only be possible due to (∆w</p><p>RRP . Following the proof in the shotgun paper <ref type="bibr" target="#b9">[10]</ref>, we get</p><p>where d is the length of w (number of features), C is a data dependent constant, L is the domain width of w j (i.e. the difference between its maximum and minimum possible values), and P is the expected number of indexes that S RRP () can actually parallelize in each iteration. Finally, we apply the strong convexity assumption to get</p><p>where m is the strong convexity constant. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards topic modeling for big data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4402</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lightlda: Big topic models on modest compute clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Accepted to International World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale parallel collaborative filtering for the netflix prize</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Aspects in Information and Management</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel markov chain monte carlo for nonparametric mixture models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Slow learners are fast</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed delayed stochastic optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel coordinate descent for l1-regularized loss minimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hadoop: The definitive guide</title>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spark: cluster computing with working sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<editor>HotCloud</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distributed GraphLab: A Framework for Machine Learning and Data Mining in the Cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>PVLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Highperformance distributed ml at scale through parameter server consistency models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pregel: a system for large-scale graph processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of data</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piccolo: building fast, distributed programs with partitioned tables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI. USENIX Association</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling distributed machine learning with the parameter server</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale matrix factorization with distributed stochastic gradient descent</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sismanis</surname></persName>
		</author>
		<idno type="DOI">10.1145/2020408.2020426</idno>
		<ptr target="http://doi.acm.org/10.1145/2020408.2020426" />
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smoothing proximal gradient method for general structured sparse learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On model parallelism and scheduling strategies for distributed machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature clustering for accelerating parallel coordinate descent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halappanavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haglin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">More effective distributed ml via a stale synchronous parallel parameter server</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cipar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Priter: A distributed framework for prioritized iterative computations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOCC</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Informationtheoretic metric learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
				<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Scalable inference in latent variable models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<editor>WSDM</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD &apos;09</title>
				<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="937" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fugue: Slow-workeragnostic distributed learning for big models on big data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in AISTATS</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalable inference in max-margin topic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="964" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable coordinate descent approaches to parallel matrix factorization for recommender systems</title>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2012 IEEE 12th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fugue: Slow-workeragnostic distributed learning for big models on big data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Seventeenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hogwild!: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Parallel coordinate descent methods for big data optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takáč</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0873</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Priter: A distributed framework for prioritizing iterative computations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1884" to="1893" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Parallel and Distributed Systems</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
