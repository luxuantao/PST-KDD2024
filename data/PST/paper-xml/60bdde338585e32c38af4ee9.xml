<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Graph-level Representation Learning with Local and Global Structure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
						</author>
						<title level="a" type="main">Self-supervised Graph-level Representation Learning with Local and Global Structure</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies unsupervised/self-supervised whole-graph representation learning, which is critical in many tasks such as molecule properties prediction in drug and material discovery. Existing methods mainly focus on preserving the local similarity structure between different graph instances but fail to discover the global semantic structure of the entire data set. In this paper, we propose a unified framework called Local-instance and Global-semantic Learning (GraphLoG) for selfsupervised whole-graph representation learning. Specifically, besides preserving the local similarities, GraphLoG introduces the hierarchical prototypes to capture the global semantic clusters. An efficient online expectation-maximization (EM) algorithm is further developed for learning the model. We evaluate GraphLoG by pre-training it on massive unlabeled graphs followed by finetuning on downstream tasks. Extensive experiments on both chemical and biological benchmark data sets demonstrate the effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning informative representations of whole graphs is a fundamental problem in a variety of domains and tasks, such as molecule properties prediction in drug and material discovery <ref type="bibr" target="#b16">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b52">Wu et al., 2018)</ref>, protein function forecast in biological networks <ref type="bibr" target="#b0">(Alvarez &amp; Yan, 2012;</ref><ref type="bibr" target="#b27">Jiang et al., 2017)</ref>, and predicting the properties of circuits in circuit design <ref type="bibr" target="#b59">(Zhang et al., 2019)</ref>. Recently, Graph Neural Networks (GNNs) have attracted a surge of interest and showed the effectiveness in learning graph representations. These methods are usually trained in a supervised 1 Shanghai Jiao Tong University 2 National Research Council Canada 3 Mila -Quebec AI Institute 4 CIFAR AI Research Chair 5 HEC Montr√©al. Correspondence to: Minghao Xu &lt;xuming-hao118@sjtu.edu.cn&gt;, Jian Tang &lt;jian.tang@hec.ca&gt;. fashion, which requires a large number of labeled data. Nevertheless, in many scientific domains, labeled data are very limited and expensive to obtain. Therefore, it is becoming increasingly important to learn the representations of graphs in an unsupervised or self-supervised fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the</head><p>Self-supervised learning has recently achieved profound success for both natural language processing, e.g. GPT <ref type="bibr" target="#b41">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref>, and image understanding, e.g. MoCo <ref type="bibr" target="#b23">(He et al., 2019)</ref> and Sim-CLR <ref type="bibr" target="#b10">(Chen et al., 2020)</ref>. However, how to effectively learn the representations of graphs in a self-supervised way is still an open problem. Intuitively, a desirable graph representation should be able to preserve the local-instance structure, so that similar graphs are embedded close to each other and dissimilar ones stay far apart. In addition, the representations of the entire set of graphs should also reflect the global-semantic structure of the data, so that the graphs with similar semantic properties are compactly embedded, which is able to benefit various downstream tasks such as graph classification or regression. Such global structure can be effectively captured by semantic clusters <ref type="bibr" target="#b6">(Caron et al., 2018;</ref><ref type="bibr" target="#b26">Ji et al., 2019)</ref>, which can be further organized hierarchically <ref type="bibr" target="#b32">(Li et al., 2020)</ref>.</p><p>There are some recent works that learn graph representation in a self-supervised manner, such as local-global mutual information maximization <ref type="bibr" target="#b50">(Velickovic et al., 2019;</ref><ref type="bibr" target="#b46">Sun et al., 2019)</ref>, structural-similarity/context prediction <ref type="bibr" target="#b37">(Navarin et al., 2018;</ref><ref type="bibr" target="#b24">Hu et al., 2019;</ref><ref type="bibr" target="#b58">You et al., 2020b)</ref>, contrastive learning <ref type="bibr" target="#b21">(Hassani &amp; Ahmadi, 2020;</ref><ref type="bibr" target="#b40">Qiu et al., 2020;</ref><ref type="bibr">You et al., 2020a)</ref> and meta-learning <ref type="bibr" target="#b34">(Lu et al., 2021)</ref>. However, all these methods are able to model only the local structure between different graph instances but fail to discover the global-semantic structure. To address this shortcoming, we are seeking for an approach that is sufficient to model both the local and global structure of a set of graphs.</p><p>To attain this goal, we propose a Local-instance and Global-semantic Learning (GraphLoG) framework for selfsupervised graph representation learning. Specifically, for preserving the local similarity between various graph instances, we seek to align the embeddings of correlated graphs/subgraphs by discriminating the correlated graph/subgraph pairs from the negative pairs. In this locally arXiv:2106.04113v1 [cs.LG] 8 Jun 2021 smooth latent space, we further introduce the additional model parameters, hierarchical prototypes 1 , to depict the latent distribution of a graph data set in a hierarchical way. For model learning, we propose to maximize the data likelihood with respect to both the GNN parameters and hierarchical prototypes via an online expectation-maximization (EM) algorithm. Given a mini-batch of graphs sampled from the data distribution, in the E-step, we infer the embeddings of these graphs with a GNN and sample the latent variable of each graph (i.e. the prototypes associated to each graph) from the posterior distribution defined by current model. In the M-step, we aim to maximize the expectation of complete-data likelihood with respect to the current model by optimizing with a mini-batch-induced objective function. Therefore, in this iterative EM process, the globalsemantic structure of the data can be gradually discovered and refined. The whole model is pre-trained with a large number of unlabeled graphs, and then fine-tuned and evaluated on some downstream tasks containing scarce labeled graphs.</p><p>To verify the effectiveness of the GraphLoG framework, we apply our method to both the chemistry and biology domains. Through pre-training on massive unlabeled molecular graphs (or protein ego-networks) using the proposed local and global objectives, the existing GNN models are able to achieve superior performance on the downstream molecular property (or biological function) prediction benchmarks.</p><p>In particular, the Graph Isomorphism Network (GIN) <ref type="bibr" target="#b54">(Xu et al., 2019)</ref> pre-trained by the proposed method outperforms the previous self-supervised graph representation learning approaches on six of eight downstream tasks of chemistry domain, and it achieves a 2.1% performance gain in terms of average ROC-AUC on eight downstream tasks.</p><p>In addition, the analytical experiments further illustrate the benefits of global structure learning through conducting ablation studies and visualizing the embedding distributions on a set of graphs.</p><p>1 Hierarchical prototypes are representative cluster embeddings organized as a set of trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Definition and Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem Definition</head><p>An ideal representation should preserve the local structure among various data instances. More specifically, we define it as follows:</p><p>Definition 1 (Local-instance Structure). The local structure aims to preserve the pairwise similarity between different instances after mapping from the high-dimensional input space to the low-dimensional latent space <ref type="bibr" target="#b43">(Roweis &amp; Saul, 2000;</ref><ref type="bibr" target="#b3">Belkin &amp; Niyogi, 2002)</ref>. For a pair of similar graphs/subgraphs, G and G , their embeddings are expected to be nearby in the latent space, as illustrated in Fig. <ref type="figure" target="#fig_1">1(a)</ref>, while the dissimilar pairs should be mapped to far apart.</p><p>The pursuit of local-instance structure alone is usually insufficient to capture the semantics underlying the entire data set. It is therefore important to discover the global-semantic structure of the data, which is concretely defined as follows:</p><p>Definition 2 (Global-semantic Structure). A real-world data set can usually be organized as various semantic clusters <ref type="bibr" target="#b14">(Furnas et al., 2017;</ref><ref type="bibr" target="#b26">Ji et al., 2019)</ref>, especially in a hierarchical way for graph-structured data <ref type="bibr" target="#b1">(Ashburner et al., 2000;</ref><ref type="bibr" target="#b9">Chen et al., 2012b)</ref>. After mapping to the latent space, the embeddings of a set of graphs are expected to form some global structures reflecting the clustering patterns of the original data. A graphical illustration can be seen in Fig. <ref type="figure" target="#fig_1">1(b)</ref>.</p><p>Problem Definition. The problem of self-supervised graph representation learning considers a set of unlabeled graphs</p><formula xml:id="formula_0">G = {G 1 , G 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , G M },</formula><p>and we aim at learning a lowdimensional vector h Gm ‚àà R Œ¥ for each graph G m ‚àà G under the guidance of the data itself. In specific, we expect the graph embeddings H = {h G1 , h G2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , h G M } follow both the local-instance and global-semantic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Preliminaries</head><p>Graph Neural Networks (GNNs). A typical graph can be represented as G = (V, E, X V , X E ), where V is a set of nodes, E denotes the edge set, X V = {X v |v ‚àà V} stands for the attributes of all nodes, and X E = {X uv |(u, v) ‚àà E} represents the edge attributes. A GNN aims to learn an embedding vector h v ‚àà R Œ¥ for each node v ‚àà V and also a vector h G ‚àà R Œ¥ for the entire graph G. For an L-layer GNN, a neighborhood aggregation scheme is performed to capture the L-hop information surrounding each node. As suggested in <ref type="bibr" target="#b16">Gilmer et al. (2017)</ref>, the l-th layer of a GNN can be formalized as follows:</p><formula xml:id="formula_1">h (l) v = f (l) U h (l‚àí1) v , f (l) M h (l‚àí1) v , h (l‚àí1) u , Xuv : u ‚àà N (v) ,<label>(1)</label></formula><p>where N (v) is the neighborhood set of v, h U stand for the message passing and update function at the l-th layer, respectively. Since h v summarizes the information of a subgraph centered around node v, we will refer to h v as subgraph embedding to underscore this point. The entire graph's embedding can be derived as below:</p><formula xml:id="formula_2">h G = f R h v |v ‚àà V ,<label>(2)</label></formula><p>where f R is a permutation-invariant readout function, e.g. mean pooling or more complex graph-level pooling function <ref type="bibr" target="#b57">(Ying et al., 2018;</ref><ref type="bibr" target="#b60">Zhang et al., 2018)</ref>.</p><p>General EM Algorithm. The basic objective of EM algorithm <ref type="bibr" target="#b11">(Dempster et al., 1977;</ref><ref type="bibr" target="#b31">Krishnan et al., 1997)</ref> is to find the maximum likelihood solution for a model containing latent variables. In such a problem, we denote the set of all observed data as X, the set of all latent variables as Z and the set of all model parameters as Œ∏.</p><p>In the E-step, using the data X and the model parameters Œ∏ t‚àí1 estimated by the last EM cycle, the posterior distribution of latent variables is derived as p(Z|X, Œ∏ t‚àí1 ) which can also be regarded as the responsibility that a specific set of latent variables are taken for explaining the observations.</p><p>In the M-step, employing the posterior distribution given by the E-step, the expectation of complete-data log-likelihood, denoted as Q(Œ∏), is evaluated for the general model parameters Œ∏ as follows:</p><formula xml:id="formula_3">Q(Œ∏) = E p(Z|X,Œ∏t‚àí1) [log p(X, Z|Œ∏)].<label>(3)</label></formula><p>The model parameters are updated to maximize this expectation in the M-step, which outputs:</p><formula xml:id="formula_4">Œ∏ t = arg max Œ∏ Q(Œ∏).<label>(4)</label></formula><p>Each cycle of EM can increase the complete-data likelihood expected by the current model, and, considering the whole progress, the EM algorithm has been demonstrated to be capable of maximizing the marginal likelihood function p(X|Œ∏) <ref type="bibr" target="#b22">(Hathaway, 1986;</ref><ref type="bibr" target="#b37">Neal &amp; Hinton, 1998)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GraphLoG: Self-supervised Graph-level Representation Learning with Local and Global Structure</head><p>In this section, we introduce our approach called Localinstance and Global-semantic Learning (GraphLoG) for self-supervised graph representation learning. The main purpose of GraphLoG is to discover and refine both the local and global structures of graph embeddings in the latent space, such that we can learn useful graph representations for the downstream task like graph classification. Specifically, GraphLoG constructs a locally smooth latent space by aligning the embeddings of correlated graphs/subgraphs. On such basis, the global structures of graph embeddings are modeled by hierarchical prototypes, and the data likelihood is maximized via an online EM algorithm. Next, we elucidate the GraphLoG framework in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Local-instance Structure of Graph Representations</head><p>Following the existing methods for dimensionality reduction <ref type="bibr" target="#b48">(Tenenbaum et al., 2000;</ref><ref type="bibr" target="#b43">Roweis &amp; Saul, 2000;</ref><ref type="bibr" target="#b3">Belkin &amp; Niyogi, 2002)</ref>, the goal of local-structure learning is to preserve the local similarity of the data before and after mapping to a low-dimensional latent space. In specific, it is expected that similar graphs or subgraphs are embedded close to each other, and dissimilar ones are mapped to far apart. Using a similarity measurement defined in the latent space, we formulate this problem as maximizing the similarity of correlated graph/subgraph pairs while minimizing that of negative pairs.</p><p>In specific, given a graph G = (V, E, X V , X E ) sampled from the data distribution P G , we obtain its correlated counterpart G = (V , E , X V , X E ) through randomly masking a part of node/edge attributes in the graph <ref type="bibr" target="#b24">(Hu et al., 2019)</ref> (see appendix for the detailed scheme). In addition, for a subgraph G v constituted by node v and its L-hop neighborhoods in graph G, we regard the corresponding subgraph G v in graph G as its correlated counterpart. Through applying an L-layer GNN model GNN Œ∏ (Œ∏ stands for GNN's parameters) upon graph G and G , the graph and subgraph embeddings are derived as follows:</p><formula xml:id="formula_5">(h V , h G ) = GNN Œ∏ (G), (h V , h G ) = GNN Œ∏ (G ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">h V = {h Gv |v ‚àà V} and h V = {h G v |v ‚àà V } repre-</formula><p>sent the set of subgraph embeddings within graph G and G , respectively.</p><p>In this phase, the objective of learning is to enhance the similarity of correlated graph/subgraph pairs while diminish that of negative pairs. Using a specific similarity measure (e.g. cosine similarity s(x, y) = x y/||x||||y|| in our practice), we seeks to optimize the following two objective functions for graph and subgraph, respectively:</p><formula xml:id="formula_7">L graph =‚àíE (G+,G + )‚àºp(G,G ),(G‚àí,G ‚àí )‚àºpn(G,G ) s(G + , G + ) ‚àí s(G ‚àí , G ‚àí ) ,<label>(6)</label></formula><formula xml:id="formula_8">L sub =‚àíE (Gu,G u )‚àºp(Gv,G v ),(Gv,G w )‚àºpn(Gv,G v ) s(G u , G u ) ‚àí s(G v , G w ) ,<label>(7)</label></formula><p>where p n (G, G ) and p n (G v , G v ) denote the noise distribution from which negative pairs are sampled. In practice, for a correlated graph pair (G, G ) or correlated subgraph pair</p><formula xml:id="formula_9">(G v , G v ), we substitute G (G v )</formula><p>randomly with another graph from the data set (a subgraph centered around another node in the same graph) to construct negative pairs.</p><p>For learning the local-instance structure of graph representations, we aim to minimize both objective functions (Eqs. 6 and 7) with respect to the parameters of GNN:</p><formula xml:id="formula_10">min Œ∏ L local ,<label>(8)</label></formula><formula xml:id="formula_11">L local = L graph + L sub . (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Global-semantic Structure of Graph Representations</head><p>It is worth noticing that the graphs in a data set may possess hierarchical semantic information. For example, drugs (i.e. molecular graphs) are represented by a five-level hierarchy in the Anatomical Therapeutic Chemical (ATC) classification system <ref type="bibr" target="#b9">(Chen et al., 2012b)</ref>. After mapping to the latent space, the embeddings of all graphs in the data set are also expected to form some global structures corresponding to the hierarchical semantic structures of the original data.</p><p>However, for the lack of explicit semantic labels in the self-supervised graph representation learning, such global structure cannot be attained via label-induced supervision.</p><p>To tackle this limitation, we introduce an additional set of model parameters, hierarchical prototypes, to represent the feature clusters in the latent space in a hierarchical way. They are formally defined as</p><formula xml:id="formula_12">C = {c l i } M l i=1 (l = 1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , L p )</formula><p>, where c l i ‚àà R Œ¥ stands for the i-th prototype at the l-th layer, L p is the depth of hierarchical prototypes, and M l denotes the number of prototypes at the l-th layer. These prototypes are structured as a set of trees (Fig. <ref type="figure" target="#fig_1">1(b)</ref>), in which each node corresponds to a prototype, and, except for the leaf nodes, each prototype possesses a set of child nodes, denoted as</p><formula xml:id="formula_13">C(c l i ) (1 i M l , l = 1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , L p ‚àí 1).</formula><p>The goal of global-semantic learning is to encourage the graphs to be compactly embedded around corresponding prototypes and, at the same time, refine hierarchical prototypes to better represent the data. We formalize this problem as optimizing a latent variable model. Specifically, for the</p><formula xml:id="formula_14">observed data set G = {G 1 , G 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , G M },</formula><p>we consider a latent variable set, i.e. the prototype assignments of all graphs</p><formula xml:id="formula_15">Z = {z G1 , z G2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , z G M } (z Gm</formula><p>is a set of prototypes that best represent G m in the latent space). The model parameters in this problem are the GNN parameters Œ∏ and hierarchical prototypes C. Since the corresponding latent variable of each graph is not given, it is hard to directly maximize the complete-data likelihood function p(G, Z|Œ∏, C). Therefore, we seek to maximize the expectation of complete-data likelihood through the EM algorithm.</p><p>The vanilla EM algorithm <ref type="bibr" target="#b11">(Dempster et al., 1977;</ref><ref type="bibr" target="#b31">Krishnan et al., 1997)</ref> requires a full pass through the data set before each parameter update, which is computationally inefficient when the size of data set is large like in our case. Therefore, we consider an online EM variant <ref type="bibr" target="#b44">(Sato &amp; Ishii, 2000;</ref><ref type="bibr" target="#b5">Capp√© &amp; Moulines, 2009;</ref><ref type="bibr" target="#b33">Liang &amp; Klein, 2009)</ref> which operates on mini-batches of data. This approach is based on the i.i.d. assumption of the data set, where both the complete-data likelihood and the posterior probability of latent variables can be factorized over each observed-latent variable pair:</p><formula xml:id="formula_16">p(G, Z|Œ∏, C) = M m=1 p(G m , z Gm |Œ∏, C),<label>(10)</label></formula><formula xml:id="formula_17">p(Z|G, Œ∏, C) = M m=1 p(z Gm |G m , Œ∏, C).<label>(11)</label></formula><p>First, we introduce the initialization scheme of model parameters.</p><p>Initialization of model parameters. Before triggering the global structure exploration, we first pre-train the GNN by minimizing L local and employ the derived GNN model as initialization, which establishes a locally smooth latent space. After that, we utilize this pre-trained GNN model to extract the embeddings of all graphs in the data set, and the K-means clustering is applied upon these graph embeddings to initialize the bottom layer prototypes (i.e.</p><formula xml:id="formula_18">{c Lp i } M Lp i=1</formula><p>) with the output cluster centers. The prototypes of upper layers are initialized by iteratively applying K-means clustering to the prototypes of the layer below. For each time of clustering, we discard the output cluster centers assigned with less than two samples to avoid trivial solutions <ref type="bibr" target="#b2">(Bach &amp; Harchaoui, 2007;</ref><ref type="bibr" target="#b6">Caron et al., 2018)</ref>.</p><p>Next, we state the details of the E-step and M-step applied in our method. E-step. In this step, we first randomly sample a mini-batch of graphs G = {G 1 , G 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , G N } (N denotes the batch size) from the data set G, and Z = {z Gn } N n=1 stands for the latent variables corresponding to these sampled graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Each latent variable z</head><formula xml:id="formula_19">Gn = {z 1 Gn , z 2 Gn , ‚Ä¢ ‚Ä¢ ‚Ä¢ , z</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lp</head><p>Gn } is a chain of prototypes from top layer to bottom layer that best represent graph G n in the latent space, and it holds that z l+1</p><p>Gn is the child node of z l Gn in the corresponding tree structure, i.e.</p><formula xml:id="formula_20">z l+1 Gn ‚àà C(z l Gn ) (l = 1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , L p ‚àí 1)</formula><p>. The posterior distribution of Z can be evaluated in a factorized way because of the i.i.d. assumption:</p><formula xml:id="formula_21">p( Z| G, Œ∏ t‚àí1 , C t‚àí1 ) = N n=1 p(z Gn |G n , Œ∏ t‚àí1 , C t‚àí1 ), (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>where Œ∏ t‚àí1 and C t‚àí1 are the model parameters from the last EM cycle. Directly evaluating each posterior distribution p(z Gn |G n , Œ∏ t‚àí1 , C t‚àí1 ) is nontrivial, which requires to traverse all the possible chains in hierarchical prototype. Instead, we adopt the idea of stochastic EM algorithm <ref type="bibr" target="#b7">(Celeux &amp; Govaert, 1992;</ref><ref type="bibr" target="#b38">Nielsen et al., 2000)</ref> and draw a sample ·∫ëGn ‚àº p(z Gn |G n , Œ∏ t‚àí1 , C t‚àí1 ) for Monte Carlo estimation. In specific, we sequentially sample a prototype from each layer in a top-down manner, and all the sampled prototypes form a connected chain from top layer to bottom layer in hierarchical prototypes. Formally, we first sample a prototype from top layer according to a categorical distribution over all the top layer prototypes, i.e. ·∫ë1</p><p>Gn ‚àº Cat(z 1</p><formula xml:id="formula_23">Gn |{Œ± i } M1 i=1 ) (Œ± i = softmax(s(c 1 i , h Gn )))</formula><p>, where s denotes the cosine similarity measurement; for the sampling at layer l (l 2), we draw a prototype from that layer based on a categorical distribution over the child nodes of prototype ·∫ël‚àí1 Gn sampled from the layer above, i.e. ·∫ël Gn ‚àº Cat(z l Gn |{Œ± c })</p><formula xml:id="formula_24">(Œ± c = softmax(s(c, h Gn )), ‚àÄc ‚àà C(·∫ë l‚àí1 Gn ))</formula><p>, such that we sample a latent variable ·∫ëGn = {·∫ë 1 Gn , ·∫ë2 Gn , ‚Ä¢ ‚Ä¢ ‚Ä¢ , ·∫ëLp Gn } which is a connected chain in hierarchical prototypes. Using the latent variables inferred as above, we seek to maximize the expectation of complete-data log-likelihood in the M-step. M-step. In this step, we aim at maximizing the expected complete-data log-likelihood with respect to the posterior distribution of latent variables, which is defined as follows:</p><formula xml:id="formula_25">Q(Œ∏, C) = E p(Z|G,Œ∏t‚àí1,Ct‚àí1) [log p(G, Z|Œ∏, C)]. (13)</formula><p>This expectation needs the computation over all data points, which cannot be attained in the online setting. As a substitute, we propose to maximize the expected log-likelihood on mini-batch G, which can be estimated using the latent variables Z est = {·∫ë Gn } N n=1 sampled in the E-step:</p><formula xml:id="formula_26">Q(Œ∏, C) = E p( Z| G,Œ∏t‚àí1,Ct‚àí1) [log p( G, Z|Œ∏, C)] ‚âà log p( G, Z est |Œ∏, C) = N n=1 log p(G n , ·∫ëGn |Œ∏, C).<label>(14)</label></formula><p>We would like to point out that Q(Œ∏, C) is a decent proxy for Q(Œ∏, C), where a proportional relation approximately holds between them (see appendix for the proof):</p><formula xml:id="formula_27">Q(Œ∏, C) ‚âà N M Q(Œ∏, C). (<label>15</label></formula><formula xml:id="formula_28">)</formula><p>We further scale Q(Œ∏, C) with the batch size to derive the log-likelihood function L(Œ∏, C) that is more stable in terms of computation:</p><formula xml:id="formula_29">L(Œ∏, C) = 1 N Q(Œ∏, C).<label>(16)</label></formula><p>To estimate L(Œ∏, C), we need to define the joint likelihood of a graph G and a latent variable z G , which is represented with an energy-based formulation in our method:</p><formula xml:id="formula_30">p(G, z G |Œ∏, C) = 1 Z(Œ∏, C) exp f (h G , z G ) ,<label>(17)</label></formula><p>where Z(Œ∏, C) denotes the partition function. We formalize the negative energy function f by measuring the similarities between graph embedding h G and the prototypes in z G and also measuring the similarities between the prototypes in z G that are from consecutive layers:</p><formula xml:id="formula_31">f (hG, zG) = Lp l=1 s hG, z l G + Lp‚àí1 l=1 s z l G , z l+1 G .<label>(18)</label></formula><p>Intuitively, f evaluates how well a latent variable z G represents graph G in the latent space, and it also measures the affinity between the consecutive prototypes along a chain from top layer to bottom layer in hierarchical prototypes.</p><p>It is nontrivial to optimize with p(G, z G |Œ∏, C) due to the intractable partition function. Inspired by Noise-Contrastive Estimation (NCE) <ref type="bibr" target="#b18">(Gutmann &amp; Hyv√§rinen, 2010;</ref><ref type="bibr">2012)</ref>, we seek to optimize with the unnormalized likelihoods, i.e. p(G, z G |Œ∏, C) = exp(f (h G , z G )), by contrasting the positive observed-latent variable pair with the negative pairs sampled from some noise distribution, which defines an objective function that well approximates L(Œ∏, C):</p><formula xml:id="formula_32">Lglobal = ‚àíE (G + ,z + G )‚àºp(G,z G ) log p(G + , z + G |Œ∏, C) ‚àí E (G ‚àí ,z ‚àí G )‚àºpn(G,z G ) log p(G ‚àí , z ‚àí G |Œ∏, C) ,<label>(19)</label></formula><p>where p n (G, z G ) is the noise distribution. In practice, we compute the outer expectation with all the positive pairs in the mini-batch, i.e. (G n , ·∫ëGn ) (1 n N ), and, for computing the inner expectation, we construct L p negative pairs for the positive pair (G n , ·∫ëGn ) by fixing the graph G n and randomly substituting one of L p prototypes in ·∫ëGn with another prototype at the same layer each time. For globalsemantic learning, we aim to minimize the global objective function L global with respect to both the GNN parameter Œ∏ and hierarchical prototypes C:</p><formula xml:id="formula_33">min Œ∏,C L global . (<label>20</label></formula><formula xml:id="formula_34">)</formula><p>In general, the proposed online EM algorithm seeks to maximize the joint likelihood p(G, Z|Œ∏, C) governed by model parameters Œ∏ and C. For a step further, we propose the following proposition that this algorithm can indeed maximize the marginal likelihood function p(G|Œ∏, C). </p><formula xml:id="formula_35">Œ∏ t ‚Üê Œ∏ t‚àí1 ‚àí ‚àá Œ∏ (L local + L global ), C t ‚Üê C t‚àí1 ‚àí ‚àá C (L local + L global ). end for</formula><p>Proposition 1. For each EM cycle, the model parameters Œ∏ and C are updated in such a way that increases the marginal likelihood function p(G|Œ∏, C), unless a local maximum is reached on the mini-batch log-likelihood function Q(Œ∏, C).</p><p>The proof of Proposition 1 is provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Optimization and Downstream Application</head><p>The GraphLoG framework seeks to learn the graph representations preserving both the local-instance and globalsemantic structure on an unlabeled graph data set G. For model optimization under this framework, we first pre-train the GNN by minimizing the local objective function L local and initialize the model parameters with the pre-trained GNN. After that, for each learning step, we sample a minibatch G from the data set and conduct an EM cycle. In the E-step, the latent variables corresponding to the minibatch, i.e. Z est , are sampled from the posterior distribution defined by the current model. In the M-step, model parameters are updated to maximize the expected log-likelihood on mini-batch G. Also, we add the local objective function to the optimization of M-step, which guarantees the local smoothness when pursuing the global-semantic structure and performs well in practice. We summarize the optimization algorithm in Alg. 1.</p><p>After the self-supervised pre-training on massive unlabeled graphs, the derived GNN model can be applied to various downstream tasks for producing effective embedding vectors of different graphs. For example, we can first pre-train a GNN model with GraphLoG on a large number of unlabeled molecules (i.e. molecular graphs). After that, for a downstream task of molecular property prediction where a small set of labeled molecules are available, we can learn a linear classifier upon the pre-trained GNN model to perform the specific graph classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Graph Neural Networks (GNNs). Recently, following the efforts of learning graph representations via optimizing random walk <ref type="bibr" target="#b39">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b47">Tang et al., 2015;</ref><ref type="bibr" target="#b17">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b36">Narayanan et al., 2017)</ref> or matrix factorization <ref type="bibr" target="#b4">(Cao et al., 2015;</ref><ref type="bibr" target="#b51">Wang et al., 2016)</ref> objectives, GNNs explicitly derive proximity-preserved feature vectors in a neighborhood aggregation way. As suggested in <ref type="bibr" target="#b16">Gilmer et al. (2017)</ref>, the forward pass of most GNNs can be depicted in two phases, Message Passing and Readout phase, and various works <ref type="bibr" target="#b13">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b30">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b20">Hamilton et al., 2017;</ref><ref type="bibr" target="#b49">Velickovic et al., 2018;</ref><ref type="bibr" target="#b57">Ying et al., 2018;</ref><ref type="bibr" target="#b60">Zhang et al., 2018;</ref><ref type="bibr" target="#b54">Xu et al., 2019)</ref> sought to improve the effectiveness of these two phases. Unlike these methods which are mainly trained in a supervised fashion, our approach aims for self-supervised learning for GNNs.</p><p>Self-supervised Learning for GNNs. There are some recent works that explored self-supervised graph representation learning with GNNs. Garc√≠a-Dur√°n &amp; Niepert (2017) learned graph representations by embedding propagation, and <ref type="bibr" target="#b50">Velickovic et al. (2019)</ref>, <ref type="bibr" target="#b46">Sun et al. (2019)</ref> achieved this goal through mutual information maximization. Also, some self-supervised tasks, e.g. edge prediction <ref type="bibr" target="#b29">(Kipf &amp; Welling, 2016)</ref>, context prediction <ref type="bibr" target="#b24">(Hu et al., 2019;</ref><ref type="bibr" target="#b42">Rong et al., 2020)</ref>, graph partitioning <ref type="bibr" target="#b58">(You et al., 2020b)</ref>, edge/attribute generation <ref type="bibr" target="#b25">(Hu et al., 2020)</ref> and contrastive learning <ref type="bibr" target="#b21">(Hassani &amp; Ahmadi, 2020;</ref><ref type="bibr" target="#b40">Qiu et al., 2020;</ref><ref type="bibr">You et al., 2020a)</ref>, have been designed to acquire knowledge from unlabeled graphs. Nevertheless, all these methods are only able to model the local relations between different graph instances. The proposed framework seeks to discover both the local-instance and global-semantic structure of a set of graphs.</p><p>Self-supervised Semantic Learning. Clustering-based methods <ref type="bibr" target="#b53">(Xie et al., 2016;</ref><ref type="bibr" target="#b56">Yang et al., 2016;</ref><ref type="bibr" target="#b61">2017;</ref><ref type="bibr" target="#b6">Caron et al., 2018;</ref><ref type="bibr" target="#b26">Ji et al., 2019;</ref><ref type="bibr" target="#b32">Li et al., 2020)</ref> are commonly used to learn the semantic information of the data in a selfsupervised fashion. Among which, DeepCluster <ref type="bibr" target="#b6">(Caron et al., 2018)</ref> proved the strong transferability of the visual representations learnt by clustering prediction to various downstream visual tasks. Prototypical Contrastive Learning <ref type="bibr" target="#b32">(Li et al., 2020)</ref> proved its superiority over the instancelevel contrastive learning approaches. These methods are mainly developed for images but not for graph-structured data. Furthermore, the hierarchical semantic structure of the data has been less explored in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the performance of GraphLoG on both the chemistry and biology domains using the procedure of pre-training followed by fine-tuning. Also, analytical studies are conducted to verify the effectiveness of local and global structure learning.  <ref type="bibr" target="#b29">(Kipf &amp; Welling, 2016)</ref> 70.5 ¬± 0.7 InfoGraph <ref type="bibr" target="#b46">(Sun et al., 2019)</ref> 70.7 ¬± 0.5 AttrMasking <ref type="bibr" target="#b24">(Hu et al., 2019)</ref> 70.5 ¬± 0.5 ContextPred <ref type="bibr" target="#b24">(Hu et al., 2019)</ref> 69.9 ¬± 0.3 GraphPartition <ref type="bibr" target="#b58">(You et al., 2020b)</ref> 71.0 ¬± 0.2 GraphCL <ref type="bibr">(You et al., 2020a)</ref> 71.2 ¬± 0.6 GraphLoG (ours)</p><p>72.9 ¬± 0.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Pre-training details. Following <ref type="bibr" target="#b24">Hu et al. (2019)</ref>, we adopt a five-layer Graph Isomorphism Network (GIN) <ref type="bibr" target="#b54">(Xu et al., 2019)</ref> with 300-dimensional hidden units and a mean pooling readout function for performance comparisons (Secs. 5.2 and 5.3). We use an Adam optimizer <ref type="bibr" target="#b28">(Kingma &amp; Ba, 2015)</ref> (learning rate: 1√ó10 ‚àí3 ) to pre-train the GNN with L local for one epoch and then train the whole model with both L local and L global for 10 epochs. For each time of K-means clustering in the initialization of hierarchical prototypes, we adopt 50 initial cluster centers. Unless otherwise specified, the batch size N is set as 512, and the hierarchical prototypes' depth L p is set as 3. These hyperparameters are selected by the grid search on the validation sets of four downstream molecule data sets (i.e. BBBP, SIDER, ClinTox and BACE), and their sensitivity is analyzed in Sec. 5.4.</p><p>Fine-tuning details. For fine-tuning on a downstream task, a linear classifier is appended upon the pre-trained GNN, and an Adam optimizer (learning rate: 1 √ó 10 ‚àí3 , fine-tuning batch size: 32) is employed to train the model for 100 epochs. We utilize a learning rate scheduler with fix step size which multiplies the learning rate by 0.3 every 30 epochs.</p><p>All the reported results are averaged over five independent runs. The source code is available at https://github. com/DeepGraphLearning/GraphLoG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance comparison.</head><p>For the experiments on both chemistry and biology domains, we compare the proposed method with existing self-supervised graph representation learning algorithms (i.e. EdgePred <ref type="bibr" target="#b29">(Kipf &amp; Welling, 2016)</ref>, InfoGraph <ref type="bibr" target="#b46">(Sun et al., 2019)</ref>, AttrMasking <ref type="bibr" target="#b24">(Hu et al., 2019)</ref>, ContextPred <ref type="bibr" target="#b24">(Hu et al., 2019)</ref>, GraphPartition <ref type="bibr" target="#b58">(You et al., 2020b)</ref> and GraphCL <ref type="bibr">(You et al., 2020a)</ref>) to verify its effec-  <ref type="formula">2019</ref>) and examine the performance of InfoGraph, GraphPartition and GraphCL based on the released source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments on Chemistry Domain</head><p>Data sets. For fair comparison, we use the same data sets as in <ref type="bibr" target="#b24">Hu et al. (2019)</ref>. In specific, a subset of ZINC15 database <ref type="bibr" target="#b45">(Sterling &amp; Irwin, 2015)</ref> with 2 million unlabeled molecules is employed for self-supervised pre-training. Eight binary classification data sets in MoleculeNet <ref type="bibr" target="#b52">(Wu et al., 2018)</ref> serve as downstream tasks, where the scaffold split scheme <ref type="bibr" target="#b8">(Chen et al., 2012a)</ref> is used for data set split.</p><p>Results. In Tab. 1, we report the performance of the proposed GraphLoG method compared with other works, where 'Random' denotes the GIN model with random initialization. Among all self-supervised learning strategies, our approach achieves the best performance on six of eight tasks, and a 2.1% performance gain is obtained in terms of average ROC-AUC. We deem that this improvement over previous works is mainly from the global structure modeling in GraphLoG, which is not included in existing methods.   proaches with a clear margin, i.e. a 1.7% performance gain. This result illustrates that the proposed approach is able to learn effective graph representations that benefit the downstream task involving fine-grained classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiments on</head><p>In Tab. 3, we further compare GraphLoG with four existing methods under four GNN architectures (i.e. GCN <ref type="bibr" target="#b30">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b20">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b49">(Velickovic et al., 2018)</ref> and GIN <ref type="bibr" target="#b54">(Xu et al., 2019)</ref>).</p><p>We can observe that GraphLoG outperforms the existing approaches on all configurations, and, compared to EdgePred, AttrMasking and ContextPred, it avoids the performance decrease relative to random initialization baseline on GAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis</head><p>Effect of different objective functions. In Tab. 4, we analyze the effect of three objective functions on the biology domain, and we continue using the GIN depicted in Sec. 5.1 in this experiment. When each objective function is individually applied (1st, 2nd and 3rd row), the one for global-semantic learning performs best, which probably benefits from its exploration of the semantic structure of the data. Through simultaneously applying different objective functions, the full model (last row) achieves the best performance, which illustrates that the learning of local and global structure are complementary to each other.</p><p>Sensitivity of hierarchical prototypes' depth L p . In this part, we discuss the selection of parameter L p which controls the number of discovered semantic hierarchies. In Fig. <ref type="figure" target="#fig_4">3</ref>(a), we plot model's performance under different L p values. It can be observed that deeper hierarchical prototypes (i.e. L p 3) achieve stable performance gain compared to the shallow ones (i.e. L p 2). Sensitivity of batch size N . In this experiment, we evaluate the effect of the batch size N on our method. <ref type="bibr">Fig. 3(b)</ref> shows the test ROC-AUC on downstream task using different batch sizes. From the line chart, we can observe that large batch size (i.e. N 256) can promote the performance of GraphLoG. Under such condition, the sampled mini-batches can better represent the whole data set and thus derive more precise likelihood expectation in Eq. 14.</p><p>Visualization. In Fig. <ref type="figure" target="#fig_3">2</ref>, we utilize the t-SNE <ref type="bibr" target="#b35">(Maaten &amp; Hinton, 2008)</ref> to visualize the graph embeddings and hierarchical prototypes on ZINC15 data set. Compared with only using the local constraints L sub and L graph (configurations (a) and (b)), more obvious feature separation is achieved after applying the global constraint L global (configuration (c)), which illustrates its effectiveness on discovering the underlying global-semantic structure of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>We design a unified framework called Local-instance and Global-semantic Learning (GraphLoG) for self-supervised graph representation learning, which models the structure of a set of unlabeled graphs both locally and globally. In this framework, we novelly propose to learn hierarchical prototypes upon graph embeddings to infer the global-semantic structure in graphs. Using the benchmark data sets from both chemistry and biology domains, we empirically verify our method's superior performance over state-of-the-art approaches on different GNN architectures.</p><p>Our future works will include further improving the global structure learning technique, unifying pre-training and finetuning, and extending our framework to other domains such as sociology, physics and material science.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of GraphLoG. (a) Correlated graphs are constrained to be adjacently embedded to pursue the local-instance structure of the data. (b) Hierarchical prototypes are employed to discover and refine the global-semantic structure of the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>representation of node v at the l-th layer, h(0)v is initialized as the node attribute X v , and f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The t-SNE visualization on ZINC15 database (i.e. the pre-training data set for chemistry domain).</figDesc><graphic url="image-1.png" coords="8,376.25,77.13,137.79,82.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Sensitivity analysis of hierarchical prototypes' depth Lp and batch size N . (All results are evaluated on biology domain.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Optimization Algorithm of GraphLoG. Input: Unlabeled graph data set G, the number of learning steps T . Output: Pre-trained GNN model GNN Œ∏ T . Pre-train GNN with local objective function (Eq. 9). Initialize model parameters Œ∏ 0 and C 0 . for t = 1 to T do Sample a mini-batch G from G. ‚ô¶ E-step: Sample latent variables Z est with GNN Œ∏t‚àí1 and C t‚àí1 .</figDesc><table><row><cell>‚ô¶ M-step:</cell></row><row><cell>Update model parameters:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Test ROC-AUC (%) on downstream molecular property prediction benchmarks.</figDesc><table><row><cell>Methods</cell><cell>BBBP</cell><cell>Tox21</cell><cell>ToxCast</cell><cell>SIDER</cell><cell>ClinTox</cell><cell>MUV</cell><cell>HIV</cell><cell>BACE</cell><cell>Avg</cell></row><row><cell>Random</cell><cell>65.8 ¬± 4.5</cell><cell>74.0 ¬± 0.8</cell><cell>63.4 ¬± 0.6</cell><cell>57.3 ¬± 1.6</cell><cell>58.0 ¬± 4.4</cell><cell>71.8 ¬± 2.5</cell><cell>75.3 ¬± 1.9</cell><cell>70.1 ¬± 5.4</cell><cell>67.0</cell></row><row><cell>EdgePred (2016)</cell><cell>67.3 ¬± 2.4</cell><cell>76.0 ¬± 0.6</cell><cell>64.1 ¬± 0.6</cell><cell>60.4 ¬± 0.7</cell><cell>64.1 ¬± 3.7</cell><cell>74.1 ¬± 2.1</cell><cell>76.3 ¬± 1.0</cell><cell>79.9 ¬± 0.9</cell><cell>70.3</cell></row><row><cell>InfoGraph (2019)</cell><cell>68.2 ¬± 0.7</cell><cell>75.5 ¬± 0.6</cell><cell>63.1 ¬± 0.3</cell><cell>59.4 ¬± 1.0</cell><cell>70.5 ¬± 1.8</cell><cell>75.6 ¬± 1.2</cell><cell>77.6 ¬± 0.4</cell><cell>78.9 ¬± 1.1</cell><cell>71.1</cell></row><row><cell>AttrMasking (2019)</cell><cell cols="4">64.3 ¬± 2.8 76.7 ¬± 0.4 64.2 ¬± 0.5 61.0 ¬± 0.7</cell><cell>71.8 ¬± 4.1</cell><cell>74.7 ¬± 1.4</cell><cell>77.2 ¬± 1.1</cell><cell>79.3 ¬± 1.6</cell><cell>71.1</cell></row><row><cell>ContextPred (2019)</cell><cell>68.0 ¬± 2.0</cell><cell>75.7 ¬± 0.7</cell><cell>63.9 ¬± 0.6</cell><cell>60.9 ¬± 0.6</cell><cell>65.9 ¬± 3.8</cell><cell>75.8 ¬± 1.7</cell><cell>77.3 ¬± 1.0</cell><cell>79.6 ¬± 1.2</cell><cell>70.9</cell></row><row><cell cols="2">GraphPartition (2020b) 70.3 ¬± 0.7</cell><cell>75.2 ¬± 0.4</cell><cell>63.2 ¬± 0.3</cell><cell>61.0 ¬± 0.8</cell><cell>64.2 ¬± 0.5</cell><cell>75.4 ¬± 1.7</cell><cell>77.1 ¬± 0.7</cell><cell>79.6 ¬± 1.8</cell><cell>70.8</cell></row><row><cell>GraphCL (2020a)</cell><cell>69.5 ¬± 0.5</cell><cell>75.4 ¬± 0.9</cell><cell>63.8 ¬± 0.4</cell><cell>60.8 ¬± 0.7</cell><cell>70.1 ¬± 1.9</cell><cell>74.5 ¬± 1.3</cell><cell>77.6 ¬± 0.9</cell><cell>78.2 ¬± 1.2</cell><cell>71.3</cell></row><row><cell>GraphLoG (ours)</cell><cell cols="2">72.5 ¬± 0.8 75.7 ¬± 0.5</cell><cell cols="7">63.5 ¬± 0.7 61.2 ¬± 1.1 76.7 ¬± 3.3 76.0 ¬± 1.1 77.8 ¬± 0.8 83.5 ¬± 1.2 73.4</cell></row><row><cell cols="5">Table 2. Test ROC-AUC (%) on downstream biological function</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>prediction benchmark.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell cols="2">ROC-AUC (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell></cell><cell cols="2">64.8 ¬± 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EdgePred</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Test ROC-AUC (%) of different methods under four GNN architectures. (All results are reported on biology domain.)</figDesc><table><row><cell>Methods</cell><cell>GCN</cell><cell>GraphSAGE</cell><cell>GAT</cell><cell>GIN</cell></row><row><cell>Random</cell><cell cols="4">63.2 ¬± 1.0 65.7 ¬± 1.2 68.2 ¬± 1.1 64.8 ¬± 1.0</cell></row><row><cell cols="5">EdgePred (2016) 68.0 ¬± 0.9 67.8 ¬± 0.7 67.9 ¬± 1.3 70.5 ¬± 0.7</cell></row><row><cell cols="5">AttrMasking (2019) 68.3 ¬± 0.8 69.2 ¬± 0.6 67.3 ¬± 0.8 70.5 ¬± 0.5</cell></row><row><cell cols="5">ContextPred (2019) 67.6 ¬± 0.3 69.6 ¬± 0.6 66.9 ¬± 1.2 69.9 ¬± 0.3</cell></row><row><cell cols="5">GraphCL (2020a) 69.1 ¬± 0.9 70.2 ¬± 0.4 68.4 ¬± 1.2 71.2 ¬± 0.6</cell></row><row><cell cols="5">GraphLoG (ours) 71.2 ¬± 0.6 70.8 ¬± 0.8 69.5 ¬± 1.0 72.9 ¬± 0.7</cell></row></table><note>tiveness. We report the results of EdgePred, AttrMasking and ContextPred fromHu et al. (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study for different objective functions on downstream biological function prediction benchmark.</figDesc><table><row><cell>Lsub</cell><cell>Lgraph</cell><cell>Lglobal</cell><cell>ROC-AUC (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>70.1 ¬± 0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.0 ¬± 0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.5 ¬± 0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.3 ¬± 0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.9 ¬± 0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>72.2 ¬± 0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>72.9 ¬± 0.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project was supported by the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft Research and Mila, Samsung Electronics Co., Ldt., Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund and a NRC Collaborative R&amp;D Project (AI4D-CORE-08). This project was also partially funded by IVADO Fundamental Research Project grant PRF2019-3583139727. Bingbing Ni is supported by National Science Foundation of China (U20B2072, 61976137).</p><p>The authors would also like to thank Meng Qu, Shengchao Liu, Zhaocheng Zhu and Zuobai Zhang for providing constructive advices during this project, and also appreciate the Student Innovation Center of SJTU for providing GPUs.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Theoretical Analysis</head><p>Theorem 1. Given a mini-batch G (batch size is N ) randomly sampled from the data set G which contains M graphs, the expected log-likelihood defined on this mini-batch, i.e. Q(Œ∏, C) = E p( Z| G,Œ∏t‚àí1,Ct‚àí1) [log p( G, Z|Œ∏, C)], is approximately proportional to the expected complete-data log-likelihood, i.e. Q(Œ∏, C) = E p(Z|G,Œ∏t‚àí1,Ct‚àí1) [log p(G, Z|Œ∏, C)]:</p><p>Proof. For each graph G n in the mini-batch, a latent variable ·∫ëGn ‚àº p(z Gn |G n , Œ∏ t‚àí1 , C t‚àí1 ) is sampled from the posterior distribution for Monte Carlo estimation, and the mini-batch log-likelihood can be estimated as follows:</p><p>Since the graphs in both mini-batch G and data set G can be regarded as randomly sampled from the data distribution P G , we deduce as below:</p><p>Here, for each graph G m in data set G, a latent variable ·∫ëGm ‚àº p(z Gm |G m , Œ∏ t‚àí1 , C t‚àí1 ) is sampled from the posterior distribution for Monte Carlo estimation.</p><p>Proposition 2. For each EM cycle, the model parameters Œ∏ and C are updated in such a way that increases the marginal likelihood function p(G|Œ∏, C), unless a local maximum is reached on the mini-batch log-likelihood function Q(Œ∏, C).</p><p>Proof. We verify this claim from the perspective of variational inference. For a mini-batch G, we suppose that q( Z) is a variational distribution over the true posterior p( Z| G, Œ∏, C). For any choice of q( Z), the following decomposition of the marginal log-likelihood log p( G|Œ∏, C) holds:</p><p>where L(q, Œ∏, C) is the evidence lower bound (ELBO) of marginal log-likelihood function, i.e. L(q, Œ∏, C) log p( G|Œ∏, C) (equality holds when q( Z) = p( Z| G, Œ∏, C)).</p><p>In the E-step, we set the variational distribution equal to the posterior distribution with respect to the current model parameters, i.e. q( Z) = p( Z| G, Œ∏ t‚àí1 , C t‚àí1 ), such that the KL divergence term vanishes, and the ELBO equals to the marginal log-likelihood log p( G|Œ∏ t‚àí1 , C t‚àí1 ). If we substitute q( Z) with p( Z| G, Œ∏ t‚àí1 , C t‚àí1 ) in the ELBO term, we see that, after the E-step, it takes the following form:</p><p>where H denotes the entropy function.</p><p>In the M-step, the variational distribution q( Z) is fixed, and thus the ELBO term equals to the expected mini-batch log-likelihood Q(Œ∏, C) plus a constant:</p><p>In this step, we seek to maximize Q(Œ∏, C) with respect to model parameters Œ∏ and C, which will increase the value of L(q, Œ∏, C) unless a local maximum is reached on Q(Œ∏, C). Except for the local maximum case, there will be new values of Œ∏ and C, denoted as Œ∏ t and C t , which gives out that:</p><p>Denoting the increase of the ELBO term after the M-step as ‚àÜL(q, Œ∏, C) = L(q, Œ∏ t , C t ) ‚àí L(q, Œ∏ t‚àí1 , C t‚àí1 ) &gt; 0, the increase of the marginal log-likelihood satisfies that:</p><p>where the KL term of log p( G|Œ∏ t‚àí1 , C t‚àí1 ) vanishes due to the operation in the E-step. Similar as the deduction in Theorem 1, the complete-data log-likelihood log p(G|Œ∏, C) and the mini-batch log-likelihood log p( G|Œ∏, C) have the following relation:</p><p>From this relation, we can derive that:</p><p>which illustrates that the EM cycle in our approach is able to increase the complete-data marginal likelihood p(G|Œ∏, C) except that a local maximum is reached on Q(Œ∏, C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Implementation Details</head><p>Attribute masking scheme. For the chemistry domain, given a molecular graph, we randomly mask the attributes of 30% nodes (i.e. atoms) in it to obtain its correlated counterpart. Specifically, we add an extra dimension to the feature of atom type and atom chirality to indicate masked attribute, and the input features of all masked atoms are set to these extra dimensions.</p><p>For the biology domain, given a protein ego-network, we randomly mask the attributes of 30% edges in it to derive its correlated counterpart. In specific, we use an extra dimension to indicate masked attribute. For an edge to be masked, the weight of its extra dimension is set as 1, and the weights of all other dimensions are set as 0.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new protein graph model for function prediction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Biology and Chemistry</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="6" to="10" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gene ontology: tool for the unification of biology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dolinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dwight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Eppig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Genetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DIFFRAC: a discriminative and flexible framework for clustering</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Grarep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On-line expectationmaximization algorithm for latent data models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Capp√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="593" to="613" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A classification em algorithm for clustering and two stochastic versions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Govaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="332" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Comparison of random forest and pipeline pilot naive bayes in prospective qsar predictions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hornak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Voigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="792" to="803" />
			<date type="published" when="2012">2012a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting anatomical therapeutic chemical (atc) classification of drugs by integrating chemical-chemical interactions and similarities</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">e35254</biblScope>
			<date type="published" when="2012">2012b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR, abs/2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>G√≥mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Information retrieval using a singular value decomposition model of latent semantic structure</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Durnais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Streeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Lochbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Forum</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garc√≠a-Dur√°n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyv√§rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyv√§rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Another interpretation of the em algorithm for mixture distributions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hathaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="56" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>CoRR, abs/1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>CoRR, abs/1905.12265</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gptgnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aptrank: an adaptive pagerank model for protein function prediction on bi-relational graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kloster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gribskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1829" to="1836" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The em algorithm</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Series in Probability and Statistics: Applied Probability and Statistics</title>
				<imprint>
			<publisher>WileyInterscience</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno>CoRR, abs/2005.04966</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online em for unsupervised models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to pre-train graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2605. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning distributed representations of graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<idno>CoRR, abs/1707.05005</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A view of the em algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR, abs/1811.06930</idno>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">2018. 1998</date>
		</imprint>
	</monogr>
	<note>Pre-training graph neural networks with kernels</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The stochastic em algorithm: Estimation and asymptotic results</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="457" to="489" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepwalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GCC: graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on largescale molecular data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On-line em algorithm for the normalized gaussian network</title>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="432" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zinc 15-ligand discovery for everyone</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno>CoRR, abs/1908.01000</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">LINE: large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 2020a. You</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Circuit-gnn: Graph neural networks for distributed circuit design</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An end-toend deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">are with 5 layers, 300-dimensional hidden units and a mean pooling readout function</title>
		<author>
			<persName><surname>Graphsage (hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GNN architecture</title>
				<editor>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Gcn</surname></persName>
		</editor>
		<editor>
			<persName><surname>Kipf</surname></persName>
		</editor>
		<editor>
			<persName><surname>Welling</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2018. 2019</date>
		</imprint>
	</monogr>
	<note>All the GNNs in our experiments. In addition, two attention heads are employed in each layer of the GAT model</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
