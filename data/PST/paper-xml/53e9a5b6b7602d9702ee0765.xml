<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face Localization and Authentication Using Color and Depth Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Filareti</forename><surname>Tsalakanidou</surname></persName>
							<email>filareti@iti.gr</email>
						</author>
						<author>
							<persName><forename type="first">Sotiris</forename><surname>Malassiotis</surname></persName>
							<email>malasiot@iti.gr</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Michael</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
							<email>strintzi@eng.auth.gr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Aristotle University of Thessaloniki</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">In-formatics and Telematics Institute</orgName>
								<orgName type="department" key="dep2">Centre for Research and Technology Hellas (ITI/CERTH)</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Informatics and Telematics Institute</orgName>
								<orgName type="department" key="dep2">Centre for Research and Technology Hellas (ITI/CERTH)</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Computer Engineering Depart-ment</orgName>
								<orgName type="institution">Aristotle University of Thessaloniki</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Centre for Research and Technology Hellas (ITI/CERTH)</orgName>
								<orgName type="institution">the Informatics and Telematics Institute</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Face Localization and Authentication Using Color and Depth Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E43AC13F27C05C33ADF16128EFEDA5DD</idno>
					<idno type="DOI">10.1109/TIP.2004.840714</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Depth maps</term>
					<term>embedded hidden Markov models</term>
					<term>face authentication</term>
					<term>face localization</term>
					<term>face recognition</term>
					<term>fusion</term>
					<term>illumination</term>
					<term>pose</term>
					<term>synthetical views</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a complete face authentication system integrating both two-dimensional (color or intensity) and three-dimensional (3-D) range data, based on a low-cost 3-D sensor, capable of real-time acquisition of 3-D and color images. Novel algorithms are proposed that exploit depth information to achieve robust face detection and localization under conditions of background clutter, occlusion, face pose alteration, and harsh illumination. The well-known embedded hidden Markov model technique for face authentication is applied to depth maps and color images. To cope with pose and illumination variations, the enrichment of face databases with synthetically generated views is proposed. The performance of the proposed authentication scheme is tested thoroughly on two distinct face databases of significant size. Experimental results demonstrate significant gains resulting from the combined use of depth and color or intensity information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>provides highly discriminatory information and is insensitive to environmental conditions, only a few techniques have been proposed that are based on range or depth images. This is mainly due to the high cost of available 3-D digitizers that makes their use prohibitive in real-world applications. Furthermore, these devices often do not operate in real time (e.g., time of flight laser scanners) or produce inaccurate depth information (e.g., stereo vision).</p><p>The work presented in this paper is partly motivated by the recent development of novel low-cost 3-D sensors that are capable of real-time 3-D acquisition <ref type="bibr" target="#b1">[2]</ref>. Another motivation comes from the fact that the 3-D structure of the face may be exploited to discriminate among individuals or aid 2-D face authentication, since the face shape data is not sensitive to variations of illumination, face pigmentation, and cosmetics, and is less sensitive to use or nonuse of glasses and facial expressions compared to 2-D surface reflectance data represented by 2-D intensity images.</p><p>A common approach adopted toward 3-D face recognition is based on the extraction of 3-D facial features by means of differential geometry techniques. Facial features invariant to rigid transformations of the face may be detected using surface curvature measures <ref type="bibr" target="#b2">[3]</ref>. Curvature information is subsequently used to extract higher level facial features. The extended Gaussian image (EGI) has been used in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref> for the representation of regions corresponding to distinct facial features. A knowledge-based approach for feature extraction has been adopted in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref>. An alternative method proposed for 3-D surface feature extraction is by means of point signatures. An application of this technique for face recognition is presented in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>, where the problem of recognizing faces is treated as a problem of nonrigid object recognition.</p><p>Feature-based face recognition techniques based on a combination of 3-D and grayscale images are used in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>. The grayscale image in <ref type="bibr" target="#b9">[10]</ref> is used to guide the detection of facial features on the 3-D surface (the eyes, for example, are more easily detected on the intensity image), while the 3-D image is used to compensate for the pose of the face.</p><p>The most important argument against techniques using a feature-based approach is that they rely on accurate 3-D maps of faces, usually extracted by expensive off-line 3-D scanners. Low-cost scanners, however, produce very noisy 3-D data. The applicability of feature-based approaches when using such data is questionable, especially if computation of curvature information is involved. Also, the computational cost associated with the extraction of the features (e.g., curvatures) is significantly high. This hinders the application of such techniques in real-world security systems. The recognition rates claimed by the above techniques <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b9">[10]</ref> were estimated using databases of limited size and without significant variations of the faces. Only recently Chang et al. <ref type="bibr" target="#b10">[11]</ref> conducted an experiment with a database of significant size (275 persons) containing both grayscale and range images, and produced comparative results of face identification using eigenfaces for 2-D, 3-D, and their combination and for varying image quality. This test, however, considered only frontal images with neutral expression, captured under constant illumination conditions.</p><p>The problem of face authentication using 3-D images was only investigated under ideal conditions, both regarding 3-D image quality and environmental and facial variability. In practice, however, the sensors are imperfect, more so in systems suitable for mass screening. Face characteristics change with time, with lighting conditions, with pose, and with face expressions; moreover, on-line, split-second authentication is needed. This far from trivial face authentication problem has not been solved by any methodology thus far proposed in the literature. To address some of the problems usually reported in 2-D and 3-D face classification, an appearance-based face authentication and recognition system integrating 3-D range data and 2-D color or intensity images is presented in this paper. The proposed system uses a structured light approach based on inexpensive off-the-self components, by which a 3-D depth map of the face along with its color image become available. It is emphasized that the proposed face classification scheme may be easily combined with the majority of existing 2-D systems to improve their performance and that it is capable of real-time face authentication, notwithstanding arbitrary pose, lighting, and expression variations.</p><p>Apart from the combination of 2-D and 3-D information, this paper introduces several novel techniques that exploit the availability of 3-D information to improve classification accuracy.</p><p>1) A novel face detection and localization method using a combination of color and depth data is presented. The algorithm exhibits robustness to background clutter, occlusion, face pose variation, and harsh illumination conditions by exploiting depth information and prior knowledge of face geometry and symmetry. The fast response of the algorithm makes it suitable for real-time applications. 2) Unlike techniques that rely on an extensive training set to achieve high recognition rates, our system requires only a few images per person. This is achieved by enriching the original database with synthetically generated images, depicting variations in pose and illumination. Thus, a cum-bersome enrollment stage is avoided, while better recognition rates are achieved. Although the idea of enlarging the training set with novel examples is not new, the investigation of such techniques for 3-D face classification is novel.</p><p>3) The performance of the proposed system is extensively evaluated using two face databases of significant size (2500 images each) containing several facial and lighting variations. The combination of 2-D and 3-D information was shown to produce consistently superior results in comparison with a 2-D state-of-the-art algorithm, demonstrating at the same time near real-time performance. The paper is organized as follows. A brief description of the 3-D sensor is given in Section II. Face localization based on depth and color information is described in Section III, while in Section IV, the embedded hidden Markov model (EHMM) method for both color and depth images is outlined. In Section V, we examine algorithms for the enrichment of face databases with novel views. Experimental results evaluating the developed techniques are presented in Section VI. Finally, a discussion of our achievements, limitations, and ideas for future work is presented in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ACQUISITION OF 3-D AND COLOR IMAGES</head><p>A 3-D and color camera capable of synchronous real-time acquisition of 3-D images and associated color 2-D images is employed <ref type="bibr" target="#b1">[2]</ref>. The 3-D data acquisition system is based on an active triangulation principle, making use of an improved and extended version of the well-known coded light approach (CLA) for 3-D data acquisition. The basic principle lying behind this device is the projection of a color-encoded light pattern on the scene and measuring its deformation on the object surfaces (see Fig. <ref type="figure" target="#fig_0">1</ref>). The 3-D camera achieves real-time image acquisition for range images and near real-time (14 fps) for color plus depth images on a standard PC. It is based on low-cost devices, an off-the-shelf CCTV-color camera, and a standard slide projector <ref type="bibr" target="#b1">[2]</ref>. By rapidly alternating the color-coded light pattern with a white-light pattern, both color and depth images are acquired (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>For the experiments in this paper, the system was optimized for an access control application scenario, leading to an average depth accuracy of 1 mm for objects located about 1 m from the camera in an effective working space of 60 50 50 cm. Computed depth values are quantized into 16 bits. The spatial resolution of the range images is equal to the horizontal color camera resolution in the horizontal dimension, while in the vertical direction it is dependent on the width of the color stripes of the projected light pattern and the bandwidth of the surface signal. For a low-bandwidth surface, such as the face, the vertical range image resolution is close to the vertical resolution of the color camera. For objects outside the working volume, the projected pattern will be fuzzy leading to undetermined depth values. The acquired range images contain artifacts and missing points, mainly over areas that cannot be reached by the projected light and/or over highly refractive (e.g., eye glasses) or low-reflective surfaces (e.g., hair and beard).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FACE LOCALIZATION USING COLOR AND DEPTH INFORMATION</head><p>Although face detection is an ongoing subject of research, face localization is a problem rarely addressed in face recognition literature. While face detection is most concerned with roughly finding all the faces in large, complex images, which include many faces and much clutter, localization emphasizes spatial accuracy, usually achieved by accurate detection of facial features.</p><p>Several face detection techniques have been proposed for grayscale images <ref type="bibr" target="#b11">[12]</ref>. These may be roughly categorized to those based on the detection of facial features, possibly exploiting their relative geometric arrangement, and those based on the classification of the brightness pattern inside an image window, obtained by exhaustively sweeping the whole image as face or nonface. Techniques in the second category were recently shown to be more successful in detecting faces in cluttered backgrounds <ref type="bibr" target="#b12">[13]</ref>; however, the correct detection rates reported were below 90%. Further shortcomings of existing face detection algorithms include their sensitivity to partial occlusion of the face (e.g., glasses and hair), hard illumination and head pose, as well as their computational complexity.</p><p>Color information, when available, is a powerful cue for detecting the face <ref type="bibr" target="#b13">[14]</ref>. However, the parameters of the color distribution were shown to rely on the environmental illumination and the response characteristics of the acquisition device. Furthermore, irrelevant skin-colored image regions will result in erroneous face candidates.</p><p>More robust face localization may be achieved by using depth information. In this paper, a face localization procedure combining both depth and color data is proposed. By exploiting depth information, the human body may be easily separated from the background, while by using a priori knowledge of its geometric structure, efficient segmentation of the head from the body (neck and shoulders) is achieved. The position of the face is further refined using the color image to locate the point that lies just above the nose, in between the eyes, by exploiting face symmetry.</p><p>In the following, we shall assume that the target face is the closest object to the camera, as is reasonable to assume for all access control applications. Even in a surveillance application, where multiple faces may be visible, the face closer to the camera will be selected for recognition, since the faces further back will be classified as background. With a small effective working space of 60 50 50 cm (see Section II), little background clutter exists. Even with a large working volume, where 50% of image pixels are on the background, the proposed detection technique works successfully. This is due to the reliance of the structured light approach on the projection of a light pattern on the object. For objects outside the working volume, the projected pattern will be fuzzy, leading to undetermined depth values over these areas. In other words, the 3-D sensor automatically segments the foreground from the background, i.e., objects within the working volume from objects outside the working volume. Nevertheless, there may be background objects inside the working volume, for example, when another person stands right in the back. Even in such a case, the depth histogram contains two sufficiently distinct modes (body and background) that may be separated with simple thresholding. The optimal threshold is selected using the algorithm in <ref type="bibr" target="#b14">[15]</ref>.</p><p>Segmentation of the head from the body relies on statistical modeling of the head-torso points in 3-D space, inspired by the approach adopted in <ref type="bibr" target="#b15">[16]</ref> for the segmentation of the arm from the body. A novel technique for the initialization of 3-D blob parameters is introduced, to ensure the successful and rapid convergence of the expectation-maximization (EM) algorithm. The probability distribution of a 3-D point is modeled as a mixture of two Gaussians (1) <ref type="bibr" target="#b1">(2)</ref> where are prior probabilities of the head and torso, respectively, and is the 3-D Gaussian distribution with mean and covariance . Maximum-likelihood estimation of the unknown parameters from the 3-D data is obtained by means of the EM algorithm <ref type="bibr" target="#b16">[17]</ref>. To avoid the convergence of the algorithm to local minima, good initial parameter values are required. In our case, these may be obtained by using 3-D moments, i.e., center of mass and covariance matrix. Let be the center of mass, the scatter matrix computed from the data points , and the eigenvectors of , ordered according to the magnitude of the corresponding eigenvalues . Initial estimates of the unknown parameters are selected by where is the orthogonal eigenvector matrix of , while are constants related to the relative size of the head with respect to the torso (in the experiments, , and were used). The physical interpretation of the above parameter selection is illustrated in Fig. <ref type="figure">2</ref>. That is, 3-D blob centers and covariances are initialized based on prior knowledge of the human body structure (e.g., head above torso) and relative dimension of body parts. In the case that no torso is apparent in the effective Fig. <ref type="figure">2</ref>. Illustration of knowledge-based initialization of 3-D blob distribution parameters. Ellipses represent iso-probability contours of posterior distributions. The lengths of the axes of the ellipses are selected on the basis of the iso-probability ellipse estimate computed using all 3-D data. working space, for example, when someone is wearing a dark shirt, the prior probability for the pixel blobs corresponding to the torso converges to zero.</p><p>A 3-D point is classified as head or torso using the maximum likelihood criterion. Experimental results demonstrate robustness of the algorithm to head orientation variability and moderate occlusion of the face (e.g., hand in front of the face; see Fig. <ref type="figure" target="#fig_1">3</ref>) leading to correct classification of face pixels in almost 100% of the images.</p><p>One shortcoming of the above iterative procedure is that the estimated segmentation is biased by erroneous depth estimates. In fact, since one side of the face is partially occluded from the structured light source, no depth is computed over the corresponding image region, hence the center of the face may not be accurately localized. Therefore, a second step is required that refines the localization using brightness information.</p><p>The aim of this second step is the localization of the point that lies in the middle of the line segment defined by the centers of the eyes. Then, an image window containing the face is centered around this point, thus achieving approximate alignment of facial features in all images, which is very important for face classification. A novel localization technique is proposed exploiting the highly symmetric structure of the face. The estimation of the horizontally oriented axis of bilateral symmetry between the eyes is sought first. Then, the vertically oriented axis of bilateral symmetry between the eyes is estimated. The intersection of these two axes defines the point of interest.</p><p>Based on the technique described in <ref type="bibr" target="#b17">[18]</ref>, which proposes a measure of bilateral symmetry for arbitrary 2-D objects, we propose a novel method for the estimation of the symmetry axis of faces by minimization of the above symmetry measure using the Hough transform. Furthermore, we introduce the idea of using the symmetry of the face to localize its center. Let be any image point. Then, is the intensity gradient at point . We also denote by and , respectively, the magnitude and phase of the gradient vector. For a pair of points and , let be their midpoint and the line that passes through and is perpendicular to the direction defined by and . Then, a measure of reflectional symmetry of the two points about the line is given by Let the line of symmetry be defined in the parametric form</p><p>, where is the slant of the line and its distance from the origin. The unknown parameters are estimated by means of a Hough transform technique described in the following.</p><p>A bounding box large enough to contain the eyes with certainty is calculated from the initial depth segmentation, using prior knowledge of the average dimensions of the eyes on the image, thus constraining the range of values of . The rotation of the head is assumed to be limited . Then, the range of parameter values is efficiently quantized and a 2-D Hough accumulator table is created (30 60 bins). Each pair of quantized parameter values defines a candidate symmetry axis . For each pixel on this line, we gather all intensity edge pairs and such that and , where is a constant that equals approximately the maximum vertical pixel size of the eyes in the image (approximately 1.5 cm in 3-D space). Then, for each pair, is incremented by . The symmetry axis is finally found by finding the cell in table which received maximum votes. The vertical axis of symmetry is similarly estimated. in this case equals approximately the inter-occular distance recorded in the image (6.5 cm on the average in 3-D space). The intersection of the estimated symmetry axes defines the center of the face. A window of constant aspect ratio is centered on this point, while its size is scaled according to the depth value of the center point.</p><p>Using the above two-step procedure, highly robust and computationally efficient face localization is achieved (Fig. <ref type="figure" target="#fig_1">3</ref>). We have tested the algorithm on 500 images, where the center of the face was manually specified. The average localization error was four pixels in the horizontal and three pixels in the vertical direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FACE CLASSIFICATION</head><p>Face classification aims to identify individuals by means of discriminatory facial attributes extracted from one or more images belonging to the same person. The challenge is, therefore, in the selection of appropriate features and their efficient matching. Face classification techniques can be roughly divided into two main categories: global approaches and feature-based techniques. In global approaches, the whole image serves as a feature vector, while in local feature approaches, a number of fiducial or control points are extracted and used for classification. Global approaches model the variability of the face by analyzing its statistical properties based on a large set of training images. Representative global techniques are eigenfaces <ref type="bibr" target="#b18">[19]</ref>, linear/Fisher discriminant analysis (LDA) <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, support vector machines (SVM) <ref type="bibr" target="#b21">[22]</ref>, and hidden Markov models (HMMs) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Feature-based techniques, on the other hand, discriminate among different faces based on measurements of structural attributes of the face. More recent approaches include elastic graph matching <ref type="bibr" target="#b24">[25]</ref> and dynamic link architecture <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>.</p><p>In this paper, we employ the EHMM technique in <ref type="bibr" target="#b22">[23]</ref> as a baseline classifier for both color and depth images. We have also tested other state-of-the-art algorithms, such as LDA and elastic bunch graphs, and we have obtained similar results. The EHMM algorithm models the face as a sequence of states corresponding to homogeneous image regions. The states are initialized so that they are roughly aligned to facial features <ref type="bibr" target="#b28">[29]</ref>. The probability distribution function corresponding to each state of the EHMM is approximated by a mixture of Gaussians. The parameters of the distribution are estimated given observations extracted from training images <ref type="bibr" target="#b28">[29]</ref>. The observation sequence for a face image is formed by the low-frequency DCT coefficients calculated from image blocks that are extracted by scanning the image from left-to-right and top-to-bottom <ref type="bibr" target="#b22">[23]</ref>. For color images, the image is transformed from RGB to YUV color space to decorrelate the color components and the DCT is applied to every component separately. Then, the observation vector is constructed by concatenation of the color components. The EHMM technique was applied to depth maps, as well.</p><p>Face classification begins with the extraction of the observations from the input face image. Next, the trained EHMMs in the database are used to calculate the probability of the observations, given the EHMM of each person. The model presenting the highest probability is considered to reveal the identity of the person appearing in the input image.</p><p>Two independent EHMM classifiers, one for color and one for depth, are combined to handle both color and depth information. In general, the use of multimodal classifiers aims at exploiting the extra information that independent classifiers can offer, in order to achieve higher recognition rates. Since classifiers based on depth information demonstrate robustness in handling cases where color or intensity-based classifiers show weakness, and vice versa, their combination results in increased recognition robustness in all cases. Let be the set of scores computed for each EHMM model in the database using the color classifier. Similarly, let be the set of scores estimated by the depth classifier. Each set of scores is normalized in the range [0,1] and a new set of combined scores is computed by <ref type="bibr" target="#b2">(3)</ref> where and are normalizing constants, chosen experimentally once during the training of the classifier. The element of with the lowest value (highest similarity) corresponds to the identity of the most similar person. The fusion technique described above was shown elsewhere to give the best recognition results <ref type="bibr" target="#b29">[30]</ref>.</p><p>The proposed method combines color and 3-D information in order to exploit the advantages of both approaches and deal successfully with the cases where one alone is not sufficient for accurate identification. In the subsequent sections, we will provide experimental results to substantiate our claim that such an approach results in a more robust recognition/authentication scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ENROLLMENT</head><p>One of the main problems in face recognition is that facial appearance is distorted by, for example, seasonal changes (aging, hairstyle, usage of cosmetics, etc.), rotation, harsh or heterogeneous illumination, and occlusions caused by glasses, scarves, etc. This problem may be partly alleviated by recording a rich training database containing representative variations. Such an approach is shown to lead to improved recognition/authentication rates <ref type="bibr" target="#b30">[31]</ref>.</p><p>In this paper, a database enrichment procedure is described that avoids a cumbersome enrollment process. A small set of images (normally less than five per person) depicting different facial expressions with and without eyeglasses are originally recorded. These are subsequently used to create canonical face images (upright pose). For each canonical image pair (color and depth), a 3-D model is constructed and used to automatically generate artificial views of the face depicting pose and illumination variations. The various steps of this procedure are described in the sequel. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Surface Interpolation</head><p>As mentioned above, face authentication algorithms may cope with moderate pose variations by enriching the training database with a set of novel views. In our case, this requires that the depth images do not contain holes. If some pixels are undetermined in the depth image, the corresponding pixels will also be missing in the artificially generated color images depicting various pose and illumination combinations. In practice, more than 20% of the face surface pixels are missing, mainly over occluded points on one side of the nose and face or over the eyes, for people wearing glasses. Therefore, a semiautomated surface interpolation procedure has been developed that exploits face symmetry. Missing pixels from the right part of the face may be copied from symmetrically located pixels on the left part of the face, or vice versa. This procedure requires the estimation of the vertical bilateral symmetry axis with high accuracy.</p><p>In order to achieve optimal rectification of training images during the enrolment of a new user, the estimation of symmetry axis is not performed automatically as described in Section III, but manually. The user has to mark the two points corresponding to the centers of the eyes and another point close to the chin or mouth on the input image, so that the vertical symmetry axis of the face is defined. The complete enrollment time is about 2-3 min for an experienced user, since less than 10 s are required for this step, and the enrollment of a new person in the database requires about five images of this person. Once the three interest points have been located and the corresponding depth values have been computed from the depth image, a 3-D coordinate frame is defined centered on the face. A warping procedure is applied to align this local coordinate frame with the frame of the camera, thus bringing the face in upright orientation. Then, the following operation is applied on the depth image : if originally (i.e., if the depth of is originally missing), then , where is the point that is reflectionally symmetric to . Note that this procedure is only applied to missing pixels of the depth map not the color image. Also, since these pixels are normally on the sides of the face and over low-reflectivity areas, the lower part of the face is not affected, and, thus, inherent facial asymmetry, which may be a cue for discrimination, is not violated.</p><p>After the symmetry-based interpolation, some pixel values may still remain undefined. These missing points are linearly interpolated using neighboring points. Since the dimension and shape of the holes varies, a 2-D Delaunay triangulation procedure is applied <ref type="bibr" target="#b31">[32]</ref>. In Fig. <ref type="figure" target="#fig_2">4</ref>, the various steps of the surface interpolation procedure are illustrated. To compute the corresponding rectified color image, the inverse 3-D transformation is first applied to the interpolated depth map. Then, the forward 3-D transformation is applied to the color image using the back projected interpolated depth values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Creation of Artificially Rotated Depth and Color Images</head><p>Pose variation presents one of the most challenging problems in face recognition, leading to significant deterioration in the performance of face recognition algorithms, since the resulting changes in appearance can be greater than the perceived variability between persons <ref type="bibr" target="#b32">[33]</ref>. Several techniques have been proposed to recognize faces under varying pose. One approach is the automatic generation of novel views resembling the pose in the probe image. This is achieved either by using a face model (active appearance model in <ref type="bibr" target="#b33">[34]</ref> and deformable 3-D model in <ref type="bibr" target="#b34">[35]</ref>) or by warping frontal images using the estimated optical flow between probe and gallery <ref type="bibr" target="#b35">[36]</ref>. Classification is, subsequently, based on the similarity between the probe image and the generated view.</p><p>An approach quite different from the above is based on building a pose varying eigenspace by recording several images of each person under varying pose. Representative techniques are the view-based subspace of <ref type="bibr" target="#b36">[37]</ref> and the predictive characterized subspace of <ref type="bibr" target="#b37">[38]</ref>. The proposed technique may be classified in this later category. However, the view-based subspace is automatically generated from a few frontal view images exploiting the information about the 3-D structure of the human face.</p><p>In order to create artificially rotated depth maps and associated color images, a 3-D model of the face is first constructed. The 3-D model consists of quadrilaterals, where the vertices of each quadrilateral are computed from the depth values of four neighboring pixels on the image grid. The 3-D coordinates of each vertex are computed from the corresponding depth value using camera calibration parameters. Also, associated with each vertex is the color value of the corresponding pixel in the color image.</p><p>A global rotation is subsequently applied to the vertices of the 3-D model. The rotation center is fixed in 3-D space so that its projection corresponds to the point between the eyes and it is at a fixed distance from the camera. The transformed 3-D model is subsequently rendered using the -buffer rendering algorithm <ref type="bibr" target="#b38">[39]</ref>, giving rise to a synthetic color and range image (Fig. <ref type="figure" target="#fig_3">5</ref>).</p><p>To limit the effect of points on the face coming into view when the face model is rotated, a mask that includes the central part of the face is used.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Simulating Illumination</head><p>Another source of variation in facial appearance is the illumination of the face. Several techniques have been proposed to cope with varying illumination. The majority of these techniques exploits the low dimensionality of the face space under varying illumination conditions and the Lambertian assumption <ref type="bibr" target="#b39">[40]</ref>. They either use several images of the same person recorded under varying illumination conditions <ref type="bibr" target="#b40">[41]</ref> or rely on the availability of 3-D face models and albedo maps <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref> to generate novel views. The main shortcoming of this approach is the requirement in practice of large example sets to achieve good reconstructions.</p><p>Our approach, on the other hand, builds an illumination varying subspace by constructing artificially illuminated color images from an original image. This normally requires availability of surface gradient information, which, in our case, may be easily computed from depth data. Since it is impossible to simulate all types of illumination conditions that one may find in the real world, we try to simulate those conditions that have the greatest effect in face recognition performance. Heterogeneous shading of the face caused by a directional light coming from one side of the face was experimentally shown to be most commonly liable for misclassification.</p><p>Given the surface normal vector computed over each point of the surface and the direction of the artificial light source specified by the azimuth angles and , the RGB color vector of a pixel in the artificial view is given by where is the corresponding color value in the original view, and weigh the effect of ambient light and diffuse reflectance respectively ( were chosen in our experiments to approximate the average reflectance of the human face). Fig. <ref type="figure" target="#fig_4">6</ref> shows an example of artificially illuminated views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL EVALUATION</head><p>The focus of the experimental evaluation was to show that the proposed integration of 2-D color (or intensity) information and 3-D range data demonstrates superior performance compared with state-of-the-art 2-D face authentication techniques and also robustness against natural facial and environmental variability. Two databases were recorded comprised of 2-D color images and their corresponding depth maps. In order to make the experimental results comparable to public evaluations of face recognition algorithms and systems, international practices such as <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b45">[46]</ref> were followed, regarding the evaluation concept and database recording methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. First Evaluation Database</head><p>The first face database for evaluation contains 2818 recordings of 50 individuals, where each recording consists of a color image (571 752 pixels, 24 bits/pixel) and the corresponding depth map (571 752 pixels, 16 bits/pixel). The images were recorded by the 3-D sensor prototype during a period of three months     various expressions, poses, as well as illumination variations during the recording of a single session, can be seen in Fig. <ref type="figure" target="#fig_7">8</ref>. During the recording of the database sessions, intraperson variability due to different hairstyle, makeup, suntan, beard, and glasses, was also observed, as can be clearly seen in Fig. <ref type="figure" target="#fig_8">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Second Evaluation Database</head><p>Due to a miscalibration of the camera during recording sessions, the quality of color data in the first database did not allow testing of color images or combination of color and depth data. Also, improved 3-D sensor prototype was developed, which led to better depth quality compared to the first database. In order to evaluate the combination of color and depth images for face authentication, as well as to examine the effect of data quality in the performance of the system, a second evaluation database was recorded. It contains 2244 recordings of 20 individuals captured during two sessions in an indoor office environment (see Table <ref type="table" target="#tab_0">I</ref>). The time between the recording sessions was at least one week. For this database, only 20 male individuals were available, other than those participating in the first database. The age of the participants was between 28 and 60.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training and Testing</head><p>For the training of the EHMMs, five image pairs per subject (two frontal, two expressions, and one with eyeglasses) taken from a single recording session were used. For the test of the face authentication system, all images of the remaining sessions were used. The procedure described in Section V was followed for every training image pair: for every pair of depth and color images, a surface interpolation method was performed to cover missing pixels in the depth map and bring the face in upright orientation. The resulting images were scaled and cropped to dimensions 140 200. Artificial views depicting pose and/or illumination variations were then generated from the resulting rectified enrollment images. Since it is impossible to simulate the full range of facial poses and all orientations of the light source, we have experimentally determined the range of face orientations that are typical for cooperating users. This range is to around axis and and to around axis . As far as the illumination parameters are concerned, we consider azimuth angles and equal to and 0 , respectively. A shift parameter was also considered to compensate for any misalignment during face localization.</p><p>Three training sets were created using different enrichment schemes. Each training set is abbreviated as TSi, in the following. TS1 is comprised of the original (rectified) enrollment images and does not contain synthetical views. TS2 results from the enrichment of TS1 with views depicting artificial rotations of and around axes . A shift of pixels along the horizontal axis was also incorporated. For every image pair of TS1, 124 synthetical views were generated in TS2. For the creation of TS3, all images in TS2 were artificially illuminated, setting and . We have trained three EHMM classifiers for each training set, using depth, color, and intensity images, respectively. Several combinations of EHMM parameters were experimentally tested. The optimal number of EHMM superstates was shown to be 3, 7, 7, 7, 7, and 3, meaning that the embedded hidden markov model consists of six superstates with 3, 7, 7, 7, 7, and 3 states each. A sampling window of 12 12 pixels presented the best results, while an observation window 3 3 (number of DCT coefficients to form an observation) was selected. Since the global intensity of the light affects only the dc component of the 2-D-DCT, while the rest of the coefficients are relatively unaffected <ref type="bibr" target="#b28">[29]</ref>, the dc coefficient is not taken into account in our experiments. Investigating the optimal size of the overlap along axes and , we found that 10 10 is a good compromise. Higher values of overlap lead to high computational load and delay the response of the verification system. The selection of the color space is also of great importance. The YCrCb color space yielded the best results among different color spaces tested.</p><p>The test phase consists of a face localization step and a face classification step. The result of face localization on an input image (as described in Section III) is a pair of cropped images (140 200 pixels). Each image contains the face of the person, so that the point between the eyes is in the center of the image. A simple linear surface interpolation is then applied to the depth image, so that missing pixels values are interpolated. A mask of elliptical shape and standard dimension is used to crop parts of the face near the boundaries of the image. In cases where only 2-D information is used (intensity or color approach), the face detection algorithm described in <ref type="bibr" target="#b46">[47]</ref> was applied. The algorithm was trained using manually cropped images from the training set and attains similar accuracy with the 3-D-based face localization, but at a very high computational cost.</p><p>The face classification system takes as input the images resulting from the previous step and employs the EMMM classification method. The probability of the observations is calculated for every enrolled person. The person associated with the highest resulting probability score is identified as the person in the test image (see Section IV).</p><p>The weights for the combination approaches (Section IV) were chosen experimentally. Multiple tests were performed on a set of test images, other than those employed for training, varying the values of in the range (0, 1), with . The selection is made in terms of minimizing the total equal error rate (EER). The EER values were plotted versus . The resulting curve exhibits a global minimum for a specific value of . This value is used for the combination of authentication scores resulting from the use of different modalities and it is kept fixed for all tests.</p><p>For each pair of test images five tests are performed, corresponding to different modality combination: depth, color, intensity, combination of depth and color, and combination of intensity and depth. The experimental results are described in detail in Section VI-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Performance Evaluation:</head><p>In this section, we evaluate the performance of the proposed face authentication system in terms of the receiver operating characteristic (ROC) diagrams and the equal error rate (EER), which are established methods for the comparison of face authentication algorithms.</p><p>Different operating points of biometric systems are commonly illustrated in a ROC diagram, where the false acceptance rate (FAR) values versus the corresponding false rejection rate (FRR) values are plotted. FAR is defined as the percentage of instances that a nonauthorized individual is falsely accepted by the system, while FRR is defined as the percentage of instances an authorized individual is falsely rejected by the system. The (FAR, FRR) pairs calculated at a specific operating point of the system (threshold of the maximum observation probability in our case) define a point on the ROC.</p><p>Comparison of different algorithms may be performed by comparing their ROC curves (the one that is closer to the lower left corner is the one demonstrating better performance) or by comparing their equal error rates. EER corresponds to the FAR value on an operating point such as FAR FRR. The operating point of a biometric system can often be manipulated by the setting of a threshold and is an application-dependent compromise between FAR and FRR. EER is mostly a statistical measurement and represents a better measure of performance than either FAR or FRR quoted in isolation. In practice, however, biometric verification applications do not operate on the EER. One may bias the recognition system toward a larger FAR but a smaller FRR (user friendly) or a larger FRR but a smaller FAR (user unfriendly, high-security oriented).</p><p>In the test procedure, each individual is considered as an impostor against all remaining persons, that are considered eligible. Each user claims his/her own identity, while each impostor claims the identity of each real user, in turn. In each claim, the observations corresponding to the test image are extracted and their probability is calculated given the EHMM of the user in the training set, whose identity is claimed. Then, for each of these claims, a distance measure or matching score between the probe and corresponding gallery images is calculated. The calculated distance measure is then compared to a threshold and accordingly the claimed identity is verified or rejected. Thus, by varying this threshold, FAR and FRR values are computed, defining points on the ROC curve.</p><p>Each authentication test can be regarded as a binary event with outcome 1 for false acceptance and 0 for correct acceptance, where is the probability for false acceptance and for correct acceptance (or false rejection and correct rejection, respectively). If consider the authentication tests as Bernoulli trials, where is the true value of FAR or FRR and are the values of FAR and FRR estimated by experimental testing of the proposed technique, then the confidence interval of and is given by <ref type="bibr" target="#b26">[27]</ref> (4)</p><p>where is the number of trials (user or impostor claims in our case) performed to estimate and and is the -percentile of the standard normal distribution with mean 0 and variance 1. The % confidence interval of FAR and FRR is indicated with horizontal and vertical error bars respectively in all ROC curves presented in Sections VI-D. For these values of , we get a confidence interval of 1% for an estimated FFR of 10% and 0.15% for an estimated FAR of 10% in the first database. The corresponding confidence intervals in the second database are 1.1% and 0.3%, respectively.</p><p>The above methodology was used to provide comparative evaluation among different techniques and for different facial variations. The approaches which utilize only color or intensity images represent the baseline of this evaluation. The performance of the approaches based on depth, combination of color and depth, and combination of intensity and depth were compared to this baseline to determine whether and in what extent the utilization of depth images can improve the performance of face authentication systems.</p><p>For ease of reference the following abbreviations will be used throughout the rest of the paper: "c" for color approach, "d" for depth, "i" for intensity, and "c d" and "i d" for the combination of color and depth and the combination of intensity and depth, respectively.</p><p>2) Experimental Results of the First Evaluation Database: In this section, we present the experimental results of the tests performed on the first evaluation database (see Section VI-A). Due to color miscalibration of the camera during recording sessions, only the methods that use intensity and/or depth information were tested on the first evaluation database.</p><p>Fig. <ref type="figure" target="#fig_9">10</ref>(a) and (b) illustrate the ROC diagrams of approaches "i," "d," and "i d." TS1 and TS3 were used respectively. The depth-only approach has the worst performance in both cases, presenting higher error rates compared to the other two approaches. The combination of depth and intensity decreases the EER of the intensity approach by almost 4%.</p><p>To investigate the benefits of incorporating artificially generated images in the training set, multiple tests were performed employing synthetical images depicting various head rotations and illumination variations. The ROC diagrams of "i" and "i d" that resulted from the use of different training sets are plotted in Fig. <ref type="figure" target="#fig_11">11</ref>. As it can be clearly seen, the use of synthetical images for training, substantially improves the authentication rate.  Note how the use of different training sets gradually decreases the error rate, when "i d" is employed: TS1, which is comprised of original images only, presents the highest EER. Enrichment of the training set with rotated and translated views (TS2) decreases the error rate by 2.5%, while the additional use of artificially illuminated images (TS3) further enhances the performance of the face authentication system and leads to a total decrement of the EER by almost 4%.</p><p>Table <ref type="table" target="#tab_1">II</ref> tabulates the EERs reported for test images depicting different appearance when different modalities are used. By an inspection of Table <ref type="table" target="#tab_1">II</ref>, it can be seen that while variations in pose, expression, and eyeglasses decrease the recognition quality slightly, illumination variations seriously affect the recognition performance in every approach, but most noticeably in the approach that uses only intensity images. While the EER reported for all test images is about 7.5% ("i," TS3), the EER for images depicting illumination variations, it is about 15.5%. The combination of 2-D and depth data is shown to improve the EER of the 2-D intensity approach whatever the variation of test images. As expected, depth is robust to lighting variations, al-though in practice strong side spot light deteriorates the quality of depth images for this database. Depth data mainly suffers from pose variations and the use of eyeglasses. The projected pattern on the face creates reflections on the glasses, resulting to a distorted depth map, while pose variations introduce occlusions of parts of the face, which may not be fully compensated.</p><p>Table <ref type="table" target="#tab_1">III</ref> tabulates the EERs achieved by test images depicting different facial variations, when different training sets are used. By inspection of the reported EERs, it can be seen that the proposed enrichment procedure benefits authentication not only for images depicting pose and illumination variations, but also for frontal neutral views, varying expressions and presence of glasses. TS3 produced the best results among all training sets used.</p><p>The proposed combination of 3-D and 2-D data, along with the enrichment of the training set with synthetical views, results in a decrement of the EER by almost 2% for frontal views, 4% for poses, 2% for expressions, 7% for images depicting illumination variations, and almost 4% for all images ("i d," TS3 versus "i," TS1).  Up to this point, the proposed system was tested only for face authentication purposes. Concluding this section, we will study the performance of the proposed approaches for face recognition. Fig. <ref type="figure" target="#fig_12">12</ref> shows the percentage of correctly identified test images within the best matches for , for the "i" and "i d" approaches, when TS1 and TS3 are employed for the training of the EHMMs. It clearly demonstrates the superiority of the combination approach followed by enrichment of synthetical images, over the intensity approach that uses only original images, as most state-of-the art algorithms. It is interesting to notice the increment of almost 10% between ("i," TS1) and ("i d," TS3) for Rank , depicted in Fig. <ref type="figure" target="#fig_12">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Experimental Results of the Second Evaluation Database:</head><p>Color calibration was consistent among recording sessions for this database, which allowed the experimental evaluation of the combined use of color and depth images. Approaches "c," "i," "d," "c d," and "i d" were tested.</p><p>Detailed experimental results of all the tests performed on the second database are outlined below, following the structure of Section VI-D.2.</p><p>Approaches "c d" and "i d" were compared against "c," "i," "d," employing training sets TS1-TS3. The resulting ROC diagrams are plotted in Fig. <ref type="figure" target="#fig_13">13</ref>. The depth-only approach has the worst performance, exactly as reported for tests performed on the first database. The performance of the intensity and color approaches is relatively similar when the training set is comprised of original images (TS1). The use of additional artificially rotated and/or translated images during training (TS2,TS3) favors more the intensity approach than the color approach. In all cases, the combination approaches "c d" and "i d" improve the performance of the color and intensity approaches distinctly, by 4% -5% and 2% -3%, respectively.</p><p>Next, we explore the merits of using artificially generated images for the training. Fig. <ref type="figure" target="#fig_14">14</ref> shows the resulting ROC diagrams    Table <ref type="table" target="#tab_2">IV</ref> tabulates the EER values of test images depicting different facial variations. The combination of 2-D and depth data is shown to improve the EER of 2-D approaches (color, intensity) significantly, whatever the facial variation of test images. The improvement is particularly significant in case of illumination variations. Note how such variations affect the authentication rate when the 2-D approach is used: while the total EER achieved by "c" is about 13%, in the case of illumination variations, the EER increases dramatically to 23.58%. The use of depth decreases the EER of illumination images from 23.58% ("c," TS1) to 15.46% ("c d," TS1). The use of additional synthetical images for training reaches an EER of 13.15% ("c d," TS3). Note, also, that "c d" is better than "i d" for all facial variations, except for illumination variations. In general, intensity is more robust than color to such variations. Concluding, we study the performance of the proposed approaches for face recognition. Fig. <ref type="figure" target="#fig_16">15</ref> shows the percentage of correctly identified test images within the best matches for , of the combination approaches versus commonly used 2-D approaches. It can be clearly seen that the combination approaches outperform the approaches that use only color or intensity. The recognition rate for all test images increases from 73% ("c," TS1) and 67% ("i," TS1) to 77% ("c d,"  TS1) and 74% ("i d," TS1), respectively. The additional use of synthetical views enhances the performance of the combination classifier even further, achieving a recognition rate of 79.2% ("c d," TS3) and 77.3% ("i d," TS3), respectively (Rank ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Comparison</head><p>Between the Two Evaluation Databases: results presented in Sections VI-D.2 and VI-D.3 establish clearly the adequacy of the proposed scheme in both databases. Tests performed on the first database provide solid experimental results, which prove that the use of depth enhances the performance of 2-D classifiers, when tested in a database comprised of many subjects and containing images depicting not only facial, but time variations, as well. The second database was recorded using different camera settings which improved the quality of depth maps (less missing pixels, higher accuracy) and allowed the experimental testing of "c" and "c d." In this section, we provide experimental results to support our claim that the higher the quality of the 3-D shape data the better the performance of the combination classifier, as well as the intuitive notion that 3-D shape data can be discriminatory providing its representation is accurate enough.</p><p>In order to establish this point, all other database variations (time variation, number of subjects, and number of images) had to be eliminated. A subset of the first database containing 20 subjects and two sessions was used for this particular experiment. The time interval between the two sessions is the same in both databases. Table VI tabulates the EERs reported for the two subdatabases and all modalities, when various training sets are used. It can be seen that the enhancement of the quality of depth maps in the second database yields better results in terms of the verification efficiency, compared to the EERs of the first database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Evaluation of the Performance of the Proposed Algorithms:</head><p>In this section, we briefly discuss and evaluate the accuracy of the proposed authentication scheme following the method presented in <ref type="bibr" target="#b27">[28]</ref>. First, we calculate the number of tests that gives statistically significant results. For the experimental evaluation of the proposed method we used two databases. As already mentioned in Section VI-D.1, the first database consists of 50 persons and 2818 pairs of test images providing 2818 user claims and 138 082 impostor claims, while the second database consists of 20 persons and 2244 images, providing 2244 user and 42 636 impostor claims.</p><p>Following the example and notation of <ref type="bibr" target="#b27">[28]</ref>, let us denote with the percentile of the standard Gaussian distribution with zero mean and unit variance. The desired EER is set to 6%, which is rather strict considering that the desired EER for the XM2VTS database, which is comprised of neutral frontal images with no significant illumination or background variations, is set to 6%, as well. The confidence interval is % and the error margin is set to 1%. The authentication tests can be considered as Bernoulli trials. In this case, we can assume with confidence that the minimum number of client claims which that the absolute value of the difference between the expected value of the FFR and the empirical value is less than , is given by ( <ref type="formula">5</ref>)</p><p>Substituting the parameters of ( <ref type="formula">5</ref>) with the aforementioned values, we get . Next, we estimate the number of test impostor claims, which is sufficient for ensuring that , according to <ref type="bibr" target="#b27">[28]</ref> (6) where is the probability that one impostor is falsely accepted as one client and is the number of clients. Setting % and , we find for the first database and for the second database, for an error margin %. Both demands on estimated minimum numbers of test impostor claims are met by the tests performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>In this paper, a face authentication system integrating 3-D range data and 2-D color or grayscale images is presented. Novel algorithms for face detection and localization are proposed, that exploit depth information to achieve robustness under background clutter, occlusions, face pose, and illumination variations. Face authentication is based on the well-known EHMM method, which is also applied to range images. Unlike most methods in 3-D face recognition literature that perform recognition or authentication using a 3-D model of the human head derived from range data, the proposed method simplifies the processing of 3-D data by regarding it as a 2-D image, with pixel values corresponding to the distance of the surface of the human head from the camera plane.</p><p>From an inspection of the experimental results presented in Section VI-D, it can be concluded that the verification performance of the approach that uses only depth maps is no better than that of the standard 2-D approaches. The resolution-accuracy of the range image acquisition system was shown to be partly responsible for the low performance of the depth-only approach. Occlusions, missing pixels, and artifacts in depth maps hinder face localization and authentication, especially when they appear near important facial features, such as the nose and the mouth. Despite the rather low performance of the depth-only approach, the combination of depth and color (or intensity) images decreases the EER of the 2-D approaches significantly.</p><p>The use of synthetic images presenting various rotations enhances the authentication of faces depicting poses other than frontal, as well as the verification of frontal views. The latter is justified by the fact that even images assumed as frontal may depict a small rotation. The use of artificial translations also compensates for any possible inaccuracies of the face localization algorithm. Experimental results presented in Tables <ref type="table" target="#tab_1">III</ref> and<ref type="table" target="#tab_3">V</ref> provide evidence to support this conclusion.</p><p>From an inspection of the experimental results reported for the second database, it can be seen that the approaches using color or intensity are especially sensitive to illumination variations, the color approach being more sensitive than the intensity approach. Although the enhancement of the training set with synthetic illumination images improves the verification capability of the proposed algorithms, the reported EERs are still higher than those reported for frontal and expression images, or pose variations. The proposed method may enhance the authentication efficiency, but unfortunately, modeling illumination sources is extremely challenging and was shown to be only partially successful. Automatic recovering of the illumination parameters by inverse rendering (using color and associated depth images) is currently under investigation.</p><p>Two In general, the 3-D acquisition system described in Section II can be improved further, so that the quality and resolution of range images are enhanced. Investigation of alternative fusion techniques and, particularly, data fusion at an earlier stage, use of more advanced surface features, and exploitation of illumination compensation techniques are among our future research plans.</p><p>Since evaluation on both databases demonstrates that the performance of the 2-D system is considerably improved by the utilization of 3-D depth data, we argue that the depth approach could be incorporated into any state-of-the-art 2-D system and such a 3-D system would be expected to yield superior performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Image captured by the color camera with color pattern projected. (b) Computed range image. Darker pixels correspond to points closer to the camera and black pixels correspond to undetermined depth values.</figDesc><graphic coords="2,155.88,65.38,278.00,116.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Face localization results. The box with the dashed line corresponds to depth-based segmentation. The horizontal line of symmetry and the intersection point with the vertical line of symmetry are illustrated. The window that encloses the face is centered on this point (continuous line).</figDesc><graphic coords="5,173.76,65.54,245.00,145.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Surface interpolation example. (a) Original color image with interest points identified. (b) Original depth image. (c) Warped depth image. (d) Symmetry-based interpolation. (e) Final linear interpolated depth image. (f) Warped color image.</figDesc><graphic coords="6,124.14,65.40,342.00,96.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Artificial rotation example. Original color image and depth map and five generated views: rotation 020 ; +20 around the vertical axis y; 015 ; +15 around the horizontal axis x and +5 around axis z.</figDesc><graphic coords="7,134.64,65.42,324.00,133.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Artificial illumination example. Original color image and depth map and four synthetical views.</figDesc><graphic coords="7,152.64,229.86,288.00,54.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. The database images were captured in an indoor office environment. The first face database consists of five recording sessions per person, captured in intervals of ten days on the average. The test population contains 33 male and 17 female individuals. The age of the participants was between 20 and 70 years old, but most of the individuals were between 20 and 35 years old. Each session contains on the average 12 recordings per person: three frontal images (frontal upright orientation, neutral facial expression), two poses (head rotated about the vertical axis ), three expressions (smile, laugh, and eyes half-closed), three illumination recordings (lights out and an extra light source coming from the left or from the right), and one recording with/without eyeglasses. Table I tabulates the number of images used for enrollment and the number of test images used for the different facial variation tests. Some examples of color images and corresponding depth maps belonging to the first evaluation database can be seen in Fig. 7. Examples of face images of an individual depicting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Examples of face color images and corresponding depth maps belonging to the first face database.</figDesc><graphic coords="8,133.14,321.68,324.00,124.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Examples of face color images and corresponding depth maps depicting various expressions, poses, as well as illumination variations, during the recording of a single session.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Examples of face color images and corresponding depth maps depicting facial variations due to hairstyle, makeup, beard, glasses, and suntan during the recording of different sessions.</figDesc><graphic coords="8,133.14,485.90,324.00,124.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. First face database. ROC curves of "i," "d," and "i+d" when training sets (a) TS1 and (b) TS3 are used. All image variation types are included in the test set. The 95% confidence interval of FAR and FRR is indicated with horizontal and vertical error bars, respectively.</figDesc><graphic coords="10,91.14,65.94,408.00,177.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>2 and VI-D.3. Since the first database consists of 2818 images of 50 persons, in total, the test procedure provides user claims and impostor claims. The second database consists of 20 people and 2244 images, providing 2244 user claims and impostor claims.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. First face database. ROC curves of (a) "i" and (b) "i + d" resulting by the employment of different training sets. All image variation types are included in the test set. The 95% confidence interval of FAR and FRR is indicated with horizontal and vertical error bars, respectively.</figDesc><graphic coords="11,86.64,65.54,420.00,184.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. First face database: Cumulative recognition rates versus rank achieved by "i" and "i+d". Training sets TS1 and TS3 were used. All image variation types are included in the test set.</figDesc><graphic coords="12,163.14,65.86,264.00,215.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Second face database. ROC curves of "c," "i," "d," "c+d," and "i+d," when training sets (a) TS1 and (b) TS3 are used. All image variation types are included in the test set. The 95% confidence interval of FAR and FRR is indicated with horizontal and vertical error bar, respectively.</figDesc><graphic coords="12,85.14,330.54,420.00,183.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Second face database: ROC curves of (a) "c+d" and (b) "i+d," resulting by the use of different training sets. All image variation types are included in the test set. The 95% confidence interval of FAR and FRR is indicated with horizontal and vertical error bars, respectively.</figDesc><graphic coords="13,86.64,65.46,420.00,186.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>of "c d" and "i d." As can be clearly seen, the enhancement of the training set with synthetical novel views rotations and translations of the head improves the authentication rate significantly. The improvement is obvious in the case of the "i d" test, where a decrement of the EER of almost 4% is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Second face database: Cumulative recognition rates versus rank achieved by 2-D approaches ("c" and "i") versus combination approaches ("i+d" and "i+d"). Training sets TS1 and TS3 were used. All image variation types are included in the test set.</figDesc><graphic coords="14,160.14,65.50,270.00,224.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>issues were mainly addressed by the present work: the use of depth data for face localization and classification and the enhancement of the training set with artificially generated views, based on the 3-D structure of human face. Both proposed methods contribute to the enhancement of the verification performance, as demonstrated by the results presented in Section VI. The combined use of depth data and 2-D intensity or color images yields better results in terms of verification efficiency compared to the use of 2-D images, which is most common among state-of-the-art face authentication systems. Additionally, the use of synthetical images, further improves the authentication rate of all approaches, whether they use 2-D or combination of 2-D and 3-D data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I NUMBER</head><label>I</label><figDesc>OF ENROLLMENT AND TEST IMAGESDEPICTING DIFFERENT FACIAL VARIATIONS ON THE FIRST AND SECOND FACE DATABASE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II FIRST</head><label>II</label><figDesc>FACE DATABASE: EQUAL ERROR RATES ACHIEVED BY "i," "d," AND"i+d" FOR DIFFERENT FACIAL VARIATIONS. TS3 WAS USED TABLE III FIRST FACE DATABASE: EQUAL ERROR RATES ACHIEVED BY "i+d" APPROACH FOR DIFFERENT FACIAL VARIATIONS AND TRAINING SETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV SECOND</head><label>IV</label><figDesc>FACE DATABASE: EQUAL ERROR RATES ACHIEVED BY "i," "c," "d," "c+d," AND "i+d" FOR DIFFERENT FACIAL VARIATIONS. TS1 WAS USED</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V SECOND</head><label>V</label><figDesc>FACE DATABASE: EQUAL ERROR RATES ACHIEVED BY "i+d" FOR DIFFERENT FACIAL VARIATIONS AND TRAINING SETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table V tabulates the EERs achieved by "i d," when different training sets are used. TS3 produced the best results among all training sets used.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI EQUAL</head><label>VI</label><figDesc>ERROR RATES ACHIEVED FOR THE SUBSETS OF THE FIRST AND SECOND FACE DATABASE (DB1 AND DB2)</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The majority of face recognition techniques employ two-dimensional (2-D) grayscale or color images <ref type="bibr" target="#b0">[1]</ref>. Although the three-dimensional (3-D) structure of the human face intuitively Manuscript received August 1, 2003; revised April 20, 2004. This work was supported in part by research projects BioSec IST-2002-001766 ("Biometrics and Security") under Information Society Technologies (IST) priority of the Sixth Framework Programme of the European Community and in part by HiScore IST-1999-10087-FP5 ("High Speed 3-D and Color Interface to the Real World"). The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Nasser Kehtarnavaz.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Face recognition: A literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<idno>CS-TR4167</idno>
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
			<pubPlace>College Park, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The hiscore camera: A real time three dimensional and color camera</title>
		<author>
			<persName><forename type="first">F</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Radig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2001-10">Oct. 2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="598" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Introductory Techniques for 3-D Computer Vision</title>
		<author>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Prentice-Hall</publisher>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page">458</biblScope>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matching range images of human faces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Int. Conf. Image Processing</title>
		<meeting>3rd IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="1990-12">Dec. 1990</date>
			<biblScope unit="page" from="722" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curvature-based face surface recognition using spherical correlation. Principal directions for curved object recognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chiaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Int. Conf. Automatic Face Gesture Recognition</title>
		<meeting>3rd IEEE Int. Conf. Automatic Face Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="372" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition based depth and curvature features</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision Pattern Recognition</title>
		<meeting>Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="808" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time normalization and feature extraction of 3-D face data using curvature characteristics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE Int. Workshop Robot Human Interactive Communication</title>
		<meeting>10th IEEE Int. Workshop Robot Human Interactive Communication</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3-D human face recognition using point signature</title>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IEEE Int. Conf. Automatic Face Gesture Recognition</title>
		<meeting>4th IEEE Int. Conf. Automatic Face Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Integrated 2-D and 3-D images for face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. Conf. Image Analysis Processing</title>
		<meeting>11th Int. Conf. Image Analysis essing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face identification using a 3-D gray-scale image-a method for lessening restrictions on facial directions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tsutsumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Int. Conf. Automatic Gesture Recognition</title>
		<meeting>3rd IEEE Int. Conf. Automatic Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="306" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face recognition using 2-D and 3-D facial data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Workshop on Multimodal User Authentication</title>
		<meeting>ACM Workshop on Multimodal User Authentication<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-12">Dec. 2003</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting faces in images: A survey</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="58" />
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face detection in color images</title>
		<author>
			<persName><forename type="first">R.-L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="696" to="706" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979-01">Jan. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detection and estimation of pointing gestures in dense disparity maps</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brumitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IEEE Int. Conf. Automatic Face Gesture Recognition</title>
		<meeting>4th IEEE Int. Conf. Automatic Face Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time head tracking and 3-D pose estimation from range data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Processing</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09">Sept. 2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="859" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context free attentional operators: The generalized symmetry transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reisfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="1995-03">Mar. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision Pattern Recognition</title>
		<meeting>Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face recognition using discriminant eigenvectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Etemad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal essing</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2148" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminant analysis of principal components for face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Int. Conf. Automatic Face Gesture Recognition</title>
		<meeting>3rd IEEE Int. Conf. Automatic Face Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face recognition with support vector machines: Global versus component-based approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heisele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision</title>
		<meeting>IEEE Int. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="688" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An embedded HMM-Based approach for face detection and recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nefian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal essing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3553" to="3556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maximum likelihood training of the embedded HMM for face detection and recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kuiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1997-07">Jul. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distortion invariant object recognition in the dynamic link architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Vorbruggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Konen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="311" />
			<date type="published" when="1993-03">Mar. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Frontal face authentication using morphological elastic graph matching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2000-04">Apr. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frontal face authentication using discriminating grids with morphological feature vectors</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="26" />
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A hidden Markov model-based approach for face detection and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nefian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Georgia Inst. Technol</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Atlanta, GA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhancing the performance of personal identity authentication systems by fusion of face verification experts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ballette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Czyz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandendorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="581" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning by a generation approach to appearance-based object recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int. Conf. Pattern Recognition</title>
		<meeting>13th Int. Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1996-08">Aug. 1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A delaunay refinement algorithm for quality 2-D mesh generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ruppert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="548" to="585" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Handbook of Face Recognition</title>
		<editor>S. Z. Li and A. K. Jain</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face identification across different poses and illuminations with a 3-D morphable model</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romdhami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Automatic Face Gesture Recognition</title>
		<meeting>5th Int. Conf. Automatic Face Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="192" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face recognition from one example view</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Computer Vision</title>
		<meeting>5th Int. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995-06">Jun. 1995</date>
			<biblScope unit="page" from="500" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">View-based and modular eigenspaces for face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision Pattern Recognition</title>
		<meeting>Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994-06">Jun. 1994</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face recognition from unfamiliar views: Subspace methods and pose dependency</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Automatic Face Gesture Recognition</title>
		<meeting>3rd Int. Conf. Automatic Face Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998-04">Apr. 1998</date>
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Computer Graphics, Principles, and Practice, 2nd ed</title>
		<author>
			<persName><forename type="first">J</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Hughes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What is the set of images of an object under all possible illumination conditions?</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Geodesic illumination basis: Compensating for illumination variations in any pose for face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. Pattern Recognition</title>
		<meeting>16th Int. Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="297" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision</title>
		<meeting>IEEE Int. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Face recognition under variable lighting using harmonic image exemplars</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision Pattern Recognition</title>
		<meeting>Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Best practices in testing and reporting performance of biometric devices v. 2.01</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wayman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Centre for Mathematics and Scientific Computing</title>
		<imprint>
			<date type="published" when="2002-08">Aug. 2002</date>
		</imprint>
		<respStmt>
			<orgName>National Physical Laboratory, U.K., Evaluation Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision Pattern Recognition</title>
		<meeting>Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="137" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Probabilistic visual learning for object detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Computer Vision</title>
		<meeting>5th Int. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995-06">Jun. 1995</date>
			<biblScope unit="page" from="786" to="793" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
