<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Traffic Sign Detection based on Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yihui</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep2">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yulong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep2">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Li</surname></persName>
							<email>lijianmin@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep2">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huaping</forename><surname>Liu</surname></persName>
							<email>hpliu@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep2">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
							<email>xlhu@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep2">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Traffic Sign Detection based on Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B9396678201F80AE853EAC0B45194DB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an approach for traffic sign detection based on Convolutional Neural Networks (CNN). We first transform the original image into the gray scale image by using support vector machines, then use convolutional neural networks with fixed and learnable layers for detection and recognition. The fixed layer can reduce the amount of interest areas to detect, and crop the boundaries very close to the borders of traffic signs. The learnable layers can increase the accuracy of detection significantly. Besides, we use bootstrap methods to improve the accuracy and avoid overfitting problem. In the German Traffic Sign Detection Benchmark, we obtained competitive results, with an area under the precision-recall curve(AUC) of 99.73% in the category "Danger", and an AUC of 97.62% in the category "Mandatory".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Driver assistance systems (DAS) have received more and more attentions from both academy and industry areas. Among various functions of DAS, the traffic sign detection has become one of the most important modules since it provides alerts for the drivers to relieve the pressure of driving. Detection of traffic signs has been a popular problem in intelligent vehicles since the middle of 1990s, and various methods have been proposed by researchers.</p><p>Because traffic signs usually have specific colors, the color-based methods are commonly used. These methods often use a threshold to separate traffic signs from background <ref type="bibr" target="#b0">[1]</ref>. Some researchers use HSI color space instead of RGB and has achieved good performance <ref type="bibr" target="#b1">[2]</ref>. A novel color space Eigen color proposed based on Karhunen-Loeve (KL), is used for traffic sign detection <ref type="bibr" target="#b2">[3]</ref>. The main disadvantage of these color-based methods is that it is difficult to set the value of threshold because the color information is not invariant in real-world environment with different lightening conditions. Methods based on shape of the traffic signs, have also been widely used. In <ref type="bibr" target="#b3">[4]</ref> a method is proposed using smoothness and Laplacian filter to detect round signs. In <ref type="bibr" target="#b4">[5]</ref> a method designed to detect triangle signs based on gradient and orientation information is proposed. In <ref type="bibr" target="#b5">[6]</ref> a detection algorithm by using Hough transform is introduced. In order to speed up the detect algorithm, <ref type="bibr" target="#b6">[7]</ref>[8] use a fast detection method based on the symmetry on Radial direction of triangle, square, diamond, octagon and round signs. Most of the methods above rely on gradient features, which are really sensitive to noise. Because color information and shape information are both useful to traffic sign detection, it is natural to combine these two kinds of features. In <ref type="bibr" target="#b9">[9]</ref> images are segmented in HSI color space, and template matching techniques are then used to find traffic signs.</p><p>Although the detection of traffic signs has been studied for years, there still exist many challenges. For example, the background clutter may introduce strong disturbances. In addition, the color of traffic sign is very sensitive to lighting conditions (sun, shadow), weather (sunny, rain, snow) and time (morning, noon, night), etc. Last but not least, the partial occlusion dramatically affects the detection performance.</p><p>Recently, Convolutional Neural Network has been adopted in object recognition for its high accuracy <ref type="bibr">[10] [11]</ref> [12] <ref type="bibr" target="#b13">[13]</ref>. In <ref type="bibr" target="#b10">[10]</ref>, a multi-layer convolutional networks is proposed to boost traffic sign recognition, using a combination of supervised and unsupervised learning. This model can learn multi stages of invariant features of image, with each layer containing a filter bank layer, a non-linear transform layer, and a spatial feature pooling layer. Feeding the responses of both two convolutional layers to the classifier can achieve an accuracy of recognition as high as 99.17%.</p><p>Inspired by the excellence of traffic sign recognition using Convolutional Neural Network (CNN), we proposed a method based on CNN, using fixed and learnable filters to detect traffic signs on scene images. To accelerate the detection speed, color information is used to choose the areas we are interested in. Besides, the responses of images convolving with fixed filters we defined before training are fed to learnable filters. The results of the two learnable filter layers are branched to a 2-layer nonlinear classifier separately. The learnable filter layers and the classifier are trained in a supervised way. The fixed filter layer can decrease the number of windows we need to analyze. We obtained good results in the competition of German Traffic Sign Detection Benchmark <ref type="bibr" target="#b14">[14]</ref>, with an AUC of 99.73% in the category "danger", and an AUC of 97.62% in the category "mandatory".</p><p>The specific contributions of this paper are as follows: we used a convolutional neural network combined with fixed and learnable filters to detect traffic signs(see section 3), and obtain competitive results in the category of "danger" and "mandatory". Besides, bootstrap method is adopted to learn from the misclassified training samples to decrease the rate of false positives and false negatives(see section 3 part C). In addition, the data augmentation of enlarging positive samples by rotation, translate, scale transformation can prevent overfitting problem(see section 4 part A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE DATASET</head><p>The task of GTSDB <ref type="bibr" target="#b14">[14]</ref> is to detect 3 types of traffic signs: "Prohibitory", "Mandatory" and "Danger". Our work mainly focused on the "Mandatory" and "Danger" category. "Mandatory" signs are circle, with white arrows in the middle of blue background. "Danger" signs are triangle with white background and red borders. Examples of the two categories are shown in Figure <ref type="figure">1</ref>. The training dataset contains 600 scene images (1360 x 800 pixels) and the traffic signs cropped in these images. The size of the traffic signs ranges from 16 x 16 to 128 x 128. The testing dataset contains 300 scene images (1360 x 800 pixels) with zero to six traffic signs occurred in each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE ARCHITECTURE</head><p>In this paper, we use color information and CNN to detect traffic signs. A simple flow chart shows the whole process of our algorithm in Figure <ref type="figure" target="#fig_1">2</ref> and Figure <ref type="figure" target="#fig_2">3</ref>. An illustration of the processing steps of our algorithm. To detect each kind of traffic signs from background or other traffic signs, first transform RGB images to gray scale images using SVM, then feed the results to CNN. The fixed layer detect ROIs, and learnable layers extract distinguishing features for the classifier to find out traffic signs of the target group.</p><p>To accelerate the detection process, we use color information to do some data preprocessing for testing dataset. It converts the original scene image into a gray scale image. Traditional color transformation methods use color spaces like HSV or Lab to deal with the difficulty introduced by color deviation due to various lighting conditions, different weather conditions or natural fade, and static or dynamic thresholds are used to segment the whole image. But we learn the threshold instead of using manually fixed or dynamic threshold to establish the mapping between RGB value and gray scale value (intensity value). The color transformation based on Support Vector Machine in the preprocess step can avoid the sensitivity to color differences in different lightening conditions <ref type="bibr" target="#b15">[15]</ref>. At first we extract pixels from training data, and classify them into positive pixels and negative pixels with Support Vector Machine (SVM). For example, in the category "mandatory", positive pixels are those blue ones inside mandatory signs, and negative pixels are non-blue pixels. Then we train a classifier <ref type="bibr" target="#b16">[16]</ref>, and use the classifier's offset as the map between RGB and gray scale value.</p><p>Convolutional Neural Networks (CNN) are hierarchical neural networks with multiple layers (see Figure <ref type="figure" target="#fig_1">2</ref>). The first layer convolves the gray scale image obtained in the color transform step with fixed filters and compares the correlation coefficient value with threshold to detect areas possibly containing traffic signs. The learnable layers extract multi-scale features for classifier to judge whether it is a traffic sign in the required category or not. During the processing of fixed layer, correlation coefficient values are used to describe the degree of matching between a filter and a test patch. A higher value indicates that the region is more likely to contain a traffic sign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fixed Layer</head><p>Filters we use in mandatory signs are shown in Figure <ref type="figure" target="#fig_3">4</ref>. The shape of this kind of signs is not sensitive to rotation  <ref type="figure" target="#fig_4">5</ref>. The reason why we use five filters to match one shape is that rotation has a significant effect on the shape of this category of signs.</p><p>Because the size of traffic signs in the images ranges from 16 x 16 to 128 x 128, multi-scale matching is required. In our experiment, we choose 1.05 as our filter scale-rate because this value can achieve satisfactory result in the acceptable computing time. For every specific filter in each scale, we extract every patch which has a correlation coefficient value larger than the threshold. By changing the threshold, we can generate a group of regions of interest (ROI).</p><p>Since this algorithm does not check the overlapping patches, there may exist a lot of ROIs around one traffic sign. In order to solve this problem, a simple algorithm is introduced to merge ROIs. For each image, the ROIs are sorted by correlation coefficient value in descending order, then the one with highest value is chosen as a positive region and all regions near this region are deleted. Repeat this step until no regions left. In this paper, nearby areas are regions whose distances of top-left points are less than 16 pixels in both x-axis and y-axis(16 is the minimum size of traffic signs in the data set).</p><p>The whole process of ROI extraction is shown in Figure <ref type="figure" target="#fig_5">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learnable Layers</head><p>The learnable layers constitute the traditional convolutional network. The choice of architecture affects the efficiency of CNN significantly. Each learnable filter layer contains a filter bank layer which convolves different filters with images, a non-linear transform layer using | tanh |, a pooling layer, and a local norm layer. We discussed several architectures to choose an appropriate one.</p><p>1) Filter Bank Layer: In <ref type="bibr" target="#b10">[10]</ref> different choices of CNN architecture are compared. A multi-stage CNN feeding the features extracted in both of the two stages into the classifier outperforms the CNN which also has two stages but only uses the second stage's responses to classify. Since the features extracted in the first stage are more local and detailed while the ones from second stage are relatively more global, feeding responses of both of the two stages can increase the accuracy. In our experiment, we adopted the multi-stage CNN feeding features of both two stages into the classifier and compared different multi-stage CNN architectures, like 6-16, 16-512, 108-200, where the left number is the number of filters extracted in the first layer while right is the number of the second layer (see Figure <ref type="figure" target="#fig_2">3</ref>). 2) L P pooling: The spatial pooling layer is often used after the feature extraction to summarize the joint distribution of the nearby pixels. In <ref type="bibr" target="#b17">[17]</ref>, pooling in a local region boosts invariance with little shift and small noise of the region. The pooling method processes the input image as:</p><formula xml:id="formula_0">O = ( I(i, j) P × G(i, j)) 1/P<label>(1)</label></formula><p>Where I stands for the input image, G stands for the gaussian kernel. The choice of P number varies from 1 to p → ∞.</p><p>When P = 1, it is average pooling; when p → ∞, it's maximum pooling. During the experiment, we set P = 4.</p><p>3) Normalization: In <ref type="bibr" target="#b18">[18]</ref>, a local normalization method is proposed, which can be divided as subtractive normalization and divisive normalization. It can decrease the relevance of nearby pixels thus boosts the contract of images with noise. The subtractive normalization computes the output of pixel x ijk as following: v ijk = x ijkipq w pq • x i,j+p,k+q where j,k is the x-coordinate and y-coordinate of pixels; i is the filter number; p,q is the width and height of kernel, w is the gaussian weight of the kernel subject to pq w pq = 1.</p><p>The divisive normalization computes the output of x ijk as following:</p><formula xml:id="formula_1">y ijk = v ijk / max(c, σ jk ) where σ jk = ( ipq w pq • v 2 i,j+p,k+q ) 1/2 , c = mean(δ jk ), ∀j, k</formula><p>Bootstrap is an efficient method to improve the performances of classifiers. Adding samples wrongly classified in the validation set to training set by bootstrap not only automatically includes traffic signs difficult to detect due to occlusion, exposure, spotted or other circumstances, but also enables the classifier to distinguish some patches which are similar to a certain category of traffic signs but are not. In the detection of category "mandatory", we add a bootstrap step during training. We detect ROI in all of 600 images of training data, use ROI as a test on the training data, and extract the patches which are wrongly classified compared to ground truth. We jitter the patches and add them to the training data like below:</p><p>1) 1-class wrongly classified as 0-class: perturbed like other 1-class training data(see section 4 part A), with 30 jitters of each patch; 2) 0-class wrongly classified as 1-class: perturbed like 1class, with 7 jitters of each patch. After training for a second time, both the false positive and false negative rate decreased more than 1% in the validation set. In the test phase, the classifier training with bootstrap improves the AUC from 93.81% to 97.62%, which shows that bootstrap is helpful to reduce the misjudgement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>In the experiment, we used a system as following: 1) CPU: Intel(R)Xeon(R) CPU E5620 @2.40GHz x 2 2) memory: 32G DDR3 The learnable layers of CNN was implemented using the EBLearn C++ open-source package <ref type="bibr" target="#b19">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preparation</head><p>The training data provided on the benchmark contains four categories: "Prohibitive", "Danger", "Mandatory" and Other traffic signs, and we only need to distinguish each category from others. For each category, the training data is divided into two classes, the class that contains traffic sign labeled 1, and the class that contains no traffic sign labeled 0. During training phase, the 1-class mainly contains the patches provided in the TrainIJCNN2013 grouped under the category and perturbed in position ([-4, 4] pixels, step 2), in scale ([.9, 1.1], step .1), in rotation([-15, 15], step 6). Each patch has 30 randomly chosen jitters. Jittered patches can increase the robustness of classification in case of different views of point, and different alignments of bounding boxes. The 0-class data contains the patches provided grouped under other 3 categories as well as small patches randomly chose from the 600 scene images. From each image, we randomly chose 15 patches, with random locations and random sizes ranging from 16 x 16 to 128 x 128 (not include or overlap traffic signs).</p><p>In order to make better use of training data, we assigned the proportion of training data and validation to be 2 : 1.</p><p>To test the efficiency of different architectures, we extracted ROIs in all 600 scene images for training and compared the results with ground truth to get labels of ROIs. These ROIs were used for test on the training data. The data size of training, validation, and test on the data of TrainIJCNN2013 are listed, each column contains the size of 0-class and 1class (see Table <ref type="table">I</ref>). Since we need to convolve the filters with patches in the same size (need not be in the same size with filters), we resized the training data (ROIs) to 32 x 32 and converted the images from RGB to YUV space. Then we extracted the data of Y channel to train our model, and discarded the U and V channels. In other words, we only used gray-scale information, because we have already used color information during the ROI extract phase(see section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on the Training Set</head><p>We generated 25 groups of ROI using the method of color transform and fixed layer. Each group of ROI was generated in a specific threshold on the correlation coefficient values. In our experiment, the value ranges from 0.41 to 0.65 step by 0.1. The sizes of regions are between 16 and 128.</p><p>To compare the performances of different architecture, we used three architectures: 6-16, 16-512, 108-200 under the category of "Mandatory" (see Table <ref type="table">II</ref>), "Danger" (see Table <ref type="table">III</ref>). FP stands for false positive, FN stands for false negative. The test set is the ROIs extracted in the TrainIJCNN2013 (see Data Preparation).  We can see 108-200 outperformed than other architectures. We suggest that more features extracted in the first stage can learn more local features and provide more randomly chosen features for the second stage to choose. However, with more complicated architecture to test, more time would be spent in training the model and detecting one image.</p><p>The training time of learnable layers is increased with the number of parameters to learn. We use the misjudgement of validation to determine the training iterations. Figure <ref type="figure" target="#fig_6">7</ref> shows the misjudgement on validation datasets of different architectures. Generally, simple architectures need more iterations to achieve the same validation error. 108-200 can achieve the lowest validation error in the given time. However, the training time of each epoch has to be considered. 6-16 can finish one epoch in 2 minutes, while 16-512 in 16 minutes, and 108-200 in 1h 15m. The detection time of CNN on the training set is shown in Table <ref type="table">IV</ref>   From Table <ref type="table">IV</ref> and Table <ref type="table">V</ref> we can see that as the number of parameters increase, the time cost rise significantly, from 0.002s to 0.100s (no parallel computing was implemented). To achieve real-time detection speed, we can use simpler architecture. In the category "Mandatory", 108-200 can achieve 0 val error after training with bootstrap, which is the best performance in the three architectures, so we detect the traffic signs with 108-200 architecture in the category "Mandatory" and "Danger". After competition, we compared performance of the three architectures in the category "Danger", and found detection of 6-16 is 0.17% more accurate than 108-200 (99.9% vs 99.73%). Besides, using simpler architecture can reduce the time spent on training and detection.</p><p>From the color transform and fixed filter layer, we can usually get 5-20 interest areas per image to be classified more carefully, and this process costs 10-30s per image depending on the assignment of threshold of color transform and fixed layer. For learnable filter layers with the architecture of 108-200, a scene image would need about 2s (0.1 s per ROI). However, considering that if we use sliding window, we need to search in multi-scale and every location in the image, which is far more than 5-20 regions currently used. If we search traffic signs size ranges from 16-16 to 128-128 in a image as large as 1600x800, we need to detect some 10000 regions, which may cost far more time with complicated networks. Considering if we use Graphics Processing Units (GPUs) or some parallel computing method, the time of detection will be decreased further. What's more, to improve the detection speed in reality, we need to use efficient tracking method to decrease the frames we actually deal with.</p><p>The error of FP and FN in the validation set decreased significantly after bootstrap (see Table <ref type="table">VI</ref>). We would compare the result of bootstrap and no boost in the test data (provided in the test phase) to see the efficiency of bootstrap. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment on the Test Set</head><p>In the test phase, we compare the result using only ROIs and using both ROIs and CNN recognition module to see the contribution of each module (see Figure <ref type="figure" target="#fig_7">8</ref>). Using learnable layers, with the same recall, we can get pretty much higher precision rate.</p><p>Based on the experiment on the training set, we compared the efficiency with bootstrap or not using the same architecture of 108-200 in the test set in Category "Mandatory"(see Figure <ref type="figure" target="#fig_9">9</ref> and Figure <ref type="figure" target="#fig_10">10</ref>). The bootstrap method improves the recall rate, with less false negatives.  In the Category "Danger", the result without bootstrap using architecture of 108-200 is good enough, so we didn't do bootstrap(see Figure <ref type="figure" target="#fig_10">10</ref>). The results of category mandatory and danger in the competition are listed in Table VII and Table VIII. Our team is "wff" with the result of 99.72% in the category "Danger", and 97.62% in the category "Mandatory". We use the same architecture to detect both categories in the competition phase, namely a fixed layer, and 108-200 learnable layers with L p pooling (P = 4).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORK</head><p>In this paper, an approach based on the combination of color transformation and Convolutional Neural Networks (CNN) is proposed. Working on the image preprocessed by color transformation, the CNN with fixed and learnable layers has achieved good results. The merits of the CNN we used are as follows: First, fixed layer can reduce the amount of areas the classifier need to deal with, which could speed up the detection significantly. Second, the ROIs generated by fixed filter are very close to the borders of traffic signs, therefore the problem of alignment is avoided, otherwise performance of supervised convolution network would degrade. Third, CNN with appropriate architecture learned in the supervised way has been proved to be suitable to extract features for traffic sign classification. Our experiment results strongly supported our conclusion.</p><p>A drawback of the proposed model is that it can not do real-time detection. Our future work is to improve the efficiency of this algorithm. Parallel algorithm can be introduced to speed up the process time of fixed and learnable layers. The process time of multiple learnable layers could be decreased by introducing sparsity in extracting features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. Samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.An illustration of the processing steps of our algorithm. To detect each kind of traffic signs from background or other traffic signs, first transform RGB images to gray scale images using SVM, then feed the results to CNN. The fixed layer detect ROIs, and learnable layers extract distinguishing features for the classifier to find out traffic signs of the target group.</figDesc><graphic coords="2,64.85,527.59,210.29,78.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Architecture of learnable layers. The outputs of two learnable layers are fed to the classifier separately. The parameters of learnable layers and the classifier are trained simultaneously in supervised way. The 2-layer classifier is fully connected with 100 neurons in the first layer and 2 neurons in the second layer.</figDesc><graphic coords="2,317.09,89.41,210.33,76.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Mandatory Filter</figDesc><graphic coords="2,317.09,541.75,210.07,107.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Danger Filter</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Process of ROI extraction</figDesc><graphic coords="3,317.09,89.41,210.07,351.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Influence of different architectures of learnable layers on the validation errors through training. Top row: the validation error of 6-16,16-512,108-200 in the category "danger". Top row: the validation error of 6-16,16-512,108-200 in the category "mandatory". Other parameters are the same, η = 1e-5, P = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. fixed layer only, fixed and learnable layers , Mandatory, test-phase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. bootstrap and no bootstrap, 108-200, Mandatory, test-phase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. 108-200, Danger, test-phase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and TableV.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Basic Research Program (973 Program) of China (Grant Nos. 2013CB329403 and 2012CB316301), National Natural Science Foundation of China (Grant Nos. 61273023 and 91120011) and Beijing Natural Science Foundation (Grant No. 4132046).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Traffic sign recognition in color image sequences</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C U</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Ag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles &apos;92 Symposium</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comparison of rgb and hsi color segmentation in real -time video images: A preliminary study on road sign detection</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W N</forename><surname>Ubong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lydia</forename><surname>Jau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siong</forename><surname>Chee</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>in Information Technology, 2008. ITSim</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Road sign detection using eigen colour</title>
		<author>
			<persName><forename type="first">L.-W</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="164" to="177" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>IET</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A study on traffic sign recognition in scene image using genetic algorithms and neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aoyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asakura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 IEEE IECON 22nd International Conference on</title>
		<meeting>the 1996 IEEE IECON 22nd International Conference on</meeting>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1838" to="1843" />
		</imprint>
	</monogr>
	<note>Industrial Electronics, Control, and Instrumentation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Angle vertex and bisector geometric model for triangular road sign detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Belaroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Tarel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards reliable traffic sign recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hoferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="324" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast shape-based road sign detection for a driver assistance system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2004 IEEE/RSJ International Conference on</title>
		<meeting>2004 IEEE/RSJ International Conference on</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
	<note>Intelligent Robots and Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A combined method for traffic sign detection and classification</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (CCPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m">Chinese Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Road sign classification using laplace kernel classifier</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paclłk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Somol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="page" from="1165" to="1173" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multiscale convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on</title>
		<imprint>
			<date type="published" when="2011-05">31 2011-aug. 5 2011</date>
			<biblScope unit="page" from="2809" to="2813" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2009 IEEE 12th International Conference on</title>
		<imprint>
			<date type="published" when="2009-02">2009-oct. 2 2009</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN&apos;11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;12</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (submitted)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Traffic sign detection by roi extraction and histogram features-based recognition</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H J L</forename><surname>Ming Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCNN</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;10</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear image representation using divisive normalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Eblearn: Open-source energy-based learning in c++</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
