<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Annotating Objects and Relations in User-Generated Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xindi</forename><surname>Shang</surname></persName>
							<email>shangxin@comp.nus.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Donglin</forename><surname>Di</surname></persName>
							<email>donglin.di@u.nus.edu</email>
						</author>
						<author>
							<persName><forename type="first">Junbin</forename><surname>Xiao</surname></persName>
							<email>junbin@comp.nus.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xun</forename><surname>Yang</surname></persName>
							<email>xunyang@nus.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<email>chuats@comp.nus.edu.sg</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Annotating Objects and Relations in User-Generated Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DEE0B4C8A5776BD9AECE6B0526F8F587</idno>
					<idno type="DOI">10.1145/3323873.3325056</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Multimedia databases</term>
					<term>Information extraction</term>
					<term>• Computing methodologies → Scene understanding</term>
					<term>Object recognition dataset</term>
					<term>video annotation</term>
					<term>video content analysis</term>
					<term>object recognition</term>
					<term>visual relation recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: An example of recognizing objects and relations in a video. Each object is spatio-temporally localized and the relations between each pair of objects are temporally localized across video frames.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Fine-grained video content analysis is crucial to bridging the gap between vision and language and enhancing the explainability of modern multimedia systems, which has been extensively demonstrated by the successes in applying object level analysis to video captioning and question answering tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>. In specific domains, such as the fashion and food recognition, the analytic granularity is even refined into landmarks <ref type="bibr" target="#b14">[15]</ref> and ingredients <ref type="bibr" target="#b3">[4]</ref>, respectively, in order to boost the retrieval performance with more discriminative representation. As this granularity tends to be finer, understanding the relations between the objects becomes especially important as well. By looking into the relations instead of simply considering the ensemble of objects, recent works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> have shown its promise in generating robust representation for video content analysis. Still, most of these research works explore the objects and relations in video by implicitly modeling and evaluation, leaving it unclear on whether or how well the proposed models understand the content at this granularity.</p><p>In fact, object and relation recognition in videos is very challenging because it requires the need to visually understand many perspectives of object entities, including appearance, identity, action, and interactions between them. For example, the appearance of the dogs in Figure <ref type="figure">1</ref> may vary a lot due to its active behavior, the change of illumination, and occlusion. It will inevitably cause great difficulty in determining the identity of the dog across video frames, which is a necessary clue for further summarization of video content. Moreover, the variance in action and interaction representation poses another challenge for the model to learn the underlying patterns and make a robust inference. To investigate the problem deeper, there are rising interests in research on video object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref> and video visual relation detection <ref type="bibr" target="#b19">[20]</ref>, which formally define and study the problem with explicit evaluation metrics. However, they are still primarily limited by the available datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> that are of small-scale and built upon video source under certain constraints.</p><p>This gives rise to the main purpose of this paper: We aim to construct a larger scale video dataset with dense annotations of objects and relations. In particular, the video sources will be collected from the Web to reflect the video content in real-world scenarios; the annotations will spatio-temporally localize the objects of interest by bounding-box trajectories, and temporally localize the relations of interest between pairs of objects. Compare with image datasets for similar purpose <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>, the constructed dataset will serve as a benchmark dataset that can be used to develop and evaluate techniques at video level directly. It is worth noting that the dataset will also address the limitations of image-level recognition that cannot deterministically learn and infer the temporal visual concepts (e.g., actions and dynamic relations), which can further aid the grounding of language in vision.</p><p>However, annotating the objects and relations in a large usergenerated video dataset requires a tremendous amount of human labor. Supposing that the length of user-generated videos is at least 150 frames (i.e., the length of typical micro videos), the workload for annotating 10,000 videos is then equivalent to annotating at least 1,500,000 images, which reaches the scale of the prevailing image datasets. By assuming the resemblance of adjacent frames, one can possibly reduce the workload through manually annotating keyframes at a fixed time interval and interpolating the remaining. But, this strategy only works for the annotation of movie and surveillance videos, because they usually have steady camera motion and predictable object activities, which assures the video continuity assumption. In contrast, the content of user-generated videos is totally free-form and often of low quality, so the choice of a large time interval in order to save the annotation cost will likely lead to poor interpolation quality at the intermediate frames, making the trade-off between the quality and cost difficult for this strategy. Therefore, in order to economically scale up the annotation in user-generated videos, we need a better strategy to select the key-frames for manual labeling.</p><p>Another critical problem that needs to be addressed is how to split the whole annotation task into several manageable and feasible subtasks. If considering annotating objects and relations in a video as a single task, it would require annotators to not only be very familiar with the many annotation requirements, but also be highly specialized in the task. In other words, the annotators need to spend a lot of time and energy to perform the task at a constant throughput. Thus, finding such a group of specialists is difficult and costly as well. On the other hand, video annotation should preferable be performed as a macro-task rather than micro-task in practice <ref type="bibr" target="#b23">[24]</ref>. This is because simple macro-tasks such as determining the type of objects, or whether the bounding-box is good, can be performed easily by most people with little cognitive overheads; whereas annotators need training and tend to make a lot of mistakes for micro-tasks such as identifying the objects to track and determining the full trajectories of objects. If the task on a video is split into too many micro-tasks and assigned to many different annotators, then there would be insufficient context for each annotator to label the task correctly according to the overall content and consistently with other annotators. Hence, the design of subtasks greatly influences the annotation cost and quality.</p><p>In Section 3, we will address the aforementioned issues by proposing an annotation pipeline that can achieve our annotation goal at a modest cost. The pipeline is typically effective for the usergenerated videos whose length varies from seconds to several minutes. Longer videos, which are generally created for special purposes, can be divided into shorter segments and fed into the pipeline. As shown in Figure <ref type="figure">2</ref>, in the procedure for object annotation, we first temporally localize the objects of interest in videos. We then spatially localize each of the objects at the key-frames generated according to an interactive scheme. At each turn, the scheme will generate a finer set of key-frames compared to that at the previous turn, so that the objects can be localized more accurately across video frames if the budget permits more turns. In the procedure for relation annotation, we conduct different annotation tasks according to the properties of different types of relations (i.e., action relation versus spatial relation). This allows us to achieve a good balance between the annotation quality and cost. Section 4 will justify the effectiveness of the proposed annotation pipeline through several aspects. Section 5 will present the resulting dataset, the Video Object Relation (VidOR) Dataset, and analyze its characteristics from various aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Video Annotation</head><p>The technique for video object annotation has been developed over a long period of time. Mihalcik and Doermann <ref type="bibr" target="#b15">[16]</ref> propose the prototype of offline annotation tool, ViPER, for annotating boundingbox trajectories in videos. Dollár et al. <ref type="bibr" target="#b4">[5]</ref> develop an analogous tool dedicated for large-scale pedestrian annotation in driving record videos. Driven by the emergence of crowdsourced labeling, Yuen et al. <ref type="bibr" target="#b30">[31]</ref> design a online video annotation system, LabelMe Video, which supports both bounding-box trajectory and polygonal path annotation by online users. VATIC <ref type="bibr" target="#b23">[24]</ref>, another online video annotation system proposed by Vondrick et al. more recently, tries to economically scale up the video object annotation through the insight of the pros and cons of different annotation strategies. As for the most important component on key-frame generation, most works adopt the strategies of either generating the key-frames at a fixed time interval or letting the annotators determining them; this limits the scalability of such approach especially for large dataset.</p><p>Recently, there is a rising number of works studying the temporal annotation of actions, activities, and relations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref>. Intuitively, scaling up the temporal annotation is much easier than the spatiotemporal annotation in video object annotation. Current strategies in temporal annotation can be categorized into two types. One category is direct temporal localization by labeling the starting and ending frames of targets in a video <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. In order to find the boundary frame accurately, it normally requires the annotator to spend much time browsing the video. The other category is to tag the presence of targets in short partitioned video segments and then automatically merge the tagging results across the segments to achieve temporal localization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. This approach generally  requires less effort and time of the annotators, but can only produce coarse-grained annotation results. While most works stick to only one of these strategies, we propose to adopt both of them in our relation annotation by carefully choosing the proper one for specific types of relations, such that the workload and cost of the annotation can be significantly minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Datasets</head><p>Video datasets can be categorized based on the annotation level. The first category of datasets are those with video-level annotation, such as CCV <ref type="bibr" target="#b9">[10]</ref>, Kinetics <ref type="bibr" target="#b2">[3]</ref> and MSR-VTT <ref type="bibr" target="#b25">[26]</ref>. They are typically used as benchmark for event recognition, action recognition and video captioning. The second category is the dataset with segment-level annotation. Typical datasets include THUMOS <ref type="bibr" target="#b8">[9]</ref>, MultiTHUMOS <ref type="bibr" target="#b27">[28]</ref>, ActivityNet <ref type="bibr" target="#b1">[2]</ref> and ActivityNet Caption <ref type="bibr" target="#b11">[12]</ref>, which are used as benchmarks for the temporal localization of action, activity and event.</p><p>Datasets with object-level annotation are another category that initiates the research on fine-grained video content analysis. Track-ingNet <ref type="bibr" target="#b16">[17]</ref> is a large-scale benchmark dataset for object tracking, which has a single object annotated by bounding-box trajectory in every video. ImageNet-VID <ref type="bibr" target="#b18">[19]</ref> is the benchmark dataset for video object detection over 30 categories of objects. In the field of human-centric tasks, the dataset provided in <ref type="bibr" target="#b4">[5]</ref> is benchmarked for pedestrian detection while AVA <ref type="bibr" target="#b5">[6]</ref> is benchmarked for spatiotemporal action localization.</p><p>An emerging category of datasets is the dataset with relationlevel annotation. Beyond the object-level dataset that only has a set of independent object annotations, this type of dataset provides structured object annotations for more comprehensive understanding of the video content. VidVRD <ref type="bibr" target="#b19">[20]</ref> is the only such dataset to date, which is built upon a subset of ImageNet-VID. However, as limited by current annotation technique, the dataset is small with sparse annotation in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANNOTATION PIPELINE</head><p>To address the problems of complex annotation as discussed above, we propose an annotation pipeline (shown in Figure <ref type="figure">2</ref>) consisting of object annotation and relation annotation, each of which is further split into two sub-procedures. Before elaborating on the details of these procedures, we first introduce several basic terms and how the pipeline applies to a set of user-generated videos to produce the annotations in general.</p><p>An atomic task is a basic task specially defined in each of the procedures, and will be completed and checked individually. Some atomic tasks are defined at the video level while some are defined at the segment level. In a procedure, although different atomic tasks can be assigned to different annotators, it is preferable to assign the contiguous atomic tasks in a video to the same annotator, because the annotator can leverage the context information in the earlier atomic tasks to produce more accurate and consistent annotations for the subsequent tasks. It will also help us to logically organize a large number of atomic tasks and efficiently achieve parallelism in the pipeline. Hence, we call a group of atomic tasks in a video as video task, and we always pack the atomic tasks in this way across the whole pipeline. Overall, every video will be sequentially assigned with the four video tasks introduced in the following subsections.</p><p>Regarding the reward mechanism, we will describe how we pay the annotators in terms of the number of points for each of the video tasks. Since each video task has different level of annotation difficulty, we aim to balance the rewards such that the annotators feel that they are being fairly compensated no matter which video task is assigned to them. Though it could be suboptimal, we found that in practice, our reward mechanism was acceptable by the annotators, who are professional annotators that we recruited online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatio-temporal Object Localization</head><p>In order to annotate an object in video, annotators are normally required to complete three sub-tasks: 1) browsing the video and finding a new object of interest; 2) drawing the bounding-box for the object at the frames that it appears; and 3) marking the object as invisible at the frames that it is occluded or out-of-camera. Apparently, the second sub-task is the heaviest because it requires the annotator to draw bounding-boxes while the other two just need to simply choose tags. Hence, a smart approach of selecting an object's key-frames other than simply skipping at a fixed time interval is imperative to reduce the workload. Several previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref> have been proposed to let the annotators select the key-frames as human can decide better than an algorithm. However, this will implicitly introduce additional cognitive loads to the annotators for making such decision, and possibly cause them to spend unnecessary amount of time in browsing the video and drawing the bounding-boxes at unimportant frames. To address this problem, in our pipeline, we first complete the much simpler first and third subtasks by temporally localizing a new object for its visible portion. Then, we propose a novel strategy, inspired by the idea of divide and conquer, to interactively generate key-frames from coarse to fine, and spatially localize the object at the key-frames.</p><p>3.1.1 Temporal Object Localization. In this video task (i.e. the first and third sub-tasks defined above), the annotator browses the video to find objects belonging to the categories of interest. Once a new object is found, the annotator creates a new tag for the object and label it with the correct category. Then, by browsing the video with respect to this object, the annotator draws a bounding-box around the object whenever it appears or disappears, so that the object can be temporally localized in the video and its identity can be specified by these drawn bounding-boxes.</p><p>In order to encourage the annotators to find more objects and temporally localize them thoroughly, they will be rewarded for every bounding-box they draw above. Specifically, we reward 6 points per such bounding-box due to the large amount of efforts needed before drawing the bounding-box. Yet, an upper bound of reward is set for each video to prevent the annotators from labeling excessive objects and bounding-boxes. The sub-tasks are deemed complete if all the apparent objects and their appearing duration are sufficiently labeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Spatial Object</head><p>Localization at Key-frames. After the temporal localization, the appearing and disappearing frames of the annotated objects are available. We next select key-frames within each visible intervals and draw bounding-boxes to spatially localize objects more accurately. For the frames that are not selected as keyframes, the system will automatically generate bounding-boxes by linear interpolation according to the adjacent key-frames and visually check if these generated bounding-boxes are sufficiently accurate in tracking the object. If not, a new key-frame is selected as an atomic task, typically at the middle of the interval, for human annotators to draw the bounding-box. The idea is that true bounding-box trajectory in a shorter interval can be more accurately approximated by linear interpolation. So, we adopt this divide and conquer strategy to gradually generate more key-frames until accurate tracking is achieved.</p><p>To check if an interval can be approximated, we use a set of robust visual trackers T to automatically track the accuracy of the bounding-boxes at both ends in the forward and backward pass, and see if any of the trackers can successfully track the interval in both direction. Specifically, supposing that t f and t b are the bounding-box trajectories produced by the tracker t in both the forward and backward direction, respectively. We determine that a interval is accurately approximated if</p><formula xml:id="formula_0">max t ∈ T {vIoU(t f , t b )} &gt; 0.5, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where vIoU is the voluminal Intersection-over-Union between the two trajectories, and KCF <ref type="bibr" target="#b7">[8]</ref> and MOSSE <ref type="bibr" target="#b0">[1]</ref> tracker are used in our implementation. If the interval is accepted by machine checking, the bounding-box trajectories in the interval of length L will be annotated with a weighted average of trajectory t * f and t * b :</p><formula xml:id="formula_2">t * avд,i = ρ i t * f ,i + ρ L-i+1 t * b,i ρ i + ρ L-i+1 , i = 1, . . . , L,<label>(2)</label></formula><p>where t * is the one that achieves maximum vIoU in Equation ( <ref type="formula" target="#formula_0">1</ref>) and t * •,i is the i-th bounding-box of the trajectory. We set ρ to 0.75 as a global assumption of the tracking precision. Otherwise, a new key-frame will be generated in the interval for the annotator. In case that the interval is large, we will evenly generate multiple keyframes. To draw the bounding-box, the annotator will be presented a short video clip starting from the previous key-frame f k -1 and then visually track the bounding-box from f k -1 to f k .</p><p>As the workload of this task is determined by the number of key-frames generated, we can easily control the cost by limiting the division depth. Also, the annotators are rewarded based on the total number of bounding-boxes drawn at the key-frames, when the video task is passed by sampling inspection. Specifically, we reward 1.5 points per bounding-box in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal Relation Localization</head><p>Given the set of spatio-temporally annotated objects in a video, annotating relations is to temporally localize the relations of interest for each pair of objects. This is because the same pair of objects may have different relations at different time intervals. Due to the huge combinations of possible object pairs at various time intervals, the number of atomic tasks increases drastically compared to that of object annotation. This also amplifies the difficulty in searching for interesting relations from a large number of candidates. Several tricks can be used to alleviate the pain by presenting part of video data to the annotator. For example, the annotator is only allowed to browse the video clip where the two objects in a pair both appear. Additionally, just presenting the bounding-box trajectories of one pair of objects at a time can also avoid confusing the annotator with too many irrelevant annotations. Further, splitting the task based on the type of relation, namely the spatial relations and action relations, helps to reduce the annotation difficulty without significant loss of quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Spatial Relation</head><p>Localization. Spatial relations are the type of relation that indicates the relative position between two objects, such as "A-in front of-B" and "A-towards-B". Usually, the spatial relation between two objects will not change frequently; and different categories of spatial relation are often mutually exclusive at a time. This allows us to annotate them at a coarse grained level with simpler atomic tasks. Hence, in addition to the tricks mentioned above, the trimmed video clips can also be further partitioned into overlapping segments of fixed temporal length, typically in 3-second segments with 1 second overlap, forming a group of simple atomic tasks. In each atomic task, the annotator is presented a single segment with a pair of objects, and asked to choose one or more categories of spatial relation presence. By merging the adjacent segments with same labeled category, it is easy to automatically consolidate the segment-level results into the overall temporal localization.</p><p>In the video task, we use sampling inspection to check the annotation quality of the atomic tasks, and the annotators are rewarded based on the total number of the atomic tasks in the video. Specifically, we reward 1 point per atomic task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Action Relation</head><p>Localization. Action relation includes the relations like "A-watch-B", "A-hold-B", "A-kick-B", etc., whose duration ranges from transient to long-lasting. Thus, annotating action relation with good quality requires the temporal localization at frame-level precision, which needs a new approach different from that for spatial relation. Moreover, action relations occur much less frequently than spatial relations, so there will be a large portion of segments without any action relation, stultifying the effect of working on segments.</p><p>While it is not likely to simplify the atomic task that needs to temporally localize action relations for a pair of objects, we can still reduce the overall workload by utilizing the information of object category provided by the previous stage. Since the categories of subject for action relation are constrained to be human and animal, we can automatically filter out many object pairs according to this constraint, reducing a large number of atomic tasks for annotation.</p><p>For quality control, as there is naturally a small portion of object pairs that are annotated with action relations, we mainly focus on checking the annotation quality on these object pairs to ensure high precision. To achieve considerable recall as well, we reward the annotators based on the number of relations localized in order to encourage them find out more. Specifically, we reward 1 point for viewing an atomic task and 8 points for localizing a relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">JUSTIFICATIONS</head><p>In this section, we will justify the effectiveness of our proposed solutions to address the main issues discussed in Section 1. To this end, we show how each of the solutions reduce the workload and cost in annotating objects and relations in user-generated videos. The reported statistics in this section is based on the annotation of 10,000 videos, whose details will be further presented in Section 5, using our proposed annotation pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effectiveness of the Key-frame Generation Scheme</head><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, the average number of manually labeled bounding-boxes is only 4.08 percent of the total number of boundingboxes, meaning that the scheme can significantly save the workload and cost for bounding-box annotation. We can also infer that the average annotation frequency is every 24 frames, which is similar to that in some related works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref> in generating key-frames at a fixed time interval. This suggests that the resulting boundingbox trajectories are at least of similar quality overall. However, our scheme is actually producing trajectories of better quality by substantially exploiting the valuable human labor, because it adaptively selects the key-frames where there are larger object or camera motions, or more severe illumination and occlusion. For example, Table <ref type="table" target="#tab_0">1</ref> demonstrates that the number of key-frames generated for active objects (e.g. frisbee and racket) is generally more than that for inactive objects (e.g. piano and baby seat). Even for a single object, the scheme can also generate key-frames adapted to the situation when tracking of the object is difficult.</p><p>On the other hand, with the assistance of human intelligence, the trackers used in our scheme can automatically produce a large proportion (87.58% as indicated in the last row of the table) of bounding-boxes with sufficient confidence and quality. In terms of training object detectors, we can use these bounding-boxes as data augmentation to train robust built-in classification models since they provide a massive quantity of variant regions over the objects in addition to the accurately localized ones. Moreover, the quality of the automatically produced bounding-boxes can be controlled by some parameters. In our annotation setting, we require the overlapping between the two trajectories produced by forward and backward tracking to be at least 0.5 in vIoU. Hence, more accurate bounding-box trajectories can be obtained by setting the requirement of the vIoU higher. However, this will reduce the number of automatically produced bounding-boxes by the trackers, while more human assistance is needed.</p><p>We can also see from Table <ref type="table" target="#tab_0">1</ref> that the use of two trackers (i.e. KCF and MOSSE) compensate for each other's weakness to achieve good tracking results, but there is still 8.34% of bounding-boxes generated by linear interpolation due to the failure of tracking at those frames. This is because existing trackers still have limitation when the tracked object is of small size with large motion, or has high deformation. However, as our scheme interactively and selectively asks human to label the bounding-boxes at the frames that are difficult for the existing trackers, we can use these bounding-boxes as difficult training samples to develop more robust visual trackers. This can further reduce the workload and cost of annotation in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of Splitting Spatial and Action Relation Localization</head><p>As mentioned in Section 2.  The number of trimmed video clips and their total duration in (subject, object) pairs that need to be annotated (the 1st group) and the relations annotated finally (the 2nd group). A trimmed video clip is a complete and continuous clip in which the pair and the relation triplet exists. We show the statistics of the (subject, object) pairs of all categories in the 1st row, while we constrain the categories of the subjects to be human and animal in the 2nd row. The other approach is to directly present the trimmed video clip to the annotators and let them find and localize the relations of interest, which is a harder task but will produce more accurate annotation results for shorter relations. In our pipeline, we split the annotation of spatial and action relations and apply different approaches in order to exploit their respective characteristics for less annotation workload and cost. Therefore, we justify the effectiveness of this solution based on analysis of the reward mechanism in Section 3.2 and the statistics provided in Table <ref type="table" target="#tab_2">2</ref>. First, we argue that it can save much cost by adopting the first approach rather than the second one in spatial relation annotation. According to the total duration in the 1st row of Table <ref type="table" target="#tab_2">2</ref>, we can partition all the trimmed video clips into around 1.83 million 3second segments with overlap of 1 second. Thus, the first approach would require a cost of 1.83 million points based on the reward mechanism. However, according to the number of trimmed clips in the 1st and 3rd rows in the table, the second approach would require a cost of 0.43+0.27 * 8 = 2.59 million points, which is 42% more than that required by the adopted method. On the other hand, spatial relations are usually of long duration, because the relative spatial states between two objects will not switch fast in general. We can also see from the 3rd row in Table <ref type="table" target="#tab_2">2</ref> that the average duration of the final annotated spatial relations is 9.7s, which is much longer than that of action relations. So it will not significantly decrease the accuracy of temporal localization to use the first approach with a 1-second granularity for spatial relations.</p><p>Second, since the subjects of action relations can only be human or animal, separating the annotation of action relations from spatial relations can naturally reduce the number of trimmed video clips that need to be annotated. Comparing the first two rows in Table <ref type="table" target="#tab_2">2</ref>, we can see that this workload can be reduced by 39%. Regarding cost, we can show that the aforementioned two approaches cost roughly the same on the annotation of action relations. For the first approach, according to the total duration in the 2nd row, we can partition all the trimmed video clips into around 1.13 million 3-second segments with overlap of 1 second, and thus this approach costs 1.13 million points. For the second approach, according to the number of trimmed video clips in the 2nd and 4th rows, it would require a cost of 0.26 + 0.11 * 8 = 1.14 million points. While there is little saving for the first approach, it still cannot offset the loss in accuracy of temporal localization, especially for the transient relations, such as "A-hit-B" and "A-throw-B". Hence, the approach used in action relation annotation should be different from that used in spatial relation annotation. This verifies the effectiveness of the proposed solution that splits the annotation of spatial and action relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE DATASET: VidOR</head><p>As one of the major contributions of this paper, we present a novel large-scale video dataset, named Video Object Relation (VidOR), comprising of 10,000 user-generated videos with dense annotations on 80 categories of objects and 50 categories of predicates. Remarkably, using the annotation pipeline introduced in the previous sections, we are able to construct the dataset at a modest cost. Figure <ref type="figure" target="#fig_4">6</ref> shows some examples from the dataset with visualization of annotations. In the rest of the paper, we will further analyze the characteristic of VidOR from several aspects and discuss several benchmark tasks based on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Video Source</head><p>We source the videos for VidOR from YFCC-100M <ref type="bibr" target="#b22">[23]</ref>, a large publicly and freely accessible multimedia collection containing 0.8 million videos from Flickr. Most of the videos are user-generated whose content ranges from indoor to outdoor, from daily life to business occasion, and from low quality to professional quality. According to our statistics, the average length of videos is around 30 seconds, but the actual video length ranges from several seconds to minutes. In order to avoid extreme cases that can cause huge difficulty in annotation, we filter out videos that have one of the following properties:</p><p>• extremely low resolution that cannot be properly viewed;</p><p>• heavily shaking camera motions in most parts of video;</p><p>• heavy artificial effect;</p><p>• containing only one object from the 80 categories;</p><p>• containing a crowd of objects from the 80 categories.</p><p>Finally, we selected 10,000 qualified videos from the source and used them for the annotation. for testing. Also, we ensure that all the annotated categories of objects and predicates will appear in each of the train/val/test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset Split</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Object Categories</head><p>We determine the set of object categories to annotate based on the categories used in the prevailing object detection datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. In order to capture the diversity in human-centric relations, we further divide the common class of human (a.k.a. person) in these datasets into adult, child and baby. This results in a set of 80 object categories, which can be found in Figure <ref type="figure">4</ref>.</p><p>Overall, there are 38,602 objects annotated in the train/val set, meaning that the average number of objects annotated per video is 4.9. Additionally, the number of objects per category roughly follows the long-tail distribution, as shown in Figure <ref type="figure">4</ref>. We can also find that there are more than half of objects belonging to the categories of human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Predicate Categories</head><p>Our selection of predicate categories to annotate is inspired by <ref type="bibr" target="#b5">[6]</ref>. We selected 50 categories of basic predicates (shown in Figure <ref type="figure">5</ref>), including 42 categories of atomic action predicates and 8 categories of common spatial predicates. This is different from <ref type="bibr" target="#b19">[20]</ref>, in which most of the predicate categories are generated by combining two categories from a small set of action and spatial predicate categories.</p><p>As ambiguity about view point exists in the definition of spatial relation, we normalize the view point to be from object instead of camera because it contains more semantic information. For example, the predicate "car-behind-child" suggests that the car is behind the child's back regardless of the camera position. In the case that the orientation of object is inapplicable (e.g. ball), the predicates such as "in front of" and "behind", will not be annotated.</p><p>We count the number of relations with respect to predicate category in the train/val set, and show the statistics in Figure <ref type="figure">5</ref>. It can be seen that the spatial relations account for a large proportion; meanwhile, the "watch" action relations has the same magnitude as most of the spatial relations. In total, there are 297,352 relations annotated in the train/val set. On average, there are 29.2 spatial relations and 8.8 action relations per video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Relation Statistics</head><p>In addition to the relation statistics based on the type of predicate, we further look into the relation statistics at triplet level. Through the combination of object pairs and predicates, there will be 320,000 possible types of relation triplets if not considering some impossible combinations. In fact, we can see from Figure <ref type="figure" target="#fig_3">3</ref> that there are 6,258 types of relation triplets (dark and grey area) appearing in the training set, and 2,410 types (grey and light area) appearing in the validation set.</p><p>Moreover, 2,115 types of relation triplets appear in both the training and validation sets, while 295 types of relation triplets only appear in the validation set. This forms a zero-shot learning scenario that requires the models to predict labels that they do not see during the training phase, which has been preliminary studied by several works. In the validation set, three are 30,142 relation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Benchmarks Tasks</head><p>We discuss and define three application scenarios in which VidOR can be readily used for benchmarking.</p><p>Video Object Detection is the first step towards relation understanding in videos. Its primary goal is to develop robust object detectors that can not only localize objects from the 80 categories with bounding boxes in every video frames, but also link the bounding boxes that indicate the same object entity into a trajectory. The challenges in VidOR requires the detectors to understand which object entities are in a video and how their locations change over time, and technically overcome the difficulties in free camera motion, as well as illumination and object deformation, etc.</p><p>The task of Video Visual Relation Detection (VidVRD) as defined in <ref type="bibr" target="#b19">[20]</ref> is well suited to VidOR. It aims to detect relations from the 80 object and 50 predicate categories, and spatio-temporally localize them by localizing the bounding-box trajectories of the subject and object within the maximal duration of the relation. In VidOR, the detection of action relations requires the detectors able to recognize the object of an action, possibly by recognizing the object's response. As for spatial relations, the detectors are required to recognize the orientation of objects (if applicable) and the relative spatial configuration between the subject and object.</p><p>While VidOR is initially constructed for object and relation recognition, we note that there are sufficient number of human and 42 categories of common action annotated in the dataset. So the dataset can be adapted as a benchmark for the task of action detection as defined in <ref type="bibr" target="#b5">[6]</ref>, which aims to detect actions from the 42 categories and spatio-temporally localize the bounding-box trajectory of the subject within the maximal duration of the action. It typically requires the action detectors to overcome the large variation within each category of action representation and learn the intention of the action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed an annotation pipeline to annotate objects and relations in user-generated videos at large scale. The pipeline addresses key issues in key-frame generation and task decomposition in order to scale up the annotation at a modest cost. To demonstrate its effectiveness, we annotated 10,000 videos in the real world and analyzed the statistics from the annotation procedure. Furthermore, we presented a novel large-scale video dataset<ref type="foot" target="#foot_0">1</ref> constructed by the proposed annotation pipeline. With the dense annotations at object and relation level, the dataset can serve as a benchmark for many multimedia tasks in fine-grained video analysis, and facilitate the research on bridging the gap between vision and language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4 5 6 2 oFigure 2 :</head><label>622</label><figDesc>Figure 2: Illustration of the proposed annotation pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>segments (3-second segments with 1 second overlap in our annotation setting) and merge the annotation results at segment level to form the overall temporal localization, leaving annotators to do the simple selection tasks for the segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Statistics of the types of relation triplets in train/val set. The dark area shows the number and portion of triplet types unique in the training set; the grey area shows those appearing in both of the training and validation set; and the light area shows those unique to the validation set.</figDesc><graphic coords="6,353.04,85.60,170.08,85.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Several video examples from VidOR dataset. The examples cover the content from indoor to outdoor scenes, including events like sport, party and wedding, etc. For each video example, we show two key-frames to reflect the changes of relations. To visualize the annotations, for example &lt;A, watch, B&gt;, the bounding-boxes of A and B are displayed around the corresponding objects in different colors, and the relation is displayed within A's bounding-box in the color of B's bounding-box. instances in total, among which 641 relation instances belong to the 295 triplet types.</figDesc><graphic coords="8,75.89,257.67,113.37,85.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The percentage (%) of bounding-boxes generated by manual labeling, KCF<ref type="bibr" target="#b7">[8]</ref> and MOSSE<ref type="bibr" target="#b0">[1]</ref> trackers (the remaining is generated by linear interpolation) against each object category. After sorting according to the percentage of manually labeled bounding-boxes, the categories of top-5 and bottom-5 are shown in the 1st and 2nd group in the table. The last row shows the statistics over all the categories.</figDesc><table><row><cell></cell><cell cols="3">manual (%) KCF (%) MOSSE (%)</cell></row><row><cell>piano</cell><cell>2.03</cell><cell>50.57</cell><cell>43.56</cell></row><row><cell>baby seat</cell><cell>2.35</cell><cell>53.41</cell><cell>41.82</cell></row><row><cell>panda</cell><cell>2.50</cell><cell>62.41</cell><cell>28.89</cell></row><row><cell>guitar</cell><cell>2.50</cell><cell>50.51</cell><cell>39.52</cell></row><row><cell>toilet</cell><cell>2.52</cell><cell>44.49</cell><cell>44.54</cell></row><row><cell>ski</cell><cell>13.90</cell><cell>33.27</cell><cell>24.23</cell></row><row><cell>bat</cell><cell>14.06</cell><cell>30.86</cell><cell>14.60</cell></row><row><cell>surfboard</cell><cell>19.51</cell><cell>30.78</cell><cell>15.04</cell></row><row><cell>frisbee</cell><cell>21.28</cell><cell>31.86</cell><cell>6.35</cell></row><row><cell>racket</cell><cell>22.08</cell><cell>26.71</cell><cell>12.37</cell></row><row><cell>all categories</cell><cell>4.08</cell><cell>56.39</cell><cell>31.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>To use the dataset for model development, we split the dataset into 7,000 videos for training, 835 videos for validation, and 2,165 videos Object statistics per category in train/val set. The categories are grouped into three upper level categories: Human(3), Animal<ref type="bibr" target="#b27">(28)</ref> and Other(49). It can be found that the number of objects in Human category accounts for 56.34% of the total object occurrences, while that in Animal and Other category are 35.78% and 7.98%, respectively.</figDesc><table><row><cell>Per Category Data Size</cell><cell>10 1 10 2 10 3 10 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Human Animal Other</cell></row><row><cell></cell><cell>adult</cell><cell>child</cell><cell>toy</cell><cell cols="2">baby</cell><cell>dog</cell><cell>chair</cell><cell>car</cell><cell>table</cell><cell>cup</cell><cell>sofa</cell><cell>ball/sports_ball</cell><cell>bottle</cell><cell>screen/monitor</cell><cell>guitar</cell><cell>cat</cell><cell>bicycle</cell><cell>backpack</cell><cell>baby_seat</cell><cell>watercraft</cell><cell>bird</cell><cell>camera</cell><cell>handbag</cell><cell cols="2">laptop</cell><cell>cellphone</cell><cell>stool</cell><cell>dish</cell><cell>duck</cell><cell>horse</cell><cell>bench</cell><cell>motorcycle</cell><cell>piano</cell><cell>ski</cell><cell>cake</cell><cell>baby_walker</cell><cell>elephant</cell><cell>fish</cell><cell>bat</cell><cell>snowboard</cell><cell>bus/truck</cell><cell>penguin</cell><cell>chicken</cell><cell>electric_fan</cell><cell>hamster/rat</cell><cell>sheep/goat</cell><cell>faucet</cell><cell>surfboard</cell><cell>aircraft</cell><cell>fruits</cell><cell>sink</cell><cell>refrigerator</cell><cell>train</cell><cell>pig</cell><cell>cattle/cow</cell><cell>skateboard</cell><cell>rabbit</cell><cell>turtle</cell><cell>tiger</cell><cell>microwave</cell><cell>panda</cell><cell>lion</cell><cell>suitcase</cell><cell>bread</cell><cell>kangaroo</cell><cell>scooter</cell><cell>vegetables</cell><cell>racket</cell><cell>oven</cell><cell>traffic_light</cell><cell>camel</cell><cell>bear</cell><cell>crab</cell><cell>antelope</cell><cell>toilet</cell><cell>stop_sign</cell><cell>snake</cell><cell>squirrel</cell><cell>leopard</cell><cell>frisbee</cell><cell>stingray</cell><cell>crocodile</cell></row><row><cell cols="5">in_front_of Figure 4: next_to 10 0 10 5 10 1 10 2 10 3 10 4 Per Category Data Size</cell><cell cols="2">watch</cell><cell cols="2">behind</cell><cell>away</cell><cell cols="2">towards</cell><cell>beneath</cell><cell cols="2">above</cell><cell cols="2">hold</cell><cell>lean_on</cell><cell cols="2">speak_to</cell><cell>ride</cell><cell cols="2">touch</cell><cell cols="2">hug</cell><cell cols="2">hold_hand_of</cell><cell cols="2">carry</cell><cell>hit</cell><cell cols="2">play(instrument)</cell><cell>push</cell><cell cols="2">grab</cell><cell cols="2">bite</cell><cell>release</cell><cell cols="2">caress</cell><cell>lift</cell><cell cols="2">pull</cell><cell cols="2">pat</cell><cell>wave</cell><cell cols="2">press</cell><cell>inside</cell><cell cols="2">use</cell><cell cols="2">point_to</cell><cell>chase</cell><cell cols="2">feed</cell><cell>kiss</cell><cell cols="2">throw</cell><cell>kick</cell><cell cols="2">smell</cell><cell cols="2">wave_hand_to</cell><cell>lick</cell><cell cols="2">drive</cell><cell>clean</cell><cell cols="2">shout_at</cell><cell cols="2">knock</cell><cell>squeeze</cell><cell cols="2">get_on</cell><cell>get_off</cell><cell cols="5">shake_hand_with Spatial Predicate cut open close Action Predicate</cell></row></table><note><p>Figure 5: Predicate statistics per category in train/val set. Each bar indicates the number of relations whose predicate belongs to that category. The two types of predicates (i.e. spatial(8) and actions(42)) are highlighted in different colors. Their proportions are 76.77% and 23.23%, respectively.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available at https://lms.comp.nus.edu.sg/research/vidor.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is part of the NExT++ project, supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>David S Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">A</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2544" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep-based ingredient recognition for cooking recipe retrieval</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collecting and annotating human activities in web videos</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heilbron</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niebles</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia Retrieval</title>
		<meeting>International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">377</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>João F Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The THUMOS challenge on action recognition for videos &quot;in the wild</title>
		<author>
			<persName><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Consumer video understanding: A benchmark database and an evaluation of human and machine performance</title>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangnan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM International Conference on Multimedia Retrieval</title>
		<meeting>the 1st ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fashion landmark detection in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="229" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The design and implementation of ViPER</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mihalcik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Describing Common Human Visual Actions in Images</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Ruggero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronchi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Mark</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xianghua</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gary</forename><forename type="middle">K L</forename><surname>Xie</surname></persName>
		</editor>
		<editor>
			<persName><surname>Tam</surname></persName>
		</editor>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>Article 52, 12 pages</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video visual relation detection</title>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1300" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised dense video captioning</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5159" to="5167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object Trajectory Proposal via Hierarchical Volume Grouping</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="344" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<author>
			<persName><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">YFCC100M: The New Data in Multimedia Research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficiently scaling up crowdsourced video annotation</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="204" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpretable Video Captioning via Trajectory Structured Localization</title>
		<author>
			<persName><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingge</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6829" to="6837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Catching the temporal regionsof-interest for video captioning</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="146" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fine-Grained Video Captioning for Sports Narrative</title>
		<author>
			<persName><forename type="first">Huanyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6006" to="6015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3261" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Labelme video: Building a video database with human annotations</title>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1451" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flowguided feature aggregation for video object detection</title>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
