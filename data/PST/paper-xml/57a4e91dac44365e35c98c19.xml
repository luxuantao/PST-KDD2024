<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-05-31">31 May 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-31">31 May 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1605.08695v2[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, machine learning has driven advances in many different fields <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr" target="#b74">73,</ref><ref type="bibr" target="#b77">76]</ref>. We attribute this success to the invention of more sophisticated machine learning models <ref type="bibr" target="#b44">[42,</ref><ref type="bibr" target="#b53">51]</ref>, the availability of large datasets for tackling problems in these fields <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b66">65]</ref>, and the development of software platforms that enable the easy use of large amounts of computational resources for training such models on these large datasets <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b22">21]</ref>.</p><p>We introduce the TensorFlow system 1 for experimenting with new models, training them on large datasets, and moving them into production. We have based TensorFlow on years of experience with our first-generation system, DistBelief <ref type="bibr" target="#b22">[21]</ref>, both simplifying and generalizing it to enable researchers to explore a wider variety of ideas with relative ease. TensorFlow supports both large-scale training and inference: it efficiently uses hundreds of powerful (GPU-enabled) servers for fast training, and it runs trained models for inference in production on various platforms, ranging from large distributed clusters in a datacenter, down to performing inference locally on mobile devices. At the same time, it is flexible and general enough to support experimentation and research into new machine learning models and system-level optimizations.</p><p>TensorFlow uses a unified dataflow graph to represent both the computation in an algorithm and the state on which the algorithm operates. We draw inspiration from the high-level programming models of dataflow systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b76">75]</ref>, and the low-level efficiency of parameter servers <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b48">46]</ref>. Unlike traditional dataflow systems, in which graph vertices represent functional computation on immutable data, TensorFlow allows vertices to represent computations that own or update mutable state. Edges carry tensors (multi-dimensional arrays) between nodes, and TensorFlow transparently inserts the appropriate communication between distributed subcomputations. By unifying the computation and state management in a single programming model, TensorFlow allows programmers to experiment with different parallelization schemes that, for example, offload computation onto the servers that hold the shared state to reduce the amount of network traffic. We have also built various coordination protocols, and achieved encouraging results with synchronous replication, echoing recent results <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b20">19]</ref> that contradict the commonly held belief that asynchronous replication is required for scalable learning <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b48">46]</ref>.</p><p>Over the past year, more than 60 teams at Google have used TensorFlow, and we have released the system as an open-source project. Thanks to our large community of users we have gained experience with many different machine learning applications. In this paper, we focus on neural network training as a challenging systems problem, and select two representative applications from this space: image classification and language modeling. These applications stress computational throughput and aggregate model size respectively, and we use them both to demonstrate the extensibility of TensorFlow, and to evaluate the efficiency and scalability of our present implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background &amp; Motivation</head><p>To make the case for developing TensorFlow, we start by outlining the requirements for a large-scale machine learning system ( §2.1), then consider how related work meets or does not meet those requirements ( §2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Requirements</head><p>Distributed execution A cluster of powerful computers can solve many machine learning problems more efficiently, using more data and larger models.</p><p>Machine learning algorithms generally perform better with more training data. For example, recent breakthroughs in image classification models have benefited from the public ImageNet dataset, which contains 136 gigabytes of digital images <ref type="bibr" target="#b66">[65]</ref>; and language modeling has benefited from efforts like the One Billion Word Benchmark <ref type="bibr" target="#b11">[10]</ref>. The scale of these datasets motivates a dataparallel approach to training: a distributed file system holds the data, and a set of workers processes different subsets of data in parallel. Data-parallelism eliminates the I/O bottleneck for input data, and any preprocessing operations can be applied to input records independently.</p><p>Effective learned models for image recognition, language modeling, document clustering, and many other problems have a large number of parameters. For example, the current state-of-the-art image classification model, ResNet, uses 2.3 million floating-point parameters to classify images into one of 1000 categories <ref type="bibr" target="#b27">[26]</ref>. The One Billion Word Benchmark has a vocabulary of 800,000 words, and it has been used to train language models with 1.04 billion parameters <ref type="bibr" target="#b41">[39]</ref>. A distributed system can shard the model across many processes, to increase the available network bandwidth when many workers are simultaneously reading and updating the model.</p><p>A distributed system for model training must use the network efficiently. Many scalable algorithms train a model using mini-batch gradient descent <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b49">47]</ref>, where a worker reads the current version of the model and a small batch of input examples, calculates an update to the model that reduces a loss function on those examples, and applies the update to the model. Mini-batch methods are most effective when each worker uses the most current model as a starting point, which requires a large amount of data to be transferred to the worker with low latency.</p><p>Accelerator support Machine learning algorithms often perform expensive computations, such as matrix multiplication and multi-dimensional convolution, which are highly parallelizable, but have many data dependencies that require a tightly coupled implementation. The recent availability of general-purpose GPUs has provided a large number of cores that can operate on fast local memory. For example, a single NVIDIA Titan X GPU card has 6 TFLOPS peak performance <ref type="bibr" target="#b61">[60]</ref>. In 2012, state-ofthe-art results for different image classification tasks were achieved using 16,000 CPU cores for three days <ref type="bibr" target="#b47">[45]</ref>, and using two GPUs for six days <ref type="bibr" target="#b44">[42]</ref>. Since then, GPU vendors have innovated in their support for machine learning: NVIDIA's cuDNN library <ref type="bibr" target="#b14">[13]</ref> for GPU-based neural network training accelerates several popular image models by 2-4× when using version R4 in place of R2 <ref type="bibr" target="#b16">[15]</ref>.</p><p>In addition to general-purpose devices, many specialpurpose accelerators for deep learning have achieved significant performance improvements and power savings. At Google, our colleagues have built the Tensor Processing Unit (TPU) specifically for machine learning, and it achieves an order of magnitude improvement in performance-per-watt compared to alternative state-of-the-art technology <ref type="bibr" target="#b40">[38]</ref>. The Movidius Deep Learning Accelerator uses a low-power Myriad 2 processor with custom vector processing units that accelerate many machine learning and computer vision algorithms <ref type="bibr" target="#b55">[53]</ref>. Ovtcharov et al. have achieved significant performance improvements and power savings for some convolutional models using field programmable gate arrays (FPGAs) <ref type="bibr" target="#b59">[58]</ref>. Since it is difficult to predict the next popular architecture for executing machine learning algorithms, we require that TensorFlow uses a portable programming model that can target a generic device abstraction, and allows its operations to be specialized for new architectures as they emerge.</p><p>Training &amp; inference support In addition to training, scalable and high-performance inference is a requirement for using models in production <ref type="bibr" target="#b19">[18]</ref>. Depending on the nature of the application, the inference may be required to produce results with very low latency in an interactive service, or execute on a disconnected mobile device. If the model is large, it might require multiple servers to participate in each inference computation, and thus require distributed computation support. Developers benefit when they can use the same code to define a model for both training and inference. Training and inference demand similar performance, so we prefer a common welloptimized system for both computations. Since inference can be computationally intensive (e.g., an image classification model might perform 5 billion FLOPS per image <ref type="bibr" target="#b71">[70]</ref>), it must be possible to accelerate it with GPUs.</p><p>Extensibility Single-machine machine learning frameworks <ref type="bibr" target="#b38">[36,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">17]</ref> have extensible programming models that enable their users to advance the state of the art with new approaches, such as adversarial learning <ref type="bibr" target="#b26">[25]</ref> and deep reinforcement learning <ref type="bibr" target="#b53">[51]</ref>. We seek a system that provides the same ability to experiment, and also allows users to scale up the same code to run in production. The system must support expressive control-flow and stateful constructs, while also satisfying our other requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related work</head><p>Single-machine frameworks Many machine learning researchers carry out their work on a single-often GPUequipped-computer <ref type="bibr" target="#b43">[41,</ref><ref type="bibr" target="#b44">42]</ref>, and many flexible singlemachine frameworks have emerged to support this scenario. Caffe <ref type="bibr" target="#b38">[36]</ref> is a high-performance framework for training declaratively specified convolutional neural networks that runs on multicore CPUs and GPUs. Theano <ref type="bibr" target="#b1">[2]</ref> allows programmers to express a model as a dataflow graph, and generates efficient compiled code for training that model. Torch <ref type="bibr" target="#b18">[17]</ref> has an imperative programming model for scientific computation (including machine learning) that supports fine-grained control over the order of execution and memory utilization.</p><p>While these frameworks do not satisfy our requirement for distributed execution, TensorFlow's programming model is close to Theano's dataflow representation ( §3).</p><p>Batch dataflow systems Starting with MapReduce <ref type="bibr" target="#b23">[22]</ref>, batch dataflow systems have been applied to a large number of machine learning algorithms <ref type="bibr" target="#b72">[71]</ref>, and more recent systems have focused on increasing expressivity and performance. DryadLINQ <ref type="bibr" target="#b75">[74]</ref> adds a high-level query language that supports more sophisticated algorithms than MapReduce. Spark <ref type="bibr" target="#b76">[75]</ref> extends DryadLINQ with the ability to cache previously computed datasets in memory, and is therefore better suited to iterative machine learning algorithms (such as k-means clustering and logistic regression) when the input data fit in memory. Dandelion extends DryadLINQ to support generating code for GPUs <ref type="bibr" target="#b64">[63]</ref> and FPGAs <ref type="bibr" target="#b17">[16]</ref>.</p><p>The principal limitation of a batch dataflow system is that it requires the input data to be immutable, and all of the subcomputations to be deterministic, so that the system can re-execute subcomputations when machines in the cluster fail. This feature-which is beneficial for many conventional workloads-makes updating a machine learning model a heavy operation. For example, the SparkNet system for training deep neural networks on Spark takes 20 seconds to broadcast weights and collect updates from five workers <ref type="bibr" target="#b54">[52]</ref>. As a result, these systems must process larger batches in each model update step, which slows convergence <ref type="bibr" target="#b10">[9]</ref>. We show in Subsection 6.3 that TensorFlow can train larger models on larger clusters with step times as short as 2 seconds.</p><p>While not a batch dataflow system, Naiad <ref type="bibr" target="#b56">[54]</ref> augments a dataflow model with streaming execution, stateful vertices, and structured timestamps ("timely dataflow") that enable it to handle incremental updates and iterative algorithms in the same computation. Naiad represents iteration using cyclic dataflow graphs, which together with mutable state make it possible to implement algorithms that require millisecond-scale latencies for coordination. Naiad is designed for computing on sparse, discrete data, and does not support GPU (or any other form of) acceleration, but we borrow aspects of timely dataflow iteration in Subsection 3.4.</p><p>Parameter servers Inspired by work on distributed key-value stores, a parameter server architecture uses a set of servers to manage shared state that is updated by a set of data-parallel workers. Unlike a standard key-value store, the write operation in a parameter server is specialized for parameter updates: it is typically an associative and commutative combiner, like addition-assignment (+=), that is applied to the current parameter value and the incoming update to produce a new parameter value.</p><p>Parameter servers emerged as an architecture for scalable topic modeling <ref type="bibr" target="#b67">[66]</ref>, and our previous system DistBelief <ref type="bibr" target="#b22">[21]</ref> showed how a similar architecture could be applied to deep neural network training. Project Adam <ref type="bibr" target="#b15">[14]</ref> demonstrated an efficient parameter server architecture for training convolutional neural networks, and Li et al.'s "Parameter Server" <ref type="bibr" target="#b48">[46]</ref> added innovations in consistency models, fault tolerance, and elastic rescaling. Despite earlier skepticism that parameter servers would be compati-ble with GPU acceleration <ref type="bibr" target="#b15">[14]</ref>, Cui et al. have recently shown that GeePS <ref type="bibr" target="#b20">[19]</ref>, a parameter server specialized for use with GPUs, can achieve speedups on modest-sized clusters.</p><p>MXNet <ref type="bibr" target="#b13">[12]</ref> is a recent system that uses a parameter server to scale training, supports GPU acceleration, and includes a flexible programming model with interfaces for many languages. While MXNet partially fulfills our extensibility requirements, the parameter server is "privileged" code, which makes it difficult for researchers to customize the handling of large models ( §4.2).</p><p>The parameter server architecture meets most of our requirements, and our DistBelief <ref type="bibr" target="#b22">[21]</ref> uses parameter servers with a Caffe-like model definition format <ref type="bibr" target="#b38">[36]</ref> to great effect. We found this architecture to be insufficiently extensible, because adding a new optimization algorithm, or experimenting with an unconventional model architecture would require our users to modify the parameter server implementation, which uses C++ for performance. While some of the practitioners who use that system are comfortable with making these changes, the majority are accustomed to writing models in high-level languages, such as Python and Lua, and the complexity of the highperformance parameter server implementation is a barrier to entry. With TensorFlow we therefore sought a highlevel programming model that allows users to customize the code that runs in all parts of the system ( §3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TensorFlow execution model</head><p>TensorFlow uses a single dataflow graph to represent all computation and state in a machine learning algorithm, including the individual mathematical operations, the parameters and their update rules, and the input preprocessing (Figure <ref type="figure" target="#fig_0">1</ref>). Dataflow makes the communication between subcomputations explicit, and therefore makes it easy to execute independent computations in parallel, and partition the computation across multiple distributed devices. Dataflow TensorFlow differs from batch dataflow systems ( §2.2) in two respects:</p><p>• The model supports multiple concurrent executions on overlapping subgraphs of the overall graph.</p><p>• Individual vertices may have mutable state that can be shared between different executions of the graph.</p><p>The key observation in the parameter server architecture <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b48">46]</ref> is that mutable state is crucial when training very large models, because it becomes possible to make in-place updates to very large parameters, and propagate those updates to parallel training steps as quickly as possible. Dataflow with mutable state enables Tensor-Flow to mimic the functionality of a parameter server, but with additional flexibility, because it becomes possible to execute arbitrary dataflow subgraphs on the machines that host the shared model parameters. As a result, our users have been able to experiment with different optimization algorithms, consistency schemes, and parallelization strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataflow graph elements</head><p>In a TensorFlow graph, each vertex represents an atomic unit of computation, and each edge represents the output from or input to a vertex. We refer to the computation at vertices as operations, and the values that flow along edges as tensors, because TensorFlow is designed for mathematical computation, and uses tensors (or multidimensional arrays) to represent all data in those computations.</p><p>Tensors In TensorFlow, we model all data as tensors (dense n-dimensional arrays) with each element having one of a small number of primitive types, such as int32, float32, or string. Tensors naturally represent the inputs to and results of the common mathematical operations in many machine learning algorithms: for example, a matrix multiplication takes two 2-D tensors and produces a 2-D tensor; and a mini-batch 2-D convolution takes two 4-D tensors and produces another 4-D tensor.</p><p>All tensors in TensorFlow are dense. This decision ensures that the lowest levels of the system can have simple implementations for memory allocation and serialization, which reduces the overhead imposed by the framework. To represent sparse tensors, TensorFlow offers two alternatives: either encode the data into variable-length string elements of a dense tensor, or use a tuple of dense tensors (e.g., an n-D sparse tensor with m non-zero elements could be represented an m × n index matrix and a length-m value vector). The size of a tensor can vary in one or more dimensions, making it possible to represent sparse tensors with differing numbers of elements, at the cost of more sophisticated shape inference.</p><p>Operations An operation takes m ≥ 0 tensors as input, and produces n ≥ 0 tensors as output. An operation has a named "type" (such as Const, MatMul, or Assign) and may have zero or more compile-time attributes that determine its behavior. An operation can be generic and variadic at compile-time: its attributes determine both the expected types and arity of its inputs and outputs.  For example, the simplest operation Const has no inputs and a single output. Const has an attribute T that determines the type of its output, and an attribute Value that determines the value that it produces. AddN is variadic: it has a type attribute T, and an integer attribute N that defines how many inputs (of type T) it accepts.</p><p>Stateful operations: variables An operation can contain mutable state that is read and/or written each time it executes. A Variable operation owns a mutable buffer that is used to store the shared parameters of a model as it is trained. A Variable has no inputs, and produces a reference handle, which acts as a typed capability for reading and writing the buffer. A Read operation takes a reference handle as input, and outputs the value of the variable as a dense tensor. Several operations can modify the underlying buffer: for example, AssignAdd takes a reference handle r and a tensor value x, and when executed performs the update State Stateful operations: queues TensorFlow includes several queue implementations, which support more advanced forms of coordination. The simplest queue is FIFOQueue, which owns an internal queue of tensors, and supports concurrent access. Like a Variable, the FIFOQueue operation produces a reference handle that can be consumed by one of the standard queue operations, such as Enqueue and Dequeue. These operations respectively push their input onto the tail of the queue, or pop the head element and output it. Enqueue will block if its given queue is full, and Dequeue will block if its given queue is empty. When queues are used in an input preprocessing pipeline, this blocking provides backpressure; it also supports synchronization ( §4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Partial and concurrent execution</head><p>TensorFlow uses the dataflow graph to represent all possible computations in a particular application, and the API for executing a graph allows the client to specify the subgraph that should be executed. A subgraph is specified declaratively: the client selects zero or more edges to feed input tensors into the dataflow, and one or more edges to fetch output tensors from the dataflow; the runtime then prunes the graph to contain the necessary set of operations. Each invocation of the API is called a step, and TensorFlow supports multiple concurrent steps on the same graph, where stateful operations enable coordination between the steps.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a typical training application, with multiple subgraphs that execute concurrently, and interact through shared variables and queues. The core training subgraph depends on a set of model parameters, and input batches from a queue. Many concurrent steps of the training subgraph update the model based on different input batches, to implement data-parallel training. To fill the input queue, concurrent preprocessing steps transform individual input records (e.g., decoding images and applying random distortions), and a separate I/O subgraph reads records from a distributed file system. A checkpointing subgraph runs periodically for fault tolerance ( §4.3).</p><p>Partial and concurrent execution is responsible for much of TensorFlow's flexibility. Adding mutable state and coordination via queues makes it possible to specify a wide variety of model architectures in "unprivileged" code, which enables advanced users to experiment without modifying the internals of the TensorFlow runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distributed execution</head><p>Dataflow simplifies distributed execution, because it makes communication between subcomputations explicit. In principle, the same TensorFlow program can be deployed to a distributed cluster of GPUs for training, a cluster of TPUs for serving, and a cellphone for mobile inference.</p><p>Each operation resides on a particular device, such as a CPU or GPU in a particular task. A device is responsible for executing a kernel for each operation assigned to it.</p><p>TensorFlow allows multiple kernels to be registered for a single operation, with specialized implementations for a particular device or data type (see §5 for details). For many operations, such as element-wise operators (Add, Sub, etc.), we use a single kernel implementation that can be compiled for CPU and GPU using different compilers.</p><p>The TensorFlow runtime places operations on devices, subject to implicit or explicit device constraints in the graph. The placement algorithm computes a feasible set of devices for each operation, calculates the sets of operations that must be colocated, and selects a satisfying device for each colocation group. Stateful operations and operations their state must be placed on the same device, which leads to implicit colocation constraints. In addition, the user may specify partial device preferences such as "any device in a particular task", or "a GPU in any task", and the runtime will respect these constraints. A typical training application will use client-side programming constructs to add constraints such that, for example, parameters are distributed among a set of "PS" tasks.</p><p>Once the operations in a graph have been placed, and the partial subgraph has been computed for a step ( §3.2), TensorFlow partitions the operations into per-device subgraphs. A per-device subgraph for device d contains all of the operations that were assigned to d, with additional Send and Recv operations that replace edges across device boundaries. Send transmits its single input to a specified device as soon as the tensor is available, using a rendezvous key to name the value. Recv has a single output, and blocks until the value for a specified rendezvous key is available locally, before producing that value. Send and Recv have specialized implementations for several device-type pairs; we describe some of these in Section 5.</p><p>We optimized TensorFlow for executing large subgraphs repeatedly with low latency. Once the graph for a step has been pruned, placed, and partitioned, its subgraphs are cached in their respective devices. A client session maintains the mapping from step definitions to cached subgraphs, so that a distributed step on a large graph can be initiated with one small message to each participating task. This model favors static, reusable graphs, but it can support dynamic computations using dynamic control flow, as the next subsection describes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dynamic control flow</head><p>Most evaluation in TensorFlow is strict: all inputs to an operation must be computed before the operation executes. Advanced algorithms-such as efficiently training a recurrent neural network <ref type="bibr" target="#b39">[37]</ref>-require dynamic control flow, which for efficiency requires non-strict evaluation.  TensorFlow supports conditional control flow using the primitive Switch and Merge operations, which are based on Arvind and Culler's original dynamic dataflow architectures <ref type="bibr" target="#b4">[4]</ref>. Switch acts like a demultiplexer: it takes a data input and a control input, and uses the control input to select which of its two outputs should produce a value. The Switch output not taken receives a special dead value, which propagates recursively through the rest of the graph until it reaches a Merge operation. Merge acts like a multiplexer: it forwards at most one non-dead input to its output, or produces a dead output if both of its inputs are dead. We use these primitives to build a nonstrict conditional subgraph (Figure <ref type="figure" target="#fig_2">2</ref>) that executes one of two branches, based on the runtime value of a tensor.</p><p>Switch and Merge also support iteration. The implementation of loops in TensorFlow is based on Switch and Merge <ref type="bibr" target="#b4">[4]</ref>, with additional structural constraints based on timely dataflow <ref type="bibr" target="#b56">[54]</ref> to simplify the distributed execution state. Like timely dataflow, TensorFlow supports multiple concurrent iterations and nested loops, but simplifies memory management by restricting each operation to producing a single value per output per iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Extensibility case studies</head><p>By choosing a unified dataflow graph to represent all computation in TensorFlow, we have enabled users to experiment with features that were built into the runtime of our previous system <ref type="bibr" target="#b22">[21]</ref>. In this section, we discuss four extensions to TensorFlow that we have built using simple dataflow primitives and "user-level" code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Differentiation and optimization</head><p>Many learning algorithms train a set of parameters using some variant of stochastic gradient descent (SGD), which entails computing the gradients of a cost function with respect to those parameters, then updating the parameters based on those gradients. We implement a user-level li-brary for TensorFlow that automatically differentiates expressions. A user can, for example, define a neural network as a composition of layers and a loss function, and the library will derive the backpropagation <ref type="bibr" target="#b65">[64]</ref>.</p><p>The differentiation algorithm performs breadth-first search to identify all of the backwards paths from the target operation (e.g., a loss function) to a set of parameters, and sums the partial gradients that each path contributes. Our users frequently specialize the gradients for some operations, and they have implemented optimizations like batch normalization <ref type="bibr" target="#b34">[32]</ref> and gradient clipping <ref type="bibr" target="#b60">[59]</ref> to accelerate training and make it more robust. We have extended the algorithm to differentiate conditional and iterative subcomputations ( §3.4), and developed techniques for managing GPU memory when iterating (and accumulating intermediate values) over long sequences in the input data (similar to GeePS <ref type="bibr" target="#b20">[19]</ref>).</p><p>TensorFlow users can also experiment with a wide range of optimization algorithms, which compute new values for the parameters in each training step. SGD is easy to implement in a parameter server: for each parameter W , gradient ∂L/∂W , and learning rate α, the update rule is W ← W − α × ∂L/∂W . A parameter server can implement SGD by using -= as the write operation, and writing α × ∂L/∂W to each W after a training step.</p><p>However, there are many more advanced optimization schemes that are difficult to express as a single write operation. For example, the Momentum algorithm accumulates a "velocity" for each parameter based on its gradient over multiple iterations, then computes the parameter update from that accumulation; and many refinements to this algorithm have been proposed <ref type="bibr" target="#b68">[67]</ref>. To implement Momentum in DistBelief <ref type="bibr" target="#b22">[21]</ref>, we had to modify the C++ code of the parameter server to change the representation of parameter data, and execute arbitrary code in the write operation; such modifications are beyond the majority of our users. Optimization algorithms are the topic of active research, and our users have implemented several on top of TensorFlow, including Momentum, Adagrad, Adadelta, RMSProp, Adam, and L-BFGS. These can be built in TensorFlow using Variable operations and primitive mathematical operations without needing to modify the underlying system, which makes it easy to experiment with new algorithms as they emerge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Handling very large models</head><p>To train a model on high-dimensional data, such as words in a corpus of text <ref type="bibr" target="#b8">[7]</ref>, it is common to use a distributed representation, which embeds a training example as a pattern of activity across several neurons, which can be  learned by backpropagation <ref type="bibr" target="#b30">[29]</ref>. For example, in a language model, a training example might be a sparse vector with non-zero entries corresponding to the IDs of words in a vocabulary, and the distributed representation for each word will be a lower-dimensional vector <ref type="bibr" target="#b7">[6]</ref>.</p><p>Inference proceeds by multiplying a batch of b sparse vectors against an n × d embedding matrix, where n is the number of words in the vocabulary, and d is the desired dimensionality, to produce a much smaller b × d dense matrix representation; for training, most optimization algorithms modify only the rows of the embedding matrix that were read by the sparse multiplication. In many Tensor-Flow models that process sparse data, n×d can amount to gigabytes of parameters: e.g., a large language model may use over 10 9 parameters with a vocabulary of 800,000 words <ref type="bibr" target="#b41">[39]</ref>, and we have experience with document models <ref type="bibr" target="#b21">[20]</ref> where the parameters occupy several terabytes. Such models are too large to copy to a worker on every use, or even to store in RAM on a single host.</p><p>We implement sparse embedding layers in the Tensor-Flow graph as a composition of primitive operations. Figure <ref type="figure" target="#fig_4">3</ref> shows a simplified graph for an embedding layer that is split across two parameter server tasks. The core operation of this subgraph is Gather, which extracts a sparse set of rows from a tensor, and TensorFlow colocates this operation with the variable on which it operates. The dynamic partition (Part) operation divides the incoming indices into variable-sized tensors that contain the indices destined for each shard, and the dynamic static (Stitch) operation reassembles the partial results from each shard into a single result tensor. Each of these operations has a corresponding gradient, so it supports automatic differentiation ( §4.1), and the result is a set of sparse update operations that act on just the values that were originally gathered from each of the shards.</p><p>While sparse reads and updates are possible in a parameter server <ref type="bibr" target="#b48">[46]</ref>, TensorFlow adds the flexibility to offload arbitrary computation onto the devices that host the shared parameters. For example, classification models typically use a softmax classifier that multiplies the final output by a weight matrix with c columns, where c is the number of possible classes; for a language model, c is the size of the vocabulary, which can be large. Our users have experimented with several schemes to accelerate the softmax calculation. The first is similar to an optimization in Project Adam <ref type="bibr" target="#b15">[14]</ref>, whereby the weights are sharded across several tasks, and the multiplication and gradient calculation are colocated with the shards. More efficient training is possible using a sampled softmax <ref type="bibr" target="#b37">[35]</ref>, which performs a sparse multiplication based on the true class for an example and a set of randomly sampled false classes. We compare the performance of these two schemes in §6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fault tolerance</head><p>Training a model can take several hours or days, even using a large number of machines <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b15">14]</ref>. It is desirable to be able to train a model using non-dedicated resources, for example using a cluster manager, like Mesos <ref type="bibr" target="#b29">[28]</ref> or Borg <ref type="bibr" target="#b73">[72]</ref>, that does not guarantee availability of the same resources for the duration of the training process. Therefore, a TensorFlow job is likely to experience failure during the training process, and we require some form of fault tolerance. However, failures are unlikely to be so common that individual operations need fault tolerance, so a mechanism like Spark's RDDs <ref type="bibr" target="#b76">[75]</ref> would impose significant overhead for little benefit. There is no need to make every write to the parameter state durable, because we can recompute any update from the input data, and many learning algorithms do not require strong consistency <ref type="bibr" target="#b63">[62]</ref>. Although we do not use strong consistency for the training state, we rely on a system like Chubby <ref type="bibr" target="#b9">[8]</ref> or ZooKeeper <ref type="bibr" target="#b32">[31]</ref> to map task IDs to IP addresses.</p><p>We implement user-level checkpointing for fault tolerance in TensorFlow, using primitive operations in the graph (Figure <ref type="figure" target="#fig_0">1</ref>): Save writes one or more tensors to a checkpoint file, and Restore reads one or more tensors from a checkpoint file. Our typical configuration connects each Variable in a task to the same Save operation, with one Save per task, to maximize the I/O bandwidth to a distributed file system. The Restore operations read named tensors from a file, and a standard Assign stores the restored value in its respective variable. During training, a typical client runs all of the Save operations periodically to produce a new checkpoint; when the client starts up, it attempts to Restore the latest checkpoint.</p><p>TensorFlow includes a client library for constructing the appropriate graph structure, and invoking Save and Restore as necessary. This behavior is customizable: the user can apply different policies to subsets of the variables in a model, or customize the checkpoint retention scheme. For example, many users retain checkpoints with the highest score in a custom evaluation metric. The implementation is also reusable: it may be used for model fine-tuning and unsupervised pre-training <ref type="bibr" target="#b45">[43,</ref><ref type="bibr" target="#b47">45]</ref>, which are forms of transfer learning, in which the parameters of a model trained on one task (e.g. recognizing general images) are used as the starting point for another task (e.g. recognizing particular breeds of dog). Having checkpoint and parameter management as programmable operations in the graph gives users the flexibility to implement schemes like these and others that we have not anticipated.</p><p>The checkpointing library does not attempt to produce consistent checkpoints: if training and checkpointing execute concurrently, the checkpoint may include none, all, or some of the updates from the training step. This is no problem for models that we train by asynchronous gradient descent <ref type="bibr" target="#b22">[21]</ref>. Consistent checkpoints require additional synchronization to ensure that checkpointing does not run concurrently with update operations. For example, one can use the scheme in next subsection to take a checkpoint after the synchronous update step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Synchronous replica coordination</head><p>SGD is robust to asynchrony <ref type="bibr" target="#b63">[62]</ref>, and previous systems train deep neural networks using asynchronous parameter updates <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b15">14]</ref>, which are believed scalable because they maintain high throughput in the presence of stragglers. The increased throughput comes at the cost of training steps using stale data. Some have recently revisited the assumption that synchronous training does not scale <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b20">19]</ref>. Since GPUs enable training with hundreds-rather than thousands <ref type="bibr" target="#b47">[45]</ref>-of machines, it may be possible to train a model synchronously in less time than asynchronous training on the same machines.</p><p>Though we designed TensorFlow for asynchronous training, we have begun experimenting with synchronous methods. The TensorFlow graph enables users to change how parameters are read and written when training a model, and we implement three alternatives. In the asynchronous case (Figure <ref type="figure">4</ref>(a)), each worker reads the current value when the step begins, and applies its gradient to the different current value at the end: this ensures high utilization, but the individual steps use stale information, making each step less effective. The synchronous cases use queues ( §3.1) to coordinate execution: a blocking queue acts as a barrier to ensure that all workers read the same parameter version, and a second queue accumulates mul-  tiple gradient updates in order to apply them atomically.</p><p>The simple synchronous version (Figure <ref type="figure">4</ref>(b)) accumulates updates from all workers before applying them, but slow workers limit overall throughput.</p><p>To mitigate stragglers, we implement backup workers (Figure <ref type="figure">4</ref>(c), <ref type="bibr" target="#b12">[11]</ref>), which are similar to MapReduce backup tasks <ref type="bibr" target="#b23">[22]</ref>. Whereas MapReduce starts backup tasks reactively-after detecting a straggler-our backup workers run proactively, and the aggregation takes the first m of n updates produced. We exploit the fact that SGD samples training data randomly, so each worker processes a different random batch. In Subsection 6.3 we show how backup workers improve throughput by up to 15%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation</head><p>We implement TensorFlow as an extensible, crossplatform library. Figure <ref type="figure" target="#fig_6">5</ref> illustrates the system architecture: a thin C API separates user-level in various languages from the core library. In this section, we discuss the implementation of the various components.</p><p>The core TensorFlow library is implemented in C++ for portability and performance: it runs on several operating systems including Linux, Mac OS X, Android, and iOS; the x86 and various ARM-based CPU architectures; and NVIDIA's Kepler, Maxwell, and Pascal GPU microarchitectures. The implementation is open-source, and we have accepted several external contributions that enable TensorFlow to run on other architectures. The distributed master translates user requests into execution across a set of tasks. Given a graph and a step definition, it prunes ( §3.2) and partitions ( §3.3) the graph to obtain subgraphs for each participating device, and caches these subgraphs so that they may be re-used in subsequent steps. Since the master sees the overall computation for a step, it applies standard optimizations such as common subexpression elimination and constant folding; pruning is a form of dead code elimination. It then coordinates execution of the optimized subgraphs across a set of tasks.</p><p>The dataflow executor in each task handles requests from the master, and schedules the execution of the kernels that comprise a local subgraph. We optimize the dataflow executor for running large, fine-grained graphs with low overhead; our current implementation dispatches approximately 2,000,000 null operations per second. The dataflow executor dispatches kernels to local devices and runs kernels in parallel when possible: e.g., by using multiple cores in a CPU device, or multiple streams on a GPU.</p><p>The runtime contains over 200 standard operations, including mathematical, array manipulation, control flow, and state management operations. Many of the operation kernels are implemented using Eigen::Tensor <ref type="bibr" target="#b36">[34]</ref>, which uses C++ templates to generate efficient parallel code for multicore CPUs and GPUs; however, we liberally use libraries like cuDNN <ref type="bibr" target="#b14">[13]</ref> to implement kernels where a more efficient specialization is possible. We have also implemented support for quantization, which enables faster inference in environments such as mobile devices and high-throughput datacenter applications, and use the gemmlowp low-precision matrix multiplication library <ref type="bibr" target="#b35">[33]</ref> to accelerate quantized computation.</p><p>We specialize Send and Recv operations for each pair of source and destination device types. Transfers between local CPU and GPU devices use the cudaMemcpyAsync() API to overlap computation and data transfer; transfers between two local GPUs use DMA to relieve pressure on the host. For transfers between tasks, TensorFlow supports multiple protocols, including gRPC over TCP, and RDMA over Converged Ethernet. We are also investigating optimizations for GPU-to-GPU communication that use collective operations <ref type="bibr" target="#b58">[57]</ref>.</p><p>Section 4 describes features that we implement totally above the C API, in user-level code. Typically, users compose standard operations to build higher-level abstractions, such as neural network layers, optimization algorithms ( §4.1), and sharded embedding computations ( §4.2). TensorFlow supports multiple client languages, and we have prioritized support for Python and C++, because our internal users are most familiar with these languages. As features become more established, we typically port them to C++, so that users can access an optimized implementation from all client languages.</p><p>If it is difficult or inefficient to represent a subcomputation as a composition of operations, users can register additional kernels that provide an efficient implementation written in C++. We have found it profitable to hand-implement fused kernels for some performance critical operations, such as a the ReLU and Sigmoid activation functions and their corresponding gradients. We are currently investigating automatic kernel fusion using Halide <ref type="bibr" target="#b62">[61]</ref> and other compiler-based techniques.</p><p>In addition to the core runtime, our colleagues have built several tools that aid users of TensorFlow. These include serving infrastructure for running inference in production, a visualization dashboard that enables users to follow the progress of a training run, a graph visualizer that helps users to understand the connections in a model, and a distributed profiler that traces the execution of a computation across multiple devices and tasks. We describe these tools in an extended whitepaper <ref type="bibr" target="#b0">[1]</ref>, and they can be downloaded from the project repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>In this section, we evaluate the performance of Tensor-Flow on several synthetic and realistic workloads. Unless otherwise stated, we run all experiments on a shared production cluster, and all figures plot median values with error bars showing the 10th and 90th percentiles.</p><p>Here we focus on system performance metrics, rather than learning objectives like time to accuracy. TensorFlow is a system that allows machine learning practitioners and researchers to experiment with new techniques, and this evaluation demonstrates that the system (i) has little overhead, and (ii) can employ large amounts of computation to accelerate real-world applications. While techniques like synchronous replication can enable some models to converge in fewer steps overall, we defer the analysis of such improvements to other papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Single-machine benchmarks</head><p>Although TensorFlow is a system for "large-scale" machine learning, it is imperative that scalability does not mask poor performance at small scales <ref type="bibr" target="#b51">[49]</ref>. Table <ref type="table" target="#tab_2">1</ref> contains results from Chintala's independent benchmark of convolutional models on TensorFlow and three singlemachine frameworks <ref type="bibr" target="#b16">[15]</ref>. All frameworks use a six-core Intel Core i7-5930K CPU at 3.5 GHz and an NVIDIA Titan X GPU. Table <ref type="table" target="#tab_2">1</ref> shows that TensorFlow achieves shorter step times than Caffe <ref type="bibr" target="#b38">[36]</ref>, and performance within 6% of the latest version of Torch <ref type="bibr" target="#b18">[17]</ref>. We attribute the similar performance of TensorFlow and Torch to the fact that both use the same version of the cuDNN library <ref type="bibr" target="#b14">[13]</ref>, which implements the convolution and pooling operations on the critical path for training; Caffe uses open-source implementations for these operations that are simpler but less efficient than cuDNN. The Neon library [56] outperforms TensorFlow on three of the models, by using handoptimized convolutional kernels <ref type="bibr" target="#b46">[44]</ref> implemented in assembly language; in principle, we could implement these kernels in TensorFlow, but we have not yet done so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Synchronous replica microbenchmark</head><p>The performance of our coordination implementation ( §4.4) is the main limiting factor for scaling with additional machines. Figure <ref type="figure" target="#fig_7">6</ref> shows that number of null training steps that TensorFlow performs per second for varying model sizes, and increasing numbers of synchronous workers. In a null training step, a worker fetches the shared model parameters from 16 PS tasks, performs a trivial computation, and sends updates to the parameters. The Scalar curve in Figure <ref type="figure" target="#fig_7">6</ref> shows the best performance that we could expect for a synchronous training step, because only a single 4-byte value is fetched from each PS task. The median step time is 1.8 ms using a single worker, growing to 8.8 ms with 100 workers. These times measure the overhead of the synchronization mechanism, and capture some of the noise that we expect when running on a shared cluster.</p><p>The Dense curves show the performance of a null step when the worker fetches the entire model. We repeat the experiment with models of size 100 MB and 1 GB, with the parameters sharded equally over 16 PS tasks. The median step time for 100 MB increases from 147 ms with one worker to 613 ms with 100 workers. For 1 GB, it increases from 1.01 s with one worker to 7.16 s with 100 workers.</p><p>For large models, it is typical that a training step accesses only a subset of the parameters, and the Sparse curves show the throughput of the embedding lookup operation from Subsection 4.2. Each worker reads 32 randomly selected entries from a large embedding matrix containing 1 GB or 16 GB of data. As expected, the step times do not vary with the size of the embedding, and TensorFlow achieves step times ranging from 5 to 20 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Image classification</head><p>Deep neural networks have achieved breakthrough performance on computer vision tasks such as recognizing objects in photographs <ref type="bibr" target="#b44">[42]</ref>, and these tasks are a key application for TensorFlow at Google. Training a network to high accuracy requires a large amount of computation, and we use TensorFlow to scale out the compu-tation across a cluster of GPU-enabled servers. In these experiments, we focus on Google's Inception-v3 model, which achieves 78.8% accuracy the ILSVRC 2012 image classification challenge <ref type="bibr" target="#b71">[70]</ref>; the same techniques apply to other deep convolutional models-such as Microsoft's ResNet <ref type="bibr" target="#b27">[26]</ref>-that TensorFlow users have implemented. We investigate the scalability of training the Inception-v3 model using multiple replicas. We configure a Tensor-Flow job with 17 PS tasks, and vary the number of worker tasks. Each worker task has one NVIDIA K40 GPU and 5 IvyBridge cores, and a PS task has 8 IvyBridge cores. We investigate the effect of coordination ( §4.4) on training performance, using up to 200 workers to validate recent promising results for synchronous training <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b20">19]</ref>. In particular, if synchronous training can be made efficient, a model such as Inception-V3 will train in fewer steps, and converge to a higher accuracy than with asynchronous training <ref type="bibr" target="#b12">[11]</ref>.</p><p>Training throughput improves to 2,300 images per second as we increase the number of workers to 200, but with diminishing returns (Figure <ref type="figure" target="#fig_10">7(a)</ref>). Figures <ref type="figure" target="#fig_10">7(b</ref>) and (c) explain the limits to scaling: as we add more workers, the step time increases, because there is more contention on the PS tasks, both at the network interface and in the aggregation of updates. As expected, for all configurations, synchronous steps are longer than asynchronous steps, because all workers must wait for the slowest worker to catch up before starting the next step. While the median synchronous step is approximately 10% longer than an asynchronous step with the same workers, above the 90th percentile the synchronous performance degrades sharply, because stragglers disproportionately impact the tail.</p><p>To mitigate tail latency, we can add backup workers, so that a step completes when the first m of n tasks produce gradients. Figure <ref type="figure" target="#fig_11">8</ref> shows the effect on step time of adding backup workers to a 50-worker Inception training job. Each additional backup worker up to and including the fourth reduces the median step time, because the probability of a straggler affecting the step decreases. Adding a fifth backup worker slightly degrades performance, because the 51st worker (i.e., the first whose result is discarded) is more likely to be a non-straggler that generates more incoming traffic for the PS tasks. Figure <ref type="figure" target="#fig_11">8</ref> also plots the normalized speedup for each configuration, which we define as t(b)/t(0)×50/(50+b) (where t(b) is the median step time with b backup workers), and which discounts the speedup by the fraction of additional resources consumed. Although adding 4 backup workers achieves the shortest overall step time (1.93 s), adding 3 achieves the highest normalized speedup (9.5%), and hence trains the model to the same quality using less aggregate GPU-time. Step time (secs) Step time (secs) Step time (seconds)</p><p>Step time </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Language modeling</head><p>Given a sequence of words, a language model predicts the most probable next word <ref type="bibr" target="#b7">[6]</ref>. Therefore, language models are integral to predictive text, speech recognition, and translation applications. In this experiment, we investigate how TensorFlow can train a recurrent neural network (viz. LSTM-512-512 <ref type="bibr" target="#b41">[39]</ref>) to model the text in the One Billion Word Benchmark <ref type="bibr" target="#b11">[10]</ref>. The vocabulary size |V | limits the performance of training, because the final layer must decode the output state into probabilities for each of |V | classes <ref type="bibr" target="#b37">[35]</ref>. The resulting parameters can be large (|V | × d for output state dimension d) so we use the techniques for handling large models from Subsection 4.2. We use a restricted vocabulary of the most common 40,000 words-instead of the full 800,000 words <ref type="bibr" target="#b11">[10]</ref>-in order to experiment with smaller configurations. Figure <ref type="figure" target="#fig_12">9</ref> shows the training throughput, measured in  words per second, for varying numbers of PS and worker tasks, and two softmax implementations. The full softmax (dashed lines) multiplies each output by a 512 × 40, 000 weight matrix sharded across the PS tasks. Adding more PS tasks increases the throughput, because TensorFlow can exploit distributed model parallelism <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b43">41]</ref> and perform the multiplication and gradient calculation on the PS tasks, as in Project Adam <ref type="bibr" target="#b15">[14]</ref>. Adding a second PS task is more effective than increasing from 4 to 32, or 32 to 256 workers. Eventually the throughput saturates, as the LSTM calculations dominate the training step. The sampled softmax (solid lines) reduces the data transferred and the computation performed at the PS tasks <ref type="bibr" target="#b37">[35]</ref>. Instead of a dense weight matrix, it multiplies the output by a random sparse matrix containing weights for the true class and a random sample of false classes. We sample 512 classes for each batch, which reduces the softmax data transfer and computation by a factor of 78.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have described the TensorFlow system and its extensible dataflow-based programming model. The core idea of this paper is that TensorFlow's dataflow representation subsumes existing work on parameter server systems, and offers a uniform programming model that allows users to harness large-scale heterogeneous systems, both for production tasks and for experimenting with new approaches. We have shown several examples of how the TensorFlow programming model supports experimentation ( §4) and demonstrated that the resulting implementations are performant and scalable ( §6).</p><p>Our initial experience with TensorFlow is encouraging. A large number of groups at Google have deployed TensorFlow in production, and TensorFlow is helping our research colleagues to make new advances in machine learning. Since we released TensorFlow as open-source software, over 8,000 people have forked the source code repository, the binary distribution has been downloaded 500,000 times, and our users have published dozens of machine learning models that use TensorFlow.</p><p>TensorFlow is a work in progress. Its flexible dataflow representation enables power users to achieve excellent performance, but we have not yet determined default policies that work well for most users. Further research on automatic optimization should bridge this gap. On the system level, we are actively developing algorithms for automatic placement, kernel fusion, memory management, and scheduling. While the current implementations of mutable state and fault tolerance suffice for applications with weak consistency requirements, we expect that some TensorFlow applications will require stronger consistency, and we are investigating how to build such policies at user-level. Finally, our users are demanding, and some have begun to chafe at the limitations of a static dataflow graph, especially for algorithms like deep reinforcement learning <ref type="bibr" target="#b53">[51]</ref>. Therefore, we face the intriguing problem of providing a system that transparently and efficiently uses distributed resources, even when the structure of the computation unfolds dynamically.</p><p>By sharing the implementation of TensorFlow and engaging with the research community, we hope that this work will spur further research in distributed systems and machine learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A schematic TensorFlow dataflow graph for a training pipeline contains subgraphs for reading input data, preprocessing, training, and checkpointing state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[r] ← State[r] + x. Subsequent Read(r) operations produce the value State [r].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A conditional graph using Switch and Merge</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Schematic dataflow graph for a sparse embedding layer containing a two-way sharded embedding matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 Figure 4 :</head><label>34</label><figDesc>Figure 4: Three parameter synchronization schemes for a single parameter in data-parallel training ( §4.4): (a) asynchronous, (b) synchronous without backup workers, and (c) synchronous with backup workers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The layered TensorFlow architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Baseline throughput for synchronous replication with a null model. Sparse accesses enable TensorFlow to handle larger models, such as embedding matrices ( §4.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (a) Inception-v3 training throughput increases with up to 200 workers. However, adding more workers gets diminishing returns because the step time increases for both (b) asynchronous and (c) synchronous replication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Backup workers reduce the step time for 50worker Inception-v3 training. 4 backup workers give the shortest overall step time, but 3 backup workers are most efficient when we normalize for the total resources used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Increasing the number of PS tasks leads to increased throughput for language model training, by parallelizing the softmax computation. Sampled softmax increases throughput by performing less computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Step times for training four convolutional models with different libraries, using one GPU. All results are for training with 32-bit floats. The fastest library for each model is shown in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Training step time (ms)</cell><cell></cell></row><row><cell>Library</cell><cell cols="4">AlexNet Overfeat OxfordNet GoogleNet</cell></row><row><cell>Caffe [36]</cell><cell>324</cell><cell>823</cell><cell>1068</cell><cell>1935</cell></row><row><cell>Neon [56]</cell><cell>87</cell><cell>211</cell><cell>320</cell><cell>270</cell></row><row><cell>Torch [17]</cell><cell>81</cell><cell>268</cell><cell>529</cell><cell>470</cell></row><row><cell>TensorFlow</cell><cell>81</cell><cell>279</cell><cell>540</cell><cell>445</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge contributions from our colleagues within Google, and from members of the wider machine learning community. In particular, we appreciate the feedback we have received both from the rest of the Google Brain team and the hundreds of DistBelief and TensorFlow users that has helped us improve the usability of functionality of the system.</p><p>Many individuals have contributed to TensorFlow, including: John Giannandrea (for creating a supportive research environment); Irina Kofman, Amy McDonald Sandjideh, and Phing Turner (project management); Ashish Agarwal, Dave Andersen, Anelia Angelova, Eugene Brevdo, Yaroslav Bulatov, Jerjou Cheng, Maciek Chociej, Craig Citro, Greg Corrado, George Dahl, Andrew Dai, Lucy Gao, mig Gerard, Ian Goodfellow, Stephan Gouws, Gunhan Gulsoy, Steinar Gunderson, Andrew Harp, Peter Hawkins, Yangqing Jia, Rafal Jozefowicz, Łukasz Kaiser, Naveen Kumar, Geoffrey Hinton, Mrinal Kalakrishnan, Anjuli Kannan, Rasmus Larsen, Yutaka Leon-Suematsu, Frank Li, Peter Liu, Xiaobing Liu, Olivia Nordquist, Chris Olah, Nishant Patil, Saurabh Saxena, Mike Schuster, Andrew Selle, Pierre Sermanet, Noam Shazeer, Jonathon Shlens, Jascha Sohl-Dickstein, Ilya Sutskever, Kunal Talwar, Philip Tucker, Vincent Vanhoucke, Oriol Vinyals, Chad Whipkey, Yonghui Wu, Ke Yang, Zongheng Yang, and Yao Zhang (general contributions to the project); Shan Carter, Doug Fritz, Patrick Hurst, Dilip Krishnan, Dan Mané, Daniel Smilkov, Fernanda Viégas, Martin Wattenberg, James Wexler, Jimbo Wilson, Kanit Wongsuphasawat, Cassandra Xia, and the Big Picture team (visualization); Chris Leary, Robert Hundt, Robert Springer, Cliff Young, and the Stream Executor team (accelerator support); Norm Jouppi and the team that created the Tensor Processing Unit; Kayur Patel, Michael Piatek, and the coLab team; and the growing community of open-source contributors and users who have helped make TensorFlow better.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="CoRR,abs/1603.04467,2016.arxiv.org/abs/1603.04467.Softwareavailablefromtensorflow.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belopolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bleecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>De Brébisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ducoffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Ebrahimi</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Harlouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Korobov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefrancois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lemieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Livezey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lowin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Orlandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Salvatier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Savard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shabanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Spieckermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Subramanyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tanguay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Tulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Willson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno>arxiv.org/abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pedestrian detection with a large-field-of-view deep network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m">IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="704" to="711" />
		</imprint>
	</monogr>
	<note>CalTech PDF</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Arvind</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Culler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of computer science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dataflow</forename><surname>Architectures</surname></persName>
		</author>
		<ptr target="www.dtic.mil/cgi-bin/GetTRDoc?Location=U2&amp;doc=GetTRDoc.pdf&amp;AD=ADA166235" />
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="225" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<idno>arxiv.org/abs/1412.7755</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
		<ptr target="www.iro.umontreal.ca/˜lisa/pointeurs/BengioDucharmeVincentJauvinjmlr.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Web 1T 5-gram version 1</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</author>
		<ptr target="catalog.ldc.upenn.edu/LDC2006T13" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Chubby lock service for looselycoupled distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<ptr target="www.usenix.org/legacy/event/osdi06/tech/fullpa-pers/burrows/burrows.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Symposium on Operating Systems Design and Implementation, OSDI &apos;06</title>
		<title level="s">USENIX Association</title>
		<meeting>the 7th Symposium on Operating Systems Design and Implementation, OSDI &apos;06<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="335" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sample size selection in optimization methods for machine learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-012-0572-5</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="155" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<idno>arxiv.org/abs/1312.3005</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting distributed synchronous SGD</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<idno>arxiv.org/abs/1604.00981</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop Track</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="www.cs.cmu.edu/muli/file/mxnet-learning-sys.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Learning Systems at Neural Information Processing Systems (LearningSys)</title>
				<meeting>the Workshop on Machine Learning Systems at Neural Information Processing Systems (LearningSys)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<idno>arxiv.org/abs/1410.0759</idno>
		<title level="m">cuDNN: Efficient primitives for deep learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Project Adam: Building an efficient and scalable deep learning training system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalyanaraman</surname></persName>
		</author>
		<ptr target="www.usenix.org/system/files/conference/osdi14/osdi14-paper-chilimbi.pdf" />
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">convnet-benchmarks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="github.com/soumith/convnet-benchmarks" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LIN-Qits: Big data on little clients</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/2485922.2485945</idno>
		<ptr target="ACM.doi.acm.org/10.1145/2485922.2485945" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture, ISCA &apos;13</title>
				<meeting>the 40th Annual International Symposium on Computer Architecture, ISCA &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Torch: A modular machine learning software library</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mariéthoz</surname></persName>
		</author>
		<ptr target="infoscience.epfl.ch/record/82802/files/rr02-46.pdf" />
	</analytic>
	<monogr>
		<title level="j">IDIAP</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The missing piece in complex analytics: Low latency, scalable model management and serving with Velox</title>
		<author>
			<persName><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>arxiv.org/abs/1409.3809</idno>
	</analytic>
	<monogr>
		<title level="m">CIDR 2015, Seventh Biennial Conference on Innovative Data Systems Research</title>
				<meeting><address><addrLine>Asilomar, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Online Proceedings</publisher>
			<date type="published" when="2015">January 4-7, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GeePS: Scalable deep learning on distributed GPUs with a GPUspecialized parameter server</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh European Conference on Computer Systems, EuroSys &apos;16</title>
				<meeting>the Eleventh European Conference on Computer Systems, EuroSys &apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07998</idno>
		<idno>arxiv.org/abs/1507.07998</idno>
		<title level="m">Document embedding with paragraph vectors</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<publisher>Google Research PDF</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<ptr target="As-sociation.research.google.com/archive/mapreduce-osdi04.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Symposium on Opearting Systems Design &amp; Implementation</title>
				<meeting>the 6th Conference on Symposium on Opearting Systems Design &amp; Implementation<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>OSDI&apos;04</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DeVISE: A deep visualsemantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="research.google.com/pubs/archive/41473.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Gonzalez-Dominguez, I. Lopez-Moreno</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Frame-by-frame language identification in short utterances using deep neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
		<ptr target="re-search.google.com/en//pubs/archive/42929.pdf" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="cc/paper/5423-generative-adversarial-nets" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. December 8-13 2014. 2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arxiv.org/abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multilingual acoustic models using distributed deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="re-search.google.com/pubs/archive/40807.pdf" />
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8619" to="8623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mesos: A platform for fine-grained resource sharing in the data center</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="Association.www.cs.berkeley.edu/˜alig/papers/mesos.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;11</title>
				<meeting>the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;11<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="295" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="www.cogsci.ucsd.edu/˜ajyu/Teaching/Cogs202-sp13/Readings/hinton86.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Annual Conference of the Cognitive Science Society</title>
				<meeting>the Eighth Annual Conference of the Cognitive Science Society<address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<ptr target="www.cs.toronto.edu/˜gdahl/papers/deepSpeechReviewSPM2012.pdf" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ZooKeeper: Wait-free coordination for internetscale systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Junqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010</title>
				<meeting>the 2010</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="www.usenix.org/legacy/event/atc10/tech/full-papers/Hunt.pdf" />
		<title level="m">USENIX Conference on USENIX Annual Technical Conference, USENIXATC&apos;10</title>
				<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="11" to="11" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>arxiv.org/abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">gemmlowp: a small selfcontained low-precision GEMM library</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<ptr target="github.com/google/gemmlowp" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Eigen library for linear algebra</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<ptr target="eigen.tuxfamily.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="www.aclweb.org/anthology/P15-1001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>arxiv.org/pdf/1408.5093</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
				<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Serial order: A parallel distributed processing approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<ptr target="cseweb.ucsd.edu/˜gary/PAPER-SUGGESTIONS/Jordan-TR-8604.pdf" />
	</analytic>
	<monogr>
		<title level="j">ICS report</title>
		<imprint>
			<biblScope unit="volume">8608</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
		<respStmt>
			<orgName>Institute for Cognitive Science, UCSD, La Jolla</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Google supercharges machine learning tasks with TPU custom chip</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
		<ptr target="cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno>arxiv.org/abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="research.google.com/pubs/archive/42455.pdf" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<idno>arxiv.org/abs/1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ima-geNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<ptr target="deeplearn-ing.cs.cmu.edu/pdfs/1111/jmlr10larochelle.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-40</biblScope>
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<idno>arxiv.org/abs/1509.09308</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Building highlevel features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>Google Research PDF</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scaling distributed machine learning with the Parameter Server</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-Y</forename><surname>Su</surname></persName>
		</author>
		<ptr target="www.usenix.org/system/files/conference/osdi14/osdi14-paper-chilimbi.pdf" />
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="583" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficient mini-batch training for stochastic optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<ptr target="www.cs.cmu.edu/˜muli/file/minibatchsgd.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6564</idno>
		<idno>arxiv.org/abs/1412.6564</idno>
		<title level="m">Move evaluation in Go using deep convolutional neural networks</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scalability! But at what COST?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<ptr target="www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Conference on Hot Topics in Operating Systems, HOTOS&apos;15</title>
				<meeting>the 15th USENIX Conference on Hot Topics in Operating Systems, HOTOS&apos;15<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>USENIX Association.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>arxiv.org/abs/1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations: Workshops Track</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
		<ptr target="dx.doi.org/10.1038/nature14236" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SparkNet: Training deep networks in Spark</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>arxiv.org/abs/1511.06051</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Movidius announces Deep Learning Accelerator and Fathom software framework</title>
		<author>
			<persName><forename type="first">Movidius</forename><surname>Ltd</surname></persName>
		</author>
		<ptr target="www.movidius.com/news/movidius-announces-deep-learning-accelerator-and-fathom-software-framework" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Naiad: a timely dataflow system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</title>
				<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>Microsoft Research PDF</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="439" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Massively parallel methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alcicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fearon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04296</idno>
		<idno>arxiv.org/abs/1507.04296</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">NCCL: Optimized primitives for collective multi-gpu communication</title>
		<ptr target="github.com/NVIDIA/nccl" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Toward accelerating deep learning at scale using specialized logic</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<ptr target="re-search.microsoft.com/apps/pubs/default.aspx?id=246506" />
	</analytic>
	<monogr>
		<title level="m">Hot Chips: A Symposium on High Performance Chips. HOTCHIPS</title>
				<imprint>
			<date type="published" when="2015-08">August 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="www.jmlr.org/proceedings/papers/v28/pascanu13.pdf" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Nvidia devtech blog post</title>
		<author>
			<persName><forename type="first">K</forename><surname>Powell</surname></persName>
		</author>
		<ptr target="blogs.nvidia.com/blog/2015/03/17/digits-devbox/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<ptr target="people.csail.mit.edu/fredo/tmp/Halide-5min.pdf" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
		<ptr target="cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dandelion: a compiler and runtime for heterogeneous systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<ptr target="research-srv.microsoft.com/pubs/201110/sosp13-dandelion-final.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</title>
				<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="49" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="www.cs.toronto.edu/hinton/absps/naturebp.pdf" />
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>arxiv.org/abs/1409.0575</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">An architecture for parallel topic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<imprint>
			<publisher>Proc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="jmlr.org/proceedings/papers/v28/sutskever13.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
				<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NIPS, 2014. papers.nips.cc/paper/5346-sequenceto-sequence-learning-with-neural</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>arxiv.org/abs/1409.4842</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;2015</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno>arxiv.org/abs/1512.00567</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Map-reduce for machine learning on multicore</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="cc/paper/3150-map-reduce-for-machine-learning-on-multicore.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Large-scale cluster management at Google with Borg</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pedrosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Korupolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
		<ptr target="re-search.google.com/pubs/archive/43438.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth European Conference on Computer Systems</title>
				<meeting>the Tenth European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Grammar as a foreign language</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<idno>arxiv.org/abs/1412.7449</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">DryadLINQ: A system for general-purpose distributed dataparallel computing using a high-level language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gunda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
		<ptr target="www.usenix.org/legacy/event/osdi08/tech/fullpa-pers/yuy/yuy.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;08</title>
				<meeting>the 8th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;08<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association</title>
				<meeting>the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">On rectified linear units for speech processing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="research.google.com/pubs/archive/40811.pdf" />
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
