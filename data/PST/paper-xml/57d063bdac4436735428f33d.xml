<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DigitSpace: Designing Thumb-to-Fingers Touch Interfaces for One-Handed and Eyes-Free Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Da-Yuan</forename><surname>Huang</surname></persName>
							<email>dayuan.huang@iis.sinica.edu.tw</email>
						</author>
						<author>
							<persName><forename type="first">Liwei</forename><surname>Chan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Media Design</orgName>
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rong-Hao</forename><surname>Liang</surname></persName>
							<email>rhliang@ntu.edu.tw</email>
						</author>
						<author>
							<persName><forename type="first">De-Nian</forename><surname>Yang</surname></persName>
							<email>dnyang@iis.sinica.edu.tw</email>
						</author>
						<author>
							<persName><forename type="first">Yi-Ping</forename><surname>Hung</surname></persName>
							<email>hung@csie.ntu.edu.tw</email>
						</author>
						<author>
							<persName><forename type="first">Bing-Yu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Academia</forename><surname>Sinica</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>May 7-12</addrLine>
									<postCode>2016</postCode>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">CHI 2016</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DigitSpace: Designing Thumb-to-Fingers Touch Interfaces for One-Handed and Eyes-Free Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ADB4D9ECC87B7B63BDF82410733A7EE2</idno>
					<idno type="DOI">10.1145/2858036.2858483</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>5</term>
					<term>m</term>
					<term>Information Interfaces and Presentation (e</term>
					<term>g</term>
					<term>HCI): Miscellaneous</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thumb-to-fingers interfaces augment touch widgets on fingers, which are manipulated by the thumb. Such interfaces are ideal for one-handed eyes-free input since touch widgets on the fingers enable easy access by the stylus thumb. This study presents DigitSpace, a thumb-to-fingers interface that addresses two ergonomic factors: hand anatomy and touch precision. Hand anatomy restricts possible movements of a thumb, which further influences the physical comfort during the interactions. Touch precision is a human factor that determines how precisely users can manipulate touch widgets set on fingers, which determines effective layouts of the widgets. Buttons and touchpads were considered in our studies to enable discrete and continuous input in an eyes-free manner. The first study explores the regions of fingers where the interactions can be comfortably performed. According to the comfort regions, the second and third studies explore effective layouts for button and touchpad widgets. The experimental results indicate that participants could discriminate at least 16 buttons on their fingers. For touchpad, participants were asked to perform unistrokes. Our results revealed that since individual participant performed a coherent writing behavior, personalized $1 recognizers could offer 92% accuracy on a cross-finger touchpad. A series of design guidelines are proposed for designers, and a DigitSpace prototype that uses magnetic-tracking methods is demonstrated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Human hands are dexterous and sensitive, allowing people to perform well-coordinated thumb-to-fingers actions such as grasping or pinching. The proposed thumb-to-fingers interface leverages this ability by providing an input mechanism that augments touch widgets on fingers, which are manipulated by the thumb, such as pressing a button on a fingertip or drawing a circle gesture on a touchpad formed by multiple fingers. Because these interfaces can be performed with only one hand and provide clear tactile cues during manipulation, they are easy to use and ideal for one-handed eyes-free input.</p><p>To propose a thumb-to-fingers interface supporting effective one-handed and eyes-free interactions, this paper presents DigitSpace, a thumb-to-fingers interface considering two crucial factors, hand anatomy and touch precision. The hand anatomy constrains the regions of fingers that can be reached by the thumb, restricts the possible thumb movements <ref type="bibr" target="#b16">[17]</ref>, and impacts the physical comfort during the interactions. Touch precision is the human factor in how users interact with touch widgets on fingers. Touch precision also affects the design of an effective layout of the widgets.</p><p>this study investigated these considerations to derive design guidelines for thumb-to-fingers interfaces.</p><p>This study evaluated two touch widgets on fingers: buttons and touchpads, which enable discrete and continuous thumb movements, respectively. By considering hand anatomy, our first study aims to mark off the regions of fingers that can be accessed with acceptable physical comfort for the two types of widgets. The experimental results indicate that participants preferred to use their dominant hand and that the comfort regions are near the fingertips.</p><p>The rest two studies examine how users can precisely perform button taps and stroke gestures in an eyes-free manner. The results show that, according to the corresponding comfort regions, the participants were able to discriminate at least 16 buttons on fingers. As for touchpad widgets, participants were asked to perform unistroke letters. The experimental results reveal that the stroke paths were impacted by hand anatomy and personal writing behaviors. Therefore, Graffiti and generalized $1 recognizers have limited recognition rates. However, since individual participants performed coherent writing behavior, personalized $1 recognizer enabled 92% accuracy on a cross-finger touchpad. Based on the results, we proposed the proper widget layouts for buttons and touchpads and compile a series of design guidelines. Finally, a DigitSpace prototype was implemented as a proof of concept. The prototype embeds Hall effect sensor arrays on fingers and a permanent magnet on the thumb, to demonstrate the benefits to interactions.</p><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, (a) with the finger segments providing high physical comfort, DigitSpace enables users rapid manipulation of the touch widgets. On a smartwatch interface, users can also (b) enter a password by tapping a virtual number pad superimposed on the finger segments, (c) draw unistroke letters on the fingerpads to search a song, and (d) increase the volume of the song by a sliding gesture. All interactions can be performed in a one-handed, eyes-free manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>This study is related to on-body touch interface and specifically related to thumb-to-fingers touch input, empirical studies of on-body touch interfaces, and thumb anatomy for onehanded touchscreen input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On-Body Touch Interface</head><p>An on-body touch interface transforms human body into touch surface. Several approaches have been proposed to enable on-body touch input at different body locations. External or wearable cameras <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">30]</ref> allow users to turn their palms into touch surfaces. NailO <ref type="bibr" target="#b12">[13]</ref> used capacitive sensing to support on-nail finger swipe gestures. By analyzing the sound conducted through bones <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>, taps on the skin can be detected by acoustic sensors worn on the arm. PUB <ref type="bibr" target="#b17">[18]</ref> enables touch interaction on the forearm by using ultrasonic sensors on the wrist. SenSkin <ref type="bibr" target="#b23">[24]</ref> detects finger function, such as pull, push and pinch on the arm, by using photo-reflective sensing. Besides focusing on sensing techniques, Weigel et al. demonstrated skin gestures that can utilize input dimensions such as pulling, pressing and squeezing <ref type="bibr" target="#b31">[31]</ref>. The hand-to-face interaction system proposed in <ref type="bibr" target="#b28">[28]</ref> uses the cheeks for touch-based interaction in consideration of modern head-mounted displays (HMDs). Earput enables touch interaction by augmenting capacitive sensing arrays behind the ears <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thumb-to-Fingers Touch Input</head><p>Another line of research has proposed the use of thumbto-fingers actions for rapid digital interactions. Ubi-Finger placed button switches on the index finger so that objects in the environment could be selected with the thumb <ref type="bibr" target="#b29">[29]</ref>. Saponas et al. <ref type="bibr" target="#b27">[27]</ref> used electromyography (EMG) to sense the thumb tappings on different fingertips. WristFlex <ref type="bibr" target="#b6">[7]</ref> also detected the pinch gestures by analyzing pressure changes from wrist-worn pressure sensors. Other works have developed continuous input methods to enable fine control. Pinchwatch <ref type="bibr" target="#b19">[20]</ref> utilized the whole digits as a touch surface and enabled users to engage eyes-free micro-interactions by various motion gestures. Similarly, FingerPad <ref type="bibr" target="#b4">[5]</ref> used magnetic tracking for private and subtle 2D touch input on the index fingertip. TIMMi <ref type="bibr" target="#b33">[33]</ref> supported multi-modal input with a device fabricated from silicone rubber and conductive threads, which is worn on the finger. Although these works are highly related to ours, they mainly focused on evaluating sensing techniques and prototypes. The human factors that influence how users interact with touch widgets were rarely involved in their discussions, and thus will be the main focus in this paper.</p><p>Additionally, previous works on thumb-to-fingers interactions have mainly focused on supporting "gesture input" <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b6">7]</ref>, whereas this paper focuses on enabling functional UI elements (i.e. button or touchpad widgets) for performing "touch input". The design guidelines obtained from the quantitative and qualitative results are helpful for the research of on-body interfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Understanding On-Body Touch Interfaces</head><p>Empirical studies of on-body touch interfaces have proposed interface design guidelines for specific body parts. On palm, Gustafson et al. evaluates the effects of tactile cues from fingers for eyes-free palm-based interactions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Studies of interfaces for the face include Serrano et al., who explored suitable user-defined gestures on the face and evaluated their social acceptance <ref type="bibr" target="#b28">[28]</ref>. Studies of interfaces for the forearm include Weigel et al., who suggested that skin-specific input can include emotional commands for richer interaction <ref type="bibr" target="#b31">[31]</ref>, and PUB <ref type="bibr" target="#b17">[18]</ref> carried out a pointing study along the forearm to examine how many buttons can be distinguished by participants. As in PUB, our STUDY 2 focused on the layout design of button widgets on fingers. Furthermore, we explore the design of appropriate touchpad widgets in STUDY 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-Handed Touchscreen Input</head><p>While moving the thumb, our inborn hand anatomy constrains coordination between the thumb <ref type="bibr" target="#b16">[17]</ref> and the interaction spaces between the thumb and fingers <ref type="bibr" target="#b15">[16]</ref>. Studies of one-handed touchscreen input have shown that, when using the thumb for input, different on-screen targets have different</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracking Fingers</head><p>#chi4good, CHI 2016, San Jose, CA, USA physical comfort, completion time <ref type="bibr" target="#b13">[14]</ref> and even touch precision <ref type="bibr" target="#b24">[25]</ref>. Additionally, how the mobile device is gripped greatly impacts the "functional regions" of the thumb <ref type="bibr" target="#b1">[2]</ref>, resulting in some out-of-reach targets on the touchscreen. These studies imply that an effective and accessible thumbto-fingers interface must consider hand anatomy. Therefore, this study first examined the regions of the fingers that can be used to perform discrete and continuous thumb-to-fingers actions with acceptable physical comfort. The effects of human factors within the comfort regions were then investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STUDY 1: COMFORT REGIONS</head><p>Among all the fingers, the thumb is extraordinary owing to its great capability to tap on the other fingers. However, the hand anatomy restricts the thumb's reachable areas on fingers and affects the physical comfort during the thumb-to-fingers actions.</p><p>This study considered hand anatomy in an analysis of the regions that can be comfortably accessed by the thumb. Within the comfort regions, users can use their fingers to manipulate touch widgets with minimal physical effort. The participants were asked to manipulate button widgets and touchpad widgets with each finger segment and to rank each action in terms of physical comfort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and Procedure</head><p>For button widgets, participants were asked to use the thumb to press a specified finger segment. As for touchpad widgets, participants needed to draw three primitive shapes clockwise and counterclockwise, respectively, including circle ( ), square ( ), and triangle ( ) on the assigned finger segment. These primitive shapes account for the possible thumb movement on a finger segment.</p><p>Before the trials, the participants were seated in front of a monitor showing a sketch of a hand. The experimenter explained the location of each finger segment on the sketch. In each trial, a finger segment was highlighted in green, indicating where to perform the assigned action, and assigned one type of the widgets. Participants were asked to manipulate the assigned widgets on the assigned finger segment. A randomized counterbalancing method was used to vary the finger segments, handedness, and widget type such that the handedness conditions were interleaved to avoid fatigue. The order of the assigned finger segments was also randomized.</p><p>After manipulating the assigned widgets on all finger segments of a hand, the participants were asked to rate the physical comfort of each segment on a 5-point Likert scale from 1 ("very uncomfortable") to 5 ("very comfortable"). Overall, each participant performed 2 (handedness) × 2 (actions) × 12 (segments) = 48 trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Eighteen right-handed participants were recruited for the experiments (8 females; age range, 21-30 years; mean age = 23.9 years; std = 1.78 years). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussions</head><p>Figure <ref type="figure">2</ref> shows the average comfort ratings given by the participants for all finger segments. We identified two factors impacting the ratings -handedness and thumb anatomy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Handedness</head><p>The handedness seems not to impact on the comfort ratings for the button taps. As shown in Figure <ref type="figure">2</ref>(a), the average scores of finger segments corresponding to the dominant and non-dominant hands are similarly distributed. For stroke gestures, however, participants tended to give higher scores for the dominant than for the non-dominant one (Figure <ref type="figure">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)).</head><p>A Wilcoxon test also showed that using dominant hand was more comfortable than using non-dominant hand (Z = -1.706, p &lt;.05). The post-study interviews also support the statistical analysis. Several participants noted that they had difficulty precisely applying directional thumb movement on the finger segments with the left hand. That is, hand dexterity affected the physical comfort during the actions. To observe the best performance of participants, the following studies focused on the dominant hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thumb Anatomy</head><p>For each hand, two tendencies were observed from the results: fingers farther away from the thumb and the inward finger segments were rated with fewer physical comfort. For button taps, the participants reported that they could easily reach the fingertips since only limited thumb movement was required. However, accessing other finger segments required more thumb movement and more rotation of the thumb joint, which increased muscle tension and physical effort. The discomfort becomes even worse when the participants need to maintain the muscle tension and further perform the gestures. Another Wilcoxon test also indicated that tapping was more comfortable than drawing stroke (Z = 3.10, p &lt;.01), matching our inference.</p><p>Tracking Fingers #chi4good, CHI 2016, San Jose, CA, USA</p><p>For an optimal thumb-to-fingers interface layout, only the finger segments with high physical comfort in thumb-tofingers interactions were analyzed in the following two studies, i.e., segments with scores higher than 3.0 as shown in Figure <ref type="figure">2</ref>(a) and 2(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>The experimental results show that, for right-handed users, the comfort regions for buttons are the first and second segments of the index, middle and ring fingers, and the first segment of the pinky finger on the right hand (Figure <ref type="figure">2</ref>(a)). On the other hand, the comfort regions for touchpads include the first and the second segments of the index and middle fingers (Figure <ref type="figure">2</ref>(b)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STUDY 2: BUTTON WIDGETS ON FINGERS</head><p>This study further elucidated discrete thumb-to-fingers actions by exploring how precisely (e.g., how many buttons) users could perform eyes-free tapping in the comfort regions of the operating fingers. We used the effective quantitative method proposed by PUB <ref type="bibr" target="#b17">[18]</ref>, which was originally designed for evaluating the performance of eyes-free touch on the forearm, to evaluate the performance of eyes-free thumb touch on other four fingers. Nonetheless, since designing touch interfaces in the thumb-to-fingers space has different ergonomic concerns with forearm, the additional factors (e.g., finger segments) obtained from STUDY 1 were also added in our experimental design.</p><p>Our pilot study with three participants confirmed that participants could easily discriminate two points on the first segment of the index finger, but no participants could discriminate more than four points. Therefore, the experiments were performed with the four layouts shown in Figure <ref type="figure">3</ref>. For each layout, each finger segment was assigned at least two and no more than four button widgets. Notably, the coincident buttons near the finger segment boundary were merged in some layouts. In these layouts, the maximal number of buttons on a finger was seven.</p><formula xml:id="formula_0">a b c d Figure 3</formula><p>. Different button layouts used in STUDY 2. The blue rectangle marks off the comfort regions from STUDY 1. Note that each finger segment contains no more than four buttons, according to the observation from our pilot study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head><p>Figure <ref type="figure">4</ref> illustrates the study environment. Each participant was instructed to sit in front of a table and to place the dominant hand in an acrylic case to occlude visual attention to the operating hand and to ensure that the task was performed eyes-free. The screen shows the task to be executed.</p><p>A LogiTech C100 camera (Figure <ref type="figure">4</ref>(b)) was positioned above the dominant hand to record where the participant tapped through a vision-based approach. The height of the camera was carefully adjusted so that the thumb and target finger could be captured at 140 dpi. An 8mm × 8mm plastic AR marker was adhered to the nail of the thumb used to perform the tap tasks. Additionally, AR markers were attached to first and second segments of each finger. Experimental results indicate that the tracking error of the developed vision-based approach is 1.2 mm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and Procedure</head><p>Before starting each layout, participants were asked to confirm that they understood the button layout displayed on a monitor. On the fingers, the points used to represent the buttons (Figure <ref type="figure">4</ref>(a)) were initially marked in green. For every trial, a random button was marked red to prompt the participants to tap the corresponding target on their fingers. The participants could press a space key on the keyboard to confirm the thumb position. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis</head><p>According to the thumb anatomy and the results observed in STUDY 1, the hypotheses are set as below:</p><p>• H1: Average accuracy will decrease as the number of buttons placed on the fingers increases since more buttons require more attention on skin sensation and subtle thumb control.</p><p>• H2: Average accuracy will decrease from the index, middle fingers to ring finger since the diagonal movement imposes extra loads on the thumb.</p><p>• H3: Average accuracy will be higher for the first segment than for the second segment since the inward movement of the thumb costs higher physical efforts than the outward movement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussions Examining Hypotheses</head><p>We first examined our hypotheses by comparing the accuracy of different button layouts. The accuracy was calculated as the number of non-overlapping data points divides by the number of total data points of each button. Non-overlapping data points refer to those located outside the 95% confidence interval of other buttons. Figure <ref type="figure">5</ref> shows broken-line graphs of the accuracy rates for each layout on each finger. The H1 and H2 were tested by a repeated measures two-way ANOVA with "button layouts" and "fingers" as independent variables. The ANOVA results showed a significant effect of button layouts (F 1.81,23.54 = 26.78, p &lt;.001) and no significant effect of fingers (F 3,39 = 1.69, p &gt;.05). The interaction was not significant (F 9,117 = 1.37, p &gt;.05).</p><p>For H1, the significant result on button layouts suggests that the decreasing accuracy with increasing number of buttons. We thus infer that participants tend to be confused when the number of button positions is too large, which supports the reliability of H1.</p><p>Notably, tests of H2 showed that the accuracy of button tapping did not significantly differ between fingers. Therefore, Tracking Fingers #chi4good, CHI 2016, San Jose, CA, USA we infer that the diagonal movements of the thumb do not significantly affect accuracy.</p><p>For H3, a repeated measures two-way ANOVA test with "button layouts" and "segments" as independent variables showed significant effects of both independent variables (F 2.05,26.69 = 21.22, p &lt;.001; F 1,13 = 25.57, p &lt;.001) with a significant interaction (F 1.7,22.8 = 16.81, p &lt;.001). Pairwise t-tests with Bonferroni corrections indicated that, in both the 6-and 7-button layouts, accuracy significantly differed between segments (both p &lt;.001). Although not all buttons pass the t-test, the results still exhibit the tendency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalized Button Layout</head><p>We next identified layouts that could be correctly discriminated by all participants. The experimental results showed that each participant could recognize at most five buttons on the index and middle fingers, four buttons on the ring finger, and two buttons on the pinky finger. Other results also suggest that, with more buttons placed on fingers, fewer participants could discriminate the buttons without ambiguity. For example, Figures <ref type="figure">6(e-f</ref>) further show the results for the ring finger in the 5-button layout and for the pinky finger in the 3-button layout. The black boxes mark off the overlapped tappings. According to the rest results, the observation can also be applied to other fingers.</p><p>The experimental results suggest that the generalized layout design is 5, 5, 4, 2 buttons for the index, middle, ring, and pinky fingers, respectively, resulting in 16 buttons in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Uniform Layout Design</head><p>The experiment showed that the first finger segment achieves higher accuracy than the second one, which indicated that a non-uniform arrangement of buttons was possible. Figure <ref type="figure">7</ref> illustrates the reasons of the alternative design. For the 6button and 7-button layouts on the index finger, at least eleven out of the fourteen participants could discriminate all buttons on the first finger segments. For each layout, however, the numbers of participants who could perfectly discriminate the buttons on the second finger segments decreased to four and three. The observation suggests that for the participants who can perceive the thumb location more precisely, non-uniform button layouts might provide a larger function set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STUDY 3: STROKE GESTURES ON FINGERS</head><p>STUDY 1 indicates that only the first two segments of the index and middle finger afford acceptable physical comfort for manipulating touchpad widgets. This study explored the optimal layout for touchpads for stroke gestures by considering two design factors: input area and tactile cues. The objective of the experiments was to improve designs for continuous thumb-to-fingers touch interface.</p><p>Input Area. Previous works have designed their interfaces based on various sizes of input areas on fingers. For example, Loclair et al. <ref type="bibr" target="#b19">[20]</ref> proposed PinchWatch that transfers the entire palm into a touch surface, and Chan et al. <ref type="bibr" target="#b4">[5]</ref> proposed to use only the index finger as a touchpad. We further investigated whether the input area affects the recognition rate of eyes-free stroke gestures since areas with different sizes require different thumb movements. To avoid ambiguity, during the study, participants were asked to rest their arms on a table in a frame of reference fixed on a horizontal plane.</p><p>Tactile Cues From Finger pads. Previous studies have shown that, because of the widespread use of modern mobile devices, users can perform precise stroke gestures on touchscreens with little visual attention <ref type="bibr" target="#b2">[3]</ref>. Unlike the surface of a smartphone, the surfaces of fingers are soft, uneven, and coarse, which might interfere the thumb's movements when performing the stroke gestures. However, the fingers can use tactile cues (i.e., skin sensation) to locate the position of the thumb. To determine the extent to which combined factors (e.g., the organic surface and tactile cues of the fingers) affect stroke gestures performed by users, we included a non-tactile condition where the participants' touchpad finger pads were covered with a solid plate to remove the softness and block the tactile cues of the fingers. Notably, this non-tactile condition was added to clarify our concerns, but not to propose adding a physical touchpad cover on the fingers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interface Conditions</head><p>Considering the above design factors, the conditions of the within-subject study were as follows:</p><p>• For the Index condition, gestures were performed with the first segment of the index finger, representing smaller input region with tactile cues.</p><p>• For the Index + Middle condition, the first segments of the index and middle fingers were aligned, and strokes in the integral space were used to represent larger input region with tactile cues.</p><p>• For the Cover condition, we covered participants' fingers with a printed board (40mm × 30mm × 1.5mm) to block the tactile feedback provided by the fingers, representing larger input region without tactile cues.</p><p>Graffiti gestures<ref type="foot" target="#foot_0">1</ref> were used as tasks since they were originally designed for eyes-free stylus input on a limited-sized touch panel, similar to our conditions. Six Graffiti letters were selected as shown in Figure <ref type="figure" target="#fig_4">8</ref>. According to previous works <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b22">23]</ref>, the selected letters are difficult to perform correctly without visual clues. Performing them correctly requires attention and precise thumb operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head><p>The study environment was identical to that in STUDY 2 except a Point Grey Grasshopper3 High-Speed Camera (up to 162 fps) was used to capture clear images during rapid thumb movements. We attached a pair of printed thin (2.0mm) board with AR marker to the participants' fingers and thumb. The board attached behind fingers estimated the 3D location and orientation of fingers, and the board attached on the thumbnail estimated the 3D location of thumb. The system continued to record at 150 frames per second and captured the markers at over 200 dpi with 1.02 mm errors. Thus, accuracy was similar to that of the Vicon system used in <ref type="bibr" target="#b30">[30]</ref>. The data points were reliably interpolated because of the highspeed tracking, and the marker loss occurs in less than 5% of frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and Procedure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training on Graffiti Letters</head><p>Before the study, participants were trained to write Graffiti letters. Participants were instructed to hold an Apple iPod Touch with their dominant hand. For each trial, a Graffiti letter was displayed and participants were asked to draw the letter inside a rectangle nearby the thumb. Small and large rectangles were used to simulate small and large input areas, respectively. The training session ended when the participant correctly performed each Graffiti letter five times in succession. The training process was accelerated by allowing the participants to review the screen to confirm the stroke path. On average, each participant took 10 minutes to learn Graffiti letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study Design</head><p>For each trial, a letter was displayed on the monitor. Participants were asked to use the thumb to draw the letter eyes-free.</p><p>The participant pressed the space bar on the keyboard by their non-dominant hands to inform our system the timestamp of the beginning and the end of the thumb movements.</p><p>Each participant wrote the six selected Graffiti letters with assigned interface condition and repeated the process eight times, resulting in 3 (conditions) × 6 (letters) × 8 (repetitions) = 144 trials. Both of the three interface conditions and the order of the letters were counterbalanced. If the participant wrote an incorrect or unrecognizable Graffiti letter, the system recorded the error and prompted the participant to begin the next trial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Twelve right-handed participants were recruited (6 females, age range, 21-27 years, mean age = 23.5 years, std = 2.2 years).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussions</head><p>First, the Graffiti recognizer described in <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b22">23]</ref> was used to calculate the accuracy of each participant under each condition. The result is shown in the left bar group of Figure <ref type="figure" target="#fig_6">10</ref>. Unexpectedly, the recognition rate was too low for practical use in all conditions; None of the conditions achieved the accuracy more than 61%, indicating that all participants could hardly draw the letters that could be successfully identified by the recognizer. The results imply that the original Graffiti recognizer is unsuitable for thumb-to-fingers stroke input. Possible reasons and potential solutions are given in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graffiti Recognizer</head><p>Overall recognition rate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis for Low Recognition Rate</head><p>The reasons for the low recognition rate were explored by visualizing all strokes by all participants, and further collected feedback from all of the participants in post-study interviews. The findings were categorized as follows. ). Since the original Graffiti recognizer applies template matching to the stroke paths with a pre-defined gesture set composed of straight lines and smooth curves, the rotated and distorted stroke paths often caused template mismatches, which reduced recognition rates. Notably, the participants were unaware of their crooked thumb movements. Four of the participants reported that "I thought that I drew a vertical line since I swiped my thumb up", suggesting that the inborn hand anatomy actually affects the potential direction of planar movement by the thumb since the joints of the thumbs are actually for high degree-of-freedom 3D rotations rather than 2D movements <ref type="bibr" target="#b16">[17]</ref>.</p><p>Misaligned Stroke Paths. According to <ref type="bibr" target="#b22">[23]</ref>, the challenge of performing the selected Graffiti letters was the alignment of the stroke. For example, the Graffiti letter "D" drawn by participants was often misrecognized as "P" since the lack of visual feedback often caused the participants to over-or under-estimate the where the stroke path should be joined. However, as shown in the Figure <ref type="figure" target="#fig_7">11</ref>(c), the key points were usually misaligned, making the stroke paths could not be correctly identified by the recognizer, thus further decreasthe accuracies. The misalignments also imply that the participants did not use tactile cues to perform the strokes. Possible explanations are discussed further below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Tactile Cues and Input Areas</head><p>Notably, a one-way within-subjects ANOVA analysis showed that input conditions did not significantly affect recognition rate (F (1.609, 22.592) = 1.606, p = .22), suggesting that both tactile cues and input areas do not affect the accuracy. For the tactile cues, eight of the participants reported that the tactile cues on fingers were not used since they "simply migrate their writing experience from smartphones to fingers". Two participants also reported that they "relied on the muscle memories learned from the training session and tried to reconstruct the stroke paths as quickly as possible. Therefore, tactile cues were not used to perform the actions". These feedback also implies the reason of the misalignments of the strokes.</p><p>As for the input areas, although the result shows no difference on the recognition rates, the Index + Middle and the Cover conditions were frequently reported more preferred since "larger input areas enables larger thumb movements, thus providing more physical comfort" reported by two participants. We therefore asked the participants to rate their preferences on using the three interfaces, following the seven-Likert scale.</p><p>The results also showed that compared to the Index condition (mean = 3.9, std = 1.1), the participants generally preferred Index + Middle (mean = 4.9, std = 1.5) and Cover (mean = 5.1, std = 1.7) conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Alternative Solution: Customized Recognizer</head><p>Based on above experimental results, we concluded that the original templates of Graffiti letters are unsuitable for thumb-to-fingers interfaces owing to the hand anatomy. Therefore, a recognizer suitable for the thumb-to-fingers interfaces is needed. Finally, the $1 recognizer <ref type="bibr" target="#b32">[32]</ref> was chosen as the recognition mechanism since it has demonstrated good performance and requires only few templates, allowing us to reuse our data.</p><p>First, a general $1 model was trained by using the strokes paths drawn by all participants. A 6-fold cross validation procedure (in which each fold contains the data from 2 participants) was then performed to evaluate the average recognition rates of all conditions. However, as shown in the middle bar group of Figure <ref type="figure" target="#fig_6">10</ref>, the improvement was limited, which suggests that the differences in writing behaviors were too large. Figure <ref type="figure" target="#fig_8">12</ref> is an example that supports this inference. One can see that although most participants were able to draw the same character similarly, the stroke paths varied drastically between different subjects. Therefore, a customized classifier was constructed for each participant. In 4-fold cross validation, the classifiers boosted accuracies to 84.9%, 92.7%, and 88.9% for Index, Index + Middle, and Cover conditions, respectively. These results suggest that, for providing a reliable stroke input mechanism, a personalized recognizer might be more effective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Based on the survey and feedback, we concluded that the hand anatomy affects the stroke paths drawn by the participants and that tactile cues were not actually used. The writing behaviors between each participant are diverse, implying that a general classifier requires deeper understandings of different writing behaviors between participants. For a reliable recognition of stroke gestures, the interface could provide a customized classification. Additionally, we suggest the use of fingerpads for the index and middle fingers to increase the space for free-form writing and better physical comfort, according to the participants' feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTERFACE DESIGN CONSIDERATIONS</head><p>Based on the experimental results, the following design guidelines are proposed for effective thumb-to-fingers interfaces.</p><p>Hand Anatomy and Comfort Regions. Our experiments showed that hand anatomy not only affects the physical comfort but also the precision of button taps and even stroke gestures. To allow users to manipulate the touch widgets com-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracking Fingers</head><p>#chi4good, CHI 2016, San Jose, CA, USA fortably and precisely, only the finger segments suggested from STUDY 1 are considered functional regions.</p><p>Tactile Feedback. STUDY 2 shows that tactile cues enable discrimination of at least 16 on-finger buttons. Additionally, according to <ref type="bibr" target="#b9">[10]</ref>, landmarks on the fingers provide users with clear tactile cues as reference points. Therefore, tactile feedback to the fingers should be preserved to enable rich shortcuts.</p><p>Cross-Finger Touchpad. Although STUDY 3 showed no significant difference in recognition rates between different sizes of input areas, feedback from participant suggests that large input areas increase physical comfort. Considering the comfort regions from STUDY 1, we proposed a cross-finger pad on the first segments of the index and middle fingers.</p><p>Customized Button Layout and Gesture Recognizer. Both STUDY 2 and STUDY 3 showed that customized button layouts and stroke gesture recognizers further increase the number of buttons and the recognition rate of the gestures. Similar suggestions were also proposed by <ref type="bibr" target="#b17">[18]</ref>. Thus, an effective thumb-to-fingers interface should enable users to customize the button layouts or the gesture commands for optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROTOTYPE IMPLEMENTATION</head><p>This section describes the implementation a prototype used to demonstrate the proposed interface design considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardware Design Consideration</head><p>Glove-based solutions were excluded to preserve tactile cues from fingers. Some studies have proposed using wearable low-fi sensors and machine learning methods to detect freehand gestures <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b6">7]</ref>. However, they could only recognize discrete gestures rather than continuous interactions, thus also excluded from our options. Some vision-based solutions have been proposed, such as mounting a camera on the body <ref type="bibr" target="#b19">[20]</ref>, the wrist <ref type="bibr" target="#b26">[26]</ref>, or a finger ring <ref type="bibr" target="#b3">[4]</ref>. Although these devices enable discrete or continuous touch input, they require complex computation, which increases power consumption.</p><p>We finally adopted a magnetic tracking method for our implementation, inspired by <ref type="bibr" target="#b4">[5]</ref> that augments Hall-sensor grid on the nail of the index finger and senses the 2D movement of a magnet on the thumb nail. A similar hardware configuration and calibration process were used in the proposed Dig-itSpace prototypes shown in Figure <ref type="figure" target="#fig_9">13</ref>, whose form is similar to the nail-ring chains (Figure <ref type="figure" target="#fig_9">13(d)</ref>), a type of finger-worn accessories. The Hall-sensor grid can detect the thumb's position with simple two-step calibration. First, background subtraction is performed to filter out noise. Second, placing the thumb on each finger, and move the thumb along the finger back and forth. Thus, the touch threshold is defined by the intensity values at the corresponding touch positions.</p><p>Notably, there are other proposed magnetic tracking solutions monitoring the signal changes of a magnetometer <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref> or computing the magnet's 3D position by using two magnetometers <ref type="bibr" target="#b5">[6]</ref>. However, the high degree of freedom in the fingers limits the effectiveness of magnetometer-based solutions for detecting touch events or increases the need for calibration. In comparison, the Hall-sensor grid solution directly transform the fingers into touch-sensitive surfaces, making the touch events easily be detected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardware</head><p>The DigitSpace prototype includes a pair of magnetic-sensing nail-ring chains, each of which consists of two parts, Nail (Figure <ref type="figure" target="#fig_9">13</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracking Algorithm</head><p>To decide the thumb's location on the fingers, the system firstly searches which Hall effect sensor on the Nails and Chains containing the highest magnitude. If the highest sensor value exceeds a threshold, the system then applies the tracking algorithm to the corresponding part.</p><p>For 2D tracking on the Nails, the calibration process and the tracking algorithm are identical to those in <ref type="bibr" target="#b4">[5]</ref>. For absolute 1D tracking of the two parts, the system applies 1D cubic interpolation to the sensor array and identifies the 1D position based on where magnetic intensity is maximal.</p><p>An evaluation was performed to measure absolute 1D tracking capability. Three points were sampled on the prototype, Tracking Fingers #chi4good, CHI 2016, San Jose, CA, USA and magnets were placed above the prototype at heights of 7, 9, 11, 13, and 15 mm. Different heights represent different finger thicknesses. The three sample points are the very first, the middle and the last Hall sensors on a Nail + Chain grid array. Each sample point for each height was examined ten times. The differences between the measured positions and the actual positions of the magnet were recorded. The experimental results lead to an average error of 1.98mm, which is sufficient to support a 5-button layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications</head><p>Based on the functions provided by the DigitSpace prototype, we propose using DigitSpace as a highly available smartwatch controller. The utility of DigitSpace was demonstrated by implementing the button, slider, and touchpad widgets shown in Figure <ref type="figure" target="#fig_0">1</ref>. A user can easily type numbers or trigger a desired function by pressing a certain position on a finger. The user enters gesture mode by simply aligning the first segments of the index and middle fingers and then quickly double tapping on one of the fingerpads. After entering the mode, the user can either perform a cross-finger stroke gestures for text input or slide his thumb along a finger to control a slider bar.</p><p>A user who is familiar with the interface can manipulate the smartwatch in an eyes-free manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitation</head><p>The limitation of the current prototype, which was inherited from STUDY 3, is that users are required to align their index and middle fingers to perform stroke gestures. Preliminary user feedback indicates that separating the fingers would increase physical comfort, which might also affect the stroke paths owing to the hand anatomy. We will address these issues in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION AND FUTURE WORK</head><p>This paper presents DigitSpace, a thumb-to-fingers interface that considers hand anatomy and touch precision. Our first study identified the regions of fingers where thumb-to-fingers touch interactions can be comfortably performed. After identifying the comfort regions, the second and the third studies further explored how to arrange two general touch widgets, buttons and touchpads, for effective discrete and continuous touch input. The study results reveal several factors that should be considered in designs for thumb-to-fingers touch interactions. We then developed a DigitSpace prototype, a wearable device based on magnetic tracking to enable thumb-to-fingers interactions and its applications. As for future work, we have seen that allowing users to open their fingers while performing thumb-to-fingers interaction will increase physical comfort. We consider to investigate whether this increase in comfort also permits a boost in user performance. Sensing techniques for supporting this feature will also be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) One-handed, eyes-free interaction can be realized more accurately and effectively within the comfort regions. (b)-(d) Examples of using a thumb-to-fingers touch interface following the design guidelines derived from the results of our studies.</figDesc><graphic coords="1,321.33,219.89,243.07,181.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 (Figure 4 .</head><label>44</label><figDesc>Figure4(c) shows that, for each trial, the system captures an image and calculates the 3D positions of the AR markers mounted on the hand of the participant. To infer the position of the thumb in relation to a finger, the system first computes a 3D vector representing the finger by the two AR markers on the finger, and then projects the 3D position of the AR marker on the thumb onto the vector. The projected point on the vector was regarded as an estimation of the thumb's position on the target finger. The system then records the spatial information of the vectors and the projected position and prompts the participant to begin the next trial.</figDesc><graphic coords="4,319.93,299.63,98.51,73.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .Figure 6 .Figure 7 .</head><label>567</label><figDesc>Figure 5. Broken-line graphs that present the accuracies of different button layouts for (a) index, (b) middle (c) ring, (d) pinky fingers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figures 6(a) to 6(d) show the results. The thicker boxes represent the coverage area of a standard deviation, and the flatter boxes represent the coverage area of two standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The Graffiti letters used in STUDY 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. The apparatus of STUDY 3, a participant (a) followed the visual instruction and (b) performed the unistroke gestures in a onehanded and eyes-free manner. The experiment conditions include (c) Index finger, (d) Index + Middle fingers, and (e) Cover conditions.</figDesc><graphic coords="7,179.21,299.24,134.84,89.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. The overall accuracy of recognizing the selected Graffiti letters by using different recognizers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Some examples of the stroke paths performed by the participants. The letters in the grey boxes represent the recognition results. The asterisk sign stands for "unknown".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. The letter "G" performed by the twelve participants in the Index + Middle condition. Note that the initial points of each stroke path were aligned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. The DigitSpace prototype, consisting of the (a) nail part and (b) chain part for sensing a (c) neodymium magnet. The sensor signals were read by a Teensy 2.0 microprocessor. The form of our prototype was inspired by (d) a nail-ring chain.</figDesc><graphic coords="9,315.45,102.30,260.21,194.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a)) and Chain (Figure13(b)). The Nail comprises a 3×3 Winson WSH138 Hall sensor grid, and the Chain contains an 1×8 Hall sensor stripes, which support continuous 2D touch input and discrete button taps, respectively. The sensors, which are separated by 4mm in the Nail part and 5mm in the Chain part, are used to detect a neodymium magnet (Figure13(c)) Each sensor element detects the intensities of both N-and S-polar magnetic field components from 0 to 200 Gauss on a 512-point scale and at a sampling rate that consistently exceeds 120 fps. We fabricated a nail-ring chain by mounting the two parts on a NinjaFlex stripe. To enable the prototype to be worn by users, we also printed a nail-shaped plate and a ring and attached them below the Nail and the end of the Chain. A Teensy 2.0 microprocessor reads the sensor values from the two nail-ring chains and transmits the data to a laptop PC to compute the 2D coordinates of the magnet mounted on the thumb nail.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://en.wikipedia.org/wiki/Graffiti_(Palm_OS)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We gratefully acknowledge the helpful comments and suggestions of the Associate Chairs and the anonymous reviewers. We also thank Yi-Ling Chen and Chin-Yu Chien for their valuable comments. This work was partly supported by Ministry of Science and Technology and MediaTek Inc. under Grants MOST104-2622-8-002-002, MOST104-2627-E-002-001, and MOST103-2218-E-002-024-MY3.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nenya: Subtle and Eyes-free Mobile Input with a Magnetically-tracked Finger Ring</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ashbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1979238</idno>
		<ptr target="http://dx.doi.org/10.1145/1978942.1979238" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;11)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2043" to="2046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling the Functional Area of the Thumb on Mobile Touchscreen Surfaces</title>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Bergstrom-Lehtovirta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Oulasvirta</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2557354</idno>
		<ptr target="http://dx.doi.org/10.1145/2556288.2557354" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)</title>
		<meeting>the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991">2014. 1991-2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Experimental Analysis of Touch-screen Gesture Designs in Mobile Environments</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bragdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Hinckley</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1979000</idno>
		<ptr target="http://dx.doi.org/10.1145/1978942.1979000" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;11)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CyclopsRing: Enabling Whole-Hand and Context-Aware Interactions Through a Fisheye Ring</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Hao</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong-Hao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing-Yu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2807442.2807450</idno>
		<ptr target="http://dx.doi.org/10.1145/2807442.2807450" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;15)</title>
		<meeting>the 28th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="549" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FingerPad: Private and Subtle Interaction Using Fingertips</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong-Hao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chang</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Yin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Huai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing-Yu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2501988.2502016</idno>
		<ptr target="http://dx.doi.org/10.1145/2501988.2502016" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;13)</title>
		<meeting>the 26th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="255" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">uTrack: 3D Input Using Two Magnetic Sensors</title>
		<author>
			<persName><forename type="first">Ke-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kent</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shwetak</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2501988.2502035</idno>
		<ptr target="http://dx.doi.org/10.1145/2501988.2502035" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;13)</title>
		<meeting>the 26th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="237" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">WristFlex: Low-power Gesture Input with Wrist-worn Pressure Sensors</title>
		<author>
			<persName><forename type="first">Artem</forename><surname>Dementyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Paradiso</surname></persName>
		</author>
		<idno type="DOI">10.1145/2642918.2647396</idno>
		<ptr target="http://dx.doi.org/10.1145/2642918.2647396" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;14)</title>
		<meeting>the 27th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="161" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imaginary Palm-based Remote Control for Eyes-free Television Interaction</title>
		<author>
			<persName><forename type="first">Niloofar</forename><surname>Dezfuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Khalilbeigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochen</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Mühlhäuser</surname></persName>
		</author>
		<idno type="DOI">10.1145/2325616.2325623</idno>
		<ptr target="http://dx.doi.org/10.1145/2325616.2325623" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Interactive Tv and Video (EuroiTV &apos;12)</title>
		<meeting>the 10th European Conference on Interactive Tv and Video (EuroiTV &apos;12)<address><addrLine>San Jose, CA, USA PalmRC; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012. 2016</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
	<note>Tracking Fingers #chi4good</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imaginary Phone: Learning Imaginary Interfaces by Transferring Spatial Memory from a Familiar Device</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Baudisch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2047196.2047233</idno>
		<ptr target="http://dx.doi.org/10.1145/2047196.2047233" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;11)</title>
		<meeting>the 24th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding Palm-based Imaginary Interfaces: The Role of Visual and Tactile Cues when Browsing</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">G</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">M</forename><surname>Baudisch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2466114</idno>
		<ptr target="http://dx.doi.org/10.1145/2470654.2466114" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;13)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">OmniTouch: Wearable Multitouch Interaction Everywhere</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrvoje</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2047196.2047255</idno>
		<ptr target="http://dx.doi.org/10.1145/2047196.2047255" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;11)</title>
		<meeting>the 24th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skinput: Appropriating the Body As an Input Surface</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desney</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.1145/1753326.1753394</idno>
		<ptr target="http://dx.doi.org/10.1145/1753326.1753394" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;10)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NailO: Fingernails As an Input Surface</title>
		<author>
			<persName><forename type="first">Hsin-Liu ; Cindy)</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Dementyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Paradiso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Schmandt</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702572</idno>
		<ptr target="http://dx.doi.org/10.1145/2702123.2702572" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3015" to="3018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Studies in One-Handed Mobile Design: Habit, Desire and Agility</title>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">K</forename><surname>Karlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
		<idno>HCIL-2006-02</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MagiWrite: Towards Touchless Digit Entry Using 3D Space Around Mobile Devices</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Ketabdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Roshandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamer</forename><forename type="middle">Ali</forename><surname>Yüksel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1851600.1851701</idno>
		<ptr target="http://dx.doi.org/10.1145/1851600.1851701" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Human Computer Interaction with Mobile Devices and Services (MobileHCI &apos;10)</title>
		<meeting>the 12th International Conference on Human Computer Interaction with Mobile Devices and Services (MobileHCI &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="443" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Functional workspace for precision manipulation between thumb and fingers in normal hands</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-N</forename><surname>Sun</surname></persName>
		</author>
		<editor>J. Electromyogr. Kines</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="829" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Coordination of thumb joints during opposition</title>
		<author>
			<persName><forename type="first">Z.-M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<editor>J. Electromyogr. Kines</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="502" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pub -Point Upon Body: Exploring Eyes-free Interaction and Methods on an Arm</title>
		<author>
			<persName><forename type="first">Shu-Yang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Huai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Yin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong-Hao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Hao</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing-Yu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2047196.2047259</idno>
		<ptr target="http://dx.doi.org/10.1145/2047196.2047259" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;11)</title>
		<meeting>the 24th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="481" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">EarPut: Augmenting Ear-worn Devices for Ear-based Interaction</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Lissermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochen</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristotelis</forename><surname>Hadjakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suranga</forename><surname>Nanayakkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Mühlhäuser</surname></persName>
		</author>
		<idno type="DOI">10.1145/2686612.2686655</idno>
		<ptr target="http://dx.doi.org/10.1145/2686612.2686655" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design</title>
		<meeting>the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="300" to="307" />
		</imprint>
	</monogr>
	<note>OzCHI &apos;14</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PinchWatch: A Wearable Devices for One-Handed Microinteractions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Loclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Baudisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MobileHCI &apos;10 Workshop on Ensembles of On-Body Devices</title>
		<meeting>MobileHCI &apos;10 Workshop on Ensembles of On-Body Devices</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SixthSense: A Wearable Gestural Interface</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pattie</forename><surname>Maes</surname></persName>
		</author>
		<idno type="DOI">10.1145/1667146.1667160</idno>
		<ptr target="http://dx.doi.org/10.1145/1667146.1667160" />
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH ASIA 2009 Sketches (SIGGRAPH ASIA &apos;09)</title>
		<meeting><address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Sound of Touch: On-body Touch and Gesture Sensing Based on Transdermal Ultrasound Propagation</title>
		<author>
			<persName><forename type="first">Adiyan</forename><surname>Mujibiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desney</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shwetak</forename><forename type="middle">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
		</author>
		<idno type="DOI">10.1145/2512349.2512821</idno>
		<ptr target="http://dx.doi.org/10.1145/2512349.2512821" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;13)</title>
		<meeting>the 2013 ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Disappearing Mobile Devices</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Baudisch</surname></persName>
		</author>
		<idno type="DOI">10.1145/1622176.1622197</idno>
		<ptr target="http://dx.doi.org/10.1145/1622176.1622197" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd Annual ACM Symposium on User Interface Software and Technology (UIST &apos;09)</title>
		<meeting>the 22Nd Annual ACM Symposium on User Interface Software and Technology (UIST &apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SenSkin: Adapting Skin As a Soft Interface</title>
		<author>
			<persName><forename type="first">Masa</forename><surname>Ogata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Sugiura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasutoshi</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiko</forename><surname>Inami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michita</forename><surname>Imai</surname></persName>
		</author>
		<idno type="DOI">10.1145/2501988.2502039</idno>
		<ptr target="http://dx.doi.org/10.1145/2501988.2502039" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;13)</title>
		<meeting>the 26th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="539" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluating One Handed Thumb Tapping on Mobile Touchscreen Devices</title>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">B</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hourcade</forename></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1375714.1375725" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface 2008 (GI &apos;08)</title>
		<meeting>Graphics Interface 2008 (GI &apos;08)<address><addrLine>Toronto, Ont., Canada, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Canadian Information Processing Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m">Tracking Fingers #chi4good</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DigiTap: An Eyes-free VR/AR Symbolic Input Device</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Prätorius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitar</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Burgbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Hinrichs</surname></persName>
		</author>
		<idno type="DOI">10.1145/2671015.2671029</idno>
		<ptr target="http://dx.doi.org/10.1145/2671015.2671029" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM Symposium on Virtual Reality Software and Technology (VRST &apos;14)</title>
		<meeting>the 20th ACM Symposium on Virtual Reality Software and Technology (VRST &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="9" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enabling Always-available Input with Muscle-computer Interfaces</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Scott</forename><surname>Saponas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desney</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravin</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<idno type="DOI">10.1145/1622176.1622208</idno>
		<ptr target="http://dx.doi.org/10.1145/1622176.1622208" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd Annual ACM Symposium on User Interface Software and Technology (UIST &apos;09)</title>
		<meeting>the 22Nd Annual ACM Symposium on User Interface Software and Technology (UIST &apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the Use of Hand-to-face Input for Interacting with Head-worn Displays</title>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barrett</forename><forename type="middle">M</forename><surname>Ens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pourang</forename><forename type="middle">P</forename><surname>Irani</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2556984</idno>
		<ptr target="http://dx.doi.org/10.1145/2556288.2556984" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)</title>
		<meeting>the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3181" to="3190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ubi-finger: A simple gesture input device for mobile and ubiquitous environment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName><surname>Yasamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Asian Information</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="111" to="120" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
	<note>Science and Life</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PalmGesture: Using Palms As Gesture Interfaces for Eyes-free Input</title>
		<author>
			<persName><forename type="first">Cheng-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Chieh</forename><surname>Hsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Tsung</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiao-Hui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2785830.2785885</idno>
		<ptr target="http://dx.doi.org/10.1145/2785830.2785885" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &apos;15)</title>
		<meeting>the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">More Than Touch: Understanding How People Use Skin As an Input Surface for Mobile Computing</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Weigel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Steimle</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2557239</idno>
		<ptr target="http://dx.doi.org/10.1145/2556288.2557239" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;14)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gestures Without Libraries, Toolkits or Training: A $1 Recognizer for User Interface Prototypes</title>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/1294211.1294238</idno>
		<ptr target="http://dx.doi.org/10.1145/1294211.1294238" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;07)</title>
		<meeting>the 20th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;07)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TIMMi: Finger-worn Textile Input Device with Multimodal Sensing in Mobile Interaction</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Ho Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Ramani</surname></persName>
		</author>
		<idno type="DOI">10.1145/2677199.2680560</idno>
		<ptr target="http://dx.doi.org/10.1145/2677199.2680560" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction (TEI &apos;15)</title>
		<meeting>the Ninth International Conference on Tangible, Embedded, and Embodied Interaction (TEI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="269" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m">Tracking Fingers #chi4good</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
