<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-resolution face recognition: a review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhifei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhenjiang</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Q</forename><forename type="middle">M Jonathan</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanli</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">Z</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Windsor</orgName>
								<address>
									<settlement>Windsor</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute of System Engineering and Control</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Low-resolution face recognition: a review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A97E74358B8458C50D3647C98A198492</idno>
					<idno type="DOI">10.1007/s00371-013-0861-x</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Review</term>
					<term>Face recognition</term>
					<term>Low-resolution</term>
					<term>Super-resolution</term>
					<term>Feature extraction</term>
					<term>Feature classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-resolution face recognition (LR FR) aims to recognize faces from small size or poor quality images with varying pose, illumination, expression, etc. It has received much attention with increasing demands for long distance surveillance applications, and extensive efforts have been made on LR FR research in recent years. However, many issues in LR FR are still unsolved, such as super-resolution (SR) for face recognition, resolution-robust features, unified feature spaces, and face detection at a distance, although many methods have been developed for that. This paper provides a comprehensive survey on these methods and discusses many related issues. First, it gives an overview on LR FR, including concept description, system architecture, and method categorization. Second, many representative methods are broadly reviewed and discussed. They are classified into two different categories, super-resolution for LR FR and resolution-robust feature representation for LR FR. Their strategies and advantages/disadvantages are elaborated. Some relevant issues such as databases and evaluations for LR FR are also presented. By generalizing their</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face recognition (FR) has been widely studied for decades due to its great potential applications. Many technologies focus on dealing with complex conditions such as aging, occlusion, disguise, and variations in pose, illumination, and expression <ref type="bibr" target="#b0">[1]</ref>. Although the recognition accuracy of face recognition in controlled environments with cooperative subjects is satisfactory, the performance in real applications such as surveillance is still an unsolved problem partially due to low-resolution (LR) image quality <ref type="bibr" target="#b1">[2]</ref>. With the growing installation of surveillance cameras in many places, there are increasing demands for face recognition in surveillance applications from small-scale stand-alone cameras in banks and supermarkets, to large-scale multiple networked close-circuit televisions in public streets <ref type="bibr" target="#b2">[3]</ref>. In such cases, subjects are far from cameras, and face regions tend to be small. This issue is called low-resolution face recognition (LR FR).</p><p>In this paper, LR FR aims to recognize faces from small size or poor quality images with varying pose, illumination, expression, etc. Traditional methods <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> based on highresolution (HR) face images could not perform well when face images have relatively LR. Compared with other noninvasive biometric authentication techniques such as gait recognition at a distance <ref type="bibr" target="#b9">[10]</ref>, LR FR is a more difficult task due to the variability of human facial features. The challenges associated with LR FR can be attributed to the following factors:</p><p>Misalignment Inaccurate alignment will severely affect the performance of HR as well as LR face recognition systems <ref type="bibr" target="#b10">[11]</ref> and it is difficult to perform automatic alignment on LR images. Noise affection As a generalized issue, LR problem causes a lot of chain effects for face recognition. The degradation in resolution together with pose, illumination, and expression variations adds complexity to the recognition <ref type="bibr" target="#b11">[12]</ref>. In other words, these variations produce much more noises for LR FR. Lack of effective features LR leads to the loss of large amounts of information. Most effective features used in HR FR such as Gabor <ref type="bibr" target="#b12">[13]</ref> and local binary pattern (LBP) <ref type="bibr" target="#b13">[14]</ref> may fail in LR case, especially very LR case, e.g., 6 × 6. Novel features insensitive to resolution are essential for LR FR. Dimensional mismatch Different resolutions between gallery images and probe ones in LR FR systems cause dimensional mismatch in traditional subspace learning methods <ref type="bibr" target="#b14">[15]</ref>.</p><p>Due to the challenges and significances for real applications, LR FR has gradually become an active research subarea of face recognition in recent years, and about 150 publications have reported its related contributions. Many promising methods have been proposed, such as multimodal tensor super-resolution (M 2 TSR) <ref type="bibr" target="#b11">[12]</ref>, simultaneous super-resolution and recognition (S 2 R 2 ) <ref type="bibr" target="#b15">[16]</ref>, discriminative super-resolution (DSR) <ref type="bibr" target="#b2">[3]</ref>, RQCr color features for degraded images <ref type="bibr" target="#b16">[17]</ref>, local frequency descriptor (LFD) <ref type="bibr" target="#b17">[18]</ref>, coupled locality preserving mappings (CLPMs) <ref type="bibr" target="#b18">[19]</ref>, multidimensional scaling (MDS) <ref type="bibr" target="#b19">[20]</ref>, and coupled kernel embedding (CKE) <ref type="bibr" target="#b20">[21]</ref>. These representative LR FR methods are listed in Table <ref type="table" target="#tab_0">1</ref>. Some comparisons between HR FR and LR FR, including advantages and disadvantages are summarized.</p><p>Generally, the straightforward way to solve LR FR problem is super-resolution (SR), which first reconstructs HR faces from several LR faces and then performs recognition with the super-resolved HR images. This kind of methods is steadily developed within the last decade. Since 2005, many researchers have studied simultaneous SR and recognition for including facial features into an SR method as prior information. In recent years, resolution-robust feature representation methods have been gradually considered. However, all of them are limited to different constraints and do not completely solve LR FR problem. Future researches are still necessary for some related problems, such as accurate alignment and unified feature spaces, toward ultimately reaching the goal of resolution-robust face recognition.</p><p>Several survey papers <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> and books <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> with good reviews on general face recognition have been published. S.Z. Li et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, and Wheeler et al. <ref type="bibr" target="#b32">[33]</ref> definitely proposed issues, challenges, and prospects of biometric sensing and recognition at a distance for practical system deployments. However, none of them made specific reviews on LR FR. Thus, the contribution of this paper is to make a comprehensive survey on LR FR with detailed reviews on the existing methods and provide discussions on some open issues within this area. As the topic has attracted researchers since the year 2000, this review generalizes the main contributions during the last decade. No review of this nature can possibly cite every paper that has been published; therefore, we include only what we believe to be representative samples of important works and broad trends from recent years.</p><p>The rest of this paper is organized as follows. Section 2 provides a brief overview on LR FR with introductions of concept description, system architecture, method categorization, etc. In Sects. 3 and 4, a detailed review on LR FR methods is presented from two aspects: Super-Resolution for LR FR (Sect. 3) and Resolution-Robust Feature Representation for LR FR (Sect. 4). Section 5 describes the performance evaluations on LR FR methods. Section 6 gives a discussion on existing problems and future trends. Section 7 concludes the review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview on LR FR</head><p>In this section, an overview on LR FR is given. First, the concept of LR FR is discussed after introducing two concepts: the best resolution and the minimal resolution. Then system architecture including the main strategies considered for LR FR is briefly described. Finally, LR FR methods are categorized, and some representative works are illustrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Concept descriptions</head><p>In an ideal scene, an HR face image with an abundance of pixels and details is important for recognition. That is to say, image resolution determines the capacity to discriminate fine features of image to a great extent. Before investigating LR FR, two concepts are introduced for describing the effects of image resolution on face recognition.</p><p>The best resolution, on which the optimal performance can be obtained with the perfect trade-off between recognition accuracy and performing speed. The problem of finding "what is the best spatial resolution for face recognition?" was originally raised by Kurita et al. <ref type="bibr" target="#b33">[34]</ref>. They observed that faces characterized by global features could often be more easily recognized in lower resolutions. Pyramidal data structure was used to represent image sets in different resolutions from LR to HR with magnification factors from 1 to 64, with the order of 2, and they found that their classifier did not perform the best at the highest resolution. This is a phenomenon worth special attention.</p><p>The minimal resolution, also called the threshold resolution, above which the performance remains steady with  <ref type="bibr" target="#b3">[4]</ref>, LDA <ref type="bibr" target="#b4">[5]</ref>, ICA <ref type="bibr" target="#b5">[6]</ref>, LPP <ref type="bibr" target="#b6">[7]</ref>, NPE <ref type="bibr" target="#b21">[22]</ref>, EBGM <ref type="bibr" target="#b12">[13]</ref>,</p><p>Bayesian FR <ref type="bibr" target="#b7">[8]</ref> M 2 TSR <ref type="bibr" target="#b11">[12]</ref>, S 2 R 2 <ref type="bibr" target="#b15">[16]</ref>, DSR <ref type="bibr" target="#b2">[3]</ref>, RQCr <ref type="bibr" target="#b16">[17]</ref>, LFD <ref type="bibr" target="#b17">[18]</ref>, CLPMs <ref type="bibr" target="#b18">[19]</ref>, MDS <ref type="bibr" target="#b19">[20]</ref>, CKE <ref type="bibr" target="#b20">[21]</ref> Advantages More information, less noises, higher robustness, more features for use Lower storing and computing costs Disadvantages Higher storing and computing costs Less information, more noises, fewer available methods and tools</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications</head><p>Entrance guard security, entertainment, etc. Long distance surveillance, law enforcement, etc.</p><p>the resolution decreasing from the best resolution, but below which the performance deteriorates rapidly. Wang et al. <ref type="bibr" target="#b34">[35]</ref> demonstrated that face image information could be divided into discriminative information (the individual information compared with other faces) and structure information (the common information of all face images under the same resolution). In addition, similar to the structure information, a new concept called face structural similarity was proposed in <ref type="bibr" target="#b35">[36]</ref>. In fact, all of them are from an interesting phenomenon in the real world. When observing a person moving toward us, what we see first is a person moving nearer, then the identity of the person when the distance exceeds a fixed value. Here, the fixed value is just the minimal resolution for the human visual system. In other words, we first obtain the structure information, and then the discriminative information as the resolution exceeds the minimal resolution.</p><p>Investigation of the minimal resolution can be found in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. Lemieux et al. <ref type="bibr" target="#b37">[38]</ref> studied principal component analysis (PCA) system with AR database, and observed that the minimal resolution of face size is about 21 × 16 pixels. Wang et al. <ref type="bibr" target="#b34">[35]</ref> explored PCA and linear discriminant analysis (LDA) system with AR database, and indicated the minimal resolution of the face size as 64 × 48 pixels. Boom et al. <ref type="bibr" target="#b36">[37]</ref> investigated PCA and LDA system with FRGC database, and found the minimal resolution of face size to be 32 × 32 pixels. Fookes et al. <ref type="bibr" target="#b38">[39]</ref> examined PCA and elastic bunch graph matching (EBGM) system with XM2VTS database, and obtained the minimal resolution of face size of about 42 × 32 pixels. From the above investigations, we can draw a conclusion that the minimal resolution depends on different methods and databases.</p><p>LR FR we address is automatically recognizing people by their face images in LR. An LR face image means that a face size is smaller than 32 × 24 pixels (with an eye-toeye distance about 10 pixels), typically taken by surveillance cameras (maximum resolution 320 × 240, QVGA) without subject's cooperation and normally contains noises and motion blurs. In an LR image, exact delineation of facial features is not so trivial both for humans and machines. Generally, LR images discussed here roughly fall into three (1) Small size, for which the probe images have the insufficient number of pixels. According to the reports of FRVT2000 <ref type="bibr" target="#b39">[40]</ref> on resolution experiment, the metric used to quantify resolution is eye-to-eye distance in pixels <ref type="bibr" target="#b40">[41]</ref>. Nevertheless, the distance is not usually adopted in most LR FR systems but replaced by face size. In the conventional methodology, small size is sufficient for face recognition. However, when the size of face captured from surveillance camera is smaller than 32 × 24 pixels, or the size of face down-sampled from HR static image is smaller than 16 × 16 and even 6 × 6 pixels, most conventional methods will be of no effect <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42]</ref>. (2) Poor quality, which is from the fact that probe images are provided in the resized form with blur (e.g., out of focus, interlace, and motion blur), variation of illumination and loss of details. Therefore, the underlying resolutions of the images are comparatively low <ref type="bibr" target="#b42">[43]</ref>, which means that even if the size of face is 200 × 200 pixels, there is no guarantee that the face is in HR. From the reports of MBGC2009 <ref type="bibr" target="#b43">[44]</ref>, the levels of "focus" and "illumination" are taken as image quality measures. However, they just consider LR problem from vision perspective rather than recognition purpose. (3) Small size &amp; Poor quality, which is from the combination of them naturally. In addition, Han et al. <ref type="bibr" target="#b44">[45]</ref> introduced a new form of LR, called low gray-scale resolution.</p><p>Fig. <ref type="figure">2</ref> The system architecture of LR FR</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">System architecture</head><p>Similar to conventional HR FR system, LR FR system also includes three main parts. That is LR face detection or tracking, LR feature extraction and LR feature classification, as illustrated in Fig. <ref type="figure">2</ref>. In general, the latter two are collectively referred to as LR FR, which is the focus in this review. LR face detection/tracking means that pre-processing, detection, tracking and segmentation of the faces are automatically performed on LR images or videos. State-of-the-art face detection methods such as AdaBoost <ref type="bibr" target="#b45">[46]</ref>, which are usually able to detect face images larger than 20 × 20, could fail in LR case. Therefore, building an efficient and effective face detection system is necessary for LR FR especially in surveillance applications. At present, two strategies are considered for this problem as follows.</p><p>The first is multicamera active vision systems, which are used for applications with large coverage areas and high detection efficiency <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>. Generally, they include two kinds of cameras. One is wide field of view (WFOV) cameras covering a large area to allow the detection and localization of subjects by a combination of motion detection, background modeling and skin detection. The other is narrow field of view (NFOV) cameras actively controlled to capture HR face images using pan, tilt, and zoom (PTZ) commands.</p><p>The second is to improve the existing HR face detection methods for solving LR problem. For example, Hayashi et al. <ref type="bibr" target="#b41">[42]</ref> proposed the first method to detect very LR faces. They adopted upper-body images and frequency-band limitations of features based on AdaBoost-based detector. The detection rate was improved from 39 % to 73 % on 6 × 6 with MIT+CMU frontal database. Recently, Zheng et al. <ref type="bibr" target="#b50">[51]</ref> developed a modified census transform technique by using boosting classifiers for detecting LR faces in color images.</p><p>Besides these two strategies, selection of the most suitable faces is also used for LR problem.</p><p>LR FR initially extracts resolution-robust features, and performs classification by matching the features to obtain the identity decision. The steps are similar to HR FR system from general framework. However, as opposed to HR FR, LR FR needs to consider the particular problem of dimensional mismatch. In practical face recognition applications, it is reasonable to assume that all gallery images are in HR. From classification perspective, LR will obviously Fig. <ref type="figure">3</ref> Three general ways used for LR FR cause the mismatch problem between gallery/probe pairs, as illustrated in Fig. <ref type="figure">3</ref>. To deal with the problem, three general ways can be considered as follows:</p><p>(1) Up-scaling (or interpolation), such as cubic interpolation, is conventionally adopted in most of the subspacebased face recognition methods. For LR images, it does not introduce any new information but potentially brings noises or artifacts. Therefore, the process of upscaling can be feasible under high-resolution or middleresolution, but may drop in performance confronting with much lower resolution. Thus, it is generally not a good way for solving LR problem. For further refined solution, super-resolution or hallucination <ref type="bibr" target="#b51">[52]</ref> can be employed to estimate HR faces from LR ones. However, it usually requires a lot of images, which belongs to the same scene with precise alignment, and it also needs large computation cost. (2) Unified feature space, also called inter-resolution (IR) space <ref type="bibr" target="#b18">[19]</ref>, is used to project HR gallery images and LR probe ones into a common space. This idea seems to be direct and reasonable for solving LR problem. However, it is difficult to find the optimal inter-resolution space.</p><p>And the two bidirectional transformations from both HR to IR and LR to IR may bring much more noises. (3) Down-scaling seems to be a feasible solution for the mismatch problem. Unfortunately, it reduces the amount of available information, especially the high-frequency information mainly for recognition. However, downscaling on both training/gallery and test/probe may improve the performance under very LR case such as 7 × 6 <ref type="bibr" target="#b2">[3]</ref>.</p><p>Here, we briefly summarize the three ways.</p><p>In the first way, most super-resolution methods are taken as indirect way. They firstly obtain super-resolved HR images from LR images and then for recognition.</p><p>The second way is to build unified feature space with optimal mapping techniques, or resolution-robust feature extraction techniques. They can be taken as direct way that performs on the original LR images.</p><p>Since the third way, i.e., down-scaling techniques are poor in performance for solving LR problem, and few researches focused on them, which are not our main considerations in this review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Categorization and representatives</head><p>Based on the above analyses, LR FR methods can generally be classified into two categories, (1) Indirect method: superresolution for LR FR, (2) Direct method: resolution-robust feature representation for LR FR. Of course, some methods may overlap category boundaries.</p><p>(1) Indirect method: super-resolution (SR) is initially used to synthesize the higher-resolution images from the LR ones, and then traditional HR FR methods could be applied for recognition. The landmark works in this category are face hallucination (Baker et al. <ref type="bibr" target="#b51">[52]</ref>) and simultaneous SR and recognition (S 2 R 2 ) (Hennings-Yeomans et al. <ref type="bibr" target="#b15">[16]</ref>). Two criteria are considered for SR applications: visual quality and recognition discriminability. However, most of these methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref> aimed to improve face appearance but failed to optimize face images from recognition perspective. Recently, a few attempts were made to achieve these two criteria under very LR case <ref type="bibr" target="#b2">[3]</ref>. (2) Direct method: resolution-robust feature representation is the process of directly extracting the discriminative information from LR images. The landmark works are color feature (Choi et al. <ref type="bibr" target="#b16">[17]</ref>) and coupled locality preserving mappings (CLPMs) (Li et al. <ref type="bibr" target="#b18">[19]</ref>). These methods can be separated into two groups further. One is feature-based method in which the resolution-robust features, such as texture <ref type="bibr" target="#b17">[18]</ref>, and subspace <ref type="bibr" target="#b55">[56]</ref> information, are used to represent faces. However, some features used in traditional HR FR methods are sensitive to resolution. The other is structure-based method, e.g., multidimensional scaling (MDS) <ref type="bibr" target="#b19">[20]</ref> in which the relationships between LR and HR are explored in resolution mismatch problem.</p><p>This categorization might provide useful information for potential readers. However, it is not unique. Alternative categorizations based on other criteria are also possible, such as relatively LR case and very LR case, single-modality based (against LR only) and multimodality based (against LR and other variations such as pose). Table <ref type="table" target="#tab_1">2</ref> summarizes representative works of LR FR within the two categories. In Sects. 3 and 4, the general principle and typical methods of each category will be discussed. They are followed by a review of specific methods, including discussions of their pros and cons and suggestions for future researches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Super-resolution for LR FR</head><p>Many researchers want to build face recognition systems with LR images obtained by web cameras or close-circuit television. However, the overall performance of LR FR needs great improvement. Compared with the development of resolution-robust face recognition methods, superresolution (SR), or hallucination methods have gained much more attentions, due to many problems that degrade the quality of face images in LR case. In this section, some typical SR methods specifically satisfying the requirement for face recognition will be reviewed. In the last decade, most of the conventional SR methods called vision-oriented SR were taken as the indirect way, which is reconstruction followed by recognition. Recently, some researchers focused on simultaneous SR and recognition, and SR mainly for recognition, called as recognition-oriented SR obtaining promising results for LR classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vision-oriented super-resolution for LR FR</head><p>The simplest way to increase resolution is direct interpolation of input images with methods such as nearest neighbor, bilinear, and bicubic. However, its performance is usually poor since no new information is added into the process <ref type="bibr" target="#b35">[36]</ref>. In contrast to the interpolation, SR increases resolutions of images or video frames using the relationships among several images. Generally, SR can be divided into two classes <ref type="bibr" target="#b72">[73]</ref>: reconstruction-based method (from input images alone) <ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref>, and learning-based method (from other images) <ref type="bibr">[36, 52-55, 57-59, 73, 78-87]</ref>.</p><p>The reconstruction-based method reconstructs HR images based on sampling theory by simulating the image formation process. However, the method has some fatal shortcomings. Baker et al. <ref type="bibr" target="#b56">[57]</ref> pointed out that the method inherits limitations when the magnification factor increases. Lin et al. <ref type="bibr" target="#b87">[88]</ref> proposed the problem "Do fundamental limits exist for the reconstruction-based SR?". They further gave explicit bounds of the magnification factor based on perturbation theory analysis. Recently, Nasrollahi et al. <ref type="bibr" target="#b88">[89]</ref> tried to solve the problem and improve the magnification factor from about two to almost four by using multilayer perceptron.</p><p>Most of the reconstruction-based methods are more suitable for synthesizing local texture, and they do not incorporate any specific prior information (e.g., face domain) about </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categorization Representatives</head><p>Indirect method: Super-resolution for LR FR Vision-oriented SR Face Hallucination <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b56">57]</ref>; Two-step Statistical Approach <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b57">58]</ref>;</p><p>Eigentransformation <ref type="bibr" target="#b35">[36]</ref>;</p><p>Extended Morphable Face Model <ref type="bibr" target="#b58">[59]</ref>.</p><p>Recognition-oriented SR Simultaneous SR and Recognition (S 2 R 2 ) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref>;</p><p>Multi-Modal Tensor SR (M 2 TSR) <ref type="bibr" target="#b11">[12]</ref>;</p><p>Discriminative SR (DSR) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b62">63]</ref>;</p><p>Support Vector Data Description (SVDD) <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>Direct method: Resolution-robust feature representation for LR FR Feature-based method Color-based Feature <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref>;</p><p>Texture-based Feature: Local Frequency Descriptor (LFD) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b68">69]</ref>;</p><p>Kernel Class-dependence Feature Analysis (KCFA) <ref type="bibr" target="#b55">[56]</ref>.</p><p>Structure-based method Eigenspace Estimation (EE) <ref type="bibr" target="#b14">[15]</ref>;</p><p>Coupled Locality Preserving Mappings (CLPMs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b69">70]</ref>;</p><p>Multi-Dimensional Scaling (MDS) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref>;</p><p>Coupled Kernel Embedding (CKE) <ref type="bibr" target="#b20">[21]</ref>.</p><p>the super-resolved images. Therefore, they are usually applicable to generic object or scenes rather than face images. Few researchers focused on enhancing face images, though Yu et al. <ref type="bibr" target="#b73">[74]</ref> proposed a new method for enhancing LR face videos. Moreover, the performance of the reconstructionbased methods is severely influenced by the following factors <ref type="bibr" target="#b87">[88]</ref>: the level of noise existing in the LR images, the accuracy of point spread function (PSF) estimation and the accuracy of alignment. In other words, higher level of noise or poorer PSF estimation and alignment will result in less improvement in resolution.</p><p>The learning-based method, also known as face hallucination <ref type="bibr" target="#b51">[52]</ref>, which is the focus in this review, is always used to enhance resolutions of face images compared with the reconstruction-based method. In essence, the learningbased SR is used to learn the relationships between LR and HR corresponding to different face images in a training set, and then use these learnt relationships to predict fine details for LR probe images (stored by image pixels, image patches, or coefficients of alternative representations). Establishing a good learning model to obtain the prior knowledge is the key to the learning-based method. At present, the commonly used learning models include the PCA model <ref type="bibr" target="#b35">[36]</ref>, image pyramid model <ref type="bibr" target="#b56">[57]</ref>, Markov model <ref type="bibr" target="#b81">[82]</ref>, etc.</p><p>How can we evaluate the quality of hallucinated HR images for face recognition? Three goals should be reached step by step. The first goal is to obtain HR images from visual perspective only. This is also the basic target for SR. Then the HR images are expected to be more like face images. Finally, we hope the HR face images are more like someone's face from recognition perspective.</p><p>For realizing the three goals, Liu et al. <ref type="bibr" target="#b57">[58]</ref> introduced two different data constraints: soft constraint and hard constraint. The former was to beautify faces and make the results more like the mean face, corresponding to the first two goals. And the latter was to faithfully reproduce facial details to be exactly the same as the input face, similar to the third goal. Enlightened by Liu's work, Zou et al. <ref type="bibr" target="#b2">[3]</ref> designed two new constraints. They included a data constraint and a discriminative constraint. The data constraint was I H -RI L 2 , to estimate the reconstruction error in the HR image space to make use of the information from HR training images. It was opposed to DI H -I L 2 used in the LR image space in the conventional methods. The discriminative constraint was to use class label information to boost recognition performance. For the detailed discussions about the two constraints, please refer to Sect. 3.2.2.</p><p>Zou et al. <ref type="bibr" target="#b2">[3]</ref> further categorized learning-based SR methods into two classes, namely maximum a posteriori (MAP)-based method <ref type="bibr">[52-54, 57, 58, 73, 79-84]</ref> and example-based method <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref>. It is noticeable that some methods overlap category boundaries. Other categories can also be discussed, such as single-frame-based and multiframe-based, intensity-based and frequency-based, global-based and local-based as well as global&amp;local. In this paper, MAP-based and example-based are taken as the main categorization and other categories as a supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">MAP-based method</head><p>In this method, the goal is to find an optimal solution maximizing the posterior probability p(I H |I L ) to obtain super-resolved HR images, i.e.,</p><formula xml:id="formula_0">I H = arg max I H p(I H |I L ) = arg max I H p(I L |I H )p(I H ) p(I L ) . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>In this formula, since p(I L ) is a constant for LR image I L is known already, the model can be simplified as follows:</p><formula xml:id="formula_2">I H = arg max I H p(I L |I H )p(I H ). (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>p(I L |I H ) is the probability of obtaining I L when HR image I H is given, and depends on the distribution of the noise deriving from the process of down-sampling. p(I H ) denotes the prior of I H . Therefore, the key of MAP-based method is to estimate p(I H ).</p><p>For p(I H ), different algorithms have different solutions. Baker et al. <ref type="bibr" target="#b56">[57]</ref> first proposed the idea of face hallucination and led the precedent of learning-based method. They estimated p(I H ) by using an image Gaussian pyramid under Bayesian formulation. The method obtained high-frequency components from a parent structure based on training face images; however, it intrinsically relied on a complicated statistical model. Similar to Baker's work, Capel et al. <ref type="bibr" target="#b82">[83]</ref> also used MAP estimators, with the difference that Capel divided a face image into six unrelated parts, and applied PCA on them separately. Dedeoglu et al. <ref type="bibr" target="#b80">[81]</ref> extended Baker's work to hallucinate face video by exploiting spatiotemporal constraints, and they reported a very high (×16) magnification factor for LR case.</p><p>Based on Baker's work <ref type="bibr" target="#b56">[57]</ref> and Freeman's work <ref type="bibr" target="#b83">[84]</ref>, Liu et al. <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b57">58]</ref>  H . However, this method depends on an explicit down-sampling function, which is sometimes unavailable in practice.</p><p>Enlightened by Liu's work, many methods treating face hallucination as a two-step problem have been proposed <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b79">80]</ref>. They all perform as the following two-step process. First, a global face image containing low-frequency information is obtained, which looks smooth and lacks some detailed features. Second, a residue face image keeping highfrequency information is synthesized. And then the residue image is piled onto the global image to get the final superresolved face images. For example, Li et al. <ref type="bibr" target="#b79">[80]</ref> used a MAP criterion for reconstructing both the global image and the residual image. Jia et al. <ref type="bibr" target="#b72">[73]</ref> proposed a unified tensor space representation for hallucinating low-frequency and middlefrequency information, and then recovered high-frequency part by patch learning.</p><p>In addition, some methods are performed in transformed feature space rather than pixel density domain. Zhang et al. <ref type="bibr" target="#b81">[82]</ref> performed SR in frequency domain with inferring discrete cosine transform (DCT) coefficients instead of estimating pixel intensities in spatial domain. Alternating component (AC) coefficients in DCT were inferred by the Markov network of low-level vision. Subspace methods are also applied to restrict the reconstructed HR image locating within face subspace, such as PCA <ref type="bibr" target="#b53">[54]</ref> and kernel PCA subspace <ref type="bibr" target="#b78">[79]</ref>. However, most of these SR methods only focus on frontal faces, and fail to deal with unconstrained variations in pose, illumination, and expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Example-based method</head><p>In this method, the HR image h is reconstructed as a linear or nonlinear combination of the HR training images h i by finding an optimal solution from the given LR image l, i.e., it can be mathematically written as</p><formula xml:id="formula_4">h = i α i h i . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>The key of example-based method is to determine the weight coefficients α i . They minimize the error caused by the linear or nonlinear approximation of the LR training images l i (the pairs of h i ) for l as follows:</p><formula xml:id="formula_6">α = arg min α i l - i α i l i 2 .</formula><p>(</p><formula xml:id="formula_7">)<label>5</label></formula><p>For obtaining α i , different algorithms have different models. Wang et al. <ref type="bibr" target="#b35">[36]</ref> proposed a representative example-based method and treated the hallucination problem as a transformation between LR and HR. They used PCA to fit an input LR face image as a linear combination of LR training images. The HR image was then synthesized by replacing the LR training images with their HR counterparts while retaining the same combination coefficients α i . However, the linear PCA model could not capture distinct structures of the input face efficiently and only focused on global estimation without paying attention to local details. Thus, the results seemed unclear, lacked detailed features, and caused some distortions. Moreover, they designed a mask to avoid artifacts on hair and background, and performed hallucination in the interior region of face. In fact, local modeling and appropriate smoothing can be adopted to handle these artifacts properly. That is to say, the idea of two-step in <ref type="bibr" target="#b52">[53]</ref> can be used to compensate high-frequency features for the work.</p><p>Compared with Wang's work <ref type="bibr" target="#b35">[36]</ref> operated in eigenface space, Liu et al. <ref type="bibr" target="#b54">[55]</ref> performed as the two-step way in patchtensor space. LR image was first partitioned into overlapped patches. HR patches were inferred respectively based on TensorPatch model and then fused together using a local distribution structure to form the hallucinated result. To further enhance the quality of the HR image, the coupled PCA method was developed for residue compensation. While the method added more details to the face, it also introduced more artifacts. Therefore, whether to adopt residue compensation techniques and when to do them is critical for superresolution (SR).</p><p>Besides eigenface and tensor space, manifold learning techniques are used for example-based SR. Manifold learning theory suggests that the subspace of face images has an embedded manifold structure. The high-dimensional structure formed by HR face images is homeomorphic with a geometric structure in LR space. It means that the features of LR and HR face images share a common topological structure, and thus, they are coherent through the structure. Therefore, some ideas of manifold learning such as local linear embedding (LLE) <ref type="bibr" target="#b77">[78]</ref> and locality preserving projection (LPP) <ref type="bibr" target="#b84">[85]</ref>- <ref type="bibr" target="#b85">[86]</ref> are introduced into SR and are discussed as follows.</p><p>Chang et al. <ref type="bibr" target="#b77">[78]</ref> introduced the idea of LLE into SR with neighbor embedding. They assumed that training LR and HR images form manifolds with similar local geometry in two distinct feature spaces and used the training image pairs to estimate the weight coefficients for reconstruction. However, they treated SR as a patch-based single-step technique without compensation for local image details.</p><p>Zhuang et al. <ref type="bibr" target="#b84">[85]</ref> developed locality preserving hallucination method based on LPP <ref type="bibr" target="#b6">[7]</ref>. It combined LPP and radial basis function (RBF) together to hallucinate a global HR face. Compared with Wang's work <ref type="bibr" target="#b35">[36]</ref>, the hallucinated global HR face contained more detailed features. However, there were more noises in the local features such as contour, nostril, and eyebrow, because LPP resulted in the loss of nonfeature information. To improve the details of the synthesized HR face, they developed a residue compensation method based on patch by neighbor embedding <ref type="bibr" target="#b77">[78]</ref>.</p><p>Inspired by Zhuang's work, Ma et al. <ref type="bibr" target="#b85">[86]</ref> employed additional constraints on neighborhood reconstruction for face hallucination. The input LR image determined the position at which the neighbors of a patch were chosen in training step, and then the hallucinated patches were reconstructed using optimal weights of the training image position-patches. The method did not incorporate any residue compensation step into hallucination. They gave the reason and discussed why the conventional two-step methods usually adopted the residue compensation step. Accord-ing to them, some detailed facial information was lost in the first global reconstruction step.</p><p>In addition, some methods performed example-based SR on single-frame LR face image <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b86">87]</ref>. For example, Park et al. <ref type="bibr" target="#b58">[59]</ref> performed SR within PCA feature space with an extended morphable face model. They defined the model by the pixel correspondence between a reference face and other faces. By using the model, all face images were separated into extended 3D-shape and texture. Then the PCA-based SR method was implemented on both shapes and textures of LR input to reconstruct the corresponding HR shapes and textures respectively, and they were further synthesized into the result.</p><p>Recently, Hu et al. <ref type="bibr" target="#b86">[87]</ref> also developed a single-frame SR method, like Liu's work <ref type="bibr" target="#b52">[53]</ref>. They used both global and local constraints for hallucination; the difference was that their global model was derived from the nonrigid warping of reference face examples and the learning of the pixel structure. The warping could capture a moderate range of face variations. And the effects of warping errors were reduced by the adaptive weighting in the local prior model. Thus, the method could infer more faithful individual structures of the target HR face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Discussion</head><p>Most of the vision-oriented SR methods have attempted to minimize mean-squared error (MSE) or maximize signalto-noise ratio (SNR) between the original HR and the reconstructed SR images. As we all know, the performances of face recognition systems mostly rely on the ability to identify key facial features, which are typically captured by high-frequency components. However, high-fidelity reconstruction of low-frequency content in SR may dominate the image <ref type="bibr" target="#b89">[90]</ref>. Therefore, obtaining a lower MSE or a higher SNR does not necessarily contribute to a better performance. That is to say, the primary goal of vision-oriented SR methods is to obtain a good visual reconstruction, but not usually designed from recognition perspective. As resolution decreases, SR becomes more vulnerable to unconstrained variations. It also introduces noises and distortions that affect recognition, especially when the probe identities could not be included in the process of training selection. Thus, one problem of what relationships exist between SR and recognition is generated.</p><p>Some researchers discussed the problem and made attempts to explore the potential of SR in recognition. Baker et al. <ref type="bibr" target="#b51">[52]</ref> stated that no new information had been added during resolution enhancement. Also, face recognition methods could be developed to theoretically work as well on the LR images as they did on the hallucination results. Gunturk et al. <ref type="bibr" target="#b53">[54]</ref> tried to reconstruct the necessary information required by face recognition system, especially with Fig. <ref type="figure">4</ref> The illustration of multimodal SR and recognition process in tensor space the consideration of the statistics of noises and motion estimation errors. However, the method is unsuitable for pose variations, and is also heavily time-consuming. Wang et al. <ref type="bibr" target="#b35">[36]</ref> explored whether hallucination could contribute to recognition. They found that the hallucinated images performed much better than the LR images but performances of both dropped when the face size decreased from 32 × 24 to 16 × 12 pixels with XM2VTS database. However, the improvement in recognition seemed not as significant as that in face appearance. They gave a relatively reasonable explanation that human visual system could better interpret the added high-frequency details in the reconstruction process.</p><p>In summary, most of the existing vision-oriented SR methods are not completely suitable for recognition. A promising way to further improve the robustness performance of SR for recognition is to embed SR into recognition. In the following subsection, we turn to recognitionoriented SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recognition-oriented super-resolution for LR FR</head><p>Recognition-oriented SR is not to use SR before recognition in the conventional way. It embeds the elements of SR methods into face recognition. Specifically, it fuses the models of the image formation process and the prior information, together with feature extraction and classification to design methods for recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b90">[91]</ref><ref type="bibr" target="#b91">[92]</ref><ref type="bibr" target="#b92">[93]</ref>. Compared with vision-oriented SR, recognition-oriented SR maybe more suitable for LR FR due to the following two observations. Firstly, it simultaneously performs SR and feature extraction with the direct goal of recognition. Secondly, it performs feature SR with the aim of reconstructing not only the low-frequency content (structure information) but also the high-frequency content (discriminative information) for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Simultaneous super-resolution and feature extraction</head><p>The method has drawn much attention. Here, we discuss two representative methods in detail as follows: multimodal tensor SR (M 2 TSR) <ref type="bibr" target="#b11">[12]</ref> and simultaneous SR and recognition (S 2 R 2 ) <ref type="bibr" target="#b15">[16]</ref>. Especially for S 2 R 2 , it is the first framework for realizing SR and recognition simultaneously.</p><p>Jia et al. <ref type="bibr" target="#b11">[12]</ref> made some pioneering explorations in this field and proposed M 2 TSR method, though it was still sequential and did not achieve complete simultaneity. It initially computed a maximum likelihood vector in the HR tensor space. Although it did not simultaneously perform against pose and illumination variation as illustrated in Fig. <ref type="figure">4</ref>, face hallucination and recognition were unified in this way. The consideration of multimodality could contribute to LR FR. However, its disadvantage was that the tensor manipulations for reconstruction demanded high computation expenses.</p><p>Hennings-Yeomans et al. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref> showed that the performances of conventional SR methods were degraded under very LR case. Thus, they proposed S 2 R 2 method to combine identification with reconstruction for dealing with LR problem by introducing the constraints between LR and Fig. <ref type="figure">5</ref> The framework of S 2 R 2 HR images in a regularization form, as illustrated in Fig. <ref type="figure">5</ref>. Formula ( <ref type="formula" target="#formula_8">6</ref>) denotes the base model of S 2 R 2 . y p , f </p><formula xml:id="formula_8">Bx -y p 2 + α 2 Lx 2 + β 2 F x -f (k) g 2 . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>In addition, the base S 2 R 2 model was improved by involving the cases of multiframes or multicameras version B (i) . Furthermore, the base SR prior model l (k) and feature extraction (F L ) were modified based on multiresolutions version (l or L). The modified model is shown in <ref type="bibr" target="#b6">(7)</ref>, where α, β, γ are the regularization parameters, and B denotes the image formation process. The base S 2 R 2 model and the improved version tested on CMU Multi-PIE database on 6 × 6 obtained the accuracies of 62.8 % and 73 %, in comparison with the PCA baseline method at 47.1 %.</p><formula xml:id="formula_10">B (i) x -y (i) p 2 + α 2 Lx -l (k) 2 + β 2 F x -f (k) g 2 + γ 2 F L Bx -f (k) L 2 . (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>Compared with the general indirect SR methods, S 2 R 2 improves identification accuracy and gets promising results on 6 × 6. However, the parametric optimization needs to be repeated for each gallery image in the database, especially for large databases; thus, their formulation is quite time-consuming. Also, this method assumes that gallery and probe images are in the same pose, frontal or localized perfectly, directly resulting in its inefficiency under many general scenarios. Therefore, how to obtain the appropriate regularization parameters and reduce the computational complexity are two important issues in this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature super-resolution</head><p>The method is also called feature hallucination, which was innovatively proposed by Li et al. <ref type="bibr" target="#b92">[93]</ref> to reconstruct HR features instead of HR images for face recognition. The kernel version of support vector data description (SVDD) <ref type="bibr" target="#b93">[94]</ref> was used to synthesize HR discriminative features both for vision and recognition perspective <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>. SVDD approximated the support of objects belonging to the normal class. Its main idea was to find a ball that could achieve two conflicting goals simultaneously. One was that it should be as small as possible and the other was that it should contain as much training data as possible with equal importance. However, the method is only for frontal faces and its generalization ability remains doubtful.</p><p>Besides, some methods are used for video applications. For example, Arandjelovic et al. <ref type="bibr" target="#b90">[91]</ref> proposed an extended generic model called shape-illumination manifold (gSIM) framework by separating illumination and down-sampling effects for feature SR. Their experiments on both the Cambridge database <ref type="bibr" target="#b94">[95]</ref> and the Toshiba database reported promising results for face recognition. However, the method requiring video sequences at enrollment makes it impractical for surveillance scenarios.</p><p>In addition, we introduce two representative recognitionoriented SR methods: nonlinear mappings on coherent features (NMCF) <ref type="bibr" target="#b91">[92]</ref> and discriminative SR (DSR) <ref type="bibr" target="#b2">[3]</ref>. Both of them introduce classification discriminability into SR process.</p><p>Huang et al. <ref type="bibr" target="#b91">[92]</ref> proposed NMCF method with canonical correlation analysis (CCA) to establish coherent features between LR and HR images represented by PCA. Motivated by Zhuang's work <ref type="bibr" target="#b84">[85]</ref>, they also applied the radial basis function (RBF) mapping to build the regression model by adopting the advantages of RBF, such as fast learning and generalization ability. NMCF was evaluated on 12 × 12 with FERET database and obtained the accuracy of 84.4 % compared with 36.9 % of the PCA baseline method.</p><p>Recently, Zou et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b62">63]</ref> proposed the DSR method with two constraints (new data constraint and discriminative Fig. <ref type="figure">6</ref> The framework of discriminative SR for recognition with two new constraints constraint). It modeled the SR problem as a regression problem in the kernel space under very LR case such as 16 × 12 and 7 × 6, as illustrated in Fig. <ref type="figure">6</ref>. Formula (8) shows the mapping relationships under the two constraints. The former is the new data constraint, while the latter is the discriminative constraint. The conventional SR method employs the data constraint DI i h -I i l 2 (D is the down-sampling operator) to make full use of the information in LR space for SR. However, this data constraint may not work well for very LR case because of the limited information carried by LR space. So, the data constraint is changed into</p><formula xml:id="formula_12">I i h -RI i l 2</formula><p>, where DR = I. The discriminative constraint is to use the class label information of the training data for improving the discriminability. DSR shows its superiority from both visual quality and recognition performance. For example, super-resolution results on 16 × 12 with Extended Yale B database are shown in Fig. <ref type="figure">7</ref>. Also, DSR obtained the recognition accuracy of 73.5 % on 7 × 6 with CMU PIE database in comparison with 40.5 % in the PCA baseline method.</p><formula xml:id="formula_13">R = arg min R 1 N N i=1 I i h -RI i l 2 + λ DR -I 2 F + γ mean I i h -RI j l 2 class I i h = class I j l -mean I i h -RI j l 2 class I i h = class I j l .<label>(8)</label></formula><p>A brief discussion on the similarities and differences of DSR and NMCF is as follows: Both DSR and NMCF require a training set containing LR and HR image pairs to learn the nonlinear mappings from LR to HR feature space, followed by the reconstruction of SR images or features. Compared with NMCF, DSR performs more efficiently when LR images are used for training/gallery sets. Conversely, when HR training/gallery sets are used, the performance of NMCF is better than DSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Discussion</head><p>Some successes have been achieved by recognition-oriented SR methods such as S 2 R 2 <ref type="bibr" target="#b15">[16]</ref> and feature SR <ref type="bibr" target="#b92">[93]</ref>. However, they just provide the framework for recognitionoriented SR, and their recognition performances largely depend on different reconstruction regularization models and feature extraction techniques. Some common problems are still unsolved in these methods. For example, it is unclear what kind of reconstruction regularization method is more appropriate for recognition. In addition, feature extraction is known to be sensitive to large appearance changes due to pose, illumination, expression, etc. To combine superresolution and feature extraction perfectly is also a big issue for the future work. A possible way to handle these problems is to adopt more robust feature extraction techniques, which is the focus in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Summary of super-resolution for LR FR</head><p>A summary of super-resolution (SR) for LR FR is as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature-based method</head><p>This method identifies an LR face directly using the features extracted from probe images in resized forms. However, all the existing resolution-robust features are improved from the features used in HR FR, such as the improved color space <ref type="bibr" target="#b16">[17]</ref> and the improved local binary pattern descriptor <ref type="bibr" target="#b17">[18]</ref>. Similar to the categorization that successfully used in <ref type="bibr" target="#b23">[24]</ref>, we further classify feature-based method into two categories, that is, the global feature-based method and local feature-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Global feature based method</head><p>In this method, the whole LR probe image, represented by a single high-dimensional vector containing the global lowfrequency information, is taken as input. The advantage of this method is to implicitly preserve all the detailed texture and shape information, which is useful for recognizing LR faces. On the other hand, this method is also easily affected by variations such as pose and illumination like they do as HR FR. Here, two kinds of global methods are introduced: color features and improved dimensionality reduction techniques.</p><p>Color features are the representative global features. Choi et al. <ref type="bibr" target="#b16">[17]</ref> first demonstrated that color-based features could significantly improve LR FR recognition performance compared with gray-based features. The idea was based on the boosting effects of color features on low-level vision <ref type="bibr" target="#b100">[101]</ref>. A new metric called variation ratio gain (VRG) as shown in <ref type="bibr" target="#b8">(9)</ref> was further defined to prove the significance of color effect on LR face images within the subspace face recognition framework. In VRG, J lum+chrom (γ ) and J lum (γ ) represent variation ratio parameterized by face resolution (γ ) for color-augmentation-based feature subspace and intensity-based ones respectively. Here, J (γ ) is the ratio between the variations of extra-personal covariance matrices and those of intrapersonal covariance matrices for classification tasks. As illustrated in Fig. <ref type="figure">8</ref>, as γ decreases, VRG (γ ) gets larger, indicating that color components can compensate a decreased extra-personal variation by intensity component with LR. Based on the phenomenon, RQCr color space was selected for LR FR. Experiments on the hybrid database collected from CMU PIE, Color FEERT, and XM2VTS with probe resolution 15 × 15 tested on "RQCr" and "R" space achieved accuracies of 68 % and 54 %, respectively.</p><formula xml:id="formula_14">VRG(γ ) = J lum+chrom (γ ) -J lum (γ ) J lum (γ ) × 100.<label>(9)</label></formula><p>VRG(γ ) demonstrates the role of color in LR classification. However, no theories can prove that RQCr is more efficient for LR case in comparison with other color spaces. Therefore, to efficiently use color-based features for boosting intensity-based features is still an open issue. It is known that reducing the correlation of different color components is certainly helpful to HR FR, and even LR FR. Yang et al. <ref type="bibr" target="#b101">[102]</ref> investigated the potential efficiency of color spaces, and proposed various normalized spaces such as the improved YRB space to enhance face recognition.</p><p>Choi et al. <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref> improved their work and proposed a color feature selection method by boosting-learning framework. Thirty-six different color components were used to form a color-component pool, and a weighted fusion scheme was used to fuse the selected color features at the feature Fig. <ref type="figure">8</ref> Average variation ratios with respect to six different resolutions level. The method was successfully evaluated on very LR images with SCface database. It improved the accuracy with RQCr space from 49.61 % to 62.78 % with the new color pool. The experiment indicated that the framework of color fusion was perhaps beneficial to LR FR. Furthermore, they adopted LBP features in color space for LR FR <ref type="bibr" target="#b65">[66]</ref>. However, the role of color features for LR images is degraded by serious illumination variations despite of their successes in face recognition.</p><p>In addition, improved dimensionality reduction techniques are also proposed for the LR problem. Abiantun et al. <ref type="bibr" target="#b55">[56]</ref> adopted the kernel class-dependence feature analysis (KCFA) method <ref type="bibr" target="#b102">[103]</ref> for dealing with very LR case on the FRGC database Experiment 4. KCFA used a set of minimum average correlation energy filters to exploit higherorder correlations between training samples in the kernel space, and obtained the accuracy of 27.1 % on 8 × 8 compared with the PCA baseline method of 12 % on HR images. Wang et al. <ref type="bibr" target="#b95">[96]</ref> proposed a new graph embedding method called FisherNPE for resolution-robust feature extraction, based on LDA and neighborhood preserving embedding (NPE) preserving both global and local structures on the data. Also, Bayesian probabilistic similarity analysis <ref type="bibr" target="#b7">[8]</ref> of intensity differences between LR and HR images was used for classification. Photon-counting LDA <ref type="bibr" target="#b103">[104]</ref> was proposed for coping with the LR FR, modeling the image pixels with Poisson distribution by the semiclassical theory of photon detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Local feature based method</head><p>In this method, the LR probe image is represented by a set of low-dimensional vectors containing the local highfrequency information. Compared with global method, local feature based method provides additional flexibility to recognize a face based on its parts, and is more robust to variations.</p><p>For example, Hadid et al. <ref type="bibr" target="#b104">[105]</ref> proposed a novel discriminative feature space for detecting and recognizing LR faces from video, employing LBP representation and SVM classifier. Ahonen et al. <ref type="bibr" target="#b68">[69]</ref> adopted local phase quantization (LPQ) method based on the assumption of point spread function. The method used the phase information of Fourier transformed images for LR FR, revealing that LPQ information in the high-frequency domain was almost invariant to blur. Afterwards, Lei et al. <ref type="bibr" target="#b17">[18]</ref> made an improvement on LPQ and proposed local frequency descriptor (LFD) using not only phase information, but also magnitude information. Furthermore, the relative relationships between phase information were adopted without the assumption of point spread function instead of the absolute value. Also, a uniform pattern mechanism <ref type="bibr" target="#b13">[14]</ref> was introduced to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Discussion</head><p>Finding resolution-robust features is a conventional issue for face recognition in the LR or HR case. Although many researchers concerned resolution-robust feature representation, the performance is far from perfect due to different complicated variations. Most methods mentioned above are only against one variation and not against multiple. For example, compared with local features, global features are more sensitive to illumination variation. With regards to pose and expression variation, local features are more susceptive than global features. A possible way to further improve the robustness may lie in the combination of localbased and global-based features. However, what features should be combined and how to combine them for concentrating their advantages are the future issues for LR FR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Structure-based method</head><p>Compared with the feature-based method concerning resolution-robust features, the structure-based method focuses on constructing the relationships between LR and HR feature space for facilitating direct comparison of LR probe images with HR gallery ones from a classification perspective. The method aims to build the holistic framework for LR matching by especially solving the particular problem in LR FR, namely dimensional mismatch. Here, we introduce three kinds of structure-based methods. Coupled mappings <ref type="bibr" target="#b18">[19]</ref> aim to find the structure relationships. Resolution estimation <ref type="bibr" target="#b42">[43]</ref> determines the kinds of structures chosen for building LR FR system. Finally, the sparse representation based method <ref type="bibr" target="#b97">[98]</ref> is adopted for representing LR probe images using HR training images from a structure perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Coupled mappings</head><p>Choi et al. <ref type="bibr" target="#b14">[15]</ref> first pointed out the dimensional mismatch problem, and proposed eigenspace estimation (EE) techniques for obtaining a common LR feature space for matching between LR and HR. Then Li et al. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b69">70]</ref> proposed a more general framework called unified feature space based on coupled mappings (CMs) <ref type="bibr" target="#b9">(10)</ref>. In CMs model, l i and h i represent an LR face image and an HR one, respectively, and A L and A H are two coupled mapping matrices. For LR FR, the mapping between each LR image and the corresponding HR image is expected to be as close as possible in the new unified feature space. Obviously, EE is one special case of CMs with A H of down-sampling and A L of identity matrix. Although CMs provides a promising framework for learning the relationships between LR and HR, it has an obvious shortcoming in poor discriminability for classification. Therefore, Li et al. introduced the locality preserving objective <ref type="bibr" target="#b6">[7]</ref> into nonparametric CMs model, and proposed coupled locality preserving mappings (CLPMs) method as shown in <ref type="bibr" target="#b10">(11)</ref>. It significantly improved the performance by involving the weight relationships (W ij ) among data points. Evaluation on FERET database obtained the accuracy of 90.1 % on 12 × 12 in comparison with PCA baseline method with 61.8 %. However, CLPMs still exhibits sensitivity to the parameters and pose variations.</p><formula xml:id="formula_15">J (A L , A H ) = N t i=1 A T L l i -A T H h i 2 , (<label>10</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">J (A L , A H ) = N t i=1 N t j =1 A T L l i -A T H h j 2 W ij . (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>Other works with the aim of improving CMs have been developed. Zhou et al. <ref type="bibr" target="#b99">[100]</ref> improved the classification discriminability of CMs by introducing linear discriminant relationships <ref type="bibr" target="#b4">[5]</ref> between intraclass scattering and interclass scattering into CMs. Ren et al. <ref type="bibr" target="#b105">[106]</ref> adopted canonical correlation analysis with local discrimination criterion <ref type="bibr" target="#b106">[107]</ref> to compute the two coupled mapping matrices. With the process of regularization and piecewiseness on feature space, the method showed its superiority compared with CMs/CLPMs in both recognition accuracy and time complexity. Furthermore, Ben et al. <ref type="bibr" target="#b107">[108]</ref> used the ideas of CMs to couple gait feature with LR face images and map them onto a common space for LR FR.</p><p>Recently, Ren at al. <ref type="bibr" target="#b20">[21]</ref> further introduced the kernel tricks into CLPMs and proposed coupled kernel embedding (CKE) method for dealing with LR FR. In the CKE model <ref type="bibr" target="#b11">(12)</ref>, Ψ and Φ represent two different nonlinear mappings such as the Gaussian-quadratic kernel function. Experiments on the CMU Multi-PIE database obtained the accuracy of 84 % on 6 × 6. Although the Rank-1 accuracy on Fig. <ref type="figure">9</ref> The framework of MDS transformation learning method SCface database is only 11 %, it still outperformed the LR baseline method with 4 %. By the kernel tricks, on one hand, CKE improved the classification performance; on the other hand, it increased the time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J (A L , A</head><formula xml:id="formula_19">H ) = N t i=1 N t j =1 A T L Ψ (l i ) -A T H Φ(h j ) 2 W ij . (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>In resolution mismatch problem, there exist three relationships (LR vs. LR, LR vs. HR, and HR vs. HR) involved in data. CMs/CLPMs only considered LR vs. HR. Deng et al. <ref type="bibr" target="#b108">[109]</ref> further considered the other two relationships and adopted regularized coupled mappings with two new color spaces to get more information. However, the efficiency of the method empirically depends on the regularized parameters.</p><p>Similar to the work in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b108">109]</ref>, Biswas et al. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref> skillfully utilized the three relationships between LR and HR, as illustrated in Fig. <ref type="figure">9</ref>. During training, they embedded LR images into a new Euclidean space in order to achieve the best distances between their HR counterparts using multidimensional scaling (MDS) <ref type="bibr" target="#b109">[110]</ref>, and a transformation matrix W was obtained. During the test, LR gallery and probe images were transformed independently using the learned transformation matrix. Then the matching process was performed. It should be emphasized that they highlighted the pose problem involved in LR recognition. This is an important contribution for researches on LR FR. They evaluated MDS on CMU Multi-PIE (8 × 6) and SCface database (12 × 10), and obtained the accuracies of 52 % and 71 %, respectively. In their experiments for LR FR, MDS performed better than sparse representation based super-resolution <ref type="bibr" target="#b110">[111]</ref>.</p><p>For further extension, here we provide a more general CMs model ( <ref type="formula" target="#formula_21">13</ref>), including super-resolution (SR) and resolution-robust feature extraction. F represents feature extraction or subspace dimensionality reduction techniques. Then different stable features can be integrated into the framework. When A L is replaced by SR constraints with the settings of A H = I M , the new model will be turned into SR. That is to say, SR is one special case of the model. Although S 2 R 2 <ref type="bibr" target="#b15">[16]</ref> is essentially different from the general indirect SR methods, it can also be represented by the new model when A and F are simultaneously used from both reconstruction and recognition perspectives. When F is provided as the form of kernel processing, the model will be turned into CKE <ref type="bibr" target="#b20">[21]</ref> and MDS <ref type="bibr" target="#b19">[20]</ref>. Finally, if F is ignored and just A is preserved, it returns to the base CMs model. In short, the new model is more general. However, to efficiently compute the two coupled mapping matrices A H and A L is also the key to the new model like the CMs model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J (A L , A</head><formula xml:id="formula_21">H ) = N t i=1 A T L F l i -A T H F h i 2 . (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>In fact, ideas similar to CMs are also applied to other problems of face recognition. Lin et al. <ref type="bibr" target="#b111">[112]</ref> proposed the idea of common discriminant feature extraction to solve the heterogeneous face recognition problems such as matching between visual (VIS) image and near infrared (NIR) image, and photo-sketch recognition. Recently, Lei et al. <ref type="bibr" target="#b112">[113]</ref> proposed coupled spectral regression (CSR) to address VIS-NIR recognition. Furthermore, they improved CSR method in <ref type="bibr" target="#b113">[114]</ref>, which was also evaluated on LR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Resolution estimation</head><p>The method is another way proposed for dealing with the dimensional mismatch problem. It determines the kinds of structures chosen for building the LR FR system. Wong et al. <ref type="bibr" target="#b42">[43]</ref> proposed two innovations for the LR problem. One was the concept of an underlying resolution, which did not rely on the size of face image. The other was that the local features sensitive to resolution were exploited for LR classification. Based on the innovations, they proposed a resolution detection and compensation framework for dynamically choosing the appropriate face recognition system. A similar method was proposed by Pedro et al. <ref type="bibr" target="#b114">[115]</ref>. They developed the concept of estimating the acquisition distance in three different scenarios (close, medium, and far distance). And the distance was taken as the weight to fuse two systems (PCA-SVM system and DCT-GMM system) at the scorelevel. They demonstrated that training with medium distance images was a good way to control the performance degradation due to the varying distance.</p><p>In a way, the compensation frameworks show the potential of multiple face recognition systems for addressing LR FR. For example, color feature selection framework <ref type="bibr" target="#b66">[67]</ref> is just a typical one. A combination of different classifiers <ref type="bibr" target="#b115">[116,</ref><ref type="bibr" target="#b116">117]</ref> also provided multimodal fusion at the classifier-level for the LR problem. In addition, a combination of several common methods was also proposed for dealing with the LR problem in <ref type="bibr" target="#b96">[97]</ref>. They first adopted sparse representation <ref type="bibr" target="#b97">[98]</ref> to describe patches represented by LBP features with different sizes. Then AdaBoost was used to select the most discriminative patches for classification. However, compared with feature selection <ref type="bibr" target="#b117">[118]</ref>, the method tested on Extended Yale B database showed poor performance. Thus, it remains doubtful whether such combination is efficient for LR FR, even just for HR FR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Sparse representation based method</head><p>Sparse representation is first proposed by Wright et al. <ref type="bibr" target="#b97">[98]</ref> for coping with robust classification problem. It is recently warmed and has become one of the standard methods of face recognition within the literature followed by many researchers. They cast the recognition problem as one of classifying among multiple linear regression models. If the number of features was sufficiently large, and the sparse representation was correctly computed, they demonstrated that the choice of features was no longer critical. It was right even in the down-sampled images, though their work was not specialized for the LR case. However, like most of the other methods, sparse representation also requires the training/gallery samples covering different variations such as pose, illumination, and expression. It will be a big obstacle for real applications.</p><p>Furthermore, Yang and Wright et al. <ref type="bibr" target="#b110">[111]</ref> adopted sparse representation for super-resolution (SRSR) on face images. More recently, inspired by their work, Bilgazyev et al. <ref type="bibr" target="#b89">[90]</ref> performed SRSR on high-frequency components learned by wavelet decomposition-based rules. They reported that the recognition performance outperformed SRSR <ref type="bibr" target="#b110">[111]</ref> and S 2 R 2 <ref type="bibr" target="#b15">[16]</ref> for the CMU PIE database. Moreover, Shekhar et al. <ref type="bibr" target="#b98">[99]</ref> proposed an LR FR method with especially handling illumination variations based on sparse representation. In short, sparse representation may provide a new theoretical framework for dealing with LR FR problem in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Discussion</head><p>The key of the coupled mappings is to obtain an interresolution space or unified feature space for solving the mismatch problem. The interresolution may be intuitively less than HR but greater than LR. However, EE <ref type="bibr" target="#b14">[15]</ref> takes a new LR space as the interresolution space. Also, CLPMs <ref type="bibr" target="#b18">[19]</ref> achieves the best performance in the interresolution less than the resolution of probe images. Thus, a question will be naturally generated as which the interresolution is suitable for LR FR. It is difficult to answer due to many factors such as different feature representations, face databases, and applications. Other strategies in the structure-based method, such as resolution estimation, system compensation, and sparse representation may provide the promising directions for addressing LR FR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Summary of resolution-robust feature representation for LR FR</head><p>A summary of resolution-robust feature representation for LR FR is as follows:</p><p>(1) In general, resolution-robust feature representation methods are mainly applicable in relatively LR cases rather than very LR cases like 6 × 6. (2) The feature-based methods can be used for multiple resolutions from HR to LR, but they need online training. However, the structure-based methods are more suitable  <ref type="table" target="#tab_3">3</ref>, which also provides a comparison between super-resolution and resolution-robust feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluations on LR FR methods</head><p>In order to have a clear idea on various LR FR methods, it is important to evaluate them based on certain evaluation criteria with some standard LR face databases. Unfortunately, such a requirement is seldom satisfied in practice due to the lack of general criteria and databases originally developed for LR FR. At present, LR FR methods are just evaluated based on HR FR criteria and databases.</p><p>As for the evaluation criteria for face recognition, generally speaking, face recognition can be described in terms of the following two tasks. One is face verification where the input is a face image and an identity label, and the output is a binary decision, yes or no, to confirm the identity label. Thus, face verification is a 1-to-1 problem. The other is face identification where the input is a face image, and the output is to assign the identity label assigned to the face by pointing out the subject. Evidently, face identification is a 1-to-N problem and also popularly called face recognition. Moreover, some new tasks have been proposed, such as screening and watch list, which are the transformed versions of verification or identification <ref type="bibr" target="#b61">[62]</ref>.</p><p>In this review, face identification (recognition) is our focus. In identification, the cumulative match characteristic (CMC) curve is usually adopted to plot the percentage of identification accuracy (IDA) vs. Rank. (IDA is the number of correctly assigned labels to the total number of input faces.) All these metrics are generally derived from HR FR and can also be used in LR FR with different resolutions. Other metrics, such as the minimal resolution and execution time, are occasionally applied to evaluate LR FR methods. However, the performances of different methods depend on different databases to some extent. Therefore, a few standard LR face databases are necessarily built for fair comparisons, which is the future work. In this section, we evaluate some representative methods on image-based standard databases and video-based real environment, respectively, to find the problems existing in LR FR. The performances and the limitations of these methods are finally summarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on image-based standard databases</head><p>While HR FR is investigated in depth, many image-based standard databases have been established to compare performances of these methods, such as AR <ref type="bibr" target="#b118">[119]</ref>, Yale <ref type="bibr" target="#b119">[120]</ref>, Extended Yale B <ref type="bibr" target="#b120">[121]</ref>, and CAS-PEAL <ref type="bibr" target="#b121">[122]</ref>. However, there is currently no database for LR FR, so that for evaluations on most of the existing LR FR methods, face images with frontal view, neutral expression, and illumination variations are selected and preprocessed such as downsampling and blurring instead of the actual LR images taken by surveillance cameras. Currently, the widely used databases for LR FR are FERET <ref type="bibr" target="#b122">[123]</ref>/Color FERET <ref type="bibr" target="#b123">[124]</ref>, CMU PIE <ref type="bibr" target="#b124">[125]</ref>/CMU Multi-PIE <ref type="bibr" target="#b125">[126]</ref>, FRGC <ref type="bibr" target="#b126">[127]</ref>, and SCface <ref type="bibr" target="#b127">[128]</ref>.</p><p>FERET <ref type="bibr" target="#b122">[123]</ref> consists of one gallery set and four probe sets (fafb, fafc, dup1, dup2). There are 1,196 images of 1,196 subjects in the gallery set and the four probe sets contain <ref type="bibr" target="#b0">1,</ref><ref type="bibr">195,</ref><ref type="bibr">194,</ref><ref type="bibr">722</ref>, and 234 images, respectively. Fafb probe images are obtained as frontal view and expression variation, and usually taken as the experiment data in most LR FR methods such as CLPMs <ref type="bibr" target="#b18">[19]</ref>, LFD <ref type="bibr" target="#b17">[18]</ref>, S 2 R 2 <ref type="bibr" target="#b15">[16]</ref>, and NMCF <ref type="bibr" target="#b91">[92]</ref>. As for other methods such as M 2 TSR <ref type="bibr" target="#b11">[12]</ref>, the experimental data are collected from the four probe sets. Moreover, Color FERET <ref type="bibr" target="#b123">[124]</ref> is also used for evaluating LR FR methods such as RQCr <ref type="bibr" target="#b16">[17]</ref>. It is worth noting that S 2 R 2 is just evaluated on 6 × 6, and LFD and RQCr are tested on blurred images.</p><p>CMU PIE <ref type="bibr" target="#b124">[125]</ref> includes 41,368 images of 68 subjects (21 samples/subject). Among them, 3,805 images have coordinate information of facial feature points. Some methods such as EE <ref type="bibr" target="#b14">[15]</ref>, RQCr <ref type="bibr" target="#b16">[17]</ref> and DSR <ref type="bibr" target="#b2">[3]</ref>, evaluated on this database, select the frontal view images with illumination variation for experiments from the 3,805 images. CMU Multi-PIE <ref type="bibr" target="#b125">[126]</ref> is a recent extension of CMU PIE database. It has a total of 337 subjects (compared with 68 subjects in CMU PIE) who participated in one to four different recording sessions, separated by at least a month (unlike CMU PIE, where all images of each subject are captured on the same day in a single session). As in CMU PIE, facial pose, expression, and illumination variations due to flashes from different angles are recorded. The frontal view images with neutral expression and illumination variations are chosen for LR FR methods, e.g., S 2 R 2 <ref type="bibr" target="#b15">[16]</ref>, MDS <ref type="bibr" target="#b19">[20]</ref>, CKE <ref type="bibr" target="#b20">[21]</ref>, and evaluated on CMU Multi-PIE, which is also similar to CMU PIE. FRGC <ref type="bibr" target="#b126">[127]</ref> consists of 50,000 images divided into training and validation sets. The training set is designed for training methods and the validation set is used for assessing performance of methods in a laboratory setting. The validation set includes 16,028 images of 466 subjects. The FRGC database consists of six experiments. Among the six experiments, in experiment 1, the gallery consists of a single controlled still image of a person and each probe consists of a single controlled still image. Experiment 2 studies the effect of using multiple still images of a person on performance. In experiment 2, each biometric sample consists of the four controlled images of a person taken in a subject session. Experiment 4 measures recognition performance from uncontrolled images. In experiment 4, the gallery consists of a single controlled still image, and the probe set consists of a single uncontrolled still image. S 2 R 2 <ref type="bibr" target="#b15">[16]</ref>, KCFA <ref type="bibr" target="#b55">[56]</ref>, and DSR <ref type="bibr" target="#b2">[3]</ref> methods are evaluated on FRGC database experiments 1, 2, and 4, respectively.</p><p>SCface <ref type="bibr" target="#b127">[128]</ref> is a database of static images of human faces. Images were taken in an uncontrolled indoor environment using five video surveillance cameras of various qualities. The database contains 4,160 static images (in visible and infrared spectrum) of 130 subjects. Images from different quality cameras mimic the real-world conditions and enable robust testing, emphasizing law enforcement and surveillance scenarios. MDS <ref type="bibr" target="#b19">[20]</ref>, DSR <ref type="bibr" target="#b2">[3]</ref>, and CKE <ref type="bibr" target="#b20">[21]</ref> methods are evaluated on this database.</p><p>The performances of LR FR methods tested on FERET, CMU PIE, CMU Multi-PIE, FRGC, and SCface are summarized in Tables 4, 5, 6, 7, and 8, respectively. Moreover, other databases such as XM2VTS <ref type="bibr" target="#b128">[129]</ref>, UMIST <ref type="bibr" target="#b129">[130]</ref>, ORL <ref type="bibr" target="#b130">[131]</ref>, and KFDB <ref type="bibr" target="#b131">[132]</ref> are also used to evaluate LR FR methods such as RQCr <ref type="bibr" target="#b16">[17]</ref>, NMCF <ref type="bibr" target="#b91">[92]</ref>, and SVDD <ref type="bibr" target="#b63">[64]</ref>, which are shown in Table <ref type="table" target="#tab_9">9</ref>. Most methods listed in Tables 4-9 aim at face identification (recognition) with CMC (Identification accuracy vs. Rank-1) as the evaluation criterion except for KCFA in Table <ref type="table" target="#tab_7">7</ref> aiming at face verification with receiver operating characteristic (ROC) curve (Verification accuracy vs. False acceptance rate).</p><p>On these databases, different methods are able to be compared on a relatively fair basis. And one can easily pick up the methods with good performances. The direct performance comparison of LR FR methods is not provided in this review mainly due to the lack of standard LR databases and evaluation protocols; however, the performances of baseline methods shown in the above tables can be taken as references for rough comparisons. Here, we discuss the results and attempt to give some relatively reasonable comparisons. Compared with other recent databases, FERET database mainly covers expression variation, which is relatively old. Tested on FERET, resolution-robust feature representation methods such as CLPMs <ref type="bibr" target="#b18">[19]</ref> slightly   outperform recognition-oriented super-resolution methods such as S 2 R 2 <ref type="bibr" target="#b15">[16]</ref> and NMCF <ref type="bibr" target="#b91">[92]</ref>. The result further supports that the former is more suitable for simple conditions, e.g., single expression variation. However, for relatively complicated databases such as FRGC and SCface, the situation will be reversed. That is to say, recognitionoriented super-resolution methods obtain much better performance, although the resolution-robust feature representation method MDS <ref type="bibr" target="#b19">[20]</ref> tested on SCface is greatly superior to DSR <ref type="bibr" target="#b2">[3]</ref>. This result is attributed to the use of frontal view probe images. In addition, the evaluations on CMU PIE or CMU Multi-PIE demonstrate that recognition-oriented super-resolution methods such as S 2 R 2 <ref type="bibr" target="#b15">[16]</ref> and DSR <ref type="bibr" target="#b2">[3]</ref> are more easily against unconstrained variations, e.g., illumination and expression. However, the resolution-robust feature representation method CKE <ref type="bibr" target="#b20">[21]</ref> also obtains promising results on CMU Multi-PIE mainly with the help of the kernel trick.</p><p>From the above analyses, we can know that it is very difficult to rank all the methods based on the existing and widely used image-based standard databases. Also, we find that no method can satisfactorily handle the LR problem in face recognition under all complicated variations. For example, M 2 TSR <ref type="bibr" target="#b11">[12]</ref> is the only method specially designed to deal with pose and illumination problem in LR classification. However, its performance on the FERET database with 14 × 9 is only 74.6 %, which is still far below the requirement of practical use. Therefore, this review mainly focuses on the discussions of different methodologies for LR FR, in hope of providing helpful technical insights and promising directions for interested researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on video-based real environment</head><p>After evaluations on image-based standard databases, let us conduct some further experiments on video-based real environment to evaluate the performances of various LR Fig. <ref type="figure" target="#fig_0">10</ref> The illustration of the surveillance camera system for different distances and illuminations. In each scenario, the left is a frame acquired by the surveillance camera, while the right is an up-sampled version of the frame. (a) 2.0 meters distance with good illumination, (b) 2.0 meters distance with poor illumination, (c) 3.0 meters distance with good illumination, (d) 3.0 meters distance with poor illumination FR methods. Some typical video databases such as CMU MoBo <ref type="bibr" target="#b132">[133]</ref>, Honda/UCSD <ref type="bibr" target="#b133">[134]</ref>, and CLEAR2006 <ref type="bibr" target="#b134">[135]</ref> are widely used for video-based face recognition even for LR FR. Recently, some researchers attempted to build LR databases to mimic real environment. Huang et al. <ref type="bibr" target="#b135">[136]</ref> presented a database named "labeled faces in the wild" (LFW), containing images that were collected from the web. Although it has natural variations in pose, illumination, expression, etc., there is no guarantee that such a database can accurately capture all variations found in the real world <ref type="bibr" target="#b136">[137]</ref>. Besides, most objects in LFW only have one or two images, which might not be enough to conduct different face recognition experiments. Yao et al. <ref type="bibr" target="#b137">[138]</ref> created a face video database, UTK-LRHM, obtained from long distances and with high magnifications, both indoors and outdoors under uncontrolled surveillance conditions. Also, they developed a wavelet transform based multiscale processing algorithm, which was used to deal with image degradations related to long-distance acquisition and was successful in improving recognition rate. Ni et al. <ref type="bibr" target="#b138">[139]</ref> manually put together a remote face database including the face images with variations due to occlusion, blur, pose, and illumination, which were taken from long distances and under unconstrained outdoor environments. The remote database was just evaluated on two state-of-the-art face recognition methods including baseline methods such as PCA, LDA, SVM, and the recently developed methods, e.g., sparse representation. But it did not form into a complete database for LR FR. In a word, there is still no LR benchmark database for public comparisons at present.</p><p>To evaluate the performances of the existing LR FR methods for real applications, we construct a video-based face database with uncooperative subjects in an uncontrolled indoor environment using a video camera (QVGA, 320 × 240). Images from the low-fidelity quality camera mimic the real-world LR FR conditions factually. The testing environment mainly includes two conditions, good/poor illumination, and 2.0/3.0 meters distance, which are illustrated in Fig. <ref type="figure" target="#fig_0">10</ref>. The database contains 800 training/gallery images (20 images per subject) and 160 testing/probe videos (4 videos per subject) from 40 subjects. Here, we do not discuss the process of detection for capturing and tracking face frames as that is out of the scope of this paper, though it is very important for video-based recognition.</p><p>In our experiments, S 2 R 2 <ref type="bibr" target="#b15">[16]</ref> and CLPMs <ref type="bibr" target="#b18">[19]</ref> are used for the real environment evaluation, as they are the representatives in recognition-oriented super-resolution method and resolution-robust feature representation method, respectively. For S 2 R 2 , all the HR training images and the corresponding down-sampled LR images are used for computing Fisherface features for representation, and learning regularization parameters and the discriminant coefficient. For the details of the system implementation, please refer to <ref type="bibr" target="#b15">[16]</ref>. In the testing procedure, since the current S 2 R 2 method needs large computation and fails to reach the requirement of real time, frame images are taken as the probe samples at a fixed distance such as 2.0 and 3.0 meters instead of performing on video directly. For CLPMs, all the HR training images and the corresponding down-sampled LR images are used for computing the two coupled mapping matrices. For the details of the training process, please refer to <ref type="bibr" target="#b18">[19]</ref>. Compared with S 2 R 2 , CLPMs is more suitable for real-time application. In experiments, only one aligned HR image (64 × 48) with frontal view and good illumination is taken as gallery for each subject. For each subject, five probe images are randomly selected at 2.0 and 3.0 meters distance, and their face sizes are resized into 32 × 24 and 21 × 15, respectively.</p><p>We design three group experiments for evaluating the effects of distance (resolution), illumination, and misalignment on the abilities of the two methods for dealing with the LR FR problem. For testing the effect of distance (resolution), the other two factors are taken as good illumination/manual alignment. In the same way, 2.0 meters distance/manual alignment and 2.0 meters distance/good illumination are set for testing the effects of illumination and alignment, respectively.</p><p>In 2.0 meters distance, S 2 R 2 and CLPMs obtain the promising identification accuracies (IDA vs. Rank-1) of 80 % and 87.5 %, which are shown in Fig. <ref type="figure" target="#fig_0">11</ref>. Compared with 2.0 meters case, the performances of S 2 R 2 and CLPMs decreased rapidly in 3.0 meters distance, and the decrease amplitude of CLPMs is larger than that of S 2 R 2 . It is worth noting that both of their decrease amplitudes are even close to that of the LR case. In the experiments for testing the Fig. <ref type="figure" target="#fig_0">11</ref> The illustration of cumulative matching characteristic (CMC) performance (IDA vs. Rank1-5) due to the effect of distance (resolution) under good illumination/manual alignment. "1" denotes 2.0 meters distance and "2" denotes 3.0 meters distance. HR denotes matching at base high-resolution and LR denotes matching at probe low-resolution, and they all use the Fisherface method Fig. <ref type="figure" target="#fig_0">12</ref> The illustration of CMC performance due to the effect of illumination under 2.0 meters distance/manual alignment. "1" denotes good illumination and "2" denotes poor illumination effect of illumination, the performances of all methods are decreased with the amplitude of about 6 % except for the LR case about 10 % when the illumination is changed from good into poor case, which are illustrated in Fig. <ref type="figure" target="#fig_0">12</ref>. For testing the effect of misalignment, the probe images have to be manually aligned with the positions of two eyes and mouth. The performances of all methods are improved by alignment, which is shown in Fig. <ref type="figure" target="#fig_0">13</ref>. And the result of CLPMs with alignment is even close to the HR case. However, S 2 R 2 without alignment is even inferior to the LR case with alignment. It is enough to demonstrate that alignment is very important for face recognition, especially for LR FR. Finally, we gather the three factors (distance, illumination, and misalignment) for testing the methods, that is, 3.0 meters distance, poor illumination, and misalignment. From Fig. <ref type="figure" target="#fig_0">14</ref>, the results of S 2 R 2 and CLPMs are very poor, and they only Fig. <ref type="bibr" target="#b12">13</ref> The illustration of CMC performance due to the effect of alignment under 2.0 meters distance/good illumination. "1" denotes manual alignment and "2" denotes misalignment Fig. <ref type="figure" target="#fig_0">14</ref> The illustration of CMC performance due to the effect of distance (resolution), illumination, and misalignment together. That is, the methods are tested on 3.0 meters distance, poor illumination, and misalignment obtain the accuracies of 30 % and 25 %, which are a little better than that of the LR case.</p><p>From the results of our tests on the real-world environment, we can see that the existing LR FR methods have not performed well under real-world scenarios. The representatives of LR FR methods such as S 2 R 2 and CLPMs are severely affected by complicated conditions, e.g., distance, illumination, misalignment. In such cases, compared with S 2 R 2 , the performance of CLPMs is much poorer, which is probably attributed to no stable features involved in CLPMs. However, S 2 R 2 is also unsuitable for real-time application mainly due to the complication of the model parameter learning. In a word, the existing LR FR methods should be greatly improved so as to be used for real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Based on the evaluations on image-based standard databases and video-based real environment, we can have an idea  <ref type="bibr" target="#b15">[16]</ref>, RQCr <ref type="bibr" target="#b16">[17]</ref> and CLPMs <ref type="bibr" target="#b18">[19]</ref>, face images are manually aligned and cropped with the positions of eyes and mouth. However, the way of manual alignment is very difficult in practical LR FR applications such as video surveillance. Furthermore, state-of-the-art face detectors such as AdaBoost <ref type="bibr" target="#b45">[46]</ref> remain poor in detecting LR face images. To cope with this problem, some researchers attempted to propose automatic alignment techniques to register LR faces in raw images. Jia et al. <ref type="bibr" target="#b72">[73]</ref> developed a pixelwise alignment method by iteratively warping the probe face to its projection in the eigenface space. Park et al. <ref type="bibr" target="#b58">[59]</ref> proposed an extended shape model with two constraints (face detection errors and shape estimation errors) to solve the misalignment problem. In a word, the development of automatic face alignment methods will facilitate the application of LR FR in real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Insensitivity to multiple variations</head><p>The performance of face recognition highly depends on unconstrained variation, e.g., pose, illumination, expression. Many methods have been proposed for reducing the effects of the changes, such as the Gabor filter function and the DCT on an edge map. However, most of the successful techniques could not be efficiently applied to LR data. For the LR case, the variations are the largest noises to some extent. Most of the existing LR FR methods assumed the constrained cases such as frontal pose, good illumination, and neutral expression, while few methods focused on facial representation against different variations. For example, Shekhar et al. <ref type="bibr" target="#b98">[99]</ref> proposed a synthesisbased method for handling the illumination-invariant LR FR problem. Chang et al. <ref type="bibr" target="#b139">[140]</ref> just examined the effect of resolution reduction with illumination variations. Arandjelovic et al. <ref type="bibr" target="#b90">[91]</ref> adopted an extension of the generic shapeillumination method, which was invariant to changes in pose, illumination, and subject motion pattern. For pose variation, only two methods namely MDS <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref> and M 2 TSR <ref type="bibr" target="#b11">[12]</ref> definitely pointed out the pose problem, which shows that pose variation is one of the biggest obstacles in LR FR. 3D face model reconstruction <ref type="bibr" target="#b140">[141]</ref> maybe an effective way for dealing with the problem. It should be emphasized that the M 2 TSR method addressed not only the pose-invariant problem, but also illumination-robust recognition. However, a majority of the methods only considered single variation by preprocessing or selecting the most suitable samples before recognition. In fact, multiple variations also cause many effects on HR FR, which are still not fully resolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Resolution-robust feature extraction</head><p>Most of the effective features used in HR FR such as texture and color may fail in LR case. Thus, it is difficult to find resolution-robust features for LR FR, especially under facial and environmental variations such as pose, illumination, and expression. Choi et al. <ref type="bibr" target="#b16">[17]</ref> made an attempt to adopt color features such as RQCr space and fusion of different color spaces for LR FR. It is known that color features are sensitive to illumination variation, which will be a big obstacle for LR FR application. Lei et al. <ref type="bibr" target="#b17">[18]</ref> proposed a novel texture descriptor named LFD based on LBP and Fourier transformation. Abiantun et al. <ref type="bibr" target="#b55">[56]</ref> used a new dimensionality reduction method called KCFA for dealing with LR feature extraction. Moreover, combination of global and local features, fusion of robust features, and effective subspace learning methods are future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Discriminative nonlinear coupled mapping</head><p>Different resolutions between HR gallery images and LR probe images cause a dimensional mismatch problem in the traditional classification framework. In fact, this problem is the most essential problem in LR FR compared with HR FR. Other problems such as misalignment, noise affection, and lack of effective features commonly exist in face recognition system no matter HR or LR case. Li et al. <ref type="bibr" target="#b18">[19]</ref> proposed the CMs/CLPMs model to build a unified feature space. Furthermore, this review gives the general CMs model including super-resolution and robust features for increasing the discriminability and generalization. Besides the relationships between LR and HR, other two relationships such as LR vs. LR and HR vs. HR were considered in <ref type="bibr" target="#b108">[109]</ref>. To sum up, three problems in CMs need to be solved. The first is to improve its ability in discriminability. The second is how to efficiently solve the eigenvalue decomposition problem and obtain the two optimal mapping matrices. The third is to generalize CMs from single LR to multiple LR applications even for across all resolutions. In addition, kernel tricks have great potentials in the application of LR FR such as CKE <ref type="bibr" target="#b20">[21]</ref>, MDS <ref type="bibr" target="#b19">[20]</ref>, KCFA <ref type="bibr" target="#b55">[56]</ref>, SVDD <ref type="bibr" target="#b63">[64]</ref>, and DSR <ref type="bibr" target="#b2">[3]</ref>. Thus, the kernel can probably be used for describing nonlinear mappings.</p><p>Except for the four main directions, the way of Multi-Frames should be particularly considered for improving LR FR systems used in video surveillance applications. Most LR FR methods focus on single-frame (image-based) recognition, i.e., from only one LR input. Few methods deal with the multiframes (video-based) case. Hadid et al. <ref type="bibr" target="#b141">[142]</ref> found that hidden Markov model (HMM) based methods with long sequences performed better than with short ones in both LR and HR, which means abundant information for recognition is included in video. Dedeoglu et al. <ref type="bibr" target="#b80">[81]</ref> proposed the concept of video hallucination by exploiting spatiotemporal regularities. Wheeler et al. <ref type="bibr" target="#b142">[143]</ref> adopted a sequence of video frames represented by the active appearance model (AAM) for LR FR. In a way, video-based feature extraction or video-to-video matching will provide a promising way for addressing the LR problem. Furthermore, multimodal biometric recognition systems, including LR FR may be used for recognition at a distance in the future. For example, multimodal face recognition is performed with LR, pose, and sketch images <ref type="bibr" target="#b143">[144]</ref>. Fusion of LR face and finger veins is used for multimodal biometric recognition <ref type="bibr">[145]</ref>. Integration of face and gait was proposed for human recognition at a distance in video <ref type="bibr">[146]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This paper devotes to providing a comprehensive survey on LR FR. After giving an overview of the LR FR concept and introducing some structural categories of the existing methods, many representative methods have been reviewed and evaluated in detail. Discussions on major challenges as well as future research directions toward complete LR FR are provided.</p><p>Although significant progress has been made in the last decade, we believe that a robust LR FR system should be effective under the following variations:</p><p>• multiple resolutions, at a distance, • noise, motion blur, • orientation, pose, partial occlusion, • illumination, expression.</p><p>From the discussions above, the conclusion can be drawn that LR is a challenging and interesting subarea in face recognition. The existing methods in LR FR fail to form a unified theoretical framework, and there are no standard databases and criteria to evaluate the performances of the LR FR methods. What are the effective features representing LR faces for recognition? How can we improve superresolution processing to satisfy both vision and recognition purpose? Is there a unified feature space where LR faces are separable? All of them will spur researchers to create more effective methods. Answers to these questions may lead to a clearer understanding of LR FR even general LR object recognition, which are similar to the findings of nonlinear mappings in pose-invariant face recognition and linear subspaces in illumination-invariant face recognition. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Some examples of LR face images</figDesc><graphic coords="3,306.43,199.56,236.88,92.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>x denote the input LR probe image, the gallery image in the kth class, and the output HR image, respectively; B, L, and F represent operators for down-sampling, smoothness and feature extraction, respectively; besides, α and β are the regularization parameters. The goal of the S 2 R 2 model is to obtain a suboptimal output HR image x for satisfying the need of vision and recognition simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 48 ( 1 )</head><label>7481</label><figDesc>Fig. 7 Comparison of different methods in Zou et al. [3]. (a) Input 16 × 12, (b) Bi-cubic, (c) Baker et al. [57], (d) Wang et al. [36], (e) Chakrabarti et al. [79], (f) Ma et al. [86], (g) Zou et al. [3], (h) Original 64 × 48</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Int. Conf. on Computer Vision and Pattern Recognition (CVPR), Colorado Springs, USA, June 2011, pp. 593-600 (2011) 145. Razzak, M.I., Khan, M.K., Alghathbar, K., Yusof, R.: Multimodal biometric recognition based on recognition based on fusion of low resolution face and finger veins. Int. J. Innov. Comput. Inf. Control 7(8), 4679-4689 (2011) 146. Zhou, X.L., Bhanu, B.: Integrating face and gait for human recognition at a distance in video. IEEE Trans. Syst. Man Cybern., Part B, Cybern. 37(5), 1119-1137 (2007) Zhifei Wang received his B.E. degree in biomedical engineering from Beijing Jiaotong University, Beijing, China, in 2006 and he is now a Ph.D. candidate in human-machine interaction at Institute of Information Science, Beijing Jiaotong University. His research interests include computer vision, face recognition, and pattern recognition. Zhenjiang Miao (M'11) received the B.E. degree from Tsinghua University, Beijing, China, in 1987, and the M.E. and Ph.D. degrees from Northern Jiaotong University, Beijing, in 1990 and 1994, respectively. From 1995 to 1998, he was a Post-Doctoral Fellow with the École Nationale Supérieure d'Electrotechnique, d'Electronique, d'Informatique, d'Hydraulique et des Télécommunications, Institut National Polytechnique de Toulouse, Toulouse, France, and was a Researcher with the Institute National de la Recherche Agronomique, Sophia Antipolis, France. From 1998 to 2004, he was with the Institute of Information Technology, National Research Council Canada, Nortel Networks, Ottawa, ON, Canada. He joined Beijing Jiaotong University, Beijing, in 2004. He is currently a Professor with Beijing Jiaotong University. His research interests include image and video processing, multimedia processing, and intelligent human-machine interaction. Q.M. Jonathan Wu (M'92-SM'09) received the Ph.D. degree in electrical engineering from the University of Wales, Swansea, U.K., in 1990. He was with the National Research Council of Canada for ten years from 1995, where he became a Senior Research Officer and a Group Leader. He is currently a Professor with the Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada. He has published more than 250 peer-reviewed papers in computer vision, image processing, intelligent systems, robotics, and integrated microsystems. His current research interests include 3-D computer vision, active video object tracking and extraction, interactive multimedia, sensor analysis and fusion, and visual sensor networks. Dr. Wu holds the Tier 1 Canada Research Chair in Automotive Sensors and Information Systems. He is an Associate Editor for the IEEE Transactions on Systems, Man, and Cybernetics Part A, and the International Journal of Robotics and Automation. He has served on technical program committees and international advisory committees for many prestigious conferences. Yanli Wan received her B.E. degree in computer science and technology from Jinan University in 2003, her M.E. degree in computer application technology from China University of Mining and Technology in 2006, and her Ph.D. degree from Beijing Jiaotong University in 2012. Her research interests include computer vision, image processing, and pattern recognition. Zhen Tang received his B.E. degree from China University of Petroleum in 2006 and his Ph.D. degree from Beijing Jiaotong University in 2012. His research interests include computer vision, image and video editing, and pattern recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Comparison between HR FR and LR FR</figDesc><table><row><cell>HR FR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Categorization of LR FR methods</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>proposed a two-step statistical method.</figDesc><table><row><cell cols="6">It integrated a parametric model called global face image</cell></row><row><cell cols="6">I ric model called local feature I l g H recording common facial properties, with a nonparamet-H carrying individualities, to</cell></row><row><cell cols="6">generate the HR image. Then the model (2) could be natu-</cell></row><row><cell cols="3">rally transformed into</cell><cell></cell><cell></cell><cell></cell></row><row><cell>I H = arg max I g H H ,I l</cell><cell>p I L |I</cell><cell>g H p I</cell><cell>g H p I l H |I</cell><cell>g H .</cell><cell>(3)</cell></row><row><cell cols="6">They applied PCA linear inferences to maximize p(I L |I × p(I g H ) and get an optimal global face image I g H , and a g H ) Markov random field prior was used to maximize p(I l H |I g H ) for obtaining a local feature I l</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Comparison between super-resolution and</cell><cell>Comparison issues</cell><cell>Super-resolution</cell><cell>Resolution-robust feature representation</cell></row><row><cell>resolution-robust feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>representation</cell><cell>Main purpose</cell><cell>For visual quality</cell><cell>For recognition discriminability</cell></row><row><cell></cell><cell>Resolution tolerance</cell><cell>Very LR</cell><cell>Relatively LR</cell></row><row><cell></cell><cell>Performing mode</cell><cell>Indirect way</cell><cell>Direct way</cell></row><row><cell></cell><cell>Computational complexity</cell><cell>High</cell><cell>Low</cell></row><row><cell></cell><cell>Unconstrained variations</cell><cell>Sensitive</cell><cell>Very sensitive</cell></row><row><cell></cell><cell>Requirements of training samples</cell><cell>Many</cell><cell>Few</cell></row><row><cell cols="2">for offline training, but they are mainly used for a sin-</cell><cell></cell><cell></cell></row><row><cell cols="2">gle resolution application with the balance between ef-</cell><cell></cell><cell></cell></row><row><cell>ficiency and speed.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(3) Obviously, the combination of feature-based and struc-</cell><cell></cell><cell></cell></row><row><cell cols="2">ture-based methods will contribute to very LR cases.</cell><cell></cell><cell></cell></row><row><cell cols="2">More characteristics about resolution-robust feature</cell><cell></cell><cell></cell></row><row><cell cols="2">representation methods are shown in Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Experiments and performances of LR FR on FERET database Frontal View" is abbreviated as "Frontal" in Tables4-9with the consideration of the table size. "1196/1195" means that 1,196 individuals for the gallery set and 1,195 individuals for the probe set fafb. Other "gallery/probe" shows the number of gallery/probe for one individual</figDesc><table><row><cell>No. of subjects</cell><cell>Probe resolution</cell><cell>Gallery/Probe</cell><cell>Primary variation</cell><cell>Method</cell><cell>Accuracy</cell><cell>Baseline</cell></row><row><cell>1196</cell><cell>12 × 12</cell><cell>1196/1195</cell><cell>Frontal, Expression</cell><cell>CLPMs [19]</cell><cell>90.1 %</cell><cell>61.8 % (PCA)</cell></row><row><cell>1196</cell><cell>33 × 30</cell><cell>1196/1195</cell><cell>Frontal, Blur</cell><cell>LFD [18]</cell><cell>8 6%</cell><cell>4 8%( L B P )</cell></row><row><cell>865</cell><cell>6 × 6</cell><cell>N/A</cell><cell>Frontal, Expression</cell><cell>S 2 R 2 [16]</cell><cell>6 2%</cell><cell>6 0%( L D A )</cell></row><row><cell>295</cell><cell>14 × 9</cell><cell>1/4</cell><cell>Pose, Illumination</cell><cell>M 2 TSR [12]</cell><cell>74.6 %</cell><cell>51.4 % (Tensor)</cell></row><row><cell>1196</cell><cell>12 × 12</cell><cell>1196/1195</cell><cell>Frontal, Expression</cell><cell>NMCF [92]</cell><cell>84.4 %</cell><cell>36.9 % (PCA)</cell></row><row><cell>140</cell><cell>15 × 15</cell><cell>1/4</cell><cell>Frontal, Blur, Color</cell><cell>RQCr [17]</cell><cell>6 8%</cell><cell>5 4%( R )</cell></row><row><cell>Note: "</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Experiments and performances of LR FR on CMU PIE database</figDesc><table><row><cell>No. of subjects</cell><cell>Probe resolution</cell><cell>Gallery/Probe</cell><cell>Primary variation</cell><cell>Method</cell><cell>Accuracy</cell><cell>Baseline</cell></row><row><cell>68</cell><cell>11 × 11</cell><cell>3/39</cell><cell>Frontal, Illumination, Neutral Expression</cell><cell>EE [15]</cell><cell>58.4 %</cell><cell>48.3 % (PCA)</cell></row><row><cell>68</cell><cell>15 × 15</cell><cell>1/20</cell><cell>Frontal, Illumination</cell><cell>RQCr [17]</cell><cell>68%</cell><cell>54%(R)</cell></row><row><cell>68</cell><cell>7 × 6</cell><cell>8/13</cell><cell>Frontal, Illumination, Expression</cell><cell>DSR [3]</cell><cell>73.5 %</cell><cell>40.5 % (PCA)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Experiments and performances of LR FR on CMU multi-PIE database</figDesc><table><row><cell>No. of subjects</cell><cell>Probe resolution</cell><cell>Gallery/Probe</cell><cell>Primary variation</cell><cell>Method</cell><cell>Accuracy</cell><cell>Baseline</cell></row><row><cell>337</cell><cell>6 × 6</cell><cell>7/13</cell><cell>Frontal, Illumination, Neutral Expression</cell><cell>CKE [21]</cell><cell>84 %</cell><cell>53 % (LPP)</cell></row><row><cell>337</cell><cell>8 × 6</cell><cell>N/A</cell><cell>Frontal, Illumination, Neutral Expression</cell><cell>MDS [20]</cell><cell>52%</cell><cell>40%(PCA)</cell></row><row><cell>224</cell><cell>6 × 6</cell><cell>1/13</cell><cell>Frontal, Illumination, Neutral Expression</cell><cell>S 2 R 2 [16]</cell><cell>62.8 %</cell><cell>47.1 % (LDA)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>Experiments and performances of LR FR on FRGC database</figDesc><table><row><cell>No. of subjects</cell><cell>Probe resolution</cell><cell>Gallery/Probe</cell><cell>Primary variation</cell><cell>Method</cell><cell>Accuracy</cell><cell>Baseline</cell></row><row><cell>300</cell><cell>6 × 6</cell><cell>1/20</cell><cell>N/A</cell><cell>S 2 R 2 [16]</cell><cell>55%</cell><cell>44%(CF A)</cell></row><row><cell>311</cell><cell>7 × 6</cell><cell>8/2</cell><cell>Near Frontal, Illumination, Neutral</cell><cell>DSR [3]</cell><cell>56.5 %</cell><cell>38.5 % (PCA)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Expression</cell><cell></cell><cell></cell><cell></cell></row><row><cell>466</cell><cell>8 × 8</cell><cell>8014/8014</cell><cell>Near Frontal, Expression, Severe</cell><cell>KCFA [56]</cell><cell>27.1 %</cell><cell>12 % (PCA)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Illumination</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>Experiments and performances of LR FR on SCface database</figDesc><table><row><cell>No. of subjects</cell><cell>Probe resolution</cell><cell>Gallery/Probe</cell><cell>Primary variation</cell><cell>Method</cell><cell>Accuracy</cell><cell>Baseline</cell></row><row><cell>130</cell><cell>16 × 16</cell><cell>N/A</cell><cell>Pose, Blur, Expression</cell><cell>CKE [21]</cell><cell>11 %</cell><cell>4 % (LPP)</cell></row><row><cell>130</cell><cell>12 × 10</cell><cell>1/4</cell><cell>Near Frontal, Expression</cell><cell>MDS [20]</cell><cell>7 1%</cell><cell>1 9%( P C A )</cell></row><row><cell>130</cell><cell>16 × 14</cell><cell>5/5</cell><cell>Pose, Blur, Expression</cell><cell>DSR [3]</cell><cell>22.5 %</cell><cell>14.5 % (PCA)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc>Experiments and performances of LR FR on other databases</figDesc><table><row><cell>Database</cell><cell cols="4">No. of subjects Probe resolution Gallery/Probe Primary variation</cell><cell>Method</cell><cell cols="2">Accuracy Baseline</cell></row><row><cell cols="2">XM2VTS 133</cell><cell>15 × 15</cell><cell>1/7</cell><cell>Frontal, Illumination</cell><cell>RQCr [17]</cell><cell>68%</cell><cell>54%(R)</cell></row><row><cell>UMIST</cell><cell>20</cell><cell>14 × 11</cell><cell>10/5</cell><cell>Pose</cell><cell cols="2">NMCF [92] 93%</cell><cell>90%(PCA)</cell></row><row><cell>ORL</cell><cell>40</cell><cell>8 × 8</cell><cell>5/5</cell><cell cols="3">Pose, Illumination, Expression NMCF [92] 95 %</cell><cell>84.5 % (PCA)</cell></row><row><cell>KFDB</cell><cell>N/A</cell><cell>16 × 16</cell><cell>N/A</cell><cell>Frontal, Neutral Expression</cell><cell cols="2">SVDD [64] 93 %</cell><cell>84 % (N/A)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10</head><label>10</label><figDesc>Comparison between evaluation on image-based and video-based database</figDesc><table><row><cell>Database</cell><cell>S 2 R 2</cell><cell>CLPMs</cell><cell>Comparison issues</cell></row><row><cell>Image-based</cell><cell>62 % (6 × 6)</cell><cell>90.1 % (12 × 12)</cell><cell>Manual Alignment/Frontal View/Expression/Image/Illumination/Blur</cell></row><row><cell>(e.g. FERET)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Video-based</cell><cell>30 % (21 × 15)</cell><cell>25 % (21 × 15)</cell><cell>Misalignment/Pose/Expression/Video/Illumination/Motion Blur/Noise</cell></row><row><cell>(real environment)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">on the performances of the existing LR FR methods. As</cell><cell>especially in surveillance scenarios. Based on the four main</cell></row><row><cell cols="3">for image-based standard databases, the down-sampled im-</cell><cell>parts of LR FR systems, including preprocessing, facial rep-</cell></row><row><cell cols="3">ages contain some variations in pose, illumination, expres-</cell><cell>resentation, feature extraction, and feature classification, we</cell></row><row><cell cols="3">sion, which can mimic real testing environment to some ex-</cell><cell>point out four challenges and corresponding four future di-</cell></row><row><cell cols="3">tent. However, the probe images with frontal view and neu-</cell><cell>rections, namely automatic alignment, insensitivity to multi-</cell></row><row><cell cols="3">tral expression are generally selected and aligned for ex-</cell><cell>ple variations, resolution-robust feature extraction, and dis-</cell></row><row><cell cols="3">periments in current testing procedures, which is unrealis-</cell><cell>criminative nonlinear coupled mapping.</cell></row><row><cell cols="3">tic for surveillance applications without subjects' coopera-</cell><cell></cell></row><row><cell cols="3">tion especially for moving subjects. As for video-based real</cell><cell>6.1 Automatic alignment</cell></row><row><cell cols="3">environment, LR images captured from surveillance cam-</cell><cell></cell></row><row><cell cols="3">eras typically contain the misalignment problem, which will</cell><cell>Alignment is one of the most important preprocessing is-</cell></row><row><cell cols="3">severely affect the performances of face recognition sys-</cell><cell>sues in face recognition, especially in LR FR. In most LR</cell></row><row><cell cols="3">tems. Besides, other particular factors in video-based test-</cell><cell>FR methods such as S 2 R 2</cell></row><row><cell cols="3">ing, e.g., noises, motion blurs, and detection of LR face</cell><cell></cell></row><row><cell cols="3">images will further make the problem much more compli-</cell><cell></cell></row><row><cell cols="3">cated. And the requirement of real-time applicability is also</cell><cell></cell></row><row><cell cols="3">an obstacle for most LR FR methods. In short, environmen-</cell><cell></cell></row><row><cell cols="3">tal conditions (e.g., noise, motion blur, distance, illumina-</cell><cell></cell></row><row><cell cols="3">tion), individualities of different subjects (e.g., pose, expres-</cell><cell></cell></row><row><cell cols="3">sion), and other factors (e.g., misalignment) will bring a</cell><cell></cell></row><row><cell cols="3">lot of effects on LR FR. Table 10 gives a concise compar-</cell><cell></cell></row><row><cell cols="3">ison of the representative methods S 2 R 2 and CLPMs evalu-</cell><cell></cell></row><row><cell cols="3">ated on image-based and video-based database, respectively.</cell><cell></cell></row><row><cell cols="3">From the table, we can see that the performance evaluated</cell><cell></cell></row><row><cell cols="3">on the video-based database is obviously inferior to image-</cell><cell></cell></row><row><cell cols="3">based database. And the promising results on image-based</cell><cell></cell></row><row><cell cols="3">database are largely attributed to the preprocessing steps</cell><cell></cell></row><row><cell cols="3">such as alignment and the selection of frontal view images.</cell><cell></cell></row><row><cell cols="3">Therefore, we can conclude that the existing LR FR meth-</cell><cell></cell></row><row><cell cols="3">ods are not fully satisfied for LR application, especially for</cell><cell></cell></row><row><cell cols="3">video-based real environment. Future directions addressing</cell><cell></cell></row><row><cell cols="3">LR FR problem are generalized in detail in the following</cell><cell></cell></row><row><cell>section.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">6 Existing problems and future trends</cell><cell></cell><cell></cell></row><row><cell cols="3">Compared with other subareas such as pose-invariant face</cell><cell></cell></row><row><cell cols="3">recognition and illumination-invariant face recognition, LR</cell><cell></cell></row><row><cell cols="3">FR is a new sub-area in face recognition. Thus, its first</cell><cell></cell></row><row><cell cols="3">priority is to guarantee efficiency and accuracy rather than</cell><cell></cell></row><row><cell cols="3">real time and low computational complexity. Although re-</cell><cell></cell></row><row><cell cols="3">searchers have exerted efforts on improving LR FR meth-</cell><cell></cell></row><row><cell cols="3">ods, some specific problems still exist in real applications,</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Key Technology R&amp;D Program of China (2012BAH01F03), National Natural Science Foundation of China (60973061), National Basic Research (973) Program of China (2011CB302203), Ph.D. Programs Foundation of Ministry of Education of China (20100009110004), Beijing Natural Science Foundation (4123104), and China Postdoctoral Science Foundation (2013M530020). The authors would like to thank Professor Shengyong Chen from Zhejiang University of Technology and the anonymous reviewers for their comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition: a literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="459" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Far-field, multi-camera, video-to-video face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pnevmatikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Polymenakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face Recognition</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Delac</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Grgic</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="467" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Very low resolution face recognition problem</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-06">June 1991. 1991</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition by independent component analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1450" to="1464" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face recognition using Laplacianfaces</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian face recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1771" to="1782" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eigenspace-based face recognition: a comparative study of different approaches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz-Del-Solar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Navarrete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern., Part C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="325" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low-resolution gait recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fleischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern., Part B, Cybern</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="986" to="996" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Toward a practical face recognition system: robust alignment and illumination by sparse representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="372" to="386" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-modal tensor face for simultaneous super-resolution and recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 10th Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE 10th Int. Conf. on Computer Vision (ICCV)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1683" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Von Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature subspace determination in video-based mismatched face recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 8th Int. Conf. on Automatic Face and Gesture Recognition (FG)</title>
		<meeting>IEEE 8th Int. Conf. on Automatic Face and Gesture Recognition (FG)<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008. 2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution and feature extraction for recognition of low-resolution faces</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Hennings-Yeomans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Color face recognition for degraded face images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern., Part B, Cybern</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1217" to="1230" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local frequency descriptor for low-resolution face recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 9th Int. Conf. on Automatic Face and Gesture Recognition (FG)</title>
		<meeting>IEEE 9th Int. Conf. on Automatic Face and Gesture Recognition (FG)<address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-03">Mar. 2011. 2011</date>
			<biblScope unit="page" from="161" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Low-resolution face recognition via coupled locality preserving mappings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multidimensional scaling for matching low-resolution face images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2019" to="2030" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coupled kernel embedding for low-resolution face recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3770" to="3783" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neighborhood preserving embedding</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 10th Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE 10th Int. Conf. on Computer Vision (ICCV)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1208" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting faces in images: a survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="58" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face recognition from a single image per person: a survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1725" to="1745" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">and 3D face recognition: a survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Riccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sabatino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2D</biblScope>
			<biblScope unit="page" from="1885" to="1906" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face recognition across pose: a review</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2876" to="2896" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Illumination invariant face recognition: a survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 1st Int. Conf. on Biometrics: Theory, Applications and Systems (BTAS)</title>
		<meeting>IEEE 1st Int. Conf. on Biometrics: Theory, Applications and Systems (BTAS)<address><addrLine>Crystal City, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building highly realistic facial modeling and animation: a survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ersotelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Handbook of Face Recognition, 2nd edn</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<title level="m">Face Processing: Advanced Modeling and Methods</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face recognition at a distance: system issues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Remote Biometrics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Tistarelli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009-08">Aug. 2009. 2009</date>
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Biometrics at a distance: issues, challenges, and prospects</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tistarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Remote Biometrics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Tistarelli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009-08">Aug. 2009. 2009</date>
			<biblScope unit="page" from="3" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face recognition at a distance</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Face Recognition</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="353" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A face recognition method using higher order local autocorrelation and multivariate analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 11th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IEEE 11th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>The Hague, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">B</biblScope>
			<biblScope unit="page" from="213" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face image resolution versus face recognition performance based on two global methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 6th Asian Conf. on Computer Vision (ACCV)</title>
		<meeting>IEEE 6th Asian Conf. on Computer Vision (ACCV)<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-01">Jan. 2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hallucinating face by eigentransformation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">O</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern., Part C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The effect of image resolution on the performance of a face recognition system</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Boom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Beumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Spreeuwers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N J</forename><surname>Veldhuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 9th Int. Conf. on Control, Automation, Robotics and Vision (ICARCV)</title>
		<meeting>IEEE 9th Int. Conf. on Control, Automation, Robotics and Vision (ICARCV)</meeting>
		<imprint>
			<date type="published" when="2006-12">Dec. 2006. 2006</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Grand Hyatt, Singapore</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Experiments on eigenfaces robustness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lemieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parizeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 16th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IAPR/IEEE 16th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">Aug. 2002. 2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="421" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluation of image resolution and super-resolution on face recognition performance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face recognition vendor test</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<ptr target="www.nist.gov/itl/iad/ig/frvt-2000.cfm" />
	</analytic>
	<monogr>
		<title level="m">FRVT2000. Evaluation Report</title>
		<imprint>
			<date type="published" when="2000-02">2000. Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FRVT 2006 and ICE 2006 largescale experimental results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="831" to="846" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A detection technique for degraded face images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1506" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic amelioration of resolution mismatches for local feature based identity inference</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 20th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IAPR/IEEE 20th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010. 2010</date>
			<biblScope unit="page" from="1200" to="1203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mbgc</surname></persName>
		</author>
		<ptr target="http://www.nist.gov/itl/iad/ig/mbgc.cfm" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gray-scale superresolution for face recognition from low gray-scale resolution face images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 17th Int. Conf. on Image Processing (ICIP)</title>
		<meeting>IEEE 17th Int. Conf. on Image essing (ICIP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010. 2010</date>
			<biblScope unit="page" from="2825" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Kauai, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards face recognition at a distance</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J D</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sizinstev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Olevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Crime and Security, the Institute of Engineering and Technology (IET), Savoy Place</title>
		<meeting>Int. Conf. on Crime and Security, the Institute of Engineering and Technology (IET), Savoy Place<address><addrLine>London, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006. 2006</date>
			<biblScope unit="page" from="570" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face recognition at a distance system for surveillance applications</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 4th Int. Conf. on Biometrics: Theory, Systems, and Applications (BTAS)</title>
		<meeting>IEEE 4th Int. Conf. on Biometrics: Theory, Systems, and Applications (BTAS)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010. 2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dual camera system for face detection in unconstrained environments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Regazzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 10th Int. Conf. on Image Processing (ICIP)</title>
		<meeting>IEEE 10th Int. Conf. on Image essing (ICIP)<address><addrLine>Barcelona, Catalonia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09">Sep. 2003. 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="681" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">PTZ camera assisted face acquisition, tracking &amp; recognition</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 4th Int. Conf. on Biometrics: Theory, Systems, and Applications (BTAS)</title>
		<meeting>IEEE 4th Int. Conf. on Biometrics: Theory, Systems, and Applications (BTAS)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010. 2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Face detection in lowresolution color images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fuentes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. on Image Analysis and Recognition (ICIAR)</title>
		<meeting>7th Int. Conf. on Image Analysis and Recognition (ICIAR)<address><addrLine>Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Povoa de Varzim</publisher>
			<date type="published" when="2010-06">June 2010. 2010</date>
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hallucianting faces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 4th Int. Conf. on Automatic Face and Gesture Recognition (FG)</title>
		<meeting>IEEE 4th Int. Conf. on Automatic Face and Gesture Recognition (FG)<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-03">Mar. 2000. 2000</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A two-step approach to hallucinating faces: global parametric model and local nonparametric model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="192" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Eigenface-domain super-resolution for face recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Gunturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">U</forename><surname>Batur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altunbasak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mersereau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="597" to="606" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hallucinating faces: tensorpatch super-resolution and coupled residue compensation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">O</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="478" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">How low can you go? Low resolution face recognition study using kernel correlation feature analysis on the FRGCv2 database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abiantun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics Symposium</title>
		<imprint>
			<date type="published" when="2006-09">Sep. 2006. 2006</date>
			<pubPlace>Maryland, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Limits on super-resolution and how to break them</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1167" to="1183" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Face hallucination: theory and practice</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="134" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An example-based face hallucination method for single-frame, low-resolution facial images</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1806" to="1816" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Recognition of low-resolution faces using multiple still images and multiple cameras</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Hennings-Yeomans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 2nd Int. Conf. on Biometrics: Theory, Systems, and Applications (BTAS)</title>
		<meeting>IEEE 2nd Int. Conf. on Biometrics: Theory, Systems, and Applications (BTAS)<address><addrLine>Virginia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008. 2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robust low-resolution face identification and verification using high-resolution features</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Hennings-Yeomans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Image Processing (ICIP)</title>
		<meeting>IEEE Int. Conf. on Image essing (ICIP)<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009. 2009</date>
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Simultaneous super-resolution and recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Hennings-Yeomans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<pubPlace>Pittsburgh, Pennsylvania, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning the relationship between high and low resolution images in kernel space for face super resolution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 20th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IAPR/IEEE 20th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010. 2010</date>
			<biblScope unit="page" from="1152" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Low resolution face recognition based on support vector data description</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1809" to="1812" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Face reconstruction with low resolution facial images feature vector projection in kernel space</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 18th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IAPR/IEEE 18th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08">Aug. 2006. 2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1179" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Using colour local binary pattern features for face recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 17th Int. Conf. on Image Processing (ICIP)</title>
		<meeting>IEEE 17th Int. Conf. on Image essing (ICIP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010. 2010</date>
			<biblScope unit="page" from="4541" to="4544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Boosting color feature selection for color face recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1425" to="1434" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A comparative study of preprocessing mismatch effects in color image based face recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="412" to="430" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Recognition of blurred faces using local phase quantization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ojansivu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 19th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IAPR/IEEE 19th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>Tampa, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008. 2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Coupled metric learning for face recognition with degraded images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Machine Learning</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="220" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pose-robust recognition of low-resolution face images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Colorado Springs, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011. 2011</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Pose-robust recognition of low-resolution face images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>accepted for future publication</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Generalized face super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="873" to="886" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Super-resolution restoration of facial images in video</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 18th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IAPR/IEEE 18th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08">Aug. 2006. 2006</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="342" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Extraction of high-resolution frames from video sequences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="996" to="1011" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Joint MAP registration and high-resolution image estimation using a sequence of undersampled images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1621" to="1633" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Restoration of single super-resolution image from several blurred, noisy and down-sampled measured images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feuer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1646" to="1658" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Superresolution of face images using kernel PCA-based prior</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="888" to="892" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">An improved two-step approach to hallucinating faces</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 3rd Int. Conf. on Image and Graphics (ICIG)</title>
		<meeting>IEEE 3rd Int. Conf. on Image and Graphics (ICIG)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12">Dec. 2004. 2004</date>
			<biblScope unit="page" from="298" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">High-zoom video hallucination by exploiting spatio-temporal regularities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dedeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>August</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning-based face hallucination in DCT domain</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Super-resolution from multiple views using learnt image models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Capel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning lowlevel vision</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Hallucinating faces: LPH superresolution and neighbor reconstruction for residue compensation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3178" to="3194" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Hallucinating face by position-patch</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2224" to="2236" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">From local pixel structure to global image super-resolution: a new face hallucination framework</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="445" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fundamental limits of reconstructionbased super-resolution algorithms under local translation</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Extracting a good quality frontal face image from a low-resolution video sequence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1353" to="1362" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Sparse representation-based super-resolution for face recognition at a distance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bilgazyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Efraty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd British Machine Vision Conference (BMVC)</title>
		<meeting>22nd British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011-08">Aug. 2011 (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A manifold approach to face recognition from low quality video across illumination and pose using implicit super-resolution</title>
		<author>
			<persName><forename type="first">O</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 11th Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE 11th Int. Conf. on Computer Vision (ICCV)<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Super-resolution method for face recognition using nonlinear mappings on coherent features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Hallucinating facial images and features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 19th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IAPR/IEEE 19th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>Tampa, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008. 2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Support vector domain description</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1191" to="1199" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Automatic face recognition for film character retrieval in feature length films</title>
		<author>
			<persName><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="581" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Scale invariant face recognition using probabilistic similarity measure</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 19th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IAPR/IEEE 19th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>Tampa, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008. 2008</date>
			<biblScope unit="page" from="3735" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Lowresolution face recognition via sparse representation of patches</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 5th Int. Conf. on Image and Graphics (ICIG)</title>
		<meeting>IEEE 5th Int. Conf. on Image and Graphics (ICIG)<address><addrLine>Xi&apos;an, Shanxi, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009. 2009</date>
			<biblScope unit="page" from="200" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Synthesis-based recognition of low resolution faces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. on Biometrics (IJCB)</title>
		<meeting>Int. Joint Conf. on Biometrics (IJCB)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10">Oct. 2011. 2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Low-resolution face recognition via simultaneous discriminant analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. on Biometrics (IJCB)</title>
		<meeting>Int. Joint Conf. on Biometrics (IJCB)<address><addrLine>Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10">Oct. 2011. 2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Color improves object recognition in normal and low vision</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Wurm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Legge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luebker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Hum. Percept. Perform</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="899" to="911" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Color space normalization: enhancing the discriminating power of color spaces for face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1454" to="1466" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Correlation pattern recognition for face recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1963" to="1976" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A linear discriminant analysis for low resolution face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yeom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 2nd Int. Conf. on Future Generation Communication and Networking Symposia (FGCN)</title>
		<meeting>IEEE 2nd Int. Conf. on Future Generation Communication and Networking Symposia (FGCN)<address><addrLine>Sanya, Hainan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008. 2008</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="230" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A discriminative feature space for detecting and recognizing faces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="797" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Piecewise regularized canonical correlation discrimination for low-resolution face recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Chinese Conf. on Pattern Recognition (CCPR)</title>
		<meeting>Chinese Conf. on Pattern Recognition (CCPR)<address><addrLine>Chongqing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">Oct. 2010. 2010</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A new canonical correlation analysis algorithm with local discrimination</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Gait feature coupling for low-resolution face recognition</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Journal on Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Low-resolution face recognition via color information and regularized coupled mappings</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Chinese Conf. on Pattern Recognition (CCPR)</title>
		<meeting>Chinese Conf. on Pattern Recognition (CCPR)<address><addrLine>Chongqing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">Oct. 2010. 2010</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Two-way multidimensional scaling: a review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>France</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern., Part C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="644" to="661" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Inter-modality face recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">O</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th European Conf. on Computer Vision (ECCV)</title>
		<meeting>9th European Conf. on Computer Vision (ECCV)<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006. 2006</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Coupled spectral regression for matching heterogeneous faces</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Miami Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009. 2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1123" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">An improved coupled spectral regression for heterogeneous face recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 5th Int. Conf. on Biometrics (ICB)</title>
		<meeting>IAPR/IEEE 5th Int. Conf. on Biometrics (ICB)<address><addrLine>New Delhi, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03">Mar. 2012 (2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Scenario-based score fusion for face recognition at a distance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010. 2010</date>
			<biblScope unit="page" from="67" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Low resolution face recognition using combination of diverse classifiers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sadeghnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Soft Computing and Pattern Recognition (SoCPaR)</title>
		<meeting>Int. Conf. on Soft Computing and Pattern Recognition (SoCPaR)</meeting>
		<imprint>
			<date type="published" when="2010-12">Dec. 2010. 2010</date>
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Face recognition at-a-distance using texture, dense-and sparsestereo reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Rara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Elhabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Starr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR/IEEE 20th Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>IAPR/IEEE 20th Int. Conf. on Pattern Recognition (ICPR)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010. 2010</date>
			<biblScope unit="page" from="1221" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Feature selection in face recognition: A sparse representation perspective</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<idno>UCB/EECS-2007-99</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>UC Berkeley</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">The AR-face database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVC Technical Report</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<ptr target="http://cvc.yale.edu/projects/yalefaces/yalefaces.html" />
		<title level="m">The Yale Face Database</title>
		<imprint/>
		<respStmt>
			<orgName>Yale University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">From few to many: illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">The CAS-PEAL large-scale Chinese face database and baseline evaluations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Zhao</surname></persName>
		</author>
		<idno>JDL-TR-04-FR-001</idno>
	</analytic>
	<monogr>
		<title level="j">Chinese Academy of Sciences</title>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
		<respStmt>
			<orgName>ICT-ISVISION</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">The FERET evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rivzvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<ptr target="http://www.nist.gov/humanid/colorferet" />
		<title level="m">Color FERET Face Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">The CMU pose, illumination and expression database</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1615" to="1618" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
			<affiliation>
				<orgName type="collaboration">Multi-PIE</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
			<affiliation>
				<orgName type="collaboration">Multi-PIE</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
			<affiliation>
				<orgName type="collaboration">Multi-PIE</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
			<affiliation>
				<orgName type="collaboration">Multi-PIE</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
			<affiliation>
				<orgName type="collaboration">Multi-PIE</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">SCface-surveillance cameras face database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Delac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grgic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="863" to="879" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">XM2VTSDB: the extended M2VTS database</title>
		<author>
			<persName><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mastas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. on Audio and Video-Based Biometric Person Authentication (AVBPA)</title>
		<meeting>2nd Int. Conf. on Audio and Video-Based Biometric Person Authentication (AVBPA)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<ptr target="http://www.sheffield.ac.uk/eee/research/iel/research/face" />
		<title level="m">The Sheffield (previously UMIST) Face Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Cambridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laboratories</forename><surname>Research</surname></persName>
		</author>
		<ptr target="http://www.uk.research.att.com/facedatabase.html" />
		<title level="m">The ORL Face Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Performance evaluation of face recognition algorithms on the Asian face database, KFDB</title>
		<author>
			<persName><forename type="first">B.-W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. on Audio and Video-Based Biometric Person Authentication (AVBPA)</title>
		<meeting>4th Int. Conf. on Audio and Video-Based Biometric Person Authentication (AVBPA)<address><addrLine>Guildford, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003. 2003</date>
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">The CMU Motion of Body (MoBo) database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno>CMU-RI-TR-01-18</idno>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Video-based face recognition using probabilistic appearance manifolds</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003. 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">The CLEAR 2006 evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEAR 2006</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4122</biblScope>
			<biblScope unit="page" from="218" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: a database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">How far can you get with a modern face recognition test set using only simple features?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Miami Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009. 2009</date>
			<biblScope unit="page" from="2591" to="2598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Improving long range and high magnification face recognition: database acquisition, evaluation, and enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="125" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Evaluation of state-of-the-art algorithms for remote face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 17th Int. Conf. on Image Processing (ICIP)</title>
		<meeting>IEEE 17th Int. Conf. on Image essing (ICIP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010. 2010</date>
			<biblScope unit="page" from="1581" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Recognition of digital images of the human face at ultra low resolution via illumination spaces</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Asian Conf. on Computer Vision (ACCV)</title>
		<meeting>8th Asian Conf. on Computer Vision (ACCV)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11">Nov. 2007. 2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="733" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Identifying noncooperative subjects at a distance using face images and inferred three-dimensional face models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fidaleo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern., Part A, Syst. Hum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="24" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">From still image to video-based face recognition: an experimental analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 6th Int. Conf. on Automatic Face and Gesture Recognition (FG)</title>
		<meeting>IEEE 6th Int. Conf. on Automatic Face and Gesture Recognition (FG)<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05">May 2004. 2004</date>
			<biblScope unit="page" from="813" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Multi-frame super-resolution for face recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 1st Int. Conf. on Biometrics: Theory, Applications and Systems (BTAS)</title>
		<meeting>IEEE 1st Int. Conf. on Biometrics: Theory, Applications and Systems (BTAS)<address><addrLine>Crystal City, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007. 2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Bypassing synthesis: PLS for face recognition with pose, low-resolution and sketch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
