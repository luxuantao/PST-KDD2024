<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Graph Collaborative Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-05-20">20 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<email>xiangwang@u.nus.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<email>xiangnanhe@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
							<email>fulifeng93@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Graph Collaborative Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-20">20 May 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3331184.3331267</idno>
					<idno type="arXiv">arXiv:1905.08108v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative Filtering</term>
					<term>Recommendation</term>
					<term>High-order Connectivity</term>
					<term>Embedding Propagation</term>
					<term>Graph Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.</p><p>In this work, we propose to integrate the user-item interactionsmore specifically the bipartite graph structure -into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the useritem graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in useritem graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec <ref type="bibr" target="#b38">[39]</ref> and Collaborative Memory Network <ref type="bibr" target="#b4">[5]</ref>. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at https://github.com/ xiangwang1223/neural_graph_collaborative_filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Personalized recommendation is ubiquitous, having been applied to many online services such as E-commerce, advertising, and social media. At its core is estimating how likely a user will adopt an item based on the historical interactions like purchases and clicks. Collaborative filtering (CF) addresses it by assuming that behaviorally similar users would exhibit similar preference on items. To implement the assumption, a common paradigm is to parameterize users and items for reconstructing historical interactions, and predict user preference based on the parameters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Generally speaking, there are two key components in learnable CF models -1) embedding, which transforms users and items to vectorized representations, and 2) interaction modeling, which reconstructs historical interactions based on the embeddings. For example, matrix factorization (MF) directly embeds user/item ID as an vector and models user-item interaction with inner product <ref type="bibr" target="#b19">[20]</ref>; collaborative deep learning extends the MF embedding function by integrating the deep representations learned from rich side information of items <ref type="bibr" target="#b29">[30]</ref>; neural collaborative filtering models replace the MF interaction function of inner product with nonlinear neural networks <ref type="bibr" target="#b13">[14]</ref>; and translation-based CF models instead use Euclidean distance metric as the interaction function <ref type="bibr" target="#b27">[28]</ref>, among others.</p><p>Despite their effectiveness, we argue that these methods are not sufficient to yield satisfactory embeddings for CF. The key reason is that the embedding function lacks an explicit encoding of the crucial collaborative signal, which is latent in user-item interactions to reveal the behavioral similarity between users (or items). To be more specific, most existing methods build the embedding function with the descriptive features only (e.g., ID and attributes), without considering the user-item interactions -which are only used to define the objective function for model training <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>. As a result, when the embeddings are insufficient in capturing CF, the methods have to rely on the interaction function to make up for the deficiency of suboptimal embeddings <ref type="bibr" target="#b13">[14]</ref>.</p><p>While intuitively useful to integrate user-item interactions into the embedding function, it is non-trivial to do it well. In Figure <ref type="figure">1</ref>: An illustration of the user-item interaction graph and the high-order connectivity. The node u 1 is the target user to provide recommendations for.</p><p>particular, the scale of interactions can easily reach millions or even larger in real applications, making it difficult to distill the desired collaborative signal. In this work, we tackle the challenge by exploiting the high-order connectivity from useritem interactions, a natural way that encodes collaborative signal in the interaction graph structure.</p><p>Running Example. Figure <ref type="figure">1</ref> illustrates the concept of high-order connectivity. The user of interest for recommendation is u 1 , labeled with the double circle in the left subfigure of user-item interaction graph. The right subfigure shows the tree structure that is expanded from u 1 . The high-order connectivity denotes the path that reaches u 1 from any node with the path length l larger than 1. Such highorder connectivity contains rich semantics that carry collaborative signal. For example, the path u 1 ← i 2 ← u 2 indicates the behavior similarity between u 1 and u 2 , as both users have interacted with i 2 ; the longer path u 1 ← i 2 ← u 2 ← i 4 suggests that u 1 is likely to adopt i 4 , since her similar user u 2 has consumed i 4 before. Moreover, from the holistic view of l = 3, item i 4 is more likely to be of interest to u 1 than item i 5 , since there are two paths connecting &lt;i 4 , u 1 &gt;, while only one path connects &lt;i 5 , u 1 &gt;.</p><p>Present Work. We propose to model the high-order connectivity information in the embedding function. Instead of expanding the interaction graph as a tree which is complex to implement, we design a neural network method to propagate embeddings recursively on the graph. This is inspired by the recent developments of graph neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>, which can be seen as constructing information flows in the embedding space. Specifically, we devise an embedding propagation layer, which refines a user's (or an item's) embedding by aggregating the embeddings of the interacted items (or users). By stacking multiple embedding propagation layers, we can enforce the embeddings to capture the collaborative signal in high-order connectivities. Taking Figure <ref type="figure">1</ref> as an example, stacking two layers captures the behavior similarity of u 1 ← i 2 ← u 2 , stacking three layers captures the potential recommendations of u 1 ← i 2 ← u 2 ← i 4 , and the strength of the information flow (which is estimated by the trainable weights between layers) determines the recommendation priority of i 4 and i 5 . We conduct extensive experiments on three public benchmarks to verify the rationality and effectiveness of our Neural Graph Collaborative Filtering (NGCF) method.</p><p>Lastly, it is worth mentioning that although the high-order connectivity information has been considered in a very recent method named HOP-Rec <ref type="bibr" target="#b38">[39]</ref>, it is only exploited to enrich the training data. Specifically, the prediction model of HOP-Rec remains to be MF, while it is trained by optimizing a loss that is augmented with high-order connectivities. Distinct from HOP-Rec, we contribute a new technique to integrate high-order connectivities into the prediction model, which empirically yields better embeddings than HOP-Rec for CF.</p><p>To summarize, this work makes the following main contributions: Extensive results demonstrate the state-of-the-art performance of NGCF and its effectiveness in improving the embedding quality with neural embedding propagation.</p><formula xml:id="formula_0">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head><p>We now present the proposed NGCF model, the architecture of which is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. There are three components in the framework: (1) an embedding layer that offers and initialization of user embeddings and item embeddings; (2) multiple embedding propagation layers that refine the embeddings by injecting highorder connectivity relations; and (3) the prediction layer that aggregates the refined embeddings from different propagation layers and outputs the affinity score of a user-item pair. Finally, we discuss the time complexity of NGCF and the connections with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding Layer</head><p>Following mainstream recommender models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>, we describe a user u (an item i) with an embedding vector e u ∈ R d (e i ∈ R d ), where d denotes the embedding size. This can be seen as building a parameter matrix as an embedding look-up table:</p><formula xml:id="formula_1">E = [ e u 1 , • • • , e u N users embeddings , e i 1 , • • • , e i M item embeddings ].<label>(1)</label></formula><p>It is worth noting that this embedding table serves as an initial state for user embeddings and item embeddings, to be optimized in an end-to-end fashion. In traditional recommender models like MF and neural collaborative filtering <ref type="bibr" target="#b13">[14]</ref>, these ID embeddings are directly fed into an interaction layer (or operator) to achieve the prediction score. In contrast, in our NGCF framework, we refine the embeddings by propagating them on the user-item interaction graph. This leads to more effective embeddings for recommendation, since the embedding refinement step explicitly injects collaborative signal into embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embedding Propagation Layers</head><p>Next we build upon the message-passing architecture of GNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37]</ref> in order to capture CF signal along the graph structure and refine the embeddings of users and items. We first illustrate the design of one-layer propagation, and then generalize it to multiple successive layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.1</head><p>First-order Propagation. Intuitively, the interacted items provide direct evidence on a user's preference <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref>; analogously, the users that consume an item can be treated as the item's features and used to measure the collaborative similarity of two items. We build upon this basis to perform embedding propagation between the connected users and items, formulating the process with two major operations: message construction and message aggregation.</p><p>Message Construction. For a connected user-item pair (u, i), we define the message from i to u as:</p><formula xml:id="formula_2">m u←i = f (e i , e u , p ui ),<label>(2)</label></formula><p>where m u←i is the message embedding (i.e., the information to be propagated). f (•) is the message encoding function, which takes embeddings e i and e u as input, and uses the coefficient p ui to control the decay factor on each propagation on edge (u, i).</p><p>In this work, we implement f (•) as:</p><formula xml:id="formula_3">m u←i = 1 |N u ||N i | W 1 e i + W 2 (e i ⊙ e u ) ,<label>(3)</label></formula><p>where W 1 , W 2 ∈ R d ′ ×d are the trainable weight matrices to distill useful information for propagation, and d ′ is the transformation size. Distinct from conventional graph convolution networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref> that consider the contribution of e i only, here we additionally encode the interaction between e i and e u into the message being passed via e i ⊙ e u , where ⊙ denotes the element-wise product. This makes the message dependent on the affinity between e i and e u , e.g., passing more messages from the similar items. This not only increases the model representation ability, but also boosts the performance for recommendation (evidences in our experiments Section 4.4.2). Following the graph convolutional network <ref type="bibr" target="#b17">[18]</ref>, we set p ui as the graph Laplacian norm 1/ |N u ||N i |, where N u and N i denote the first-hop neighbors of user u and item i. From the viewpoint of representation learning, p ui reflects how much the historical item contributes the user preference. From the viewpoint of message passing, p ui can be interpreted as a discount factor, considering the messages being propagated should decay with the path length.</p><p>Message Aggregation. In this stage, we aggregate the messages propagated from u's neighborhood to refine u's representation. Specifically, we define the aggregation function as:</p><formula xml:id="formula_4">e (1)</formula><formula xml:id="formula_5">u = LeakyReLU m u←u + i ∈N u m u←i ,<label>(4)</label></formula><p>where e</p><p>u denotes the representation of user u obtained after the first embedding propagation layer. The activation function of LeakyReLU <ref type="bibr" target="#b22">[23]</ref> allows messages to encode both positive and small negative signals. Note that in addition to the messages propagated from neighbors N u , we take the self-connection of u into consideration: m u←u = W 1 e u , which retains the information of original features (W 1 is the weight matrix shared with the one used in Equation ( <ref type="formula" target="#formula_3">3</ref>)). Analogously, we can obtain the representation e <ref type="bibr" target="#b0">(1)</ref> i for item i by propagating information from its connected users. To summarize, the advantage of the embedding propagation layer lies in explicitly exploiting the first-order connectivity information to relate user and item representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.2</head><p>High-order Propagation. With the representations augmented by first-order connectivity modeling, we can stack more embedding propagation layers to explore the high-order connectivity information. Such high-order connectivities are crucial to encode the collaborative signal to estimate the relevance score between a user and item.</p><p>By stacking l embedding propagation layers, a user (and an item) is capable of receiving the messages propagated from its l-hop neighbors. As Figure <ref type="figure" target="#fig_1">2</ref> displays, in the l-th step, the representation of user u is recursively formulated as:</p><formula xml:id="formula_7">e (l ) u = LeakyReLU m (l ) u←u + i ∈N u m (l ) u←i ,<label>(5)</label></formula><p>wherein the messages being propagated are defined as follows,</p><formula xml:id="formula_8">m (l ) u←i = p ui W (l ) 1 e (l −1) i + W (l ) 2 (e (l −1) i ⊙ e (l −1) u ) , m (l ) u←u = W (l ) 1 e (l −1) u ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">W (l ) 1 , W<label>(l )</label></formula><p>2 , ∈ R d l ×d l −1 are the trainable transformation matrices, and d l is the transformation size; e (l −1) i is the item representation generated from the previous message-passing steps, memorizing the messages from its (l-1)-hop neighbors. It further contributes to the representation of user u at layer l. Analogously, we can obtain the representation for item i at the layer l.</p><p>As Figure <ref type="figure" target="#fig_2">3</ref> shows, the collaborative signal like u 1 ← i 2 ← u 2 ← i 4 can be captured in the embedding propagation process. Furthermore, the message from i 4 is explicitly encoded in e</p><p>(3) u 1 (indicated by the red line). As such, stacking multiple embedding propagation layers seamlessly injects collaborative signal into the representation learning process.</p><p>Propagation Rule in Matrix Form. To offer a holistic view of embedding propagation and facilitate batch implementation, we provide the matrix form of the layer-wise propagation rule (equivalent to Equations ( <ref type="formula" target="#formula_7">5</ref>) and ( <ref type="formula" target="#formula_8">6</ref>)):</p><formula xml:id="formula_10">E (l ) = LeakyReLU (L + I)E (l −1) W (l ) 1 + LE (l −1) ⊙ E (l −1) W (l ) 2 , (7)</formula><p>where E (l ) ∈ R (N +M )×d l are the representations for users and items obtained after l steps of embedding propagation. E (0) is set as E at the initial message-passing iteration, that is e (0) u = e u and e (0) i = e i ; and I denote an identity matrix. L represents the Laplacian matrix for the user-item graph, which is formulated as:</p><formula xml:id="formula_11">L = D − 1 2 AD − 1 2 and A = 0 R R ⊤ 0 ,<label>(8)</label></formula><p>where R ∈ R N ×M is the user-item interaction matrix, and 0 is allzero matrix; A is the adjacency matrix and D is the diagonal degree matrix, where the t-th diagonal element</p><formula xml:id="formula_12">D t t = |N t |; as such, the nonzero off-diagonal entry L ui = 1/ |N u ||N i |, which is equal to p ui used in Equation (3)</formula><p>. By implementing the matrix-form propagation rule, we can simultaneously update the representations for all users and items in a rather efficient way. It allows us to discard the node sampling procedure, which is commonly used to make graph convolution network runnable on large-scale graph <ref type="bibr" target="#b24">[25]</ref>. We will analyze the complexity in Section 2.5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Prediction</head><p>After propagating with L layers, we obtain multiple representations for user u, namely {e</p><formula xml:id="formula_13">(1) u , • • • , e (L)</formula><p>u }. Since the representations obtained in different layers emphasize the messages passed over different connections, they have different contributions in reflecting user preference. As such, we concatenate them to constitute the final embedding for a user; we do the same operation on items, concatenating the item representations {e</p><formula xml:id="formula_14">(1) i , • • • , e (L)</formula><p>i } learned by different layers to get the final item embedding:</p><formula xml:id="formula_15">e * u = e (0) u ∥• • • ∥e (L) u , e * i = e (0) i ∥• • • ∥e (L) i ,<label>(9)</label></formula><p>where ∥ is the concatenation operation. By doing so, we not only enrich the initial embeddings with embedding propagation layers, but also allow controlling the range of propagation by adjusting L. Note that besides concatenation, other aggregators can also be applied, such as weighted average, max pooling, LSTM, etc., which imply different assumptions in combining the connectivities of different orders. The advantage of using concatenation lies in its simplicity, since it involves no additional parameters to learn, and it has been shown quite effectively in a recent work of graph neural networks <ref type="bibr" target="#b36">[37]</ref>, which refers to layer-aggregation mechanism. Finally, we conduct the inner product to estimate the user's preference towards the target item:</p><formula xml:id="formula_16">ŷNGCF (u, i) = e * u ⊤ e * i .<label>(10)</label></formula><p>In this work, we emphasize the embedding function learning thus only employ the simple interaction function of inner product. Other more complicated choices, such as neural network-based interaction functions <ref type="bibr" target="#b13">[14]</ref>, are left to explore in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimization</head><p>To learn model parameters, we optimize the pairwise BPR loss <ref type="bibr" target="#b25">[26]</ref>, which has been intensively used in recommender systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>It considers the relative order between observed and unobserved user-item interactions. Specifically, BPR assumes that the observed interactions, which are more reflective of a user's preferences, should be assigned higher prediction values than unobserved ones. The objective function is as follows,</p><formula xml:id="formula_17">Loss = (u,i, j)∈ O − ln σ ( ŷui − ŷuj ) + λ ∥Θ∥ 2 2 ,<label>(11)</label></formula><p>where </p><formula xml:id="formula_18">O = {(u, i, j)|(u, i) ∈ R + , (u, j) ∈ R − }</formula><p>2 } L l =1 } denotes all trainable model parameters, and λ controls the L 2 regularization strength to prevent overfitting. We adopt mini-batch Adam <ref type="bibr" target="#b16">[17]</ref> to optimize the prediction model and update the model parameters. In particular, for a batch of randomly sampled triples (u, i, j) ∈ O, we establish their representations [e (0) , • • • , e (L) ] after L steps of propagation, and then update model parameters by using the gradients of the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Model Size.</head><p>It is worth pointing out that although NGCF obtains an embedding matrix (E (l ) ) at each propagation layer l, it only introduces very few parameters -two weight matrices of size d l × d l −1 . Specifically, these embedding matrices are derived from the embedding look-up table E (0) , with the transformation based on the user-item graph structure and weight matrices. As such, compared to MF -the most concise embeddingbased recommender model, our NGCF uses only 2Ld l d l −1 more parameters. Such additional cost on model parameters is almost negligible, considering that L is usually a number smaller than 5, and d l is typically set as the embedding size, which is much smaller than the number of users and items. For example, on our experimented Gowalla dataset (20K users and 40K items), when the embedding size is 64 and we use 3 propagation layers of size 64 × 64, MF has 4.5 million parameters, while our NGCF uses only 0.024 million additional parameters. To summarize, NGCF uses very few additional model parameters to achieve the high-order connectivity modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.4.2</head><p>Message and Node Dropout. Although deep learning models have strong representation ability, they usually suffer from overfitting. Dropout is an effective solution to prevent neural networks from overfitting. Following the prior work on graph convolutional network <ref type="bibr" target="#b28">[29]</ref>, we propose to adopt two dropout techniques in NGCF: message dropout and node dropout. Message dropout randomly drops out the outgoing messages. Specifically, we drop out the messages being propagated in Equation ( <ref type="formula" target="#formula_8">6</ref>), with a probability p 1 . As such, in the l-th propagation layer, only partial messages contribute to the refined representations. We also conduct node dropout to randomly block a particular node and discard all its outgoing messages. For the l-th propagation layer, we randomly drop (M + N )p 2 nodes of the Laplacian matrix, where p 2 is the dropout ratio.</p><p>Note that dropout is only used in training, and must be disabled during testing. The message dropout endows the representations more robustness against the presence or absence of single connections between users and items, and the node dropout focuses on reducing the influences of particular users or items. We perform experiments to investigate the impact of message dropout and node dropout on NGCF in Section 4.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Discussions</head><p>In the subsection, we first show how NGCF generalizes SVD++ <ref type="bibr" target="#b18">[19]</ref>.</p><p>In what follows, we analyze the time complexity of NGCF.</p><p>2.5.1 NGCF Generalizes SVD++. SVD++ can be viewed as a special case of NGCF with no high-order propagation layer. In particular, we set L to one. Within the propagation layer, we disable the transformation matrix and nonlinear activation function.</p><p>Thereafter, e</p><p>u and e <ref type="bibr" target="#b0">(1)</ref> i are treated as the final representations for user u and item i, respectively. We term this simplified model as NGCF-SVD, which can be formulated as:</p><formula xml:id="formula_21">ŷNGCF-SVD = (e u + i ′ ∈N u p ui ′ e i ′ ) ⊤ (e i + u ′ ∈N i p iu ′ e i ).<label>(12)</label></formula><p>Clearly, by setting p ui ′ and p u ′ i as 1/ |N u | and 0 separately, we can exactly recover SVD++ model. Moreover, another widely-used item-based CF model, FISM <ref type="bibr" target="#b15">[16]</ref>, can be also seen as a special case of NGCF, wherein p iu ′ in Equation ( <ref type="formula" target="#formula_21">12</ref>) is set as 0. </p><formula xml:id="formula_22">L l =1 |R + |d l d l −1 + L l =1 |R + |d l ).</formula><p>Empirically, under the same experimental settings (as explained in Section 4), MF and NGCF cost around 20s and 80s per epoch on Gowalla dataset for training, respectively; during inference, the time costs of MF and NGCF are 80s and 260s for all testing instances, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>We review existing work on model-based CF, graph-based CF, and graph neural network-based methods, which are most relevant with this work. Here we highlight the differences with our NGCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model-based CF Methods</head><p>Modern recommender systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref> parameterize users and items by vectorized representations and reconstruct user-item interaction data based on model parameters. For example, MF <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref> projects the ID of each user and item as an embedding vector, and conducts inner product between them to predict an interaction. To enhance the embedding function, much effort has been devoted to incorporate side information like item content <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>, social relations <ref type="bibr" target="#b32">[33]</ref>, item relations <ref type="bibr" target="#b35">[36]</ref>, user reviews <ref type="bibr" target="#b2">[3]</ref>, and external knowledge graph <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref>. While inner product can force user and item embeddings of an observed interaction close to each other, its linearity makes it insufficient to reveal the complex and nonlinear relationships between users and items <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Towards this end, recent efforts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref> focus on exploiting deep learning techniques to enhance the interaction function, so as to capture the nonlinear feature interactions between users and items. For instance, neural CF models, such as NeuMF <ref type="bibr" target="#b13">[14]</ref>, employ nonlinear neural networks as the interaction function; meanwhile, translationbased CF models, such as LRML <ref type="bibr" target="#b27">[28]</ref>, instead model the interaction strength with Euclidean distance metrics.</p><p>Despite great success, we argue that the design of the embedding function is insufficient to yield optimal embeddings for CF, since the CF signals are only implicitly captured. Summarizing these methods, the embedding function transforms the descriptive features (e.g., ID and attributes) to vectors, while the interaction function serves as a similarity measure on the vectors. Ideally, when user-item interactions are perfectly reconstructed, the transitivity property of behavior similarity could be captured. However, such transitivity effect showed in the Running Example is not explicitly encoded, thus there is no guarantee that the indirectly connected users and items are close in the embedding space. Without an explicit encoding of the CF signals, it is hard to obtain embeddings that meet the desired properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph-Based CF Methods</head><p>Another line of research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref> exploits the user-item interaction graph to infer user preference. Early efforts, such as ItemRank <ref type="bibr" target="#b6">[7]</ref> and BiRank <ref type="bibr" target="#b11">[12]</ref>, adopt the idea of label propagation to capture the CF effect. To score items for a user, these methods define the labels as her interacted items, and propagate the labels on the graph. As the recommendation scores are obtained based on the structural reachness (which can be seen as a kind of similarity) between the historical items and the target item, these methods essentially belong to neighbor-based methods. However, these methods are conceptually inferior to model-based CF methods, since there lacks model parameters to optimize the objective function of recommendation.</p><p>The recently proposed method HOP-Rec <ref type="bibr" target="#b38">[39]</ref> alleviates the problem by combining graph-based with embedding-based method. It first performs random walks to enrich the interactions of a user with multi-hop connected items. Then it trains MF with BPR objective based on the enriched user-item interaction data to build the recommender model. The superior performance of HOP-Rec over MF provides evidence that incorporating the connectivity information is beneficial to obtain better embeddings in capturing the CF effect. However, we argue that HOP-Rec does not fully explore the high-order connectivity, which is only utilized to enrich the training data 1 , rather than directly contributing to the 1 The enriched trained data can be seen as a regularizer to the original training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Convolutional Networks</head><p>By devising a specialized graph convolution operation on useritem interaction graph (cf. Equation ( <ref type="formula" target="#formula_3">3</ref>)), we make NGCF effective in exploiting the CF signal in high-order connectivities. Here we discuss existing recommendation methods that also employ graph convolution operations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. GC-MC <ref type="bibr" target="#b28">[29]</ref> applies the graph convolution network (GCN) <ref type="bibr" target="#b17">[18]</ref> on user-item graph, however it only employs one convolutional layer to exploit the direct connections between users and items. Hence it fails to reveal collaborative signal in high-order connectivities. PinSage <ref type="bibr" target="#b40">[41]</ref> is an industrial solution that employs multiple graph convolution layers on item-item graph for Pinterest image recommendation. As such, the CF effect is captured on the level of item relations, rather than the collective user behaviors. SpectralCF <ref type="bibr" target="#b41">[42]</ref> proposes a spectral convolution operation to discover all possible connectivity between users and items in the spectral domain. Through the eigen-decomposition of graph adjacency matrix, it can discover the connections between a user-item pair. However, the eigen-decomposition causes a high computational complexity, which is very time-consuming and difficult to support large-scale recommendation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We perform experiments on three real-world datasets to evaluate our proposed method, especially the embedding propagation layer.</p><p>We aim to answer the following research questions:</p><p>• RQ1: How does NGCF perform as compared with state-of-the-art CF methods? • RQ2: How do different hyper-parameter settings (e.g., depth of layer, embedding propagation layer, layer-aggregation mechanism, message dropout, and node dropout) affect NGCF? • RQ3: How do the representations benefit from the high-order connectivity?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Description</head><p>To evaluate the effectiveness of NGCF, we conduct experiments on three benchmark datasets: Gowalla, Yelp2018, and Amazon-book, which are publicly accessible and vary in terms of domain, size, and sparsity. We summarize the statistics of three datasets in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Gowalla: This is the check-in dataset <ref type="bibr" target="#b20">[21]</ref> obtained from Gowalla, where users share their locations by checking-in. To ensure the quality of the dataset, we use the 10-core setting <ref type="bibr" target="#b9">[10]</ref>, i.e., retaining users and items with at least ten interactions. Yelp2018: This dataset is adopted from the 2018 edition of the Yelp challenge. Wherein, the local businesses like restaurants and bars are viewed as the items. We use the same 10-core setting in order to ensure data quality. Amazon-book: Amazon-review is a widely used dataset for product recommendation <ref type="bibr" target="#b8">[9]</ref>. We select Amazon-book from the collection. Similarly, we use the 10-core setting to ensure that each user and item have at least ten interactions.</p><p>For each dataset, we randomly select 80% of historical interactions of each user to constitute the training set, and treat the remaining as the test set. From the training set, we randomly select 10% of interactions as validation set to tune hyper-parameters. For each observed user-item interaction, we treat it as a positive instance, and then conduct the negative sampling strategy to pair it with one negative item that the user did not consume before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluation Metrics.</head><p>For each user in the test set, we treat all the items that the user has not interacted with as the negative items. Then each method outputs the user's preference scores over all the items, except the positive ones used in the training set. To evaluate the effectiveness of top-K recommendation and preference ranking, we adopt two widely-used evaluation protocols <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref>: recall@K and ndcg@K. By default, we set K = 20. We report the average metrics for all users in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Baselines.</head><p>To demonstrate the effectiveness, we compare our proposed NGCF with the following methods:</p><p>• MF <ref type="bibr" target="#b25">[26]</ref>: This is matrix factorization optimized by the Bayesian personalized ranking (BPR) loss, which exploits the user-item direct interactions only as the target value of interaction function. • NeuMF <ref type="bibr" target="#b13">[14]</ref>: The method is a state-of-the-art neural CF model which uses multiple hidden layers above the element-wise and concatenation of user and item embeddings to capture their nonlinear feature interactions. Especially, we employ two-layered plain architecture, where the dimension of each hidden layer keeps the same. • CMN <ref type="bibr" target="#b4">[5]</ref>: It is a state-of-the-art memory-based model, where the user representation attentively combines the memory slots of neighboring users via the memory layers. Note that the firstorder connections are used to find similar users who interacted with the same items. • HOP-Rec <ref type="bibr" target="#b38">[39]</ref>: This is a state-of-the-art graph-based model, where the high-order neighbors derived from random walks are exploited to enrich the user-item interaction data. • PinSage <ref type="bibr" target="#b40">[41]</ref>: PinSage is designed to employ GraphSAGE <ref type="bibr" target="#b7">[8]</ref> on item-item graph. In this work, we apply it on user-item interaction graph. Especially, we employ two graph convolution layers as suggested in <ref type="bibr" target="#b40">[41]</ref>, and the hidden dimension is set equal to the embedding size. • GC-MC <ref type="bibr" target="#b28">[29]</ref>: This model adopts GCN <ref type="bibr" target="#b17">[18]</ref> encoder to generate the representations for users and items, where only the first-order neighbors are considered. Hence one graph convolution layer, where the hidden dimension is set as the embedding size, is used as suggested in <ref type="bibr" target="#b28">[29]</ref>.</p><p>We also tried SpectralCF <ref type="bibr" target="#b41">[42]</ref> but found that the eigen-decomposition leads to high time cost and resource cost, especially when the number of users and items is large. Hence, although it achieved promising performance in small datasets, we did not select it for comparison. For fair comparison, all methods optimize the BPR loss as shown in Equation <ref type="bibr" target="#b10">(11)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Parameter Settings.</head><p>We implement our NGCF model in Tensorflow. The embedding size is fixed to 64 for all models. For HOP-Rec, we search the steps of random walks in {1, 2, 3} and tune the learning rate in {0.025, 0.020, 0.015, 0.010}. We optimize all models except HOP-Rec with the Adam optimizer, where the batch size is fixed at 1024. In terms of hyperparameters, we apply a grid search for hyperparameters: the learning rate is tuned amongst {0.0001, 0.0005, 0.001, 0.005}, the coefficient of L 2 normalization is searched in {10 −5 , 10 −4 , • • • , 10 1 , 10 2 }, and the dropout ratio in {0.0, 0.1, • • • , 0.8}. Besides, we employ the node dropout technique for GC-MC and NGCF, where the ratio is tuned in {0.0, 0.1, • • • , 0.8}. We use the Xavier initializer <ref type="bibr" target="#b5">[6]</ref> to initialize the model parameters. Moreover, early stopping strategy is performed, i.e., premature stopping if recall@20 on the validation data does not increase for 50 successive epochs. To model the CF signal encoded in thirdorder connectivity, we set the depth of NGCF L as three. Without specification, we show the results of three embedding propagation layers, node dropout ratio of 0.0, and message dropout ratio of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Comparison (RQ1)</head><p>We start by comparing the performance of all the methods, and then explore how the modeling of high-order connectivity improves under the sparse settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Overall Comparison.</head><p>Table <ref type="table" target="#tab_1">2</ref> reports the performance comparison results. We have the following observations:</p><p>• MF achieves poor performance on three datasets. This indicates that the inner product is insufficient to capture the complex relations between users and items, further limiting the performance. NeuMF consistently outperforms MF across all cases, demonstrating the importance of nonlinear feature interactions between user and item embeddings. However, neither MF nor NeuMF explicitly models the connectivity in the embedding learning process, which could easily lead to suboptimal representations. • Compared to MF and NeuMF, the performance of GC-MC verifies that incorporating the first-order neighbors can improve the representation learning. However, in Yelp2018, GC-MC underperforms NeuMF w.r.t. ndcg@20. The reason might be that GC-MC fails to fully explore the nonlinear feature interactions between users and items.</p><p>• CMN generally achieves better performance than GC-MC in most cases. Such improvement might be attributed to the neural attention mechanism, which can specify the attentive weight of each neighboring user, rather than the equal or heuristic weight used in GC-MC. • PinSage slightly underperforms CMN in Gowalla and Amazon-Book, while performing much better in Yelp2018; meanwhile, HOP-Rec generally achieves remarkable improvements in most cases. It makes sense since PinSage introduces high-order connectivity in the embedding function, and HOP-Rec exploits high-order neighbors to enrich the training data, while CMN considers the similar users only. It therefore points to the positive effect of modeling the high-order connectivity or neighbors. • NGCF consistently yields the best performance on all the datasets.</p><p>In particular, NGCF improves over the strongest baselines w.r. Towards this end, we perform experiments over user groups of different sparsity levels. In particular, based on interaction number per user, we divide the test set into four groups, each of which has the same total interactions. Taking Gowalla dataset as an example, the interaction numbers per user are less than 24, 50, 117, and 1014 respectively. Figure <ref type="figure" target="#fig_5">4</ref> illustrates the results w.r.t. ndcg@20 on different user groups in Gowalla, Yelp2018, and Amazon-Book; we see a similar trend for performance w.r.t. recall@20 and omit the part due to the space limitation. We find that:</p><p>• NGCF and HOP-Rec consistently outperform all other baselines on all user groups. It demonstrates that exploiting high-order connectivity greatly facilitates the representation learning for inactive users, as the collaborative signal can be effectively captured. Hence, it might be promising to solve the sparsity issue in recommender systems, and we leave it in future work. • Jointly analyzing Figures <ref type="figure" target="#fig_5">4(a</ref>), 4(b), and 4(c), we observe that the improvements achieved in the first two groups (e.g., 6.78% and 3.75% over the best baselines separately for &lt; 24 and &lt; 50 in Gowalla) are more significant than that of the others (e.g.,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Study of NGCF (RQ2)</head><p>As the embedding propagation layer plays a pivotal role in NGCF, we investigate its impact on the performance. We start by exploring the influence of layer numbers. We then study how the Laplacian matrix (i.e., discounting factor p ui between user u and item i) affects the performance. Moreover, we analyze the influences of key factors, such as node dropout and message dropout ratios. We also study the training process of NGCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.1</head><p>Effect of Layer Numbers. To investigate whether NGCF can benefit from multiple embedding propagation layers, we vary the model depth. In particular, we search the layer numbers in the range of {1, 2, 3, 4}. Table <ref type="table" target="#tab_3">3</ref> summarizes the experimental results, wherein NGCF-3 indicates the model with three embedding propagation layers, and similar notations for others. Jointly analyzing Tables <ref type="table" target="#tab_3">2 and 3</ref>, we have the following observations:</p><p>• Increasing the depth of NGCF substantially enhances the recommendation cases. Clearly, NGCF-2 and NGCF-3 achieve consistent improvement over NGCF-1 across all the board, which considers the first-order neighbors only. We attribute the improvement to the effective modeling of CF effect: collaborative user similarity and collaborative signal are carried by the secondand third-order connectivities, respectively. • When further stacking propagation layer on the top of NGCF-3, we find that NGCF-4 leads to overfitting on Yelp2018 dataset. This might be caused by applying a too deep architecture might introduce noises to the representation learning. The marginal improvements on the other two datasets verifies that conducting three propagation layers is sufficient to capture the CF signal. • When varying the number of propagation layers, NGCF is consistently superior to other methods across three datasets. It again verifies the effectiveness of NGCF, empirically showing that explicit modeling of high-order connectivity can greatly facilitate the recommendation task. To investigate how the embedding propagation (i.e., graph convolution) layer affects the performance, we consider the variants of NGCF-1 that use different layers. In particular, we remove the representation interaction between a node and its neighbor from the message passing function (cf. Equation ( <ref type="formula" target="#formula_3">3</ref>)) and set it as that of PinSage and GC-MC, termed NGCF-1 PinSage and NGCF-1 GC-MC respectively. Moreover, following SVD++, we obtain one variant based on Equations ( <ref type="formula" target="#formula_21">12</ref>), termed NGCF-1 SVD++ . We show the results in Table <ref type="table" target="#tab_4">4</ref> and have the following findings:</p><p>• NGCF-1 is consistently superior to all variants. We attribute the improvements to the representation interactions (i.e., e u ⊙ e i ), which makes messages being propagated dependent on the affinity between e i and e u and functions like the attention mechanism <ref type="bibr" target="#b1">[2]</ref>. Whereas, all variants only take linear transformation into consideration. It hence verifies the rationality and effectiveness of our embedding propagation function. • In most cases, NGCF-1 SVD++ underperforms NGCF-1 PinSage and NGCF-1 GC-MC . It illustrates the importance of messages passed by the nodes themselves and the nonlinear transformation. • Jointly analyzing Tables <ref type="table" target="#tab_4">2 and 4</ref>, we find that, when concatenating all layers' outputs together, NGCF-1 PinSage and NGCF-1 GC-MC achieve better performance than PinSage and GC-MC, respectively. This emphasizes the significance of layer-aggregation mechanism, which is consistent with <ref type="bibr" target="#b36">[37]</ref>.</p><p>4.4.3 Effect of Dropout. Following the prior work <ref type="bibr" target="#b28">[29]</ref>, we employ node dropout and message dropout techniques to prevent NGCF from overfitting. Figure <ref type="figure" target="#fig_7">5</ref> plots the effect of message dropout ratio p 1 and node dropout ratio p 2 against different evaluation protocols on different datasets.</p><p>Between the two dropout strategies, node dropout offers better performance. Taking Gowalla as an example, setting p 2 as 0.2 leads to the highest recall@20 of 0.1514, which is better than that of message dropout 0.1506. One reason might be that dropping out all outgoing messages from particular users and items makes the representations robust against not only the influence of particular   edges, but also the effect of nodes. Hence, node dropout is more effective than message dropout, which is consistent with the findings of prior effort <ref type="bibr" target="#b28">[29]</ref>. We believe this is an interesting finding, which means that node dropout can be an effective strategy to address overfitting of graph neural networks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of High-order Connectivity (RQ3)</head><p>In this section, we attempt to understand how the embedding propagation layer facilitates the representation learning in the embedding space. Towards this end, we randomly selected six users from Gowalla dataset, as well as their relevant items. We observe how their representations are influenced w.r.t. the depth of NGCF. Figures <ref type="figure" target="#fig_12">7(a</ref>) and 7(b) show the visualization of the representations derived from MF (i.e., NGCF-0) and NGCF-3, respectively. Note that the items are from the test set, which are not paired with users in the training phase. There are two key observations:</p><p>• The connectivities of users and items are well reflected in the embedding space, that is, they are embedded into the near part of the space. In particular, the representations of NGCF-3 exhibit discernible clustering, meaning that the points with the same colors (i.e., the items consumed by the same users) tend to form the clusters. • Jointly analyzing the same users (e.g., 12201 and 6880) across  embedding propagation layer is capable of injecting the explicit collaborative signal (via NGCF-3) into the representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this work, we explicitly incorporated collaborative signal into the embedding function of model-based CF. We devised a new framework NGCF, which achieves the target by leveraging high-order connectivities in the user-item integration graph. The key of NGCF is the newly proposed embedding propagation layer, based on which we allow the embeddings of users and items interact with each other to harvest the collaborative signal.</p><p>Extensive experiments on three real-world datasets demonstrate the rationality and effectiveness of injecting the user-item graph structure into the embedding learning process. In future, we will further improve NGCF by incorporating the attention mechanism <ref type="bibr" target="#b1">[2]</ref> to learn variable weights for neighbors during embedding propagation and for the connectivities of different orders. This will be beneficial to model generalization and interpretability. Moreover, we are interested in exploring the adversarial learning <ref type="bibr" target="#b12">[13]</ref> on user/item embedding and the graph structure for enhancing the robustness of NGCF. This work represents an initial attempt to exploit structural knowledge with the message-passing mechanism in model-based CF and opens up new research possibilities. Specifically, there are many other forms of structural information can be useful for understanding user behaviors, such as the cross features <ref type="bibr" target="#b39">[40]</ref> in context-aware and semantics-rich recommendation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, item knowledge graph <ref type="bibr" target="#b30">[31]</ref>, and social networks <ref type="bibr" target="#b32">[33]</ref>. For example, by integrating item knowledge graph with user-item graph, we can establish knowledge-aware connectivities between users and items, which help unveil user decision-making process in choosing items. We hope the development of NGCF is beneficial to the reasoning of user online behavior towards more effective and interpretable recommendation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>We highlight the critical importance of explicitly exploiting the collaborative signal in the embedding function of model-based CF methods. • We propose NGCF, a new recommendation framework based on graph neural network, which explicitly encodes the collaborative signal in the form of high-order connectivities by performing embedding propagation. • We conduct empirical studies on three million-size datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of NGCF model architecture (the arrowed lines present the flow of information). The representations of user u 1 (left) and item i 4 (right) are refined with multiple embedding propagation layers, whose outputs are concatenated to make the final prediction. design of one-layer propagation, and then generalize it to multiple successive layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of third-order embedding propagation for user u 1 . Best view in color.</figDesc><graphic url="image-2.png" coords="3,331.03,83.69,211.85,101.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>denotes the pairwise training data, R + indicates the observed interactions, and R − is the unobserved interactions; σ (•) is the sigmoid function; Θ = {E, {W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2. 5 . 2</head><label>52</label><figDesc>Time Complexity Analysis. As we can see, the layer-wise propagation rule is the main operation. For the l-th propagation layer, the matrix multiplication has computational complexity O(|R + |d l d l −1 ), where |R + | denotes the number of nonzero entires in the Laplacian matrix; and d l and d l −1 are the current and previous transformation size. For the prediction layer, only the inner product is involved, for which the time complexity of the whole training epoch is O( L l =1 |R + |d l ). Therefore, the overall complexity for evaluating NGCF is O(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison over the sparsity distribution of user groups on different datasets. Wherein, the background histograms indicate the number of users involved in each group, and the lines demonstrate the performance w.r.t. ndcg@20.</figDesc><graphic url="image-3.png" coords="8,66.97,88.67,156.37,96.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effect of node dropout and message dropout ratios.</figDesc><graphic url="image-10.png" coords="9,136.12,202.97,75.65,78.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Test performance of each epoch of MF and NGCF.</figDesc><graphic url="image-9.png" coords="9,56.04,202.97,75.65,78.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4. 4 . 4</head><label>44</label><figDesc>Test Performance w.r.t. Epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6</head><label>6</label><figDesc>shows the test performance w.r.t. recall of each epoch of MF and NGCF. Due to the space limitation, we omit the performance w.r.t. ndcg which has the similar trend. We can see that, NGCF exhibits fast convergence than MF on three datasets. It is reasonable since indirectly connected users and items are involved when optimizing the interaction pairs in mini-batch. Such an observation demonstrates the better model capacity of NGCF and the effectiveness of performing embedding propagation in the embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figures 7 (</head><label>7</label><figDesc>a) and 7(b), we find that, when stacking three embedding propagation layers, the embeddings of their historical items tend to be closer. It qualitatively verifies that the proposed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of the learned t-SNE transformed representations derived from MF and NGCF-3. Each star represents a user from Gowalla dataset, while the points with the same color denote the relevant items. Best view in color.</figDesc><graphic url="image-12.png" coords="9,320.95,88.67,116.01,116.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Users #Items #Interactions Density</cell></row><row><cell>Gowalla</cell><cell>29, 858 40, 981</cell><cell>1, 027, 370 0.00084</cell></row><row><cell>Yelp2018</cell><cell>31, 831 40, 841</cell><cell>1, 666, 869 0.00128</cell></row><row><cell cols="2">Amazon-Book 52, 643 91, 599</cell><cell>2, 984, 108 0.00062</cell></row><row><cell cols="3">model's embedding function. Moreover, the performance of HOP-</cell></row><row><cell cols="3">Rec depends heavily on the random walks, which require careful</cell></row><row><cell cols="3">tuning efforts such as a proper setting of decay factor.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall Performance Comparison.</figDesc><table><row><cell></cell><cell cols="2">Gowalla</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell></row><row><cell></cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell></row><row><cell>MF</cell><cell>0.1291</cell><cell>0.1878</cell><cell>0.0317</cell><cell>0.0617</cell><cell>0.0250</cell><cell>0.0518</cell></row><row><cell>NeuMF</cell><cell>0.1326</cell><cell>0.1985</cell><cell>0.0331</cell><cell>0.0840</cell><cell>0.0253</cell><cell>0.0535</cell></row><row><cell>CMN</cell><cell>0.1404</cell><cell>0.2129</cell><cell>0.0364</cell><cell>0.0745</cell><cell>0.0267</cell><cell>0.0516</cell></row><row><cell cols="2">HOP-Rec 0.1399</cell><cell>0.2128</cell><cell>0.0388</cell><cell>0.0857</cell><cell>0.0309</cell><cell>0.0606</cell></row><row><cell>GC-MC</cell><cell>0.1395</cell><cell>0.1960</cell><cell>0.0365</cell><cell>0.0812</cell><cell>0.0288</cell><cell>0.0551</cell></row><row><cell>PinSage</cell><cell>0.1380</cell><cell>0.1947</cell><cell>0.0372</cell><cell>0.0803</cell><cell>0.0283</cell><cell>0.0545</cell></row><row><cell>NGCF</cell><cell cols="6">0.1547  *  0.2237  *  0.0438  *  0.0926  *  0.0344  *  0.0630  *</cell></row><row><cell cols="2">%Improv. 10.18%</cell><cell>5.07%</cell><cell>12.88%</cell><cell>8.05%</cell><cell>11.32%</cell><cell>3.96%</cell></row><row><cell>p-value</cell><cell cols="6">1.01e-4 5.38e-3 4.05e-3 2.00e-4 4.34e-2 7.26e-3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>t. recall@20 by 10.18%, 12.88%, and 11.32% in Gowalla, Yelp2018, and Amazon-Book, respectively. By stacking multiple embedding propagation layers, NGCF is capable of exploring the high-order connectivity in an explicit way, while CMN and GC-MC only utilize the first-order neighbors to guide the representation learning. This verifies the importance of capturing collaborative signal in the embedding function. Moreover, compared with PinSage, NGCF considers multi-grained representations to infer user preference, while PinSage only uses the output of the last layer. This demonstrates that different propagation layers encode different information in the representations. And the improvements over HOP-Rec indicate that explicit encoding CF in the embedding function can achieve better representations. We conduct one-sample t-tests and p-value &lt; 0.05 indicates that the improvements of NGCF over the strongest baseline are statistically significant.</figDesc><table><row><cell>4.3.2 Performance Comparison w.r.t. Interaction Sparsity</cell></row><row><cell>Levels. The sparsity issue usually limits the expressiveness of</cell></row><row><cell>recommender systems, since few interactions of inactive users are</cell></row><row><cell>insufficient to generate high-quality representations. We investigate</cell></row><row><cell>whether exploiting connectivity information helps to alleviate this</cell></row><row><cell>issue.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effect of embedding propagation layer numbers (L).</figDesc><table><row><cell cols="2">Gowalla</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell></row><row><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell></row><row><cell cols="6">NGCF-1 0.1511 0.2218 0.0417 0.0889 0.0315 0.0618</cell></row><row><cell cols="6">NGCF-2 0.1535 0.2238 0.0429 0.0909 0.0319 0.0622</cell></row><row><cell cols="6">NGCF-3 0.1547 0.2237 0.0438 0.0926 0.0344 0.0630</cell></row><row><cell cols="6">NGCF-4 0.1560 0.2240 0.0427 0.0907 0.0342 0.0636</cell></row><row><cell cols="6">0.49% for &lt; 117 Gowalla groups). It verifies that the embedding</cell></row><row><cell cols="6">propagation is beneficial to the relatively inactive users.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effect of graph convolution layers. GC-MC 0.1451 0.2165 0.0369 0.0812 0.0288 0.0562 NGCF-1 PinSage 0.1457 0.2170 0.0390 0.0845 0.0285 0.0563 4.4.2 Effect of Embedding Propagation Layer and Layer-Aggregation Mechanism.</figDesc><table><row><cell></cell><cell cols="2">Gowalla</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell></row><row><cell></cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell></row><row><cell>NGCF-1</cell><cell cols="6">0.1511 0.2218 0.0417 0.0889 0.0315 0.0618</cell></row><row><cell>NGCF-1 SVD++</cell><cell cols="6">0.1447 0.2160 0.0380 0.0828 0.0277 0.0556</cell></row><row><cell>NGCF-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This research is part of NExT++ research and also supported by the Thousand Youth Talents Program 2018. NExT++ is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attentive Collaborative Filtering: Multimedia Recommendation with Item-and Component-Level Attention</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and Reviews</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno>WWW. 639-648</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collaborative Memory Network for Recommendation Systems</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Ebesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>In AISTATS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ItemRank: A Random-Walk Based Scoring Algorithm for Recommender Engines</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augusto</forename><surname>Pucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2766" to="2771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Factorization Machines for Sparse Predictive Analytics</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BiRank: Towards Ranking on Bipartite Graphs. TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial Personalized Ranking for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 173-182</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Collaborative Metric Learning. In WWW</title>
		<imprint>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FISM: factored item similarity models for top-N recommender systems</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="659" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling User Exposure in Recommendation</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="951" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FastShrinkage: Perceptually-aware Retargeting Toward Mobile Platforms</title>
		<author>
			<persName><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zepeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratn</forename><surname>Rajiv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingjie</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">RecWalk: Nearly Uncoupled Random Walks for Top-N Recommendation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Athanasios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Nikolakopoulos</surname></persName>
		</author>
		<author>
			<persName><surname>Karypis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural Compatibility Modeling with Attentive Knowledge Distillation</title>
		<author>
			<persName><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Latent relational metric learning via memory-based attention for collaborative ranking</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><surname>Hui</surname></persName>
		</author>
		<idno>WWW. 729-739</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative Deep Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">KGAT: Knowledge Graph Attention Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">TEM: Tree-enhanced Embedding Model for Explainable Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 1543-1552</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Item Silk Road: Recommending Items from Information Domains to Social Users</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explainable Reasoning over Knowledge Graphs for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Collaborative Denoising Auto-Encoders for Top-N Recommender Systems</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<idno>WSDM. 153-162</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relational Collaborative Filtering:Modeling Multiple Item Relations for Recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep Item-based Collaborative Filtering for Top-N Recommendation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">HOP-rec: high-order proximity for implicit recommendation</title>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interpretable Fashion Matching with Rich Attributes</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD (Data Science track</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spectral collaborative filtering</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
