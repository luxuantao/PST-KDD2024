<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spectral Networks and Deep Locally Connected Networks on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-05-21">21 May 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
							<email>bruna@cims.nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
							<email>woj.zaremba@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
							<email>aszlam@ccny.cuny.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">The City College of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spectral Networks and Deep Locally Connected Networks on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-05-21">21 May 2014</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1312.6203v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) have been extremely succesful in machine learning problems where the coordinates of the underlying data representation have a grid structure (in 1, 2 and 3 dimensions), and the data to be studied in those coordinates has translational equivariance/invariance with respect to this grid. Speech <ref type="bibr" target="#b10">[11]</ref>, images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref> or video <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b17">18]</ref> are prominent examples that fall into this category.</p><p>On a regular grid, a CNN is able to exploit several structures that play nicely together to greatly reduce the number of parameters in the system:</p><p>1. The translation structure, allowing the use of filters instead of generic linear maps and hence weight sharing. 2. The metric on the grid, allowing compactly supported filters, whose support is typically much smaller than the size of the input signals. <ref type="bibr" target="#b2">3</ref>. The multiscale dyadic clustering of the grid, allowing subsampling, implemented through stride convolutions and pooling.</p><p>If there are n input coordinates on a grid in d dimensions, a fully connected layer with m outputs requires n ? m parameters, which in typical operating regimes amounts to a complexity of O(n 2 ) parameters. Using arbitrary filters instead of generic fully connected layers reduces the complexity to O(n) parameters per feature map, as does using the metric structure by building a "locally connected" net <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>. Using the two together gives O(k ? S) parameters, where k is the number of feature maps and S is the support of the filters, and as a result the learning complexity is independent of n. Finally, using the multiscale dyadic clustering allows each succesive layer to use a factor of 2 d less (spatial) coordinates per filter.</p><p>In many contexts, however, one may be faced with data defined over coordinates which lack some, or all, of the above geometrical properties. For instance, data defined on 3-D meshes, such as surface tension or temperature, measurements from a network of meteorological stations, or data coming from social networks or collaborative filtering, are all examples of structured inputs on which one cannot apply standard convolutional networks. Another relevant example is the intermediate representation arising from deep neural networks. Although the spatial convolutional structure can be exploited at several layers, typical CNN architectures do not assume any geometry in the "feature" dimension, resulting in 4-D tensors which are only convolutional along their spatial coordinates.</p><p>Graphs offer a natural framework to generalize the low-dimensional grid structure, and by extension the notion of convolution. In this work, we will discuss constructions of deep neural networks on graphs other than regular grids. We propose two different constructions. In the first one, we show that one can extend properties ( <ref type="formula" target="#formula_3">2</ref>) and (3) to general graphs, and use them to define "locally" connected and pooling layers, which require O(n) parameters instead of O(n 2 ). We term this the spatial construction. The other construction, which we call spectral construction, draws on the properties of convolutions in the Fourier domain. In R d , convolutions are linear operators diagonalised by the Fourier basis exp(i? ?t), ?, t ? R d . One may then extend convolutions to general graphs by finding the corresponding "Fourier" basis. This equivalence is given through the graph Laplacian, an operator which provides an harmonic analysis on the graphs <ref type="bibr" target="#b0">[1]</ref>. The spectral construction needs at most O(n) paramters per feature map, and also enables a construction where the number of parameters is independent of the input dimension n. These constructions allow efficient forward propagation and can be applied to datasets with very large number of coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>Our main contributions are summarized as follows:</p><p>? We show that from a weak geometric structure in the input domain it is possible to obtain efficient architectures using O(n) parameters, that we validate on low-dimensional graph datasets. ? We introduce a construction using O(1) parameters which we empirically verify, and we discuss its connections with an harmonic analysis problem on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Spatial Construction</head><p>The most immediate generalisation of CNN to general graphs is to consider multiscale, hierarchical, local receptive fields, as suggested in <ref type="bibr" target="#b2">[3]</ref>. For that purpose, the grid will be replaced by a weighted graph G = (?, W ), where ? is a discrete set of size m and W is a m?m symmetric and nonnegative matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Locality via W</head><p>The notion of locality can be generalized easily in the context of a graph. Indeed, the weights in a graph determine a notion of locality. For example, a straightforward way to define neighborhoods on W is to set a threshold ? &gt; 0 and take neighborhoods</p><formula xml:id="formula_0">N ? (j) = {i ? ? : W ij &gt; ?} .</formula><p>We can restrict attention to sparse "filters" with receptive fields given by these neighborhoods to get locally connected networks, thus reducing the number of parameters in a filter layer to O(S ? n), where S is the average neighborhood size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multiresolution Analysis on Graphs</head><p>CNNs reduce the size of the grid via pooling and subsampling layers. These layers are possible because of the natural multiscale clustering of the grid: they input all the feature maps over a cluster, and output a single feature for that cluster. On the grid, the dyadic clustering behaves nicely with respect to the metric and the Laplacian (and so with the translation structure). There is a large literature on forming multiscale clusterings on graphs, see for example <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. Finding multiscale clusterings that are provably guaranteed to behave well w.r.t. Laplacian on the graph is still an open area of research. In this work we will use a naive agglomerative method.</p><p>Figure <ref type="figure">1</ref> illustrates a multiresolution clustering of a graph with the corresponding neighborhoods.</p><p>Figure <ref type="figure">1</ref>: Undirected Graph G = (? 0 , W ) with two levels of clustering. The original points are drawn in gray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Locally Connected Networks</head><p>The spatial construction starts with a multiscale clustering of the graph, similarly as in <ref type="bibr" target="#b2">[3]</ref> We consider K scales. We set ? 0 = ?, and for each k = 1 . . . K, we define ? k , a partition of ? k-1 into d k clusters; and a collection of neighborhoods around each element of ? k-1 :</p><formula xml:id="formula_1">N k = {N k,i ; i = 1 . . . d k-1 } .</formula><p>With these in hand, we can now define the k-th layer of the network. We assume without loss of generality that the input signal is a real signal defined in ? 0 , and we denote by f k the number of "filters" created at each layer k. Each layer of the network will transform a f k-1 -dimensional signal indexed by ? k-1 into a f k -dimensional signal indexed by ? k , thus trading-off spatial resolution with newly created feature coordinates.</p><p>More formally, if</p><formula xml:id="formula_2">x k = (x k,i ; i = 1 . . . f k-1 ) is the d k-1 ? f k-1</formula><p>is the input to layer k, its the output x k+1 is defined as</p><formula xml:id="formula_3">x k+1,j = L k h ? ? f k-1 i=1 F k,i,j x k,i ? ? (j = 1 . . . f k ) ,<label>(2.1)</label></formula><p>where F k,i,j is a d k-1 ? d k-1 sparse matrix with nonzero entries in the locations given by N k , and L k outputs the result of a pooling operation over each cluster in ? k . This construcion is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>In the current code, to build ? k and N k we use the following construction: and ? k is found as an covering for W k<ref type="foot" target="#foot_0">1</ref> . This is just one amongst many strategies to perform hierarchicial agglomerative clustering. For a larger account of the problem, we refer the reader to <ref type="bibr" target="#b9">[10]</ref>.</p><formula xml:id="formula_4">W 0 = W A k (i, j) = s?? k (i) t?? k (j) W k-1 (s, t) , (k ? K) W k = rownormalize(A k ) , (k ? K) N k = supp(W k ) . (k ? K)</formula><p>If S k is the average support of the neighborhoods N k , we verify from (2.1) that the number of parameters to learn at layer k is</p><formula xml:id="formula_5">O(S k ? |? k | ? f k ? f k-1 ) = O(n) .</formula><p>In practice, we have</p><formula xml:id="formula_6">S k ? |? k | ? ? ? |? k-1 |</formula><p>, where ? is the oversampling factor, typically ? ? (1, 4).</p><p>The spatial construction might appear na?ve, but it has the advantage that it requires relatively weak regularity assumptions on the graph. Graphs having low intrinsic dimension have localized neighborhoods, even if no nice global embedding exists. However, under this construction there is no easy way to induce weight sharing across different locations of the graph. One possible option is to consider a global embedding of the graph into a low dimensional space, which is rare in practice for high-dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spectral Construction</head><p>The global structure of the graph can be exploited with the spectrum of its graph-Laplacian to generalize the convolution operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Harmonic Analysis on Weighted Graphs</head><p>The combinatorial Laplacian L = D -W or graph Laplacian L = I -D -1/2 W D -1/2 are generalizations of the Laplacian on the grid; and frequency and smoothness relative to W are interrelated through these operators <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref>. For simplicity, here we use the combinatorial Laplacian. If x is an m-dimensional vector, a natural definition of the smoothness functional</p><formula xml:id="formula_7">||?x|| 2 W at a node i is ?x 2 W (i) = j W ij [x(i) -x(j)] 2 ,</formula><formula xml:id="formula_8">and ?x 2 W = i j W ij [x(i) -x(j)] 2 ,<label>(3.1)</label></formula><p>With this definition, the smoothest vector is a constant:</p><formula xml:id="formula_9">v 0 = arg min x?R m x =1 ?x 2 W = (1/ ? m)1 m .</formula><p>Each succesive</p><formula xml:id="formula_10">v i = arg min x?R m x =1 x?{v0,...,vi-1} ?x 2</formula><p>W is an eigenvector of L, and the eigenvalues ? i allow the smoothness of a vector x to be read off from the coefficients of x in [v 0 , ...v m-1 ], equivalently as the Fourier coefficients of a signal defined in a grid. Thus, just an in the case of the grid, where the eigenvectors of the Laplacian are the Fourier vectors, diagonal operators on the spectrum of the Laplacian modulate the smoothness of their operands. Moreover, using these diagonal operators reduces the number of parameters of a filter from m 2 to m.</p><p>These three structures above are all tied together through the Laplacian operator on the d-</p><formula xml:id="formula_11">dimensional grid ?x = d i=1 ? 2 x ?u 2 i :</formula><p>1. Filters are multipliers on the eigenvalues of the Laplacian ?.</p><p>2. Functions that are smooth relative to the grid metric have coefficients with quick decay in the basis of eigenvectors of ?.</p><p>3. The eigenvectors of the subsampled Laplacian are the low frequency eigenvectors of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extending Convolutions via the Laplacian Spectrum</head><p>As in section 2.3, let W be a weighted graph with index set denoted by ?, and let V be the eigenvectors of the graph Laplacian L, ordered by eigenvalue. Given a weighted graph, we can try to generalize a convolutional net by operating on the spectrum of the weights, given by the eigenvectors of its graph Laplacian.</p><p>For simplicity, let us first describe a construction where each layer k = 1 . . . K transforms an input vector x k of size |?| ? f k-1 into an output x k+1 of dimensions |?| ? f k , that is, without spatial subsampling:</p><formula xml:id="formula_12">x k+1,j = h ? ? V f k-1 i=1 F k,i,j V T x k,i ? ? (j = 1 . . . f k ) ,<label>(3.2)</label></formula><p>where F k,i,j is a diagonal matrix and, as before, h is a real valued nonlinearity.</p><p>Often, only the first d eigenvectors of the Laplacian are useful in practice, which carry the smooth geometry of the graph. The cutoff frequency d depends upon the intrinsic regularity of the graph and also the sample size. In that case, we can replace in (3.2) V by V d , obtained by keeping the first d columns of V .</p><p>If the graph has an underlying group invariance this construction can discover it; the best example being the standard CNN; see 3.3. However, in many cases the graph does not have a group structure, or the group structure does not commute with the Laplacian, and so we cannot think of each filter as passing a template across ? and recording the correlation of the template with that location. ? may not be homogenous in a way that allows this to make sense, as we shall see in the example from Section 5.1.</p><p>Assuming only d eigenvectors of the Laplacian are kept, equation (3.2) shows that each layer re-</p><formula xml:id="formula_13">quires f k-1 ? f k ? d = O(|?|)</formula><p>paramters to train. We shall see in section 3.4 how the global and local regularity of the graph can be combined to produce layers with O(1) parameters, i.e. such that the number of learnable parameters does not depend upon the size of the input.</p><p>This construction can suffer from the fact that most graphs have meaningful eigenvectors only for the very top of the spectrum. Even when the individual high frequency eigenvectors are not meaningful, a cohort of high frequency eigenvectors may contain meaningful information. However this construction may not be able to access this information because it is nearly diagonal at the highest frequencies.</p><p>Finally, it is not obvious how to do either the forwardprop or the backprop efficiently while applying the nonlinearity on the space side, as we have to make the expensive multiplications by V and V T ; and it is not obvious how to do standard nonlinearities on the spectral side. However, see 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Rediscovering standard CNN's</head><p>A simple, and in some sense universal, choice of weight matrix in this construction is the covariance of the data. Let X = (x k ) k be the input data distribution, with x k ? R n . If each coordinate j = 1 . . . n has the same variance,</p><formula xml:id="formula_14">? 2 j = E |x(j) -E(x(j))| 2</formula><p>, then diagonal operators on the Laplacian simply scale the principal components of X. While this may seem trivial, it is well known that the principal components of the set of images of a fixed size are (experimentally) correspond to the Discrete Cosine Transform basis, organized by frequency. This can be explained by noticing that images are translation invariant, and hence the covariance operator</p><formula xml:id="formula_15">?(j, j) = E ((x(j) -E(x(j)))(x(j ) -E(x(j ))))</formula><p>satisfies ?(j, j ) = ?(j -j ), hence it is diagonalized by the Fourier basis. Moreover, it is well known that natural images exhibit a power spectrum</p><formula xml:id="formula_16">E(| x(?)| 2 ) ? ? -2</formula><p>, since nearby pixels are more correlated than far away pixels. It results that principal components of the covariance are essentially ordered from low to high frequencies, which is consistent with the standard group structure of the Fourier basis.</p><p>The upshot is that, when applied to natural images, the construction in 3.2 using the covariance as the similarity kernel recovers a standard convolutional network, without any prior knowledge. Indeed, the linear operators V F i,j V T from Eq (3.2) are by the previous argument diagonal in the Fourier basis, hence translation invariant, hence "classic" convolutions. Moreover, Section 4.1 explains how spatial subsampling can also be obtained via dropping the last part of the spectrum of the Laplacian, leading to max-pooling, and ultimately to deep convolutonal networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">O(1) construction with smooth spectral multipliers</head><p>In the standard grid, we do not need a parameter for each Fourier function because the filters are compactly supported in space, but in (3.2), each filter requires one parameter for each eigenvector on which it acts. Even if the filters were compactly supported in space in this construction, we still would not get less than O(n) parameters per filter because the spatial response would be different at each location.</p><p>One possibility for getting around this is to generalize the duality of the grid. On the Euclidian grid, the decay of a function in the spatial domain is translated into smoothness in the Fourier domain, and viceversa. It results that a funtion x which is spatially localized has a smooth frequency response x = V T x. In that case, the eigenvectors of the Laplacian can be thought of as being arranged on a grid isomorphic to the original spatial grid.</p><p>This suggests that, in order to learn a layer in which features will be not only shared across locations but also well localized in the original domain, one can learn spectral multipliers which are smooth. Smoothness can be prescribed by learning only a subsampled set of frequency multipliers and using an interpolation kernel to obtain the rest, such as cubic splines. However, the notion of smoothness requires a geometry in the domain of spectral coordinates, which can be obtained by defining a dual graph W as shown by (3.1). As previously discussed, on regular grids this geometry is given by the notion of frequency, but this cannot be directly generalized to other graphs.</p><p>A particularly simple and navie choice consists in choosing a 1-dimensional arrangement, obtained by ordering the eigenvectors according to their eigenvalues. In this setting, the diagonal of each filter F k,i,j (of size at most |?|) is parametrized by</p><formula xml:id="formula_17">diag(F k,i,j ) = K ? k,i,j ,</formula><p>where K is a d ? q k fixed cubic spline kernel and ? k,i,j are the q k spline coefficients. If one seeks to have filters with constant spatial support (ie, whose support is independent of the input size |?|), it follows that one can choose a sampling step ? ? |?| in the spectral domain, which results in a constant number q k ? |?| ? ? -1 = O(1) of coefficients ? k,i,j per filter.</p><p>Although results from section 5 seem to indicate that the 1-D arrangement given by the spectrum of the Laplacian is efficient at creating spatially localized filters, a fundamental question is how to define a dual graph capturing the geometry of spectral coordinates. A possible algorithmic stategy is to consider an input distribution X = (x k ) k consisting on spatially localized signals and to construct a dual graph W by measuring the similarity of in the spectral domain: X = V T X. The similarity could be measured for instance with E((|x| -E(|x)|)) T (|x| -E(|x|)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Relationship with previous work</head><p>There is a large literature on building wavelets on graphs, see for example <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref>. A wavelet basis on a grid, in the language of neural networks, is a linear autoencoder with certain provable regularity properties (in particular, when encoding various classes of smooth functions, sparsity is guaranteed). The forward propagation in a classical wavelet transform strongly resembles the forward propagation in a neural network, except that there is only one filter map at each layer (and it is usually the same filter at each layer), and the output of each layer is kept, rather than just the output of the final layer. Classically, the filter is not learned, but constructed to facilitate the regularity proofs.</p><p>In the graph case, the goal is the same; except that the smoothness on the grid is replaced by smoothness on the graph. As in the classical case, most works have tried to construct the wavelets explicitly (that is, without learning), based on the graph, so that the corresponding autencoder has the correct sparsity properties. In this work, and the recent work <ref type="bibr" target="#b20">[21]</ref>, the "filters" are constrained by construction to have some of the regularity properties of wavelets, but are also trained so that they are appropriate for a task separate from (but perhaps related to) the smoothness on the graph. Whereas <ref type="bibr" target="#b20">[21]</ref> still builds a (sparse) linear autoencoder that keeps the basic wavelet transform setup, this work focuses on nonlinear constructions; and in particular, tries to build analogues of CNN's.</p><p>Another line of work which is rellevant to the present work is that of discovering grid topologies from data. In <ref type="bibr" target="#b18">[19]</ref>, the authors empirically confirm the statements of Section 3.3, by showing that one can recover the 2-D grid structure via second order statistics. In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> the authors estimate similarities between features to construct locally connected networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multigrid</head><p>We could improve both constructions, and to some extent unify them, with a multiscale clustering of the graph that plays nicely with the Laplacian. As mentioned before, in the case of the grid, the standard dyadic cubes have the property that subsampling the Fourier functions on the grid to a coarser grid is the same as finding the Fourier functions on the coarser grid. This property would eliminate the annoying necessity of mapping the spectral construction to the finest grid at each layer to do the nonlinearity; and would allow us to interpret (via interpolation) the local filters at deeper layers in the spatial construction to be low frequency.</p><p>This kind of clustering is the underpinning of the multigrid method for solving discretized PDE's (and linear systems in general) <ref type="bibr" target="#b23">[24]</ref>. There have been several papers extending the multigrid method, and in particular, the multiscale clustering(s) associated to the multigrid method, in settings more general than regular grids, see for example <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref> for situations as in this paper, and see <ref type="bibr" target="#b23">[24]</ref> for the algebraic multigrid method in general. In this work, for simplicity, we use a naive multiscale clustering on the space side construction that is not guaranteed to respect the original graph's Laplacian, and no explicit spatial clustering in the spectral construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical Experiments</head><p>The previous constructions are tested on two variations of the MNIST data set. In the first, we subsample the normal 28 ? 28 grid to get 400 coordinates. These coordinates still have a 2-D structure, but it is not possible to use standard convolutions. We then make a dataset by placing d = 4096 points on the 3-D unit sphere and project random MNIST images onto this set of points, as described in Section 5.2.</p><p>In all the experiments, we use Rectified Linear Units as nonlinearities and max-pooling. We train the models with cross-entropy loss, using a fixed learning rate of 0.1 with momentum 0.9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Subsampled MNIST</head><p>We first apply the constructions from sections 3.2 and 2.3 to the subsampled MNIST dataset. Figure <ref type="figure" target="#fig_1">3</ref> shows examples of the resulting input signals, and Figures <ref type="figure">4,</ref><ref type="figure">5</ref> show the hierarchical clustering constructed from the graph and some eigenfunctions of the graph Laplacian, respectively. The performance of various graph architectures is reported in Table <ref type="table" target="#tab_0">1</ref>. To serve as a baseline, we compute the standard Nearest Neighbor classifier, which performs slightly worse than in the full MNIST dataset (2.8%). A two-layer Fully Connected neural network reduces the error to 1.8%. The geometrical structure of the data can be exploited with the CNN graph architectures. Local Receptive Fields adapted to the graph structure outperform the fully connected network. In particular, two layers of filtering and max-pooling define a network which efficiently aggregates information to the final classifier. The spectral construction performs slightly worse on this dataset. We considered a frequency cutoff of N/2 = 200. However, the frequency smoothing architecture described in section 3.4, which contains the smallest number of parameters, outperforms the regular spectral construction.</p><p>These results can be interpreted as follows. MNIST digits are characterized by localized oriented strokes, which require measurements with good spatial localization. Locally receptive fields are constructed to explicitly satisfy this constraint, whereas in the spectral construction the measurements are not enforced to become spatially localized. Adding the smoothness constraint on the spectrum of the filters improves classification results, since the filters are enforced to have better spatial localization.</p><p>This fact is illustrated in Figure <ref type="figure" target="#fig_3">6</ref>. We verify that Locally Receptive fields encode different templates across different spatial neighborhoods, since there is no global strucutre tying them together. On the other hand, spectral constructions have the capacity to generate local measurements that generalize across the graph. When the spectral multipliers are not constrained, the resulting filters tend to be spatially delocalized, as shown in panels (c)-(d). This corresponds to the fundamental limitation of Fourier analysis to encode local phenomena. However, we observe in panels (e)-(f) that a simple smoothing across the spectrum of the graph Laplacian restores some form of spatial localization and creates filters which generalize across different spatial positions, as should be expected for convolution operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MNIST on the sphere</head><p>We test in this section the graph CNN constructions on another low-dimensional graph. In this case, we lift the MNIST digits to the sphere. The dataset is constructed as follows. We first sample 4096 random points S = {s j } j?4096 from the unit sphere S 2 ? R 3 . We then consider an orthogonal basis E = (e 1 , e 2 , e 3 ) of R 3 with e 1 = 1 , e 2 = 2 , e 3 = 3 and a random covariance operator ? = (E+W ) T (E+W ), where W is a Gaussian iid matrix with variance ? 2 &lt; 1. For each signal x i from the original MNIST dataset, we sample a covariance operator ? i from the former distribution and consider its PCA basis U i . This basis defines a point of view and in-plane rotation which we use  1.8 400-LRF3200-MP800-LRF800-MP400-10</p><p>1.6 ? 10 5 1.3 400-SP1600-10 (d 1 = 300, q = n)</p><p>3.2 ? 10 3 2.6 400-SP1600-10 (d 1 = 300, q = 32) 1.6 ? 10 3 2.3 400-SP4800-10 (d 1 = 300, q = 20) 5 to project x i onto S using bicubic interpolation. Figure <ref type="figure">7</ref> shows examples of the resulting projected digits. Since the digits '6' and '9' are equivalent modulo rotations, we remove the '9' from the dataset. Figure <ref type="figure">8</ref> shows two eigenvectors of the graph Laplacian.</p><formula xml:id="formula_18">? 10 3 1.8 (a) (b) (c) (d) (e)<label>(f)</label></formula><p>We first consider "mild" rotations with ? 2 = 0.2. The effect of such rotations is however not negligible. Indeed, table <ref type="table" target="#tab_1">2</ref> shows that the Nearest Neighbor classifer performs considerably worse than in the previous example. All the neural network architectures we considered significatively improve over this basic classifier. Furthermore, we observe that both convolutional constructions match the fully connected constructions with far less parameters (but in this case, do not improve its performance). Figure <ref type="figure">9</ref> displays the filters learnt using different constructions. Again, we verify 5.6 4096-LRF4620-MP2000-FC300-9 8 ? 10 5 6 4096-LRF4620-MP2000-LRF500-MP250-9 2 ? 10 5 6.5 4096-SP32K-MP3000-FC300-9 (d 1 = 2048, q = n) 9 ? 10 5 7 4096-SP32K-MP3000-FC300-9 (d 1 = 2048, q = 64) 9 ? 10 5 6</p><p>that the smooth spectral construction consistently improves the performance, and learns spatially localized filters, even using the naive 1-D organization of eigenvectors, which detect similar features across different locations of the graph (panels (e)-(f)).</p><p>Finally, we consider the uniform rotation case, where now the basis U i is a random basis of R 3 . In that case, the intra-class variability is much more severe, as seen by inspecting the performance of the Nearest neighbor classifier. All the previously described neural network architectures significantly improve over this classifier, although the performance is notably worse than in the mild rotation scenario. In this case, an efficient representation needs to be fully roto-translation invariant. Since this is a non-commutative group, it is likely that deeper architectures perform better than the models considered here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Using graph-based analogues of convolutional architectures can greatly reduce the number of parameters in a neural network without worsening (and often improving) the test error, while simultaneously giving a faster forward propagation. These methods can be scaled to data with a large number of coordinates that have a notion of locality.</p><p>There is much to be done here. We suspect with more careful training and deeper networks we can consistently improve on fully connected networks on "manifold like" graphs like the sampled sphere.</p><p>Table <ref type="table">3</ref>: Classification results on the MNIST-sphere dataset generated using uniformly random rotations, for different architectures method Parameters Error Nearest Neighbors NA 80 4096-FC2048-FC512-9 10 7 52 4096-LRF4620-MP2000-FC300-9 8 ? 10 5 61 4096-LRF4620-MP2000-LRF500-MP250-9 2 ? 10 5 63 4096-SP32K-MP3000-FC300-9 (d 1 = 2048, q = n) 9 ? 10 5 56 4096-SP32K-MP3000-FC300-9 (d 1 = 2048, q = 64) 9 ? 10 5 50</p><p>Furthermore, we intend to apply these techniques to less artifical problems, for example, on netflix like recommendation problems where there is a biclustering of the data and coordinates. Finally, the fact that smoothness on the naive ordering of the eigenvectors leads to improved results and localized filters suggests that it may be possible to make "dual" constructions with O(1) parameters per filter in much more generality than the grid.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Spatial Construction as described by (2.1), with K = 2. For illustration purposes, the pooling operation is assimilated with the filtering stage. Each layer of the transformation loses spatial resolution but increases the number of filters.</figDesc><graphic url="image-2.png" coords="4,108.00,81.86,422.80,147.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Subsampled MNIST examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Clusters obtained with the agglomerative clustering. (a) Clusters corresponding to the finest scale k = 1, (b) clusters for k = 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Subsampled MNIST learnt filters using spatial and spectral construction. (a)-(b) Two different receptive fields encoding the same feature in two different clusters. (c)-(d) Example of a filter obtained with the spectral construction. (e)-(f) Filters obtained with smooth spectral construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Figure 7: Examples of some MNIST digits on the sphere.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification results on MNIST subsampled on 400 random locations, for different architectures. FCN stands for a fully connected layer with N outputs, LRFN denotes the locally connected construction from Section 2.3 with N outputs, MPN is a max-pooling layer with N outputs, and SPN stands for the spectral layer from Section 3.2.</figDesc><table><row><cell>method</cell><cell cols="2">Parameters Error</cell></row><row><cell>Nearest Neighbors</cell><cell>N/A</cell><cell>4.11</cell></row><row><cell>400-FC800-FC50-10</cell><cell>3.6 ? 10 5</cell><cell>1.8</cell></row><row><cell>400-LRF1600-MP800-10</cell><cell>7.2 ? 10 4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification results on the MNIST-sphere dataset generated using partial rotations, for</figDesc><table><row><cell>different architectures</cell><cell></cell><cell></cell></row><row><cell>method</cell><cell cols="2">Parameters Error</cell></row><row><cell>Nearest Neighbors</cell><cell>N/A</cell><cell>19</cell></row><row><cell>4096-FC2048-FC512-9</cell><cell>10 7</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>An -covering of a set ? using a similarity kernel K is a partition P = {P1, . . . , Pn} such that sup n sup x,x ?Pn K(x, x ) ? .</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spectral Graph Theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diffusion wavelets. Appl. Comp. Harm. Anal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="94" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph wavelets for spatial traffic analysis</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Crovella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">D</forename><surname>Kolaczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi supervised learning</title>
		<author>
			<persName><forename type="first">Matan</forename><surname>Gavish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>
			<persName><forename type="first">Johannes</forename><surname>Frankranz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Emergence of complex-like cells in a temporal product network with local receptive fields</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR, abs/1006.0448</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiresolution signal processing for meshes</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sweldens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schr?der</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Proceedings (SIGGRAPH 99)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference and prediction</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids: Receptive field learning for pooled image features</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3370" to="3377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Metis -unstructured graph partitioning and sparse matrix ordering system, version 2.0</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient multilevel eigensolvers with applications to data analysis tasks. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kushnir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1377" to="1391" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast multiscale clustering and manifold identification</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kushnir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meirav</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achi</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1876" to="1891" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>?ce:title?Similarity-based Pattern Recognition?/ce:title?</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tiled convolutional neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3361" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning the 2-d topology of images</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Joliveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bal?zs</forename><surname>K?gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Signal Processing</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="306" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Rustamov</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks applied to house numbers digit classification</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV&apos;10)</title>
		<meeting>European Conference on Computer Vision (ECCV&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Trottenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><surname>Multigrid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Academic Press, Inc</publisher>
			<pubPlace>Orlando, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
		<idno>149</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
