<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KPGT: Knowledge-Guided Pre-training of Graph Transformer for Molecular Property Prediction</title>
				<funder ref="#_u7hTa2B #_JFuHV8J #_NE7CMke">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Tsinghua-Toyota Joint Research Fund</orgName>
				</funder>
				<funder ref="#_Udu85S6">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-02">2 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Han</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianyang</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><surname>Kpgt</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University Haidian Qu</orgName>
								<address>
									<settlement>Beijing Shi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University Haidian Qu</orgName>
								<address>
									<settlement>Beijing Shi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University Haidian Qu</orgName>
								<address>
									<settlement>Beijing Shi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<addrLine>11 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KPGT: Knowledge-Guided Pre-training of Graph Transformer for Molecular Property Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-02">2 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539426</idno>
					<idno type="arXiv">arXiv:2206.03364v1[q-bio.BM]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph representation learning</term>
					<term>self-supervised learning</term>
					<term>graph transformer</term>
					<term>graph pre-training</term>
					<term>molecular property prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing accurate deep learning models for molecular property prediction plays an increasingly essential role in drug and material discovery. Recently, due to the scarcity of labeled molecules, self-supervised learning methods for learning generalizable and transferable representations of molecular graphs have attracted lots of attention. In this paper, we argue that there exist two major issues hindering current self-supervised learning methods from obtaining desired performance on molecular property prediction, that is, the ill-defined pre-training tasks and the limited model capacity.</p><p>To this end, we introduce Knowledge-guided Pre-training of Graph Transformer (KPGT), a novel self-supervised learning framework for molecular graph representation learning, to alleviate the aforementioned issues and improve the performance on the downstream molecular property prediction tasks. More specifically, we first introduce a high-capacity model, named Line Graph Transformer (LiGhT), which emphasizes the importance of chemical bonds and is mainly designed to model the structural information of molecular graphs. Then, a knowledge-guided pre-training strategy is proposed to exploit the additional knowledge of molecules to guide the model to capture the abundant structural and semantic information from large-scale unlabeled molecular graphs. Extensive computational tests demonstrated that KPGT can offer superior performance over current state-of-the-art methods on several molecular property prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Unsupervised learning; Learning latent representations; Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Molecular property prediction is of great significance to design novel molecules with desired properties for drug and material discovery <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b54">55]</ref>. Representing molecules as graphs, where nodes correspond to the atoms and edges correspond to the chemical bonds, deep learning models, especially graph neural networks (GNNs), have been widely used for molecular graph representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b63">64]</ref>. However, due to the limited amount of labeled data and the giant chemical space, such deep learning models trained with supervised learning strategies often perform poorly, especially on the prediction of out-of-distribution data samples <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Following the significant success of self-supervised learning methods in the fields of natural language processing (NLP) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref> and computer vision (CV) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, many recent works have proposed to employ self-supervised learning strategies to pre-train GNNs by leveraging the large-scale unlabeled molecules and have achieved superior prediction performance in comparison with supervised learning methods on the downstream molecular property prediction tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>. Despite such fruitful progress, the prediction performance is still far from ideal. In this paper, we argue that current self-supervised learning methods on molecular graphs still encounter the following two main issues:</p><p>The ill-defined pre-training tasks. The performance of selfsupervised learning methods crucially depends on the design of pre-training tasks. So far, the self-supervised learning methods on molecular graphs can be roughly divided into two categories, that is, generative and contrastive methods according to their design of pre-training tasks. The generative methods follow the maskedlanguage models in the NLP field, e.g., BERT <ref type="bibr" target="#b9">[10]</ref>, through masking a portion of molecular graphs, e.g., edges, nodes or subgraphs, and then learning to retrieve the original graphs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref>. On the other hand, following the pioneering works in CV, the contrastive methods on molecular graphs first generate graph augmentations through strategies like node replacing, node dropping and edge perturbation for molecular graphs, and then learn to match the augmented graphs with the corresponding original molecular graphs in the embedding space <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63]</ref>. However, unlike the word masking of human languages and the augmentation of images (e.g., resizing and rotation), which do not impact the fundamental semantics of the raw inputs, a small modification of molecular graphs can greatly change the characteristics of the corresponding molecules (an illustrative example is shown in Figure <ref type="figure">4</ref>). Therefore, current self-supervised learning methods on molecular graphs can only capture the structural similarity of graphs and the simple construction rules (e.g., valency rule) of molecules, but fail to induce the abundant semantics related to molecular properties from chemical structures which are potentially more important to the downstream learning tasks.</p><p>The limited model capacity. Due to the giant chemical space and the wide breadth of the molecular properties ranging from quantum mechanics, physical chemistry, biophysics to physiology <ref type="bibr" target="#b54">[55]</ref>, a high-capacity model is required to capture sufficient information from enormous unlabeled molecules. Meanwhile, the success of self-supervised learning methods in the NLP and CV domains is indispensable to the emerging backbone networks with increasing amounts of parameters <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>. Especially, transformerbased models have been proven to predominantly yield excellent prediction performance in these fields. However, applying the transformer-based architectures in the self-supervised learning on molecular graphs is underexplored. Most of the previous defined self-supervised learning methods on molecular graphs mainly employ GNNs, e.g., Graph Isomorphism Network (GIN) <ref type="bibr" target="#b56">[57]</ref>, as the backbone networks, which provide only limited model capacity and potentially fail to capture the wide range of the information required for the prediction of various properties for a diversity of molecules.</p><p>To this end, we propose Knowledge-guided Pre-training of Graph Transformer (KPGT), a novel self-supervised learning framework, to alleviate the aforementioned issues and learn more generalizable, transferable and robust representations of molecular graphs to improve the performance of the downstream molecular property prediction tasks. First, we propose a high-capacity model, named Line Graph Transformer (LiGhT), which represents molecular graphs as line graphs to emphasize the importance of chemical bonds, and also introduces path encoding and distance encoding to accurately preserve the structural information of molecules. Then, we design a knowledge-guided pre-training strategy based on a generative self-supervised learning scheme. Instead of directly predicting the randomly masked nodes using the contextual information, our proposed strategy leverages the additional knowledge of molecules (i.e., molecular descriptors and fingerprints), which serves as the semantics lost in the masked graph to guide the prediction of the masked nodes, thus making the model capture the abundant structural and semantic information from large-scale unlabeled molecules.</p><p>We conducted extensive computational experiments to evaluate the performance of our proposed method. We first pre-trained KPGT on a large-scale unlabeled molecule dataset and then applied the pre-trained model to eleven downstream molecular property datasets. The test results demonstrated that KPGT achieved superior results in comparison with current state-of-the-art self-supervised learning methods and thus verified the effective design of KPGT.</p><p>To summarize, our work makes the following three main contributions: <ref type="bibr" target="#b0">(1)</ref> We point out that current self-supervised learning methods on molecular graphs have two major issues, that is, the ill-defined pre-training tasks and the limited model capacity; <ref type="bibr" target="#b1">(2)</ref> we propose KPGT, a novel self-supervised learning framework, consisting of a novel graph transformer architecture, LiGhT, and a knowledge-guided pre-training strategy, to alleviate the current issues in the representation learning of molecular graphs and improve the performance on the downstream molecular property prediction tasks; (3) we conduct extensive tests to demonstrate that our proposed method can offer superior performance over current state-of-the-art methods on several molecular property prediction tasks. The source code and datasets to reproduce our computational test results are available at https://github.com/lihan97/KPGT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Molecular Representation Learning</head><p>Many efforts have been devoted to improving molecular representation learning for accurate molecular property prediction. The early feature-based methods exploited fixed molecular representation, e.g., molecular descriptors and fingerprints, to represent molecules in a vector space <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b48">49]</ref>. This kind of method highly relied on complicated feature engineering to produce promising prediction performance. Then, deep learning based models were introduced to yield more expressive molecular representations. In particular, with their great advantage in modeling graph-structured data, GNNs had been widely applied for molecular graph representation learning. For example, Gilmer et al. <ref type="bibr" target="#b16">[17]</ref> introduced a message passing framework for molecular property prediction. Then many works proposed to modify the message passing mechanism to improve the expressiveness of GNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b55">56]</ref>. Please refer to <ref type="bibr" target="#b53">[54]</ref> for a compact review on molecular property prediction with GNNs. The message passing operators employed in the GNNs aggregated only local information and failed to capture the long-range dependence in molecules. Recently, graph transformers have emerged to model such long-range dependencies. The applications of transformerbased architectures to molecular graphs have remained limited mainly due to the difficulty of properly modeling the structural information of molecular graphs. Therefore, many efforts have been devoted to accurately preserving the structural information of molecular graphs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b60">61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Supervised Learning on Molecular Graphs</head><p>Due to the data scarcity and the giant chemical space, self-supervised learning on molecular graphs has emerged as a core direction recently. Current self-supervised learning methods on molecular graphs can be roughly categorized into generative and contrastive ones, where they differed in the design of the pre-training tasks.</p><p>The generative methods followed the masked-language model in the NLP field through masking nodes, edges or subgraphs and learning to retrieve the original graphs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref>. On the other hand, the contrastive methods followed the pioneer works in the CV field. For example, You et al. <ref type="bibr" target="#b62">[63]</ref> designed a framework with four graph augmentation strategies, that is, node dropping, edge perturbation, attribute masking and subgraph generating. You et al. <ref type="bibr" target="#b61">[62]</ref> further improved this framework by automatically and adaptively selecting data augmentations. In addition, Xu et al. <ref type="bibr" target="#b57">[58]</ref> proposed to learn the hierarchical prototypes upon graph embeddings to infer the global-semantic structure in molecular graphs. Moreover, St?rk et al. <ref type="bibr" target="#b45">[46]</ref> and Liu et al. <ref type="bibr" target="#b31">[32]</ref> exploited the 3D geometry of molecules to improve the pre-training of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KNOWLEDGE-GUIDED PRE-TRAINING OF GRAPH TRANSFORMER (KGPT)</head><p>In this section, we elaborate on the design of our self-supervised learning framework, Knowledge-guided Pre-training of Graph Transformer (KPGT). First, we introduce how we transform molecular graphs into line graphs. Then we provide the detailed implementation of our proposed Line Graph Transformer (LiGhT) for encoding the molecular line graphs. Finally, we introduce our knowledgeguided pre-training strategy.</p><p>3.1 Line Graph Transformer (LiGhT)  As an illustrative example shown in Figure <ref type="figure" target="#fig_0">1</ref>, a molecular graph G can be transformed to a molecular line graph ? = { V, ?} as follows: (1) For each edge in G, e.g., ? ?,? , create a node v?,? in ?; (2) for every two edges in G that have a node in common, create an edge between their corresponding nodes in ?.</p><p>Then, for each node v?,? in ?, its initial feature embedding ? v?,? ? R ? v is defined as:</p><formula xml:id="formula_0">? v?,? = ?????? (? ? ? ? ? + ? ? ? ? ? , ? ? ? ? ?,? ),<label>(1)</label></formula><p>where ? v stands for the dimension of the feature embeddings of nodes in molecular line graphs,</p><formula xml:id="formula_1">? ? ? R ? v 2 ?? ? and ? ? ? R ? v 2 ?? ?</formula><p>stand for the trainable projection matrices, and ?????? (?) stands for the concatenation operator. For clarity, we denote the nodes in a molecular line graph as V = { v? } ? ? [1,? v ] in the following sections, where ? v stands for the number of nodes in the molecular line graph.</p><p>3.1.2 Graph Transformer Architecture. In this section, we propose Line Graph Transformer (LiGhT), a novel transformer-based architecture, to encode the molecular line graphs and learn global feature representations for molecular property prediction (see Figure <ref type="figure" target="#fig_2">2</ref>).</p><p>LiGhT is built upon a classic transformer encoder initially proposed in <ref type="bibr" target="#b49">[50]</ref>. The classic transformer consists of multiple transformer layers, each of which is composed of a multi-head self-attention module, followed by a feed-forward network (FFN) that includes residual connection and layer normalization operations. More specifically, given a molecular line graph and the corresponding node feature matrix ? ? R ? v ?? v , the multi-head selfattention module at layer ? is calculated as:</p><formula xml:id="formula_2">? ?,? = ? ?-1 ? ?,? ? , ? ?,? = ? ?-1 ? ?,? ? , ? ?,? = ? ?-1 ? ?,? ? , ? ?,? = ?? ? ???? ( ? ?,? (? ?,? ) ? ?? ? ? ), ? ?,? = ? ?,? ? ?,? , ? ? = ?????? (? ?,1 , ? ?,2 , . . . , ? ?,? ? ),<label>(2)</label></formula><p>where ? ?-1 stands for the node feature matrix at the (? -1)-th layer,</p><formula xml:id="formula_3">? ?,? ? ? R ? v ?? ? , ? ?,? ? ? R ? v ?? ? and ? ?,? ? ? R ? v ?? ? stand for the trainable projection matrices of the ?-th head at layer ?, ? ? = ? v ? ?</formula><p>stands for the dimension of each self-attention head, ? ? stands for the number of self-attention heads, and ?? ? ???? (?) stands for the softmax operator. The output ? ? is then passed to a FFN:</p><formula xml:id="formula_4">?? = ?? (? ?-1 + ? ? ), ? ? = ?? (? ? 2 ???? (? ? 1 ?? ) + ?? ),<label>(3)</label></formula><p>where ?? (?) stands for the LayerNorm operator <ref type="bibr" target="#b2">[3]</ref>, ???? (?) stands for the GELU activation function <ref type="bibr" target="#b23">[24]</ref>, and</p><formula xml:id="formula_5">? ? 1 ? R 4? v ?? v and ? ? 2 ? R ? v ?4? v</formula><p>stand for the trainable projection matrices at layer ?.</p><p>Unlike GNNs, which aggregate information from neighboring nodes, the global receptive field of the transformer enables each node to attend to the information at any position in one transformer layer. Nevertheless, such a classic transformer architecture ignores the connectivity of graphs, thus causing a significant loss in the structural information of molecules. In the NLP field, the structural information of chain-structured language can be preserved by giving each token a position embedding or encoding the relative distance between any two tokens <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50]</ref>, which actually cannot be easily generalized to the graph-structured data. Here, we introduce path encoding and distance encoding to inject the inductive bias in the multi-head self-attention module and thus enable LiGhT to encode the structural information of the molecular line graphs.</p><p>Path Encoding. For each pair of nodes v? and v ? in the molecular line graph, we first derive the shortest path between them and then encode the path features to an attention scalar ? ? ?,? in a path attention matrix ? ? ? R ? v ?? v as follows:</p><formula xml:id="formula_6">( v? 1 , v? 2 , . . . , v? ? ? ) = ?? ( v? , v ? ), ? ? ?,? = ? ? ? 1 ? ? ? ? ?? ?=1 ? ? ? ? ? ? ? ,<label>(4)</label></formula><p>where stands for the feature of the ?-th node in the shortest path, ? ? ? ? R ? ? ?? v stands for the trainable projection matrix for the ?-th node in the path, ? ? ? ? R 1?? ? stands for a trainable projection matrix to project the path embedding to an attention scalar, and ? ? stands for the dimension of the path embedding.</p><p>Distance Encoding. Following <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b60">61]</ref>, we also leverage the distances between pairs of nodes to further encode the spatial relations in the molecular line graphs. More specifically, given nodes v? and v ? in a molecular line graph, we encode their distance to an attention scalar ? ? ?,? in a distance attention matrix ? ? ? R ? v ?? v as follows:</p><p>? ?,? = ??? ( v? , v ? ),</p><formula xml:id="formula_7">? ? ?,? = ? ? 2 ???? (? ? 1 ? ?,? ),<label>(5)</label></formula><p>where ??? (?) stands for the shortest path distance functoin, ? ?,? stands for the derived distance between v? and v ? , ? ? 1 ? R ? ? ?1 and ? ? 2 ? R 1?? ? stand for the trainable projection matrices, and ? ? stands for the dimension of the distance embedding.</p><p>Then, to introduce the encoded structural information into the model, we rewrite the formula of the attention matrix ? ?,? ? R ? v ?? v in the Eq. 2 as follows:</p><formula xml:id="formula_8">? ?,? = ?? ? ???? ( ? ?,? (? ?,? ) ? ?? ? ? + ? ? + ? ? ).<label>(6)</label></formula><p>Here, we discuss the main advantages of our proposed model compared with the previously defined graph transformers:</p><p>First, by representing molecular graphs as line graphs, LiGhT emphasizes the importance of chemical bonds in molecules. Chemical bonds are the lasting attractions between atoms, which can be categorized into various types according to the ways they hold atoms together resulting in different properties of the formed molecules. However, the previously defined transformer architectures either omit the edge features or only introduce chemical bonds as the bias in the self-attention module, ignoring the rich information from chemical bonds <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b60">61]</ref>. In our case, LiGhT fills this gap and fully exploits the intrinsic features of chemical bonds.</p><p>Second, although strategies like path encoding have already been proposed in previous graph transformer architectures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b60">61]</ref>, when encoding the paths, they only consider the edge features and ignore the node features in the paths. On the other hand, our path encoding strategy incorporates the features of the complete paths between pairs of nodes, thus encoding the structural information more precisely compared to the previous methods.</p><p>In summary, LiGhT provides a reliable backbone network for accurately modeling the structural and semantic information of molecular line graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge-Guided Pre-training Strategy</head><p>Our pre-training strategy is derived mainly based on a generative self-supervised learning scheme. More specifically, we randomly choose a proportion of nodes in molecular line graphs for prediction. Following <ref type="bibr" target="#b9">[10]</ref>, for a chosen node, we first replace it with (1) a mask token 80% of the time, (2) a random node 10% of the time, and (3) the unchanged node 10% of the time, and then predict the type of the original node with a cross-entropy loss.</p><p>As mentioned above, directly applying such masking operations may lose the initial semantics of molecular graphs, thus making current generative methods ill-defined. In addition, without semantics, the only dependence between the masked nodes and their adjacent nodes is the valency rule, which is much weaker in comparison with the dependence between neighboring tokens in natural languages or neighboring pixels in images. As an example shown in Figure <ref type="figure">5</ref>, if we mask an oxygen atom in the molecular graph, the oxygen atom is the expected prediction but many other atoms can also construct valid molecules according to the valency rules. Therefore, such weak dependence cannot always guide the model to make the right predictions, which may cause the model to simply memorize the whole dataset.</p><p>To alleviate these issues, we introduce additional knowledge of molecules in our pre-training framework, which serves as the semantics lost in the masked graphs to guide the prediction of the masked nodes.</p><p>For molecules, there is a large number of well-defined molecular descriptors and fingerprints that have been widely used to represent molecular characteristics and have been proven to be effective for molecular property prediction in the previous research <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b48">49]</ref>. Molecular descriptors are typically generated by logical and mathematical procedures to quantitatively describe the physical and chemical profiles of molecules <ref type="bibr" target="#b58">[59]</ref>. For example, LogP is a type of molecular descriptor measuring the lipophilicity of the molecules. The molecular fingerprints describe a molecular structure by a bit string, where one indicates the existence of one substructure and vice versa <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref>. Therefore, molecular descriptors and fingerprints can provide valuable global and local information of molecules. Moreover, most of the molecular descriptors and fingerprints are readily available by current cheminformatics tools <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>. Thus, it will not introduce extra budget in the pre-training process.</p><p>To incorporate such additional knowledge in our pre-training framework, we define a special node for each molecular line graph, named knowledge node (K node), and make connections between Next, LiGhT is employed to update the feature embeddings of nodes in the graph. Finally, the derived node embeddings of the masked nodes and K node are used to predict the node types and masked knowledge, respectively.</p><p>K node and each node individually. The raw features of the K node are initialized by the quantitative molecular descriptors and fingerprints. As such, in the pre-training, the extra knowledge can be attended by other nodes through the self-attention module in the transformer and thus guide the prediction of the masked nodes.</p><p>In the pre-training, we also randomly mask a proportion of the initial features of K nodes and learn to predict the masked molecular descriptors and fingerprints. The prediction of the masked molecular descriptors is formulated as a regression task equipped with an RMSE loss, while the prediction of fingerprints is formulated as a binary classification task equipped with a cross-entropy loss.</p><p>An illustrative example of our self-supervised learning strategy is shown in Figure <ref type="figure" target="#fig_3">3</ref>. In summary, by explicitly introducing additional knowledge in the pre-training, KPGT is able to correct the current ill-defined generative pre-training tasks and thus enables the model to capture both the abundant structural and the rich semantics information from the molecular graphs, which can be essential to the downstream molecular property prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Settings</head><p>Dataset. For pre-training, we used two million unique unlabeled molecular SMILES samples from the ChEMBL29 dataset <ref type="bibr" target="#b13">[14]</ref>. We employed RDKit <ref type="bibr" target="#b17">[18]</ref> to abstract the molecular graphs from the SMILES strings and initialize the features of nodes and edges (see Appendix A.2.1 for more details).</p><p>After pre-training, we evaluated the effectiveness of KPGT on eleven molecular property datasets that have been widely used in the previous research, including eight classification datasets and three regression datasets, covering properties from multiple domains (see Appendix A. <ref type="bibr" target="#b2">3</ref> for more details about these datasets).</p><p>In practice, the test molecules can be structurally different from the training molecules. Therefore, we used scaffold splitting, which split a dataset according to the graph substructures of the molecules in the dataset, offering a more challenging yet realistic scenario. We split each dataset into training, validation and test sets with a ratio of 8:1:1. Following the previous research <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b54">55]</ref>, for each dataset, we performed three repeated tests with three randomly seeded splittings and then reported the means and standard deviations.</p><p>Additional Knowledge. In this work, we did not carefully collect the additional knowledge, though it would better benefit KPGT, intuitively. More specifically, we collected two sets of knowledge derived from RDKit <ref type="bibr" target="#b17">[18]</ref>. The first set contained 200 molecular descriptors that had been widely used for molecular property prediction <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b59">60]</ref>. The second set contained the most common 512 RDKit fingerprints, where each bit indicated the existence of a specific path of length between one and seven in the molecules.</p><p>Training Details. We mainly used Pytorch <ref type="bibr" target="#b38">[39]</ref> and DGL <ref type="bibr" target="#b51">[52]</ref> to implement KPGT. More specifically, we implemented a 12-layer LiGhT as the backbone network with a hidden size of 768 and the number of self-attention heads as 12. A mean pooling operation that averaged all the nodes in individual graphs was applied on top of the model to extract molecular representations. An Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with weight decay 1? -6 and learning rate 2? -4 was used to optimize the model. The model was trained with batch size 1024 for a total of 100,000 steps. The KPGT had around 100 million parameters. We set the masking rate of both nodes and additional knowledge to 0.5. The pre-training of KPGT took about two days on four Nvidia A100 GPUs.</p><p>For the downstream fine-tuning tasks, we added a predictor, which was a 2-layer multi-layer perceptron (MLP) with ReLU activation <ref type="bibr" target="#b1">[2]</ref>, on top of the base feature extractors. The cross-entropy loss and RMSE loss were implemented for binary classification tasks and regression tasks, respectively. The model was trained using an Adam optimizer with batch size 32 for another 50 epochs. Early stopping was employed with patience set to 20. For each method, we  <ref type="bibr" target="#b19">[20]</ref> 0.817 (0.034) 0.873 (0.016) 0.730 (0.017) 0.603 (0.025) 0.881 (0.061) 0.844 (0.054) 0.818 (0.025) 0.712 (0.011) 0.785 Masking <ref type="bibr" target="#b24">[25]</ref> 0.823 (0.004) 0.864 (0.028) 0.729 (0.039) 0.573 (0.012) 0.869 (0.050) 0.868 (0.061) 0.798 (0.025) 0.663 (0.018) 0.773 Contextpred <ref type="bibr" target="#b24">[25]</ref> 0.840 (0.009) 0.877 (0.026) 0.732 (0.017) 0.609 (0.013) 0.882 (0.065) 0.857 (0.053) 0.806 (0.012) 0.714 (0.018) 0.790 Masking+Sup <ref type="bibr" target="#b24">[25]</ref> 0.824 (0.021) 0.859 (0.020) 0.796 (0.079) 0.606 (0.005) 0.888 (0.041) 0.872 (0.051) 0.827 (0.021) 0.715 (0.007) 0.799 Contextpred+Sup <ref type="bibr" target="#b24">[25]</ref> 0.855 (0.023) 0.875 (0.011) 0.802 (0.079) 0.620 (0.009) 0.885 (0.053) 0.859 (0.055) 0.840 (0.023) 0.724 (0.015) 0.807 GraphLoG <ref type="bibr" target="#b57">[58]</ref> 0.830 (0.014) 0.846 (0.008) 0.667 (0.021) 0.615 (0.013) 0.871 (0.054) 0.850 (0.080) 0.796 (0.025) 0.677 (0.019) 0.769 GraphCL <ref type="bibr" target="#b62">[63]</ref> 0.825 (0.018) 0.887 (0.019) 0.691 (0.065) 0.587 (0.026) 0.875 (0.048) 0.821 (0.066) 0.805 (0.017) 0.696 (0.023) 0.774 JOAO <ref type="bibr" target="#b61">[62]</ref> 0.826 (0.029) 0.879 (0.020) 0.741 (0.047) 0.640 (0.010) 0.861 (0.066) 0.837 (0.058) 0.823 (0.022) 0.711 (0.014) 0.790 GROVER <ref type="bibr" target="#b43">[44]</ref> 0.840 (0.030) 0.887 (0.006) 0.874 (0.048) 0.638 (0.005) 0.892 (0.044) 0.876 (0.038) 0.838 (0.017) 0.696 (0.014) 0.818 3DInfomax <ref type="bibr" target="#b45">[46]</ref> 0.811 (0.048) 0.877 (0.014) 0.887 (0.033) 0.585 (0.017) 0.880 (0.054) 0.866 (0.047) 0.805 (0.032) 0.716 (0.013) 0.804 GraphMVP <ref type="bibr" target="#b31">[32]</ref> 0.818 (0.012) 0.860 (0.034) 0.719 (0.044) 0.584 (0.026) 0.865 (0.061) 0.820 (0.066) 0.799 (0.018) 0.689 (0.010) 0.769 KPGT 0.855 (0.011) 0.908 (0.010) 0.946 (0.022) 0.649 (0.009) 0.905 (0.028) 0.889 (0.047) 0.848 (0.013) 0.746 (0.002) 0.843 tried 48 hyper-parameter combinations to find the best results for each task. For more training details please refer to Appendix A.2.2. Evaluation Protocols. The downstream molecular property prediction tests were performed under two evaluation protocols, that is, the feature extraction setting and the transfer learning setting. For the feature extraction setting, we first fixed the pre-trained model and used it as a feature extractor to obtain the molecular graph representations of data samples, and then trained the predictor to make predictions. For the transfer learning setting, we fine-tuned all the parameters in the model.</p><p>Baselines. We comprehensively evaluated KPGT against twelve state-of-the-art self-supervised learning methods on molecular graphs, including six generative methods, i.e., Edgepred <ref type="bibr" target="#b19">[20]</ref>, Masking <ref type="bibr" target="#b24">[25]</ref>, Contextpred <ref type="bibr" target="#b24">[25]</ref>, Masking+Sup <ref type="bibr" target="#b24">[25]</ref>, Contextpred+Sup <ref type="bibr" target="#b24">[25]</ref> and GROVER <ref type="bibr" target="#b43">[44]</ref>, and six contrastive methods, including Infomax <ref type="bibr" target="#b50">[51]</ref>, GraphLoG <ref type="bibr" target="#b57">[58]</ref>, GraphCL <ref type="bibr" target="#b62">[63]</ref>, JOAO <ref type="bibr" target="#b61">[62]</ref>, 3DInfomax <ref type="bibr" target="#b45">[46]</ref> and GraphMVP <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Downstream Tasks</head><p>We reported the evaluation results of molecular property prediction under both feature extraction and transfer learning settings in Tables <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_2">2</ref>. The comprehensive test results suggested the following trends:</p><p>Observation 1. From the test results, KPGT outperformed baseline methods with considerable margins on all classification and regression datasets under both feature extraction and transfer learning settings. For the feature extraction setting, the overall relative improvement was 5% on all datasets, with 3.5% on classification datasets and 8.9% on regression datasets. For the transfer learning setting, the overall relative improvement was 3.9% on all datasets, with 2.2% on classification datasets and 8.3% on regression datasets. This significant improvement demonstrated that KPGT can provide a useful tool for accurate molecular property prediction. Observation 2. Methods incorporating additional knowledge of molecules generally achieved better performance. Among the baseline methods, Contextpred+Sup, Masking+Sup and GROVER incorporated additional knowledge of molecules by introducing graph-level pre-training tasks, that is, learning to predict specific properties of molecules, e.g., bio-activities and motifs, using the graph representations. Although their node-level pre-training tasks (i.e., predicting the masked nodes or subgraphs) were ill-defined, these methods still generally outperformed other baseline methods that did not exploit additional knowledge, which indicated that incorporating additional knowledge was important to the success of the self-supervised learning methods on molecular graphs.</p><p>Observation 3. Methods employing high-capacity backbone networks achieved better performance. Among all the baseline methods, GROVER also employed a transformer-based model with around 100 million parameters as its backbone network and achieved superior performance in comparison with other baseline methods. This result further validated our point that a high-capacity backbone network was necessary to yield promising results for self-supervised learning on molecular graphs.</p><p>Observation 4. The methods under the transfer learning setting generally achieved better performance over those under the feature extraction setting. This result was reasonable because fine-tuning all the parameters enabled the models to capture more task-specific information. However, on some datasets, KPGT under the transfer learning setting only achieved comparable (i.e., on the SIDER and MetStab datasets) or even worse (i.e., on the BACE dataset) performance compared to that under the feature extraction setting, which may be caused by the catastrophic forgetting problem in the finetuning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>. Therefore, exploring different fine-tuning strategies is a potential future direction to further improve the performance of molecular property prediction. Next, we asked whether simply using the same additional knowledge employed in KPGT could provide better results. We introduced two baseline methods, which also incorporated the same additional knowledge employed in our framework. First, we directly concatenated the molecular descriptors and fingerprints employed in our framework as input and then passed through a 2-layer MLP for prediction (denoted by MD+FP). Note that the previous method GROVER <ref type="bibr" target="#b43">[44]</ref> had already concatenated the molecular descriptors employed in our framework to the graph representation derived from the readout function. Here, we further concatenated the fingerprints to such output and passed through a 2-layer MLP for the final prediction (denoted by GROVER+FP). For a fair comparison, we fixed the parameters of the feature extractors in both GROVER+FP and KPGT (i.e., the feature extraction setting). As shown in Table <ref type="table" target="#tab_5">5</ref>, KPGT still outperformed the baseline methods. These results confirmed that our self-supervised learning strategy can potentially learn the abundant semantics of molecules beyond the introduced additional knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Effect of Different Masking</head><p>Rates. In our self-supervised learning strategy, we masked a proportion of the nodes and additional knowledge in the molecular line graph. Here, we evaluated the effect of different masking rates. In particular, we individually pretrained KPGT with the masking rates of 15%, 30%, 50% and 60%, and reported the corresponding prediction results on the downstream tasks (see Table <ref type="table" target="#tab_6">6</ref>). The results showed that setting the masking rate to 50% achieved the best prediction performance. Such an optimal masking rate was much larger than that equipped in the previous  <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref>, which indicated that the additional knowledge incorporated in KPGT accurately guided the model to predict the masked nodes and thus enabled it to capture the semantics of molecules. Moreover, recent research in the field of CV also has shown that larger masking rates in the pre-training resulted in better results on the downstream tasks <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we first point out the two major issues of current self-supervised learning methods on molecular graphs, that is, the ill-defined pre-training tasks and the limited model capacity. Next, we propose a novel self-supervised learning framework, named Knowledge-guided Pre-training of Graph Transformer (KPGT), which consists of two main components, that is, the Line Graph Transformer (LiGhT) and a knowledge-guided pre-training strategy, to alleviate the aforementioned issues and learn more generalizable, transferable and robust representations of molecular graphs to improve the performance of the downstream molecular property prediction tasks. Extensive test results demonstrated that KPGT can offer superior performance over current state-of-the-art methods on several molecular property prediction datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative example of the transformation of a molecular graph to a molecular line graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3. 1 . 1</head><label>11</label><figDesc>Molecular Line Graph. Given a molecule, it can be represented as a molecular graph G = (V, E), where V = {? ? } ? ? [1,? ? ] stands for the set of nodes (i.e., atoms), E = {? ?,? } ?,? ? [1,? ? ] stands for the set of edges (i.e., chemical bonds), and ? ? stands for the number of nodes. The initial features of node ? ? are represented by ? ? ? ? R ? ? and the initial features of edge ? ?,? are represented by ? ? ?,? ? R ? ? , where ? ? and ? ? stand for the dimensions of features of nodes and edges, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the Line Graph Transformer (LiGhT) architecture. LiGhT is on the basis of a classic transformer encoder and introduces path encoding (PE) and distance encoding (DE) into the multi-head self-attention module to capture the structural information of molecules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of the knowledge-guided pre-training strategy.For a molecular graph, we first transform it to a line graph and extract its additional knowledge. Then, we randomly mask a proportion of the nodes in the line graph and the same proportion of the additional knowledge and utilize the masked knowledge to initialize the virtually constructed K node. Next, LiGhT is employed to update the feature embeddings of nodes in the graph. Finally, the derived node embeddings of the masked nodes and K node are used to predict the node types and masked knowledge, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The AUROC performance of different methods on the classification datasets under both feature extraction and transfer learning settings. The higher result is better (marked by ?). AVG represents the averaging results over all the datasets. The numbers in brackets are the standard deviations. The best result for each dataset is marked in bold and the second-best result is underlined. (0.044) 0.806 (0.037) 0.743 (0.014) 0.585 (0.021) 0.852 (0.054) 0.784 (0.072) 0.738 (0.021) 0.609 (0.021) 0.741 GraphMVP [32] 0.741 (0.012) 0.836 (0.028) 0.696 (0.051) 0.563 (0.023) 0.833 (0.071) 0.669 (0.057) 0.778 (0.022) 0.645 (0.006) 0.720 KPGT 0.868 (0.011) 0.896 (0.010) 0.856 (0.028) 0.644 (0.018) 0.890 (0.048) 0.885 (0.047) 0.838 (0.020) 0.727 (0.016) 0.825 (0.008) 0.840 (0.026) 0.661 (0.026) 0.616 (0.024) 0.888 (0.056) 0.837 (0.066) 0.816 (0.021) 0.690 (0.012) 0.773 Edgepred</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Classification dataset ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Feature extraction setting</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BACE</cell><cell>BBBP</cell><cell>ClinTox</cell><cell>SIDER</cell><cell>Estrogen</cell><cell>MetStab</cell><cell>Tox21</cell><cell>ToxCast</cell><cell>AVG</cell></row><row><cell>Infomax [51]</cell><cell cols="9">0.825 (0.033) 0.833 (0.017) 0.637 (0.027) 0.573 (0.016) 0.878 (0.061) 0.763 (0.044) 0.764 (0.038) 0.662 (0.010) 0.742</cell></row><row><cell>Edgepred [20]</cell><cell cols="9">0.807 (0.014) 0.823 (0.011) 0.637 (0.059) 0.545 (0.019) 0.851 (0.070) 0.792 (0.064) 0.702 (0.040) 0.620 (0.004) 0.722</cell></row><row><cell>Masking [25]</cell><cell cols="9">0.739 (0.059) 0.823 (0.031) 0.564 (0.089) 0.555 (0.032) 0.805 (0.075) 0.588 (0.102) 0.722 (0.017) 0.591 (0.023) 0.673</cell></row><row><cell>Contextpred [25]</cell><cell cols="9">0.822 (0.053) 0.875 (0.009) 0.607 (0.062) 0.585 (0.025) 0.872 (0.059) 0.791 (0.078) 0.757 (0.028) 0.671 (0.009) 0.748</cell></row><row><cell>Masking+Sup [25]</cell><cell cols="9">0.818 (0.039) 0.849 (0.024) 0.792 (0.043) 0.611 (0.003) 0.888 (0.038) 0.819 (0.054) 0.828 (0.013) 0.685 (0.013) 0.786</cell></row><row><cell cols="10">Contextpred+Sup [25] 0.831 (0.018) 0.852 (0.039) 0.765 (0.061) 0.612 (0.026) 0.871 (0.051) 0.807 (0.039) 0.829 (0.026) 0.687 (0.008) 0.782</cell></row><row><cell>GraphLoG [58]</cell><cell cols="9">0.766 (0.040) 0.799 (0.025) 0.526 (0.101) 0.583 (0.016) 0.867 (0.071) 0.747 (0.063) 0.691 (0.041) 0.608 (0.011) 0.698</cell></row><row><cell>GraphCL [63]</cell><cell cols="9">0.802 (0.045) 0.836 (0.021) 0.663 (0.018) 0.576 (0.011) 0.864 (0.050) 0.779 (0.062) 0.758 (0.036) 0.639 (0.012) 0.740</cell></row><row><cell>JOAO [62]</cell><cell cols="9">0.834 (0.033) 0.854 (0.023) 0.747 (0.039) 0.612 (0.007) 0.854 (0.061) 0.800 (0.062) 0.801 (0.023) 0.653 (0.014) 0.769</cell></row><row><cell>GROVER [44]</cell><cell cols="9">0.809 (0.046) 0.885 (0.009) 0.809 (0.048) 0.591 (0.030) 0.870 (0.048) 0.818 (0.056) 0.827 (0.019) 0.708 (0.015) 0.790</cell></row><row><cell cols="2">3DInfomax [46] 0.810 Method</cell><cell></cell><cell></cell><cell cols="3">Classification dataset ? Transfer learning setting</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BACE</cell><cell>BBBP</cell><cell>ClinTox</cell><cell>SIDER</cell><cell>Estrogen</cell><cell>MetStab</cell><cell>Tox21</cell><cell>ToxCast</cell><cell>AVG</cell></row><row><cell>Infomax [51]</cell><cell>0.839</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The RMSE performance of different methods on the regression datasets under both feature extraction and transfer learning settings. The lower result is better (marked by ?). AVG represents the averaging results over all the datasets. The numbers in brackets are the standard deviations. The best result for each dataset is marked in bold and the second-best result is underlined.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Regression dataset ?</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Feature extraction setting</cell><cell></cell><cell cols="2">Transfer learning setting</cell></row><row><cell></cell><cell>FreeSolv</cell><cell>ESOL</cell><cell>Lipo</cell><cell>AVG FreeSolv</cell><cell>ESOL</cell><cell>Lipo</cell><cell>AVG</cell></row><row><cell>Infomax [51]</cell><cell cols="7">4.119 (0.974) 1.462 (0.076) 0.978 (0.076) 2.186 3.416 (0.928) 1.096 (0.116) 0.799 (0.047) 1.770</cell></row><row><cell>Edgepred [20]</cell><cell cols="7">3.849 (0.950) 2.272 (0.213) 1.030 (0.024) 2.384 3.076 (0.585) 1.228 (0.073) 0.719 (0.013) 1.674</cell></row><row><cell>Masking [25]</cell><cell cols="7">3.646 (0.947) 2.100 (0.040) 1.063 (0.028) 2.270 3.040 (0.334) 1.326 (0.115) 0.724 (0.012) 1.697</cell></row><row><cell>Contextpred [25]</cell><cell cols="7">3.141 (0.905) 1.349 (0.069) 0.969 (0.076) 1.820 2.890 (1.077) 1.077 (0.029) 0.722 (0.034) 1.563</cell></row><row><cell>Masking+Sup [25]</cell><cell cols="7">3.210 (0.876) 1.387 (0.007) 0.725 (0.033) 1.774 2.883 (0.559) 1.297 (0.114) 0.681 (0.026) 1.620</cell></row><row><cell cols="8">Contextpred+Sup [25] 3.105 (0.701) 1.477 (0.038) 0.754 (0.032) 1.779 2.383 (0.624) 1.178 (0.026) 0.681 (0.030) 1.414</cell></row><row><cell>GraphLoG [58]</cell><cell cols="7">4.174 (1.077) 2.335 (0.073) 1.104 (0.024) 2.537 2.961 (0.847) 1.249 (0.010) 0.780 (0.020) 1.663</cell></row><row><cell>GraphCL [63]</cell><cell cols="7">4.014 (1.361) 1.835 (0.111) 0.945 (0.024) 2.264 3.149 (0.273) 1.540 (0.086) 0.777 (0.034) 1.822</cell></row><row><cell>JOAO [62]</cell><cell cols="7">3.466 (1.114) 1.771 (0.053) 0.933 (0.027) 2.056 3.950 (1.202) 1.220 (0.028) 0.710 (0.031) 1.960</cell></row><row><cell>GROVER [44]</cell><cell cols="7">2.991 (1.052) 0.928 (0.027) 0.752 (0.010) 1.557 2.385 (1.047) 0.986 (0.126) 0.625 (0.006) 1.332</cell></row><row><cell>3DInfomax [46]</cell><cell cols="7">2.919 (0.243) 1.906 (0.246) 1.045 (0.040) 1.957 2.639 (0.772) 0.891 (0.131) 0.671 (0.033) 1.400</cell></row><row><cell>GraphMVP [32]</cell><cell cols="7">2.532 (0.247) 1.937 (0.147) 0.990 (0.024) 1.819 2.874 (0.756) 1.355 (0.038) 0.712 (0.025) 1.647</cell></row><row><cell>KPGT</cell><cell cols="7">2.314 (0.841) 0.848 (0.103) 0.656 (0.023) 1.273 2.121 (0.837) 0.803 (0.008) 0.600 (0.010) 1.175</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison results of different backbone networks on classification and regression datasets, measured in terms of AUROC and RMSE, respectively, under the transfer learning setting.</figDesc><table><row><cell>Backbone</cell><cell>Classification dataset ? AVG</cell><cell>Regression dataset ? AVG</cell></row><row><cell>GIN [57]</cell><cell>0.817</cell><cell>1.229</cell></row><row><cell>Vanilla Transformer</cell><cell>0.822</cell><cell>1.260</cell></row><row><cell>Graphormer [61]</cell><cell>0.826</cell><cell>1.189</cell></row><row><cell>LiGhT</cell><cell>0.829</cell><cell>1.162</cell></row><row><cell cols="2">4.3 Ablation Studies</cell><cell></cell></row><row><cell cols="3">4.3.1 How Powerful is the LiGhT Backbone Network? To evaluate</cell></row><row><cell cols="3">the expressive power of LiGhT, we replaced LiGhT in KPGT with</cell></row><row><cell cols="3">other backbone networks, including GIN, vanilla Transformer (i.e.,</cell></row><row><cell cols="3">without path encoding and distance encoding) and Graphormer [61].</cell></row><row><cell cols="3">For a fair comparison, we restricted all the models to have nearly the</cell></row><row><cell cols="3">same number of parameters (i.e., 3.5 million). Then we individually</cell></row><row><cell cols="3">pre-trained the models with our self-supervised learning strategy.</cell></row><row><cell cols="3">As shown in Table 3, our backbone network LiGhT consistently</cell></row><row><cell cols="3">outperformed GIN, vanilla Transformer and Graphormer, which</cell></row><row><cell cols="3">verified that LiGhT can provide a reliable backbone network for</cell></row><row><cell cols="3">self-supervised learning on molecular graphs.</cell></row><row><cell cols="3">4.3.2 How Powerful is the Knowledge-Guided Pre-training Strategy?</cell></row><row><cell cols="3">To evaluate our self-supervised learning strategy, we first compared</cell></row></table><note><p><p><p>the prediction performances of the pre-trained LiGhT (KPGT) and the LiGhT without pre-training, both of which followed the same hyper-parameter settings. As shown in Table</p>4</p>, the pre-trained model consistently achieved superior prediction performance and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison results of the pre-trained LiGhT (KPGT) and LiGhT (i.e., without pre-training) on classification and regression datasets in terms of AUROC and RMSE, respectively, under the transfer learning setting.</figDesc><table><row><cell></cell><cell>Classification dataset ?</cell><cell>Regression dataset ?</cell></row><row><cell></cell><cell>AVG</cell><cell>AVG</cell></row><row><cell>LiGhT</cell><cell>0.803</cell><cell>1.473</cell></row><row><cell>KPGT</cell><cell>0.843</cell><cell>1.175</cell></row><row><cell>Relative improvement</cell><cell>5.1%</cell><cell>18.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison results between KPGT and baseline methods, which also incorporated the same additional knowledge, on classification and regression datasets, measured in terms of AUROC and RMSE, respectively, under the feature extraction setting.</figDesc><table><row><cell></cell><cell>Classification dataset ?</cell><cell>Regression dataset ?</cell></row><row><cell></cell><cell>AVG</cell><cell>AVG</cell></row><row><cell>MD+FP</cell><cell>0.776</cell><cell>1.698</cell></row><row><cell>GROVER+FP</cell><cell>0.789</cell><cell>1.541</cell></row><row><cell>KPGT</cell><cell>0.825</cell><cell>1.273</cell></row><row><cell cols="3">the relative improvements were 5.1% and 18.8% on classification</cell></row><row><cell cols="2">and regression tasks, respectively.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation results of KPGT with different masking rates on classification and regression datasets, measured in terms of AUROC and RMSE, respectively, under the transfer learning setting.</figDesc><table><row><cell>Masking rate</cell><cell>Classification dataset ? AVG</cell><cell>Regression dataset ? AVG</cell></row><row><cell>15%</cell><cell>0.827</cell><cell>1.228</cell></row><row><cell>30%</cell><cell>0.835</cell><cell>1.216</cell></row><row><cell>50%</cell><cell>0.843</cell><cell>1.175</cell></row><row><cell>60%</cell><cell>0.832</cell><cell>1.186</cell></row><row><cell cols="2">methods (i.e., 15%)</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2021YFF1201300</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61872216</rs>, <rs type="grantNumber">T2125007</rs> to JZ, <rs type="grantNumber">31900862</rs> to DZ), the <rs type="institution">Turing AI Institute of Nanjing</rs>, and the <rs type="funder">Tsinghua-Toyota Joint Research Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Udu85S6">
					<idno type="grant-number">2021YFF1201300</idno>
				</org>
				<org type="funding" xml:id="_u7hTa2B">
					<idno type="grant-number">61872216</idno>
				</org>
				<org type="funding" xml:id="_JFuHV8J">
					<idno type="grant-number">T2125007</idno>
				</org>
				<org type="funding" xml:id="_NE7CMke">
					<idno type="grant-number">31900862</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>A.2.1 Molecular Graph Construction. We abstracted the molecular graph and initialized the features of nodes and edges in the molecular graphs via RDKit <ref type="bibr" target="#b17">[18]</ref>. Tables <ref type="table">7</ref> and<ref type="table">8summarize</ref> the atom and bond features used in our experiments, respectively.  <ref type="table">9</ref>. For KPGT and baseline methods, we tried 48 hyper-parameter combinations of different learning rates, dropout rates, and weight decay listed in Table <ref type="table">9</ref> via grid search to find the best results for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Datasets</head><p>The detailed information of the molecular property datasets used in our experiments is summarized in Table <ref type="table">10</ref>. The details of each molecular property dataset are listed below:</p><p>? BACE is a collection of molecules that can act as the inhibitors of human ?-secretase 1 (BACE-1) <ref type="bibr" target="#b46">[47]</ref>. ? BBBP is a dataset that records whether a molecule can penetrate the blood-brain barrier <ref type="bibr" target="#b33">[34]</ref>. ? ClinTox is a collection of drugs approved through the Food and Drug Administration (FDA) and eliminated due to the toxicity during clinical trials <ref type="bibr" target="#b15">[16]</ref>.</p><p>? SIDER records the adverse drug reactions of marked drugs <ref type="bibr" target="#b28">[29]</ref>.</p><p>? ToxCast contains multiple toxicity labels for molecules obtained through high-throughput screening tests <ref type="bibr" target="#b41">[42]</ref>. ? Tox21 is a public database containing the toxicity of compounds <ref type="bibr" target="#b0">[1]</ref>. ? Estrogen contains the molecules with known activities towards the estrogen receptors extracted from the ChEMBL dataset <ref type="bibr" target="#b14">[15]</ref>. ? MetStab is a dataset measuring the half-life time of molecules within an organism <ref type="bibr" target="#b39">[40]</ref>. ? ESOL is a dataset recording the solubility of compounds <ref type="bibr" target="#b8">[9]</ref>.</p><p>? Lipophilicity is a dataset measuring the molecular membrane permeability and solubility <ref type="bibr" target="#b14">[15]</ref>. ? FreeSolv contains the hydration free energy of small molecules in water from both experiments and alchemical free energy calculation <ref type="bibr" target="#b36">[37]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tox21 Challenge</title>
		<ptr target="http://tripod.nih.gov/tox21/challenge/" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Learning using Rectified Linear Units (ReLU)</title>
		<author>
			<persName><forename type="first">Abien</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agarap</forename></persName>
		</author>
		<idno>CoRR abs/1803.08375</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Machine learning for molecular and materials science</title>
		<author>
			<persName><forename type="first">T</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">W</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexandr</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aron</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName><surname>Walsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">559</biblScope>
			<biblScope unit="page" from="547" to="555" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Molecular fingerprint similarity search in virtual screening</title>
		<author>
			<persName><forename type="first">Adri?</forename><surname>Cereto-Massagu?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="58" to="63" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Path-Augmented Graph Transformer Network</title>
		<author>
			<persName><forename type="first">Benson</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>CoRR abs/1905.12712</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ESOL: estimating aqueous solubility directly from molecular structure</title>
		<author>
			<persName><forename type="first">Delaney</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1000" to="1005" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ADMETlab: a platform for systematic ADMET evaluation based on a comprehensively collected ADMET database</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning-Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Jiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defang</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ai-Ping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Sheng</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vishrav Chaudhary, et al. 2021. Beyond english-centric multilingual machine translation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">GPT-3: Its nature, scope, limits, and consequences. Minds and Machines</title>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Floridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Chiriatti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="681" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The ChEMBL database in 2017</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Gaulton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="945" to="D954" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ChEMBL: a large-scale bioactivity database for drug discovery</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Gaulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louisa</forename><forename type="middle">J</forename><surname>Bellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Bento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Hersey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><surname>Mcglinchey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Michalovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bissan</forename><surname>Al-Lazikani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1100" to="D1107" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A data-driven approach to predicting successes and failures of clinical trials</title>
		<author>
			<persName><forename type="first">Kaitlyn</forename><forename type="middle">M</forename><surname>Gayvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><forename type="middle">S</forename><surname>Madhukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Elemento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell chemical biology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1294" to="1301" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5589557</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5589557" />
		<title level="m">rdkit/rdkit: 2021_09_2 (Q3 2021) Release</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring network structure, dynamics, and function using NetworkX</title>
		<author>
			<persName><forename type="first">Aric</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Swart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Chult</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Los Alamos, NM (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Los Alamos National Lab.(LANL)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Masked Autoencoders Are Scalable Vision Learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>CoRR abs/2111.06377</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the national academy of sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The SIDER database of drugs and side effects</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivica</forename><surname>Letunic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><forename type="middle">Juhl</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1075" to="D1079" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting</title>
		<author>
			<persName><forename type="first">Xilai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3925" to="3934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules</title>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Furkan Demirel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="8464" to="8476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07728</idno>
		<title level="m">Pre-training Molecular Graph Representation with 3D Geometry</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09883</idno>
		<title level="m">Swin Transformer V2: Scaling Up Capacity and Resolution</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Bayesian approach to in silico blood-brain barrier penetration modeling</title>
		<author>
			<persName><forename type="first">Ines Filipa</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">L</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">O</forename><surname>Falcao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1686" to="1697" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Maziarka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Danel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slawomir</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Rataj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacek</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<idno>CoRR abs/2002.08264</idno>
		<title level="m">Molecule Attention Transformer</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margot</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05667</idno>
		<title level="m">Graphit: Encoding graph structure in transformers</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FreeSolv: a database of experimental and calculated hydration free energies, with input files</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mobley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guthrie</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mordred: a molecular descriptor calculator</title>
		<author>
			<persName><forename type="first">Hirotomo</forename><surname>Moriwaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Shi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norihito</forename><surname>Kawashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MetStabOn-online platform for metabolic stability predictions</title>
		<author>
			<persName><forename type="first">Sabina</forename><surname>Podlewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafa?</forename><surname>Kafel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of molecular sciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1040</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep Learning for the Life Sciences</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ToxCast chemical landscape: paving the road to 21st century toxicology</title>
		<author>
			<persName><forename type="first">Ann</forename><forename type="middle">M</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical research in toxicology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1225" to="1251" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-Attention with Relative Position Representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2018. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Hannes</forename><surname>St?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04126</idno>
		<title level="m">Stephan G?nnemann, and Pietro Li?. 2021. 3D Infomax improves GNNs for Molecular Property Prediction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Computational modeling of ?-secretase 1 (BACE-1) inhibitors using ligand based approaches</title>
		<author>
			<persName><forename type="first">Govindan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiah Aldrin</forename><surname>Denny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1936" to="1949" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Machine learning for chemical discovery</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ADMET in silico modelling: towards prediction paradise?</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Waterbeemd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews Drug discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="192" to="204" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR 2019. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhonglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Barati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farimani</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10056</idno>
		<title level="m">MolCLR: molecular contrastive learning of representations via graph neural networks</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A compact review of molecular property prediction with graph neural networks</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kohlbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M?laine</forename><surname>Kuenemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Garon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ducrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Langer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug Discovery Today: Technologies</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">MoleculeNet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism</title>
		<author>
			<persName><forename type="first">Zhaoping</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="8749" to="8760" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Selfsupervised Graph-level Representation Learning with Local and Global Structure</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="11548" to="11558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Molecular descriptors in chemoinformatics, computational combinatorial chemistry, and virtual screening</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jurgen</forename><surname>Bajorath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorial chemistry &amp; high throughput screening</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="363" to="372" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Analyzing learned molecular representations for property prediction</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="3370" to="3388" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Do Transformers Really Perform Badly for Graph Representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS 2021</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning Automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12121" to="12132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Graph Contrastive Learning with Augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
