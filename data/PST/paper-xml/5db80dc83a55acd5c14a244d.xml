<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">META LEARNING FOR END-TO-END LOW-RESOURCE SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-26">26 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jui-Yang</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan-Jui</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<email>hungyilee@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">META LEARNING FOR END-TO-END LOW-RESOURCE SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-26">26 Oct 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1910.12094v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>meta-learning</term>
					<term>low-resource</term>
					<term>speech recognition</term>
					<term>language adaptation</term>
					<term>IARPA-BABEL</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we proposed to apply meta learning approach for low-resource automatic speech recognition (ASR). We formulated ASR for different languages as different tasks, and meta-learned the initialization parameters from many pretraining languages to achieve fast adaptation on unseen target language, via recently proposed model-agnostic meta learning algorithm (MAML). We evaluated the proposed approach using six languages as pretraining tasks and four languages as target tasks. Preliminary results showed that the proposed method, MetaASR, significantly outperforms the state-of-the-art multitask pretraining approach on all target languages with different combinations of pretraining languages. In addition, since MAML's model-agnostic property, this paper also opens new research direction of applying meta learning to more speech-related applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the recent advances of deep learning, integrating the main modules of automatic speech recognition (ASR) such as acoustic model, pronunciation lexicon, and language model into a single end-to-end model is highly attractive. Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b0">[1]</ref> lends itself on such end-to-end approaches by introducing an additional blank symbol and specifically-designed loss function optimizing to generate the correct character sequences from the speech signal directly, without framewise phoneme alignment in advance. With many recent results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, end-to-end deep learning has created a larger interest in the speech community.</p><p>However, to build such an end-to-end ASR system requires a huge amount of paired speech-transcription data, which is costly. For most languages in the world, they lack sufficient paired data for training. Pretraining on other language sources as the initialization, then fine-tuning on target language is the dominant approach under such low-resource setting, also known as multilingual transfer learning / pretraining (MultiASR) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. The backbone of MultiASR is a multitask model with shared hidden layers (encoder), and many language-specific heads. The model structure is designed to learn an encoder to extract language-independent representations to build a better acoustic model from many source languages. The success of "language-independent" features to improve ASR performance compared to monolingual training has been shown in many recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Besides directly training the model with all the source languages, there are various variants of MultiASR approaches. Language-adversarial training approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> introduced language-adversarial classification objective to the shared encoder, negating the gradients backpropagated from the language classifier to encourage the encoder to extract more language-independent representations. Hierarchical approaches <ref type="bibr" target="#b12">[13]</ref> introduced different granularity objectives by combining both character and phoneme prediction at different levels of the model.</p><p>In this paper, we provide a novel research direction following up on the idea of multilingual pretraining -Meta learning. Meta learning, or learning-to-learn, has recently received considerable interest in the machine learning community. The goal of meta learning is to solve the problem of "fast adaptation on unseen data", which is aligned with our low-resource setting. With its success in computer vision under the few-shot learning setting <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, there have been some works in language and speech processing, for instance, Fig. <ref type="figure">2</ref>: Multilingual CTC model architecture language transfer in neural machine translation <ref type="bibr" target="#b9">[10]</ref>, dialogue generation <ref type="bibr" target="#b16">[17]</ref>, and speaker adaptive training <ref type="bibr" target="#b17">[18]</ref>, but not multilingual pretraining for speech recognition.</p><p>We use model-agnostic meta-learning algorithm (MAML) <ref type="bibr" target="#b18">[19]</ref> in this work. As its name suggestes, MAML can be applied to any network architecture. MAML only modifies the optimization process following meta learning training scheme. It does not introduce additional modules like adversarial training or requires phoneme level annotation (usually through lexicon) such as hierarchical approaches. We evaluated the effectiveness of the proposed meta learning algorithm, MetaASR, on the IARPA BABEL dataset <ref type="bibr" target="#b19">[20]</ref>. Our experiments reveal that MetaASR outperforms MultiASR significantly across all target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multilingual CTC Model</head><p>We used the model architecture as illustrated in Fig. <ref type="figure">2</ref>, the shared encoder is parameterized by θ, and the set of languagespecific heads are parameterized by θ h,l (the head for l-th language). Let the dataset be D, composed of paired data</p><formula xml:id="formula_0">(X, C). Let X = x 1 , x 2 , • • • , x T with length T as input fea- ture, C = c 1 , c 2 , • • • , c L with length L as target label.</formula><p>X is encoded into sequence of hidden states through the shared encoder, then fed into the language-specific head of the corresponding language with softmax activation to output the prediction sequence Ĉ = ĉ1 , ĉ2 , • • • , ĉL with length L .</p><p>CTC Loss. CTC computes the posterior probability as below,</p><formula xml:id="formula_1">P (C|X) = π∈Z(C) P (π|X) (1)</formula><p>where π is the repeated character sequence of C with additional blank label, and Z(C) is the set of all possible sequences given C. For each π, we can approximate the posterior probability as below,</p><formula xml:id="formula_2">P (π|X) ≈ L i=1 P ( ĉi |X)<label>(2)</label></formula><p>Take X belonging to the l-th language for instance, the loss function of the model on D is then defined as:</p><formula xml:id="formula_3">L D (θ, θ h,l ) = − log P (C|X)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Meta Learning for Low-Resource ASR</head><p>The idea of MAML is to learn initialization parameters from a set of tasks. In the context of ASR, we can view different languages as different tasks. Given a set of source tasks The two functions, Learn and MetaLearn, will be described in the following two subsections.</p><formula xml:id="formula_4">D = {D 1 , D 2 , • • • , D K },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Learn: Language-specific learning</head><p>Given any initial parameters θ 0 of the shared encoder (either random initialized or obtained from pretrained model) and the dataset D t . The language-specific learning process is to minimize the CTC loss function defined in Eq. 3.</p><formula xml:id="formula_5">θ , θ h,t = Learn(D t ; θ 0 ) = arg min θ,θh,t L Dt (θ, θ h,t ) = arg min θ,θh,t (X,C)∈Dt − log P (C|X)<label>(4)</label></formula><p>The learning process is optimized through gradient descent, the same as MultiASR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">MetaLearn</head><p>The initialization parameters found by MAML should not only adapt to one language well, but for as many languages as possible. To achieve this goal, we define the meta learning process and the corresponding meta-objective as follows.</p><p>In each meta learning episode, we sample batch of tasks from D, then sample two subsets from each task k as training </p><formula xml:id="formula_6">θ k , θ h,k = Learn(D tr k ; θ)<label>(5)</label></formula><p>Then evaluate the effectiveness of the obtained parameters on D te k . The goal of MAML is to find θ, the initialization weights of the shared encoder for fast adaptation, so the metaobjective is defined as</p><formula xml:id="formula_7">L meta D (θ) = E k∼D E D tr k ,D te k L D te k (θ k , θ h,k )<label>(6)</label></formula><p>Therefore, the meta learning process is to minimize the loss function defined in Eq. 6.</p><formula xml:id="formula_8">θ = MetaLearn(D) = arg min θ L meta D (θ)<label>(7)</label></formula><p>We use meta gradient obtained from Eq. 6 to update the model through gradient descent.</p><formula xml:id="formula_9">θ ← θ − η k ∇ θ L D te k (θ k , θ h,k )<label>(8)</label></formula><p>η is the meta learning rate. And noted that only the shared encoder is updated via Eq. 8.</p><p>MultiASR optimizes the model according to Eq. 4 on all source languages directly, without considering how learning happens on the unseen language. Although the parameters found by MultiASR is good for all source languages, it may not adapt well on the target language. Unlike MultiASR, MetaASR explicitly integrates the learning process into its framework via simulating language-specific learning first, then meta-updates the model. Therefore, the parameters obtained are more suitable to adapt on the unseen language. We illustrate the concept in Fig. <ref type="figure" target="#fig_0">1</ref>, and show it in the experimental results in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENT</head><p>In this work, we used data from the IARPA BABEL project <ref type="bibr" target="#b19">[20]</ref>. The corpus is mainly composed of conversational telephone speech (CTS). We selected 6 languages as non-target languages for multilingual pretraining: Bengali (Bn), Tagalog (Tl), Zulu (Zu), Turkish (Tr), Lithuanian (Lt), Guarani (Gn), and 4 target languages for adaptation: Vietnamese (Vi), Swahili (Sw), Tamil (Ta), Kurmanji (Ku), and experimented different combinations of non-target languages for pretraining. Each language has Full Language Pack (FLP) and Limited Language Pack (LLP, which consists of 10% of FLP).</p><p>We followed the recipe provided by Espnet <ref type="bibr" target="#b20">[21]</ref> for data preprocessing and final score evaluation. We used 80dimensional Mel-filterbank and 3-dimensional pitch features as acoustic features. The size of the sliding window is 25ms, and the stride is 10ms. We used the shared encoder with a 6-layer VGG extractor with downsampling and a 6-layer bidirectional LSTM network with 360 cells in each direction as used in the previous work <ref type="bibr" target="#b7">[8]</ref>.</p><p>Meta Learning. For each episode, we used a single gradient step of language-specific learning with SGD when computing the meta gradient. Noted that in Eq. 8, if we expanded the loss term in the summation via Eq. 4, we would find the second-order derivative of θ appear. For computation efficiency, some previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> showed that we could ignore the second-order term without affecting the performance too much. Therefore, we approximated Eq. 8 as follows.</p><formula xml:id="formula_10">θ ← θ − η k ∇ θ k L D te k (θ k , θ h,k )<label>(9)</label></formula><p>Also known as First-order MAML (FOMAML). We multi-lingually pretrained the model for 100K steps for both MultiASR and MetaASR. When adapting to one certain language, we used the LLP of the other three languages as validation sets to decide which pretraining step we should pick. Then we fine-tuned the model 18 epochs for the target language on its FLP, 20 epochs on its LLP, and evaluated the performance on the test set via beam search decoding with beam size 20 and 5-gram language model re-scoring, as Table 1 and 2 displayed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>Performance Comparison of CER on FLP. As presented in Table <ref type="table" target="#tab_0">1</ref>, compared to monolingual training (that is, without using pretrained parameters as initialization, denoted as nopretrain), both MultiASR and MetaASR improved the ASR performance using different combinations of pretraining lan-  Learning Curves. The advantage of MetaASR over Multi-ASR is clearly shown in Fig. <ref type="figure" target="#fig_3">3 and 4</ref>. Given the pretrained parameters of the specific pretraining step, we fine-tuned the model for 20 epochs and reported the lowest CER on its validation set. The above process represented one point of the curve. For MultiASR, the performance of adaptation saturated in the early stage and finally degraded. As Fig. <ref type="figure" target="#fig_0">1</ref> illustrates, the training scheme of MultiASR tended to overfit on pretraining languages, and the learned parameters might not be suitable for adaptation. From Fig. <ref type="figure" target="#fig_2">3</ref>, we can see that in the later stage of pretraining, using such pretrained weights even yields worse performance than random initialization. In contrast, for MetaASR, not only the performance is better than MultiASR during the whole pretraining process, but it also gradually improves as pretraining continues without degrading. The adaptation of all languages using different pretraining languages show similar trends. We only showed the results of Swahili here due to space limitations. Impact on Training Set Size. In addition to adapting on FLP of the target languages, we have also fine-tuned on LLP of them, and the result is shown in Table <ref type="table" target="#tab_1">2</ref>. On Vietnamese, Swahili, and Kurmanji, MetaASR also outperforms MultiASR. Both of MultiASR and MetaASR improve the performance, but the gap compared to the no-pretrain model is smaller than fine-tuning on FLP. On Tamil, weights from pretrained model was even worse than random initialization. We will evaluate more combinations of target languages and pretraining languages to investigate the potential of our proposed method in such ultra low-resource scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we proposed a meta learning approach to multilingual pretraining for speech recognition. The initial experimental results showed its potential in multilingual pretraining. In future work, we plan to use more combinations of languages and corpora to evaluate the effectiveness of MetaASR extensively. Besides, based on MAML's modelagnostic property, this approach can be applied to a wide range of network architectures such as sequence-to-sequence model, and even different applications beyond speech recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Illustration: Difference of the learned parameters from MultiASR &amp; MetaASR. The solid lines represent the learning process of pretraining, either multitask or meta learning. The dashed lines represent the language-specific adaptation. (The figure is modified from [10])</figDesc><graphic url="image-1.png" coords="1,317.03,301.82,113.39,94.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>MAML learns from D to obtain good initialization parameters θ for the shared encoder. θ yields fast task-specific learning (fine-tuning) on target task D t and obtains θ t and θ h,t (the parameters obtained after fine-tuning on D t ). MAML can be formulated as below, θ t , θ h,t = Learn(D t ; θ ) = Learn(D t ; MetaLearn(D)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Learning curves on Swahili's LLP pretrained on Bn, Tl, Zu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Learning curves on Swahili's LLP pretrained on Bn, Tl, Zu, Tr, Lt, Gn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Character error rate (% CER) w.r.t the pretraining languages set for all 4 target languages' FLP to simulate the language-specific learning process to obtain θ k and θ h,k .</figDesc><table><row><cell>Model</cell><cell>Vietnamese</cell><cell>Swahili</cell><cell>Tamil</cell><cell>Kurmanji</cell></row><row><cell></cell><cell cols="4">multi meta multi meta multi meta multi meta</cell></row><row><cell>(no-pretrain)</cell><cell>71.8</cell><cell>47.5</cell><cell>69.9</cell><cell>64.3</cell></row><row><cell>Bn Tl Zu</cell><cell cols="4">57.4 49.9 48.1 41.4 65.6 57.5 61.1 57.0</cell></row><row><cell cols="5">Tr Lt Gn 63.7 49.5 57.2 41.8 68.2 57.7 65.6 57.0</cell></row><row><cell>Bn Tl Zu Tr Lt Gn</cell><cell cols="4">59.7 50.1 48.8 42.9 65.6 58.9 62.6 57.6</cell></row><row><cell cols="2">and testing set, denoted as D tr k , D te k , respectively. First, we use D tr k</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Character error rate (% CER) w.r.t the pretraining languages set for all 4 target languages' LLP Table1clearly shows that the proposed MetaASR significantly outperforms MultiASR across all target languages. We were also interested in the impact of the choices of pretraining languages and found that the performance variance of MetaASR is smaller than MultiASR. It might be due to the fact that MetaASR focuses more on the learning process rather than fitting on source languages.</figDesc><table><row><cell>Model</cell><cell>Vietnamese</cell><cell>Swahili</cell><cell>Tamil</cell><cell>Kurmanji</cell></row><row><cell></cell><cell cols="4">multi meta multi meta multi meta multi meta</cell></row><row><cell>(no-pretrain)</cell><cell>74.7</cell><cell>65.0</cell><cell>72.4</cell><cell>68.9</cell></row><row><cell>Bn Tl Zu</cell><cell cols="4">65.0 58.1 62.6 57.5 70.4 73.7 67.6 64.6</cell></row><row><cell cols="5">Tr Lt Gn 64.9 58.0 64.1 59.6 73.7 74.7 69.7 63.0</cell></row><row><cell>Bn Tl Zu Tr Lt Gn</cell><cell cols="4">64.1 58.7 61.9 59.6 70.0 68.2 66.7 64.1</cell></row><row><cell>guages.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Wav2letter: an end-to-end convnetbased speech recognition system</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilingual deep neural network based acoustic modeling for rapid language adaptation</title>
		<author>
			<persName><forename type="first">Ngoc Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Imseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7639" to="7643" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An investigation of deep neural networks for multilingual speech recognition training and adaptation</title>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
				<meeting>of INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>number CONF</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling</title>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Karthick Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Sri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequence-based multi-lingual low resource speech recognition</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4909" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multilingual training and cross-lingual adaptation on ctc-based acoustic model</title>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10025</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Meta-learning for low-resource neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08437</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial multilingual training for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">Jiangyan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="4899" to="4903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Massively multilingual adversarial speech recognition</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02210</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical multi task learning with ctc</title>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<idno>abs/1807.07104</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Meta-learning for low-resource natural language generation in task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05644</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning to adapt: a meta-learning approach for speaker adaptation</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Fainberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10239</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech recognition and keyword spotting for low-resource languages: Babel project research at cued</title>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName><surname>Shakti</surname></persName>
		</author>
		<author>
			<persName><surname>Rath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technologies for Under-Resourced Languages</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Enrique Yalta Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00015</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reptile: a scalable metalearning algorithm</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
