<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What-and-Where to Match: Deep Spatially Multiplicative Integration Networks for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lin</forename><surname>Wu</surname></persName>
							<email>lin.wu@uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<email>wangy@cse.unsw.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of New South Wales</orgName>
								<address>
									<postCode>2052</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
							<email>xueli@itee.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
							<email>junbin.gao@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">Discipline of Business Analytics</orgName>
								<orgName type="institution" key="instit1">The University of Sydney Business School</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What-and-Where to Match: Deep Spatially Multiplicative Integration Networks for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A644A9E4648EA4A7154A8F98C9E5918E</idno>
					<idno type="DOI">10.1016/j.patcog.2017.10.004</idno>
					<note type="submission">Received date: 17 May 2017 Revised date: 20 September 2017 Accepted date: 6 October 2017 Preprint submitted to Elsevier 10•10•2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Multiplicative integration gating</term>
					<term>Convolutional neural networks</term>
					<term>Recurrent neural networks</term>
					<term>Person re-identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Matching pedestrians across disjoint camera views, known as person re-identification (re-id), is a challenging problem that is of importance to visual recognition and surveillance. Most existing methods exploit local regions with spatial manipulation to perform matching in local correspondences. However, they essentially extract fixed representations from pre-divided regions for each image and then perform matching based on these extracted representations. For models in this pipeline, local finer patterns that are crucial to distinguish positive pairs from negative ones cannot be captured, and thus making them underperformed. In this paper, we propose a novel deep multiplicative integration gating function, which answers the question of what-and-where to match for effective person re-id. To address what to match, our deep network emphasizes common local patterns by learning joint representations in a multiplicative way. The network comprises two Convolutional Neural Networks (CNNs) to extract convolutional activations, and generates relevant descriptors for pedestrian matching.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• A novel deep architecture to emphasize common local patterns is proposed to learn flexible joint representations for person re-identification.</p><p>• The proposed method introduces a multiplicative integration gating function to embed two convolutional features to their joint representations, which are effective in discriminating positive pairs from negative pairs.</p><p>• Spatial dependencies are incorporated into feature learning to address the cross-view misalignment.</p><p>• Extensive experiments and empirical analysis are provided in experimental part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-id refers to matching pedestrians observed from disjoint camera views based on visual appearance. It has been attracting great attention due to its significance in visual recognition and surveillance. The major challenge in person re-id lies in the uncontrolled spatial misalignment between images due to severe camera view changes or human-pose variations. Following that, persons may resemble each other, and different identities can only be distinguished by subtle difference in the body parts and small outfit elements (e.g., backpack, handbag). To this end, person re-id has benefited a lot from matching distinctive parts of persons on patch-level matching <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> or local region aggregation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, which can address the spatial misalignment to some extent. However, these methods essentially perform a two-stage mechanism where handcrafted features are first extracted from discovered clusters of patches, and then spatial constraint is enforced to ensure spatial relationship in the matching process (See Fig. <ref type="figure" target="#fig_15">1</ref>). Despite their gain in performance, one salient drawback is the independence of feature extraction and localization for which feature description and spatial manipulation are individually pre-defined.</p><p>Recently, Convolutonal Neural Network (CNN) models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> are proposed for person re-id, and most of the frameworks are designed in a Siamese fashion that learns an embedding where similar pairs (i.e., images belonging to the same identity) are close to each other whilst dissimilar pairs (i.e., in Fig. <ref type="figure">2</ref>. Similar to Siamese CNNs <ref type="bibr" target="#b9">[10]</ref>, we instantiate two identical VGG-Net In contrast, we propose to apply recurrent connections to render learned joint features not only spatially-correlated but also robust against spatial transformations. Our approach does not require any patch-level correspondence annotations between image pairs as it directly integrates mid-level CNN features by joint embeddings. The convolutions, multiplicative gating function, spatially recurrent layer are end-to-end trainable for person re-id by back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The proposed architecture is inspired by the bilinear CNNs <ref type="bibr" target="#b13">[19]</ref> whereas our model embeds spatial recurrence coupled with a fundamentally different way to capture both local and global spatial information, rather than orderless pooling on the location of features alike the bilinear CNNs. We also remark that a standard multiplicative integration network <ref type="bibr" target="#b14">[20]</ref> is not applicable to address the promotion of pairwise correlations between common local patterns from cross-view pedestrian images. This is mainly because fusing two information flows using Hadamard product directly <ref type="bibr" target="#b14">[20]</ref> is not hypothesized to deal with the modality discrepancy in which two input images from disjoint camera views exhibit different visual statistics. This may result in an interference in gradient</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>computation with respect to each input, which is dependent on the other input in Hadamard product of the two inputs. To this end, we propose a novel integration gating function which is designed using two linear mappings for embedding two convolutional activations, followed by Hadamard product to learn joint representations in a multiplicative way, and a linear mapping to project the joint representations into an output vector. This gating function is appealing by providing the subsequent four directional RNNs <ref type="bibr">[15]</ref> with better generalization and easier optimization. Thus, the proposed approach is more advantageous by localizing and learning common features from critical patches of identities, which can discriminate persons and align local regions in displacement.</p><p>Contributions. The contributions of our work are four-fold:</p><p>• We present an end-to-end deep network that is able to stress common local patterns against cross view changes, and thus improving matching confidence in person re-id.</p><p>• We propose a multiplicative integration gating function to embed two stream convolutional features into joint representations, while show that the resulting features processed by spatially recurrent pooling deliver better results than alternative spatial dependency modeling methods including global average pooling, additional convolution and SPP <ref type="bibr">[16]</ref>.</p><p>• The proposed integration gate with Hadamard product allows cross-view feature alignment and facilitate end-to-end training without introducing extra parameters.</p><p>• Our approach is demonstrated to achieve state-of-the-art results on VIPeR, CUHK03 and Market 1501 benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Person Re-identification</head><p>Many person re-identification methods focus on improving feature design against severe visual appearance changes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">21,</ref><ref type="bibr" target="#b16">22]</ref> or seeking proper met-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>rics to measure the cross-view appearance similarity <ref type="bibr" target="#b17">[23,</ref><ref type="bibr" target="#b18">24,</ref><ref type="bibr" target="#b19">25,</ref><ref type="bibr" target="#b20">26,</ref><ref type="bibr" target="#b21">27]</ref>. Since these methods do not effectively address the spatial misalignment among patch matching, their recognition results are still under-performed.</p><p>To combat the spatial misalignment, some patch-level matching methods with attention to spatial layout are proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">28,</ref><ref type="bibr" target="#b4">5]</ref> which segment images into patches and perform patch-level matching with spatial relations.</p><p>Methods in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> separate images into semantic parts (e.g., head, torso and legs), and measure similarities between the corresponding semantic parts. However, these methods assumes the presence of the silhouette of the individual and accuracy of body parser, rendering them not applicable when body segmentations are not reliable. Moreover, they are still suffering mismatching between distant patches. To avoid the dependency on body segments and reduce patchwise mismatching, saliency-based approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> are developed to estimate the saliency distribution relationship and control path-wise matching process. Some metric learning approaches <ref type="bibr" target="#b23">[29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">27,</ref><ref type="bibr" target="#b24">30,</ref><ref type="bibr" target="#b25">31]</ref>  With the resurgence of Convolutional Neural Networks (CNNs) in a variety of tasks such as image classification [14, <ref type="bibr" target="#b26">32,</ref><ref type="bibr" target="#b27">33]</ref> and frequency domain <ref type="bibr" target="#b28">[34]</ref>, a number of end-to-end deep Siamese CNN architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">35,</ref><ref type="bibr" target="#b30">36]</ref>   <ref type="bibr" target="#b9">[10]</ref> has some sharing with us in emphasizing finer local patterns across pairs of images, and thus flexible representations can be produced for the same image according to the images they are paired with. However, their matching gate is to compare the local feature similarities of input pairs from the mid-level, which is unable to mediate the pairwise correlations to seek joint representations effectively. Moreover, S-CNN <ref type="bibr" target="#b9">[10]</ref> manually partitions images into horizontal stripes, and this renders S-CNN unable to address spatial misalignment. The other work close to us is the end-to-end comparative attention network (CAN) <ref type="bibr" target="#b12">[13]</ref> that learns to selectively focus on parts of paired person images, and adaptively compare their appearance. However, CAN needs to generate multiple glimpses from the same image to localize different parts in which spatial relationship is not explicitly modeled. Moreover, CAN is using a standard visual attention model which is more likely to generate similar attention maps at different time steps <ref type="bibr" target="#b31">[37]</ref>, and thus smaller regions cannot be discovered to differentiate visually similar persons. In contrast to aforementioned methods, our model introduces multiplicative integration gating mechanism to learn joint representations attentively from common local finner patterns while subject to spatial recurrence to effectively address the spatial misalignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Two-stream Models</head><p>"Two-stream" architectures have been used to analyze videos where one network models the temporal aspect, while the other network models the spatial dimensions <ref type="bibr" target="#b32">[38]</ref>. Bilinear models <ref type="bibr" target="#b33">[39]</ref> can model two-factor variations and provide richer representations than linear models. To exploit this advantage, fully-connected layers in neural networks can be replaced with bilinear pooling which yields the outer product of two vectors. It allows all pairwise interactions among given features. Recently, the model of Bilinear CNNs <ref type="bibr" target="#b13">[19]</ref> is an application of this technique in fine-grained visual recognition that generalizes orderless texture descriptors such as VLAD [18], and Fisher vector. However,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. To combine information flows from two different sources, multiplicative integration can be viewed as a general way by using Hadamard product on two input sources <ref type="bibr" target="#b14">[20]</ref>. Our model is inspired by multiplicative integration while we introduce a joint embedding into the integration gating function which is capable of learning locally common patterns against cross-view changes. Meanwhile, spatial dependencies are preserved into the feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Spatially Multiplicative Integration Networks</head><p>In this section, we formulate the deep spatially multiplicative integration networks to learn locally joint representations for person re-identification. Specifically, it can be formulated as a quadruple M = ([g A , g B ], B, P, L), where g A and g B are two non-linear encoders in regards to a pair of images, B is the multiplicative integration block, P is spatial pooling function, and L is the loss function. The overall framework is illustrated in Fig. <ref type="figure">2</ref>, where given the input in pairs, our model starts from two-stream convolutions (section 3.1) to localize regions and extract features, which are integrated by the Hadamard product in the multiplicative integration block at each convolution position (section 3.2).</p><p>The resulting features are fed into spatial recurrent pooling to propagate information through lateral connections and equip features with spatial dependencies (section 3.3). For the similarity function, we employ the cosine similarity function and binomial deviance loss function <ref type="bibr" target="#b34">[40]</ref> (section 3.4). The whole network is end-to-end trainable and learned features are boosted to localize common distinct regions for person re-id (see Fig. <ref type="figure" target="#fig_4">3</ref>). In what follows, we will present each component of the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Two-stream Convolutional Neural Networks</head><p>We consider CNNs to extract features from inputs in pairs. Specifically, g A and g B are instantiated with the VGG-Net [14]. We pre-train the VGG-Net on the ImageNet dataset <ref type="bibr" target="#b26">[32]</ref>, and truncate at the convolutional layer including non-linearities as the feature functions. The advantage is that the resulting CNNs can process images of an arbitrary size in a single feed-forward propagation and generate outputs indexed by the location in the image and feature channels. The VGG-Net is characterized by the increased depth with very small convolution filters, which are effective on classification and localization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>For notational simplicity, we refer to the last convolutional layer of CNNs i.e., the 5 th convolutional layer, conv 5 = g A (I), conv 5 = g B ( Ī) for the the image pair [I, Ī] as input and the activations of the last convolution as the output. This is mainly because CNNs extract low-level features at the bottom layers and learn more abstract concepts such as the parts or more complicated texture patterns at the mid-level (i.e., conv 5 ). These mid-level features are more informative than higher-level features <ref type="bibr" target="#b9">[10]</ref> and contain finer details that are crucial to increase the similarity for positive pairs. Hence, we propose a multiplicative integration gating function to attend extracted local patterns and learn flexible joint representations for an image pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiplicative Integration Gating</head><p>To integrate two CNN information flows g A and g B , we propose a fusion design in a form of multiplicative integration. Given the activations from the previous convolutional block g A ∈ R K×K×D and g B ∈ R K×K×D , where K × K denote the shape of the last convolution and D is feature depth, we propose to</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>use the Hadamard product to fuse g A and g B :</p><formula xml:id="formula_2">F (i, j, :) = P T (U T g A (i, j, :) + bA) (V T g B (i, j, :) + bB) + b,<label>(1)</label></formula><p>where 0 ≤ i, j ≤ K, F ∈ R K×K×D denote the integrated features, and each vector of F (i, j, :) is determined by the two linear mappings Applying non-linear activation functions may help to increase representative capacity of the model. We apply non-linear activation right after linear mappings for input vectors: be the gradient of loss function L w.r.t F , then by the chain rule of gradients we have:</p><formula xml:id="formula_3">U ∈ R D×d , V ∈ R D×d</formula><formula xml:id="formula_4">F (i, j, :) = P T σ(U T g A (i, j, :)) σ(V T g B (i, j, :)) + b,<label>(2)</label></formula><formula xml:id="formula_5">∂L ∂g A = U diag(V T g B ) ∂L ∂F T , ∂L ∂g B = V diag(U T g B ) ∂L ∂F T . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>By looking at the gradients, we see that the matrix V and the paired input g B are directly involved in the gradient computation by gating the matrix U , hence more capable of altering the updates of the learning nets. Fig. <ref type="figure" target="#fig_6">4</ref> illustrates the gradient scheme. While the outputs from two CNN streams are combined at each spatial location in a multiplicative way, the spatial context is not preserved.</p><p>To this end, a spatially recurrent pooling P over all locations is subsequently performed. The resulting features are flattened and fed into a loss function L to determine the matching measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spatially Recurrent Pooling with IRNNs</head><p>Our architecture for incorporating spatial dependency into feature learning is shown in Fig. <ref type="figure" target="#fig_7">5</ref>. This action explicitly allows the spatial manipulation of feature maps within the network, and can effectively address the spatial alignment issue in matching persons in cross-view setting. We employ four RNNs that sweep over the entire feature map in four different directions <ref type="bibr" target="#b35">[41]</ref>: bottom to top, top to bottom, left to right, and right to left. The recurrent layers ensure that each feature activation in its output is an activation at a spatially specific location with respect to the whole image.</p><p>On top of the integrated features F , we place RNNs that move laterally across the activations, and produce an output F the same size as F . Thus, the temporal dependency learning in RNNs is converted to spatial domain. The recurrent neural networks can be implemented using several forms: LSTM <ref type="bibr" target="#b36">[42]</ref>,</p><p>259 gated recurrent units (GRU) <ref type="bibr" target="#b37">[43]</ref>, and plain tanh recurrent neural networks. IRNN layer corresponds to apply recurrent matrix and the nonlinearity at each step, from which the output is computed by concatenating the hidden states from the four directional IRNNs at each spatial location. Each IRNN takes the output of 1 × 1 convolution, and updates its hidden states via:</p><formula xml:id="formula_7">F dir i,j ← max W dir hh F dir i,j-1 + F dir i,j , 0 ,<label>(4)</label></formula><p>where dir indicates one of the four directions that moves to dir ∈ {lef t, right, up, down}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>268</head><p>In Eq.( <ref type="formula" target="#formula_7">4</ref>), the input-to-hidden transition is not presented because it has been 269 computed as part of the 1 × 1 convolution, and then copied in-place to each 270 hidden recurrent layer. In our implementation, the number of hidden units is Compared with alternatives, the two stacked 4-dir IRNN layers have fewer parameters and can propagate information through lateral connections that span across the whole image. Our empirical studies on test set in person reid show that stacked 4-dir IRRNs are able to achieve better performance than alternatives (in Section 4 Table <ref type="table" target="#tab_0">1</ref>). In fact, after the first 4-dir IRRN, a feature map is produced that summarizes nearby objects at every position in the image, that is, the first IRRN can create a summary of the features to the left/right/top/down of every cell, as illustrated in Fig. <ref type="figure" target="#fig_10">7</ref>. The subsequent 1 × 1 convolution mix these priors as a dimension reduction. The second 4-dir IRRN can ensure every cell on the output depends on every cell of the input, and producing contextual features both global and local. In this way, the features vary locally by spatial position, while each cell is a global summary of the image with respect to that specific spatial location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>The last layer in the network uses a similarity function to measure the whether two images (i, j) belong to the same person or not given the output features learned by the deep model. Once the spatially integrated features F are trained, we calculate the Euclidean distance between g A , F and g B , F , re-</p><formula xml:id="formula_8">spectively, that is, ||g A -F || 2 , ||g B -F || 2 .</formula><p>Then, fully-connect layer is added to produce their final representations which can be fed into a loss function.</p><p>Specifically, we use the cosine similarity function and the binomial deviance loss function for training:</p><formula xml:id="formula_9">L = i,j W ln(exp -α(S-β) M +1)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T W indicates a weight matrix that is defined as</p><formula xml:id="formula_10">W i,j =    1 n1 , positive pair 1 n2 , negative pair</formula><p>where n 1 and n 2 are the count of positive and negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>299</head><p>The network can be trained by back-propagation gradients of the loss func-300 tion. The integrated form simplifies the gradients at the gating layer, and the 301 recurrent layer is smooth, and continuous function. The gradients of the loss 302 function ( <ref type="formula" target="#formula_9">5</ref>) is straightforward <ref type="bibr" target="#b34">[40]</ref>, and the gradients of the layers below the 303 multiplicative integration layer can be computed using the chain rule. We perform experiments on three benchmarks for person re-id: VIPeR <ref type="bibr" target="#b40">[46]</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>307</head><p>CUHK03 <ref type="bibr" target="#b7">[8]</ref>, and Market-1501 <ref type="bibr" target="#b41">[47]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementations</head><p>It is known that augmenting training data often leads to better generalization <ref type="bibr" target="#b26">[32,</ref><ref type="bibr" target="#b43">49,</ref><ref type="bibr" target="#b44">50]</ref>. We carry out two primary data augmentation in experiments:   chosen images has very small number of positive pairs, it will not impair the 340 learning process since the binomial deviance loss is weighted in align with the 341 number of positive/negative pairs. We deploy the dropout on recurrent layers 342 with probability of 0.5, while each optimization run is early stopped based on  • (2) Fisher vectors with CNN features (FV-CNN): We construct a descriptor using FV pooling of CNN filter bank responses with 64 GMM components <ref type="bibr" target="#b45">[51]</ref>. FV is computed on outputs of the last convolution layer of CNN.</p><formula xml:id="formula_11">Rank @ R R = 1 R = 10 R = 20 R = 1 R = 10 R = 20 R = 1 R = 10 R = 20</formula><p>• (3) Fisher vectors with SIFT features (FV-SIFT): It uses dense SIFT features <ref type="bibr" target="#b1">[2]</ref> over a set of 14 dense overlapping 32 × 32 pixels regions with a step stride of 16 pixels in both direction. The features are PCA projected before learning a GMM with 256 components.</p><p>• (4) Bilinear CNN <ref type="bibr" target="#b13">[19]</ref> with spatial pyramid pooling [16] (B-CNN+SPP):</p><p>To have fair comparison, we perform a 2-level pyramid [16]: 2 × 2 and 1 × 1 subdivisions over the resulting bilinear features.</p><p>• (5) Bilinear CNN <ref type="bibr" target="#b13">[19]</ref> with stacked four directional RNNs [15] (B-CNN+IRNNs):</p><p>It uses four RNNs that move four directions upon the bilinear pooling features to preserve the spatial manipulation.</p><p>• (6) Multiplication Integration networks <ref type="bibr" target="#b14">[20]</ref> with spatial pyramid pooling</p><p>[16] (MI+SPP): Two identical VGG-Net are used to extract features from images, and the last convolutions are integrated by the Hadamard product, followed by the SPP to impose spatial relationships.</p><p>• (7) Our approach: We consider three variants varied on the initialization of the two-stream CNNs: (i) initialized by two VGG M-Nets <ref type="bibr" target="#b46">[52]</ref> (     Compared to the methods based on path-matching with additional spatial constraint, such as SDALF <ref type="bibr" target="#b4">[5]</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to Baselines</head><p>eSDC <ref type="bibr" target="#b3">[4]</ref>, SalMatch <ref type="bibr" target="#b0">[1]</ref>, and NLML <ref type="bibr" target="#b6">[7]</ref>, our approach outperforms consistently by performing localization and spatial manipulation jointly. Compared to recent CNN models including JointRe-id <ref type="bibr" target="#b8">[9]</ref>, Multi-channel <ref type="bibr" target="#b11">[12]</ref>, SI-CI <ref type="bibr" target="#b50">[56]</ref>, S-CNN <ref type="bibr" target="#b9">[10]</ref>, and S-LSTM <ref type="bibr" target="#b51">[57]</ref>,</p><p>our method has performance gain by      more effective than lower body (S1). In future, we will study different matching properties regarding to body parts.</p><p>The Effect of Spatial Dependencies. In this experiment, we investigate the effects of spatial manipulations in matching pedestrian images. Fig. <ref type="figure" target="#fig_24">9</ref> (b) shows the high responses from spatially recurrent features, and the evaluation of its matching rate is shown in Fig. <ref type="figure" target="#fig_24">9</ref>, where spatial recurrent achieves rank-1 rate 59.83%, a large margin over individual region matching and linear integrated local similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel deep spatially recurrent model for person  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 : 39 AFigure 2 :</head><label>1392</label><figDesc>Figure 1: Left: Two-stage similarity measurement divides images into regions (r=0 indicates the whole image), and extracts features to compute visual differences in spatial correspondences (S r (x r a , x r b )). Right: Our approach learns flexible representations from local common regions and perform spatial manipulation in an end-to-end manner. images belonging to different identities) are separated by a distance. The strik-22</figDesc><graphic coords="5,118.02,151.33,353.16,137.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>40 [ 53 1</head><label>4053</label><figDesc>14] with shared parameters and pre-trained on ImageNet, whose outputs from 41 the last convolution are integrated in a multiplicative way using the Hadamard 42 product (element-wise multiplication) at each location of their convolution acti-43 vations. The effect of multiplication naturally results in a gating type structure 44 in which two CNN stream features are the gates of each other by reconciling 45 their pairwise correlations to enhance common local patterns. However, the in-46 puts come from different modalities (disjoint camera views) with different visual 47 statistics 1 , and thus making it difficult to learn feature transformations for a 48 pair of subregions. To this end, we propose to embed two inputs using two 49 linear mappings, followed by Hadamard product to learn joint representations 50 by multiplications. This will promote the common local subregions along the 51 higher layers so that the network propagates more relevant features through 52 higher layers of the deep nets. Since the gradients with respect to each input In practice, the complex configurations are the combinations of view points, poses, lightings and photometric settings, and thus pedestrian images are multi-modal. A C C E P T E D M A N U S C R I P T is directly dependent on the other input in their Hadamard product, the gating mechanism alters the gradient properties by boosting the joint embedding and the back propagated gradients corresponding to the promoted local similarities. This can encourage the lower and middle layers to learn filters to extract locally similar patterns that can effectively discriminate positive pairs from negatives. To incorporate spatial relationship into feature learning, stacked four directional recurrent neural networks (RNNs) [15] are employed to convert the temporal dependency learning into spatial domains. One may easily add a spatial pyramid pooling (SPP) layer [16] on top of convolution layers to aggregate local features by partitioning images from finer to coarser levels. Unfortunately, SPP still exploits local inputs due to the local receptive fields rather than the contextual information of the whole image. An alternative way is multi-scale orderless pooling [17], which extracts CNN activations for local patches at scale levels and performs orderless VLAD pooling [18]. However, it cannot achieve the global contextual coherence and spatial consistency over critical patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>make attempts to extract low-level features from local regions and perform local matching within each subregions. They aim to learn local similarities and global similarity, which can be leveraged into an unified framework. Despite their effectiveness in local similarity measurement with some spatial constraints, they have limitations in the scenarios where corresponding local regions are roughly associated. In essence, the above methods are developed on a two-stage scheme in which feature extraction and spatial arrangement are performed separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>are proposed for person re-id with the objective of projecting the images of similar pairs to be closer to each other while those of dissimilar pairs to be distant from each other. However, current networks extract fixed representations for each image without consideration on transformed local patterns which are crucial to discriminate positive pairs from negatives. In contrast, we present a model to learn flexible representations from detected common local patterns which are A C C E P T E D M A N U S C R I P T robust against cross-view transformations. It enables automatic interactions between common part localization, feature extraction, and similarity estimation. S-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Spatial attention effect to extract relevant features from local regions to perform matching across cameras.</figDesc><graphic coords="12,184.35,151.19,201.64,110.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>for embedding two input vectors g A (i, j, :) and g B (i, j, :), by the Hadamard product (element-wise multiplication). d is a hyper-parameter to decide the dimension of joint embedding. P ∈ R d×D denotes the linear mapping with a bias b ∈ R D to project the joint representations into an output vector. b A ∈ R d and b B ∈ R d are bias vectors for their respective linear projections U and V .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Computing gradients in the multiplicative integration gating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The architecture of four-directional IRNN [15]. The spatial RNNs are instantiated using "IRNN" units [15] that are composed of RNNs with ReLU recurrent transitions, and initialized to the identity matrix. All transitions from/to the hidden states are computed with 1 × 1 convolutions, which enables more efficient computation in recurrence. The final spatially recurrent pooled features are outputs of two-layer IRNNs, where the spatial resolutions remain the same as bilinear features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>260Figure 6 :</head><label>6</label><figDesc>Figure 6: Incorporating context with different respective fields. (a) 2× stacked convolutions on top of bilinear features allow a cell in input to affect a 5 × 5 window in output. (b) In global average pooling, each cell in the output depends on the entire input with the same value repeated. (c) In spatial pyramid pooling, each cell in the input is put through by a multi-level pooling. (d) In 4-dir IRNN, each cell in the output depends on the entire input with spatially varied value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>272Fig. 6 (</head><label>6</label><figDesc>Fig. 6 (b), global pooling provides information about the entire image and one 277</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The output of the first IRNN layer. Each cell in the output summarizes the features with respect to the left/right/top/down.where is element-wise multiplication, i and j are the number of training images, and S = [S i,j ] n×n is the similarity matrix for image pairs (n is total number of training images. S i,j = cosine(x i , x j )). α and β are hyper parameters. The matrix M encodes the training supervision that is defined as</figDesc><graphic coords="18,221.32,151.36,127.69,82.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>308•</head><label></label><figDesc>The VIPeR dataset contains 632 individuals taken from two cameras with 309 arbitrary viewpoints and varying illumination conditions. The 632 per-310 A C C E P T E D M A N U S C R I P T son's images are randomly divided into two equal halves, one for training and the other for testing. • The CUHK03 dataset includes 13,164 images of 1360 pedestrians. It is captured with six surveillance cameras. Each identity is observed by two disjoint camera views, yielding an average 4.8 images in each view. We perform experiments on manually labeled dataset with pedestrian bounding boxes. The dataset is randomly partitioned into training, validation, and test with 1160, 100, and 100 identities, respectively.• The Market-1501 data set contains 32,643 fully annotated boxes of 1501 pedestrians, making it the largest person re-id dataset to date. Each identity is captured by at most six cameras and boxes of person are obtained by running a state-of-the-art detector, the Deformable Part Model (DPM)<ref type="bibr" target="#b42">[48]</ref>. The dataset is randomly divided into training and testing sets, containing 750 and 751 identities, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>flipping and shifting. For flipping, we flipped each sample horizontally, which allows the model observe mirrored images of the original during training. For shifting, we shift the image by 5 pixels to the left, 5 pixels to the right and after this processing, we further shift the image by 10 pixels to the top, 10 pixels to the bottom. This two-step shifting procedure makes the model more robust to slight shifting of a person. The shifting was done with padding the borders of images. Training pairs in each batch are formed by randomly selecting 128 images from all cameras. The label for each training pair is assigned accordingly to the identity number (+1 for the same identity, -1 for different identities). The training batch is shuffled in each epoch to ensure the network can see divergent image pairs during training. Parameters in the cost function are set as α = 2, β = 0.5. Even though each mini-batch that contains randomly A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>343</head><label></label><figDesc>validation error. We employ the VGG-Net model [14] to extract CNN features, 344 and the outputs of the last convolution with non-linearities are used as features 345 with 512-dim features at each location. In our experiments, we adopt the widely 346 used single-shot modality with Cumulative Matching Characteristic (CMC) as 347 metric. This evaluation is performed ten times, and average CMC results are 348 reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>350• ( 1 )</head><label>1</label><figDesc>The proposed method contains two novel ingredients: (1) the multiplicative 351 integration computation that is able to attend common local regions helpful in 352 matching, and (2) the spatially recurrence that serves to embed spatial depen-353 dencies into feature learning. To reveal how each ingredient contributes to the 354 performance improvement, we consider the following four deep baselines: 355 CNN with fully-connected layers (FC-CNN): The input image is resized 356 to 224 × 224 and mean-subtracted before propagating it through CNN. 357 For fine-tuning, we replace the 1000-way classification layer with a k-way 358 softmax layer where k is the number of identity classes in each person re-359 id dataset. The parameters of the softmax layer are initialized randomly 360 A C C E P T E D M A N U S C R I P T and the training is stopped by monitoring the validation error. The layer before softmax layer is used to extract features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Ours [VGG-M,VGG-M]); (ii) initialized by a VGG D-Net [14] and an M-Net (Ours [VGG-D, VGG-M]); (iii) initialized by two VGG D-Nets (Ours [VGG-D, VGG-D]). The M-Net is characterized by the decreased stride and smaller receptive field of the first convolutional layer. The D-Net increased the depth with very small convolution filters, which is effective on classification and localization tasks. For both M-Net and D-Net, The input paired images are resized to 224×224 and feature are truncated at the A C C E P T E D M A N U S C R I P T last convolutional layer including non-linearities as the feature functions. Two stream features are then put through integration gating function and spatially recurrent pooling. The recurrent layer has 512 hidden states for each location's encoding. Thus, the final spatially recurrent feature is of size 512 × 196, which is comparable to FV-CNN (512 × 128) and FV-SIFT (512 × 80). For fine-tuning, we first initialize the convolution layers using fine-tuned FC-CNN and then the entire model is fine-tuned with loss function using back-propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>To examine the component effectiveness contributed by multiplicative integration and IRNNs, we calculate the CMC values from two baselines: B-CNN+IRNNs and MI+SPP. The results show that B-CNN + IRRNs outperforms B-CNN + SPP in all rankings due to the effectiveness of recurrence which renders temporal dependency into spatial domains. On the other hand, MI+SPP has noticeable performance gain over B-CNN+SPP and B-CNN+IRNNs. This suggests the importance of MI to person re-id for its effectiveness in seeking finer common local patterns across views. In addition, comparative results from three variants of our model show that two identical CNN streams based on VGG-D Net are superior to alternatives using VGG-M Nets and/or VGG-D Net. The main reason is two identical CNNs with shared parameters are suitable to person re-id to extract common patterns. Thus, our method (VGG-D, VGG-D) is used as the default in all experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>4. 5 .</head><label>5</label><figDesc>Comparison to State-of-the-art Approaches</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance comparison with state-of-the-art approaches using CMC curves on the VIPeR and CUHK03 data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>4. 6 .</head><label>6</label><figDesc>More Empirical Analysis on Our ApproachUnderstanding the Integrated Features. The proposed model is motivated to seek common local subregions by joint deep embeddings. In this experiment, we analyze the network's specification into roles of localization and flexible appearance modeling in person re-id scenario when the network is initialized symmetrically and fine-tuned in a Siamese fashion. Fig.9(a) shows that the network tends to activate on highly semantic and common regions, such as back bag, and body parts. To evaluate the matching ability of these detected regions, we compute the local similarities corresponding to pairs of attentive regions. Suppose a pair of images has R (we set R = 2) detected regions, we extract their features, denoted as x a and x b , respectively. Then, for each matching region r, we learn a similarity value: S r (x a , x b ) = φ r (x a , x b ), W r F , r = {1, . . . , R}, whereφ(x a , x b ) = (x a -x b )(x a -x b ) T , and •, • F is the Frobenius inner product. Thus, S r (x ax b ) = (x ax b )W r (x ax b ) T corresponds to the Mahalanobis distance.As shown in<ref type="bibr" target="#b23">[29]</ref>, local similarities are complementary and can be combined into an integrated measurement:S Local = R r=1 S r (x a , x b ). InFig.9, S1, S2 and Local show the matching rate of using detected regions independently, and an integrated similarity. We can see that (1) the integrated local similarity outperforms individual regions using deep features, and (2) upper body parts (S2) are A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>re-identification that learns common local features with spatial manipulation to facilitate patch-level matching. The proposed scheme introduces an integration gating function to jointly embed pair-wised input images to discover the common patterns that are helpful in discriminating the positive pairs from negative ones for person re-id. To incorporate spatial dependencies into feature learning, stacked spatially recurrent pooling are embodied to make the learned A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Left (a): Common distinct regions detected by integration gating computations. Left (b): High responses of spatially recurrent features. Raw RGB images in columns display the same identities observed by disjoint camera views. Right: CMC curves for comparison on individual/integrated local similarity and spatially recurrent measurement. representations spatially contextual. Comprehensive experiments show that our designed network achieves the superior performance on person re-identification.For the future work, we will continue to improve the models of part localization and matching with attention model.</figDesc><graphic coords="27,106.39,172.16,196.21,140.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CMC result at rank-R (R=1,10,20) recognition rate.</figDesc><table><row><cell>Dataset</cell><cell>VIPeR</cell><cell>CUHK03</cell><cell>Market-1501</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Rank-1, -10, -20 recognition rate of various methods on the VIPeR data set (test person =316).</figDesc><table><row><cell>Method</cell><cell>R = 1</cell><cell>R = 10</cell><cell>R = 20</cell></row><row><cell>JointRe-id [9]</cell><cell>34.80</cell><cell>74.79</cell><cell>82.45</cell></row><row><cell>LADF [27]</cell><cell>29.34</cell><cell>75.98</cell><cell>88.10</cell></row><row><cell>SDALF [5]</cell><cell>19.87</cell><cell>49.37</cell><cell>65.73</cell></row><row><cell>eSDC [4]</cell><cell>26.31</cell><cell>58.86</cell><cell>72.77</cell></row><row><cell>kLFDA [23]</cell><cell>32.33</cell><cell>79.72</cell><cell>90.95</cell></row><row><cell>SalMatch [1]</cell><cell>30.16</cell><cell>62.50</cell><cell>75.60</cell></row><row><cell>MLF [2]</cell><cell>29.11</cell><cell>65.20</cell><cell>79.90</cell></row><row><cell>SCSP [29]</cell><cell>53.54</cell><cell>91.49</cell><cell>96.65</cell></row><row><cell>Multi-channel [12]</cell><cell>47.80</cell><cell>84.80</cell><cell>91.10</cell></row><row><cell>NLML [7]</cell><cell>42.30</cell><cell>85.23</cell><cell>94.25</cell></row><row><cell>NullReid [53]</cell><cell>42.28</cell><cell>82.94</cell><cell>92.06</cell></row><row><cell>DGDropout [54]</cell><cell>38.40</cell><cell>86.60</cell><cell>90.89</cell></row><row><cell>E-Metric [55]</cell><cell>40.90</cell><cell>85.05</cell><cell>92.00</cell></row><row><cell>SI-CI [56]</cell><cell>35.80</cell><cell>83.50</cell><cell>97.07</cell></row><row><cell>S-LSTM [57]</cell><cell>42.40</cell><cell>79.40</cell><cell>-</cell></row><row><cell>S-CNN [10]</cell><cell>37.80</cell><cell>77.40</cell><cell>-</cell></row><row><cell>SSM [58]+GOG [59]</cell><cell>50.73</cell><cell>89.97</cell><cell>95.63</cell></row><row><cell>CAN [13]</cell><cell>47.20</cell><cell>89.20</cell><cell>95.80</cell></row><row><cell>PIE [60]</cell><cell>18.10</cell><cell>38.92</cell><cell>49.40</cell></row><row><cell>Ours</cell><cell>49.11</cell><cell>87.66</cell><cell>93.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Rank</figDesc><table><row><cell cols="4">-1, -10, -20 recognition rate</cell></row><row><cell cols="4">of various methods on the CUHK03 data</cell></row><row><cell cols="2">set (test person =100).</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>R = 1</cell><cell>R = 10</cell><cell>R = 20</cell></row><row><cell>JointRe-id [9]</cell><cell>54.74</cell><cell>91.50</cell><cell>97.31</cell></row><row><cell>FPNN [8]</cell><cell>20.65</cell><cell>51.32</cell><cell>83.06</cell></row><row><cell>NullReid [53]</cell><cell>58.90</cell><cell>92.45</cell><cell>96.30</cell></row><row><cell>SDALF [5]</cell><cell>5.60</cell><cell>36.09</cell><cell>51.96</cell></row><row><cell>eSDC [4]</cell><cell>8.76</cell><cell>38.28</cell><cell>53.44</cell></row><row><cell>kLFDA [23]</cell><cell>48.20</cell><cell>66.38</cell><cell>76.59</cell></row><row><cell>LOMO+XQDA [30]</cell><cell>52.20</cell><cell>92.14</cell><cell>96.25</cell></row><row><cell>PersonNet [35]</cell><cell>64.80</cell><cell>94.92</cell><cell>98.20</cell></row><row><cell>DGDropout [54]</cell><cell>72.60</cell><cell>93.50</cell><cell>96.70</cell></row><row><cell>E-Metric [55]</cell><cell>61.32</cell><cell>96.50</cell><cell>97.50</cell></row><row><cell>S-LSTM [57]</cell><cell>57.30</cell><cell>88.30</cell><cell>91.00</cell></row><row><cell>S-CNN [10]</cell><cell>61.80</cell><cell>89.30</cell><cell>92.20</cell></row><row><cell>PIE [60]</cell><cell>62.40</cell><cell>91.85</cell><cell>95.85</cell></row><row><cell>SSM [58]+GOG [59]</cell><cell>71.82</cell><cell>92.54</cell><cell>96.64</cell></row><row><cell>Ours</cell><cell>73.23</cell><cell>96.73</cell><cell>97.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Rank-1 and mAP of various methods on the Market-1501 data set (test person =751).</figDesc><table><row><cell>Method</cell><cell>R = 1</cell><cell>mAP</cell></row><row><cell>SDALF [5]</cell><cell>20.53</cell><cell>8.20</cell></row><row><cell>eSDC [4]</cell><cell>33.54</cell><cell>13.54</cell></row><row><cell>kLFDA [23]</cell><cell>44.37</cell><cell>23.14</cell></row><row><cell>XQDA [30]</cell><cell>43.79</cell><cell>22.22</cell></row><row><cell>BoW [47]</cell><cell>34.40</cell><cell>14.09</cell></row><row><cell>SCSP [29]</cell><cell>51.90</cell><cell>26.35</cell></row><row><cell>NullReid [53]</cell><cell>61.02</cell><cell>35.68</cell></row><row><cell>S-CNN [10]</cell><cell>65.88</cell><cell>39.55</cell></row><row><cell>SSM [58]</cell><cell>82.21</cell><cell>68.80</cell></row><row><cell>CAN [13]</cell><cell>60.30</cell><cell>35.90</cell></row><row><cell>PIE [60]</cell><cell>65.68</cell><cell>41.12</cell></row><row><cell>PIE [60] + KISSME [61]</cell><cell>79.33</cell><cell>55.95</cell></row><row><cell>Ours</cell><cell>67.15</cell><cell>40.24</cell></row><row><cell>Ours + KISSME [61]</cell><cell>83.40</cell><cell>60.37</cell></row><row><cell>Ours + XQDA [30]</cell><cell>86.15</cell><cell>65.25</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In initialization, gradients are propagated back-towards with full strength.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Junbin Gao's research was partially by Australian Research Council Discovery Projects funding scheme (Project No. DP140102270) and the University of Sydney Business School ARC Bridging Fund.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>introducing integration gating function to produce flexible representations by attending common patterns and also spatial recurrent layer to effectively address the spatial misalignment. As CAN <ref type="bibr" target="#b12">[13]</ref> is similar to our method in the sense that discriminative local regions are discovered adaptively to pairs of person images, we compare our approach against CAN <ref type="bibr" target="#b12">[13]</ref> in the VIPeR and Market-1501 datasets. Experimental results from Table <ref type="table">2</ref> and Table <ref type="table">4</ref> show that our method outperforms CAN <ref type="bibr" target="#b12">[13]</ref> at rank-1 value on two datasets by achieving 49.11% against 47.20% and 67.15% against 60.30%, respectively. On VIPeR dataset, our method is slightly inferior to SCSP <ref type="bibr" target="#b23">[29]</ref> in which SCSP achieves 53.54% at rank-1 accuracy while our method attains 49.11%. This is because VIPeR is very small and does not have sufficient training samples for each identity to predict common local patterns faithfully through the deep nets.</p><p>Comparison results with respect to more recent state-of-the-art PIE <ref type="bibr" target="#b54">[60]</ref> and Supervised Smoothed Manifold (SSM) <ref type="bibr" target="#b52">[58]</ref> suggest a number of observations. First, our method improves PIE <ref type="bibr" target="#b54">[60]</ref> by rank-1 accuracy value 31.01, 10.83, and 1.47 for VIPeR, CUHK03, and Market-1501, respectively. The PIE <ref type="bibr" target="#b54">[60]</ref> is a robust pedestrian descriptor which addresses the misalignment by introducing pose invariant embedding. However, they extract fixed representations from body parts of each pedestrian bounding box and fuse them into a pose invariant figure. This manual extraction is unable to learn flexible representations that account for common local patterns in the paired images. Second, SSM <ref type="bibr" target="#b52">[58]</ref>+GOG <ref type="bibr" target="#b53">[59]</ref> slightly outperforms our method (49.11%) by achieving 50.73% at rank-1 accuracy of the VIPeR dataset. The main reason is SSM <ref type="bibr" target="#b52">[58]</ref> is able to estimate the similarity value between two instances with reference to other pairs of instances by inspecting the manifold structure. Thus, SSM is more suitable for a small dataset of VIPeR. However, our method has the performance improvement over SSM <ref type="bibr" target="#b52">[58]</ref>+GOG <ref type="bibr" target="#b53">[59]</ref> on CUHK03 dataset by 1.41. In Market-1501, our method combined with XQDA <ref type="bibr" target="#b24">[30]</ref> outperforms SSM <ref type="bibr" target="#b52">[58]</ref> by a notable margin of 3.94 at rank-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>[14] K. Simonyan, A. Zisserman, Very deep convolutional networks for largescale image recognition, in: arXiv preprint arXiv:1409.1556, 2014.</p><p>[15] Q. V. Le, N. Jaitly, G. E. Hinton, A simple way to initialize recurrent neural networks for rectified linear units, in: arXiv:1504.00941, 2015.</p><p>[16] K. He, X. Zhang, S. Ren, J. Sun, Spatial pyramid pooling in deep convolutional networks for visual recognition, in: ECCV, 2014, pp. 346-361.</p><p>[17] Y. Gong, L. Wang, R. Guo, S. Lazebnik, Multi-scale orderless pooling of deep convolutional activation features, in: ECCV, 2014, pp. 392-407.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Person re-identification by salience matching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning mid-level filters for person reidentifiation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised learning of generative topic saliency for person re-identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>BMVC</publisher>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Human re-identification by matching compositional template with cluster sampling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Nonlinear local metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05169</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep linear discriminant analysis on fisher networks: A hybrid architecture for person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="238" to="250" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="1449" to="1457" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">On multiplicative integration with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>NIPS</publisher>
			<biblScope unit="page" from="2856" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Person re-identification by camera correlation aware feature augmentation</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attributes driven tracklet-to-tracklet person re-identification using latent prototypes space mapping</title>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Person re-identification using kernel-based metric learning methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep adaptive feature embedding with local sample distributions for person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="275" to="288" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ACCV</publisher>
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3318" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning locallyadaptive decision functions for person verification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3610" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Person re-identification with correspondence structure learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="3200" to="3208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1268" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple metric learning based on bar-shape descriptor for person re-identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="218" to="234" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NIPS</publisher>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cnnpack: Packing convolutional neural networks in the frequency domain</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>NIPS</publisher>
			<biblScope unit="page" from="253" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Personnet: Person re-identification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
		<idno>CoRR abs/1601.07255</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Structured deep hashing with convolutional neural networks for fast person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04179</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep metric learning for person reidentification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICPR</publisher>
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Renet: A recurrent neural network based alternative to convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00393</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wide to see better, in: arXiv e-prints</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int&apos;l. Workshop on Perf. Eval. of Track. and Surv&apos;l</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparsity-based occlusion handling method for person re-identification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Modeling</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="61" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised metric fusion over multiview data by graph random walk-based cross-view diffusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="70" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust subspace clustering for multi-view data by exploiting correlation consensus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3939" to="3949" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep filter banks for texture recognition and description</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3828" to="3837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="732" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1288" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A siamese long shortterm memory architecture for human re-identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1281" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1363" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07732</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
