<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-step Adversarial training with Dropout Scheduling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-18">18 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-step Adversarial training with Dropout Scheduling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-18">18 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.08628v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning models have shown impressive performance across a spectrum of computer vision applications including medical diagnosis and autonomous driving. One of the major concerns that these models face is their susceptibility to adversarial attacks. Realizing the importance of this issue, more researchers are working towards developing robust models that are less affected by adversarial attacks. Adversarial training method shows promising results in this direction. In adversarial training regime, models are trained with mini-batches augmented with adversarial samples. Fast and simple methods (e.g., single-step gradient ascent) are used for generating adversarial samples, in order to reduce computational complexity. It is shown that models trained using single-step adversarial training method (adversarial samples are generated using noniterative method) are pseudo robust. Further, this pseudo robustness of models is attributed to the gradient masking effect. However, existing works fail to explain when and why gradient masking effect occurs during single-step adversarial training. In this work, (i) we show that models trained using single-step adversarial training method learn to prevent the generation of single-step adversaries, and this is due to over-fitting of the model during the initial stages of training, and (ii) to mitigate this effect, we propose a singlestep adversarial training method with dropout scheduling. Unlike models trained using existing single-step adversarial training methods, models trained using the proposed single-step adversarial training method are robust against both single-step and multi-step adversarial attacks, and the performance is on par with models trained using computationally expensive multi-step adversarial training methods, in white-box and black-box settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning models are susceptible to adversarial samples: samples with imperceptible, engineered noise designed to manipulate model's output <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>. Further, Szegedy et al. <ref type="bibr" target="#b33">[34]</ref> observed that these adversarial samples are transferable across multiple models i.e., adversarial samples generated on one model might mislead other models. Due to which, models deployed in the real world are susceptible to black-box attacks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref>, where limited or no knowledge of the deployed model is available to the attacker. Various schemes have been proposed to defend against adversarial attacks (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23]</ref>), in this direction Adversarial Training (AT) procedure <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40]</ref> shows promising results.</p><p>In adversarial training regime, models are trained with mini-batches containing adversarial samples typically generated by the model being trained. Adversarial sample generation methods range from simple methods <ref type="bibr" target="#b12">[13]</ref> to complex optimization methods <ref type="bibr" target="#b23">[24]</ref>. In order to reduce computational complexity, non-iterative methods such as Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b12">[13]</ref> are typically used for generating adversarial samples. Further, it has been shown that models trained using single-step adversarial training methods are pseudo robust <ref type="bibr" target="#b34">[35]</ref>:</p><p>• Although these models appears to be robust to singlestep attacks in white-box setting (complete knowledge of the deployed model is available to the attacker), they are susceptible to single-step attacks (non-iterative methods) in black-box attack setting <ref type="bibr" target="#b34">[35]</ref>.</p><p>• Further, these models are susceptible to multi-step attacks (iterative methods) in both white-box setting <ref type="bibr" target="#b17">[18]</ref> and black-box setting <ref type="bibr" target="#b9">[10]</ref>.</p><p>Tramer et al. <ref type="bibr" target="#b34">[35]</ref> demonstrated that models trained using single-step adversarial training method converges to degenerative minima, and exhibit gradient masking effect. Singlestep adversarial sample generation methods such as FGSM, compute adversarial perturbations based on the linear approximation of the model's loss function i.e., image is perturbed in the direction of the gradient of loss with respect to the input image. Gradient masking effect causes this linear approximation of loss function to become unreliable for generating adversarial samples during single-step adversarial training. Madry et al. <ref type="bibr" target="#b21">[22]</ref> demonstrated that models trained using adversarial samples that maximize the training loss are robust against single-step and multi-step at-tacks. Such samples could be generated using the Projected Gradient Descent (PGD). However, PGD method is an iterative method, due to which training time increases substantially. Though prior works have enabled to learn robust models, they fail to answer the following important questions: (i) Why models trained using single-step adversarial training method exhibit gradient masking effect? and (ii) At what phase of the single-step adversarial training, the model starts to exhibit gradient masking effect?</p><p>In this work, we attempt to answer these questions and propose a novel single-step adversarial training method to learn robust models. First, we show that models trained using single-step adversarial training method learn to prevent the generation of single-step adversaries, and this is due to over-fitting of the model during the initial stages of training. Over-fitting of the model on single-step adversaries causes linear approximation of loss function to become unreliable for generating adversarial samples i.e., gradient masking effect. Finally, we propose a single-step adversarial training method with dropout scheduling to learn robust models. Note that, just adding dropout layer (typical setting: dropout layer with fixed dropout probability after FC+ReLU layer) does not help the model trained using single-step adversarial training method to gain robustness. Prior works observed no significant improvement in the robustness of models (with dropout layers in typical setting), trained using normal training and single-step adversarial training methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. Results for these settings are shown in section 4.1. Unlike typical setting, we introduce dropout layer after each non-linear layer (i.e., dropout-2D after conv2D+ReLU, and dropout-1D after FC+ReLU) of the model, and further decay its dropout probability as training progress. Interestingly, we show that this proposed dropout setting has significant impact on the model's robustness. The major contributions of this work can be listed as follows:</p><p>• We show that models trained using single-step adversarial training method learns to prevent the generation of single-step adversaries, and this is due to over-fitting of the model during the initial stages of training.</p><p>• Harnessing on the above observation, we propose a single-step adversarial training method with dropout probability scheduling. Unlike models trained using existing single-step adversarial training methods, models trained using the proposed method are robust against both single-step and multi-step attacks.</p><p>• The proposed single-step adversarial training method is much faster than multi-step adversarial training methods, and achieves on par results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Notations</head><p>Consider a neural network f trained to perform image classification task, and θ represents parameters of the neural network. Let x represents the image from the dataset and y true be its corresponding ground truth label. The neural network is trained using loss function J (e.g., cross-entropy loss), and ∇ x J represents the gradient of loss with respect to the input image x. Adversarial image x adv is generated by adding norm-bounded perturbation δ to the image x. Perturbation size ( ) represents the l ∞ norm constraint on the generated adversarial perturbation i.e., ||δ|| ∞ ≤ . Please refer to supplementary document for details on adversarial training and attack generation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Works</head><p>Following the findings of Szegedy et al. <ref type="bibr" target="#b33">[34]</ref>, various attacks (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed. Further, in order to defend against adversarial attacks, various schemes such as adversarial training (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>) and input pre-processing (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>) have been proposed. Athalye et al. <ref type="bibr" target="#b0">[1]</ref> showed that obfuscated gradients give a false sense of robustness, and broke seven out of nine defense papers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref> accepted to ICLR 2018. In this direction, adversarial training method <ref type="bibr" target="#b21">[22]</ref>, shows promising results for learning robust deep learning models. Kurakin et al. <ref type="bibr" target="#b17">[18]</ref> observed that models trained using single-step adversarial training methods are susceptible to multi-step attacks. Further, Tramer et al. <ref type="bibr" target="#b34">[35]</ref> demonstrated that these models exhibit gradient masking effect, and proposed Ensemble Adversarial Training (EAT) method. However, models trained using EAT are still susceptible to multi-step attacks in white-box setting. Madry et al. <ref type="bibr" target="#b21">[22]</ref> demonstrated that adversarially trained model can be made robust against white-box attacks, if perturbation crafted while training maximizes the loss. Zhang et al. <ref type="bibr" target="#b39">[40]</ref> proposed a regularizer for multi-step adversarial training, that encourages the output of the network to be smooth. On the other hand, works such as <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b35">[36]</ref> propose a method to learn models that are provably robust against norm bounded adversarial attacks. However, scaling these methods to deep networks and large perturbation sizes is difficult. Whereas, in this work we show that it is possible to learn robust models using single-step adversarial training method, if over-fitting of the model on adversarial samples is prevented during training. We achieve this by introducing dropout layer after each non-linear layer of the model with a dropout schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Over-fitting and its effect during adversarial training</head><p>In this section, we show that models trained using singlestep adversarial training method learn to prevent the genera-  tion of single-step adversaries, and this is due to over-fitting of the model during the initial stages of training. First, we discuss the criteria for learning robust models using adversarial training method, and then we show that this criteria is not satisfied during single-step adversarial training method. Most importantly, we show that over-fitting effect is the reason for failure to satisfy the criteria.</p><p>Madry et al. <ref type="bibr" target="#b21">[22]</ref> demonstrated that it is possible to learn robust models using adversarial training method, if adversarial perturbations (l ∞ norm bounded) crafted while training maximizes the model's loss. This training objective is formulated as a minimax optimization problem (Eq. 1). Where ψ represents the feasible set e.g., for l ∞ norm constraint attacks ψ = {δ : ||δ|| ∞ ≤ }, and D is the training set.</p><formula xml:id="formula_0">min θ E (x,y)∈D max δ∈ψ J f (x + δ; θ), y true<label>(1)</label></formula><formula xml:id="formula_1">R = loss adv loss clean<label>(2)</label></formula><p>At each iteration, norm bounded adversarial perturbations that maximizes the training loss should be generated. Further, the model's parameters (θ) should be updated so as to decrease the loss on such adversarial samples. Madry et al. <ref type="bibr" target="#b21">[22]</ref> solves the maximization step by generating adversarial samples using an iterative method named Projected Gradient Descent (PGD). In order to quantify the extent of inner maximization of Eq. ( <ref type="formula" target="#formula_0">1</ref>), we compute loss ratio R using Eq. ( <ref type="formula" target="#formula_1">2</ref>). Loss ratio is defined as the ratio of loss on the adversarial samples to the loss on its corresponding clean samples for a given perturbation size . The metric R captures the extent of inner maximization achieved by the generated adversarial samples i.e., factor by which loss has increased by perturbing the clean samples.</p><p>A sample is said to be an adversarial sample if it is capable of manipulating the model's prediction. Such manipulations could be achieved by perturbing the samples along the adversarial direction <ref type="bibr" target="#b12">[13]</ref>. A perturbation is said to be an adversarial perturbation when it causes loss on the perturbed sample to increase. This implies that the loss on the adversarially perturbed samples should be greater than the loss on the corresponding unperturbed samples i.e., loss adv &gt; loss clean . Based on these facts, R can be interpreted in the following manner:</p><p>• Generated perturbation is said to be an adversarial perturbation if R &gt;1 i.e., loss adv &gt; loss clean</p><p>• R &lt;1 i.e., loss adv &lt; loss clean , implies that the generated perturbation is not an adversarial perturbation.</p><p>The attack method fails to generate adversarial perturbations for the given model.</p><p>We obtain the plot of R versus iteration for models trained using single-step adversarial training method <ref type="bibr" target="#b12">[13]</ref> and multi-step adversarial training method <ref type="bibr" target="#b21">[22]</ref>. Column-1 of Fig. <ref type="figure" target="#fig_0">1</ref> and Fig. <ref type="figure" target="#fig_1">2</ref> show these plots obtained for LeNet+ trained on MNIST dataset <ref type="bibr" target="#b18">[19]</ref> using single-step and multistep adversarial training methods respectively. It can be observed that during single-step adversarial training, R initially increases and then starts to decay rapidly. Further R becomes less than one after 20 (×100) iterations. This implies that single-step adversarial sample generation method is unable to generate adversarial perturbations for the model, leading to adversarial training without useful adversarial samples. We demonstrate this behavior of the model to prevent the inclusion of adversarial samples is due to over-fitting on the adversarial samples. Typically during normal training, loss on the validation set is monitored to detect over-fitting effect i.e., validation loss increases when the model starts to over-fit on the training set. Unlike normal training, during adversarial training we monitor the loss on the clean and adversarial validation set. A normally trained model is used for generating adversarial validation set, so as to ensure that the generated adversarial validation samples are independent of the model being trained. Column-2 and column-3 of Fig. <ref type="figure" target="#fig_0">1</ref> shows the plot of loss versus iteration during training of LeNet+ on MNIST dataset using single-step adversarial training. It can be observed that, when R starts to decay, loss on the adversarial validation set starts to increase. This increase in the validation loss indicates overfitting of the model on the single-step adversaries. Whereas, during multi-step adversarial training method, R initially increases and then saturates (column-1, Fig. <ref type="figure" target="#fig_1">2</ref>). Further, no such over-fitting effect is observed for the entire training duration (column-3, Fig. <ref type="figure" target="#fig_1">2</ref>). Note that, a normally trained model was used for generating FGSM ( =0.3) adversarial validation set, and we observe similar trend if a normally trained model of different architecture is used for generating FGSM adversarial validation set, please refer to supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effect of dropout layer</head><p>In the previous section, we showed that models trained using single-step adversarial training learn to prevent the generation of single-step adversaries. Further, we demonstrated that this behavior of models is due to over-fitting. Dropout layer <ref type="bibr" target="#b32">[33]</ref> has been shown to be effective in mitigating over-fitting during training, and typically dropout-1D layer is added after FC+ReLU layers in the networks. We refer to this setting as typical setting. Prior works which used dropout layer during single-step adversarial training observed no significant improvement in the model's robustness. This is due to the use of dropout layer in typical setting. Whereas, we empirically show that it is necessary to introduce dropout layer after every non-linear layer of the model (proposed dropout setting i.e., dropout-2D after Conv2D+ReLU layer and dropout-1D after FC+ReLU layer) to mitigate over-fitting during single-step adversarial training, and to enable the model to gain robustness against adversarial attacks (single-step and multi-step attacks). We train LeNet+ with dropout layer in typical setting and in the proposed setting respectively, on MNIST dataset using single-step adversarial training method for different values of dropout probability. After training, we obtain the performance of these resultant models against PGD attack ( =0.3, step =0.01, steps=40). Column-1 of Fig. <ref type="figure" target="#fig_2">3</ref> shows the trend of accuracy of these models for PGD attack with respect to the dropout probability used while training. It can be observed that the gain in the robustness of adversarially trained model with dropout layer in the proposed setting is significantly better compared to the adversarially trained model with dropout layer in typical setting (FAT-TS). From column-2 of Fig. <ref type="figure" target="#fig_2">3</ref>, it can be observed that the robustness of adversarially trained model with dropout layer in the proposed setting, increases with the increase in the dropout probability (p) and reaches a peak value at p=0.4. Further increase in the dropout probability causes decrease in the accuracy on both clean and adversarial samples. Based on this observation, we propose an improved single-step adversarial training in the next subsection. Furthermore, we perform normal training of LeNet+ with dropout layers in typical setting and in the proposed setting, on MNIST dataset. From column-1 of Fig. <ref type="figure" target="#fig_2">3</ref>, it can be observed that there is no significant improvement in the robustness of these normally trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SADS: Single-step Adversarial training with Dropout Scheduling</head><p>Column-1 of Fig. <ref type="figure" target="#fig_2">3</ref> indicates that use of dropout layer in typical setting is not sufficient to avoid over-fitting on adversarial samples, and we need severe dropout regime involving all the layers (i.e., proposed setting: dropout layer after Conv2D+ReLU and FC+ReLU layers) of the network in order to avoid over-fitting. For the proposed dropout regime, determining exact dropout probability is network dependent and is difficult. Further, having high dropout probability causes under-fitting of the model, and having low dropout probability causes the model to over-fit on the adversarial samples.</p><p>Based on these observations, we propose a single-step adversarial training method with dropout scheduling (Algorithm 1). In the proposed training method, we introduce dropout layer after each non-linear layer of the model to be trained. We initialize these dropout layers with a high dropout probability P d . Further, during training we linearly decay the dropout probability of all the dropout layers and this decay in the dropout probability is controlled by the hyper-parameter r d . The hyper-parameter, r d is expressed in terms of maximum training iterations (e.g., r d =1/2 implies that dropout probability reaches zero when the current training iteration is equal to half of the maximum training iterations). In experimental section 5, we show the effectiveness of the proposed training method. Note that dropout layer is only used while training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we show the effectiveness of models trained using the proposed single-step adversarial training method (SADS) in white-box and black-box settings. We perform the sanity tests described in <ref type="bibr" target="#b6">[7]</ref>, in order to verify that models trained using SADS are robust and does not exhibit obfuscated gradients (Athalye et al. <ref type="bibr" target="#b0">[1]</ref> demonstrated that models exhibiting obfuscated gradients are not robust against adversarial attacks). We show results on MNIST <ref type="bibr" target="#b18">[19]</ref>, Fashion-MNIST <ref type="bibr" target="#b36">[37]</ref> and CIFAR-10 <ref type="bibr" target="#b15">[16]</ref> datasets. We use LeNet+ (please refer to supplementary document for details on network architecture) for both MNIST and Fashion-MNIST datasets. For CIFAR-10 dataset, WideResNet-28-10 [39] is used. These models are trained using SGD with momentum.</p><p>Step-policy is used for learning rate scheduling. For all datasets, images are preprocessed to be in [0,1] range. For CIFAR-10, random crop and horizontal flip are performed for data-augmentation. Evaluation: We show the performance of models against adversarial attacks in white-box and black-box setting. For SADS, we report mean and standard deviation over three runs.</p><p>Attacks: For l ∞ based attacks, we use Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b12">[13]</ref>, Iterative Fast Gradient Sign Method (IFGSM) <ref type="bibr" target="#b16">[17]</ref>, Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type="bibr" target="#b9">[10]</ref> and Projected Gradient Descent  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance in White-box setting</head><p>We train models on MNIST, Fashion-MNIST and CIFAR-10 datasets respectively, using NT, FAT, PAT, TRADES and SADS (Algorithm 1) training methods. Models are trained for 50, 50 and 100 epochs on MNIST, Fashion-MNIST and CIFAR-10 datasets respectively. For SADS, we set the hyper-parameter P d and r d to (0.8, 0.5), (0.8, 0.75) and (0.5, 0.5) for MNIST, Fashion-MNIST and CIFAR-10 datasets respectively. Table <ref type="table" target="#tab_0">1</ref>, 2 and 3 shows the performance of these models against single-step and multi-step attacks in white-box setting, rows represent the training method and columns represent the attack generation method. It can be observed that models trained using FAT are not robust against multi-step attacks. Whereas, models trained using PAT, TRADES and SADS are robust against both single-step and multi-step attacks. Unlike PAT and TRADES, the proposed SADS method is a single-step adversarial training method.</p><p>PGD attack with large steps: Engstrom et al. <ref type="bibr" target="#b10">[11]</ref> demonstrated that the performance of models trained using certain adversarial training methods degrade significantly with increase in the number of steps of PGD attack. In order to verify that such behavior is not observed in models trained using SADS, we obtain the plot of classification accuracy on PGD test-set versus steps of PGD attack. Fig. <ref type="figure" target="#fig_4">4</ref> shows these plots obtained for models trained using PAT and SADS on MNIST, Fashion-MNIST and CIFAR-10 datasets respectively. It can be observed that the accuracy of models on PGD test set initially decreases slightly and then saturates. Even for PGD attack with large steps, there is no significant degradation in the performance of models trained using PAT and SADS methods. In supplementary document, we show the effect of hyper-parameters of the proposed training method.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance in Black-box setting</head><p>In this subsection, we show the performance of models trained using different training methods against adversarial attacks in black-box setting. Typically, a substitute model (source model) is trained on the same task using normal training method, and this trained substitute model is used for generating adversarial samples. The generated adversarial samples are transferred to the deployed model (target model). We use FGSM and MI-FGSM methods for generating adversarial samples, since samples generated using these methods show good transfer rates <ref type="bibr" target="#b9">[10]</ref>. Table <ref type="table" target="#tab_3">4</ref> shows the performance of models trained using different methods, in black-box setting. It can be observed that the performance of models trained using PAT and SADS in black-box setting is better than that in white-box setting. Further, it can be observed that the performance of models trained on MNIST and CIFAR-10 datasets using FAT is worse in black-box setting than compared in white-box setting. Please refer to supplementary file for details on network architecture of source models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance against DeepFool and C&amp;W attacks</head><p>DeepFool <ref type="bibr" target="#b23">[24]</ref> and C&amp;W <ref type="bibr" target="#b7">[8]</ref> attacks generate adversarial perturbations with minimum l 2 norm, that is required to fool the classifier. These methods measure the robustness of the model in terms of the average l 2 norm of the generated adversarial perturbations for the test set. For an undefended model, adversarial perturbation with small l 2 norm Table <ref type="table">5</ref>: DeepFool and C&amp;W attacks: Performance of models trained using different training methods against DeepFool and C&amp;W attacks. These attack methods measure the robustness of the model based on the average l 2 norm of the generated perturbations, higher the better. Success defines the percentage of samples of test set that has been misclassified. Note that, for models trained using PAT and SADS, perturbations with relatively large l 2 norm is required to fool the classifier. is enough to fool the classifier. Whereas for robust models, adversarial perturbation with relatively large l 2 norm is required to fool the classifier. Table <ref type="table">5</ref>, shows the performance of models trained using NT, FAT, PAT and SADS methods, against DeepFool and C&amp;W attacks. It can be observed that models trained using PAT and SADS have relatively large average l 2 norm. Whereas, for models trained using NT and FAT have small average l 2 norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Sanity tests</head><p>We perform sanity tests described in <ref type="bibr" target="#b6">[7]</ref> to verify whether models trained using SADS are adversarially robust and are not exhibiting obfuscated gradients. We perform following sanity tests:  <ref type="table" target="#tab_3">4</ref>, it can be observed that white-box attacks are stronger than black-box attacks for models trained using SADS. Fig. <ref type="figure" target="#fig_5">5</ref> shows the accuracy plot for the model on test set versus perturbation size of PGD attack, obtained for models trained using SADS. It can be observed that the model's accuracy falls to zero for large perturbation size ( ). From Fig. <ref type="figure" target="#fig_5">5</ref>, it can be observed that PGD attack success rate (attack success rate is equal to (100 -model's accuracy)%) increases with increase in the distortion bound (perturbation size) of the attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Time Complexity</head><p>In order to quantify the complexity of different training methods, we measure training time per epoch (seconds) for models trained using different training methods. Table <ref type="table" target="#tab_4">6</ref> shows the training time per epoch for models trained on MNIST and CIFAR-10 datasets respectively. Note that the training time of SADS and FAT is of the same order. The increase in the training time for PAT and TRADES is due to their iterative nature of generating adversarial samples. We ran this timing experiment on a machine with NVIDIA Titan Xp GPU, with no other jobs on this GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have demonstrated that models trained using single-step adversarial training methods learn to prevent the generation of adversaries due to over-fitting of the model during the initial stages of training. To mitigate this effect, we have proposed a novel single-step adversarial training method with dropout scheduling. Unlike existing single-step adversarial training methods, models trained using the proposed method achieves robustness not only against single-step attacks but also against multi-step attacks. Further, the performance of models trained using the proposed method is on par with models trained using multi-step adversarial training methods, and is much faster than multi-step adversarial training methods.   <ref type="table">8</ref> shows the setup used for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by <ref type="bibr" target="#b21">[22]</ref>. At each iteration all the clean samples in the mini-batch are replaced with their corresponding adversarial samples generated using the model being trained. Projected Gradient Descent (PGD) method is used for generating these samples. TRADES: Multi-step adversarial training method proposed by <ref type="bibr" target="#b39">[40]</ref>. The method proposes a regularizer that encourages the output of the network to be smooth. The training mini-batches contain clean and their corresponding adversarial samples. These adversarial samples are generated using Projected Gradient Descent with modified loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S 4. Additional plots to illustrate over-fitting effect</head><p>In the main paper, we showed over-fitting effect during training of LeNet+ on MNIST dataset using single-step adversarial training. Fig. <ref type="figure" target="#fig_6">6</ref> shows the plot of validation loss, obtained for ResNet-34 trained on CIFAR-10 dataset using single-step adversarial training. We observe over-fitting effect even when model with different architecture is used for generating adversarial validation set. Fig. <ref type="figure" target="#fig_7">7</ref> shows the validation loss obtained for LeNet+ trained on MNIST dataset using single-step adversarial training. Normally trained models with different architecture are used for generating adversarial validation set.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S 5. Effect of Hyper-Parameters</head><p>In order to show the effect of hyper-parameters, we train LeNet+ shown in table 7 on MNIST dataset, using SADS with different hyper-parameter settings. Validation set accuracy of the model for PGD attack ( = 0.3 and steps = 40) is obtained for each hyper-parameter setting with one of them being fixed and the other being varied. Effect of hyper-parameter P d : The hyper-parameter P d defines the initial dropout probability applied to all dropout layers. We train LeNet+ on MNIST dataset, using the proposed method for different initial dropout probability P d . Column-1 of Fig. <ref type="figure" target="#fig_8">8</ref> shows the effect of varying dropout probability from 0.3 to 0.9. It can be observed that the robustness of the model to multi-step attack initially increases with the increase in the value of P d (P d &lt; 0.8), and further increase in P d causes the model's robustness to decrease, and this is due to under-fitting. Effect of hyper-parameter r d : The hyper-parameter r d decides the iteration at which dropout probability reaches zero and is expressed in terms of maximum training iteration. Column-2 of Fig. <ref type="figure" target="#fig_8">8</ref> shows the effect varying r d from 1/4 to 1. It can be observed that for r d &lt; 0.5, there is degradation in the robustness of the model against multi-step attacks. This is because, during the initial stages of training, learning rate is high and the model can easily over-fit to adversaries generated by single-step method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S 6. Comparison with Ensemble Adversarial Training</head><p>We train WideResNet-28-10 [39] on CIFAR-10 [16] dataset using EAT and SADS. Table <ref type="table">8</ref> shows the setup used for EAT. Pre-trained models are used for generating adversarial samples during EAT. Table <ref type="table" target="#tab_6">9</ref> shows the recognition accuracy of models trained using EAT and SADS in white-box attack setting. It can be observed that the model trained using SADS is robust to both single-step (FGSM) and multi-step attacks (PGD), whereas models trained using EAT are susceptible to multi-step attack.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Single-step adversarial training: Trend of R , training loss, and validation loss during single-step adversarial training, obtained for LeNet+ trained on MNIST dataset. Column-1: plot of R versus training iteration. Column-2: training loss versus training iteration. Column-3: validation loss versus training iteration. Note that, when R starts to decay, loss on adversarial validation set starts to increase indicating that the model is over-fitting on the adversarial samples.</figDesc><graphic url="image-1.png" coords="3,51.34,72.00,490.05,145.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multi-step adversarial training: Trend of R , training loss, and validation loss during multi-step adversarial training, obtained for LeNet+ trained on MNIST dataset. Column-1: plot of R versus training iteration. Column-2: training loss versus training iteration. Column-3: validation loss versus training iteration. Note that, for the entire training duration R does not decay, and no over-fitting effect can be observed.</figDesc><graphic url="image-2.png" coords="3,73.62,273.02,445.49,132.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Column-1: Effect of dropout probability of dropout layers in typical setting and in the proposed setting on the model's robustness against PGD attack ( =0.3, step =0.01 and steps=40). Obtained for LeNet+ trained on MNIST dataset. NT-TS: Normal training with dropout layer in typical setting. FAT-TS: Single-step adversarial training with dropout layer in typical setting. NT-PS: Normal training with dropout layer in the proposed setting. Proposed: Single-step adversarial training with dropout layer in the proposed setting. Column-2: Effect of dropout probability on the model's accuracy on clean and PGD adversarial validation set ( =0.3, step =0.01 and steps=40). Obtained for LeNet+ with dropout layer in the proposed setting, trained using single-step adversarial training method on MNIST dataset.</figDesc><graphic url="image-3.png" coords="5,50.11,72.00,233.88,106.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 5 7 iteration = iteration + 1 8</head><label>1571</label><figDesc>Single-step Adversarial training with Dropout Scheduling (SADS) Input: Training mini-batch size (m) Maximum training iterations (M ax itertion ) Hyper-parameters: P d , r d 1 Initialization Randomly initialize network N iteration = 0 prob = P d Insert dropout layer after each non-linear layer of the network N Set dropout probability (p) of all the dropout layers with prob while iteration ≤ M ax itertion do 2 Read minibatch B={x 1 , .., x m } from training set 3 Compute FGSM adversarial sample {x 1 adv , ..., x m adv } from corresponding clean samples {x 1 , ..., x m } using the current state of the network N 4 Make new minibatch B * = {x 1 adv , ..., x m adv } /*Forward pass, compute loss, backward pass, and update parameters*/ Do one training step of Network N using minibatch B * /*Update dropout probability of Dropout-1D and Dropout-2D layers with prob*/ 6 prob = max( 0, P d • (1 − iteration r d •M axitertion ) ) end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot of accuracy of the model trained using PAT and SADS, on PGD adversarial test set versus steps of PGD attack with fixed . For PGD attack we set ( , step ) to (0.3,0.01), (0.1,0.01) and (8/255,2/255) for MNIST, Fashion-MNIST and CIFAR-10 datasets. Note, x-axis is in logarithmic scale.</figDesc><graphic url="image-4.png" coords="7,73.62,72.00,445.51,128.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Plot of accuracy versus perturbation size of PGD attack, obtained for models trained using SADS. It can be observed that the accuracy of the model is zero for PGD attack with large perturbation size.</figDesc><graphic url="image-5.png" coords="7,73.62,241.72,445.49,122.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Single-step adversarial training: Trend of validation loss during single-step adversarial training, obtained for ResNet-34 trained on CIFAR-10 dataset. Adversarial validation set is generated using column-1: ResNet-34, column-2: ResNet-18, column-3: VGG-16 and column-4: VGG-19.</figDesc><graphic url="image-6.png" coords="12,51.34,72.00,490.05,109.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Single-step adversarial training: Trend of validation loss during single-step adversarial training, obtained for LeNet+ trained on MNIST dataset. Adversarial validation set is generated using column-1: Model-A, column-2: Model-B, column-3: Model-C and column-4: Model-D. FGSM Adversarial Training (FAT): During training, at each iteration a portion of clean samples in the mini-batch are replaced with their corresponding adversarial samples generated using the model being trained. Fast Gradient Sign Method (FGSM) is used for generating these adversarial samples. Ensemble Adversarial Training (EAT) [35]: At each iteration a portion of clean samples in the mini-batch are replaced with their corresponding adversarial samples. These adversarial samples are generated by the model being trained or by one of the model from the fixed set of pre-trained models. Table8shows the setup used for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by<ref type="bibr" target="#b21">[22]</ref>. At each iteration all the clean samples in the mini-batch are replaced with their corresponding adversarial samples generated using the model being trained. Projected Gradient Descent (PGD) method is used for generating these samples. TRADES: Multi-step adversarial training method proposed by<ref type="bibr" target="#b39">[40]</ref>. The method proposes a regularizer that encourages the output of the network to be smooth. The training mini-batches contain clean and their corresponding adversarial samples. These adversarial samples are generated using Projected Gradient Descent with modified loss function.</figDesc><graphic url="image-8.png" coords="12,310.04,565.96,233.89,94.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of hyper-parameter P d and r d of SADS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: MNIST: Trend of R , training loss, and validation loss during SADS training method, obtained for LeNet+ trained on MNIST dataset. Column-1: plot of R versus iteration. Column-2: training loss versus iteration. Column-3: validation loss versus iteration. Note that, for the entire training duration R does not decay, and no over-fitting effect can be observed.</figDesc><graphic url="image-9.png" coords="13,51.34,72.00,490.05,147.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Fashion-MNIST: Trend of R , training loss, and validation loss during SADS training method, obtained for LeNet+ trained on Fashion-MNIST dataset. Column-1: plot of R versus iteration. Column-2: training loss versus iteration. Column-3: validation loss versus iteration. Note that, for the entire training duration R does not decay, and no over-fitting effect can be observed.</figDesc><graphic url="image-10.png" coords="13,52.59,254.92,490.05,145.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>S 7 .</head><label>7</label><figDesc>Fig.9and 10 show the trend of R , training and validation loss, obtained for models trained using SADS. It can be observed that for the entire training duration R does not decay and no over-fitting effect can be observed. Table 8: Setup used for Ensemble Adversarial Training. Network to be trained Pre-trained Models WRN-28-10 (Ens-A) WRN-28-10, ResNet-34 CIFAR-10 WRN-28-10 (Ens-B) WRN-28-10, VGG-19 WRN-28-10 (Ens-C) ResNet-34, VGG-19</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MNIST: White-Box setting. Classification accuracy (%) of models trained on MNIST dataset using different training methods. For all attacks =0.3 is used and for PGD attack step =0.01 is used. For both IFGSM and PGD attacks, steps is set to 40.</figDesc><table><row><cell>Training</cell><cell></cell><cell cols="2">Attack Method</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Clean FGSM IFGSM</cell><cell>PGD</cell></row><row><cell>NT</cell><cell>99.24</cell><cell>11.65</cell><cell>0.31</cell><cell>0.01</cell></row><row><cell></cell><cell cols="3">Multi-step adversarial training</cell><cell></cell></row><row><cell>PAT</cell><cell>98.41</cell><cell>95.56</cell><cell cols="2">92.64 92.08</cell></row><row><cell cols="2">TRADES 98.70</cell><cell>96.30</cell><cell cols="2">95.14 95.05</cell></row><row><cell></cell><cell cols="3">Single-step adversarial training</cell><cell></cell></row><row><cell>FAT</cell><cell>99.34</cell><cell>89.04</cell><cell>1.19</cell><cell>0.17</cell></row><row><cell>SADS</cell><cell>98.89</cell><cell>94.78</cell><cell cols="2">89.35 88.51</cell></row><row><cell></cell><cell>±0.01</cell><cell>±0.19</cell><cell cols="2">±0.09 ±0.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Fashion-MNIST: White-Box attack. Classification accuracy (%) of models trained on Fashion-MNIST dataset using different training methods. For all attacks =0.1 is used and for PGD attack step =0.01 is used. For both IFGSM and PGD attacks, steps is set to 40.</figDesc><table><row><cell>Training</cell><cell></cell><cell cols="2">Attack Method</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Clean FGSM IFGSM</cell><cell>PGD</cell></row><row><cell>NT</cell><cell>91.42</cell><cell>6.46</cell><cell>1.01</cell><cell>0.16</cell></row><row><cell></cell><cell cols="3">Multi-step adversarial training</cell><cell></cell></row><row><cell>PAT</cell><cell>84.55</cell><cell>77.30</cell><cell cols="2">75.95 75.18</cell></row><row><cell cols="2">TRADES 86.69</cell><cell>80.39</cell><cell cols="2">78.94 78.04</cell></row><row><cell></cell><cell cols="3">Single-step adversarial training</cell><cell></cell></row><row><cell>FAT</cell><cell>90.45</cell><cell>83.43</cell><cell cols="2">21.26 16.65</cell></row><row><cell>SADS</cell><cell>85.21</cell><cell>75.81</cell><cell cols="2">71.14 69.51</cell></row><row><cell></cell><cell>±0.08</cell><cell>±1.31</cell><cell cols="2">±1.01 ±1.43</cell></row><row><cell cols="5">(PGD) [22]. For l 2 based attack, we use DeepFool [24] and</cell></row><row><cell cols="2">Carlini &amp; Wagner [8].</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Perturbation size: For l ∞ based attacks, we set pertur-</cell></row><row><cell cols="5">bation size ( ) to the values described in [22] i.e., =0.3,</cell></row><row><cell cols="5">0.1 and 8/255 for MNIST, Fashion-MNIST and CIFAR-10</cell></row><row><cell cols="2">datasets respectively.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Comparisons: We compare the performance of the pro-</cell></row><row><cell cols="5">posed single-step adversarial training method (SADS)</cell></row><row><cell cols="5">with Normal training (NT), FGSM adversarial training</cell></row><row><cell cols="5">(FAT) [18], Ensemble adversarial training (EAT) [35], PGD</cell></row><row><cell cols="5">adversarial training (PAT) [22], and TRADES [40]. Note</cell></row></table><note>that, FAT, EAT and SADS (ours) are single-step adversarial training methods, whereas PAT and TRADES are multi-step adversarial training methods. Results for EAT are shown in supplementary document.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>CIFAR-10: White-Box attack. Classification accuracy (%) of models trained on CIFAR-10 dataset using different training methods. For all attacks =8/255 is used and for PGD attack step =2/255 is used. For both IFGSM and PGD attacks, steps is set to 7.</figDesc><table><row><cell>Training</cell><cell></cell><cell cols="2">Attack Method</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Clean FGSM IFGSM</cell><cell>PGD</cell></row><row><cell>NT</cell><cell>94.75</cell><cell>28.16</cell><cell>0.07</cell><cell>0.03</cell></row><row><cell></cell><cell cols="3">Multi-step adversarial training</cell><cell></cell></row><row><cell>PAT</cell><cell>85.70</cell><cell>53.96</cell><cell cols="2">48.65 47.23</cell></row><row><cell cols="2">TRADES 87.20</cell><cell>56.34</cell><cell cols="2">51.21 50.03</cell></row><row><cell></cell><cell cols="3">Single-step adversarial training</cell><cell></cell></row><row><cell>FAT</cell><cell>94.04</cell><cell>98.54</cell><cell>0.31</cell><cell>0.09</cell></row><row><cell>SADS</cell><cell>82.01</cell><cell>51.99</cell><cell cols="2">46.37 45.66</cell></row><row><cell></cell><cell>±0.06</cell><cell>±1.02</cell><cell cols="2">±1.17 ±1.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Black-box setting: Performance of models trained on MNIST, Fashion-MNIST and CIFAR-10 datasets using different training method, against adversarial attacks in black-box setting. Source models are used for generating adversarial samples, and the target models are tested on these generated adversarial samples.</figDesc><table><row><cell></cell><cell>MNIST</cell><cell></cell></row><row><cell></cell><cell>Source Model</cell><cell>NT</cell><cell>Target Model FAT PAT SADS</cell></row><row><cell>Model-A</cell><cell cols="3">FGSM ( =0.3) MI-FGSM ( =0.3, steps=40) 10.69 72.44 95.83 94.80 29.09 79.49 96.01 95.06</cell></row><row><cell>Model-B</cell><cell cols="3">FGSM ( =0.3) MI-FGSM ( =0.3, steps=40) 12.32 70.79 95.97 94.81 28.13 72.39 96.15 95.11</cell></row><row><cell></cell><cell cols="2">Fashion-MNIST</cell></row><row><cell>Model-A</cell><cell cols="3">FGSM ( =0.1) MI-FGSM ( =0.1, steps=40) 33.04 88.36 81.20 80.68 36.66 88.26 81.32 80.86</cell></row><row><cell>Model-B</cell><cell cols="3">FGSM ( =0.1) MI-FGSM ( =0.1, steps=40) 38.01 84.72 79.84 78.59 39.03 85.40 80.01 78.94</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell></cell></row><row><cell>VGG-11</cell><cell cols="3">FGSM ( =8/255) MI-FGSM ( =8/255, steps=7) 31.61 76.35 78.36 77.95 48.46 78.70 78.12 77.97</cell></row><row><cell cols="2">DenseNet-FGSM ( =8/255)</cell><cell cols="2">39.58 86.90 80.29 80.06</cell></row><row><cell>BC-100</cell><cell cols="3">MI-FGSM ( =8/255, steps=7) 28.50 86.42 80.42 80.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Success Mean l 2 Success Mean l 2 Success Mean l 2 Success Mean l 2 Success Mean l 2 Comparison of training time per epoch of models trained on MNIST and CIFAR-10 datasets respectively, obtained for different training methods.</figDesc><table><row><cell></cell><cell></cell><cell>MNIST</cell><cell></cell><cell></cell><cell></cell><cell cols="2">F-MNIST</cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell></row><row><cell></cell><cell cols="2">DeepFool</cell><cell>CW</cell><cell></cell><cell cols="2">DeepFool</cell><cell>CW</cell><cell></cell><cell cols="2">DeepFool</cell><cell>CW</cell><cell></cell></row><row><cell cols="3">Success Mean l 2 NT 99.35 1.837</cell><cell>100</cell><cell>1.659</cell><cell>93.73</cell><cell>0.796</cell><cell>100</cell><cell>0.709</cell><cell>96</cell><cell>0.20</cell><cell>100</cell><cell>0.12</cell></row><row><cell>FAT</cell><cell>99.37</cell><cell>1.455</cell><cell>100</cell><cell>0.798</cell><cell>93.11</cell><cell>1.514</cell><cell>100</cell><cell>1.167</cell><cell>96</cell><cell>0.25</cell><cell>100</cell><cell>0.10</cell></row><row><cell>PAT</cell><cell>85.68</cell><cell>4.633</cell><cell>99</cell><cell>2.779</cell><cell>90.29</cell><cell>2.635</cell><cell>100</cell><cell>1.572</cell><cell>92</cell><cell>1.22</cell><cell>100</cell><cell>0.88</cell></row><row><cell>SADS</cell><cell>95.89</cell><cell>3.692</cell><cell>100</cell><cell>2.321</cell><cell>90.68</cell><cell>2.305</cell><cell>100</cell><cell>1.308</cell><cell>93</cell><cell>0.97</cell><cell>100</cell><cell>0.71</cell></row><row><cell></cell><cell cols="2">±0.06 ±0.033</cell><cell cols="4">0± ±0.027 ±0.26 ±0.102</cell><cell cols="4">±0 ±0.188 ±0.32 ±0.043</cell><cell cols="2">±0 ±0.014</cell></row><row><cell cols="2">Method</cell><cell cols="3">Training time per epoch (sec.) MNIST CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NT</cell><cell></cell><cell>∼ 2.7</cell><cell cols="2">∼ 104</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FAT</cell><cell></cell><cell>∼ 4.1</cell><cell cols="2">∼ 159</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PAT</cell><cell></cell><cell>∼ 53</cell><cell cols="2">∼ 820</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">TRADES ∼ 104</cell><cell cols="2">∼ 1558</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SADS</cell><cell>∼ 4.3</cell><cell cols="2">∼ 187</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>CIFAR-10: White-Box attack. Classification accuracy (%) of models trained on CIFAR-10 dataset using different training methods. For all attacks =8/255 is used and for PGD attack step =2/255 and steps=7 is used.</figDesc><table><row><cell>Training</cell><cell cols="3">Attack Method</cell></row><row><cell>Method</cell><cell cols="3">Clean FGSM PGD-7</cell></row><row><cell cols="2">EAT Ens-A 92.92</cell><cell>59.56</cell><cell>19.21</cell></row><row><cell>EAT Ens-B</cell><cell>92.75</cell><cell>63.40</cell><cell>5.34</cell></row><row><cell>EAT Ens-C</cell><cell>93.11</cell><cell>59.74</cell><cell>12.03</cell></row><row><cell>SADS</cell><cell>82.01</cell><cell>51.99</cell><cell>45.66</cell></row><row><cell></cell><cell>±0.06</cell><cell>±1.02</cell><cell>±1.26</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: This work was supported by Uchhatar Avishkar Yojana (UAY) project (IISC 010), MHRD, Govt. of India.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary S 1. Contents</head><p>• Section S 2: Network architecture.</p><p>• Section S 3: Adversarial training and Attack generation methods.</p><p>• Section S 4: Additional plots to illustrate over-fitting effect during single-step adversarial training.</p><p>• Section S 5: Effect of hyper-parameters  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Adversarial Sample Generation Methods</head><p>In this subsection, we discuss the formulation of adversarial attacks. Fast Gradient Sign Method (FGSM): Non-iterative attack method proposed by <ref type="bibr" target="#b12">[13]</ref>. This method generates l ∞ norm bounded adversarial perturbation based on the linear approximation of loss function.</p><p>Iterative Fast Gradient Sign Method (IFGSM) <ref type="bibr" target="#b16">[17]</ref>: Iterative version of FGSM attack. At each iteration, adversarial perturbation of small step size (α) is added to the image. In our experiments, we set α = /steps.</p><p>Projected Gradient Descent (PGD) <ref type="bibr" target="#b21">[22]</ref>: Initially, a small random noise sampled from Uniform distribution (U ) is added to the image. Then at each iteration, perturbation of small step size ( step ) is added to the image, and followed by re-projection.</p><p>x</p><p>x</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type="bibr" target="#b9">[10]</ref>: Introduces a momentum term into the IFGSM formulation. Here, µ represents the momentum term. α represents step size and is set to /steps.</p><p>x</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S 3.2. Adversarial Training Methods</head><p>In this subsection we explain the existing adversarial training methods. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pattern recognition systems under attack: Design issues and research challenges</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">07</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularizer to mitigate gradient masking effect during single-step adversarial training</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arya</forename><surname>Baburaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gray-box adversarial training</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Colin Raffel, and Ian Goodfellow. Thermometer Encoding: One Hot Way To Resist Adversarial Examples</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06705</idno>
		<title level="m">On evaluating adversarial robustness</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04644</idno>
		<title level="m">Towards Evaluating the Robustness of Neural Networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guneet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">D</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2005">2018. 1, 2, 5</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Evaluating and understanding the robustness of adversarial logit pairing</title>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10272</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FDA: Feature Disruptive Attack</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2011">2015. 1, 2, 3, 4, 5, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Countering Adversarial Images using Input Transformations</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adversarial Machine Learning. In ACM Workshop on Security and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011">2011</date>
			<publisher>AISec</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research)</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial Machine Learning at Scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2006">2017. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/.4,5" />
		<title level="m">The mnist database of handwritten digits</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and blackbox attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsipras</forename><surname>Dimitris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2011">2018. 1, 2, 3, 4, 6, 11</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On Detecting Adversarial Perturbations</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Bischoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2007">2016. 1, 2, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalizable Data-Free Objective for Crafting Universal Adversarial Perturbations</title>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
				<imprint>
			<date type="published" when="2002">Oct. 2019. 2</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2452" to="2465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast Feature Fool: A data independent approach to universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utsav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babu</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
				<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Berkay</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia Conference on Computer and Communications Security</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04508</idno>
		<title level="m">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Certified Defenses against Adversarial Examples</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models</title>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ensemble Adversarial Training: Attacks and Defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2006">2018. 1, 2, 6</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kolter</forename><surname>Zico</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00851</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mitigating Adversarial Effects Through Randomization</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wide Residual Networks</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
				<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Theoretically Principled Trade-off between Robustness and Accuracy</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2006">2019. 1, 2, 6</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
