<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open Challenges in Modelling, Analysis and Synthesis of Human Behaviour in Human-Human and Human-Machine Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Alessandro</forename><surname>Vinciarelli</surname></persName>
							<email>alessandro.vinciarelli@glasgow.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Esposito</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Second University of Naples</orgName>
								<address>
									<settlement>Caserta</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elisabeth</forename><surname>Andre</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Augsburg</orgName>
								<address>
									<settlement>Augsburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesca</forename><surname>Bonin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Chetouani</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University Pierre et Marie Curie</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Cristani</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">University of Verona</orgName>
								<address>
									<settlement>Verona</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ferdinand</forename><surname>Fuhrmann</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">Joanneum Research</orgName>
								<address>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elmer</forename><surname>Gilmartin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zakia</forename><surname>Hammal</surname></persName>
							<affiliation key="aff8">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Heylen</surname></persName>
							<affiliation key="aff9">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<settlement>Enschede</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rene</forename><surname>Kaiser</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">Joanneum Research</orgName>
								<address>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>Koutsombogera</surname></persName>
							<affiliation key="aff10">
								<orgName type="department">Institute for Language and Speech Processing</orgName>
								<address>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">National Technical University of Athens</orgName>
								<address>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steve</forename><surname>Renals</surname></persName>
							<affiliation key="aff12">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
							<affiliation key="aff13">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Albert</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Salah</surname></persName>
							<affiliation key="aff14">
								<orgName type="institution">Bogazici University</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cogn</forename><surname>Comput</surname></persName>
						</author>
						<title level="a" type="main">Open Challenges in Modelling, Analysis and Synthesis of Human Behaviour in Human-Human and Human-Machine Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCD207B205ED513E5C95D183EAE5CBB2</idno>
					<idno type="DOI">10.1007/s12559-015-9326-z</idno>
					<note type="submission">Received: 17 February 2015 / Accepted: 9 March 2015 Ó Springer Science+Business Media New York 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human behaviour</term>
					<term>Social interactions</term>
					<term>Virtuous data practices</term>
					<term>Multimodal embodiment</term>
					<term>Cognitive modelling</term>
					<term>Semantic processing</term>
					<term>Roadmap to application</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modelling, analysis and synthesis of behaviour are the subject of major efforts in computing science, especially when it comes to technologies that make sense of human-human and human-machine interactions. This article outlines some of the most important issues that still need to be addressed to ensure substantial progress in the field, namely (1) development and adoption of virtuous data collection and sharing practices, (2) shift in the focus of interest from individuals to dyads and groups, (3) endowment of artificial agents with internal representations of users and context, (4) modelling of cognitive and semantic processes underlying social behaviour and (5) identification of application domains and strategies for moving from laboratory to the real-world products.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Modelling, analysis and synthesis of human behaviour are the subject of major efforts in computing science <ref type="bibr" target="#b137">[138,</ref><ref type="bibr" target="#b139">140]</ref>. In principle, the problem can be addressed in purely technological terms, i.e. by applying the same methodologies and approaches that can be used for any other type of data accessible to machines. For example, speech has been analysed using methodologies that can be applied to any other signal, and similarly, computer vision has addressed the problem of tracking people using the same methodologies that can be used to track any other moving object. Furthermore, robotics and computer graphics addressed the synthesis of human motion by simply reproducing its observable aspects.</p><p>However, human behaviour is governed by cognitive, social and psychological phenomena that, while not being observable, must be taken into account to build technologies more robust, effective and human-centred. The first attempts in this direction were done in the early 1990s, when automatic analysis and synthesis of facial expressions were addressed for the first time not only in terms of observable facial muscles activity, but also in terms of emotion expression <ref type="bibr" target="#b46">[47]</ref>. Interdisciplinary collaboration between computing on one side and, on the other side, psychology and cognitive sciences proved to be a crucial and fruitful milestone.</p><p>Nowadays, domains like affective computing <ref type="bibr" target="#b99">[100]</ref>, social signal processing (SSP) <ref type="bibr" target="#b137">[138]</ref>, social robotics <ref type="bibr" target="#b22">[23]</ref>, intelligent virtual agents <ref type="bibr" target="#b31">[32]</ref>, human communication synamics <ref type="bibr" target="#b86">[87]</ref>, sentic computing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b102">103]</ref> are well established and have a well-delimited and well-recognized scope in the computing community. Recent technological achievements include social robots that deal with autistic children <ref type="bibr" target="#b118">[119]</ref>, computers that make sense of human personality in various contexts <ref type="bibr" target="#b136">[137]</ref>, artificial agents that sustain emotionally rich conversations with their users <ref type="bibr" target="#b122">[123]</ref>, intelligent frameworks for multimodal affective interaction <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b104">105]</ref>, approaches that detect phenomena as subtle as mimicry <ref type="bibr" target="#b42">[43]</ref>, and the list could continue.</p><p>However, modelling, analysis and synthesis of human behaviour are far from being solved problems. This article outlines a few major issues that need to be addressed to substantially improve the current state of the art:</p><p>• Virtuous practices for design, collection and distribution of data. Without data it is difficult, if not impossible, to develop technologies revolving around human behaviour. However, widely shared practices for making of data an asset for the entire community are still missing (see Section ''The Data'') <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b72">73]</ref>. • From individuals to interaction. A group of interacting people is more than the mere sum of its members. However, most current analysis approaches still focus on individuals. Furthermore, methodologies addressing groups as a whole, especially when it comes to mutual influence processes, are still at an early stage of development (see Section ''Behaviour Analysis'') <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b59">60]</ref>. • From shallow to deep interactions. Human-human interactions take place in highly specific contexts where people typically have a long history of previous relations. However, current artificial agents typically miss an internal representation of both context and others, resulting in shallow interactions with their users. Attempts to go beyond such a state-of-affairs are still limited (see Section ''Multimodal Embodiment'') <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b117">118]</ref>. • Integration of semantic and cognitive aspects. Social life is determined to a large extent by unconscious, cognitive processes. However, most current approaches for analysis and synthesis of human behaviour do not try to model how people make sense of others and give meaning to their experiences (see Section ''Computational Models of Interaction'') <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b105">106]</ref>. • Applications. Real-world applications are the ultimate test bed for any technology expected to interact with humans. However, only a relatively few domains are seriously planning the adoption of technologies dealing with human behaviour (see Section ''Applications'') <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b133">134]</ref>.</p><p>The rest of the article describes each of the above issues in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Data</head><p>Corpora and data collections are a necessary prerequisite for modelling, analysis and synthesis of human behaviour.</p><p>In fact, analysis and synthesis are not possible without learning from data showing contexts and phenomena of interest. Furthermore, efficiency in experiments and replicability of results are difficult without a framework for comprehensive and easily interoperable data annotation and analysis. In other words, the multimodal research community cannot progress without virtuous data collection, annotation and sharing practices that make highquality data accessible and easy to process. This section outlines the challenges arising at various stages of corpus design, collection, annotation, curation and distribution and proposes strategies that should underpin the best practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Design and Collection</head><p>Collections of data portraying multimodal interaction behaviours cover a wide spectrum of verbal, non-verbal, social and communicative phenomena. However, most current resources do not address all aspects of social interactions, but focus on the investigation of specific contexts and settings. The probable reason is that the range of spoken interactions, or ''speech-exchange systems'' <ref type="bibr" target="#b115">[116]</ref>, humans engage in is enormous. Corpora may consist of audio-visual material gathered from conventional media (radio and television) and the web, or recordings made during laboratory experiments, possibly using advanced sensors (e.g. high-definition cameras, gaze trackers, microphone arrays, RGB depth cameras like the Kinect, physiological sensors). Overall, the large number of settings and data acquisition approaches reflects the wide variety of design and research goals that data collections are functional to <ref type="bibr" target="#b137">[138]</ref>.</p><p>When interactions are recorded in a laboratory setting, the most common way to ensure that people actually engage in social exchanges is to use tasks aimed at eliciting conversation. Typical cases include the description of routes on a map (e.g. the HCRC Map Task Corpus <ref type="bibr" target="#b4">[5]</ref>), spotting differences between similar pictures (e.g. the London UCL Clear Speech in Interaction Database <ref type="bibr" target="#b11">[12]</ref> and the Wildcat Corpus <ref type="bibr" target="#b134">[135]</ref>), participation in real or simulated professional meetings (e.g. the ICSI Meeting Corpus <ref type="bibr" target="#b67">[68]</ref> and the AMI Corpus <ref type="bibr" target="#b83">[84]</ref>). This way of collecting data produces useful corpora of non-scripted dialogues. However, it is unclear whether the motivation of subjects involved in an artificial task can be considered genuine. Therefore, it is not sure that the resulting corpora can be used to make reliable generalizations about natural conversations <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b91">92]</ref>.</p><p>Attempts to collect data in more naturalistic settings have focused initially on real-world phone calls like, e.g. the suicide hotline and emergency-line conversations described in <ref type="bibr" target="#b115">[116]</ref>. A broadest domain of topics is available in the corpora of real phone conversations (e.g. Switchboard <ref type="bibr" target="#b52">[53]</ref> and ESP-C <ref type="bibr" target="#b30">[31]</ref>). The main drawbacks of these resources are that they are unimodal, and furthermore, it is not clear whether phone-mediated and face-to-face conversations can be considered equivalent. The effort of capturing face-to-face, real-life spoken interactions has led to collection of corpora like Santa Barbara <ref type="bibr" target="#b44">[45]</ref>, ICE <ref type="bibr" target="#b54">[55]</ref>, BNC <ref type="bibr" target="#b15">[16]</ref> and Gothenburg Corpus <ref type="bibr" target="#b0">[1]</ref>. However, the effectiveness of these collections is still limited by unimodality (the only exception is the Gothenburg Corpus) and relatively low quality of the recordings.</p><p>What emerges from the above is that the collection of data suitable for multimodal research entails a trade-off between the pursuit of real-life, naturalistic resources and the need of high-quality material suitable for automatic processing. This typically leads to the choice of laboratory settings, where the sensing apparatus is as unobtrusive as possible and the scenario is carefully designed to avoid biases. This led to hybrid multimodal corpora showing encounters recorded in the laboratory, but without prescribed task or subject of discussion imposed on participants. These include collections of free-talk meetings or first encounters between strangers (e.g. the Swedish Spontal <ref type="bibr" target="#b45">[46]</ref>, the NOMCO and MOMCO Danish and Maltese corpora <ref type="bibr" target="#b94">[95]</ref>, casual conversations between acquaintances and strangers <ref type="bibr" target="#b92">[93]</ref>). Some of the latest corpora include physiological signals, motion capture information (e.g. DANS and Spontal <ref type="bibr" target="#b45">[46]</ref>) and breathing data.</p><p>The availability of new sensors, capable of capturing information non-accessible in previous corpora, makes old data less useful due to sparsity of the type of signals collected (many are audio only) and the impossibility of investigating the range of interconnected signals and cues of interest to current researchers. This issue could serve as a caution to current data collectors. It would be very useful if researchers future-proofed corpora by gathering a range of signals as wide as possible at the data collection stage, hopefully slowing the onset of data obsolescence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Annotation, Curation and Distribution</head><p>Creating recordings is becoming increasingly cheaper and easier, but annotating them in view of modelling, analysis and synthesis of social behaviour remains a time-consuming and labour-intensive task. In fact, enriching data with descriptive and semantic information is usually done manually. Recent advances in sensing technologies have introduced flexibility in automatically collecting features of interest enabling the creation of data sets rich with information on multimodal behaviour that can be further augmented with manual encodings. However, analysis and modelling of multimodal interaction are hampered by the lack of a comprehensive annotation scheme or taxonomy incorporating speech, gestures and other multimodal interaction features.</p><p>Many spoken dialogue annotation schemes are based on speech/dialogue acts and their function in updating dialogue state <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b74">75]</ref>. The ISO 24617-2 standard for functional dialogue annotation <ref type="bibr" target="#b64">[65]</ref> comprehensively covers information transfer and dialogue control/interaction management functions of utterances, but coverage of social or interactional functions is restricted to ''social obligation management'' (salutations, self-introduction, apologizing, thanking and valedictions). There is also a need to include annotation of multimodal cues. The MUMIN scheme <ref type="bibr" target="#b1">[2]</ref> allows coding of multimodal aspects of dialogue, particularly in terms of their contribution to interaction management and turn-taking, but has not yet been integrated into larger dialogue taxonomies. An important advantage of the ISO standard and indeed of the information state update paradigm <ref type="bibr" target="#b19">[20]</ref> is its multidimensionality, whereby a ''markable'' or ''area of interest'' can be tagged in several orthogonal ways. This scheme may thus be extensible to account for many interactional and multimodal aspects of interaction. A more extensive taxonomy of communicative acts encompassing various modalities is highly desirable.</p><p>While many databases are publicly available, many others are still not shared. The shortage of desirable annotated data is also due to the lack of standardization, intellectual property rights (IPR) restrictions and privacy issues arising from research ethics. Data sets involved in tasks related to human behaviour analysis come with strict terms of use. Providers of data should thus ensure that data reuse is permitted through a set of appropriate licensing conditions. More importantly, data sets should be indexed so that all interested parties are able to identify different types of resources they wish to access and/or acquire. The multidisciplinarity of the field also calls for true and continuous cooperation among disciplines to make the most of complementary expertise in resource development and processing <ref type="bibr" target="#b125">[126]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Issues and Challenges</head><p>Methodologies aimed at creation and dissemination of data should be fostered by both users and providers to maximize availability and usability. The goal should be the creation of data ecosystems that support the whole multimodal value chain-from design to distribution-through definition of best practices (e.g. like those available in natural language processing) and set-up of infrastructures for resource use and sharing <ref type="bibr" target="#b101">[102]</ref>. These infrastructures will address the following needs:</p><p>• A framework for managing and sharing data collections; • Legal and technical solutions for privacy protection in a number of use scenarios; • Data visibility and encouragement to data sharing, reuse and repurposing for new research questions; • Identification of gaps and missing resources.</p><p>Establishing such an ecosystem in the area of multimodal interaction is necessary to support the increasingly demanding requirements of real-world applications. In particular, the creation of an effective data ecosystem promises to have the following advantages:</p><p>• Integration of social and multimodal annotation into standard dialogue annotation schemes; • Building of knowledge bases informing the design of real-world and impact-oriented applications; • Coverage of a wide, possibly exhaustive spectrum of contexts and situations; • Better analysis of context and genre in social interactions.</p><p>Overall, a solid shared data ecosystem would greatly streamline the acquisition of relevant scientific understanding of multimodal interaction, and thus expedite the use of this knowledge in the research and development of a range of novel real-world applications (see Section ''Applications''). The challenge remains at defining, labelling and annotating the high-level behaviours associated with human-human interaction. For this purpose, experts in multimodal signal processing and machine learning work hand-in-hand with psychologists, clinicians and other domain experts to transfer knowledge gained over years of labelling human behaviours to a machine readable code that is amenable to computational manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behaviour Analysis</head><p>Previous research on social behaviour analysis has focused on individuals, whether it comes to the detection of specific actions and cues (e.g. facial expressions, gestures and prosody) or the measurement of social and psychological phenomena (e.g. valence and arousal and personality traits). With advances in methodology, there is increasing interest in advancing beyond action detection in individuals to detection and understanding of interpersonal influence.</p><p>Recent work includes comparing patterns of interpersonal influence under different conditions (e.g. with or without visual feedback, during high versus low conflict, and during negative and positive affect <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b135">136]</ref>) and the relation between interpersonal influence and outcome variables (e.g. friendship or relationship quality <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b106">107]</ref>). Key issues are feature extraction and representation, timeseries methodologies and outcomes. Unless otherwise noted, the rest of this section focuses on dyads (i.e. two interacting individuals) rather than larger social groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection of Behavioural Cues</head><p>The first step in computing interpersonal influence is to extract and represent relevant behavioural features from one or more modalities. Methodologies include motiontracking <ref type="bibr" target="#b8">[9]</ref> for body motion, computer vision <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b135">136]</ref> for facial expression, head motion and other visual displays, signal processing <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b135">136]</ref> for voice quality, timing and speech, and manual measurements by human observers <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b80">81]</ref>. Because of their objectivity, quantitative measurement, efficiency and reproducibility, automatic measures are desirable. We address their limitations and challenges in Section ''Open Issues and Challenges''.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modelling Interpersonal Influence</head><p>Independent of specific methods of feature extraction, two main approaches have been used to analyse interpersonal influence. The first includes analytic and descriptive models that seek to quantify the extent to which behaviour of an individual accounts for the behaviour of another. The second includes prediction and classification models that seek to measure behavioural matching between interactive partners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analytic/Descriptive Models</head><p>Windowed cross-correlation is one of the most commonly used measures of similarity between two time series <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b106">107]</ref>. It uses a temporally defined window to measure successive lead-lag relationships over relatively brief timescales <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b84">85]</ref>. By using small window sizes, assumptions of stationarity are less likely violated. When time series are highly correlated at zero lag, they are said to be synchronous. When they are highly correlated at negative or positive lags, reciprocity is indicated. Patterns of cross-correlation may change across multiple windows, consistently with descriptions of mismatch and repair processes (e.g. in mother-infant dyads <ref type="bibr" target="#b34">[35]</ref>).</p><p>Other approaches are recurrence analysis, accommodation and spectral analysis. Recurrence analysis <ref type="bibr" target="#b113">[114]</ref> seeks to detect similar patterns of change or movement in time series, which are referred to as ''recurrence points''. Accommodation <ref type="bibr" target="#b121">[122]</ref>, also referred to as convergence, entrainment or mimicry <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b98">99]</ref>, refers to the tendency of dyadic partners to adapt their communicative behaviour to each other. Accommodation is based on a time-aligned moving average between time series. Spectral methods are particularly suitable for rhythmic processes. Spectral analysis measures phase shifts <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b113">114]</ref> and coherence <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b113">114]</ref> or power spectrum overlap <ref type="bibr" target="#b93">[94]</ref>.</p><p>These methods may suggest that one of the interaction participants influences the other (e.g. infant smile in response to the mother's smile), but it is more rigorous to say that they detect co-occurrence patterns that do not necessarily correspond to causal or influence relationships. Correlation or co-occurrence across multiple time series might be due to chance.</p><p>A critical issue when attempting to detect dependence between time series is to rule out random cross-correlation or random cross-phase coherence. Two types of approaches may be considered. One of the most common is to apply surrogate statistical tests <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b112">113]</ref>. For instance, the time series may be randomized. Statistics that summarize the relation between time series (e.g. correlations) then can be compared between the original and randomized series. If the statistics differ between the original and randomized series, that suggests a non-random explanation. Alternatively, time-and frequency-domain time-series approaches have been proposed to address this problem <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b77">78]</ref>.</p><p>Surrogate and time-series approaches both involve analysis of observational measures. Yet another approach is to introduce experimental perturbations into naturally occurring behaviour. In a videoconference, the output of one person's behaviour may be processed using an active appearance model and modified in real time without their knowledge. Using this approach, it has been found that attenuated head nods in an avatar resulted in increased head nods and lateral head turns in the other person <ref type="bibr" target="#b17">[18]</ref>. Recent advances in image processing make possible real-time experimental paradigms to investigate the direction of effects in interpersonal influence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction/Classification Models</head><p>In many applications, it is of interest to detect moments of similar behaviour between partners. For example, smiles in interactions between mothers and infants could be learned, and then their joint occurrence detected automatically. Mutual or synchronous head nodding, as in back-channelling, would be another example. A method to detect joint states using semi-supervised learning was proposed in <ref type="bibr" target="#b140">[141]</ref>. Similarly, one could use supervised or unsupervised methods to learn phase relations between partners. This would include coordinated increasing or decreasing intensity of positive affect or mimicry. In <ref type="bibr" target="#b116">[117]</ref>, Hidden Markov Models (bi-grams) are employed to learn parentinfant interaction dynamics. This modelling is coupled with non-negative matrix factorization for the extraction of a social signature of typical and autistic children. In <ref type="bibr" target="#b43">[44]</ref>, a set of one-class SVM-based models are used to recognize the gestures of task partners during EEG hyper-scanning. A measure of ''imitation'' is then derived from the likelihood ratio between the models.</p><p>To reveal bidirectional feedback effects, parametric approaches such as actor-partner analysis have been proposed <ref type="bibr" target="#b71">[72]</ref>. In actor-partner analysis, data are analysed while taking into account both participants in the dyad simultaneously. As an example, Hammal et al. <ref type="bibr" target="#b59">[60]</ref> used actor-partner analysis to measure the reciprocal relationship between head movements of intimate partners in conflict and non-conflict interaction. Each participant's head movement was used as both predictor and outcome variable in the analyses. The pattern of mutual influence varied markedly depending on conflict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Issues and Challenges</head><p>Critical challenges are access to well-annotated data from dyads or other social groups (see Section ''The Data''), further advances in automated measurement and improved analysis methodologies (see Section ''Computational Models of Interaction''). Because the distribution of spontaneous social interaction data has been constrained by confidentiality restrictions, investigators have been unable to train on and analyse each other's data. That limits advances in our methods. Often, however, participants would be agreeable to sharing their audio-video data if only asked. When participants have been given the opportunity to consent to such use by the research community, they often have consented. This has encouraged efforts to open access to data sources that would have been unavailable in the past (see Section ''The Data''). The US National Institutes of Health <ref type="bibr" target="#b84">[85]</ref> among others supports data-sharing efforts.</p><p>The current state of automated measurement presents limits. First, automatic feature extraction typically results in moderate rates of missing data, such as when head rotation exceeds the operational parameters of the system or face occlusion occurs. This is particularly germane when applying algorithms to participants much different than ones on which they were trained <ref type="bibr" target="#b57">[58]</ref>. Second, while communication is multimodal, automated feature extraction typically is limited to one or few modalities. Despite advances in natural language processing (NLP) <ref type="bibr" target="#b124">[125]</ref>, sampling and integration of speech with non-verbal measures remains a challenge. Third, optimal approaches to multimodal fusion are an open research question and may hinge on specific applications. In manual measurement, coders often use multimodal descriptors <ref type="bibr" target="#b34">[35]</ref>. Comparable descriptors for automated feature extraction have yet to appear. In part for these reasons, some investigators have considered a combination of automatic and manual measurement <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b59">60]</ref> or combination of overlapping algorithms for feature extraction <ref type="bibr" target="#b96">[97]</ref>.</p><p>A further key challenge is to propose statistical and computational approaches suitable for content and temporal structure of dyadic interactions. Various sequential learning models, such as hidden Markov models (HMMs) or conditional random fields (CRFs), are typically used to characterize the temporal structure of social interactions. Further approaches of this type will be of great benefit for automatic analysis and understanding of interpersonal communication in social interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal Embodiment</head><p>In the past ten years, significant amount of effort has been dedicated to exploring the potential of social signal processing in human interaction with embodied conversational agents and social robots. The social interaction capability of an artificial agent may be defined as the ability of a system to interact seamlessly with humans. This definition implies the following:</p><p>• The human produces and expects responses to social signals in the communication with the agent; • The agent not only is perceptive to the social signals emitted by the human, but also uses social signals to further its own purposes.</p><p>Particularly, the latter point implies a rich internal representation of humans and human-human interactions for the agent.</p><p>Needless to say, specific aspects of embodied social interaction cannot be studied under laboratory conditions alone; naturalistic social settings and people's daily environments are needed to situate the user-agent communication (see Section ''The Data'' for issues related to the collection of data in naturalistic settings). Looking at the recent literature, current goals in multimodal embodied interaction are focused on implementing sets of social communication skills in the agent, including detection of humour, empathy, compassion and affect <ref type="bibr" target="#b139">[140]</ref>. Basic skills like facial emotion recognition, gaze detection, dialogue management and non-verbal signal processing are still far from being effective. Similarly, synthesis and timing of non-verbal signals and appropriate ways of signalling apparent social cues are studied. This section identifies two major challenges in this area. The first is that in these studies, typically, the cultural context is held fixed. One may argue that even humans have troubles selecting correct responses when the cultural setting is not familiar, but studies on artificial agents typically take place in very restricted domains, and naturalistic contexts are absent. The second problem is that the social behaviour of the agent is often not grounded in a rich internal representation and lacks depth <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b117">118]</ref>. When an agent shows signs of enjoying humour, it does that according to an internal rule triggered to display amusement as the appropriate response to a certain number of interactional situations. This way of modelling social exchanges is very rudimentary, and while it can be the initial step for implementing a social agent, it is very far from implementing the complexity and richness of social communications in real life. The two issues mentioned above are strongly connected; without a proper internal representation, shallow models cannot be expected to adapt to different social contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approaches to Multimodal Embodiment</head><p>Social signals are strongly contextualized. For example, in a situation of bereavement, a gesture that is performed close to the interacting party can easily be interpreted as showing sympathy. The same gesture could be entirely inappropriate in a different context. The interpretation of social signals depends not only on the correct perception and categorization of the signal, but also on the evaluation and active interpretation of the interacting parties. While humans are adept at this, artificial agents lack the semantic background knowledge to deal with subtleties. Subsequently, the human-agent interaction needs to assume that the technology is limited and compensates for its shortcomings by structuring the interaction in a way that the exchange follows signals that are clear and simple, tailored to the capabilities of the agent, but still rich enough to convey the internal states of the agent to the human and vice versa.</p><p>Technologies for realizing individual components of a social agent have reached a great level of advancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cogn Comput</head><p>International benchmarking campaigns, such as the series of the Audio-Visual Emotion Challenges (AVEC) <ref type="foot" target="#foot_1">1</ref> , have considerably fostered progress in the area of social signal interpretation. This is important for artificial agents that need to understand their users in naturalistic settings, but it is obvious that human-agent interactions do not necessarily need to use the same signals as those used in humanhuman interaction. The archetypical example is a domesticated cat, which produces a different set of social signals than a human, but seamlessly communicates over this set. The contribution of benchmarking campaigns is essential to the development of new solutions. Realistic data, naturalistic behaviours and real-time processing are key aspects for these campaigns. The latter aspect is particularly important, as most challenges focus on offline processing, but the online mode, which is essential for real, situated social interactions, is a much more difficult setting <ref type="bibr" target="#b119">[120]</ref>. Candace Sidner and Charles Rich <ref type="bibr" target="#b111">[112]</ref> coined the term always-on relational agents to describe the vision of a robotic or virtual character that lives as a permanent member in a human household, which remains a grand challenge. In a related perspective, Barbara Grosz <ref type="bibr" target="#b55">[56]</ref> stated that: ''Is it imaginable that a computer (agent) team member could behave, over the long term and in uncertain, dynamic environments, in such a way that people on the team will not notice it is not human''. The perception, negotiation and generation of social cues in a context is necessary to achieve this condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Issues and Challenges</head><p>As tools become more diversified and layered, it becomes possible to create agents with more depth. Work done in the SEMAINE project <ref type="bibr" target="#b122">[123]</ref> has shown that simple backchannel signals, such as ''I see'', may suffice to create the illusion of a sensitive listener. However, to engage humans over a longer period of time, a deeper understanding of the dialogue would be necessary. While a significant amount of work has been done on the semantic/ pragmatic processing in the area of NLP, work that accounts for a close interaction between the communication streams required for semantic/pragmatic processing and social signal processing is rare (see Section ''Computational Models of Interaction''). The integration of social signal processing with semantic and pragmatic analysis may help to resolve ambiguities. Particularly, short utterances tend to be highly ambiguous when solely the linguistic data are considered. An utterance like ''right'' may be interpreted as a confirmation, as well as a rejection, if intended cynically, and so may the absence of an utterance. Preliminary studies have shown that the consideration of social cues may help to improve the robustness of semantic and pragmatic analysis <ref type="bibr" target="#b20">[21]</ref> (see Section ''Computational Models of Interaction'').</p><p>Finding the right level of sensitivity is very important in creating seamless interaction, and this requires strong adaptation skills for the agent. Mike Mozer's <ref type="bibr" target="#b88">[89]</ref> early experiments on the adaptive neural network house established that people tolerate only to a limited extent the mistakes of an ''intelligent'' system. This is true for social signals as well; agents that act and react inappropriately will most likely irritate users <ref type="bibr" target="#b5">[6]</ref>. Treating all user behaviours as possible input to the agent (called the ''Midas Touch Problem'' <ref type="bibr" target="#b60">[61]</ref>) will result in poor interactions and confused users.</p><p>Recent work in the framework of the ''Natural Interaction with Social Robots Topic Group''<ref type="foot" target="#foot_2">2</ref> (NISR-TG) proposes to use several levels to describe the social ability of an agent:</p><p>• Level 0: The agent does not interact with the human;</p><p>• Level 1: The agent perceives the human as an object (useful for orienting and navigating); • Level 2: The agent perceives the human as another agent that is represented explicitly and can be reidentified time and again; • Level 3: A two-way interaction is possible provided that the interacting human knows and obeys some conventions and behaviours required by the agent's system; • Level 3a: A two-way interaction is possible with the ability of spoken language interaction; • Level 4: The agent adapts its behaviour to the interaction partners during the interaction; • Level 5: The agent recognizes different users and adjusts its behaviour accordingly; • Level 6: The agent is capable to interact with more than one users; • Level 7: The agent is endowed with personality traits that can be recognized as such by the users and results in displaying different behaviours in the same situations; • Level 8: The agent is capable to learn and accumulate experience over multiple interactions; • Level 9: The agent is capable to build and sustain relationships with its users.</p><p>Progressing through the levels, the agent is expected to gain one-way and two-way interaction capabilities, followed by a more advanced set of skills including adaptation, multiparty interaction management and the incorporation of social constructs like personality.</p><p>Humans adapt their social behaviours during interactions based on explicit or implicit cues they receive from the interlocutor. In order to establish longer-lasting relationships between artificial companions and human users, artificial companions need to be able to adjust their behaviour on the basis of previous interactions. That is, they should remember previous interactions and learn from them <ref type="bibr" target="#b9">[10]</ref>. To this end, sophisticated mechanisms for the simulation of self-regulatory social behaviours will be required. Furthermore, social interactions will have to be personalized to individuals of different gender, personality and cultural background. For example, cultural norms and values determine whether it is appropriate to show emotions in a particular situation <ref type="bibr" target="#b82">[83]</ref> and how they are interpreted by others <ref type="bibr" target="#b81">[82]</ref>. While offline learning is prevalent in current systems exploiting SSP techniques, future work should explore the potential of online learning in order to enable continuous social adaptation processes. For the integration of context, novel sensor technologies can be used by the agent in ways that are not available to humans in an ordinary interaction <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b138">139]</ref>. Multimodality can also be harnessed in expressing social signals in novel ways, for instance, by adding haptic cues to visual displays <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>At a finer level, a single interaction between two agents also involves an interactive alignment (also see Section ''Behaviour Analysis''), where the interacting parties converge on similar representations at different levels of linguistic processing <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b100">101]</ref>. The alignment at higher levels (e.g. common goals) relies on the alignment of lower levels (e.g. objects of joint attention). This requires that the agents model their interaction partners, anticipate interaction directions, align their communication acts, as well as actions <ref type="bibr" target="#b114">[115]</ref>. We can safely assume that research in cognitive science and linguistics will be essential in achieving these goals (see Section ''Computational Models of Interaction'').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Models of Interaction</head><p>Broadly based on the work of Tomasello <ref type="bibr" target="#b129">[130,</ref><ref type="bibr" target="#b130">131]</ref> (and others) human-human interaction can be represented as a three-step process: sharing attention, establishing common ground and forming shared goals (a.k.a. joint intentionality). Two prerequisites for successful human-human communication via joint intentionality are:</p><p>• The ability to form a successful model of the cognitive state of people around us, i.e. decoding not only overt, but also covert communication signals also referred to as ''recursive mind-reading''; • Establishing and building trust, a truly human trait.</p><p>Affective computing, SSP and behavioural signal processing (BSP) address the first prerequisite, building machines that can understand the emotional, social and cognitive state of an individual. A layered view of humanmachine interaction from the cognitive and computational perspectives are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. This section reviews computational models and associated challenges for each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Attention and Saliency</head><p>Unlike computers, humans are able to process only the most salient parts of an image, a sound or a brochure, literally ignoring the rest. Being able to model and predict what a human sees and hears in an audio-visual scene is the first step towards forming a cognitive representation of that scene, as well as establishing common ground in interaction scenarios.</p><p>Saliency-and attention-based models have played a significant role in multimedia processing in the past decade <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b127">128]</ref>, exploiting low-level cues from the (mostly) visual, audio and spoken language (transcription) modalities: they have proved very successful in identifying salient events in multimedia for a variety of applications. However, attention-based algorithms typically use only perceptually motivated low-level (framebased) features and employ no high-level semantic information, with few exceptions in very specific cases (mostly in the visual domain) <ref type="bibr" target="#b89">[90]</ref>.</p><p>Challenges still remain on: (1) mid-and high-level feature extraction including incorporating semantics (scenes, objects, actions) and (2) computational models for the multimodal fusion of the bottom-up (gestalt-based) and top-down (semantic-based) attentional mechanisms. Also applying these multimodal salient models to realistic human-human (especially) and human-computer interaction scenarios remains a challenge. The most promising research direction for these challenges seems to be deep learning, where the integration among levels of diverse granularity of knowledge is the core skill <ref type="bibr" target="#b13">[14]</ref>. Finally, identifying the dynamics of attention, i.e. constructing joint (interactional) attention models, remains an open problem in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common Ground and Concept Representations</head><p>While interacting, humans process and disambiguate multimodal cues, integrating low-, mid-and high-level cognitive functions, specifically using the low-level machinery of cue selection (as discussed above) via (joint) attention, the mid-level machinery of semantic disambiguation via common ground and shared conceptual representations and the high-level machinery of intention awareness <ref type="bibr" target="#b61">[62]</ref>.</p><p>Since establishing common ground is a prerequisite for successful communication, an essential module of an ideal interacting machine should model an extensive cognitive semantic/pragmatic representation, that is, a network of concepts and their relations that form the very essence of common ground, and in this sense formal ontologies may help.</p><p>Formal ontologies <ref type="bibr" target="#b56">[57]</ref> are a top-down (knowledgebased) semantic representation that has been used for interaction modelling mainly by the research community, e.g. <ref type="bibr" target="#b141">[142]</ref>. The main advantages of ontologies are description clarity (via mathematical logic) and inference power. However, the following challenges remain to make ontologies a viable representation for practical interactional systems:</p><p>• Mapping between the semantic and lexical/surface representations, a.k.a. the ''lexicalization'' of ontologies necessary both for natural language (NL) understanding <ref type="bibr" target="#b33">[34]</ref> and for NL generation <ref type="bibr" target="#b7">[8]</ref>; the same problem holds also in the visual domain, where a proper ''visual ontology'' is missing or available in very restricted domains <ref type="bibr" target="#b131">[132]</ref>; • Representing ambiguous semantics <ref type="bibr" target="#b3">[4]</ref>; • Representing complex semantics, e.g. time relationships <ref type="bibr" target="#b12">[13]</ref>; • Combining ontology-driven semantics with bottom-up (data-driven) approaches, e.g. for grammar induction <ref type="bibr" target="#b51">[52]</ref> and in general for computer vision.</p><p>Grounding exists only in the context of our semantic, affective and interactional cognitive representations and should be addressed as such. This poses the grand challenge of using ''big data'' to construct such cognitive representations, as well as defining the ''topology'' (unified vs. distributed) and processing logic (parallel/serial) of these representations. Cognitively motivated conceptual representations, e.g. common-sense reasoning <ref type="bibr" target="#b103">[104]</ref> and transfer learning <ref type="bibr" target="#b95">[96]</ref>, and novel machine learning algorithms, e.g. extreme learning machines <ref type="bibr" target="#b24">[25]</ref> (1) achieve rapid learning and adaptation to new concepts and situations from very few examples (situational learning and understanding) and ( <ref type="formula">2</ref>) provide grounding in interaction and problem-solving settings (negotiating common ground).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From Semantics to Behaviour and Interaction</head><p>Even if it was possible to solve the multimodal understanding problem by mapping from signal(s) to semantics (a monumental task by itself), it would still be only half of the way. Assuming that a conceptual representation is in place (see above), this section discusses how to model jointly semantics and affect. Given that the cognitive semantic space is both distributed and fragmented into subspaces, the mapping from semantics to affective labels should also be distributed and fragmented. Semantic-affective models (SAM) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b132">133]</ref> are based on the assumption that semantic similarity implies affective similarity. Thus, affective models can be simply constructed as mappings from semantic neighbourhoods to affective scores. In the SAM model proposed in <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref>, the affective label of a token can be expressed as a map (trainable linear combination) of its semantic similarities to a set of seed words and the affective ratings of these words. The model can be extended to also handle many-to-many mappings between multiple layers of cognitive representations. The model is consistent with (and implementable via) the multilayered cognitive view of representation and deep learning models. Although good performance can be obtained for language-and image-processing applications, the challenge remains on how to apply this model to audio and video, where the segmentation of the stream into tokens is not straightforward. Also, the model works very well at estimating the affective content of single tokens (words, images); going from a single token to a sequence of tokens (e.g. word to sentences) is a hard open problem. Last but not least, generalizing this model to other behavioural labels remains a grand challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Issues and Challenges</head><p>The previous sections have identified major challenges that lie ahead in the fields of affective, social and behavioural signal processing as it pertains to interaction modelling. The sections have also argued that it is very improbable that one can successfully address these major challenges without taking into account the peculiarities of human cognition.</p><p>The proposition of this paper is that the solution of these problems should be grounded on human cognition, including modelling the errors (cognitive biases) and nonlinear logic of humans <ref type="bibr" target="#b110">[111]</ref>. Although ''pure'' machine learning algorithms often achieve good performance for classification of low-and mid-level labels, they are less successful with higher-level behavioural classification tasks. This can be partially attributed to the ambiguity, abstraction, subjectivity and representation depth inherent in high-level cognitive tasks. Cognitively inspired models can represent the very errors, biases, subjective beliefs and attitudes of a human. Thus, adopting a human-centred approach becomes increasingly important as we move from signals to behaviours and interaction. The recent achievements of cognitively motivated machine learning paradigms such as representation, transfer and deep learning further validate this view. Interaction modelling poses new challenges and opens up fruitful research directions for the years to come.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications</head><p>Effectiveness in real-world applications is the ultimate test for any technology-oriented research effort. While being an opportunity for methodological progress and acquisition of key insights about human psychology and cognition, research on modelling, analysis and synthesis of human behaviour aims at achieving impact in terms of both commercial exploitation, i.e. development of products that reach the market and result into jobs creation, and solutions to societal problems, i.e. development of systems that improve the quality of life, especially when it comes to disadvantaged categories.</p><p>Addressing the issues and challenges presented in this work will certainly advance the state of the art, but it will increase the chances of success for a wide spectrum of realworld technologies as well (the list is not exhaustive):</p><p>• Analysis of agent-customer interactions at call centres <ref type="foot" target="#foot_3">3</ref>with the goal of improving the quality of services <ref type="bibr" target="#b49">[50]</ref>; • Improvement of tutoring systems aimed at supporting students in individual and collective learning processes <ref type="bibr" target="#b120">[121]</ref>; • Creation of speech synthesizers<ref type="foot" target="#foot_4">4</ref> that convey both verbal and non-verbal aspects of a text <ref type="bibr" target="#b123">[124]</ref>; • Enrichment of multimedia indexing systems with social and affective information <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>; • Recommendation systems that take into account stable individual characteristics (e.g. personality traits) and transient states (e.g. emotions) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b128">129]</ref>; • Socially intelligent surveillance and monitoring systems <ref type="bibr" target="#b39">[40]</ref>.</p><p>The rest of this section focuses on three application domains that address crucial issues and aspects of everyday life, namely health care, human-machine interactions and human-human conversations. The three cases account for three major steps in the process that leads from the laboratory to the real world:</p><p>• The development of a vision based on current state of the art and major technological trends in the case of healthcare personal agents (see Section ''The Healthcare Personal Agent: A Vision for the Future of Medicine''); • The realization of a prototype that addresses one specific application (intelligent control centres), but results into the definition of principles that can be transferred to other areas (see Section ''Building a Working Prototype: The Example of Intelligent Control Centres''); • The definition of concrete steps bridging the gap between research, industry and society in the case of conversational technologies (see Section ''Roadmapping Research and Innovation in Conversational Interaction Technologies'').</p><p>The description of the case studies above will provide insights regarding the interdependency between the challenges outlined so far and application-driven needs.</p><p>The Healthcare Personal Agent: A Vision for the Future of Medicine Advances in mobile technologies, such as voice, video, touch screens, web 2.0 capabilities and integration of various on-board sensors and wearable computers, have rendered mobile devices as ideal units for delivery of healthcare services <ref type="bibr" target="#b10">[11]</ref>. At the same time, the dawn of the data-driven economy has stirred the innovation of processes and products. Unfortunately, the innovation has been slow in the healthcare sector where much innovation is needed to improve the quality of the service at various end-points (hospitals, healthcare professionals, patients) and reduce costs. The 2012 survey in <ref type="bibr" target="#b90">[91]</ref> reports that in Europe, there were more than one hundred health apps in a variety of languages (Turkish, Italian, Swedish, etc...) and domains (mental problems, self-diagnosis, heart-monitoring, etc.). Such growing number of smartphone applications can track user activity, sleeping and eating habits and covert and overt signals such as blood pressure, heart rate, skin temperature, speech, location, movement by either using the on-board sensors of the smartphone or interacting with various wearable and healthcare monitoring devices.</p><p>In the recent years, there has been a growing research interest in creating such applications which can interact with people though context-aware multimodal interfaces and have been used for various healthcare services ranging from monitoring and accompanying the elderly <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b85">86]</ref> to real-time measuring of healthcare quality <ref type="bibr" target="#b28">[29]</ref> and providing healthcare interventions for long-term behaviour changes <ref type="bibr" target="#b87">[88]</ref>.</p><p>Such agents can be useful in keeping track of patient activity in-between visits or to ensure the patients are taking their medicines on time, or that they follow their advised health routine (see Section ''Multimodal Embodiment'' for challenges related to ''always on'' agents).</p><p>In the future, healthcare personal agent research and development should plan for an agenda where current limitations are addressed and new avenues are explored. Such agenda can directly impact the quality of life and health of people by disrupting current models of delivering healthcare services. Agents will have different physical and virtual appearance (see Section ''Multimodal Embodiment'' for challenges in embodiment) ranging from avatars to robots (e.g. <ref type="bibr" target="#b85">[86]</ref>). Covert signal streams from wearable and mobile sensors may be effectively used to model user state in terms of his/her physiological responses to external stimuli, events and medical protocol he/she is following (see Section ''Behaviour Analysis'' for challenges related to behaviour analysis).</p><p>Personal agents need to be able to handle basic and complex emotions such as empathy. In the healthcare domain, the ability to handle emotions is critical to manage and support, for instance, daily healthcare routine. The affective signals and communication need to be adapted for target patient groups such as children, elderly people. By far one of the most important social and cognitive skills of a conversational agent is the ability to carry out a dialogue with a human (see Section ''Roadmapping Research and Innovation in Conversational Interaction Technologies''). Different models of user interaction might be needed for different users/user groups and different application domains (e.g. robotic surgery vs. bank fund transfer vs information seeking). An application tracking brushing habits of kids might achieve better results with gamification, while an obesity monitoring agent should use motivational feedback to improve user compliance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building a Working Prototype: The Example of Intelligent Control Centres</head><p>Human-computer interaction is one of the domains that directly benefit from multimodal technologies for human behaviour understanding. This applies in particular to applications where machines must adapt as intelligent as possible to the natural and spontaneous behaviour of their users because these need to concentrate their attention and cognitive efforts on difficult and demanding tasks.</p><p>Reducing the cognitive load and enabling immediate reaction to alarms in idle times are key requirements that have driven the development of the innovative control centre described in <ref type="bibr" target="#b68">[69]</ref>. Comparable efforts on concrete applications have worked on ship bridges <ref type="bibr" target="#b73">[74]</ref> and crises response control rooms <ref type="bibr">[64]</ref>.</p><p>In control centres, teams of human operators collaborate to monitor and manipulate external processes, such as in industrial production, IT and telecommunication infrastructure, or public infrastructure such as transportation networks and tunnels. In this domain, innovation towards user interfaces has been picked up slowly since it is limited by governmental regulation or short-term return-on-investment considerations. Surprisingly, many of the systems in use were first built decades ago and have been extended iteratively without proper redesign of their user interfaces until today. Recent generations of operators, however, are digital natives and hence familiar with mobile devices, gesture interfaces and touch screens, for example. While considerable business opportunities can be expected in the next decade to redesign the interfaces in such control rooms, many research challenges remain to be addressed.</p><p>Most current systems feature redundant input devices, little context awareness, and expose operators to information overflow. The support for distribution of tasks and collaboration in general leaves to be desired. One key enabling factor in the redesign of such complex systems is the Cogn Comput dynamic interpretation of the operators' actions and interactions as a team while taking the current situation (goal, alarm and stress level, etc.) into account (see Section ''Behaviour Analysis'' for the challenges related to understanding the behaviour of groups). Inspired by a human-centred design approach, the concept recently proposed in <ref type="bibr" target="#b68">[69]</ref> experiments with the combination of visual cues, micro (i.e. fingers and hands only) and macro gestural interaction, an acoustic interface with individualized sound radiation, and intelligent data processing (semantic lifting, see <ref type="bibr" target="#b69">[70]</ref>) into a single, universal interface. The concept is considering specific needs of the operators and the length of work shifts, which, for example, led to the omission of wearable devices such as headsets. Figure <ref type="figure" target="#fig_1">2</ref> illustrates several components of this multimodal interaction concept <ref type="bibr" target="#b68">[69]</ref>. The work made clear that while research has been addressing the combination of input and output devices of multiple modalities, a lot more applied research is required on their interplay regarding specific tasks in real industry settings.</p><p>In a safety critical environment, user interaction requires different levels of robustness and precision according to the tasks. Control centre operators conduct very specific tasks that call for different interaction devices and concepts. Their integration and dynamic adaptation is a challenge. An underlying aim is to actively manage the cognitive load of the operators, mainly to ensure quick reaction in alarm situations. There are idle times where operators essentially take a break but do not leave their workplace, lengthy passive monitoring tasks and very urgent alarm handling situations. A significant impact can be expected in this domain by improved user behaviour analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roadmapping Research and Innovation in Conversational Interaction Technologies</head><p>The research community in multimodal conversational interaction has advanced significantly in recent years; however-despite the fast growth of multimodal smartphone technologies, for example-innovation and commercial exploitation are not always closely connected to research advances. To develop and integrate research and innovation in this area, it is thus important to identify the key innovation drivers and most promising elements across science, technology, products and services on which to focus in the future.</p><p>Technology roadmapping is a process to lay out a path from science and technology development through integrated demonstration to products and services that address business opportunities and societal needs. Often performed by individual businesses, it can also be used to put together all of the different viewpoints and information sources available in a large stakeholder community as a way of helping them work together and achieve more. The EU ROCKIT project, driven by a broad vision for conversational interaction technologies, has constructed a technology roadmap for conversational interaction technologies (http://www.citia.eu).</p><p>In consultation with researchers and companies of every size (including several workshops involving about 100 researchers and technologists), the ROCKIT support action constructed a technology roadmap for conversational interaction technologies. Since research and business environments can change rapidly, the resultant roadmap is structured to enable stakeholders to steer through change and understand how they can achieve their goals in a changing context. For this reason, the roadmap is not just a series of steps that go from current science and technology outcomes to future profitable products and services, but conveys the relationships among societal drivers of change, products and services, use cases for them and research results.</p><p>The ROCKIT roadmap connects the strong research base with commercial and industrial activity and with policy makers. To develop the roadmap, and to make tangible links between research and innovation, a small number of target scenarios have been developed. Each scenario includes its societal and technological drivers, research aspects, market and business drivers and potential test beds. We identified a number of common themes coming out of ROCKIT's consultations with stakeholders, in particular accessibility, multilinguality, the importance of design, privacy by design, systems for all of humanhuman, human-machine and human-environment interactions, robustness, security, potentially ephemeral interactions and using the technology to enable fun.</p><p>Building on these themes, together with the different social, commercial and technological drivers, we have identified five possible target application scenarios:</p><p>• Adaptable interfaces for all: Interfaces which recognize who you are, where you are and eventually what you want, by drawing on a profiled knowledge base about your habits and preferences. They will therefore be able to adapt to your disability, language, visual competency, specific need for speech or typed input depending on whether you are driving/working with two hands on a repair job or are seated in front of a keyboard, physical or virtual, or are prostrate in bed (see Section ''Multimodal Embodiment'' for challenges related to agents with internal representations of users and ability to adapt to context and interactions). • Smart personal assistants: Multisensory agents able to integrate heterogeneous sources of knowledge, display social awareness and behave naturally in multiuser situations (see Section ''Multimodal Embodiment'' for challenges related to synthesis of social behaviour). • Active access to complex unstructured information:</p><p>Linking knowledge to rich interaction will enable the development of agents which can search proactively and can make inferences from their (possibly limited) knowledge, to enable people to be notified of relevant things faster and to help people reach understanding of complex situations involving many streams of information (see Section ''Computational Models of Interaction'' for challenges in representing knowledge and cognitive processes). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>This article has described some of the most important challenges and issues that need to be addressed in order to achieve substantial progress in technologies for the modelling, analysis and synthesis of human behaviour, especially for what concerns social interactions. Section ''The Data'' has shown that data, while being a crucial resource, cannot become an asset for the community without widely accepted practices for design, collection and distribution. Section ''Behaviour Analysis'' has proposed to move the focus of analysis approaches from individuals involved in an interaction to phenomena that shape groups of interacting people (e.g. interpersonal influence and social contagion). Section ''Multimodal Embodiment'' highlighted the need of endowing machines, in particular embodied conversational agents, with an internal representation of their users. Section ''Computational Models of Interaction'' has focused on the possibility of integrating models of human cognitive processes and semantics in technologies dealing with human behaviour. Finally, Section ''Applications'' has overviewed application domains that can benefit, or are already benefiting, from technologies aimed at modelling, analysis and synthesis of behaviour. While addressing relatively distinct problems, the challenges above have a few aspects in common that might guide at least the first steps required to address them. The first is that human behaviour is always situated and context dependent. Therefore, technologies for dealing with human behaviour should try to address highly specific aspects of the contexts where they are used rather than trying to be generic. Conversely, it should be always kept in mind that an approach effective in a given situation or context might not work in others. The second is the need of considering both verbal and non-verbal aspects of human-human and human-machine interactions. So far, verbal content and semantics tend to be neglected, the reason being that nonverbal aspects are more honest and, furthermore, taking into account what people say violates the privacy. The third is to model explicitly the processes that drive interactive behaviour in humans, e.g. the development of internal representation of others.</p><p>The last part of the article has considered three application case studies that account for different steps of the process that leads from laboratory to real-world applications. Healthcare personal agents have been proposed as a case of research vision that builds upon current technology trends (in particular the diffusion of mobile devices and the availability of large amounts of data) to design new applications of technologies for analysis of behaviour. The case of the intelligent control centres has shown that the implementation of an application-driven prototype provides insights on how technologies revolving around behaviour should progress. Finally, the case of conversational technologies has given an example of how a roadmapping process can contribute to bridge the gap between research and application.</p><p>Needless to say, the issues proposed in this article do not necessarily cover the entire spectrum of problems currently facing the community. Furthermore, new challenges and issues are likely to emerge, while the community addresses those described in this work. However, dealing with the problems proposed in this article will certainly lead to substantial improvements of the current state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 A</head><label>1</label><figDesc>Fig.1A layered view of human-machine interaction (ASR and TTS stand for automatic speech recognition and text to speech, respectively)</figDesc><graphic coords="9,178.71,59.24,365.76,210.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Left: Comfortable sensor-equipped chair. Micro gestures allow for natural interaction during lengthy passive monitoring periods. Middle: Operators at workstations are tracked and an acoustic</figDesc><graphic coords="12,53.98,557.36,487.44,119.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>• Communicative robots: Embodied agents able to display personality and to generate and interpret social signals (see Sections ''Behaviour Analysis'' and ''Multimodal Embodiment'' for related challenges). • Shared collaboration and creativity: Empowering and augmenting communication between people. This will include new approaches to social sharing (across languages), design platforms, which enable people to build their own tools and scalable systems that enable groups to collaborate with shared goals, facilitate problem solving and provide powerful mechanisms for engagement.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Cogn Comput</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>http://sspnet.eu/avec2014/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>http://homepages.herts.ac.uk/*comqkd/TG-NaturalInteraction WithSocialRobots.html.Cogn Comput</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>See http://www.cogitocorp.com for a company working on the analysis of call centre conversations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>See https://www.cereproc.com for a company active in the field.Cogn Comput</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This paper is the result of the discussions held at the ''International Workshop on Roadmapping the Future of Multimodal Interaction Research including Business Opportunities and Challenges'' (sspnet.eu/2014/06/rfmir/), held in conjunction with the ACM International Conference on Multimodal Interaction (2014). Alexandros Potamianos was partially supported by the BabyAffect and CogniMuse projects under funds from the General Secretariat for Research and Technology (GSRT) of Greece, and the EU-IST FP7 SpeDial project. Dirk Heylen was supported by the Dutch national program COMMIT. Giuseppe Riccardi was partially funded by EU-IST FP &amp; SENSEI project. Mohamed Chetouani was partially supported by the Labex SMART (ANR-11-LABX-65) under French state funds managed by the ANR within the Investissements d'Avenir programme under reference ANR-11-IDEX-0004-02. Jeffrey Cohn and Zakia Hammal were supported in part by grant GM105004 from the US National Institutes of Health. Albert Ali Salah was partially funded by the Scientific and Technological Research Council of Turkey (TUBITAK) under Grant No. 114E481.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The spoken language corpus at the department of linguistics, Go ¨te-borg university</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bjo ¨rnberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gro ¨nqvist L, Ahlse ´n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¨c</forename><surname>Ottesjo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forum Qual Soc Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The MUMIN coding scheme for the annotation of feedback, turn management and sequencing phenomena</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cerrato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jokinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Navarretta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang Resour Eval</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="273" to="287" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Studying movement synchrony using time series and regression models</title>
		<author>
			<persName><forename type="first">U</forename><surname>Altmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Program and Abstracts of the Proceedings of COST 2102 International Training School on Cognitive Behavioural Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Hu ¨bler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Wrann</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information seeking spoken dialogue systems-part I: semantics and pragmatics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ammicht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Multimed</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="532" to="549" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The HCRC map task corpus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kowtko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang Speech</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="366" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting unconscious user signals in multimodal human-computer interaction</title>
		<author>
			<persName><forename type="first">´e</forename><surname>Andre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Multimed Comput Commun Appl</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1s</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Challenges for social embodiment</title>
		<author>
			<persName><forename type="first">´e</forename><surname>Andre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="35" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lampouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<idno>arXiv:14056164</idno>
		<title level="m">Generating natural language descriptions from owl ontologies: the NaturalOWL system. 2014. arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatiotemporal symmetry and multifractal structure of head movements during dyadic conversation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Ashenfelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Boker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Waddell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vitanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Exp Psychol Human Percept Perform</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1072</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term socially perceptive and interactive robot companions: challenges and future perspectives</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aylett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M ;</forename><surname>Hanheide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Multimodal Interfaces, ICMI 2011</title>
		<meeting>the 13th International Conference on Multimodal Interfaces, ICMI 2011<address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-11">November, 2011. 14-18</date>
			<biblScope unit="page" from="323" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smart health monitoring systems: an overview of design and modeling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gholamhosseini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Med Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9898</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LUCID: a corpus of spontaneous and read clear speech in british english</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DiSS-LPSS Joint Workshop</title>
		<meeting>the DiSS-LPSS Joint Workshop</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">SOWL: a framework for handling spatio-temporal information in owl 2.0. Rule-based reasoning, programming, and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Batsakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Petrakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation learning: a review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Empathic touch by relational agents</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Bickmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bnc-Consortium</surname></persName>
		</author>
		<ptr target="http://www.hcu.ox.ac.uk/BNC" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Windowed cross-correlation and peak picking for the analysis of variability in the association between behavioral time series</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Boker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Rotondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="355" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effects of damping head movement and facial expression in dyadic conversation using real-time facial expression tracking and synthesized avatars</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Boker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos Trans R Soc B</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="page" from="3485" to="3495" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Topics for the future: genre differentiation, annotation, and linguistic content integration in interaction analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bonin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gilmartin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DIPPER: description and formalisation of an information-state update dialogue system architecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGdial Workshop on Discourse and Dialogue</title>
		<meeting>SIGdial Workshop on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting emotions to disambiguate dialogue acts</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´e</forename><surname>Andre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligent User Interfaces</title>
		<meeting>the International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A metric for no-reference video quality assessment for hd tv delivery based on saliency maps</title>
		<author>
			<persName><forename type="first">H</forename><surname>Boujut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Multimedia and Expo</title>
		<meeting>IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Designing sociable robots</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Breazeal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dialogue control functions and interaction design</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NATO ASI Series F Comput Syst Sci</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page">197</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extreme learning machines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="30" to="31" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sentic album: content-, concept-, and context-based online personal photo management system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="496" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sentic computing: a common-sensebased framework for concept-level sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentic medoids: organizing affective common sense knowledge in a multi-dimensional vector space</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mazzocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Networks, no. 6677 in LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentic PROMs: application of sentic computing to the development of a novel unified framework for measuring health-care quality</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="533" to="543" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Affective space 2: enabling affective intuition for concept-level sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Approaches to conversational speech rhythm: speech activity in two-person telephone dialogues</title>
		<author>
			<persName><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Congress of the Phonetic Sciences</title>
		<meeting>the International Congress of the Phonetic Sciences</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="343" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Embodied conversational agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Role of inter-personal synchrony in extracting social signatures: some case studies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lexinfo: a declarative model for the lexicon-ontology interface</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buitelaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mccrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sintek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semant Sci Serv Agents World Wide Web</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="51" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mother-infant face-to-face interaction: influence is bidirectional and unrelated to periodic cycles in either partner&apos;s behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tronick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dev Psychol</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="386" to="392" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Measuring facial action by manual coding, facial emg, and automatic facial image analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of nonverbal behavior research methods in the affective sciences</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Harrigan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="9" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coding dialogs with the DAMSL annotation scheme</title>
		<author>
			<persName><forename type="first">M</forename><surname>Core</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Fall Symposium on Communicative Action in Humans and Machines</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition meets formal ontologies: towards a semantic visual understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferrario</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="23" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On-line adaptive background modelling for audio surveillance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bicego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human behavior analysis in video surveillance: a social signal processing perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Del</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring social augmentation concepts for public speaking using peripheral feedback and real-time behavior analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Css</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scho ¨ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Luyten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´e</forename><surname>Andre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Mixed and Augmented Reality</title>
		<meeting>the International Symposium on Mixed and Augmented Reality</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multimodal coordination: exploring relevant features and measures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Delaherche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Social Signal Processing</title>
		<meeting>the International Workshop on Social Signal Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpersonal synchrony : a survey of evaluation methods across disciplines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Delaherche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdhaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saint-Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Viaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="365" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic measure of imitation during social interaction: a behavioral and hyperscanning-EEG benchmark</title>
		<author>
			<persName><forename type="first">E</forename><surname>Delaherche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Santa Barbara corpus of spoken American English. CD-ROM</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Chafe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spontal: a swedish spontaneous dialogue corpus of audio, video and motion capture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hellmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stro ¨mbergsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>House</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Language Resources and Evaluation Conference</title>
		<meeting>Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hager</surname></persName>
		</author>
		<ptr target="http://face-and-emotion.com/dataface/nsfrept/nsf_contents.htm.1992" />
		<title level="m">Final report to NSF of the planning workshop on facial expression understanding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multimodal saliency and fusion for movie summarization based on aural, visual, textual attention</title>
		<author>
			<persName><forename type="first">G</forename><surname>Evangelopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zlatintsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rapantzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Skoumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Multimed</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1553" to="1568" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Perception of congruent facial and haptic expressions of emotions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaffary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ammi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Applied Perception</title>
		<meeting>the ACM Symposium on Applied Perception</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="135" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Classification of emotional speech units in call centre interactions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karabetsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koutsombogera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Riviello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Cognitive Infocommunications</title>
		<meeting>IEEE International Conference on Cognitive Infocommunications</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="403" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint action, interactive alignment, and dialog</title>
		<author>
			<persName><forename type="first">S</forename><surname>Garrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics Cognit Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="304" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fusion of knowledge-based and data-driven approaches to grammar induction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Georgiladakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Iosif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Petrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SWITCHBOARD: telephone speech corpus for research and development</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE Int Conf Acoust Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Time series analysis: a comprehensive introduction for social scientists</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gottman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ICE: the international corpus of English</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engl Today</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3" to="7" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">What question would Turing pose today?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="73" to="81" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Guarino</surname></persName>
		</author>
		<title level="m">Proceedings of the international conference on formal ontology in information systems</title>
		<meeting>the international conference on formal ontology in information systems<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>IOS press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Intra-and interpersonal functions of head motion in emotion communication</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges, in conjunction with the 16th ACM International Conference on Multimodal Interaction ICMI 2014</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges, in conjunction with the 16th ACM International Conference on Multimodal Interaction ICMI 2014</meeting>
		<imprint>
			<date type="published" when="2014-11">November 2014</date>
			<biblScope unit="page" from="19" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Head movement dynamics during normal and perturbed parentinfant interaction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Messinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the biannual Humaine Association Conference on Affective Computing and Intelligent Interaction</title>
		<meeting>the biannual Humaine Association Conference on Affective Computing and Intelligent Interaction</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Interpersonal coordination of head motion in distressed couples</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="155" to="167" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Highly realistic 3D presentation agents with visual attention capability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Smart Graphics</title>
		<meeting>International Symposium on Smart Graphics</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Intention awareness: improving upon situation awareness in human-centric environments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human-Centric Comput Inform Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Affective neural networks and cognitive learning systems for big data analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Automatic behavior understanding in crisis response control rooms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ijsselmuiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Grosselfinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mu ¨nch D, Arens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ambient Intelligence</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="97" to="112" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Cogn Comput</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m">ISO Language resource management: semantic annotation framework (SemAF), part 2: Dialogue acts</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A saliency-based search mechanism for overt and covert shifts of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1489" to="1506" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rhythms of dialogue in early infancy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jaffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Crown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jasnow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monogr Soc Res Child</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="149" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The ICSI meeting corpus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Janin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE Int Conf Acoust Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="364" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multimodal interaction for future control centers: interaction concept and implementation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fuhrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Virtual director. Media production: delivery and interaction for platform independent systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Wiley</publisher>
			<biblScope unit="page" from="209" to="259" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Biologically inspired auditory attention models with applications in speech and audio processing</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kalinli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Southern California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The statistical analysis of data from small groups</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mannetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pierro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kashy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">126</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multimodal analytics and its data ecosystem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koutsombogera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Conceptual design as a driver for innovation in offshore ship bridge development</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kristiansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Maritime Transport VI</title>
		<imprint>
			<biblScope unit="page" from="386" to="398" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Information state and dialogue management in the TRINDI dialogue move engine toolkit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Lang Eng</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3&amp;4</biblScope>
			<biblScope unit="page" from="323" to="340" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Analyzing verbal data: principles, methods, and problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lemke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1471" to="1484" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Second international handbook of science education</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Saliency inspired modeling of packet-loss visibility in decoded videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reibman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Video Processing and Quality Metrics for Consumer Electronics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Investigating spousal influence using moment-to-moment affect data from marital conflict</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gottman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Fam Psychol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="300" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Distributional semantic models for affective text analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Malandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Iosif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Audio Speech Lang Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2379" to="2392" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Affective language model adaptation via corpus selection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Malandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Babeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Measuring behavior: an introductory guide</title>
		<author>
			<persName><forename type="first">P</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bateson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Cultural influences on the perception of emotion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cross-Cultural Psychol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Cultural similarities and differences in display rules</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motiv Emot</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="214" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">The AMI meeting corpus</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bourban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karaiskos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Methods and Techniques in Behavioral Research</title>
		<meeting>the International Conference on Methods and Techniques in Behavioral Research</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Automated measurement of facial expression in infant-mother interaction: a pilot study</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Messinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infancy</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="305" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Development of cellphone-type tele-operated android</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nishio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asia Pacific Conference on Computer Human Interaction</title>
		<meeting>the Asia Pacific Conference on Computer Human Interaction</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="665" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Modeling human communication dynamics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process Mag</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="112" to="116" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Using syntactic and semantic structural kernels for classifying definition questions in jeopardy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="712" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The neural network house: an environment hat adapts to its inhabitants</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Spring Symposium on Intelligent Environments</title>
		<meeting>AAAI Spring Symposium on Intelligent Environments</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="110" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">An integrated model of top-down and bottom-up attention for optimizing detection speed</title>
		<author>
			<persName><forename type="first">V</forename><surname>Navalpakkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">European Directory of Health Apps 2012-2013</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nead</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Patent View</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Conversational informatics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohmoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mohammad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">D64: a corpus of richly recorded conversational interaction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Oertel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Multimodal User Interf</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Social coordination dynamics: measuring human bonding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oullier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>De Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Jantzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kelso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lagarde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soc Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="192" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The NOMCO multimodal nordic resource-goals and characteristics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ahlse ´n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jokinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference</title>
		<meeting>the Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Facial action recognition for facial expression analysis from static face images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern Part B: Cybern</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1449" to="1461" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">On phonetic convergence during conversational interaction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Acoust Soc Am</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="2382" to="2393" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Honest signals</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Affective computing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Toward a mechanistic psychology of dialogue</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Brain Sci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="169" to="190" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">The META-SHARE language resources sharing infrastructure: principles, challenges, solutions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Piperidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Language Resources and Evaluation Conference</title>
		<meeting>Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Enhanced SenticNet with affective labels for concept-based opinion mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="31" to="38" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">EmoSenticSpace: a novel framework for affective commonsense reasoning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl Based Syst</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="108" to="123" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Towards an intelligent framework for multimodal affective data analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="104" to="116" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Cognitive multimodal processing: from signal to behavior</title>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Nonverbal synchrony in psychotherapy: coordinated body movement reflects relationship quality and outcome</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ramseyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tschacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Consult Clin Psychol</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="284" to="295" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Dense saliency-based spatiotemporal feature points for action recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rapantzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1454" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">ROCKIT: roadmap for conversational interaction technologies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu-Belis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Girenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Petukova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joscelyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kompis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aliwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sabbah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="39" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Towards healthcare personal agents</title>
		<author>
			<persName><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="53" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Grounding emotions in humanmachine conversational systems. Intelligent technologies for interactive entertainment. Lecture notes in computer science</title>
		<author>
			<persName><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tu ¨r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="144" to="154" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Collaborative discourse, engagement and always-on relational agents</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</author>
		<idno>FS-10-05</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Fall Symposium on Dialog with Robots</title>
		<meeting>the AAAI Fall Symposium on Dialog with Robots</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Looking to understand: the coupling between speakers&apos; and listeners&apos; eye movements and its relationship to discourse comprehension</title>
		<author>
			<persName><forename type="first">D</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit Sci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1060" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Rocking together: dynamics of intentional and unintentional interpersonal coordination</title>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Isenhower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Mov Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="867" to="891" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Situated Communication</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rickheit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wachsmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mouton de Gruyter</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Rickheit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Wachsmuth</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A simplest systematics for the organization of turn-taking for conversation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sacks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schegloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jefferson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="page" from="696" to="735" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Do parents recognize autistic deviant behavior long before diagnosis? Taking into account interaction using computational methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saint-Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahdhaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Cassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Laznik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Apicella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Muratori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maestro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Muratori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">393</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Natural multimodal interaction with a social robot: What are the premises?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="43" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Robots for use in autism research</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Admoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mataric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Rev Biomed Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="275" to="294" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Spotting laughter in natural multiparty conversations: a comparison of automatic online and offline approaches using audiovisual data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glodek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Interact Intell Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Multimodal prediction of expertise and leadership in learning groups</title>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Multimodal Learning Analytics</title>
		<meeting>the International Workshop on Multimodal Learning Analytics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Dyadic behavior analysis in depression severity assessment interviews</title>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimodal Interaction</title>
		<meeting>the ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Building autonomous sensitive artificial listeners</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schro ¨der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bevacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ter Maat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pammi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Sevin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><surname>Wo ¨llmer M</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="183" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Expressive speech synthesis: past, present, and possible futures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schroeder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="111" to="126" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Affective information processing</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Computational paralinguistics: emotion, affect, and personality in speech and language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Paralinguistics in speech and language-state-of-the-art and the challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mu ¨ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Speech Lang</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="39" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Mutual interpersonal postural constraints are involved in cooperative conversation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shockley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Exp Psychol Human Percept Perform</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="326" to="332" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">View-based object recognition using saliency maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="445" to="460" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Using affective parameters in a content-based recommender system for images</title>
		<author>
			<persName><forename type="first">ˇm</forename><surname>Tkalc ˇic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Burnik</surname></persName>
		</author>
		<author>
			<persName><surname>Kos ˇir A</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Model User-Adapt Interact</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Origins of human communication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tomasello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Understanding and sharing intentions: the origins of cultural cognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Call</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Behne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Brain Sci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="675" to="691" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Ontological inference for image and video analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Town</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Vision Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Measuring praise and criticism: inference of semantic orientation from association</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Inform Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="315" to="346" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Automatic behaviour understanding in medicine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</title>
		<meeting>the Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">The wildcat corpus of native-and foreign-accented english: communicative efficiency across conversational dyads with varying language alignment profiles</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Van Engen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baese-Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang Speech</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="510" to="540" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A system for real-time multimodal analysis of nonverbal affective social interaction in usercentric media</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Multimed</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="576" to="590" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A survey of personality computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mohammadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="291" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Social signal processing: survey of an emerging domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1743" to="1759" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Mobile Social Signal Processing: vision and research issues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Mobile Human-Computer Interaction</title>
		<meeting>the ACM International Conference on Mobile Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="513" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Bridging the gap between social animal and unsocial machine: a survey of social signal processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>'errico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="87" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Unsupervised temporal commonality discovery</title>
		<author>
			<persName><forename type="first">W-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Z</forename></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Verbmobil: Foundations of speech-to-speech translation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wahlster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
