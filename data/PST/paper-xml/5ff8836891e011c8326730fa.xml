<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GAMMA: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sheng-Chun</forename><surname>Kao</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
							<email>tushar@ece.gatech.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>9 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GAMMA: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3400302.3415639</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Genetic Algorithm</term>
					<term>ML accelerator</term>
					<term>Reconfigurable device</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>DNN layers are multi-dimensional loops that can be ordered, tiled, and scheduled in myriad ways across space and time on DNN accelerators. Each of these choices is called a mapping. It has been shown that the mapping plays an extremely crucial role in overall performance and efficiency, as it directly determines the amount of reuse that the accelerator can leverage from the DNN. Moreover, instead of using a fixed mapping for every DNN layer, research has revealed the benefit of optimizing per-layer mappings. However, determining the right mapping, given an accelerator and layer is still an open question. The immense space of mappings (or map-space) makes brute-forced exhaustive search methods unapproachable. In this paper, we propose a domain-specific genetic algorithm-based method, GAMMA, which is specially designed for this HW-mapping problem. In contrast to prior works that either target simple rigid accelerators with a limited map-space or choose from a restricted set of mappings, we construct an extremely flexible map-space and show that GAMMA can explore the space and determine an optimized mapping with high sample efficiency. We quantitatively compare GAMMA with many popular optimization methods and observe GAMMA consistently finds better solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Hardware ? Hardware accelerators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>recommendation systems. However, DNNs are often strictly constrained by end-to-end latency or energy. This has opened up extensive research on computationally efficient DNN models <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b58">59]</ref> and inference hardware accelerators <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>The architecture of DNN accelerators is determined by two key components: HW resources and HW mapping strategy. The HW resources (Fig. <ref type="figure" target="#fig_0">1(c</ref>)) comprise of the total on-chip compute (hereby referred to as "PEs"), local scratchpad (SL) buffer in each PE, a global scratchpad (SG), and a network on chip (NoC) connecting them. The HW mapping strategy comprises of the tile sizes, computation order, and parallelization strategy (Fig. <ref type="figure" target="#fig_0">1(b)</ref>). The design of computation order and parallelization strategy is also known as dataflow <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>. The HW mapping and/or the HW resources are either fixed at design-time <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>, or can be tuned at compile time (if the accelerator is reconfigurable, such as CGRA-based <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b61">62]</ref> or FPGA <ref type="bibr" target="#b67">[68]</ref>). The architectures are often designed based on the expected dimensions and shapes of the DNNs and heuristics. For example, the NVDLA <ref type="bibr" target="#b0">[1]</ref> dataflow keeps weights stationary at PEs and parallelizes across input channels and output channels, optimizing for mid and late layers of many CNNs like ResNet <ref type="bibr" target="#b20">[21]</ref> that exhibit this property. The Eyeriss <ref type="bibr" target="#b9">[10]</ref> dataflow parallelizes across the activation and filter rows and keeps filter rows stationary.</p><p>Prior research <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b67">68]</ref> has shown that there are no mapping strategies that can be efficient across all the layers of a DNN model. To exploit the benefit of different mappings, in this paper, we consider accelerators where the number of PEs and buffer sizes are fixed at design time, but the mapping can be configured at compile-time for each layer. The goal is to find the HW mapping for each layer of DNN models that optimizes the objective (min. latency/ energy).</p><p>Although multiple prior works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b67">68]</ref> have studied the mapping problem for DNN accelerators, the HW-mapping search space (exceeding ? (10 36 ) even for a single layer of a DNN, as shown later in Section 2.6) makes the problem highly challenging. To cope with this challenge, most prior works restrict the search space. For e.g., coarse-grained strided exhaustive search <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref>, random search <ref type="bibr" target="#b34">[35]</ref>, fixed parallelism <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b67">68]</ref>, or limited search for tile sizes for one or more fixed dataflows <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b65">66]</ref>). Alternately, ML-based search techniques have also been leveraged for guided search to increase sampling efficiency <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b60">61]</ref>; however, they need to restrict some aspects of the mapping space (e.g., fixing the parallelism levels) to adapt to the ML algorithms. Such restrictions of the mapping space can lead to local optimal mappings which are significantly sub-optimal, as recent works have highlighted <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>To efficiently deal with the massive search space of HW-mappings, we propose GAMMA (Genetic Algorithm-based Mapper for ML Accelerators). Unlike prior works, GAMMA performs a complete search, considering all three aspects of HW-mapping (tiling strategy, computation order, and parallelization strategy). Furthermore, GAMMA can explore up to three levels of parallelism within the mapping, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), unlike prior works that target one to two levels. Thus GAMMA can work across both singleaccelerator <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref> and multi-accelerator <ref type="bibr" target="#b45">[46]</ref> systems. The key novelty in GAMMA is (i) a specialized genetic encoding of all three aspects of HW mapping, (ii) specialized mutation and crossover operators to evolve new mappings, and (iii) new genetic operators to model the behavior of adding and removing levels of parallelism. We also develop a closed-loop workflow by integrating GAMMA with a popular analytical cost-model for DNN mappings called MAESTRO <ref type="bibr" target="#b1">[2]</ref> to fully automate the mapping search problem.</p><p>We examine GAMMA on multiple popular DNN models, VGG16 <ref type="bibr" target="#b48">[49]</ref>, MobileNet-V2 <ref type="bibr" target="#b43">[44]</ref>, ResNet-18 <ref type="bibr" target="#b20">[21]</ref>, ResNet-50 <ref type="bibr" target="#b20">[21]</ref>, and MnasNet <ref type="bibr" target="#b59">[60]</ref>. We make quantitative comparisons with eight popular optimizations methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>. We observe that GAMMA can find HW-mapping with the HW performance (latency/energy) consistently better than other methods. Further, we show that GAMMA can be run over multiple stages, enabling it to leverage throughput slack from non-bottlenecked DNN layers and further reduce total system energy by 58% (for ResNet-18) and 78% (for VGG16) and power by 95% (for ResNet-18) and 99% (for VGG16).</p><p>The contributions of this paper are as follows:</p><p>? Comprehensive Map Space. GAMMA constructs and searches through a comprehensive map-space comprising of computation order, tile-sizes, parallelization strategy, and up to three parallelization levels, enabling it to target a wide variety of fixed and flexible single and multi-accelerator systems.</p><p>? Generic Encoding scheme. The proposed encoding scheme transforms the HW-mapping problem to an optimization problem, which enables the user to directly use off-the-shelf optimization algorithms for mapping. These form our baselines.</p><p>? New Genetic Algorithm operators. GAMMA introduces three new GA operators, enabling a domain-specific flexible search space unlike most off-the shelf optimization algorithms. ? Autonomous Workflow. We automate GAMMA as a blackbox optimizer for the HW-mapping problem. This reduces the learning curve and saves manual-tuning effort for ML practitioners exploring the HW-mapping space. GAMMA encapsulates an end-to-end workflow, which generates outputs compatible with an open-source cost model <ref type="bibr" target="#b1">[2]</ref>. We will open-source the GAMMA infrastructure after the paper gets published.</p><p>The paper is organized as follows. Section 2 provides relevant background on DNN accelerators and optimization methods; Section 4 describes GAMMA; Section 5 presents comprehensive evaluations; Section 6 discusses related works and Section 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION 2.1 Layer types in DNNs</head><p>There are myriads of DNN models and most of them are built using different combinations of some common layers. Convolutional layers (2D/depth-wise/point-wise) dominate in DNNs like ResNet-50 <ref type="bibr" target="#b20">[21]</ref>, MobileNet-V2 <ref type="bibr" target="#b43">[44]</ref>, and InceptionNet <ref type="bibr" target="#b57">[58]</ref> targeting image processing tasks. Fully connected layers or MLPs are often used as the last layer in many DNNs and as hidden layers of RNNs. Different layer types expose different amounts of data reuse opportunities, which can be exploited by DNN accelerators. In this work, we consider both CNNs and MLPs that dominate modern DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DNN Accelerator Architectures</head><p>2.2.1 HW resources. Spatial DNN accelerators comprise of a 2D array of Processing Elements (PEs), as shown in Fig. <ref type="figure" target="#fig_0">1(c)</ref>. Each PE has a MAC to compute partial sums, and local scratchpad (SL) buffers to store weights, activations, and partial sums. The accelerators also house a global scratchpad (SG), shared among PEs, to prefetch activations and weights from DRAM for the next tile of computation that will be mapped over the PEs and SL buffers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Dataflow.</head><p>In addition to the HW resources, each accelerator needs to select a dataflow strategy to stage data movement in order to leverage data reuse. Dataflow comprises of computation order and parallelism strategy, which we describe in Section 2.3. It directly affects the amount of data reuse the accelerator is able to leverage. Dataflows can be fixed at design-time <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref> or configured at compile-time <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>. In this work, we assume an accelerator with configurable dataflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mapping DNNs over Accelerators</head><p>Mapping refers to the dataflow strategy (i.e., computation order and parallelism) coupled with the tiling strategy <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref>. We describe the three components of a DNN mapping next.</p><p>Computation order. 2D convolution, shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), is a six loop-nest (or seven loops when considering batching). We can swap the order of these loops randomly, incurring 6! choices. Different orderings lead to different temporal reuse (aka "stationary" <ref type="bibr" target="#b9">[10]</ref>) or spatial reuse (multi-casting) opportunities for the operands.</p><p>Parallelism strategy. The parallelism strategy comprises of (i) the number of levels of parallelism, and (ii) the specific loops to parallelize or spatially unroll <ref type="bibr" target="#b66">[67]</ref> at each level). Different dimensions of parallelism at each level incur different data movement patterns, multi-casting behavior, and reuse opportunities <ref type="bibr" target="#b30">[31]</ref>. A simple example is the NVDLA <ref type="bibr" target="#b0">[1]</ref> architecture that employs a fixed 2-level mapping along the K and C dimensions for convolutions.</p><p>The number of parallelism levels depends on the number of independent spatial dimensions within the accelerator substrate. In this work, we assume up to three levels of parallelism (e.g,. rows x cols x multiple arrays) as shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. This sets the possible choices to 6 ? 5 ? 4 = 120. A 3-level mapping can work on a flexible accelerator fabric <ref type="bibr" target="#b29">[30]</ref> or multiple 2D accelerators (Fig. <ref type="figure" target="#fig_0">1(c)</ref>).</p><p>Tiling strategy. We refer to tiling strategy as designing the tiling size of each dimension (e.g., input(C)/output(K) channel dimensions, Y/X dimensions of activations, R/S dimensions of weights in Fig. <ref type="figure" target="#fig_0">1(a)</ref>). The tile size of each dimension could range from 1 to the size of the dimension. For e.g., the second layer of VGG16 <ref type="bibr" target="#b48">[49]</ref>, (K=64, C=64, Y=224, X=224, R=3, S=3), forms a tile size search space of ? (10</p><formula xml:id="formula_0">9 ) = 64 2 ? 224 2 ? 3 2 .</formula><p>The tile sizes determine the number of operands of each tensor (input/weight/output) that need to be present within the accelerator buffers at each time-step. Tile sizes depend on both the dataflow strategy and the size of the buffers. Tiles of data move between DRAM and the SG buffers, and between the SG and SL buffers. (As discussed above, some of these data items may remain stationary in the buffers for longer, while some stream in and out, depending on the computation order). The rate at which different tiles move determines the off-chip and on-chip bandwidth requirement; bandwidth less than this leads to stalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Target Accelerator Systems</head><p>Many DNN accelerators come with a fixed dataflow strategy baked into silicon at design-time <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>. The job of a mapper is to simply find tile sizes for each layer to fit within the buffers. However, there is no perfect dataflow that is supreme for all layers in a DNN model <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref>. This led to a suite of flexible accelerators <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref> that allow dataflow configurability <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref> at compile-time via flexible buffering and connectivity to make the accelerator futureproof to emerging DNNs. This work considers such accelerators and hence assumes that the mapping (computation order, parallelizing dimensions and tile sizes) are configured at compile-time. The only constraint for the mapper from hardware is the maximum number of parallelism levels (which depends on the accelerator array flexibility) and maximum tile-sizes (limited by the buffer sizes). It is certainly possible to restrict further aspects of the mapping, if the hardware desires, within our framework. Alternately, our proposed framework can also be used at design-time to determine the optimal dataflow for accelerators built for specific DNNs types. We present our target accelerators in Section 4 and Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">MAESTRO: A Cost Model for evaluating the cost of DNN Mappings</head><p>Frameworks like MAESTRO <ref type="bibr" target="#b1">[2]</ref> and Timeloop <ref type="bibr" target="#b34">[35]</ref> use detailed analytical modeling to evaluate different mapping strategies of a DNN on the accelerators. We leverage MAESTRO <ref type="bibr" target="#b1">[2]</ref> as our underlying cost model because of its ability to support the target detailed mapping space. It supports most of the common DNN layers such as CONV, depth-wise CONV, and Fully connected. Given a DNN layer, a HW resource configuration (PE, SL size, SG size, NoC latency and bandwidth), and a mapping strategy, MAESTRO estimates the statistics such as latency, energy, runtime, power, and area.</p><p>Impact of Mapping on Performance and Energy. In Fig. <ref type="figure" target="#fig_1">2</ref>(a) and (b), we randomly sampled 10K possible HW mappings and use the cost model to estimate its corresponding HW performance. Each datapoint reflects a valid HW mapping design for the same DNN layer (the second layer of VGG16). From Fig. <ref type="figure" target="#fig_1">2</ref>(c), we can observe several order of difference on HW performance when the mapping varies. This validates similar studies <ref type="bibr" target="#b34">[35]</ref> and shows how critical the HW-mapping is to the performance of a DNN accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Mapping Search-Space aka Map Space</head><p>The mapping space of a 1-level mapper in the example of the second layer of VGG16 is ? (10 12 ) = ? (10 9 ? 6! ? 6). A N-level mapper increase the mapping space by the power of N, leading to ? (10 12? ), which is ? (10 36 ) when considering three level of parallelism. This design space is hard to enumerate even with coarse-grained stridden enumeration. Therefore, a domain-specific specialized optimization algorithm is needed to search the design space with sample efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Baseline Search Methods</head><p>Many search/optimization methods exist today for architects to perform Design-Space Exploration (DSE) and form our baselines. We leverage eight optimization methods, including Random Search, Genetic Algorithm (GA) <ref type="bibr" target="#b22">[23]</ref>, Differential Evolution (DE) <ref type="bibr" target="#b37">[38]</ref>, (1 + ?)-ES <ref type="bibr" target="#b41">[42]</ref>, Covariance matrix adaptation evolution strategy (CMA-ES) <ref type="bibr" target="#b19">[20]</ref>, Test-based Population-Size Adaptation (TBPSA) <ref type="bibr" target="#b21">[22]</ref>, Particle Swarm Optimisation (PSO) <ref type="bibr" target="#b25">[26]</ref>, Passive Portfolio (pPortfolio) <ref type="bibr" target="#b12">[13]</ref>. We summarize them in Table <ref type="table" target="#tab_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GAMMA ALGORITHM AND WORKFLOW 3.1 Challenges with Baseline Methods</head><p>The baseline methods in Section 2.7 can all be used for HW-mapping search. However many algorithms (e.g., CMA-ES, PSO, DE, and also standard GA) work in rigid search space, i.e., the number of parameters in a design point is pre-defined, which restricts the levels of parallelism (Section 2.3) to a pre-defined number and shrinks the potential search space by log scale. Our goal is to parameterize the level of parallelism as well. We need a framework that accepts input with flexible lengths. There are many possible ways to realize such a flexible framework, such as adding an extra auto-encoder <ref type="bibr" target="#b26">[27]</ref> or using an sequence-to-sequence structure <ref type="bibr" target="#b56">[57]</ref> at the input layer. However, they require another level of optimization, training, or approximation, which brings in relatively large overhead comparing to the optimization algorithm (DE, ES, standard GA) itself.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Genetic Algorithms (GA)</head><p>In this work, we develop a GA-based search technique. We list some common terminology for GA, namely gene, genome, elite, population, we will use across this paper in Table <ref type="table" target="#tab_0">1</ref>. A genome is a mapping solution in our context. We reproduce the next generation by mutation and crossover. The goal of GA is to retain well-performing genes across the evolution.</p><p>Benefits of GA. GA is one of the most popular algorithms for the scheduling problem for its lightness and simplicity <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b63">64]</ref>. Research shows GA reaches competitive performance with deep reinforcement learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b54">55]</ref>, and hyperparameter optimization problem. STOKE <ref type="bibr" target="#b44">[45]</ref> and TensorComprehensions <ref type="bibr" target="#b60">[61]</ref> use GA to search the space of DNN code optimization.</p><p>Challenges with standard GA. Standard GA still falls into the pits of the algorithm needing rigid input length. To this end, we develop a way to adopt GA to our problem by designing a novel evolution mechanism, which allows it to be flexible, without adding up immense overhead such as adding encoder or training an seqto-seq model. We discuss these details next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GAMMA Encoding scheme</head><p>We design a specific genetic encoding scheme for the HW mapping problem. For a 1-level mapper, we encode them into a (7, 2) dimensions of the genome, which contains 7 pairs of genes, as shown in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. A pair of the gene contains a DNN layer tensor notation (e.g, K, C) and its tile size. The ordering of pairs reflects the computation order. The first pair of gene tells the parallelizing dimension. The 2-level mapper in Fig. <ref type="figure" target="#fig_2">3(b</ref>) is encoded in the same manner. ? ?1 describes number of parallel L1-mappers, which is constrained by the number of available PEs (the number of PEs defines the maximum amount of parallelism.), and the corresponding tile size of the chosen parallelizing dimension (since we need at least one element to distribute into each parallelism unit). The L1-mapper describes the inner loop. The L2-mapper describes the outer loop, while containing ? ?1 number of instances of L1-mapper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoding Genomes into a Mapping</head><p>We outline how we describe the three aspects of mapping space in the cost model, and show how the genomes from GAMMA are decoded into the cost model's description. In MAESTRO, we note the parallelized dimension as SpatialMap and remaining dimensions as TemporalMap. Therefore we mark the first element (indicating parallelizing dimension) in (a) as Spa-tialMap in (b). A level of mapper in GAMMA can be translated as a ??????? in MAESTRO, in which tiling strategy, computation order and parallelism dimensions are fully described. We formulate multiple level of parallelism by concatenating the cluster. The L1 and L2 mapper are decoded into the bottom and upper cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Algorithm Flow</head><p>Fig. <ref type="figure" target="#fig_4">5</ref> shows the flow of the GAMMA algorithm. We discuss the detail of each function block next. We first describe how we adopt the generic evolution operators, Crossover and Mutation, to the HWmapping problem, and then introduce three additional evolution operators (Reorder, Growth and and Aging) in GAMMA.</p><p>Initialization. Assuming population size ?, we randomly initialize ? number of the 1-level mappers. The only restriction is each tile size is smaller than the corresponding layer dimension.</p><p>Evolution: Crossover. Crossover is to take advantage of the genes in some well-performing genomes, which forms a parents subset. We randomly pick two genomes from the parents subset. We blend their genes by interchanging the value of the tile size.</p><p>Evolution: Mutation -Parallel Dim. With a certain probability, which is set by the mutation rate of the algorithm, we mutate the parallelism dimension by randomly sampling one of the 6 dimensions of the tensor and setting it as a new parallelism dimension.</p><p>Evolution: Mutation -Tile Size. With a certain probability, we randomly pick paired genes and assign a new random tile size for them. If the tile-size in the mapping does not fit within the SL buffer of the PE for that operand, it is given a large penalty during its evaluation, as we discuss later in Section 3.6.3.</p><p>Evolution: Reorder. Reorder is another format of mutation. We pick two paired genes and swap their position in the genome, which reflect the reordering of the mapping.</p><p>Evolution: Growth. With a certain probability, we grow the genome by appending a randomly initialized 1-level genome to the current genome, as shown in Fig. <ref type="figure" target="#fig_2">3(b)</ref>. The original L1 mapper will be promoted to L2-mapper, and the newborn genome is noted as the new L1-mapper.</p><p>Evolution: Aging. The natural phenomenon of a person's DNA keep shortening in the lifespan is known as DNA aging. With a certain probability, we will "age" the genome by cutting out the tail of genome, an L1-mapper, which moves genome from (b) back to (a) in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>Evaluate and Selection. After evolution, we evaluate the populations by interacting with the evaluation environment (Env), which we will describe in Section 3.6.3. Env will feedback the fitness of each individual. We select the population that is eligible to enter the next generation by the ranking of their fitness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Flow for Automated Mapping Search</head><p>3.6.1 Constraint. Our target is to find the HW mapping of a DNN layer that fits within limited HW resources -PEs and buffers. Different mapping lead to drastically different requirement of HW resources, especially the buffer sizes, as shown in Fig. <ref type="figure" target="#fig_1">2(c</ref>). Note that a mapping implicitly runs over multiple time iterations if the number of computations in the dimension to paralleize exceeds the number of available PEs; however, the local (SL) buffer and global (SG) buffer sizes to run each iteration (which comes from the tile sizes) is a hard constraint when searching through the map-space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Objective.</head><p>The target is to minimize the objective. The objective could be any HW performance index that the user is interested in such as latency, power, energy, area, energy-delay-product (EDP), or other combinations of them. Minimizing the objective is not a trivial task since there is no straight-forward solution even for a common tensor shape of a DNN layer. Minimizing the latency as an example, the most efficient choice of parallelizing dimension involves the shape of tensor, available PEs to parallelize, available SL/SG buffers to house the fetched data, and the tile size of each dimension. All the decisions (or genes) correlate and jointly decide the latency. Some common heuristics of parallelizing across activations dimensions at the early layers and across channel dimensions at the late layers in CNN becomes challenging when involving HW resource constraint and the multi-level parallelism flexibility of mapping strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Interactive Environment (Env). Structure:</head><p>The Env is initialized with the target DNN layer, the accelerator constraint, and the optimization objective (latency/energy/power). Env contains a HW performance cost model, where we leverage MAESTRO <ref type="bibr" target="#b1">[2]</ref> for its ability to model and evaluate arbitrary spatial accelerators and mappings. When interacting with GAMMA, Env takes in an entire generation of populations, decodes them into the input format of the cost model as describe in Section 3.4, and feeds into the cost model to gather the statistics of their HW performance. Finally, using the fitness function, which we discuss next, Env extracts fitness scores and returns them to GAMMA.</p><p>Fitness Function: We extract the corresponding reward value (= -Perf. index) from the statistics according to the set objective, and substitute it into the fitness function. We give the individual a large penalty -a negative infinite -when the constraint is not met. That is, the evolved mapping require more HW resources than the accelerators' constraint, which is then not suitable for the targeting accelerators. The fitness function is summarized as following.</p><formula xml:id="formula_1">??????? = ??????, if constraint met -??? ?????, others<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY 4.1 Models and Platforms</head><p>DNN Models. In our experiment, we consider five CNN models with different complexity: VGG16 <ref type="bibr" target="#b48">[49]</ref>, MobileNet-V2 <ref type="bibr" target="#b43">[44]</ref>, ResNet-50 <ref type="bibr" target="#b20">[21]</ref>, ResNet-18 <ref type="bibr" target="#b20">[21]</ref>, MnasNet <ref type="bibr" target="#b59">[60]</ref>. HW resources of Platforms. We consider two platforms with different number of HW resource: cloud platform (which resembles the HW resources in cloud TPU <ref type="bibr" target="#b24">[25]</ref>) and edge platform (which resembles the HW resources in Eyeriss chip <ref type="bibr" target="#b10">[11]</ref>), as shown in Table <ref type="table">2</ref>.</p><p>Target Systems. We consider three kinds of accelerator systems as shown in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Target Search Methods and Parameters.</head><p>We compare three sets of methods, described below. We set the maximum sampling points as 10K for all methods and compare the HW performance of their searched solutions.</p><p>Baseline Optimization Methods. We compare with a suite of optimization methods whose implementations are adopted from Nevergrad <ref type="bibr" target="#b39">[40]</ref>. The methods, and their experimental parameter settings are summarized in Table <ref type="table" target="#tab_2">4</ref>.</p><p>Fixed Dataflows from prior accelerators. We also compare with some widely recognized HW-mappings inspired by dataflows within prior accelerators: NVDLA-like <ref type="bibr" target="#b0">[1]</ref> (parallelizing K and C dim.), Eyeriss-like <ref type="bibr" target="#b9">[10]</ref> (parallelizing Y and R dim.), and ShiDianNaolike <ref type="bibr" target="#b14">[15]</ref> (parallelizing Y and X dim). All three of them have fixed 2-level parallelism dimensions. We create custom mappings (i.e., dataflow + tile-size) by setting appropriate tile sizes that fit within the SL buffers for both the edge and cloud platforms .</p><p>GAMMA. We set the populations=200, generations=50, and the mutation/crossover rate and execution rate of other evolving functions as 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION 5.1 S1: Fixed 2D Accel.</head><p>In Fig. <ref type="figure" target="#fig_6">6</ref>(a), we run the baseline algorithms and GAMMA to search for the HW-mapping for each of the 20 layers in ResNet-18 with S1 setting (i.e., 2 levels of parallelism) and edge platform constraint. We record the best solution (lowest latency) of each algorithm after they execute 10K samples (most comparing algorithms converge after 10K samples). Fig. <ref type="figure" target="#fig_6">6</ref>(a) shows the latency of each algorithm's solutions, where we also plot the corresponding latency when using fixed dataflow. We observe the baseline algorithms and the fixeddataflows are with competitive performance. However, GAMMA can consistently find better solutions than both methods.</p><p>Valid solution and different platform constraints. The HWmapping is invalid when its requirement of HW resources exceeds the platform constraint. Some methods cannot find any valid solutions that conform to the constraint after 10K sampling. Therefore some methods have no solutions (NAN) in some cases as shown in Fig. <ref type="figure" target="#fig_6">6(a)</ref>. We did not show the result of Random Search here, since it ends up finding no solution for most of the cases, which also infers the complexity of the search space (it cannot find a valid solution in 10K samples). When we have more HW resource budget as Cloud platform in Fig. <ref type="figure" target="#fig_6">6</ref>(b), all baseline optimization methods can start to find valid solutions and optimize on them. This shows how the imposed constraints increase the complexity of the problem.</p><p>Despite the fact that the optimization methods fail in some cases, their solutions are competitive to manual-design ones (fixeddataflow) when they succeed, as shown in Fig. <ref type="figure" target="#fig_6">6(a)-(b</ref>). It shows the potential of automating the HW-mapping design process by properly formulating it into an optimization problem, which can significantly relieve the domain expert's effort on the back-andforth tuning process. The challenge is the occasional failing of some methods. Moreover, GAMMA can consistently find valid and better solutions than others. Comparing with others, GAMMA finds solutions costing 224? to 440? less latency in Edge platform and 153? to (1.3E+7)? less latency in Cloud platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">S2: Flexible 2D Accel.</head><p>5.2.1 Objective: Latency. Fig. <ref type="figure" target="#fig_6">6(c)-(d</ref>) compares latency for accelerators with flexible aspect ratios (i.e., the accelerator can support both 1-level and 2-levels of parallelism). One interesting observation is that the fixed ShiDianNao-like dataflow and NVDLA-like dataflow shows better performance than baseline optimization methods at early and late layers respectively. This is because ShiDianNao-like dataflow parallelizes along X, Y dimensions and early layers of ResNet-18 have high X-Y values; similarly NVDLA-like parallelizes along C-K and late layers have high C-K values. For the baseline methods, since the number of parallelism dimensions is fixed, we search for the best 1-level and 2-level solution for 5K points each, and pick the better one. GAMMA searches across both 1-level and 2level via the growth and aging operators described earlier. GAMMA finds valid and better solutions than others. Compared with other techniques, GAMMA finds solutions with 209? to 1,035? less latency in Edge and 337? to (7.1E+5)? less latency in Cloud platform. 5.2.2 Objective: Energy. Energy consumption depends on the number of active computations, memory accesses, and SL/SG buffer usages. In the Edge platform in Fig. <ref type="figure" target="#fig_8">8(a)</ref>, fixed dataflow show no advantage, and most optimization methods can find better solutions than the fixed dataflows. In the Cloud platform in Fig. <ref type="figure" target="#fig_8">8</ref>(b), optimization methods show competitive or better performance than Eyeriss-like and ShiDianNao-like dataflow. However, NVDLA-like dataflow shows high energy efficiency, since the K, C dimensions expand in ResNet-18, which gives more advantage to the dataflow that is skilled at layer with large K, C dimensions (NVDLA-like). They finish the computation with shorter time and less memory access, and hence cost less energy. However, these advantages do not show up when NVDLA-like dataflow is in the tight constraint (Edge platform), which has less SL/SG buffer and limited the parallelism opportunity. Across all fixed dataflow and optimization methods, GAMMA finds solutions costing 11? to 36? less energy in Edge platform and 2? to 42? less energy in Cloud platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">S3</head><p>: Scale-out Flexible 2D Accel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.1</head><p>The growth of search space. Increasing the level of parallelism will exponentially increase the search space, which makes the performance of the optimization methods more critical to the found solutions. As shown in Fig. <ref type="figure" target="#fig_6">6(e)-(f</ref>), the number of cases that methods fail to find solution becomes significant. However, GAMMA can consistently find valid and better solutions, costing 241? to 644? less latency in Edge platform and 657? to (1.2E+5)? less latency in Cloud platform. Fig. <ref type="figure" target="#fig_6">6(g)-(</ref>h) shows the end-to-end latency of GAMMA in different accelerator systems. We can find GAMMA performs the best in S3, where the design space is several order larger than S1 and S2 but with more flexibility. It shows that GAMMA can explore the design space with sample efficiency and takes advantage of the flexibility of the mapping space. 5.3.2 Deep-dive into found solution. Fig. <ref type="figure" target="#fig_7">7</ref> shows the HW-mapping solutions found by GAMMA on ResNet-18. At the early layer (Y, X dominant, Y=224, X=224), GAMMA found a mapper that parallelizes NAN: The method cannot find a solution that fits in the platform constraint within 10K samples.</p><p>Table <ref type="table">5</ref>: End-to-end performance and energy on S3 for a suite of DNN models using fixed mappings versus GAMMA. Bold means lowest values.  along Y dim at L2-mapper. At the medium layer (Y=56, K=128, C=64, X=56), GAMMA found a 3-level mapper, which maps across Y, K, and C dimensions. At the late layer (K, C dominant, K=512, C=256), GAMMA parallelize C at L2-mapper and K at L1-mapper. From the above observation, we find the automatically evolved solutions are consistent with some heuristic and insight from the manualdesigned dataflows<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.3</head><p>Other DNN models and end-to-end performance. Table <ref type="table">5</ref> shows the performance of GAMMA comparing to fixed dataflow on other widely-used DNN models. Here, for the interest of space, we only list the end-to-end performance, which is the sum of latency/energy of all the layers. Table <ref type="table">5</ref> shows no fixed dataflow is good across all DNNs and platforms. For e.g., when considering latency, ShiDianNaolike performs the best on Edge platform, and NVDLA-like performs the best on Cloud platform. In contrast, the energy numbers follow NVDLA-like &lt; Eyeriss-like &lt; ShiDianNao-like; NVDLA-like gets advantage over the other two for energy via reuse across K and C dimensions (which dominate in most CNN-based models). Among all experiments in Table <ref type="table">5</ref>, GAMMA always provides the lowest latency and energy. Across models and platforms, GAMMA finds solutions costing 5? to (1.2E+5)? less latency and 2? to (1.6E+4)? less energy. Fig. <ref type="figure" target="#fig_9">9</ref> tracks how GAMMA converges to its solution across generations; this shows its sample efficiency via rapid improvement over generations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Two-stage Optimization for Inter-layer</head><p>So far in this paper, we consider three systems: S1, S2, and S3, to parallelize the computation of a DNN layer, whose scenario can be termed as intra-layer parallelism. Next, we show how GAMMA can also be applied to the scenario of inter-layer parallelism. We consider a S3 system with inter-layer parallelism scenario, used in prior systems <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b61">62]</ref>, where each accelerator is handling one layer of a model, and the entire model is executed as layer-wise pipeline manner on the system. 5.4.1 Motivation. The pipelined system can often bring higher throughput. However, it also owns the problem of being bottleneck by critical block. The layer-wise pipelined accelerator can be bottlenecked by some computation-heavy layer. As the bottleneck latency exists and may not be able to be further optimized, in this case, we relax other non-critical blocks by relaxing their timing constraint to achieve overall lower energy/power of the system. 5.4.2 Structure. We apply a two-stage optimization method, where we optimize latency first and then power/energy at the second stage.</p><p>Stage I: optimize latency. We use GAMMA to find the mapping that optimizes the latency of each layer. We identify the bottleneck layer, whose latency decides the pipeline latency of the system.</p><p>Stage II: optimize power/energy. With the pipeline latency decided, we relax other layers by applying GAMMA again but optimizing power/energy at this stage with the awareness of not exceeding the pipeline latency. This is formulated by adding a heavy penalty when the searched solution exceeds the pipeline latency.</p><p>With the designed two-stage optimization, we could optimize the throughput of a layer-wise pipelined system at the first stage and further optimize its power/energy efficiency at the second stage. 5.4.3 Results. Table <ref type="table" target="#tab_3">6</ref> shows the HW performance of each layer in ResNet-18 in the 2-stage optimization scheme. In the Latency-Power experiment, we optimize latency first and their power next. After the first stage, it shows that the latency is bottleneck by the second layer, and it decides the pipeline latency. With the awareness of the pipeline latency, we optimize power at the second stage and find we could reduce the power by 95% comparing to the first stage when remaining at the same pipeline latency. The Latency-Energy experiment shows 58% reduction on energy consumption. Likewise, we execute the same flow on VGG16 and found it also effectively reduce the power by 99% and energy by 78% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORKS 6.1 Dataflow Design in DNN Accelerators</head><p>Dataflow design has been a popular topic in the research of DNN Accelerators. Multiple hand-designed dataflows have been used across accelerators, categorized <ref type="bibr" target="#b9">[10]</ref> as output-stationary <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>, weight-stationary <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36]</ref>, row-stationary <ref type="bibr" target="#b9">[10]</ref>, input stationary, and no local reuse <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b68">69]</ref>. In this work, we provide a framework to automatically determine an optimized dataflow and mapping. GAMMA can be used at compile-time to configure in the mapping if the underlying accelerator supports multiple dataflows <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>, or at design-time to determine the right dataflow for a custom accelerator developed for running a fixed set of DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">HW Mapping Space Search and Exploration</head><p>Many recent works have been developed to tackle DNN HW mapping. However, since the search space is extremely large, many of them restrict the search space by considering only part of the aspects of the HW mapping search space. Some consider a limited combination of HW mappings and pick among them <ref type="bibr" target="#b32">[33]</ref>. Some constrain the parallelizing dimension to a few choices <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b62">63]</ref>. Some fixed the computation order to a subset of all combinations <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b67">68]</ref>. Some vastly reduce the space of tiling sizes <ref type="bibr" target="#b51">[52]</ref> by a heuristic, or large step size, e.g., power of two <ref type="bibr" target="#b53">[54]</ref>. Interstellar <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b66">67]</ref> considers all three aspects of HW-mapping, but they constrain the search space by limiting the choice on each aspect such as the choices of loop order, parallelizing dimension. All these prior arts exclusively rely on exhaustive/random search with the help of coarse-grained striding enumeration or heuristics-based pruning. On the other hand, to search the mapping space with sample efficiency, Suda et. al <ref type="bibr" target="#b55">[56]</ref> and TensorComprehensions <ref type="bibr" target="#b60">[61]</ref> uses genetic algorithm, AutoTVM <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> uses simulated annealing and boosted tree, Reagen et. al, <ref type="bibr" target="#b40">[41]</ref> uses Bayesian optimization, RELEASE <ref type="bibr" target="#b3">[4]</ref> uses RL to formulated a more guided search by ML technique. However, these ML-based algorithms need to work in a pre-defined rigid design space, where the level of parallelism is restricted, and hence the mapping space is constrained. The mappers in Timeloop <ref type="bibr" target="#b34">[35]</ref> and Simba <ref type="bibr" target="#b45">[46]</ref> explore the full search space; however, they rely on exhaustive/random search. In this work, we explore a full search space, but with a ML-based guided search method with sample efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Finding optimum mappings of DNNs on accelerators is critical for performance, but is challenging to automate due to an extremely large layer-specific and HW-specific search-space. In this paper, we propose a GAMMA, a genetic algorithm-based technique for the HW-mapping problem. GAMMA consistently outperforms other search techniques. With new DNN models and new accelerators being proposed at an unprecedented rate, GAMMA allows researchers to quickly explore the HW efficiency of emerging DNNs without time-consuming human-in-the-loop mapping and tuning processes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The six dimensions of a CONV operation. K: output channels, C: input channels, Y: input height, X: input width, R: filter height, S: filter width, Y': output height, X': output width. (b) A 3-level mapping example on input activations. Each level represents parallelism across a spatial dimension of the accelerator. (c) Physical mapping on accelerator. The L1-mapper parallelizes dim C across rows, L2-mapper parallelizes dim Y across cols, and L3mapper parallelizes dim K across multiple arrays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The HW performance of randomly sampled HW-mapping in the design space on an example layer (second layer of VGG16). (a) The Energy to Latency plot, (b) The Latency to Area plot, and (c) The HW performance statistics of the sampled HW mappings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The GAMMA encoding example of (a) 1-level mapper and (b) 2-level mapper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) GAMMA's description of a 2-level mapper and (b) its decoded description for cost model (MAESTRO) of a NVDLA-like [1] 2-level mapper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a)The structure and algorithm flow of GAMMA, and (b) The summary of evolution in GAMMA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 (</head><label>4</label><figDesc>a) is a mapper description in GAMMA and Fig. 4(b) is its corresponding description in the cost model (MAESTRO). The order of K, C, X, Y, R, S from left to right in (a) and top to bottom in (b) reflect the computation order. The number paired with dimension in (a) and the number inside the bracket in (b) reflects the tiling size on each dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The performance of found solutions across a suite of optimization methods on different target systems (S1, S2, S3) and different platform constraints (Edge, Cloud) for ResNet-18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: GAMMA's found mapping of early, medium, and late layer of ResNet-18 in Fig. 6(e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The energy consumption of S2 on ResNet-18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: End-to-end latency improvement over generations with GAMMA for S3 system and edge platform constraint.</figDesc><graphic url="image-9.png" coords="7,437.99,374.09,120.26,84.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Terminology in Genetic Algorithm (GA).</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : The HW resources in different platforms.Table 3 : Three target systems (Accel's infrastructures).</head><label>23</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 : Baseline optimization methods.</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 : Two stage optimization for inter-layer parallelism on ResNet-18 * and VGG16 ? for a multi-accelerator (S3) pipelined de- ployment. In the 1st state, we optimize for latency and identify the bottleneck layer (highlighted in bold), which determines the pipeline latency. In the 2nd stage, we optimize for energy (or power) by allowing the latency of other layers to increase, while staying less than the pipeline latency.</head><label>6</label><figDesc>We only display the layers with unique shape. Maximum and Average are calculated based on all 20 layers of ResNet-18. ? We display the summary of VGG16 for the interest of space.</figDesc><table /><note><p>*</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The found solutions also show that by relaxing some tile size heuristics such as deciding tile size by the integral multiple of PEs array sizes could help reach better solutions.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">NVDLA Deep Learning Accelerator</title>
		<ptr target="http://nvdla.org" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">MAESTRO tool</title>
		<ptr target="http://maestro.ece.gatech.edu/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI 16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Byung</forename><surname>Hoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahn</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12799</idno>
		<title level="m">Reinforcement Learning and Adaptive Sampling for Optimized DNN Compilation</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Origami: A convolutional network accelerator</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Cavigelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th edition on Great Lakes Symposium on VLSI</title>
		<meeting>the 25th edition on Great Lakes Symposium on VLSI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A dynamically configurable coprocessor for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Srimat</forename><surname>Chakradhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="269" to="284" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3389" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI 18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ISCA</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="367" to="379" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin And</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISSCC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="262" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scheduling multiprocessor tasks with genetic algorithms</title>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">C</forename><surname>Corr?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="825" to="837" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A Comparison of Active and Passive Portfolio Management</title>
		<author>
			<persName><forename type="first">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DMazerunner: Executing perfectly nested loops on dataflow accelerators</title>
		<author>
			<persName><forename type="first">Dave</forename><surname>Shail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TECS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ShiDianNao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neuflow: A runtime reconfigurable dataflow processor for vision</title>
		<author>
			<persName><forename type="first">Cl?ment</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cvpr 2011 Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tetris: Scalable and efficient neural network acceleration with 3d memory</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="751" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tangram: Optimized coarse-grained dataflow for scalable NN accelerators</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="807" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The CMA evolution strategy: a comparing review</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards a new evolutionary computation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="75" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evolution under strong noise: A self-adaptive evolution strategy can reach the lower performance bound-the pccmsa-es</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hellwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on PPSN3</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Genetic algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific american</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A genetic algorithm for multiprocessor scheduling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA. IEEE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICNN</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The tensor algebra compiler</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Kjolstad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>OOPSLA</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Loo. py: transformation-based code generation for GPUs and CPUs</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kl?ckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLA Workshop on Libraries, Languages, and Compilers for Array Programming</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="82" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Maeri: Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="461" to="475" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepx: A software accelerator for low-power deep learning inference on mobile devices</title>
		<author>
			<persName><surname>Nicholas D Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPSN. IEEE</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Wenyan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="553" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimizing loop operation and dataflow in FPGA acceleration of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Timeloop: A systematic approach to dnn accelerator evaluation</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<editor>ISPASS. IEEE</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">4.6 A1. 93TOPS/W scalable deep learning/inference processor with tetra-parallel MIMD architecture for big-data applications</title>
		<author>
			<persName><forename type="first">Seongwook</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISSCC Digest of Technical Papers. IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Memory-centric accelerator design for convolutional neural networks. In 31st ICCD</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Peemen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Differential evolution</title>
		<author>
			<persName><surname>Kenneth V Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Optimization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="187" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Nevergrad -A gradient-free optimization platform</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<ptr target="https://GitHub.com/FacebookResearch/Nevergrad" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A case for efficient accelerator design space exploration via Bayesian optimization</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Reagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISLPED. IEEE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Step-Size Adaptation Based on Non-Local Use of Selection Information</title>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Rechenberg</surname></persName>
		</author>
		<idno>PPSN3</idno>
	</analytic>
	<monogr>
		<title level="m">Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. frommann-holzbog</title>
		<meeting><address><addrLine>Stuttgart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1973">1994. 1973. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolution strategies as a scalable alternative to reinforcement learning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stochastic superoptimization</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Schkufza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Simba: Scaling deep-learning inference with multi-chip-module-based architecture</title>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Maximizing CNN accelerator efficiency through resource partitioning</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="535" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Genetic simulated annealing for scheduling datadependent tasks in heterogeneous environments</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Shroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Heterogeneous Computing Workshop (HCW&apos;96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="98" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mapping and scheduling heterogeneous task graphs using genetic algorithms</title>
		<author>
			<persName><forename type="first">Harmel</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th IEEE heterogeneous computing workshop (HCW&apos;96)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="86" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">HyPar: Towards hybrid parallelism for deep learning accelerator array</title>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="56" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards efficient microarchitectural design for accelerating unsupervised gan-based deep learning</title>
		<author>
			<persName><forename type="first">Mingcong</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lift: a functional data-parallel IR for high-performance GPU code generation</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Steuwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="74" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Optimally scheduling CNN convolutions for efficient memory access</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Stoutchinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01492</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning</title>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Petroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Such</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks</title>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA&apos;16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Tensor comprehensions: Framework-agnostic highperformance machine learning abstractions</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scaledeep: A scalable compute architecture for learning and evaluating deep networks</title>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">DeepTools: Compiler and Execution Runtime Extensions for RaPiD AI Accelerator</title>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="102" to="111" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A genetic-algorithm-based approach for task matching and scheduling in heterogeneous computing environments</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Heterogeneous Computing Workshop</title>
		<meeting>Heterogeneous Computing Workshop</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="72" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">DLVM: A modern compiler infrastructure for deep learning systems</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03016</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs</title>
		<author>
			<persName><forename type="first">Xuechao</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Interstellar: Using Halide&apos;s Scheduling Language to Analyze DNN Accelerators</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="369" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Optimizing fpga-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Optimizing fpga-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
