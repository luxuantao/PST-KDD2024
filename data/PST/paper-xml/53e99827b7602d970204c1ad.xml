<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parameterized Animation Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziyad</forename><forename type="middle">S</forename><surname>Hakura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jerome</forename><forename type="middle">E</forename><surname>Lengyel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Snyder</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parameterized Animation Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7AE2F1C98C3F8A596C208E5A3236DE01</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We generalize image-based rendering by exploiting texture-mapping graphics hardware to decompress ray-traced "animations". Rather than 1D time, our animations are parameterized by two or more arbitrary variables representing view/lighting changes and rigid object motions. To best match the graphics hardware rendering to the input ray-traced imagery, we describe a novel method to infer parameterized texture maps for each object by modeling the hardware as a linear system and then performing least-squares optimization. The parameterized textures are compressed as a multidimensional Laplacian pyramid on fixed size blocks of parameter space. This scheme captures the coherence in parameterized animations and, unlike previous work, decodes directly into texture maps that load into hardware with a few, simple image operations. We introduce adaptive dimension splitting in the Laplacian pyramid and separate diffuse and specular lighting layers to further improve compression. High-quality results are demonstrated at compression ratios up to 800:1 with interactive playback on current consumer graphics cards.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The central problem of computer graphics is real-time rendering of physically illuminated, dynamic environments. Though the computation needed is beyond current capability, specialized graphics hardware that renders texture-mapped polygons continues to get cheaper and faster. We exploit this hardware to decompress animations computed and compiled offline. The decompressed imagery retains the full gamut of stochastic ray tracing effects, including indirect lighting with reflections, refractions, and shadows.</p><p>For synthetic scenes, the time and viewpoint parameters of the plenoptic function <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> can be generalized to include position of lights, viewpoint, or objects, surface reflectance properties, or any other degrees of freedom in the scene. For example, we can construct a 2D space combining viewpoint movement along a 1D trajectory with independent 1D swinging of a light source. Our goal is maximum compression of the resulting arbitrary-dimensional parameterized animation that maintains satisfactory quality and decodes in real time. Once the encoding is downloaded over a network, the decoder can take advantage of specialized hardware and high bandwidth to the graphics system to allow a user to explore the parameter space. High compression reduces downloading time over the network and conserves server and client storage.</p><p>Our approach infers and compresses parameter dependent texture maps for individual objects rather than combined views of the entire scene, illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>To infer a texture map means to find one which when applied to a hardware-rendered geometric object matches the offline-rendered image. Encoding a separate texture map for each object better captures its coherence across the parameter space independently of where in the image it appears. Object silhouettes are correctly rendered from actual geometry and suffer fewer compression artifacts. Figure <ref type="figure">2</ref> illustrates our system. Ray-traced images at each point in the parameter space are input to the compiler together with the scene geometry, lighting models, and viewing parameters. The compiler targets any desired type of graphics hardware by modeling the hardware as a linear system. It then infers texture resolution and texture samples for each object at each point in the parameter space to produce as good a match as possible on that hardware to the "gold-standard" images. We use pyramidal regularization <ref type="bibr" target="#b19">[20]</ref> in our texture inference to provide smooth "hole-filling" for occluded regions without a specialized post-processing pass. Per-object texture maps are then compressed using a novel, multi-dimensional compression scheme that automatically allocates storage between different objects and their separated diffuse and specular lighting layers. The interactive runtime consists of a traditional hardware-accelerated rendering engine and a texture decompression engine that caches to speed decoding and staggers block origins to distribute decompression load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>Image-Based Rendering (IBR). IBR has sought increasingly accurate approximations of the plenoptic function <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, including use of pixel flow <ref type="bibr" target="#b3">[4]</ref>, and tabulation of a 4D field, called a "light field" or "lumigraph" <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref> to interpolate views. Layered depth images (LDI) <ref type="bibr" target="#b28">[30]</ref> are another representation of the radiance field better able to handle disocclusions and have found use in the rendering of glossy environments <ref type="bibr" target="#b1">[2]</ref>. Extending to a 5D or higher field permits changes to the lighting environment <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b25">27]</ref>. The challenge of such methods is efficient storage of the high-dimensional image fields.</p><p>For spatially coherent scenes, it has been observed that geometry-based surface fields better capture coherence in the light field, achieving a more efficient encoding than view-based images like the LDI or lumigraph <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b25">27]</ref>. Our work generalizes parameterizations based solely on viewpoint. We also encode an entire texture at each point in parameter space that can be accessed in constant time independent of the size of the whole representation. Other encoding strategies, such as Miller's <ref type="bibr" target="#b23">[25]</ref>, must visit an irregular scattering of samples over the entire 4D space to reconstruct the texture for a particular view and thus make suboptimal use of graphics hardware.</p><p>Another IBR hybrid uses view-dependent textures (VPT) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5]</ref> in which geometric objects are texture-mapped using a projective mapping from view-based images. VPT methods depend on viewpoint movement for proper antialiasing -novel views are generated by reconstructing using nearby views that see each surface sufficiently "head-on". Such reconstruction is incorrect for highly specular surfaces. We instead infer texture maps that produce antialiased reconstructions independently at each parameter location, even for spaces without viewpoint change. We also use "intrinsic" texture parameteri-zations (i.e., viewpoint-independent (u, v) coordinates per vertex given as input on each mesh) rather than view-based ones. We can then capture the view-independent lighting in a single texture map rather than a collection of views to obtain better compression.</p><p>Interactive Photorealism. Another approach to interactive photorealism seeks to improve hardware shading models rather than fully tabulating radiance. Diefenbach <ref type="bibr" target="#b7">[8]</ref> used shadow volumes and recursive hardware rendering to compute approximations to global rendering. Even using many parallel graphics pipelines (8 for <ref type="bibr" target="#b32">[34]</ref>) these approaches can only handle simple scenes, and, because of limitations on the number of passes, do not capture all the effects of a full offline photorealistic rendering, including multiple bounce reflections and refractions and accurate shadows.</p><p>Texture Recovery/Model Matching. The recovery of texture maps from images is closely related to surface reflectance estimation in computer vision <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">39]</ref>. We greatly simplify the problem by using known geometry and separating diffuse and specular lighting layers during the offline rendering. We focus instead on the problem of inferring textures for particular graphics hardware that "undo" its undesirable properties, like poor-quality texture filtering. A related idea is to compute the best hardware lighting to match a gold standard <ref type="bibr" target="#b36">[38]</ref>. Separating diffuse from specular shading to better exploit temporal and spatial coherence is a recurring theme in computer graphics <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Compression. Various strategies for compressing the dual-plane lumigraph parameterization have been proposed. Levoy et al. <ref type="bibr" target="#b17">[18]</ref> used vector quantization and entropy coding to get compression ratios up to 118:1 while Lalonde et al. <ref type="bibr" target="#b14">[15]</ref> used a wavelet basis with compression ratios of 20:1. Miller et al. <ref type="bibr" target="#b23">[25]</ref> compressed the 4D surface light field using a block-based DCT encoder with compression ratios of 20:1. Nishino et al. <ref type="bibr" target="#b25">[27]</ref> used an eigenbasis to encode surface textures achieving compression ratios of 20:1 with eigenbases having 8-18 vectors.</p><p>Another relevant area of work is animation compression. Standard video compression uses simple block-based transforms and image-based motion prediction. Wallach et al. <ref type="bibr" target="#b35">[37]</ref> used rendering hardware to accelerate standard MPEG encoding. Guenter et al. <ref type="bibr" target="#b10">[11]</ref> observed that compression is greatly improved by exploiting information available in synthetic animations. Levoy <ref type="bibr" target="#b16">[17]</ref> showed how simple graphics hardware could be used to match a synthetic image stream produced by a simultaneously-executing, high-quality server renderer by exploiting polygon rendering and transmitting a residual signal. We extend this work to the matching of multidimensional animations containing non-diffuse, offline-rendered imagery using texture-mapping graphics hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parameterized Texture Inference</head><p>To infer the texture maps that best match the input gold-standard rendered frames, we first model the graphics hardware as a large sparse linear system (Section 3.3), and then perform a least-squares optimization on the resulting system (Section 3.4). To achieve a good encoding, we first segment the input images (Section 3.1), and choose an appropriate texture domain and resolution (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Segmenting Ray-Traced Images</head><p>Each geometric object has a parameterized texture that must be inferred from the raytraced images. These images are first segmented into per-object pieces to prevent bleeding of information from different objects across silhouettes. To perform per-object segmentation, the ray tracer generates a per-object mask as well as a combined image, all at supersampled resolution. For each object, we filter the portion of the combined image indicated by the mask and divide by the fractional coverage computed by applying the same filter to the object's mask.</p><p>A second form of segmentation separates the view-dependent specular information from the view-independent diffuse information for the common case that the parameter space includes at least one view dimension. Figure <ref type="figure" target="#fig_2">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimizing Texture Coordinates and Resolutions</head><p>Since parts of an object may be occluded or off-screen, only part of its texture domain is accessed. The original texture coordinates of the geometry are used as a starting point and then optimized so as to: 1) to ensure adequate sampling of the visible texture image with as few samples as possible, 2) to allow efficient computation of texture coordinates at run-time, and 3) to minimize encoding of the optimized texture coordinates. To satisfy the last two goals, we choose and encode a parameter-dependent affine transformation on the original texture coordinates rather than re-specify them at each vertex. One affine transformation is chosen per object per block of parameter space (see Section 4). The first step of the algorithm finds the linear transformation, R(u, v), minimizing the following objective function, inspired by <ref type="bibr" target="#b20">[21]</ref> </p><formula xml:id="formula_0">R(u, v)= a b c d u v , f(x) = edges i W i s i -R(u i0 , v i0 ) -R(u i1 , v i1 ) min (s i , R(u i0 , v i0 ) -R(u i1 , v i1 ) ) 2 (1)</formula><p>where s i represents the length on the screen of a particular triangle edge, i 0 and i 1 represent the edge vertices, and W i is a weighting term which sums screen areas of triangles on each side of the edge. At each point in the parameter block, the sum in f is taken over visible triangle edges determined by rasterizing triangle identifiers into a zbuffer after clipping to the view frustum.</p><p>This minimization choses a mapping that is as close to an isometry as possible by minimizing length difference between triangle edges in texture space and projected to the image. We divide by the minimum edge length so as to equally penalize edges that are an equal factor longer and shorter. ∇f (x) is calculated analytically for use in conjugate gradient minimization.</p><p>In the second step, we ensure that the object's texture map contains enough samples by scaling the R found previously. We check the greatest local stretch (singular value) across all screen pixels in which the object is visible, using the Jacobian of the mapping from texture to screen space. If the maximum singular value exceeds a threshold, we scale R by the maximum singular value in the corresponding direction of maximal stretch, and iterate until the maximum singular value is reduced below the threshold. This essentially adds more samples to counteract the worst-case stretching of the projected texture.</p><p>Finally, the minimum-area bounding rectangle on the transformed texture coordinates determines the resulting texture resolution and affine texture transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modeling Hardware Rendering as a Linear System</head><p>A simple texture inference algorithm maps each texel to the image and then filters the neighboring region to reconstruct the texel's value <ref type="bibr" target="#b21">[22]</ref>. One problem with this approach is reconstruction of texels near arbitrarily-shaped object boundaries and occluded regions (Figure <ref type="figure" target="#fig_2">3-d,e</ref>). Such occluded regions produce undefined texture samples which complicates building of MIPMAPs. Finally, the simple algorithm does not take into account how texture filtering is performed on the target graphics hardware.</p><p>A more principled approach is to model the hardware texture mapping operation in the form of a linear system:</p><formula xml:id="formula_1">A                s 0,0 filter coefficients s 0,1 filter coefficients . . . s m-1,n-1 filter coefficients                x                x 0 0,0 . . . x 0 u-1,v-1 level 0 x 1 0,0 . . . x 1 u 2 -1, v 2 -1    level 1 . . . x l-1 0,0 . . . x l-1 u 2 l-1 -1, v 2 l-1 -1    level l -1                = b                s 0,0 s 0,1 . . . s m-1,n-1               <label>(2)</label></formula><p>where vector b contains the ray-traced image to be matched, matrix A contains the filter coefficients applied to individual texels by the hardware, and vector x represents the texels from all l-1 levels of the MIPMAP to be inferred. Superscripts in x entries represent MIPMAP level and subscripts represent spatial location. This model ignores hardware nonlinearities in the form of rounding and quantization. All three color components of the texture share the same matrix A. Each row in matrix A corresponds to a particular screen pixel, while each column corresponds to a particular texel in the texture's MIPMAP pyramid. The entries in a given row of A represent the hardware filter coefficients that blend texels to produce the color at a given screen pixel. Hardware filtering requires only a small number of texel accesses per screen pixel, so the matrix A is very sparse. We use hardware z-buffering to determine object visibility on the screen, and need only consider rows (screen pixels) where the object is visible. Filter coefficients should sum to one in any row. Accuracy of inferred filter coefficients is limited by the color component resolution of the framebuffer, typically 8 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Obtaining Matrix</head><p>To accelerate the simple algorithm, we observe that multiple columns in the matrix A can be filled in parallel as long as texel projections do not overlap on the screen and we can determine which pixels derive from which texels. An algorithm that subdivides texture space and checks that alternate texture block projections do not overlap can be devised based on this observation. A better algorithm recognizes that since just a single color component is required to infer the matrix coefficients, the other color components (typically 16 or 24 bits) can be used to store a unique texel identifier that indicates the destination column for storing the filtering coefficient. With this algorithm, described in-depth in <ref type="bibr" target="#b11">[12]</ref>, the matrix A can be inferred in 108 renderings, independent of texture resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Least-Squares Solution</head><p>Removing irrelevant image pixels from Equation (2), A becomes an n s × n t matrix, where n s is the number of screen pixels in which the object is visible, and n t is the number of texels in the object's texture MIPMAP pyramid. Once we have obtained the matrix A, we solve for the texture represented by the vector x by minimizing a function f (x) defined via</p><formula xml:id="formula_2">f (x) = Ax -b 2 , ∇f (x) = 2A T (Ax -b) (3)</formula><p>subject to the constraint 0 ≤ x k i,j ≤ 1. Given the gradient, ∇f (x), the conjugate gradient method can be used to minimize f (x). The main computation of the solution's inner loop multiplies A with a vector x representing the current solution estimate and, for the gradient, A T with Ax -b. Since A is a sparse matrix with each row containing a small number of nonzero elements (exactly 8 with trilinear filtering), the cost of multiplying A or A T with a vector is proportional to n s . Another way to express f (x) and ∇f (x) is:</p><formula xml:id="formula_3">f (x) = xA T Ax -2x • A T b + b • b, ∇f (x) = 2A T Ax -2A T b (4)</formula><p>In this formulation, the inner loop's main computation multiplies A T A, an n t × n t matrix, with a vector. Since A T A is also sparse, though less so than A, the cost of multiplying A T A with a vector is proportional to n t . We use the following heuristic to decide which set of equations to use:</p><formula xml:id="formula_4">if (2n s ≥ Kn t ) Use A T A method: Equation (4) else Use A method: Equation (3)</formula><p>where K is a measure of relative sparsity of A T A compared to A. We use K = 4. The factor 2 in the test arises because Equation (3) requires two matrix-vector multiplies while Equation ( <ref type="formula">4</ref>) only requires one. The solver can be sped up by using an initial guess vector x that interpolates the solution obtained at lower resolution. The problem size can then be gradually scaled up until it reaches the desired texture resolution. Alternatively, once a solution is found at one point in the parameter space, it can be used as an initial guess for neighboring points, which are immediately solved at the desired texture resolution.</p><p>Segmenting the ray-traced images into view-dependent and view-independent layers allows us to collapse the view-independent textures across multiple viewpoints. To compute a single diffuse texture, we solve the following problem:</p><formula xml:id="formula_5">A     A v0 A v1 . . . A vn-1     x = b     b v0 b v1 . . . b vn-1    <label>(5)</label></formula><p>where matrix A concatenates the A matrices for the individual viewpoints v 0 through v n-1 , vector b concatenates the ray-traced images at the corresponding viewpoints, and vector x represents the single diffuse texture to be solved.</p><p>Regularization. One of the consequences of setting up the texture inference problem in the form of Equation ( <ref type="formula" target="#formula_1">2</ref>) is that only texels actually used by the graphics hardware are solved, leaving the remaining texels undefined. To support movement away from the original viewpoint samples and to make the texture easier to compress, all texels should be defined. This can be achieved with pyramidal regularization of the form:</p><formula xml:id="formula_6">f reg (x) = f (x) + ε n s n t Γ(x)<label>(6)</label></formula><p>In te rp o la te where Γ(x) takes the difference between texels at each level of the MIPMAP with an interpolated version of the next coarser level as illustrated in Figure <ref type="figure">4</ref>. The objective function f sums errors in screen space, while the regularization term sums errors in texture space. This requires a scale of the regularization term by n s /n t . We compute ∇f reg analytically. This regularizing term essentially imposes a filter constraint between levels of the MIPMAP, with user-defined strength ε ≥ 0. We currently define Γ using simple bilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Texture Inference Results</head><p>Figure <ref type="figure">6</ref> (see appendix) shows results of our least squares texture inference on a glass parfait object. The far left of the top row (a) is the original image to be matched. The next three columns are hardware-rendered from inferred textures using three filtering modes: bilinear, trilinear, and anisotropic. <ref type="foot" target="#foot_0">1</ref> The corresponding texture maps are shown in the first three columns of the next row (b). These three examples did not use pyramidal regularization. <ref type="foot" target="#foot_1">2</ref> Most of the error in these examples is incurred on the parfait's silhouettes due to mismatch between hardware and ray-traced rendering.</p><p>Bilinear filtering provides the sharpest, most accurate result because it uses only the finest level MIPMAP and thus has the highest frequency domain with which to match the original. Trilinear MIPMAP filtering produces a somewhat worse result, and anisotropic filtering is in between. Observe (Fig. <ref type="figure">6b</ref>) that more texture area is filled from the finest pyramid level for anisotropic filtering compared to trilinear, especially near the parfait stem, while bilinear filtering altogether ignores the higher MIPMAP levels. Bilinear filtering produces this highly accurate result only at the exact parameter values (e.g., viewpoint locations) and image resolutions where the texture was inferred. The other schemes are superior if viewpoint or image resolution are changed from those samples.</p><p>The next two columns show results of pyramidal regularization with anisotropic filtering. Inference with ε=0.1 is almost identical to inference with no pyramidal regularization (labeled "anisotropic"), but ε=0.5 causes noticeable blurring. Regularization makes MIPMAP levels tend toward filtered versions of each other; we exploit this by compressing only the finest level and re-creating higher levels by on-the-fly decimation.</p><p>Finally, the far right column in (a) shows the "forward mapping" method in which texture samples are mapped to the object's image layer and interpolated using a highquality filter (we used a separable Lanczos-windowed sinc function). To handle occlusions, we first filled undefined samples with a simple boundary-reflection algorithm. Forward mapping produces a blurry and inaccurate result because it does not account for how graphics hardware filters the textures. In addition, reflection hole-filling produces artificial, high-frequency information in occluded regions that is expensive to encode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parameterized Texture Compression</head><p>The multidimensional field of textures for each object is compressed by subdividing into parameter space blocks as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Larger block sizes better exploit coherence but are more costly to decode during playback; we used 8 × 8 blocks in our 2D examples. Adaptive Laplacian Pyramid. We encode parameterized texture blocks using a Laplacian pyramid <ref type="bibr" target="#b2">[3]</ref> where the "samples" at each pyramid level are entire 2D images. We use standard 2D compression (e.g., JPEG and SPIHT <ref type="bibr" target="#b26">[28]</ref> encodings) to exploit spatial coherence over (u, v) space. Each level of the Laplacian pyramid thus consists of a series of encoded 2D images. Parameter and texture dimensions are treated asymmetrically because parameters are accessed along an unpredictable 1D subspace selected by the user at run-time. We avoid processing large fractions of the representation to decode a given parameter sample by using the Laplacian pyramid with small block size, requiring just log 2 (n) simple image additions where n is the number of samples in each dimension of the block. Furthermore, graphics hardware can perform the necessary image additions using multiple texture stages, thus enabling on-the-fly decompression. Image coders often assume that both image dimensions are equally coherent. This is untrue of parameterized animations where, for example, the information content in a viewpoint change can greatly differ from that of a light source motion. To take advantage of differences in coherence across different dimensions, we use an adaptive Laplacian pyramid that subdivides more in dimensions with less coherence, illustrated in Figure <ref type="figure">5</ref>. Coarser levels still have 4 times fewer samples. Automatic Storage Allocation. To encode the Laplacian pyramid, storage must be assigned to its various levels. We apply standard bit allocation techniques from signal compression <ref type="bibr" target="#b8">[9]</ref>. Curves of mean squared error (MSE) versus storage, called rate/distortion curves, are plotted for each pyramid level and points of equal slope on each curve selected subject to a total storage constraint. We effectively minimize the sum of MSEs across all levels of the pyramid, because a texture image at a given point in parameter space is reconstructed as a sum of images from each level, so an error in any level contributes equally to the resulting error.</p><p>There is also a need to perform storage allocation across objects; that is, to decide how much to spend in the encoding of object i's texture vs. object j's. We use the same method as for allocating between pyramid levels, except that the error measure is A i E i , where A i is the screen area and E i the MSE of object i. This minimizes the sum of squared errors on the screen no matter how the screen area is decomposed into objects. When objects have both specular and diffuse reflectance, our error measure sums across these lighting layers, each with an independent rate distortion curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Runtime System</head><p>The runtime system decompresses and caches texture images, applies affine transformations to vertex texture coordinates, and generates rendering calls to the graphics system. Movement off (or between) the original viewpoint samples is allowed by rendering from that viewpoint using the closest texture sample. Higher-order interpolation would improve smoothness at the expense of more texture map accesses. The texture caching system decides which textures to keep in memory in decompressed form. Because the user's path through parameter space is unpredictable, we use an adaptive caching strategy that reclaims memory when the number of frames since last use exceeds a given lifetime. Images near the top of the pyramid are more likely to be reused and are thus assigned longer lifetimes. See <ref type="bibr" target="#b11">[12]</ref> for more details.</p><p>If blocks of all objects are aligned, then many simultaneous cache misses occur whenever the user crosses a block boundary, creating a computational spike as multiple levels in the new blocks' Laplacian pyramids are decoded. We mitigate this problem by staggering the blocks -using different block origins for different objects. Our least-squares texture inference method created parameterized textures for each object. The resulting texture fields were compressed using a variety of methods, including adaptive 2D Laplacian pyramids of both DCT-and SPIHT-encoded levels. To test the benefits of the Laplacian pyramid, we also encoded each block using MPEG on a 1D zig-zag path through the parameter space varying most rapidly along the dimension of most coherence. A state-of-the-art MPEG4 encoder [24] was used. Finally, we compared against direct compression of the original images (rather than renderings using compressed textures), again using MPEG4.</p><p>Figure <ref type="figure">8</ref> shows the results at two targeted compression rates: 384:1 (middle row) and 768:1 (bottom row). All texture-based images were generated on graphics hardware using 2 × 2 antialiasing; their MSEs were computed from the framebuffer contents, averaged over an entire block. Both Laplacian pyramid texture encodings (right two columns) achieve reasonable quality at 768:1, and quite good quality at 384:1. The view-based MPEG encoding, "MPEG-view", is inferior with obvious block artifacts on object silhouettes, even though MPEG encoding constraints did not allow as much compression as the other examples.</p><p>For MPEG encoding of textures we tried two schemes: one using a single I-frame per block, and another using 10 I-frames. The decoding complexity for 10I/block is roughly comparable to our DCT Laplacian pyramid decoding. Single I-frame/block maximizes compression. The 10I/block MPEG-texture results have obvious block artifacts at both quality levels especially on the vase and background wallpaper. The 1I/block MPEGtexture results are better <ref type="foot" target="#foot_2">3</ref> , but still inferior to the pyramid schemes at the 768:1 target as MPEG only exploit coherence in one dimension. Unlike the MPEG-view case, the MPEG-texture schemes use our novel features: hardware-targeted texture inference, separation of lighting layers, and optimal storage allocation across objects. System Performance. Average compilation and preprocessing time per point in parameter space is shown in Table <ref type="table" target="#tab_2">1</ref>. It can be seen that total compilation time is a small fraction of the time to produce the ray-traced images.</p><p>To determine playback performance, we measured average and worst-case frame rates (fps) for a diagonal trajectory that visits a separate parameter sample at every frame, shown in Table <ref type="table" target="#tab_3">2</ref>. <ref type="foot" target="#foot_3">4</ref> The performance bottleneck is currently software decoding speed. Reducing texture resolution by an average of 91% using a manually specified reduction factor per object provides acceptable quality at about 31fps with DCT.  <ref type="figure">7</ref> (appendix) for encodings using MPEG-view and Laplacian SPIHT.</p><p>In this example, the parameter space is much more coherent in the rotation dimension than in the view dimension, because gewgaw rotation only changes the relatively small reflected or refracted image of the gewgaw in the other objects. MPEG can exploit this coherence very effectively using motion compensation along the rotation dimension, and so the difference between our approach and MPEG is less than in the previous example. Though our method is designed to exploit multidimensional coherence and lacks motion compensation, our adaptive pyramid produces a slightly better MSE and a perceptually better image.</p><p>Real-time performance for this demo is approximately the same as for demo1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>Synthetic imagery can be very generally parameterized with combinations of view, light, or object positions, among other parameters, to create a multidimensional animation. While real-time graphics hardware fails to capture full ray-traced shading effects, it does provide a useful operation for decoding such animations compiled beforehand: texturemapped polygon rendering. We encode a parameterized animation using parameterized texture maps, exploiting the great coherence in these animations better than view-based representations. This paper describes how to infer parameterized texture maps from segmented imagery to obtain a close match to the original and how to compress these maps efficiently, both in terms of storage and decoding time. Results show that compression factors up to 800:1 can be achieved with good quality and real-time decoding.</p><p>Our simple sum of diffuse and specular texture maps is but a first step toward more predictive graphics models supported by hardware to aid compression. Examples include parameterized environment maps, hardware shadowing algorithms, and per-vertex shading models. The discipline of measuring compression ratios vs. error for encoding photorealistic imagery is a useful benchmark for proposed hardware enhancements.</p><p>Other extensions include use of perceptual metrics for guiding compression and storage allocation, handling nonrigidly deforming geometry and photorealistic camera models, and automatic generation of texture parameterizations. Finally, we are interested in measuring storage requirements with growing dimension of the parameter space and hypothesize that such growth is quite small in many useful cases. There appear to be two main impediments to increasing the generality of the space that can be explored: slowness of offline rendering and decompression. The first obstacle may be addressed by better exploiting coherence across the parameter space in the offline renderer, using ideas similar to <ref type="bibr" target="#b12">[13]</ref>. The second can be overcome by absorbing some of the decoding functionality into the graphics hardware. We expect the ability to load compressed textures directly to hardware in the near future. A further enhancement would be to load compressed parameter-dependent texture block pyramids. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An 8 × 8 block of parameterized textures for a glass parfait object is shown. In this example, dimension p1 represents a 1D viewpoint trajectory while p2 represents the swinging of a light source. Note the imagery's coherence.</figDesc><graphic coords="1,277.13,539.22,95.89,91.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>illustrates segmentation for an example ray-traced image. We use a modified version of Eon, a Monte Carlo distribution ray-tracer<ref type="bibr" target="#b29">[31]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmentation of Ray-Traced Images. (a) Complete Image, (b,c) Segmentation into diffuse and specular layers respectively, (d,e) Examples of further segmentation into per object layers.</figDesc><graphic coords="4,127.30,252.37,68.16,81.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>A. A simple but impractical algorithm for obtaining A examines the screen output from a series of renderings, each setting only a single texel of interest to a nonzero value, as follows: Initialize z-buffer with visibility information by rendering entire scene For each texel in MIPMAP pyramid, Clear texture, and set individual texel to maximum intensity Clear framebuffer, and render all triangles that compose object For each non-zero pixel in framebuffer, Divide screen pixel value by maximum framebuffer intensity Place fractional value in A[screen pixel row][texel column]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 Fig. 4 .</head><label>24</label><figDesc>Fig. 4.Pyramidal regularization is computed by taking the sum of squared differences between texels at each level of the MIPMAP with the interpolated image of the next higher level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 Fig. 5 .</head><label>35</label><figDesc>Fig. 5. Adaptive Laplacian Pyramid</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 7 .Fig. 8 .</head><label>678</label><figDesc>Fig. 6. Texture Inference Results: (a) shows close-ups of the projected texture, compared to the original rendering on the far left. The highlight within the red box is a good place to observe differences. The next row shows the inverted error signal, scaled by a factor of 20, over the parfait. The bottom row contains the mean-squared error (MSE) from the original image. (b) shows corresponding texture maps. Pink regions represent undefined regions of the texture.</figDesc><graphic coords="13,126.04,570.34,113.44,116.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>6 Results 6.1 Demo1: Light × View Compression Results.</head><label></label><figDesc>The first example scene (Figure8in appendix, top) consists of 6 static objects (4384 triangles): a reflective vase, glass parfait, reflective table top, table stand, walls, and floor. The 2D parameter space has 64 viewpoint samples circling around the table at 1.8 • /sample and 8 different positions of a swinging, spherical light source. The image field was encoded using eight 8×8 parameter space blocks, each requiring storage 640×480×3×8×8= 56.25MB/block.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Compilation</figDesc><table><row><cell></cell><cell>time</cell></row><row><cell>texture coord. opt.</cell><cell>1 sec</cell></row><row><cell cols="2">solving for textures 4.83 min</cell></row><row><cell>compression</cell><cell>.58 min</cell></row><row><cell>total compilation</cell><cell>5.43 min</cell></row><row><cell>ray tracing</cell><cell>5 hours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Runtime performance</figDesc><table><row><cell cols="2">Encoding Texture</cell><cell cols="2">Worst fps Average fps</cell></row><row><cell cols="2">Laplacian undecimated</cell><cell>2.46</cell><cell>4.76</cell></row><row><cell>DCT</cell><cell>decimated</cell><cell>18.4</cell><cell>30.7</cell></row><row><cell cols="2">Laplacian undecimated</cell><cell>0.27</cell><cell>0.67</cell></row><row><cell>SPIHT</cell><cell>decimated</cell><cell>2.50</cell><cell>5.48</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.2 Demo2: View × Object Rotation In</head><label></label><figDesc>the second example, we added a rotating, reflective "gewgaw" on the table. The parameter space consists of a 1D circular viewpoint path, containing 24 samples at 1.5 • /sample, and the rotation angle of the gewgaw, containing 48 samples at 7.5 • /sample. Results are shown in Figure</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Results were achieved with the NVidia Geforce chip supporting anisotropy factors up to</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p><ref type="bibr" target="#b1">2</ref> Without pyramidal regularization, we find that another regularization term is needed to ensure that the texture solution lies in the interval [0, 1]. Refer to<ref type="bibr" target="#b11">[12]</ref> for details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>MSE=25.9 at 768:1 target and MSE=10.1 at 384:1 target compression. See<ref type="bibr" target="#b11">[12]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Measured with Nvidia Geforce 256 chip, 32MB video/16MB AGP memory on Pentium III 733Mhz PC.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Extremely useful advice and discussions were provided by Henrique Malvar, Philip Chou, and Brian Guenter on compression, John Platt on optimization and regularization, and Steve Marschner on solving for texture maps and ray tracing. We thank Anoop Gupta and Turner Whitted for their guidance and support. Peter Shirley's Eon ray tracer was a valuable research tool.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Plenoptic Function and the Elements of Early Vision</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Models of Visual Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Increased Photorealism for Interactive Architectural Walkthroughs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lastra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="1999-04">1999. April 1999</date>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Laplacian Pyramid as a Compact Image Code</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983-04">April 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">View Interpolation for Image Synthesis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 93</title>
		<imprint>
			<date type="published" when="1993-08">August 1993</date>
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Compression for Streaming Texture Intensive Animations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fleishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 99</title>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling and Rendering Architecture from Photographs: A Hybrid Geometry-and Image-Based Approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96</title>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient View-Dependent Image-Based Rendering with Projective Texture Maps</title>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borshukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In 9th Eurographics Rendering Workshop</title>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pipeline Rendering: Interaction and Realism Through Hardware-Based Multi-Pass Rendering</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Diefenbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Vector Quantization and Signal Compression, Kluwer Academic</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="606" to="610" />
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Lumigraph</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96</title>
		<imprint>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion Compensated Compression of Computer Animation Frames</title>
		<author>
			<persName><forename type="first">B</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mersereau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 93</title>
		<imprint>
			<date type="published" when="1993-08">August 1993</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Parameterized Animation Compression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
		<idno>MSR-TR-2000-50</idno>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiple Viewpoint Rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Halle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98</title>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Light Field Techniques for Reflections and Refractions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Eurographics Rendering Workshop</title>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page">375</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactive Rendering of Wavelet Projected Light Fields</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fournier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Interface &apos;99</title>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rendering with Coherent Layers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97</title>
		<imprint>
			<date type="published" when="1997-08">August 1997</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Polygon-Assisted JPEG and MPEG compression of Synthetic Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95</title>
		<imprint>
			<date type="published" when="1995-08">August 1995</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Light Field Rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96</title>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-Based Rendering for Non-Diffuse Synthetic Scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In 9th Eurographics Workshop on Rendering</title>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient Multiscale Regularization with Applications to the Computation of Optical Flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luettgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="64" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive Texture Mapping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verroust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 93</title>
		<imprint>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Inverse Rendering for Computer Graphics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Marschner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Plenoptic Modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95</title>
		<imprint>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lazy Decompression of Surface Light Fields for Pre-computed Global Illumination</title>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poncelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In 9th Eurographics Rendering Workshop</title>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="281" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Implementation and Analysis of an Image-Based GLobal Illumination Framework for Animated Environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nimeroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dorsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="1996-12">Dec. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eigen-Texture Method: Appearance Compression based on 3D Model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June, 1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="618" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A New, Fast, and Efficient Image Codec Based on Set Partitioning in Hierarchical Trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pearlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="1996-06">June 1996</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object Shape and Reflectance Modeling from Observation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97</title>
		<imprint>
			<date type="published" when="1997-08">August 1997</date>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Layered Depth Images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98</title>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monte Carlo Methods for Direct Lighting Calculations</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimmerman</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="1996-01">January 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient Glossy Global Illumination with Interactive Viewing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Scheel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Interface &apos;99</title>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Interactive Rendering of Globally Illuminated Glossy Scenes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Stürzlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bastos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics Rendering Workshop</title>
		<imprint>
			<biblScope unit="page" from="93" to="102" />
			<date type="published" when="1997-06">1997. June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards interactive, photorealistic rendering of indoor scenes: A hybrid approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Udeshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Rendering Workshop 1999</title>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="367" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-based Rendering with Controllable Illumination</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics Rendering Workshop</title>
		<imprint>
			<biblScope unit="page" from="13" to="22" />
			<date type="published" when="1997-06">1997. June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Two-Pass Solution to the Rendering Equation: A Synthesis of Ray-Tracing and Radiosity Methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Greenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987-07">July 1987</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accelerated MPEG Compression of Dynamic Polygonal Scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kunapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 94</title>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fitting Virtual Lights for Non-Diffuse Walkthroughs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alppay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lafortune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97</title>
		<imprint>
			<date type="published" when="1997-08">August 1997</date>
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inverse Global Illumination: Recovering Reflectance Models of Real Scenes From Photographs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hawkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 99</title>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
