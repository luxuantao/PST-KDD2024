<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FROM AUDIO TO SEMANTICS: APPROACHES TO END-TO-END SPOKEN LANGUAGE UNDERSTANDING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-09-24">24 Sep 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Galen</forename><surname>Chuang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neeraj</forename><surname>Gaur</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhongdi</forename><surname>Qu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Austin</forename><surname>Waters</surname></persName>
						</author>
						<title level="a" type="main">FROM AUDIO TO SEMANTICS: APPROACHES TO END-TO-END SPOKEN LANGUAGE UNDERSTANDING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-24">24 Sep 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">BCC76A760DD95F9C27816001E128830E</idno>
					<idno type="arXiv">arXiv:1809.09190v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>spoken language understanding</term>
					<term>sequence-tosequence</term>
					<term>end-to-end training</term>
					<term>multi-task learning</term>
					<term>speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional spoken language understanding systems consist of two main components: an automatic speech recognition module that converts audio to a transcript, and a natural language understanding module that transforms the resulting text (or top N hypotheses) into a set of domains, intents, and arguments. These modules are typically optimized independently. In this paper, we formulate audio to semantic understanding as a sequence-to-sequence problem <ref type="bibr" target="#b1">[1]</ref>. We propose and compare various encoder-decoder based approaches that optimize both modules jointly, in an end-to-end manner. Evaluations on a real-world task show that 1) having an intermediate text representation is crucial for the quality of the predicted semantics, especially the intent arguments and 2) jointly optimizing the full system improves overall accuracy of prediction. Compared to independently trained models, our best jointly trained model achieves similar domain and intent prediction F 1 scores, but improves argument word error rate by 18% relative.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Understanding semantics from a user input or a query is central to any human computer interface (HCI) that aims to interact naturally with users. Spoken dialogue systems that aim to solve this for specific tasks have been a focus of research for more than two decades <ref type="bibr" target="#b2">[2]</ref>. With the widespread adoption of smart devices like Google-Home <ref type="bibr" target="#b3">[3]</ref>, Amazon Alexa, Apple Siri and Microsoft Cortana, spoken language understanding (SLU) is moving to the forefront of HCI.</p><p>Typically, SLU involves multiple modules. An automatic speech recognition system (ASR) first transcribes the user query into a transcript. This is then fed to a module that does natural language understanding (NLU) 1 . NLU itself involves domain classification, intent detection, and slot filling <ref type="bibr" target="#b2">[2]</ref>. In traditional NLU systems, first the high level domain of a transcript is identified. Subsequently, intent detection and slot filling are performed according to the predicted domain's semantic template. Intent detection identifies the finergrained intent class a given transcript belongs to. Slot filling, or argument prediction, 2 is the task of extracting semantic components, like the argument values corresponding to the domain. Figure <ref type="figure" target="#fig_0">1</ref> shows example transcripts and their corresponding domain, intent, and arguments. Recent work <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref> has shown that jointly optimizing these three tasks improves the overall quality of the NLU component. For conciseness, we use the word semantics to refer to all three of domain, intent and arguments. Even though user interactions in an SLU system start as a voice query, most NLU systems assume that the transcript of the request is available or obtained independently. The NLU module is typically optimized independent of ASR. While accuracy of ASR systems have improved over the years <ref type="bibr" target="#b6">[6,</ref><ref type="bibr">7]</ref>, errors in recognition worsen NLU performance. This problem gets exacerbated on smart devices, where interactions tend to be more conversational. However, not all ASR errors are equally bad for NLU. For most applications, the semantics consist of an action with relevant arguments; a large part of the transcript of the ASR module has no impact on the end result as long as intent classification and predicted arguments are accurate. For example, for a user query, "Set an alarm at two o'clock," intent, "alarm," and its arguments, 'two o'clock', are more important than filler words, like 'an'. Joint optimization can focus on improving those aspects of transcription accuracy that are aligned with the end goal, whereas independent optimization fails at that objective. Furthermore, for some applications, there are intents that are more naturally predicted from audio compared to transcript. For example, when training an automated assistant, like Google Duplex <ref type="bibr" target="#b8">[8]</ref> or an airline travel assistant, it would be useful to identify acoustic events like background noise, music and other non-verbal cues as special intents, and tailor the assistant's response accordingly to improve the overall user experience. Hence, training various components of the SLU system jointly can be advantageous.</p><p>There have been some early attempts at using audio to perform NLU. Domain and intent are predicted directly from audio in <ref type="bibr" target="#b9">[9]</ref>, and this approach is shown to perform competitively, but worse than predicting from transcript. Alternatively, using multiple ASR hypothesis <ref type="bibr" target="#b10">[10]</ref> or the word confusion network <ref type="bibr" target="#b11">[11]</ref> or the recognition lattice <ref type="bibr" target="#b12">[12]</ref> have been proposed to account for ASR errors, but independent optimization of ASR and NLU can still lead to sub-optimal performance. In <ref type="bibr" target="#b13">[13]</ref>, an ASR correcting module is trained jointly with NLU component. To account for ASR errors, multiple ASR hypotheses are generated during training as additional input sequences, which are then error-corrected by the slot-filling model. While the slot-filling module is trained to account for the errors, the ASR module is still trained independent of NLU. Similar to the work in <ref type="bibr" target="#b9">[9]</ref>, an end-to-end system is proposed in <ref type="bibr" target="#b14">[14]</ref> that does intent classification directly from speech, with an intermediate ASR task. But unlike the current work, it uses a connectionist temporal classification (CTC) <ref type="bibr" target="#b15">[15]</ref> acoustic model, and only performs intent prediction. In the current work, we show that NLU, i.e., domain, intent, and argument prediction can be done jointly with ASR starting directly from audio and with a quality of performance that matches or surpasses an independently trained counterpart.</p><p>The systems presented in this study are motivated by the encoder-decoder based sequence-to-sequence (Seq2Seq) <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b1">1]</ref> approach that has shown to perform well for machine translation <ref type="bibr" target="#b18">[18]</ref> and speech recognition tasks <ref type="bibr" target="#b19">[19]</ref>. Encoder-decoder based approaches provide an attractive framework to implementing SLU systems, since the attention mechanism allows for jointly learning an alignment while predicting a target sequence that has a many-to-one relationship with its input <ref type="bibr" target="#b20">[20]</ref>. Such techniques have already been used in NLU <ref type="bibr" target="#b21">[21]</ref>, but using ASR transcripts, not audio, as input to the system.</p><p>In this work, we present and compare various end-to-end approaches to SLU for joinlty predicting semantics from audio. The presented techniques simplify the overall architecture of SLU systems. Using a large training set comparable to what is typically used for building large-vocabulary ASR systems, we show that not only can predicting semantics from audio be competitive, it can in some conditions outperform the conventional two-stage approach. To the best of our knowledge, this is the first study that shows all three of domain, intent, and arguments can be predicted from audio with competitive results.</p><p>The rest of the paper is organized as follows. Section 2 presents various models and architectures explored in this work. The experimental setup and results are described in Section 3 and Section 4, respectively. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM ARCHITECTURE</head><p>Our work is based on the encoder-decoder framework augmented by attention. We start by reviewing this framework in Section 2.2. There are multiple ways to model an end-to-end SLU system. One can either predict semantics directly from audio, ignoring the transcript, or have separate modules for predicting the transcript and semantics that are optimized jointly. These different approaches and the corresponding formulation are described in Sections 2.4 -2.6. Figure <ref type="figure" target="#fig_1">2</ref> shows a schematic representation of these architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notation</head><p>To review the general encoder-decoder framework, we denote the input and output sequences by A = {a1, . . . , aK } and B = {b1, . . . , bL}, where K and L denote their lengths. In this work, since we start from audio, the input sequence to the model are acoustic features (we describe the details of acoustic feature computation in Section 3.3), while the output sequence, depending on the model architecture, may be the transcript, the corresponding semantics, or both. While the semantics of an utterance is best represented as structured data, we use a simple deterministic scheme for serializing it by first including the domain and intent, followed by the argument labels and their values (see Table <ref type="table">1</ref>). More details are described in Section 3.2. For the rest of the paper, we denote the input acoustic features by X = {x1, . . . , xT }, where T stands for the total number of time frames. The transcript is represented as a sequence of graphemes. It is denoted as W = {w1, . . . , wN }, where N stands for the number of graphemes in the transcript. The semantics sequence is represented by S = {s1, . . . , sM }, where M stands for the number of tokens. The tokens come from a dictionary consisting of the domain, intent, argument labels, and graphemes to represent the argument values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Encoder-decoder framework</head><p>Given the training pair (A, B) and model parameters θ, a sequenceto-sequence model computes the conditional probability P (B|A; θ). This can be done by estimating the terms of the probability using chain rule:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transcript Serialized Semantics</head><p>"can you set an alarm for 2 p.m." &lt;DOMAIN&gt;&lt;PRODUCTIVITY&gt;&lt;INTENT&gt;&lt;SET ALARM&gt;&lt;DATETIME&gt;2 p.m. "remind me to buy milk" &lt;DOMAIN&gt;&lt;PRODUCTIVITY&gt;&lt;INTENT&gt;&lt;ADD REMINDER &gt;&lt;SUBJECT&gt;buy milk "next song please" &lt;DOMAIN&gt;&lt;MEDIA CONTROL&gt; "how old is barack obama" &lt;DOMAIN&gt;&lt;NONE&gt; Table <ref type="table">1</ref>: Example transcripts and their corresponding serialized semantics.</p><formula xml:id="formula_0">P (B|A; θ) = L i=1 P (bi|b1, . . . , bi-1, A; θ)<label>(1)</label></formula><p>The parameters of the model are learned by maximizing the conditional probabilities for the training data:</p><formula xml:id="formula_1">θ = argmax θ (A,B) logP (B|A; θ)<label>(2)</label></formula><p>In the encoder-decoder framework <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b1">1]</ref>, the model is parameterized as a neural network, most commonly a recurrent neural network, consisting of two main parts: An encoder that receives the input sequence and encodes it into a higher level representation, and a decoder that generates the output from this representation after first being fed a special start-of-sequence symbol. Decoding terminates when the decoder emits the special end-of-sequence symbol. The modeling power of encoder-decoder framework has been improved by the addition of an attention mechanism <ref type="bibr" target="#b17">[17]</ref>. This mechanism was introduced to overcome the bottleneck of having to encode the entire variable length input sequence in a single vector. At each output step, the decoder's last hidden state is used to generate an attention vector over the entire encoded input sequence, which is used to summarize and propagate the needed information from the encoder to the decoder at every output step. In this work, we use multi-headed attention <ref type="bibr" target="#b22">[22]</ref> that allows the decoder to focus on multiple parts of the input when generating each output. The effectiveness of this type of attention for ASR was explored and verified in <ref type="bibr" target="#b23">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Direct model</head><p>In the direct model the semantics of an utterance are directly predicted from the audio. The model does not learn to fully transcribe the input audio; it learns to only transcribe parts of the transcript that appear as argument values. Conceptually, this is the simplest formulation for end-to-end semantics prediction. But it also makes the task challenging, since the model has to implicitly learn to ignore parts of the transcript that is not part of an argument and the corresponding audio, while also inferring the domain and intent in the process.</p><p>Following the notation introduced in Section 2.2, the model directly computes P (S|X ; θ), as in Equation <ref type="formula" target="#formula_0">1</ref>. The encoder takes the acoustic features, X , as input and the decoder generates the semantic sequence, S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Joint model</head><p>This model still consists of an encoder and a decoder, similar to the direct model, but the decoder generates the transcript followed by domain, intent, and arguments. The output of this model is thus the concatenation of transcript and its corresponding semantics: [W : S] where [:] denotes concatenation of the first and the second sequence.</p><p>This formulation conditions intent and argument prediction on the transcript:</p><formula xml:id="formula_2">P (S, W|X ; θ) = P (S|W, X ; θ)P (W|X ; θ)<label>(3)</label></formula><p>This model retains the simplicity of the direct model, while simultaneously making learning easier by introducing an intermediate transcript representation corresponding to the input audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Multitask model</head><p>Multitask learning <ref type="bibr" target="#b24">[24]</ref> (MTL) is a widely used technique when learning related tasks, typically with limited data. Related tasks act as inductive bias, improving generalization of the main task by choosing parameters that are optimal for all tasks. Although predicting the text transcript is not necessary for domain, intent and argument prediction, it is a natural secondary task that can potentially offer a strong inductive bias while learning. In MTL, we factorize P (S, W|X ; θ) as:</p><formula xml:id="formula_3">P (S, W|X ; θ) = P (S|X ; θ)P (W|X ; θ).<label>(4)</label></formula><p>In the case of neural nets, multitask learning is typically done by sharing hidden representations between tasks <ref type="bibr" target="#b25">[25]</ref>. In this work, we do this by sharing the encoder and having separate decoders for predicting transcripts and semantics. We then learn parameters that optimize both tasks:</p><formula xml:id="formula_4">θ = argmax θ (X ,W,S) logP (W|X ; θe, θ W d ) + logP (S|X ; θe, θ S d ),<label>(5)</label></formula><p>where, θ = (θe, θ W d , θ S d ). θe, θ W d , θ S d are the parameters of the shared encoder, the decoder that predicts the transcript, and the decoder that predicts semantics, respectively. The shared encoder learns representations that enable both transcript and semantics prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Multistage model</head><p>Multistage (MS) model, when trained under the maximum likelihood criterion, is most similar to the conventional approach of training the ASR and NLU components independently. In MS modeling, semantics are assumed to be conditionally independent of acoustics given the transcript:</p><formula xml:id="formula_5">P (S, W|X ; θ) = P (S|W; θ)P (W |X; θ).<label>(6)</label></formula><p>Given this formulation, θ can be learned as:</p><formula xml:id="formula_6">θ = argmax θ (X ,W,S) logP (W|X ; θ W ) + logP (S|X ; θ W , θ S ),<label>(7)</label></formula><p>Here, θ W , θ S are, respectively, the parameters of the first stage, which predicts the transcript, and the second stage, which predicts semantics. For each training example, we assume that the triplet (X , W, S) is available. As a result, the two terms in Eq. 6 can be independently optimized, thereby reducing the model to a conventional 2-stage SLU system. In practice, however, it is possible to weakly tie the two stages together during training by using the predicted W at each time-step and allowing the gradients to pass from the second stage to the first stage through that label index. In Sec. 4, we will present results using alternative strategies to pick W from the first stage to propagate to the second stage, like the argmax of the softmax layer or sampling from the multinomial distribution induced by the softmax layer. By weakly tying the two stages, we allow the first stage to be optimized jointly with the second stage, based on the criterion that is relevant for both stages.</p><p>One of the advantages of the multistage approach is that the parameters for the 2 tasks are decoupled. Therefore, we can easily use different corpora to train each stage. Typically, the amount of data available to train a speech recognizer far exceeds the amount available to train an NLU system. In such cases, we can use the available ASR training data to tune the first stage and finally train the entire system using whatever data is available to train jointly. Furthermore, a stronger coupling between the 2 stages can be made when optimizing alternative loss criterion like the minimum Bayes risk (MBR) <ref type="bibr" target="#b26">[26]</ref> <ref type="bibr" target="#b27">[27]</ref>. We'll leave these aspects of multistage modeling to future work, as the focus of current study is more to understand the feasibility of predicting directly from audio and training jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>Our training data consists of 24M anonymized English utterances transcribed by humans. Similarly, our test set consists of 16K handtranscribed utterances. Both training and testing sets represent a slice of traffic from Google Home that we are interested in. The labeling for domain, intent, and arguments is generated from passing the ground-truth transcription through context free grammars (CFG). The CFGs are used to parse and transform ground-truth transcripts to domain, intent, and arguments. We only consider nonconversational (one-shot) queries in this work. In total, there are 5 domains: MEDIA, MEDIA CONTROL, PRODUCTIVITY, DE-LIGHT, and NONE. As the name suggests, any utterance that cannot be classified into the first four domains is labeled NONE. We consider ∼20 intents in this study, such as SET ALARM, SELF NOTE, etc., and two arguments: DATETIME and SUBJECT. The distribution of domains in the train and test sets is shown in Table <ref type="table" target="#tab_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Serializing/De-serializing Semantics</head><p>We use a simple scheme for serializing semantics: The domain is specified first using a special tag '&lt;DOMAIN&gt;' followed by its name. If the domain is further divided into intents, we use the tag '&lt;INTENT&gt;' followed by the intent's name. Any optional arguments are specified similarly using the name of the argument and its corresponding value. Table <ref type="table">1</ref> shows a few example transcripts and their corresponding serialized semantics.</p><p>At inference time, the predicted semantics sequence, S, is deserialized in a similar fashion to extract the domain, intent, and argument label and values. This is done using a simple parser that tokenizes the sequence by the domain tag, intent tag and argument name and treats the sequence in between them as the corresponding values. This parser is agnostic to the order of these special tags, i.e., the domain tag can come ahead of the intent tag. In the case of the joint model where the output sequence is the concatenation of the transcript and semantics, the first observed special tag or argument name marks the start of the semantic sequence.</p><p>The vocabulary that we use includes the domain and intent tags, domain, intent and argument names, (i.e., all symbols enclosed in "&lt;" and "&gt;" in Table <ref type="table">1</ref>) as well as English graphemes for representing transcript and argument values. The graphemes in this study are limited to lowercase English alphabets and digits, punctuation and a few other special symbols such as underscore, brackets, startof-sentence, and end-of-sentence. The total size of the vocabulary is 110. Note that the special tags used for representing semantics are each a single ouput, e.g., "&lt;DOMAIN&gt;" is one output and not eight graphemes "&lt;, D, ..., N, &gt;".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Models</head><p>All experiments use the same acoustic features: 80-dimensional log-Mel filterbanks, computed with a 25 msec window, shifted every 10 msec. Similar to <ref type="bibr" target="#b28">[28]</ref>, features from 3 contiguous frames are stacked, resulting in a 240-dimensional vector. These stacked features are downsampled by a factor of 3 generating inputs at 30ms frame rate that the encoder operates on.</p><p>Table <ref type="table">3</ref>: Model architectures used in the experiments. In each of the Enc/Dec columns, the first number indicates the number of layers and the second number shows the number of cells per layer. The cell type in all the models is Long Short Term Memory (LSTM). The last column shows the total number of parameters (in million).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Enc.</p><formula xml:id="formula_7">1 Dec.1 Enc.2 Dec.2 #Params (UniDi) (BiDi) Direct 5×1400 2×1024 - - 97M Joint 5×1400 2×1024 - - 97M Multitask 5×1400 2×512 - 2×512 86M Multistage 5×700 2×512 5×700 2×512 84M</formula><p>Table <ref type="table">3</ref> summarizes the architecture of the various models used in our experiments. We maintain a similar number of parameters (within 15% difference) across models to allow for a fair comparison. All encoder and decoders use Long Short Term Memory (LSTM) <ref type="bibr" target="#b29">[29]</ref> cells. The first encoder in all models is unidirectional, while the second encoder (in multistage models) uses bidirectional LSTMs <ref type="bibr" target="#b30">[30]</ref>. Prior work <ref type="bibr" target="#b5">[5]</ref> has shown that using bidirectional cells for encoding a transcript for the task of classifying its domain and intent achieves better performance compared to the unidirectional version. The first layer in all decoders is an embedding layer of size 128. The second encoder in the multistage model, which takes the transcript as input, also uses an embedding layer of the same size. All decoders use 4-headed additive attention <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22]</ref>. Our Baseline is the multistage model in which the two stages that do ASR and NLU are trained independently, but using the same training data. We consider 2 variants of the multistage model that weakly couples the 2 stages. Multistage (ArgMax) passes the argmax of the softmax layer of the first stage decoder, which predicts transcripts, to the second stage. Multistage (SampledSoftmax), on the other hand, passes on an unbiased sample from multinomial distribution represented by the output of the softmax layer <ref type="bibr" target="#b32">[32]</ref>.</p><p>All neural networks are trained from scratch with the crossentropy criterion in the TensorFlow framework <ref type="bibr" target="#b33">[33]</ref>. We use beam search during inference with a beam size of 8. The models are trained using Tensor Processing Units <ref type="bibr" target="#b34">[34]</ref> using the Adam optimizer <ref type="bibr" target="#b35">[35]</ref> and synchronous gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Metrics</head><p>We use the typical ASR and NLU metrics for evaluation. For models that generate the transcript, we measure and report word error rate (WER). For semantics, we measure multi-class F 1 scores <ref type="bibr" target="#b36">[36]</ref> for domain and intent. NLU systems that use in-out-begin (IOB) format for tagging arguments (see <ref type="bibr" target="#b5">[5]</ref> for an example of IOB format) report F 1 scores for argument tags (e.g., <ref type="bibr" target="#b36">[36]</ref> in the case of named-entities), but it is not clear how to measure this metric when the input transcript and the output arguments do not match, or when the input is audio. For example, if ground truth semantics contains "&lt;DATETIME&gt;five p.m." but the hypothesized semantics is "&lt;DATETIME&gt;high p.m.", it would be useful to have an error metric that captures the misrecognition of "five" to "high". For that reason, we choose to report WER for the arguments, instead of the F 1 scores. In our computation, we count over triggers and misses towards 100% WER. For example, if the ground truth semantics contains a DATETIME argument, but the recognized semantics does not, that instance has a 100% WER for DATETIME. We compute per argument WER and report the weighted average where each argument's WER is weighted according to its number of occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>Table <ref type="table" target="#tab_1">4</ref> compares domain, intent, and argument prediction performance of the models presented in the previous section. As can be seen, all models perform relatively similarly when it comes to classifying the domains. The Joint model works the best, with an F 1 score of 96.8%. Direct model, which has the lowest F 1 score, is only worse by 0.6% absolute. Performance on intent prediction is slightly worse, on average, compared to domain prediction. The Multitask and Joint models achieve the best F 1 scores of 95.8% and 95.7%, respectively. Both these models use the encoded acoustic features as input to the decoder, and unlike the Direct model, also predict the transcripts. This shows that having access to acoustic features and having an intermediate text representation are both important when predicting intent.</p><p>Comparing the Baseline model with the multistage models that weakly couple the 2 stages, Multistage (ArgMax) and Multistage (SampledSoftmax), we can see that they all work very similarly when it comes to domain and intent prediction, and are generally worse than Joint and Multitask models. This further shows the importance of complimenting transcripts with acoustic features when predicting intent.</p><p>The differences in argument WER is more pronounced among the different models. Direct model performs the worst, getting a WER of 18.2. This shows that including transcription loss while training end-to-end models can help improve argument prediction. Contrary to domain and intent F 1 scores, Multistage (ArgMax) and Multistage (SampledSoftmax), work better than the Joint and Multitask models. Nevertheless, all jointly optimized models work better than the independently trained baseline. Notably, Multistage (SampledSoftmax) model improves upon the baseline multistage model by 18% relative.</p><p>Since the domain, intent and argument labeling for training and test data was obtained using CFG-parsers, we did a second experiment that used the predicted transcript from the various models, pipelined with the same CFG-parsers. The CFG-parsers are used to derive domain, intent, and arguments from the predicted transcript. Results are shown in Table <ref type="table" target="#tab_2">5</ref>. The table also shows the overall WER obtained by the various models. Compared to the results in Table <ref type="table" target="#tab_1">4</ref>, we can see that domain F 1 scores are similar, but intent F 1 scores are better. Interestingly, the argument WER significantly improved. For example, for the Baseline model, WER improved from 15.0 to 11.9. While this is not entirely surprising, since this strategy of predicting semantics matches what is used for generating ground truth labels for training data, it is interesting to see that models that are optimized jointly still work better in terms of intent F 1 scores and argument WER. For example, the Multitask model gets an intent F 1 score of 97.2, which is better than the baseline by 1.3 points. Similarly, Multistage (SampledSoftmax) and Joint models get an argument WER of 11.3, which is 0.6% absolute better than the baseline. The results show that joint training can also help improve performance of the ASR component of the model when using the original CFG-parser for intent prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUDING REMARKS</head><p>In this work, we have proposed and evaluated multiple end-to-end approaches to SLU that optimize the ASR and NLU components of the system jointly. We show that joint optimization results in better performance not just when we do end-to-end domain, intent, and argument prediction, but also when using the transcripts generated by a jointly trained end-to-end model and a conventional CFG-parsers for NLU. Our results highlight several important aspects of joint optimization. We show that having an intermediate text representation is important when learning SLU systems end-to-end. As expected, our results also show that joint optimization helps the model focus on errors that matter more for SLU as evidenced by the lower argument WERs obtained by models that couple ASR and NLU. It was also observed that direct prediction of semantics from audio by ignoring the ground truth transcript, does not perform as well.</p><p>There are several interesting avenues to improve performance going forward. As noted before, the amount of training data that is available to train ASR is usually several times larger than what is available to train NLU systems. It would be interesting to understand how a jointly optimized model can make use of ASR data to improve performance. For optimization, the current work uses the cross-entropy loss. Future work will consider more task specific losses, like MBR, that optimizes intent and argument prediction accuracy directly. It is also important to understand how to incorporate new grammars with limited training data into an end-to-end system. The CFG-parsing based approach that decouples itself from ASR can easily incorporate additional grammars. But end-to-end optimization relies on data to learn new grammars, making the introduction of new domains more challenging.</p><p>Framing spoken language understanding as a sequence to sequence problem that is optimized end-to-end significantly simplifies the overall complexity. It is also easy to scale such models to more complex tasks, e.g., tasks that involve multiple intents within a single user input, or tasks for which it is not easy to create a CFG-based parser. The ability to run inference without the need of additional resources like a lexicon, language models and parsers also make them ideal for deploying on devices with limited compute and memory footprint.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>"Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example transcripts and their corresponding domain, intent, and arguments. Only arguments that have corresponding values in a transcript are shown. For example, song name and station name are both arguments in the MEDIA domain but only one has a corresponding value in each of the MEDIA examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Different model architectures investigated in this paper. X stands for acoustic features, W for transcripts and S for semantics (domain, intent and arguments). Dotted lines represent both the conditioning of output label on its history and the attention module, which is treated as a part of the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Distribution of domains considered in this study in the training and test data.</figDesc><table><row><cell>domain</cell><cell cols="2">Train Test</cell></row><row><cell>MEDIA</cell><cell>30%</cell><cell>20%</cell></row><row><cell cols="2">MEDIA CONTROL 8%</cell><cell>16%</cell></row><row><cell>PRODUCTIVITY</cell><cell>7%</cell><cell>5%</cell></row><row><cell>DELIGHT</cell><cell>2%</cell><cell>2%</cell></row><row><cell>NONE</cell><cell>53%</cell><cell>56%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>Domain and intent F 1 scores, and argument WER for the predicted semantics.</figDesc><table><row><cell>Model</cell><cell cols="3">Domain F1 Intent F1 Arg WER</cell></row><row><cell>Baseline</cell><cell>96.6</cell><cell>95.1</cell><cell>15.04</cell></row><row><cell>Direct</cell><cell>96.2</cell><cell>94.2</cell><cell>18.22</cell></row><row><cell>Joint</cell><cell>96.8</cell><cell>95.7</cell><cell>14.93</cell></row><row><cell>Multitask</cell><cell>96.7</cell><cell>95.8</cell><cell>15.02</cell></row><row><cell>Multistage (ArgMax)</cell><cell>96.5</cell><cell>95.4</cell><cell>14.84</cell></row><row><cell cols="2">Multistage (SampledSoftmax) 96.5</cell><cell>95.2</cell><cell>12.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Transcription WER, domain and intent F 1 scores, and argument WER when NLU is performed on the model's top recognized transcript using the CFG-parser that was used for generating truth semantic labels during training.</figDesc><table><row><cell>Model</cell><cell cols="4">WER Domain F1 Intent F1 Arg WER</cell></row><row><cell>Baseline</cell><cell>5.9</cell><cell>96.4</cell><cell>95.9</cell><cell>11.89</cell></row><row><cell>Direct</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint</cell><cell>5.5</cell><cell>96.5</cell><cell>96.5</cell><cell>11.28</cell></row><row><cell>Multitask</cell><cell>5.7</cell><cell>95.4</cell><cell>97.2</cell><cell>11.71</cell></row><row><cell>Multistage (ArgMax)</cell><cell>5.9</cell><cell>96.5</cell><cell>96.3</cell><cell>12.59</cell></row><row><cell cols="2">Multistage (SampledSoftmax) 6.0</cell><cell>96.5</cell><cell>96.5</cell><cell>11.26</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>The authors would like to thank Edgar Gonzàlez Pellicer, Alex Kouzemtchenko, Ben Swanson, Ashish Venugopal, Kai Zhao in help in obtaining labels used as truth for semantics, and Kanishka Rao for discussions around the joint model. We thank Khe Chai Sim and Amarnag Subramanya for helpful feedback on earlier drafts of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Spoken language understanding: Systems for extracting semantic information from speech</title>
		<author>
			<persName><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Acoustic modeling for google home</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Caroselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Golan</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kean</forename><surname>Chin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>INTERSPEECH-2017</publisher>
			<biblScope unit="page" from="399" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Onenet: Joint domain, intent, slot prediction for spoken language understanding</title>
		<author>
			<persName><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="547" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multi-domain joint semantic frame parsing using bi-directional rnn-lstm</title>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gökhan</forename><surname>Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="715" to="719" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparing human and machine errors in conversational speech transcription</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="137" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName><forename type="first">George</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lynn-Li</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bergul</forename><surname>Roomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IN-TERSPEECH</title>
		<meeting>IN-TERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Google duplex: An AI system for accomplishing real-world tasks over the phone</title>
		<ptr target="https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html" />
		<imprint>
			<biblScope unit="page" from="2018" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards endto-end spoken language understanding</title>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08395</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A reranking approach for recognition and classification of speech input in conversational dialogue systems</title>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Morbini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Van Segbroeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panayiotis</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shri</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond asr 1-best: Using word confusion networks in spoken language understanding</title>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Béchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="514" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Latticernn: Recurrent neural networks over lattices</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Gandhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariya</forename><surname>Rastrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Hoffmeister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="695" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incorporating asr errors with attention-based, jointly trained rnn for intent detection and slot filling</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pongtep</forename><surname>Angkititrakul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spoken language understanding without speech recognition</title>
		<author>
			<persName><forename type="first">Yuan-Ping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>¸alar Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">Oct. 2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequenceto-sequence models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
	<note>2016 IEEE International Conference</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="685" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">State-ofthe-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katya</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06114</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Optimizing expected word error rate via sampling for speech recognition</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Minimum word error rate training for attention-based sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01818</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franoise</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldip K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katya</forename><surname>Gonina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01769</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Norman P Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 44th Annual International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Computer Architecture (ISCA)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>CONLL &apos;03</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
