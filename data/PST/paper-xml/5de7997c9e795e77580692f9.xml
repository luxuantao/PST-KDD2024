<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Constrained Reinforcement Learning Has Zero Duality Gap</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Santiago</forename><surname>Paternain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Systems Engineering</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luiz</forename><forename type="middle">F O</forename><surname>Chamon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Systems Engineering</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miguel</forename><surname>Calvo-Fullana</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Systems Engineering</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
							<email>aribeiro@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Systems Engineering</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Constrained Reinforcement Learning Has Zero Duality Gap</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autonomous agents must often deal with conflicting requirements, such as completing tasks using the least amount of time/energy, learning multiple tasks, or dealing with multiple opponents. In the context of reinforcement learning (RL), these problems are addressed by (i) designing a reward function that simultaneously describes all requirements or (ii) combining modular value functions that encode them individually. Though effective, these methods have critical downsides. Designing good reward functions that balance different objectives is challenging, especially as the number of objectives grows. Moreover, implicit interference between goals may lead to performance plateaus as they compete for resources, particularly when training on-policy. Similarly, selecting parameters to combine value functions is at least as hard as designing an all-encompassing reward, given that the effect of their values on the overall policy is not straightforward. The later is generally addressed by formulating the conflicting requirements as a constrained RL problem and solved using Primal-Dual methods. These algorithms are in general not guaranteed to converge to the optimal solution since the problem is not convex. This work provides theoretical support to these approaches by establishing that despite its non-convexity, this problem has zero duality gap, i.e., it can be solved exactly in the dual domain, where it becomes convex. Finally, we show this result basically holds if the policy is described by a good parametrization (e.g., neural networks) and we connect this result with primal-dual algorithms present in the literature and we establish the convergence to the optimal solution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Autonomous agents must often deal with conflicting requirements, such as completing a task in the least amount of time/energy, learning multiple tasks or contexts, dealing with multiple opponents or with several specifications that are designed to guide the agent in the learning process. In the context of reinforcement learning <ref type="bibr" target="#b0">[1]</ref>, these problems are generally addressed by combining modular value functions that encode them individually, by multiplying each signal by its own coefficient, which controls the emphasis placed on it <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Although effective, the multi-objective problem <ref type="bibr" target="#b4">[5]</ref> has several downsides. First, for each set of penalty coefficients, there exists a different, optimal solution, also known as Pareto optimality <ref type="bibr" target="#b5">[6]</ref>. In practice, the exact coefficient is selected through a time consuming and a computationally intensive process of hyper-parameter tuning that often times are domain dependent, as showed in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. Moreover, implicit interference between the goals may lead to training plateaus as they compete for resources in the policy <ref type="bibr" target="#b9">[10]</ref>.</p><p>An alternative, is to embed all conflicting requirements in a constrained RL problem and to use a primal-dual algorithm as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> that chooses the parameters automatically. The main advantage of this approach is that constraints ensure satisfying behavior without the need for manually selecting the penalty coefficients. In these algorithms the policy update is on a faster time-scale than the multiplier update. Thus, effectively, these approaches work as if the dual problem of the constrained reinforcement learning problem was being solved. Thus, guaranteeing to obtain the feasible solution with the smallest suboptimality. Yet, there is no guarantee on how small the suboptimality is. In this work we provide an answer to the previous question. In particular we establish that:</p><p>1. Despite its non-convexity, constrained reinforcement learning for policies belonging to a general distribution class has zero duality gap, i.e., it can be solved exactly in the dual domain, where the problem is actually convex 2. Since working with generic distributions as policies is in general intractable, we extend this result to parametrized policies, by showing that the suboptimality bound also holds when the parametrization is a universal approximator, e.g., a neural network <ref type="bibr" target="#b11">[12]</ref>). 3. We leverage these theoretical results to establish that the family of primal-dual algorithms for constrained reinforcement learning, e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, in fact converge to the optimal solution under mild assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Constrained Markov Decision Processes (CMDPs) <ref type="bibr" target="#b12">[13]</ref> are an active field of research. CMDP applications cover a vast number of topics, such as: electric grids <ref type="bibr" target="#b13">[14]</ref>, networking <ref type="bibr" target="#b14">[15]</ref>, robotics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and finance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The most common approaches to solve this problems can be divided under the following categories. Manual selection of Lagrange multipliers: constrained Reinforcement Learning problems can be solved through by maximizing an unconstrained Lagrangian, for a specific multiplier <ref type="bibr" target="#b1">[2]</ref>. The combination of different rewards with manually selected Lagrange multipliers has been applied for instance to learning complex movements for humanoids <ref type="bibr" target="#b3">[4]</ref> or to limit the variance of the constraint that needs to be satisfied <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Integrating prior knowledge about the system transitions is exploited in order to project the action chosen by the policy to a set that ensures the satisfaction of the constraints <ref type="bibr" target="#b20">[21]</ref>. Primal-dual algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, allow us to choose dynamically the multipliers by find the best policy for the current set of parameters and then taking steps along the gradient of the Lagrangian with respect to the multipliers. These allow to consider general constraints and the algorithm is reward agnostic and it does not require the use of prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Constrained Reinforcement Learning</head><p>Let t ? N ? {0} denote the time instant and S ? R n and A ? R d be compact sets describing the possible states and actions of an agent described by a Markovian dynamical system with transition probability density p, i.e., p (s t+1 | {s u , a u } u?t ) = p (s t+1 | s t , a t ) for s t ? S and a t ? A for all t. The agent chooses actions sequentially based on a policy ? ? P(S), where P(S) is the space of probability measures on (A, B(A)) parametrized by elements of S, where B(A) are the Borel sets of A. The action taken by the agent at each state results in rewards defined by the functions r i : S ? A ? R, for i = 0, . . . , m, that the agent accumulates over time. These rewards describe different objectives that the agent must achieve, such as completing a task, remaining within a region of the state space, or not running out of battery. The goal of constrained RL is then to find a policy ? ? P(S) that meets these objectives by solving the problem</p><formula xml:id="formula_0">P max ??P(S) V 0 (?) E s,? ? t=0 ? t r 0 (s t , ?(s t )) subject to V i (?) E s,? ? t=0 ? t r i (s t , ?(s t )) ? c i , i = 1, . . . , m,<label>(PI)</label></formula><p>where ? ? (0, 1) is a discount factor and c i ? R represent the i-th reward specification. It is important to contrast the formulation in (PI) with the unconstrained, regularized problem commonly found in the literature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> maximize ??P(S)</p><formula xml:id="formula_1">V 0 (?) + m i=1 w i (V i (?) -c i ) , (<label>PI</label></formula><formula xml:id="formula_2">)</formula><p>where w i ? 0 are the regularization parameters. First, (PI) precludes the manual balancing of different requirements through the choice of w i . Even with expert knowledge, tuning these parameters can be as hard as solving the RL problem itself, since there is no straightforward relation between the value of w i and the value V i (? ) given by the final policy. What is more, note that the objective of ( PI) can be written as a single value function V (?) E s,? [ ? t=0 ? t r(s t , ?(s t ))] for r(s t , ?(s t )) = r 0 (s t , ?(s t )) + m i=1 w i r i (s t , ?(s t )). In other words, choosing the value of w i amounts to designing a reward that simultaneously encodes different, possibly conflicting, objectives and/or requirements. Given the challenge that can be designing good reward functions for a single task, it is ready that this regularized approach is neither efficient nor effective.</p><p>Though promising, solving the constrained RL problem in (PI) is intricate. Indeed, it is both infinite dimensional and non-convex, so that it is in general not tractable in the primal domain. Its dual problem, on the other hand, is convex and has dimensionality equal to the number of constraints. However, since (PI) is not a convex program, its dual problem in general only provides an upper bound on P . How good the policy obtained by solving the dual problem is depends on the tightness of this bound. What is more, formulating the problem in the dual domain is at least as hard as solving ( PI), which is also infinite dimensional and non-convex. In the sequel, we address these two issues by first showing that (PI) has no duality gap (Section 3), i.e., that the upper bound on P from the dual problem is tight. This implies that (PI) can be solved exactly in the dual domain. Then, we show that we lose (almost) nothing by parametrizing the policies ? (Section 4), which immediately addresses the issue of dimensionality in (PI)-( PI). Finally, we put forward and analyze a primaldual algorithm for constrained RL (Section 5), showing that under mild conditions it yields a locally optimal, feasible solution of (PI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Constrained Reinforcement Learning Has Zero Duality Gap</head><p>Let us start by formalizing the concept of dual problem. Let the vector ? ? R m + collect the Lagrange multipliers of the constraints of (PI) and define its Lagrangian as</p><formula xml:id="formula_3">L(?, ?) V 0 (?) + m i=1 ? i (V i (?) -c i ) . (<label>1</label></formula><formula xml:id="formula_4">)</formula><p>The dual function is then the point-wise maximum of (1) with respect to the policy ?, i.e.,</p><formula xml:id="formula_5">d(?) max ??P(S) L(?, ?).<label>(2)</label></formula><p>The dual function (2) provides an upper bounds on the value of (PI), i.e., d(?) ? P for all ? ? R m + <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Section 5.1.3]</ref>. The tighter the bound, the closer the policy obtained from (2) is to the optimal solution of (PI). Hence, the dual problem is that of finding the tightest of these bounds:</p><formula xml:id="formula_6">D min ??R m + d(?). (DI)</formula><p>Note that the dual function ( <ref type="formula" target="#formula_5">2</ref>) can be related to the unconstrained, regularized problem ( PI) from Section 2 by taking ? i = w i in (1). Hence, (2) takes on the optimal value of ( PI) for all possible regularization parameters. Problem (DI) then finds the best regularized problem, i.e., that whose value is closest to P . It turns out, this problem is tractable if d(?) can be evaluated, since (DI) is a convex program (the dual function is the point-wise maximum of a set of linear functions and is therefore convex) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Section 3.2.3]</ref>.</p><p>Despite these similarities, (DI) [and consequently ( PI)] do not necessarily solve the same problem as (PI). In other words, there need not be a relation between the optimal dual variables ? from (DI) or the regularization parameters w i and the specifications c i of (PI). This depends on the value of the duality gap ? = D -P . Indeed, if ? is small, then so is the suboptimality of the policies obtained from (DI). In the limit case where ? = 0, problems (PI)-(DI) and ( PI) would all be essentially equivalent. Since (PI) is not a convex program, however, this result does not hold immediately. Still, we calim in Theorem 1 that (PI) has zero duality gap under Slater's conditions. Before stating the Theorem we define the perturbation function associated to problem (PI) which is fundamental for the proof of the result and for future reference. For any ? ? R n , the perturbation function associated to (PI) is defined as</p><formula xml:id="formula_7">P (?) max ??P(S) V 0 (?) subject to V i (?) ? c i + ? i , i = 1 . . . m. (PI )</formula><p>Notice that P (0) = P , the optimal value of (PI). We formally state next the conditions under which Problem (PI) has zero duality gap.</p><p>Theorem 1. Suppose that r i is bounded for all i = 0, . . . , m and that Slater's condition holds for (PI). Then, strong duality holds for (PI), i.e., P = D .</p><p>Proof. This proof relies on a well-known result from perturbation theory connecting strong duality to the convexity of the perturbation function defined in(PI ). We formalize this result next.</p><p>Proposition 1 (Fenchel-Moreau). If (i) Slater's condition holds for (PI) and (ii) its perturbation function P (?) is concave, then strong duality holds for (PI).</p><p>Proof. See, e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr">Cor. 30.2.2]</ref>.</p><p>Condition (i) of Proposition 1 is satisfied by the hypotheses of Theorem 1. It suffices then to show that the perturbation function is concave [(ii)], i.e., that for every ? 1 , ? 2 ? R m , and ? ? (0, 1),</p><formula xml:id="formula_8">P ?? 1 + (1 -?)? 2 ? ?P ? 1 + (1 -?)P ? 2 . (<label>3</label></formula><formula xml:id="formula_9">)</formula><p>If for either perturbation ? 1 or ? 2 the problem becomes infeasible then P (? 1 ) = -? or P (? 2 ) = -? and thus (3) holds trivially. For perturbations that keep the problem feasible, suppose P (? 1 ) and P (? 2 ) are achieved by the policies ? 1 ? P(S) and ? 2 ? P(S) respectively. Then,</p><formula xml:id="formula_10">P (? 1 ) = V 0 (? 1 ) with V i (? 1 ) -c i ? ? 1 i and P (? 2 ) = V 0 (? 2 ) with V i (? 2 ) -c i ? ? 2 i for i = 1, . . . , m.</formula><p>To establish (3) it suffices to show that for every ? ? (0, 1) there exists a policy ? ? such that</p><formula xml:id="formula_11">V i (? ? ) -c i ? ?? 1 i + (1 -?)? 2 i and V 0 (? ? ) = ?V 0 (? 1 ) + (1 -?)V 0 (? 2 )</formula><p>. Notice that any policy ? ? satisfying the previous conditions is a feasible policy for the slack</p><formula xml:id="formula_12">c i + ?? 1 i + (1 -?)? 2 i .</formula><p>Hence, by definition of the perturbed function (PI ), it follows that</p><formula xml:id="formula_13">P ?? 1 + (1 -?)? 2 ? V 0 (? ? ) = ?V 0 (? 1 ) + (1 -?)V 0 (? 2 ) = ?P ? 1 + (1 -?)P ? 2 . (4)</formula><p>If such policy exists, the previous equation implies <ref type="bibr" target="#b2">(3)</ref>. Thus, to complete the proof of the result we need to establish its existence. To do so we start by formulating a linear program equivalent to (PI ). Notice that for any i = 0, . . . , m we can write</p><formula xml:id="formula_14">V i (?) = (S?A) ? ? t=0 ? t r i (s t , a t ) p ? (s 0 , a 0 , . . .) ds 0 . . . da 0 . . . .<label>(5)</label></formula><p>Since the reward functions are bounded the Dominated Convergence Theorem holds. This allows us to exchange the order of the sum and the integral. Moreover, using conditional probabilities and the Markov property of the transition of the system we can write V i (?) as</p><formula xml:id="formula_15">V i (?) = ? t=0 ? t (S?A) ? r i (s t , a t ) ? u=1 p(s u |s u-1 , a u-1 )?(a u |s u )p(s 0 )?(a 0 |s 0 ) ds 0 . . . da 0 . . . .<label>(6)</label></formula><p>Notice that for every u &gt; t the integrals with respect to a u and s u yield one, since they are integrating density functions. Thus, the previous expression reduces to</p><formula xml:id="formula_16">V i (?) = ? t=0 ? t (S?A) t r i (s t , a t ) t u=1</formula><p>p(s u |s u-1 , a u-1 )?(a u |s u )p(s 0 )?(a 0 |s 0 ) ds 0 . . . ds t da 0 . . . da t . <ref type="bibr" target="#b6">(7)</ref> Notice that the probability density of being at state s and choosing action a under the policy ? at time t can be written as</p><formula xml:id="formula_17">p t ? (s t , a t ) = (S?A) t-1 t u=1 p(s u |s u-1 , a u-1 )?(a u |s u )p(s 0 )?(a 0 |s 0 ) ds 0 . . . ds t-1 da 0 . . . da t-1 .<label>(8)</label></formula><p>Thus, using again the Dominated Convergence Theorem, one can write compactly <ref type="bibr" target="#b6">(7)</ref> as</p><formula xml:id="formula_18">V i (?) = S?A r i (s, a) ? t=0 ? t p t ? (s, a) dsda.<label>(9)</label></formula><p>By defining the occupation measure ?(s, a)</p><formula xml:id="formula_19">= (1 -?) ? t=0 ? t p t ? (s, a) it follows that (1 - ?)V i (?) = S?A r i (s, a)?(s, a) dsda.</formula><p>Denote by M(S, A) the measures over S ? A and define the set R as the set of all occupation measures induced by the policies ? ? P(S) as</p><formula xml:id="formula_20">R := ? ? M(S, A) ?(s, a) = (1 -?) ? t=0 ? t p ? (s t = s, a t = a) ,<label>(10)</label></formula><p>where It follows from [24, Theorem 3.1] that the set of occupation measures R is convex and compact. Hence, we can write the following linear program equivalent to (PI )</p><formula xml:id="formula_21">P (?) max ??R 1 1 -? S?A r 0 (s, a)?(s, a) dsda subject to 1 1 -? S?A r i (s, a)?(s, a) dsda ? c i + ? i , i = 1, . . . , m.</formula><p>(PI )</p><p>Let ? 1 , ? 2 ? R be the occupation measures associated to ? 1 and ? 2 . Since, R is convex, there exists a policy ? ? ? P(S) such that its corresponding occupation measure is</p><formula xml:id="formula_22">? ? = ?? 1 + (1 -?)? 2 ? R.</formula><p>Notice that ? ? satisfies the constraints with slack c i + ?? 1 i + (1?)? 2 i for i = 1, . . . , m since the integral is linear and ? 1 and ? 2 satisfy the constraints with slacks c i + ? 1 i and c i + ? 2 i respectively. Thus, it follows that</p><formula xml:id="formula_23">P (?? 1 + (1 -?)? 2 ) ? 1 1 -? S?A r 0 (s, a)? ? (s, a) dsda = ?V 0 (? 1 ) + (1 -?)V 0 (? 2 ),<label>(11)</label></formula><p>where we have used again the linearity of the integral. Since ? i are such that V 0 (? 1 ) = P (? 1 ) and V 0 (? 2 ) = P (? 2 ), inequality (3) follows. This completes the proof that the perturbation function is concave.</p><p>Theorem 1 establishes a fundamental equivalence between the constrained (PI) and the dual problem (DI) [and therefore also ( PI)]. Indeed, since (PI) has no duality gap, its solution can be obtained by solving (DI). What is more, the trade-offs expressed by the w i in ( PI) are the same as those expressed by the specifications c i in the sense that they trace the same Pareto front. Nevertheless, note that the relationship between c i and w i is not trivial and that specifying the constrained problem is often considerably simpler. Theorem 1 establishes that this is indeed a valid transformation, since both problems are equivalent. Observe that due to the non-convexity of the objective in RL problems, this result is in fact not immediate.</p><p>The theoretical importance of the previous result notwithstanding, it does not yield a procedure to solve (PI) since evaluating the dual function involves a maximization problem that is intractable for general classes of distributions. In the next section, we study the effect of using a finite parametrization for the policies and show that the price to pay in terms of duality gap depends on how "good" the parametrization is. If we consider, for instance, a neural network-which are universal function approximators <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>-the loss in optimality can be made arbitrarily small.</p><p>4 There is (almost) no price to pay by parametrizing the policies</p><p>We consider next the problem where the policies are parametrized by a vector ? ? R p . This vector could be for instance the coefficients of a neural network or the weights of a linear combination of functions. In this work, we focus our attention however on a widely used class of parametrizations that we term near-universal, which are able to model any function in P(S) to within a stated accuracy. We formalize this concept in the following definition.</p><p>Definition 1. A parametrization ? ? is an -universal parametrization of functions in P(S) if, for some &gt; 0, there exists for any ? ? P(S) a parameter ? ? R p such that</p><formula xml:id="formula_24">max s?S A |?(a|s) -? ? (a|s)| da ? .<label>(12)</label></formula><p>The previous definition includes all parametrizations that induce distributions that are close to distributions in P(S) in total variational norm. Notice that this is a milder requirement than approximation in uniform norm which is a property that has been established to be satisfied by radial basis functions networks <ref type="bibr" target="#b28">[29]</ref>, reproducing kernel Hilbert spaces <ref type="bibr" target="#b29">[30]</ref> and deep neural networks <ref type="bibr" target="#b11">[12]</ref>. Notice that the objective function and the constraints in Problem (PI) involve an infinite horizon and thus, the policy is applied an infinite number of times. Hence, the error introduced by the parametrization could a priori accumulate and induce distributions over trajectories that differ considerably from the distributions induced by policies in P(S). We claim in the following lemma that this is not the case.</p><p>Lemma 1. Let ? and ? ? be occupation measures induced by the policies ? ? P(S) and ? ? respectively, where ? ? is an -parametrization of ?. Then, it follows that</p><formula xml:id="formula_25">S?A |?(s, a) -? ? (s, a)| dsda ? 1 -? . (<label>13</label></formula><formula xml:id="formula_26">)</formula><p>The previous result, although derived as a technical result required to bound the duality gap for parametric problems, has a natural interpretation. larger ? -the more the operation is concerned about rewards far in the future -the larger the error in the approximation of the occupation measure.</p><p>Having defined the concept of universal approximator, we shift focus to writing the parametric version of the constrained reinforcement learning problem. This is, to find the parameters that solve (PI), where now the policies are restricted to the functions induced by the chosen parametrization</p><formula xml:id="formula_27">P ? max ? V 0 (?) E s,? ? ? t=0 ? t r 0 (s t , ? ? (s t )) subject to V i (?) E s,? ? ? t=0 ? t r i (s t , ? ? (s t )) ? c i , i = 1 . . . m.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(PII)</head><p>Notice that the problem (PII) is similar to the original problem (PI), with the only difference that the expectations are now with respect to distributions induced by the parameter vector ?. As done in the previous section, let ? ? R m and define the dual function associated to (PII) as</p><formula xml:id="formula_28">d ? (?) min ??R p L ? (?, ?) min ??R p V 0 (?) + m i=1 ? i (V i (?) -c i ) ,<label>(14)</label></formula><p>Likewise we define the dual problem as finding the tightest upper bound for (PII)</p><formula xml:id="formula_29">D ? minimize ??R m + d ? (?). (DII)</formula><p>As previously stated, the reason for introducing the parametrization is to turn the original functional optimization problem into a tractable problem in which the optimization variable is a finite dimensional vector of parameters. Yet, there is a cost for introducing the aforementioned parametrization: the duality gap is no longer null. The latter means that the solution obtained through the dual problem is sub-optimal. We claim however that this gap is bounded by a function that is linear with the approximation error , and thus if the parametrization has a good representation power the price to pay is almost zero. This is the subject of the following theorem.</p><p>Theorem 2. Suppose that r i is bounded for all i = 0, . . . , m by constants B ri &gt; 0 and define B r = max i=1...m B ri . Let ? be the solution to the dual problem associated to (PI ) for perturbation ? i = B r /(1?) for all i = 1, . . . , m. Then, under the hypothesis of Theorem 1 it follows that</p><formula xml:id="formula_30">P ? D ? ? P -(B r0 + ? 1 B r ) 1 -? , (<label>15</label></formula><formula xml:id="formula_31">)</formula><p>where P is the optimal value of (PI), and D ? the value of the parametrized dual problem (DII).</p><p>The implication of the previous result is that there is almost no price to pay by introducing a parametrization. By solving the dual problem (DII) the sub-optimality achieved is of order , i.e., the error on the representation of the policies. Notice that this error could be made arbitrarily small by increasing the representation ability of the parametrization, by for instance increasing the dimension of the vector of parameters ?. The latter means that if we can compute the dual function it is possible to solve (PI) approximately. Moreover, working on the dual domain provides two computational advantages; on one hand, the dimension of the problem is the number of constraints in (PI). In addition, the dual function is always convex, hence gradient descent on the dual domain solves the problem of interest. In the next section we propose an algorithm to solve (PI) approximately based on the previous discussion.</p><p>Before doing so notice that we have not assumed anything about the feasibility of problem (PII). Notice that if the problem is infeasible then we have that D ? = -? and thus the upper bound on (15) holds trivially. On the other hand if the problem is infeasible it also means that there is no policy ? ? P(S) that satisfies the constraints of (PI) with slack B r /(1?) since ? is an -universal approximation of P(S). Hence the perturbed problem is infeasible which yields a dual multiplier ? that has infinite norm. Thus the right hand side of ( <ref type="formula" target="#formula_30">15</ref>) holds as well. In that sense, as long as the parameterization introduced keeps the problem feasible the price to pay for parameterizing is almost zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Solving Constrained Reinforcement Learning Problems</head><p>As previously stated, the dual function is always a convex function since it is the point-wise maximum of linear functions. Thus the dual problem (DII) can be efficiently solved using (sub)gradient descent, with the caveat that because we require the dual iterates to remain in the positive orthant, we include a projection onto this space after taking the gradient step</p><formula xml:id="formula_32">? k+1 = [? k -??d ? (? k )] + ,<label>(16)</label></formula><p>where ? &gt; 0 is the step-size of the algorithm, [?] + denotes the projection onto R m + and ?d ? (?) denotes -with a slight abuse of notation -a vector in the subgradient of d ? (?). The latter can be computed by virtue of Dankin's Theorem (see e.g. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">Chapter 3]</ref>) by evaluating the constraints in the original problem (PII) at the primal maximizer of the Lagrangian. Thus, the main theoretical difficulty in this computation lies on finding said maximizer since the Lagrangian is non-convex with respect to ?. However, maximizing the Lagrangian with respect to ? corresponds to learning a policy that uses as reward the following linear combination of rewards</p><formula xml:id="formula_33">r ? (s, a) = r 0 (s, a) + m i=1 ? i r i (s, a).<label>(17)</label></formula><p>Indeed, using the linearity of the expectation, the cumulative discounted cost for the reward r ? (s, a) yields</p><formula xml:id="formula_34">E s,? ? t=0 ? t r ? (s t , a t ) = E s,? ? ? t=0 ? t r 0 (s t , a t ) + m i=1 ? i E s,? ? ? t=0 ? t r i (s t , a t ) = L(?, ?).<label>(18)</label></formula><p>And therefore reinforcement learning algorithms such as policy gradient <ref type="bibr" target="#b31">[32]</ref> or actor-critic methods <ref type="bibr" target="#b32">[33]</ref> can be used to find the parameters ? such that they maximize the Lagrangian. The good performance of these algorithms is rooted in the fact that they are able to maximize the expected cumulative reward or at least to achieve a value that is close to the maximum. The next assumption formalizes this idea. Assumption 1. Let ? ? be a parametrization of functions in P(S) and let L ? (?, ?) with ? ? R m + be the Lagrangian associated to (PII). Denote by ? (?), ? ? (?) ? R P the maximum of L(?, ?) and a local maximum respectively achieved by a generic reinforcement learning algorithm. Then, there exists ? &gt; 0 such that for all ? ? R m + it holds that L ? (? (?), ?) ? L ? (? ? (?), ?) + ?. Notice that the previous assumption only means that we are able to solve the regularized unconstrained problem approximately. This means that the parameter at time k + 1 is</p><formula xml:id="formula_35">? k+1 ? argmax ??R p L(? k , ?).<label>(19)</label></formula><p>Then, the dual variable is updated following the gradient descent scheme suggested in <ref type="bibr" target="#b15">(16)</ref>, where we replace the subgradient of the dual function by the constraint of the primal problem (PII). Defining ?d k V (? k+1 )s, the update yields</p><formula xml:id="formula_36">? k+1 = ? k -? ?d k + = [? k -? (V (? k+1 ) -s)] + .<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 dualDescent</head><p>Input: ? 1: Initialize: ? 0 = 0, ? 0 = 0 2: for k = 0, 1 . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Compute an approximation of ? k+1 ? argmax L ? (?, ? k ) with a RL algorithm 4:</p><p>Compute the dual ascent step ? k+1 = [? k? (V (? k+1 )s)] + . 5: end</p><p>The algorithm given by ( <ref type="formula" target="#formula_35">19</ref>)-( <ref type="formula" target="#formula_36">20</ref>) is summarized under Algorithm 1. The previous algorithm relies on the fact that the ?d k does not differ much from ?d ? (? k ). We claim in the following proposition that this is the case. In particular, we establish that the constraint evaluation does not differ from the subgradient in more than ?, the error on the primal maximization defined in Assumption 1.</p><p>Proposition 2. Under Assumption 1, the constraint in (PII) evaluated at a local maximizer of Lagrangian ? ? (?) approximate the subgradient of the dual function <ref type="bibr" target="#b13">(14)</ref>. In particular it follows that</p><formula xml:id="formula_37">d ? (?) -d ? (? ? ) ? (? -? ? ) V (? ? (?)) -s + ?. (<label>21</label></formula><formula xml:id="formula_38">)</formula><p>The previous proposition is key in establishing convergence of the algorithm proposed since allows us to claim that the dual updated is an approximation of a dual descent step. We formalize this result next and we establish a maximum number of dual steps required to achieve a desired accuracy.</p><p>Theorem 3. Let ? ? be an universal parametrization of P(S) according to Definition 1, B r = max i=1...m B ri with B ri &gt; 0 bounds on the rewards r i and ? ? (0, 1) be the discount factor. Then, if Slater's conditions hold for (PII), under Assumption 1 and for any ? &gt; 0, the sequence of updates of Algorithm 1 with step size ? converges in K &gt; 0 steps, with</p><formula xml:id="formula_39">K ? ? 0 -? ? 2 2?? ,<label>(22)</label></formula><p>to a neighborhood of P -the solution of (PI)satisfying</p><formula xml:id="formula_40">P -(B r0 + ? 1 B r ) 1 -? ? d ? (? K ) ? P + ? B 2 + ? + ?. (<label>23</label></formula><formula xml:id="formula_41">)</formula><p>where</p><formula xml:id="formula_42">B = m i=1 (B ri /(1 -?) -c i )</formula><p>2 and ? is the solution of (DI).</p><p>The previous result establishes a bound on the number of dual iterations required to converge to a neighborhood of the optimal solution. This bound is linear with the inverse of the desired accuracy ?. Notice that the size of the neighborhood to which the dual descent algorithm converges depends on the representation ability of the parametrization chosen, and the goodness of the solution of the maximization of the Lagrangian. Since the cost of running policy gradient or actor-critic algorithms until convergence before updating the dual variable might result in an algorithm that is computationally prohibitive, an alternative that is common in the context of optimization is to update both variables in parallel <ref type="bibr" target="#b33">[34]</ref>. This idea can be applied in the context of reinforcement learning as well, where a policy gradient -or actor critic as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> -update is followed by an update of the multipliers along the direction of the constraint violation. In these algorithms the update on the policy is on a faster scale than the update of the multipliers, and therefore they operate from a theoretical point of view as <ref type="bibr" target="#b0">(1)</ref>. In particular, the proofs in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> rely on the fact that this different time-scale is such that allows to consider the multiplier as constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Numerical Example</head><p>In this section, we include a numerical example in order to showcase the consequences of our theoretical results. As an illustrative example, we consider a gridworld navigation scenario. This scenario, illustrated in Figure <ref type="figure">1</ref>, consists of an agent attempting to navigate from a starting position to a goal. To do so, the agent must cross from the left side of the world to the right side using either one of two bridges. The bridge above is deemed "unsafe". The agents uses a softmax policy with four possible actions (moving up, down, left, and right) over a table-lookup of states and actions. The agent receives a reward r(s, a) = 10 for reaching the goal and a reward of r(s, a) = -1 for each step it wanders outside of goal. The scenario is designed such that the shortest path requires crossing the unsafe path (red bridge), while the safe path (blue bridge) requires a longer detour. Using our formulation, we constrain the agent to not cross the unsafe bridge with 99% probability.</p><p>We train the agent via Algorithm 1, agent and plot in Fig. <ref type="figure">2</ref>, the resulting normalized duality gap. We consider two cases, an inexact primal maximization via policy gradient and, an exact primal maximization. In order to obtain the global primal minimizer, for a given value of the dual variables ?, the optimal primal minimizer can be easily found via Dijkstra's algorithm. We show that by solving Step 4 of Algorithm 1 exactly the duality gap effectively vanishes (red curve). We also showcase a curve in which Step 4 is replaced by a single policy gradient step (blue curve). Since the minimization in Step 4 is done approximately, the duality gap decreases at a slower rate and will only converge to a neighborhood of zero (as per Theorem 3). In any of the two cases, ultimately, the agent learns to navigate from start to goal by crossing the safe bridge (blue path in Fig. <ref type="figure">1</ref>). Now, we turn our attention to the effect of the parametrization size. We consider parametrization of different coarseness via state aggregation, as shown in Fig. <ref type="figure">1</ref>. This will correspond, as per Definition 1, in parametrizations with lager values of , i.e., looser approximators. Figure <ref type="figure">3</ref> displays the effect of using coarser parametrizations, as the parametrization becomes coarser, the duality gap increases (as per Theorem 2). Specially, for very coarse parametrizations (such as the cyan case), the agent cannot learn a successful policy due to the poor covering properties of its parametrization and resultantly such problem will have a large duality gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Throughout this work we have developed a duality theory for constrained reinforcement learning problems. In particular we have established that for policies belonging to a general class of distributions, the duality gap of this problems is null and therefore by solving the problem on the dual domain -which always yields a finite dimensional convex problem -yields the same result as solving the original problem directly. Moreover, it establishes the equivalence between the constrained problem and the regularized problem -or manual selection of multipliers -in the sense that both problems track the same Pareto optimal front. These theoretical implications however do not imply that it is always possible to solve the problem. To be able to solve the dual problem, one is required to evaluate the dual function, which might result intractable in several problems, for instance in cases where arbitrary policies are considered. To overcome this limitation, we have shown that for sufficiently rich parametrizations the zero duality gap result holds approximately. However, for the most part, the parametrizations considered in the literature are not necessarily universal approximators of distributions since in general the output of the neural network reduces to the mean -and in some cases the variance -of a distribution.</p><p>Regardless of these limitations, the primal dual algorithm considered here and those proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> provide a manner to solve constrained policy optimization problems without the need to perform an exhaustive search over the weights that we assign to each reward function, as it is the case in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Likewise, the need of imposing constraints might arise directly from the algorithm design, this is for instance the case in Trust Region Policy Optimization <ref type="bibr" target="#b34">[35]</ref>, where a constraint on the divergence of the policy is included. Although our theorems do not guarantee that the zero duality gap result holds under these constraints, since they reduce to a projection onto a convex set it would not be surprising that it could be adapted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :Figure 3 :</head><label>123</label><figDesc>Figure 1: Safe (blue) and unsafe (red) optimal path. Parametrization coarseness is on the bottom left.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
	<note>Reinforcement learning: An introduction</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An actor-critic algorithm for constrained markov decision processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems &amp; control letters</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="207" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constrained policy optimization</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName><forename type="first">Xue Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">143</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A geometric approach to multi-criterion reinforcement learning</title>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nahum</forename><surname>Shimkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="325" to="360" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-objective reinforcement learning using sets of Pareto dominating policies</title>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Van Moffaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Now?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3483" to="3512" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11074</idno>
		<title level="m">Reward constrained policy optimization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miljan</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Krakovna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Orseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09883</idno>
		<title level="m">Ai safety gridworlds</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Simple random search provides a competitive approach to reinforcement learning</title>
		<author>
			<persName><forename type="first">Horia</forename><surname>Mania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07055</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ray interference: a source of plateaus in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Borsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11455</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An online actor-critic algorithm with function approximation for constrained markov decision processes</title>
		<author>
			<persName><forename type="first">Shalabh</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="688" to="708" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Constrained Markov decision processes</title>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Altman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Control and optimization meet the smart power grid: Scheduling of power demands for optimal energy management</title>
		<author>
			<persName><forename type="first">Iordanis</forename><surname>Koutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandros</forename><surname>Tassiulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Energy-efficient Computing and Networking</title>
		<meeting>the 2nd International Conference on Energy-efficient Computing and Networking</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization of web service-based control system for balance between network traffic and delay</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianchuan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1152" to="1162" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Risk-sensitive and robust decision-making: a cvar optimization approach</title>
		<author>
			<persName><forename type="first">Yinlam</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1522" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates</title>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3389" to="3396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Portfolio optimization with conditional value-at-risk objective and constraints</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Krokhmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Palmquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Uryasev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of risk</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="43" to="68" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Policy gradients with variance related risk criteria</title>
		<author>
			<persName><forename type="first">Dotan</forename><surname>Di Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6404</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Variance adjusted actor critic algorithms</title>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.3697</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Safe exploration in continuous action spaces</title>
		<author>
			<persName><forename type="first">Gal</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnamurthy</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Paduraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08757</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex analysis</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A convex analytic approach to markov decision processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability Theory and Related Fields</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="583" to="602" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the approximate realization of continuous mappings by neural networks</title>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Funahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of control, signals and systems</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The expressive power of neural networks: A view from the width</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6231" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Resnet with one-neuron hidden layers is a universal approximator</title>
		<author>
			<persName><forename type="first">Hongzhou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6169" to="6178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Universal approximation using radial-basis-function networks</title>
		<author>
			<persName><forename type="first">Jooyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the relation between universality, characteristic kernels and rkhs embedding of measures</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="773" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Dimitri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athena</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><surname>Scientific</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convex optimization algorithms</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1008" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Studies in linear and nonlinear programming</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">J</forename><surname>Arrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Hurwicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
			<publisher>Stanford University Press</publisher>
			<pubPlace>CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
