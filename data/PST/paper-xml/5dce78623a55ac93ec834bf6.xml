<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">System architecture for network-attached FPGAs in the Cloud using partial reconfiguration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Burkhard</forename><surname>Ringlein</surname></persName>
							<email>burkhard.ringlein@fau.de</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Zurich R?schlikon</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Chair of Computer Architecture Friedrich</orgName>
								<orgName type="institution">Alexander University</orgName>
								<address>
									<settlement>Erlangen, N?rnberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francois</forename><surname>Abel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Zurich R?schlikon</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Ditter</surname></persName>
							<email>alexander.ditter@fau.de</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Zurich R?schlikon</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Chair of Computer Architecture Friedrich</orgName>
								<orgName type="institution">Alexander University</orgName>
								<address>
									<settlement>Erlangen, N?rnberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Beat Weiss ?</roleName><forename type="first">Christoph</forename><surname>Hagleitner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Zurich R?schlikon</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dietmar</forename><surname>Fey</surname></persName>
							<email>dietmar.fey@fau.de</email>
							<affiliation key="aff1">
								<orgName type="department">Chair of Computer Architecture Friedrich</orgName>
								<orgName type="institution">Alexander University</orgName>
								<address>
									<settlement>Erlangen, N?rnberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">System architecture for network-attached FPGAs in the Cloud using partial reconfiguration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/FPL.2019.00054</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cloud computing</term>
					<term>network-attached FPGA</term>
					<term>stand-alone FPGA</term>
					<term>partial reconfiguration</term>
					<term>data centers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emerging applications such as deep neural networks, bioinformatics or video encoding impose a high computing pressure on the Cloud. Reconfigurable technologies like Field-Programmable Gate Arrays (FPGAs) can handle such computeintensive workloads in an efficient and performant way. To seamlessly incorporate FPGAs into existing Cloud environments and leverage their full power efficiency, FPGAs should be directly attached to the data center network and operate independent of power-hungry CPUs. This raises new questions about resource management, application deployment and network integrity. We present a system architecture for managing a large number of network-attached FPGAs in an efficient, flexible and scalable way. To ensure the integrity of the infrastructure, we use partial reconfiguration to separate the non-privileged user logic from the privileged system logic. To create a really scalable and agile cloud service, the management of all resources builds on the Representational State Transfer (REST) concept.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The growth of computing infrastructures has long been driven by Moore's law and the use of homogeneous CPUcentric platforms. These traditional systems are suffering due to extreme power density on the chips. Therefore, modern supercomputers and Cloud Data Centers (DC) take a new approach by exploiting high-performance and low-power heterogeneous devices. Field-Programmable Gate Arrays (FPGA) are such heterogeneous devices. Their energy efficiency and low latency have the potential to drastically improve the power density of compute nodes while also delivering results faster. Consequently, more and more FPGAs are making their way into DCs where they are used as accelerators to boost the computing power and overall power efficiency of individual server nodes.</p><p>Two cloud architectures have recently proposed to turn FPGA resources into independent cloud computer nodes by enabling the FPGAs to communicate directly at DC scale <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>.</p><p>In <ref type="bibr" target="#b0">[1]</ref>, the FPGA is placed as a network-side "bump-inthe-wire" between the servers' Network Interface Controller (NIC) and the Ethernet network switches. The hyperscale infrastructure we proposed in <ref type="bibr" target="#b1">[2]</ref> goes one step further by disaggregating the FPGA accelerator from the server and by turning the FPGA into a stand-alone computing resource. Such network-attached FPGAs can be deployed at large scale and independently of the number of CPU servers in the DC. The network attachment allows them to seamlessly connect with each other as well as with more than one CPU. The resulting disaggregated heterogeneous computing infrastructure is capable to dynamically adapt to the scale of any workload.</p><p>Meanwhile, large-scale applications ranging from business analytics to scientific simulations have started to scale out using distributed frameworks such as Hadoop, Spark, and Tensorflow. These frameworks will consume an immense number of FPGAs, which must be provided and managed in an efficient and flexible way.</p><p>We propose an architecture for acquiring, distributing, configuring and operating stand-alone network-attached FPGAs at a large scale in DC infrastructures. We start by defining a list of major requirements for building such a scalable deployment by investigating the constraints expressed by the DC provider and the FPGA users. Based on this analysis, we propose an architecture with three tiers covering all levels from the cores implementing the user application to the DC-management.</p><p>Stand-alone network-attached FPGAs come with specific management demands such as network setup, creation of user subnets, network routing and the distribution of the configuration bitfiles. We deliver these services via the network and show how they seamlessly integrate into the OpenStack DC management software.</p><p>We enforce the use of partial reconfiguration to physically isolate the system management functions from the user logic within the network-attached FPGA. Partial Reconfiguration (PR) is the ability to dynamically modify blocks of the FPGA logic while the remaining logic continues to operate without interruption <ref type="bibr" target="#b2">[3]</ref>. This approach protects the integrity of the DC network by creating a separation between privileged and non-privileged user logic functions.</p><p>The rest of this paper is structured as follows: The next section discusses related research in this field. After that, we consider the constraints of a system architecture for a hyperscale FPGA service and present our implementation to realize such a service. Finally, we evaluate our implementation and draw some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A well-known project for incorporating FPGAs in datacenter-level applications is Microsoft's Catapult project. In 2014, Putnam et al. used PCIe-attached FPGAs in DC servers to accelerate the ranking engine of Bing. Their results showed an almost twofold increase of the throughput with comparable latency while consuming only 10% more energy <ref type="bibr" target="#b3">[4]</ref>. The logic in the FPGA is split into an infrastructure part (Shell) and an application logic (Role) to increase the reuse of logic by the algorithm <ref type="bibr" target="#b0">[1]</ref>. Their application was completely ported to Verilog.</p><p>In May 2018, Microsoft announced Project Brainwave, which builds on their Catapult Project and is expected to deliver real-time Artificial Intelligence (AI) analysis in the future <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref>.</p><p>The Catapult Fabric deploys FPGAs in a DC environment but they are not provided to users as a "cloud service". This is, for example, done by Amazon with their F1 Instances <ref type="bibr" target="#b6">[7]</ref> or by IBM's SuperVessel Cloud <ref type="bibr" target="#b7">[8]</ref>. Amazon offers FPGAs as part of its AWS cloud service and marketplace. As explained in the AWS EC2 FPGA Development Kit <ref type="bibr" target="#b8">[9]</ref>[10], FPGAs are attached via PCIe to a Virtual Machine (VM) and the logic is also split into a Shell, which contains among others the logic for PCIe, DRAM, and DMA, and a custom logic for the actual application. The user submits and uploads her or his application as a design-checkpoint (i.e. the sources must be included) to AWS. The AWS platform then merges it with the Shell logic and generates the FPGA image (i.e. bitstream). In contrast, SuperVessel brings FPGAs into an OpenStack-based Cloud by attaching them to POWER8 processors via IBMs CAPI (Coherent Accelerator Processor Interface) <ref type="bibr" target="#b10">[11]</ref>.</p><p>A joint project between IBM and Microsoft was performed by Chen et al. in 2014 <ref type="bibr" target="#b11">[12]</ref> to analyse the impediments of using FPGAs as shareable resource in the Cloud. Their concept revolves around an FPGA board attached to a host CPU via PCIe, but in contrast to Aamazon's F1, it is possible to run multiple user kernels on one FPGA simultaneously using PR.</p><p>There are several projects from the Electronics Research Group of the University of Toronto <ref type="bibr" target="#b12">[13]</ref> [14] <ref type="bibr" target="#b14">[15]</ref>. Starting in 2014, they introduced Virtualized FPGA Resources (VFRs) to the OpenStack-based Canadian SAVI testbed. The FPGAs are accessed through a VM via PCIe passthrough and the partial reconfiguration of the VFRs is triggered through the Joint Test Action Group (JTAG) connection. As next step, the "hardware abstraction layer" and the "hypervisor in the FPGA" were extended to be compatible with the Xilinx SDAccel platform with OpenCL kernels on the host VM. In <ref type="bibr" target="#b15">[16]</ref> the group demonstrates their framework with an acceleration of virtual network functions. The performance of the demonstrated system is limited by the throughput of the host VM.</p><p>The integration of an Ethernet controller into FPGAs is described by Blott et al. <ref type="bibr" target="#b16">[17]</ref>, who implemented a 10Gbps NIC and complete UDP and TCP offload engines. The authors achieved an increase of up to 36 times in requests per second per Watt in comparison to x86 servers with optimized software (SW). Also, their FPGA implementation of a key-value store resulted in a round-trip time between 3.5?s and 4.5?s, which is an improvement of two orders of magnitude over standard x86 approaches.</p><p>In <ref type="bibr" target="#b17">[18]</ref> Knodel et al. provided a detailed description of another approach for PCIe-attached FPGAs in the Cloud. They also present a comprehensive security model for such a kind of Reconfigurable Common Cloud Computing Environment.</p><p>In our previous work in <ref type="bibr" target="#b1">[2]</ref>[19] <ref type="bibr" target="#b19">[20]</ref>[21] we advocated for the integration of a NIC into the FPGA logic (called iNIC), and for the deployment of such FPGAs as stand-alone resources via a direct attachment to the DC network. We refer to such connected FPGAs as stand-alone network-attached FPGAs or disaggregated FPGAs. We showed that iNICs enable denser packaging, lower power consumption and cost, and increased flexibility with respect to supported protocol stacks. We also demonstrated a distributed text analytics application with regular expressions and compared it to a SW-only implementation as well as to an FPGA-accelerated implementation with PCIe-attachment. The comparisons showed that the proposed network-attached FPGAs outperform the two other implementations by a factor of 40 regarding latency and a factor of 18 regarding the throughput <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSTEM ARCHITECTURE PROPOSAL</head><p>Our goal is to deliver FPGA resources in the Cloud similarly to CPU VMs. This requires a cloud service that is easy-to-use, flexible and scalable.</p><p>To build our SW system part, we can learn from common cloud services -e.g. as defined in <ref type="bibr" target="#b21">[22]</ref> -and build on TCP/IP as communication layer and the Representational State Transfer (REST) concept <ref type="bibr" target="#b22">[23]</ref> as management and interaction principle, because both have proven to scale well. REST is a distributed system framework between nodes of arbitrary kind and designed to use a stateless communication protocol, typically the Hypertext Transfer Protocol (HTTP). The scalability of its stateless operation was demonstrated by CPU-based applications in the Web, and we anticipate the same for FPGAs, because the REST principles are platform independent. Furthermore, REST builds on uniform interfaces and enforces strong separations between the components. Web services that implement such an architecture are referred to as RESTful.</p><p>Additionally, in contrast to e.g. the cloud service of AWS [10, see "Step 3"], we want to allow the user to generate the FPGA bitstream under his/her exclusive control before uploading it to our cloud service. The goal is to protect the intellectual property of a user in the same way as a SW binary running on a VM.</p><p>This proposal builds on our prior work in <ref type="bibr" target="#b1">[2]</ref> which describes the hardware (HW) of a dense FPGA platform that decouples the FPGA from the CPU of the server and connects the FPGAs directly to the DC network. This turns an FPGA into a disaggregated, stand-alone computing resource and renders the platform particularly cost and energy-efficient because the number of deployed FPGAs becomes independent of the number of servers.</p><p>The platform in <ref type="bibr" target="#b1">[2]</ref> integrates 32 FPGAs and a 64 port 10 Gb/s Ethernet switch with full cross-sectional bandwidth onto a passive water cooled carrier referred to as Sled. The switch of the Sled acts as a leaf switch that aggregates 32 * 10 GbE links from the FPGAs and then connects to the core of the DC network via 8 * 40 GbE up-links. The goal of this leaf-spin architecture is to avoid increasing the port density of the core switches. Each FPGA is mounted on a card that contains 16GB of DDR4 RAM, a Flash module to store the boot bitstream, and a tiny micro controller (?C) for powering the FPGA on/off as well as writing some configuration values to the FPGA. Additionally, each Sled contains a service processor to control the ?Cs and the Ethernet switch. The concept is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. We will refer to this platform as Sled FPGA (SF).</p><p>With respect to the HW platform described above, we will now propose an infrastructure management framework for serving such network-attached FPGAs as virtual entities or clusters to users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Requirements</head><p>Taking the above assumptions into account, we derived the following requirements for our system architecture: R1) An application must only consist of a partial bitstream. R2) An application must be independent of the physical FPGA, i.e. a hardware abstraction must be provided. R3) The internal logic of the FPGA must be separated into privileged and non-privileged parts. The iNIC must be part of the privileged part. R4) The privileged logic must always come from sources controlled by the provider. R5) The FPGA card must be able to perform all operational tasks on its own, i.e. configure the application, set up the user network, administrate memory regions, and eventually set up the runtime environment. R6) We assume a single physical communication channel between the FPGA and the DC network. R7) All management tasks must be done with RESTful Application Program Interfaces (API). R8) If possible, reuse existing DC services to execute (sub-) tasks. R9) Multiple FPGAs must be able to connect with each other via TCP/IP (as well as with more than one CPU) to build a cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. High-Level Architecture</head><p>Our system architecture proposal is split into three levels (see Fig. <ref type="figure" target="#fig_0">1</ref>): A Data Center Resource Manager (DCRM), a Sled Manager (SM), and an FPGA Manager Core (FMC). There is one resource manager per DC to control many Sleds. The DCRM handles the user images and maintains a database of FPGA resources.</p><p>There is one sled manager for every 32 FPGAs. The SM runs on a service processor that is part of the Sled. It powers the FPGAs on and off, monitors the physical parameters of the FPGAs, and runs the SW management stack of the Ethernet switch.</p><p>There is one FMC per FPGA. The FMC contains a simplified HTTP server that provides support for the REST API calls issued by the DCRM.</p><p>In the end, the components of all levels work together to provide the requested FPGA resources in a fast and secure way.</p><p>For a cloud service based on SF, the necessary information about an FPGA resource at DC level is the FPGA type (e.g. XKU060), the IP address of the Sled service processor to which the FPGA belongs (see Fig. <ref type="figure" target="#fig_0">1</ref>), the slot number on that Sled (between 0 and 31), the type of available RAM on the module, and the current state of this FPGA (e.g. AVAILABLE, USED, or MAINTENANCE) . The DCRM maintains the database of FPGA resources containing such metadata. The resources can be added, modified and deleted by administrators. To allow a secure operation, the DCRM service also takes care of the authorization of users and the allowed usage quotas per user.</p><p>A user can upload images, i.e. FPGA bitstreams, which are also stored in the DCRM database. Each user can upload and delete her/his images.</p><p>If a user requests to configure one specific image to an FPGA, this image gets instantiated on an FPGA, i.e. configured, and then the combination of the two becomes an instance. Hence, the basic formula for our architecture is FPGA resource + image = instance.</p><p>The DCRM supports heterogeneous clusters by assigning multiple FPGA instances and CPU VMs to the same subnetwork, providing unique node-ids for each instance, and propagating the necessary routing information to all devices as required by R9. The DCRM database also contains all the information about currently instantiated FPGA instances or clusters.</p><p>Operating under the control of the DCRM, the SM controls the power states of the FPGAs and writes the MAC and IP addresses into the FPGA, according to the API calls from the DCRM.</p><p>For SF, one single network interface carries both user and management traffic and the FPGA logic consequently needs to be separated into privileged and non-privileged (R3). Otherwise, the integrity of the DC network would be at risk, because the user could take control of the network interface to break out of her/his designated subnet, start denial-of-service attacks or harm the provider network in other ways.</p><p>The FMC controls the application and is part of the privileged logic. As mentioned before, we use PR to split the logic into privileged and unprivileged. To enable PR at runtime, it is necessary to define the placement and interface of the PR regions in advance <ref type="bibr" target="#b23">[24]</ref>. Hence, PR adds further flexibility to the design, but also defines clear interfaces between the static and dynamic logic. Additionally, PR has the advantage of speeding up the deployment of partial bit files as well as protecting the user's intellectual property. The privileged logic is statically configured at power on and is referred to as the Shell. Therefore, by using PR we can control the privileged logic (R4) and define a hardware abstraction (R2). The Shell also provides runtime information, such as the node id if the FPGA is part of a cluster. The FMC is controlled by the DC resource manager via a RESTful HTTP API as defined by R7. Finally, the unprivileged application logic, also referred to as Role, is dynamically configured using a partial bitfile that is sent through the network every time a new PR is required.</p><p>Usually, to deploy an application on a compute cloud, the user need not take care of I/O-connections, the details of the hardware setup, or different versions of a carrier board. This should be true for FPGA clouds as well and is stated by R2.</p><p>Consequently, the combination of Shell, Role, and the interface between them does not only separate the privileges within the FPGA but also abstracts the details of the network and memory hardware from the user as required by R2. This abstraction is provided by the Shell via industry-standard and easy-to-use AXI-Protocols <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION</head><p>In this section we describe our system architecture in detail and justify the design decisions based on the formulated requirements.</p><p>DCRM, SM, and FCM provide RESTful HTTP APIs. These APIs provide HTTP methods like GET &lt;entity-name&gt;, POST &lt;entity-name&gt;, or PUT &lt;entity-name&gt;. Usually, as defined in RFC 2616 <ref type="bibr" target="#b25">[26]</ref>, the GET method is for reading this entity, the POST method requests or adds a new entity, and the PUT modifies an existing entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DC-level management</head><p>New resources are introduced to the DCRM with a POST /resource request (R7). Only the DC provider is allowed to add or modify the resources and therefore the permission is checked first. For the user management the OpenStack Keystone module (see <ref type="bibr" target="#b26">[27]</ref>) is used (R8). In our architecture, an FPGA is uniquely identified by the Sled-IP on which it resides and by the slot number on that Sled. Hence, the DCRM ensures that each such pair exists only once in the database before assigning a unique id as required by the REST-concept.</p><p>The user can upload his/her "FPGA image" using a POST /image method at the resource manager. The user must submit, besides the partial bitfile, the type of HW abstraction used -that is the type of Shell used (R2) -and the type of the FPGA board for which the bitfile was implemented. The OpenStack Glance module (see <ref type="bibr" target="#b26">[27]</ref>) is used for storing the bitfiles and their metadata (R8).</p><p>Using a POST /instance or POST /cluster request (R7), the user informs the DCRM about the images he wants to deploy and which image should be assigned to which node-id in case of a cluster. Due to the use of a HW abstraction and PR, no other information is required and the images are mapped to the resources based on the metadata of the images. The DCRM then starts to power on the required FPGAs (if necessary), to distribute the network configurations using the SMs and to distribute the partial bitfiles of the application to the FMCs. Finally, the routing information within a cluster is propagated and the network setup is verified before the information about the topology is returned to the user. Again, instances and clusters are assigned a unique id after their submission and can be manipulated later based on this id.</p><p>The corresponding network abstraction of the FPGA instances, i.e. node-id and TCP/UDP stream, are also available as a SW library to be used on CPUs. As a consequence, application kernels on FPGAs and CPUs can easily talk to each other through using the node-ids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sled-level management</head><p>The concept of SF requires a service processor to manage the resources that are shared in every group of 32 FPGAs on a Sled. These include: two power converters, an Ethernet switch and a USB hub for communicating with the ?Cs located on every FPGA card.</p><p>This manager is controlled by the provider (R4) and it grants the Sled as a pool of 32 FPGAs to the DCRM. The SM controls the power on/off of the FPGAs and issues lowlevel management commands to the ?Cs (e.g. reset, write IP address, get temperature or program the Flash memory). The SM API is also designed in a RESTful manner (R7).</p><p>Due to security considerations, we decided to use the ?Cs to perform these tasks instead of using a combination of Wakeon-LAN (WoL) and Dynamic Host Configuration Protocol (DHCP). The "magic packets" for WoL are sent via layer 2, which can easily be exploited in a DC environment, unless we would introduce extra filters for layer 2 in the Sled. This solution appeared to entail more of an overhead to us and probably comes with additional implications compared to using small and cheap ?Cs for this task.</p><p>This ?C should not be considered as a bus-attached processor. This device is always on and consumes less than 0.3 Watts. Its role is similar to a Baseboard Management Controller in providing basic functions such as power state, start-up, control and monitoring of the FPGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. FPGA-level management</head><p>After the ?C is done with assigning the FPGA its MAC and IP addresses, the remaining tasks to execute during deployment are performing the partial reconfiguration of the application, starting and resetting the application, and, if necessary, setting up the memory layout or routing information for the application. The FMC understands the following REST API calls issued by the DCRM:</p><p>? POST /configure: Submits a partial bitfile and triggers the PR of the Role region. ? GET /status: Returns some app-specific status information. ? PUT /node_id: Sets the node-id register of the Role.</p><p>? POST /routing: Sends the routing information of a cluster to the FPGA. It checks the validity of the request and the CRC checksums within the partial bitfile and responds accordingly with the HTTP status code.</p><p>One core requirement of RESTful architectures is that one resource is managed by one single controller entity. This is necessary for an architecture in order to enable scaling and easy maintenance. Therefore, we introduced the FMC as third level of management to be responsible for all management modifications regarding the internal state of the FPGA, including the execution of the PR. This also enables the concurrent handling of FPGA resources, in contrast to using a serial bus like USB for 32 devices. As a result, with this RESTful management core inside the static design of the FPGA we fulfil the requirements 1, 5, and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Shell-Role Architecture</head><p>Figure <ref type="figure" target="#fig_1">2</ref> gives an overview of our Shell-Role-Architecture (SRA) and the required functional cores inside an FPGA. The FPGA implements an Ethernet IP core from Xilinx to access the 10GbE network. The transport and network layers are implemented with TCP, UDP, Internet Control Message Protocol (ICMP) and IPv4 engines. The I/O component interfaces with the ?C and subsequently with the SM. The connection to the DDR4 memory is also an IP core from Xilinx. Based on the network configuration, e.g. different VLANs or ports, the management traffic is forwarded to the FMC and the user traffic to the routing core (R6). The interface between the Role and the Shell is the abstract HW interface (R2). The memory port consists of two AXI4-master ports, the node_id is a 32-bit register, start and reset are onebit signals. The Network Interface consists of several AXI4-stream connections for data and meta-data. The entire Shell is coded in HLS except for the above-mentioned Xilinx cores.</p><p>To deploy the Role, the FMC writes the received and verified PR bitfile to the Xilinx HWICAP core to trigger the PR (R1). While the PR is being executed, the FMC decouples all logic from the Role. The Role can be specified in VHDL, Verilog, HLS or any combination thereof.</p><p>Finally, when a Xilinx Debug Bridge IP Core <ref type="bibr" target="#b27">[28]</ref> is inserted into a PR region, it can have its traffic forwarded via TCP for remote debugging in the Cloud. Our SRA comes along with a framework to compile and build user applications. A user only needs to issue one make command to get the application bitstream, without taking all the dependencies into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>We evaluated our system architecture on the SF platform using Xilinx XCKU60 FPGAs.</p><p>Table <ref type="table" target="#tab_0">I</ref> shows the overhead in terms of the FPGA resource usage for the runtime and the management cores. The resource usage for the FMC, which includes the REST implementation, and the Network Routing Core (NRC) are in the range of one to two percent of all available resources. This additional resource usage of the logic for the self-management of the FPGA is relatively low and therefore supports our stand-alone network-attachment proposal.   Delivering the partial bitfile over TCP reduced this amount of time by a factor of 16. Since the FPGA module manages itself and can run completely independently, SF in combination with the proposed system architecture is able to completely leverage the energy efficiency of FPGAs.</p><p>As a proof of concept, we set ourselves the goal of running a Message Protocol Interface (MPI) <ref type="bibr" target="#b28">[29]</ref> application on a cluster of eight FPGAS and one CPU. In SW, the MPI program is compiled by the MPI compiler. Afterwards, in a cluster of 8 + 1 CPUs, the command mpirun distributes the compiled application binary across the cluster, e.g. using Secure Shell (SSH) before executing it automatically <ref type="bibr" target="#b29">[30]</ref> <ref type="bibr" target="#b30">[31]</ref>.</p><p>To run the same program on our platform, we developed a cross-compiler that transforms parts of the MPI code into HLS-synthesizeable code, similar to the approach in <ref type="bibr" target="#b31">[32]</ref>. This HLS code is subsequently imported as a Role into our SRA and the partial bitstream is generated automatically. Analogous to the SW-only process, our run-command uploads the resulting bitfiles and distributes them via the DCRM API. Next, the routing tables of the FPGAs are propagated and the network setup is verified. Listing 1 is the output of this run-command. This example demonstrates how an HPC application developer without specific FPGA experience is able to compile and execute an MPI application on a cluster of FPGAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Agile and scalable management of large amounts of FPGAs is a key element for their further integration into data centers. However, the flexibility and performance-per-cost advantages provided by FPGAs can only be leveraged in the Cloud if they can be deployed quickly and independent of other computing resources. In order to accommodate both, we proposed and implemented a system architecture for network-attached FPGAs in the Cloud using partial reconfiguration as one step towards making FPGAs true first-class citizens in the Cloud. The implemented end-to-end architecture realizes an easy-touse cloud service, protects the intellectual property of users and enables heterogeneous clusters while complying with all derived requirements. Hence, it makes it possible to deploy an application on multiple FPGAs just by uploading a set of partial bitfiles and their corresponding clustering structure. To the best of our knowledge, this research is the first to implement a stand-alone PR over network and enables the RESTful management of network-attached FPGAs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. System architecture for the Sled FPGA platform. 32 FPGAs, one switch and a service processor are combined on one carrier board and called Sled<ref type="bibr" target="#b1">[2]</ref>. The management tasks are split into three levels -DC, Sled, and FPGA. A Sled is half of a 2U chassis. The OpenStack compute resources (Nova) as CPU nodes are also available for creating heterogeneous clusters.</figDesc><graphic url="image-2.png" coords="3,119.83,73.00,377.13,193.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Shell Role Architecture. All management tasks are controlled by the FPGA management core.</figDesc><graphic url="image-3.png" coords="6,106.74,73.07,404.04,156.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>user@test0 $ ./SF_mpirun &lt;path-to-SW-binary&gt; &lt; path-to-PR-bitstream&gt; 8 Upload PR bitfiles... Creating FPGA cluster... Id of new cluster: 1 Ping all nodes, build ARP table... Starting MPI subprocess at 2019-03-21 21:26:30.455471 rank 1 addr: 10.12.200.17 rank 2 addr: 10.12.200.20 rank 3 addr: 10.12.200.19 rank 4 addr: 10.12.200.21 rank 5 addr: 10.12.200.22 rank 6 addr: 10.12.200.23 rank 7 addr: 10.12.200.24 rank 8 addr: 10.12.200.25 Here is rank 0, size is 9. Distribute data and start client nodes. [..........] MPI execution time: 3.177521s MPI subprocess finished at 2019-03-21 21:26:33.640393. Listing 1. Snippet of one SF-MPIRUN execution example. In this case the user requested a cluster with eight FPGAs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RESOURCE</head><label>I</label><figDesc>USAGE OF THE FMC AND NRC IN A XCKU060Table II compares the deployment time of a bitstream via a JTAG interface with that of our TCP-based approach. The JTAG speed for the experiments in Table II was set to 5 Mbit s . As expected, the reconfiguration times of the partial bitfile are significantly shorter than the ones of the entire design.</figDesc><table><row><cell>Resource</cell><cell>Available</cell><cell>Total</cell><cell>Used FMC</cell><cell>NRC</cell></row><row><cell>LUT</cell><cell>331680</cell><cell>112891</cell><cell>1970</cell><cell>860</cell></row><row><cell>LUTRAM</cell><cell>146880</cell><cell>12795</cell><cell>55</cell><cell>16</cell></row><row><cell>FF</cell><cell>663360</cell><cell>140668</cell><cell>6729</cell><cell>2193</cell></row><row><cell>BRAM</cell><cell>1080</cell><cell>303</cell><cell>5</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">CONFIGURATION TIMES</cell><cell></cell></row><row><cell>operation JTAG configuration of the complete design</cell><cell>file size in MB 24.1</cell><cell>average time in seconds 42.8</cell><cell>average speed in kB s 560.747</cell></row><row><cell>POST /configure of partial bitfile over TCP</cell><cell>4.4</cell><cell>0.5</cell><cell>8, 264.112</cell></row><row><cell>JTAG configuration of partial bitfile</cell><cell>4.4</cell><cell>8.2</cell><cell>535.714</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A cloud-scale acceleration architecture</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-49</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Proceedings -2017 IEEE 25th Annual Symposium on High-Performance Interconnects</title>
		<author>
			<persName><forename type="first">F</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hagleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paredes</surname></persName>
		</author>
		<idno type="DOI">10.1109/HOTI.2017.13</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="29" to="32" />
		</imprint>
	</monogr>
	<note>An FPGA platform for hyperscalers</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic partial reconfiguration in FPGAs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng-Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/IITA.2009.334</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Symposium on Intelligent Information Technology Application</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="445" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2015.42</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="10" to="22" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Real-time AI: Microsoft announces preview of Project Brainwave</title>
		<author>
			<persName><forename type="first">A</forename><surname>Linn</surname></persName>
		</author>
		<ptr target="https://blogs.microsoft.com/ai/build-2018-project-brainwave/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Serving DNNs in Real Time at Datacenter Scale with Project Brainwave</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2018.022071131</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Amazon EC2 F1 Instances</title>
		<ptr target="https://aws.amazon.com/ec2/instance-types/f1/" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Amazon.com, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><surname>Research -China</surname></persName>
		</author>
		<ptr target="https://www.research.ibm.com/labs/china/supervessel.html" />
		<title level="m">OpenPOWER Cloud -Accelerating Cloud Computing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://github.com/aws/aws-fpga" />
		<title level="m">Overview of AWS EC2 FPGA Development Kit</title>
		<imprint>
			<publisher>Amazon.com, Inc</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="https://github.com/aws/aws-fpga/blob/master/hdk/README.md" />
		<title level="m">AWS FPGA Hardware Development Kit (HDK)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">SuperVessel: The Open Cloud Service for OpenPOWER</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>White paper, IBM corporation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enabling FPGAs in the cloud</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2597917.2597929</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Conference on Computing Frontiers -CF &apos;14</title>
		<meeting>the 11th ACM Conference on Computing Frontiers -CF &apos;14<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FPGAs in the cloud: Booting virtualized hardware accelerators with OpenStack</title>
		<author>
			<persName><forename type="first">S</forename><surname>Byma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Steffan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bannazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leon-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<idno type="DOI">10.1109/FCCM.2014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -2014 IEEE 22nd International Symposium on Field-Programmable Custom Computing Machines, FCCM</title>
		<meeting>-2014 IEEE 22nd International Symposium on Field-Programmable Custom Computing Machines, FCCM</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Heterogeneous virtualized network function framework for the data center</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tarafdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Eskandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leon-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<idno type="DOI">10.23919/FPL.2017.8056790</idno>
	</analytic>
	<monogr>
		<title level="m">2017 27th International Conference on Field Programmable Logic and Applications</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enabling Flexible Network FPGA Clusters in a Heterogeneous Cloud Data Center</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tarafdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bannazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leon-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<idno type="DOI">10.1145/3020078.3021742</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays -FPGA &apos;17</title>
		<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays -FPGA &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Designing for FPGAs in the Cloud</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tarafdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Eskandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<idno type="DOI">10.1109/MDAT.2017.2748393</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Design and Test</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="29" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Achieving 10Gbps line-rate key-value stores with FPGAs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vissers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>B?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Istv?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotCloud (USENIX Workshop on Hot Topics in Cloud Computing)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FPGAs and the Cloud -An Endless Tale of Virtualization , Elasticity and Efficiency</title>
		<author>
			<persName><forename type="first">O</forename><surname>Knodel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Genssler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Erxleben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Spallek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Advances in Systems and Measurements</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3 &amp; 4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enabling FPGAs in hyperscale data centers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hagleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herkersdorf</surname></persName>
		</author>
		<idno type="DOI">10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.199</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)</title>
		<imprint>
			<date type="published" when="2015">2015. 2016</date>
			<biblScope unit="page" from="1078" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Disaggregated FPGAs: network performance comparison against bare-metal servers, virtual machines and linux containers</title>
		<idno type="DOI">10.1109/CloudCom.2016.0018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom</title>
		<meeting>the International Conference on Cloud Computing Technology and Science, CloudCom</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Network-attached FPGAs for data center applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Polig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hagleitner</surname></persName>
		</author>
		<idno type="DOI">10.1109/FPT.2016.7929186</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Field-Programmable Technology</title>
		<meeting>the 2016 International Conference on Field-Programmable Technology</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grance</surname></persName>
		</author>
		<title level="m">The NIST Definition of Cloud Computing Recommendations of the National Institute of Standards and Technology</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology, Information Technology Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Architectural Styles and the Design of Network-based Software Architectures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Fielding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Irvine</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">Xilinx</forename><surname>Inc</surname></persName>
		</author>
		<idno>2017.4]</idno>
	</analytic>
	<monogr>
		<title level="m">Vivado Design Suite User Guide: Partial Reconfiguration (UG909</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AMBA AXI and ACE Protocol Specification</title>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arm</title>
		<imprint>
			<biblScope unit="page" from="1" to="306" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><surname>Fielding</surname></persName>
		</author>
		<ptr target="https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html" />
		<title level="m">RFC2616 -Hypertext Transfer Protocol -HTTP/1.1</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Openstack</forename><surname>Community</surname></persName>
		</author>
		<ptr target="https://www.openstack.org/assets/software/projectmap/openstack-map.pdf" />
		<title level="m">OpenStack Component Map</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Debug Bridge v3.0 LogiCORE IP Product Guide PG245</title>
		<author>
			<persName><forename type="first">Xilinx</forename><surname>Inc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MPI : A STAN-DARD MESSAGE PASSING INTERFACE</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONTRI-BUTION TO TOP 500 REPORT</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Openmpi</forename><surname>Community</surname></persName>
		</author>
		<ptr target="https://www.open-mpi.org/doc/" />
		<title level="m">Open MPI Documentation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">MPICH Documentation</title>
		<author>
			<persName><surname>Mpich Community</surname></persName>
		</author>
		<ptr target="http://www.mpich.org/documentation/guides/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A modular heterogeneous stack for deploying fpgas and cpus in the data center</title>
		<author>
			<persName><forename type="first">N</forename><surname>Eskandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tarafdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ly-Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289602.3293909</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA &apos;19</title>
		<meeting>the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA &apos;19<address><addrLine>Seaside, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="262" to="271" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
