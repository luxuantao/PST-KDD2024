<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian kernel based classification for financial distress detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-01-18">18 January 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tony</forename><surname>Van Gestel</surname></persName>
							<email>tony.vangestel@dexia.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DEXIA Group</orgName>
								<orgName type="institution">Credit Risk Modelling, RMG</orgName>
								<address>
									<addrLine>Square Meeus 1</addrLine>
									<postCode>B-1000</postCode>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">ESAT</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">SCD-SISTA</orgName>
								<address>
									<addrLine>Kasteelpark Arenberg 10</addrLine>
									<postCode>B-3001</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bart</forename><surname>Baesens</surname></persName>
							<email>bart.baesens@econ.kuleuven.ac.be</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Applied Economic Sciences</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">LIRIS</orgName>
								<address>
									<addrLine>Naamsestraat 69</addrLine>
									<postCode>B-3000</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johan</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
							<email>johan.suykens@esat.kuleuven.ac.be</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">ESAT</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">SCD-SISTA</orgName>
								<address>
									<addrLine>Kasteelpark Arenberg 10</addrLine>
									<postCode>B-3001</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Van Den Poel</surname></persName>
							<email>dirk.vandenpoel@ugent.be</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Marketing</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<addrLine>Hoveniersberg 24</addrLine>
									<postCode>9000</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dirk-Emma</forename><surname>Baestaens</surname></persName>
							<email>dirk.baestaens@fortisbank.com</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Fortis Bank Brussels</orgName>
								<orgName type="department" key="dep2">Financial Markets Research</orgName>
								<address>
									<addrLine>Warandeberg 3</addrLine>
									<postCode>B-1000</postCode>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marleen</forename><surname>Willekens</surname></persName>
							<email>marleen.willekens@econ.kuleuven.ac.be</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Applied Economic Sciences</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">LIRIS</orgName>
								<address>
									<addrLine>Naamsestraat 69</addrLine>
									<postCode>B-3000</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian kernel based classification for financial distress detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-01-18">18 January 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">4B3E9CC07DAF0244913D604018F7A624</idno>
					<idno type="DOI">10.1016/j.ejor.2004.11.009</idno>
					<note type="submission">Received 7 August 2003; accepted 3 November 2004</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Credit scoring</term>
					<term>Kernel Fisher discriminant analysis</term>
					<term>Least Squares Support Vector Machine classifiers</term>
					<term>Bayesian inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Corporate credit granting is a key commercial activity of financial institutions nowadays. A critical first step in the credit granting process usually involves a careful financial analysis of the creditworthiness of the potential client. Wrong decisions result either in foregoing valuable clients or, more severely, in substantial capital losses if the client subsequently defaults. It is thus of crucial importance to develop models that estimate the probability of corporate bankruptcy with a high degree of accuracy. Many studies focused on the use of financial ratios in linear statistical models, such as linear discriminant analysis and logistic regression. However, the obtained error rates are often high. In this paper, Least Squares Support Vector Machine (LS-SVM) classifiers, also known as kernel Fisher discriminant analysis, are applied within the Bayesian evidence framework in order to automatically infer and analyze the creditworthiness of potential corporate clients. The inferred posterior class probabilities of bankruptcy are then used to analyze the sensitivity of the classifier output with respect to the given inputs and to assist in the credit assignment decision making process. The suggested nonlinear kernel based classifiers yield better performances than linear discriminant analysis and logistic regression when applied to a real-life data set concerning commercial credit granting to mid-cap Belgian and Dutch firms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Corporate bankruptcy does not only cause substantial losses to the business community, but also to society as a whole. Therefore, accurate bankruptcy prediction models are of critical importance to various stakeholders (i.e. management, investors, employees, shareholders and other interested parties) as it provides them with timely warnings. From a managerial perspective, financial failure forecasting tools allow to take timely strategic actions such that financial distress can be avoided. For other stakeholders, such as banks, efficient and automated credit rating tools allow to detect clients that are to default their obligations at an early stage. Hence, accurate bankruptcy prediction tools will enable them to increase the efficiency of one of their core activities, i.e. commercial credit assignment.</p><p>Financial failure occurs when the firm has chronic and serious losses and/or when the firm becomes insolvent with liabilities that are disproportionate to assets. Widely identified causes and symptoms of financial failure include poor management, autocratic leadership and difficulties in operating successfully in the market. The common assumption underlying bankruptcy prediction is that a firmÕs financial statements appropriately reflect all these characteristics. Several classification techniques have been suggested to predict financial distress using ratios and data originating from these statements. While early univariate approaches used ratio analysis, multivariate approaches combine multiple ratios and characteristics to predict potential financial distress <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Linear multiple discriminant approaches (LDA), like AltmanÕs Z-Scores, attempt to identify the most efficient hyperplane to linearly separate between successful and non-successful firms. At the same time, the most significant combination of predictors is identified by using a stepwise selection procedure. However, these techniques typically rely on the linear separability assumption, as well as normality assumptions.</p><p>Motivated by their universal approximation property, multilayer perceptron (MLP) neural networks <ref type="bibr" target="#b3">[4]</ref> have been applied to model nonlinear decision boundaries in bankruptcy prediction and credit assignment problems <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Although advanced learning methods like Bayesian inference <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> have been developed for MLPs, their practical design suffers from drawbacks like the non-convex optimization problem and the choice of the number of hidden units. In Support Vector Machines (SVMs), Least Squares SVMs (LS-SVMs) and related kernel based learning techniques <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, the inputs are first mapped into a high dimensional kernel induced feature space in which the regressor or classifier are constructed by minimizing an appropriate convex cost function. Applying MercerÕs theorem, the solution is obtained in the dual space from a finite dimensional convex quadratic programming problem for SVMs or a linear Karush-Kuhn-Tucker system in the case of LS-SVMs, avoiding explicit knowledge of the high dimensional mapping and using only the related positive (semi) definite kernel function.</p><p>In this paper, we apply LS-SVM classifiers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>, also known as kernel Fisher Discriminant Analysis <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, within the Bayesian evidence framework <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> to predict financial distress of Belgian and Dutch firms with middle market capitalization. After having inferred the hyperparameters of the LS-SVM classifier on different levels of inference, we apply a backward input selection procedure by ranking the model evidence of the different input sets. Posterior class probabilities are obtained by marginalizing over the model parameters in order to infer the probability of making a correct decision and to detect difficult cases that should be referred to further investigation. The obtained results are compared with linear discriminant analysis and logistic regression using leave-one-out cross-validation <ref type="bibr" target="#b21">[22]</ref>.</p><p>This paper is organized as follows. The linear and nonlinear kernel based classification techniques are reviewed in Sections 2-4. Bayesian learning for LS-SVMs is outlined in Section 5. Empirical results on financial distress prediction are reported in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Empirical linear discriminant analysis</head><p>Given a number n of explanatory variables or inputs x ¼ ½x 1 ; . . . ; x n 2 R n of a firm, the problem we are concerned with is to predict whether this firm will default its obligations (y = À1) or not (y = +1). This problem corresponds to a binary classification problem with class C À ðy ¼ À1Þ denoting the class of (future) bankrupt firms and class C þ ðy ¼ þ1Þ the class of solvent firms. Let p(xjy) denote the class probability density of observing the inputs x given the class label y and let p + = P(y = +1), p À = P(y = À1) denote the prior class probabilities, then the Bayesian decision rule to predict ŷ is as follows:</p><formula xml:id="formula_0">ŷ ¼ sign½Pðy ¼ þ1jxÞ À Pðy ¼ À1jxÞ;<label>ð1Þ</label></formula><formula xml:id="formula_1">ŷ ¼ sign½logðPðy ¼ þ1jxÞÞ À logðPðy ¼ À1jxÞÞ;<label>ð2Þ</label></formula><formula xml:id="formula_2">ŷ ¼ sign½logðpðxjy ¼ þ1ÞÞ À logðpðxjy ¼ À1ÞÞ þ logðp þ =p À Þ;<label>ð3Þ</label></formula><p>where the third expression is obtained by applying BayesÕ formula</p><formula xml:id="formula_3">pðyjxÞ ¼ pðyÞpðxjyÞ Pðy ¼ þ1Þpðxjy ¼ þ1Þ þ Pðy ¼ À1Þpðxjy ¼ À1Þ</formula><p>and omitting the normalizing constant in the denominator. This Bayesian decision rule is known to yield optimal performance as it minimizes the risk of misclassification for each instance x. In the case of Gaussian class densities with means m À , m + and equal covariance matrix R x , the Bayesian decision rule becomes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> </p><formula xml:id="formula_4">ŷ ¼ sign½w T x þ b ¼ sign½z ð<label>4Þ</label></formula><p>with latent variable z = w T x + b and where</p><formula xml:id="formula_5">w ¼ R À1 x ðm þ À m À Þ and b ¼ w T ðm þ þ m À Þ=2 þ logðp þ =p À Þ</formula><p>. This is known as Linear Discriminant Analysis (LDA). In the case of unequal class covariance matrices, a quadratic discriminant is obtained <ref type="bibr" target="#b22">[23]</ref>.</p><p>As the class densities p(xjy) are typically unknown in practice, one has to estimate the decision rule from given training data D ¼ fðx i ; y i Þg N i¼1 . A common way to estimate the linear discriminant (4) is by solving</p><formula xml:id="formula_6">ðŵ; bÞ ¼ arg min w;b 1 2 X N i¼1 ðy i À ðw T x i þ bÞÞ 2 :<label>ð5Þ</label></formula><p>The solution ðŵ; bÞ follows from a linear set of equations of dimension (n + 1) • (n + 1) and corresponds 1  to the Fisher Discriminant solution <ref type="bibr" target="#b24">[25]</ref>, which has been used in the pioneering paper of Altman <ref type="bibr" target="#b0">[1]</ref>. The least squares formulation with binary targets (À1, +1) has the additional interpretation as an asymptotical optimal least squares approximation to the Bayesian discriminant function P(y = +1jx) À P(y = À1jx) <ref type="bibr" target="#b22">[23]</ref>. This formulation is also often used for training neural network classifiers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. Instead of minimizing a least squares cost function or estimating the covariance matrices, one may also relate the probability P(y = +1) to the latent variable z via the logistic link function <ref type="bibr" target="#b25">[26]</ref>. The probabilistic interpretation of the inverse link function P(y = +1) = 1/(1 + exp(Àz)) allows to estimate ŵ and b from maximum likelihood <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_7">ðŵ; bÞ ¼ arg min w;b X N i¼1 log 1 þ expðÀy i ðw T x i þ bÞÞ À Á :<label>ð6Þ</label></formula><p>1 More precisely, Fisher related the maximization of the Rayleigh quotient to a regression approach with targets ðÀN =n À No analytic solution exists, but the solution can be obtained by applying NewtonÕs method corresponding to an iteratively reweighted least squares algorithm <ref type="bibr" target="#b23">[24]</ref>. The first use of applying logistic regression for bankruptcy prediction has been reported in <ref type="bibr" target="#b26">[27]</ref>.</p><formula xml:id="formula_8">D ; N =n þ D Þ, with n þ D and</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Support vector machines and kernel based learning</head><p>The Multilayer Perceptron (MLP) neural network is a popular neural network for both regression and classification and has often been used for bankruptcy prediction and credit scoring in general <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Although there exist good training algorithms (e.g. Bayesian inference) to design the MLP, there are still a number of drawbacks like the choice of the architecture of the MLP and the existence of multiple local minima, which implies that the estimated parameters may not be uniquely determined. Recently, a new learning technique emerged, called Support Vector Machines (SVMs) and related kernel based learning methods in general, in which the solution is unique and follows from a convex optimization problem <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. The regression formulations are also related to kernel Fisher discriminant analysis <ref type="bibr" target="#b19">[20]</ref>, Gaussian processes and regularization networks <ref type="bibr" target="#b32">[33]</ref>, where the latter have been applied to modelling option prices <ref type="bibr" target="#b33">[34]</ref>.</p><p>Although the general nonlinear version of Support Vector Machines (SVM) is quite recent, the roots of the SVM approach for constructing an optimal separating hyperplane for pattern recognition date back to 1963 and 1964 <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linear SVM classifier: Separable case</head><p>Consider a training set of N data points fðx i ; y i Þg N i¼1 , with input data x i 2 R n and corresponding binary class labels y i 2 {À1, +1}. When the data of the two classes are separable (Fig. <ref type="figure" target="#fig_2">1a</ref>), one can say that</p><formula xml:id="formula_9">w T x i þ b P þ1 if y i ¼ þ1; w T x i þ b 6 À1 if y i ¼ À1:</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&amp;</head><p>This set of two inequalities can be combined into one single set as follows:</p><formula xml:id="formula_10">y i ðw T x i þ bÞ P þ1; i ¼ 1; . . . ; N :<label>ð7Þ</label></formula><p>As can be seen from Fig. <ref type="figure" target="#fig_2">1a</ref>, multiple solutions are possible. From a generalization perspective, it is best to choose the solution with largest margin 2/kwk 2 . Support vector machines are modelled within a context of convex optimization theory <ref type="bibr" target="#b36">[37]</ref>. The general methodology is to start formulating the problem in the primal weight space as a constrained optimization problem, next formulate the Lagrangian, take the conditions for optimality and finally solve the problem in the dual space of Lagrange multipliers, which are also called support values. The optimization problem for the separable case aims at maximizing the margin 2/kwk 2 subject to the constraint that all training data points need to be correctly classified. This gives the following primal (P) problem in w:</p><formula xml:id="formula_11">min w;b J P ðwÞ ¼ 1 2 w T w s:t: y i ðw T x i þ bÞ P 1; i ¼ 1; . . . ; N :<label>ð8Þ</label></formula><p>The Lagrangian for this constraint optimization problem is Lðw; b; aÞ ¼ 0:5w T w À P N i¼1 a i ðy i ðw T x i þ bÞ À 1Þ, with Lagrange multipliers a i P 0 (i = 1, . . . , N). The solution is the saddle point of the Lagrangian:</p><formula xml:id="formula_12">max a min w;b L:<label>ð9Þ</label></formula><p>The conditions for optimality for w and b are</p><formula xml:id="formula_13">oL ow ! w ¼ X N i¼1 a i y i x i ; oL ob ! X N i¼1 a i y i ¼ 0: 8 &gt; &lt; &gt; :<label>ð10Þ</label></formula><p>From the first condition in <ref type="bibr" target="#b9">(10)</ref>, the classifier (4) expressed in terms of the Lagrange multipliers (support values) becomes</p><formula xml:id="formula_14">yðxÞ ¼ sign X N i¼1 a i y i x T i x þ b ! :<label>ð11Þ</label></formula><p>Replacing (10) into (9), the dual (D) problem in the Lagrange multipliers a is the following Quadratic Programming problem (QP):</p><formula xml:id="formula_15">max a J D ðaÞ ¼ À 1 2 X N i;j¼1 y i y j x T i x j a i a j þ X N i¼1 a i ¼ À 1 2 a T Xa þ 1 T a s:t: X N i¼1 a i y i ¼ 0; a i P 0; i ¼ 1; . . . ; N ;<label>ð12Þ</label></formula><formula xml:id="formula_16">with a = [a 1 , . . . , a N ] T , 1 ¼ ½1; . . . ; 1 T 2 R N and X 2 R N ÂN</formula><p>, where X ij ¼ y i y j x T i x j (i, j = 1, . . . , N). The matrix X is positive (semi-) definite by construction. In the case of a positive definite matrix, the solution to this QP problem is global and unique. In the case of a positive semi-definite matrix, the solution is global, but not necessarily unique in terms of the Lagrange multipliers a i , while still a unique solution in terms of w ¼ P N i¼1 a i y i x i is obtained <ref type="bibr" target="#b36">[37]</ref>. An interesting property, called the sparseness property, is that many of the resulting a i values are equal to zero. The training data points x i corresponding to non-zero a i are called support vectors. These support vectors are located close to the decision boundary. From a non-zero support value a i &gt; 0, b is obtained from y i (w T x i + b) À 1 = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Linear SVM classifier: Non-separable case</head><p>In most practical, real-life classification problems, the data are non-separable in linear or nonlinear sense, due to the overlap between the two classes (see Fig. <ref type="figure" target="#fig_2">1b</ref>). In such cases, one aims at finding a classifier that separates the data as much as possible. The SVM classifier formulation <ref type="bibr" target="#b7">(8)</ref> is extended to the nonseparable case by introducing slack variables n i P 0 in order to tolerate misclassifications <ref type="bibr" target="#b37">[38]</ref>. The inequalities are changed into</p><formula xml:id="formula_17">y i ðw T x i þ bÞ P 1 À n i ; i ¼ 1; . . . ; N ;<label>ð13Þ</label></formula><p>where the ith inequality is violated when n i &gt; 1.</p><p>In the primal weight space, the optimization problem becomes min w;b;n</p><formula xml:id="formula_18">J P ðwÞ ¼ 1 2 w T w þ c X N i¼1 n i s:t: y i ðw T x i þ bÞ P 1 À n i ; i ¼ 1; . . . ; N ; n i P 0; i ¼ 1; . . . ; N ;<label>ð14Þ</label></formula><p>where c is a positive real constant that determines the trade-off between the large margin term 0.5w T w and error term</p><formula xml:id="formula_19">P N i¼1 n i . The Lagrangian is equal to L ¼ 0:5w T w þ c P N i¼1 n i À P N i¼1 a i ðy i ðw T x i þ bÞ À 1 þ n i Þ À P N i¼1 m i n i , with</formula><p>Lagrange multipliers a i P 0, m i P 0 (i = 1, . . . , N). The solution is given by the saddle point of the Lagrangian max a;m min w;b;n Lðw; b; n; a; mÞ, with conditions for optimality</p><formula xml:id="formula_20">oL ow ! w ¼ X N i¼1 a i y i x i ; oL ob ! X N i¼1 a i y i ¼ 0; oL on i ! 0 6 a i 6 c; i ¼ 1; . . . ; N : 8 &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; :<label>ð15Þ</label></formula><p>Replacing (15) in ( <ref type="formula" target="#formula_18">14</ref>) yields the following dual QP-problem:</p><formula xml:id="formula_21">max a J D ðaÞ ¼ À 1 2 X N i;j¼1 y i y j x T i x j a i a j þ X N i¼1 a i ¼ À 1 2 a T Xa þ 1 T a s:t: X N i¼1 a i y i ¼ 0; 0 6 a i 6 c; i ¼ 1; . . . ; N :<label>ð16Þ</label></formula><p>The bias term b is obtained as a by-product of the QP-calculation or from a non-zero support value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Kernel trick and Mercer condition</head><p>The linear SVM classifier is extended to a nonlinear SVM classifier by first mapping the inputs in a nonlinear way x # u(x) into a high dimensional space, called feature space in SVM terminology. In this high dimensional feature space, a linear separating hyperplane w T u(x) + b = 0 is constructed using <ref type="bibr" target="#b11">(12)</ref>, as is depicted in Fig. <ref type="figure">2</ref>.</p><p>A key element of nonlinear SVMs is that the nonlinear mapping u( AE ) : x # u(x) may not be explicitly known, but is defined implicitly in terms of the positive (semi-) definite kernel function satisfying the Mercer condition</p><formula xml:id="formula_22">Kðx 1 ; x 2 Þ ¼ uðx 1 Þ T uðx 2 Þ:<label>ð17Þ</label></formula><p>Given the kernel function K(x 1 , x 2 ), the nonlinear classifier is obtained by solving the dual QP-problem, in which the product x T i x j is replaced by u(x i ) T u(x j ) = K(x i , x j ), e.g., X = [y i y j u(x i ) T u(x j )]. The nonlinear SVM classifier is then obtained as</p><formula xml:id="formula_23">yðxÞ ¼ sign½w T uðxÞ þ b ¼ sign X N i¼1 a i y i Kðx i ; xÞ þ b " # :<label>ð18Þ</label></formula><p>In the dual space, the score z ¼ P N i¼1 a i y i Kðx i ; xÞ þ b is obtained as a weighted sum of the kernel functions evaluated in the support vectors and the evaluated point x, with weights a i y i .</p><p>A popular choice for the kernel function is the radial basis function (RBF) kernel Kðx i ;</p><formula xml:id="formula_24">x j Þ ¼ expfÀkx i À x j k 2 2 =r 2 g</formula><p>, where r is a tuning parameter. Other typical kernel functions are the linear kernel</p><formula xml:id="formula_25">Kðx i ; x j Þ ¼ x T i x j ; the polynomial kernel Kðx i ; x j Þ ¼ ðs þ x T i x j Þ</formula><p>d with degree d and tuning parameter s P 0; and MLP kernel Kðx i ;</p><formula xml:id="formula_26">x j Þ ¼ tanhðj 1 x T i x j þ j 2 Þ.</formula><p>The latter is not positive semi-definite for all choices of the tuning parameters j 1 and j 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Least Squares Support Vector Machines</head><p>The LS-SVM classifier formulation can be obtained by modifying the SVM classifier formulation as follows: min w;b;e</p><formula xml:id="formula_27">J P ðwÞ ¼ 1 2 w T w þ c 2 X N i¼1 e 2 C;i<label>ð19Þ</label></formula><formula xml:id="formula_28">s:t: y i ½w T uðx i Þ þ b ¼ 1 À e C;i ; i ¼ 1; . . . ; N :<label>ð20Þ</label></formula><p>Besides the quadratic cost function, an important difference with standard SVMs is that the formulation consists now of equality instead of inequality constraints <ref type="bibr" target="#b15">[16]</ref>.</p><p>The LS-SVM classifier formulation ( <ref type="formula" target="#formula_27">19</ref>), ( <ref type="formula" target="#formula_28">20</ref>) implicitly corresponds to a regression interpretation ( <ref type="formula" target="#formula_30">22</ref>), <ref type="bibr" target="#b22">(23)</ref> with binary targets y i = ±1. By multiplying the error e C,i with y i and using y 2 i ¼ 1, the sum of squared error term</p><formula xml:id="formula_29">P N i¼1 e 2 C;i becomes X N i¼1 e 2 C;i ¼ X N i¼1 ðy i e C;i Þ 2 ¼ X N i¼1 e 2 i ¼ ðy i À ðw T uðx i Þ þ bÞÞ 2<label>ð21Þ</label></formula><p>with the regression error e i = y i À (w T u(x i ) + b) = y i e C,i . The LS-SVM classifier is then constructed as follows:</p><p>Fig. <ref type="figure">2</ref>. Illustration of SVM based classification. The inputs are first mapped in a nonlinear way to a high-dimensional feature space (x # u(x)), in which a linear separating hyperplane is constructed. Applying the Mercer condition (K(x i ,x j ) = u(x 1 ) T u(x 2 )), a nonlinear classifier in the input space is obtained. min w;b;e</p><formula xml:id="formula_30">J P ¼ 1 2 w T w þ c 1 2 X N i¼1 e 2 i<label>ð22Þ</label></formula><formula xml:id="formula_31">s:t: e i ¼ y i À ðw T uðx i Þ þ bÞ; i ¼ 1; . . . ; N :<label>ð23Þ</label></formula><p>Observe that the cost function is a weighted sum of a regularization term J w ¼ 0:5w T w and an error term J e ¼ 0:5 P N i¼1 e 2 i . One then solves the constrained optimization problem ( <ref type="formula" target="#formula_30">22</ref>), ( <ref type="formula" target="#formula_31">23</ref>) by constructing the Lagrangian Lðw; b; e; aÞ ¼ w</p><formula xml:id="formula_32">T w þ c 1 2 P N i¼1 e 2 i À P N i¼1 a i ðw T uðx i Þ þ b þ e i À y i Þ, with Lagrange multipliers a i 2 R (i = 1, . . . , N).</formula><p>The conditions for optimality are given by</p><formula xml:id="formula_33">oL ow ¼ 0 ! w ¼ X N i¼1 a i x i ; oL ob ¼ 0 ! X N i¼1 a i y i ¼ 0; oL oe ¼ 0 ! a ¼ ce; oL oa i ¼ 0 ! w T uðx i Þ þ b þ e i À y i ¼ 0; i ¼ 1; . . . ; N : 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; :<label>ð24Þ</label></formula><p>After elimination of the variables w and e, one gets the following linear Karush-Kuhn-Tucker (KKT) system of dimension (N + 1) • (N + 1) in the dual space <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>:</p><p>;</p><p>with y = [y 1 ; . . . ; y N ], 1 = [1; . . . ; 1], and a ¼ ½a 1 ; . . . ; a N 2 R N and where MercerÕs theorem <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> is applied within the X matrix:</p><formula xml:id="formula_35">X ij ¼ uðx i Þ T uðx j Þ ¼ Kðx i ; x j Þ.</formula><p>The LS-SVM classifier is then obtained as follows:</p><formula xml:id="formula_36">ŷ ¼ sign½w T uðxÞ þ b ¼ sign X N i¼1 a i Kðx; x i Þ þ b " #<label>ð26Þ</label></formula><formula xml:id="formula_37">with latent variable z ¼ P N i¼1 a i Kðx; x i Þ þ b.</formula><p>The support values a i (i = 1, . . . , N) in the dual classifier formulation determine the relative weight of each data point x i in the classifier decision <ref type="bibr" target="#b25">(26)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Bayesian interpretation and inference</head><p>The LS-SVM classifier formulation allows to estimate the classifier support values a and bias term b from the data D, given the regularization parameter c and the kernel function K, e.g., an RBF kernel with parameter r. Together with the set of explanatory ratios/inputs I f1; . . . ; ng, the kernel function and its parameters define the model structure M. These regularization and kernel parameters and input set need to be estimated from the data as well. This is achieved within the Bayesian evidence framework <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> that applies BayesÕ formula on three levels of inference <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>:</p><formula xml:id="formula_38">Posterior ¼ Likelihood Â Prior Evidence :<label>ð27Þ</label></formula><p>(1) The primal and dual model parameters w, b and a, b are inferred on the first level.</p><p>(2) The regularization parameter c = f/l is inferred on the second level, where l and f are additional parameters in the probabilistic inference. (3) The parameter of the kernel function, e.g., r, the (choice of) the kernel function K and the optimal input set are represented in the structural model description M, which is inferred on level 3.</p><p>A schematic overview of the three levels of inference is depicted in Fig. <ref type="figure">3</ref>, from which the hierarchical approach is observed in which the likelihood of level i is obtained from level i À 1 (i = 2, 3). Given the least squares formulation, the model parameters are multivariate normal distributed allowing for analytic expressions<ref type="foot" target="#foot_2">2</ref> on all levels of inference. In each subsection, BayesÕ formula is explained first, while practical expressions, computations and interpretations are given afterwards. All complex derivations are given in Appendix A. </p><p>where the last step is obtained since the evidence pðD j log l; log f; MÞ is a normalizing constant that does not depend upon w and b.</p><p>For the prior, no correlation between w and b is assumed: pðw; b j log l; MÞ ¼ pðw j log l; MÞpðb j MÞ / pðw j log l; MÞ, with a multivariate Gaussian prior on w with zero mean and covariance matrix l À1 I nu (n u being the dimension of the feature space) and an uninformative, flat prior on b:</p><formula xml:id="formula_40">pðw j log l; MÞ ¼ l 2p n f 2 exp À l 2 w T w ; pðb j MÞ ¼ constant:<label>ð29Þ</label></formula><p>The uniform prior distribution on b can be approximated by a Gaussian distribution with standard deviation r b ! 1. The prior states a belief that without any learning from data, the coefficients are zero with an uncertainty denoted by the variance 1/l.</p><p>It is assumed that the data are independently identically distributed for expressing the likelihood</p><formula xml:id="formula_41">pðDjw; b; log f; MÞ / Y N i¼1 pðy i ; x i jw; b; log f; MÞ / Y N i¼1 pðe i jw; b; log f; MÞ / f 2p N 2 exp À f 2 X N i¼1 e 2 i ! ;<label>ð30Þ</label></formula><p>where the last step is by assumption. This corresponds to the assumption that the z-score w T u(x) + b is Gaussian distributed around the targets +1 and À1.</p><p>Given that the prior (29) and likelihood <ref type="bibr" target="#b29">(30)</ref> are multivariate normal distributions, the posterior (28) is a multivariate normal distribution<ref type="foot" target="#foot_4">3</ref> in [w; b] with mean ½w mp ; b mp 2 R nuþ1 and covariance matrix Q 2 R ðnuþ1ÞÂðnuþ1Þ . An alternative expression for the posterior is obtained by substituting ( <ref type="formula" target="#formula_40">29</ref>) and ( <ref type="formula" target="#formula_41">30</ref>) into <ref type="bibr" target="#b27">(28)</ref>. These approaches yield Fig. <ref type="figure">3</ref>. Different levels of Bayesian inference. The posterior probability of the model parameters w and b is inferred from the data D by applying Bayes formula on the first level for given hyperparameters l (prior) and f (likelihood) and the model structure M. The model parameters are obtained by maximizing the posterior. The evidence on the first level becomes the likelihood on the second level when applying Bayes formula to infer l and f (with c = f/l) from the given data D. The optimal hyperparameters l mp and f mp are obtained by maximizing the corresponding posterior on level 2. Model comparison is performed on the third level in order to compare different model structures, e.g., with different candidate input sets and/or different kernel parameters. The likelihood on the third level is equal to the evidence from level 2. Comparing different model structures M, that model structure with the highest posterior probability is selected.</p><p>pðw; bj log l; log f;</p><formula xml:id="formula_42">MÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi detðQ À1 Þ ð2pÞ nuþ1 s exp À 1 2 ½w À w mp ; b À b mp Q À1 ½w À w mp ; b À b mp ð31Þ / l 2p n f 2 exp À l 2 w T w f 2p N 2 exp À f 2 X N i¼1 e 2 i ! ;<label>ð32Þ</label></formula><p>respectively. The evidence is a normalizing constant in (28) independent of w and b such that</p><formula xml:id="formula_43">R R Á Á Á R pðw; bjD; log l; log f; MÞdw 1 Á Á Á dw nu db ¼ 1.</formula><p>Substituting the expressions for the prior (29), likelihood <ref type="bibr" target="#b29">(30)</ref> and posterior <ref type="bibr" target="#b31">(32)</ref>  </p><formula xml:id="formula_44">5.1.2<label>ð33Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Computation and interpretation</head><p>The model parameters with maximum posterior probability are obtained by minimizing the negative logarithm of ( <ref type="formula">31</ref>) and (32):</p><formula xml:id="formula_45">ðw mp ; b mp Þ ¼ arg min w;b J P;1 ðw; bÞ ¼ J P;1 ðw mp ; b mp Þ þ 1 2 ð½w À w mp ; b À b mp Q À1 ½w À w mp ; b À b mp Þ<label>ð34Þ</label></formula><formula xml:id="formula_46">¼ l 2 w T w þ f 2 X N i¼1 e 2 i ;<label>ð35Þ</label></formula><p>where constants are neglected in the optimization problem. Both expressions yield the same optimization problem and the covariance matrix Q is equal to the inverse of the Hessian H of J P;1 . The Hessian is expressed in terms of the matrix U = [u(x 1 ), . . . u(x N )] T with regressors, as derived in the appendix.</p><p>Comparing <ref type="bibr" target="#b34">(35)</ref> with <ref type="bibr" target="#b21">(22)</ref>, one obtains the same optimization problem for c = f/l up to a constant scaling. The optimal w mp and b mp are computed in the dual space from the linear KKT-system <ref type="bibr" target="#b24">(25)</ref> with c = f/ l and the scoring function z ¼ w T mp uðxÞ þ b mp is expressed in terms of the dual parameters a and bias term b mp via <ref type="bibr" target="#b25">(26)</ref>.</p><p>Substituting ( <ref type="formula" target="#formula_40">29</ref>), ( <ref type="formula" target="#formula_41">30</ref>) and ( <ref type="formula" target="#formula_42">32</ref>) into <ref type="bibr" target="#b32">(33)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>:</head><p>The model evidence consists of the likelihood of the data and an Occam factor that penalizes for too complex models. The Occam factor consists of the regularization term 0:5w T mp w mp and the ratio (l n u /det H) 1/2 which is a measure for the volume of the posterior probability divided by the volume of the prior probability. Strong contractions of the posterior versus prior space indicates too many free parameters and, hence, overfitting on the training data. The evidence will be maximized on level 2, where also dual space expressions are derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Inference of hyper-parameters (level 2)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Bayes' formula</head><p>The optimal regularization parameters l and f are inferred from the given data D by applying BayesÕ rule on the second level <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>: pðlog l; log fjD; MÞ ¼ pðDj log l; log f; MÞpðlog l; log fÞ pðDjMÞ :</p><p>The prior pðlog l; log fjMÞ ¼ pðlog ljMÞpðlog fjMÞ ¼ constant is taken to be a flat uninformative prior (r log l , r log f ! 1). The level 2 likelihood pðDj log l; log f; MÞ is equal to the level 1 evidence <ref type="bibr" target="#b35">(36)</ref>. In this way, Bayesian inference implicitly embodies OccamÕs razor: on level 2 the evidence of the level 1 is optimized so as to find a trade-off between the model fit and a complexity term to avoid overfitting <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>The level 2 evidence is obtained in a similar way as on level 1 as the likelihood for the maximum a posteriori times the ratio of the volume of the posterior probability and the volume of the prior probability:</p><p>pðDjMÞ ' pðDj log l mp ; log f mp ; MÞ r log ljD r log fjD r log l r log f ;</p><p>where one typically approximates the posterior probability by a multivariate normal probability function with diagonal covariance matrix diagð½r 2 log ljD ; r 2 log ljD Þ 2 R 2Â2 . Neglecting all constants, BayesÕ formula (37) becomes pðlog l; log fjD; MÞ / pðDj log l; log f; MÞ;</p><p>where the expressions for the level 1 evidence are given by ( <ref type="formula" target="#formula_44">33</ref>) and (36).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Computation and interpretation</head><p>In the primal space, the hyperparameters are obtained by minimizing the negative logarithm of ( <ref type="formula">36</ref>) and (39):</p><formula xml:id="formula_50">ðl mp ; f mp Þ ¼ arg min l;f J P;2 ðl; fÞ ¼ lJ w ðw mp Þ þ fJ e ðw mp ; b mp Þ þ 1 2 log det H À n u 2 log l À N 2 log f:<label>ð40Þ</label></formula><p>Observe that in order to evaluate <ref type="bibr" target="#b39">(40)</ref> one needs also to calculate w mp and b mp for the given l and f and evaluate the level 1 cost function.</p><p>The determinant of H is equal to (see Appendix A for details)</p><formula xml:id="formula_51">detðHÞ ¼ ðfN Þ detðlI nu þ fU T M c UÞ;</formula><p>with the idempotent centering matrix</p><formula xml:id="formula_52">M c ¼ I N À 1=N 11 T ¼ M 2 c 2 R N ÂN .</formula><p>The determinant is also equal to the product of the eigenvalues. The n e non-zero eigenvalues k 1 ; . . . ; k ne of U T M c U are equal to the n e nonzero eigenvalues of M c UU T M c ¼ M c XM c 2 R N ÂN , which can be calculated in the dual space. Substituting the determinant detðHÞ <ref type="bibr" target="#b39">(40)</ref>, one obtains the optimization problem in the dual space</p><formula xml:id="formula_53">¼ fN l nuÀne Q ne i¼1 ðl þ fk i Þ into</formula><formula xml:id="formula_54">J D;2 ðl; fÞ ¼ lJ w ðw mp Þ þ fJ e ðw mp ; b mp Þ 1 2 X n e i¼1 logðl þ fk i Þ À n e 2 log l À n e À 1 2 log f;<label>ð41Þ</label></formula><p>where it can be shown by matrix algebra that</p><formula xml:id="formula_55">lJ w ðw mp Þ þ fJ e ðw mp ; b mp Þ ¼ 1 2 y T M c 1 l M c XM c þ 1 f I N À1 M c</formula><p>y. An important concept in neural networks and Bayesian learning in general is the effective number of parameters. Although there are n u + 1 free parameters w 1 ; . . . ; w nu , b in the primal space, the use of these parameters <ref type="bibr" target="#b34">(35)</ref> is restricted by the use of the regularization term 0.5w T w. The effective number of parameters d eff is equal to d eff ¼ P i k i;u =k i;r , where k i,u , k i,r denote the eigenvalues of the Hessian of the unregularized cost function J 1;u ¼ fE D and the regularized cost function J 1;r ¼ lE W þ fE D <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. For LS-SVMs, the effective number of parameters is equal to</p><formula xml:id="formula_56">d eff ¼ 1 þ X n e i¼1 fk i l þ fk i ¼ 1 þ X n e i¼1 ck i 1 þ ck i ;<label>ð42Þ</label></formula><formula xml:id="formula_57">with c ¼ f=l 2 R þ .</formula><p>The term +1 appears because no regularization is applied on the bias term b. As shown in the appendix, one has that n e 6 N À 1 and, hence, also that d eff 6 N, even in the case of high dimensional feature spaces.</p><p>The conditions for optimality for (41) are obtained by putting oJ 2 =ol ¼ oJ 2 =of ¼ 0. One obtains<ref type="foot" target="#foot_5">4</ref> </p><formula xml:id="formula_58">oJ 2 =ol ¼ 0 ! 2l mp J w ðw mp ; l mp ; f mp Þ ¼ d eff ðl mp ; f mp Þ À 1;<label>ð43Þ</label></formula><formula xml:id="formula_59">oJ 2 =of ¼ 0 ! 2f mp J e ðw mp ; b mp ; l mp ; f mp Þ ¼ N À d eff ;<label>ð44Þ</label></formula><p>where the latter equation corresponds to the unbiased estimate of the noise variance 1=f mp ¼</p><formula xml:id="formula_60">1 2 P N i¼1 e 2 i =ðN À d eff Þ.</formula><p>Instead of solving the optimization problem in l and f, one may also reformulate ( <ref type="formula" target="#formula_54">41</ref>) using ( <ref type="formula" target="#formula_58">43</ref>), <ref type="bibr" target="#b43">(44)</ref> in terms of c = f/l and solve the following scalar optimization problem:</p><formula xml:id="formula_61">min c X N À1 i¼1 log k i þ 1 c þ ðN À 1Þ logðJ w ðw mp Þ þ cJ e ðw mp ; b mp ÞÞ<label>ð45Þ</label></formula><p>with</p><formula xml:id="formula_62">J e ðw mp ; b mp Þ ¼ 1 2c 2 y T M c VðK þ I N =cÞ À2 V T M c y;<label>ð46Þ</label></formula><formula xml:id="formula_63">J w ðw mp Þ ¼ 1 2 y T M c VKðK þ I=cÞ À2 V T M c y;<label>ð47Þ</label></formula><formula xml:id="formula_64">J w ðw mp Þ þ cJ e ðw mp ; b mp Þ ¼ 1 2 y T M c VðK þ I N =cÞ À1 V T M c y<label>ð48Þ</label></formula><p>with the eigenvalue decomposition M c XM c = V T KV. Given the optimal c mp from (45) one finds the effective number of parameters</p><formula xml:id="formula_65">d eff from d eff ¼ 1 þ P ne i¼1 ck i =ð1 þ ck i Þ.</formula><p>The optimal l mp and f mp are obtained from l mp ¼ ðd eff À 1Þ=ð2J w ðw mp ÞÞ and f mp ¼ ðN À d eff Þ=ð2J e ðw mp ; b mp ÞÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Model comparison (level 3)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Bayes' formula</head><p>The model structure M of the model determines the remaining parameters of the kernel based model: the selected kernel function (linear, RBF, etc.), the kernel parameter (RBF kernel parameter r) and selected explanatory inputs. The model structure is inferred on level 3.</p><p>Consider, e.g., the inference of the RBF-kernel parameter r, where the model structure is denoted by M r . BayesÕ formula for the inference of M r is equal to pðM r jDÞ / pðDjM r ÞpðM r Þ; ð49Þ where no evidence pðDÞ is used in the expression on level 3 as it is in practice impossible to integrate over all model structures. The prior probability pðM r Þ is assumed to be constant. The likelihood is equal to the level 2 evidence (38).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Computation and interpretation</head><p>Substituting the evidence ( <ref type="formula" target="#formula_48">38</ref>) into (49) and taking into account the constant prior, the BayesÕ rule (38) becomes pðMjDÞ ' pðDj log l mp ; log f mp ; MÞ r log ljD r log fjD r log l r log f :</p><p>As uninformative priors are used on level 2, the standard deviations r log l and r log f of the prior distribution both tend to infinity and are omitted in the comparisons of different models in (50). The posterior error bars can be approximated analytically as r 2 log ljD ' 2=ðd eff À 1Þ and r 2 log fjD ' 2=ðN À d eff Þ, respectively <ref type="bibr" target="#b12">[13]</ref>. The level 3 posterior becomes pðM r jDÞ ' pðDj log l mp ; log</p><formula xml:id="formula_67">f mp ; M r Þ r log ljD r log fjD r log l r log f / ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi l ne mp f N À1 mp ðd eff À 1ÞðN À d eff Þ Q ne i¼1 ðl mp þ f mp k i Þ s ;<label>ð51Þ</label></formula><p>where all expressions can be calculated in the dual space. A practical way to infer the kernel parameter r is to calculate (51) for a grid of possible kernel parameters r 1 , . . . , r m and to compare the corresponding posterior model parameters pðM r 1 jDÞ; . . . ; pðM rm jDÞ. An additional observation is that the RBF-LS-SVM classifier may not always yield a monotonic relation between the evolution of the ratio (e.g., solvency ratio) and the default risk. This is due to the nonlinearity of the classifier and/or multivariate correlations. In case monotonous relations are important, one may choose to use a combined kernel function K(x 1 , x 2 ) = jK lin (x 1 , x 2 ) + (1 À j)K RBF (x 1 , x 2 ), where the parameter j 2 [0, 1] can be determined on level 3. In this paper, the use of an RBF-kernel is illustrated.</p><p>Model comparison is also used to infer the set of most relevant inputs <ref type="bibr" target="#b20">[21]</ref> out of the given set of candidate explanatory variables by making pairwise comparisons of models with different input sets. In a backward input selection procedure, one starts from the full candidate input set and removes in each input pruning step that input that yields the best model improvement (or smallest decrease) in terms of the model probability (51). The procedure is stopped when no significant decrease of the model probability is observed. In the case of equal prior model probabilities pðM i Þ ¼ pðM j Þ ("i, j) the models M i and M j are compared according to their Bayes factor</p><formula xml:id="formula_68">B ij ¼ pðDjM i Þ pðDjM j Þ ¼ pðDj log l i ; log f i ; M i Þ pðDj log l j ; log f j ; M j Þ r log l i jD r log f i jD r log l j jD r log f j jD :<label>ð52Þ</label></formula><p>According to <ref type="bibr" target="#b38">[39]</ref>, one uses the values in Table <ref type="table" target="#tab_2">1</ref> in order to report and interpret the significance of model M i improving on model M j . </p><p>This expression will then be used in BayesÕ rule (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Computation and interpretation</head><p>In the level 1 formulation, it was assumed that the errors e are normally distributed around the targets ±1 with variance f À1 , i.e., pðxjy ¼ þ1; w; b; f;</p><formula xml:id="formula_70">MÞ ¼ ð2p=fÞ À1=2 expðÀ1=2fe 2 þ Þ;<label>ð54Þ</label></formula><formula xml:id="formula_71">pðxjy ¼ À1; w; b; f; MÞ ¼ ð2p=fÞ À1=2 expðÀ1=2fe 2 À Þ;<label>ð55Þ</label></formula><p>with e + = +1 À (w T u(x) + b) and e À = À1 À (w T u(x) + b), respectively. The assumption that the mean zscores per class are equal to +1 and À1 will be relaxed and for the calculation of the moderated output, it is assumed that the scores z are normally distributed with centers t + (Class +1) and t À (Class À1) <ref type="bibr" target="#b19">[20]</ref>. Defining the Boolean vectors</p><formula xml:id="formula_72">1 þ ¼ ½y i ¼ þ1 2 R N and 1 À ¼ ½y i ¼ À1 2 R N</formula><p>, with elements 0 and 1 whether the observation i is an element of C À and C þ for 1 + and vice versa for 1 À . The centers are estimated as</p><formula xml:id="formula_73">t þ ¼ w T m u þ þ b and t À ¼ w T m u À þ b with the feature vector class means m u;þ ¼ 1=N þ P y i ¼þ1 uðx i Þ ¼ 1=N þ U T 1 þ and m u;À ¼ 1=N À P y i ¼À1 uðx i Þ ¼ 1=N À U T 1 À .</formula><p>The variances are denoted by 1/f + and 1/f -, respectively, and represent the uncertainty around the projected class centers t + and t -. It is typically assumed that f + = f -= f ± .</p><p>The parameters w and b are estimated from the data with resulting probability density function <ref type="bibr" target="#b30">(31)</ref>. Due to the uncertainty on w (and b), the errors e + and e À have expected value 6</p><formula xml:id="formula_74">ê ¼ w T mp ðuðxÞ À m u Þ ¼ X N i¼1 Kðx; x i Þ À t ;</formula><p>where t ¼ w T mp m u is obtained in the dual space as t ¼ 1=N a T X1 . The expression for the variance is</p><formula xml:id="formula_75">r 2 e ¼ ½uðxÞ À m u T Q 11 ½uðxÞ À m u :<label>ð56Þ</label></formula><p>The dual formulations for the variance are derived in the appendix based on the singular value decomposition (A.7) of Q 11 and is equal to</p><formula xml:id="formula_76">r 2 e ¼ 1 l Kðx; xÞ À 2 lN hðxÞ T 1 þ 1 lN 2 1 T X1 À f l hðxÞ À 1 N þ 1 T T Â M c ðlI N þ fM c XM c Þ À1 M c hðxÞ À 1 N 1 T X ;<label>ð57Þ</label></formula><p>with • either + or À. The vector hðxÞ 2 R N has elements h i (x) = K(x, x i ). 6 The • notation is used to denote either + or À, since analogous expressions are obtained for classes C þ and C À , respectively. where we omitted the hyperparameters l, f, f ± for notational convenience. Approximate analytic expressions exist for marginalizing over the hyperparameters, but can be neglected in practice as the additional variance is rather small <ref type="bibr" target="#b12">[13]</ref>.</p><p>The moderated likelihood (53) is then equal to</p><formula xml:id="formula_77">pðxjy ¼ 1; f; MÞ ¼ ð2p=ðf AE þ r 2 e ÞÞ À1=2 expðÀ1=2ê 2 =ðf À1 AE þ r 2 e ÞÞ:<label>ð58Þ</label></formula><p>Substituting (58) into the Bayesian decision rule (3), one obtains a quadratic decision rule as the class variances f À1 AE þ r 2 eÀ and f À1 AE þ r 2 eþ are not equal. Assuming that r 2 eþ ' r 2 eÀ and defining r e ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffi ffi r eþ r eÀ p , the Bayesian decision rule becomes</p><formula xml:id="formula_78">ŷ ¼ sign 1 l X N i¼1 a i Kðx; x i Þ À m dþ þ m dÀ 2 þ f À1 AE þ r 2 e ðxÞ m dþ À m dÀ log Pðy ¼ þ1Þ Pðy ¼ À1Þ " # :<label>ð59Þ</label></formula><p>The variance f À1 AE ¼ P N i¼1 e 2 AE;i =ðN À d eff Þ is estimated in the same way as f mp on level 2. The prior probabilities P(y = +1) and P(y = À1) are typically estimated as pþ</p><formula xml:id="formula_79">¼ N þ =ðN þ þ N À Þ and pÀ ¼ N À =ðN þ þ N À Þ,</formula><p>but can also be adjusted to reject a given percentage of applicants or to optimize the total profit taking into account misclassification costs. As (59) depends explicitly on the prior probabilities, it also allows to make point-in-time credit decisions where the default probabilities and recovery rates depend upon the point in the business cycle. Difficult cases having almost equal posterior class probabilities Pðy ¼ þ1jx; D; MÞ ' Pðy ¼ À1jx; D; MÞ can be decided to not being automatically processed and to being referred to a human expert for further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Bayesian classifier design</head><p>Based on the previous theory, the following practical design scheme to design the LS-SVM classifier in the Bayesian framework is suggested:</p><p>(1) Preprocess the data by completing missing values and handling outliers. Standardize the inputs to zero mean and unit variance. (2) Define models M i by choosing a candidate input set I i , a kernel function K i and kernel parameter, e.g., r i in the RBF kernel case. For all models M i , with i ¼ 1; . . . ; n M (with n M the number of models to be compared), compute the level 3 posterior: (a) Find the optimal hyperparameters l mp and f mp by solving the scalar optimization problem (45) in c = f/l related to maximizing the level 2 posterior. <ref type="foot" target="#foot_7">7</ref> With the resulting c mp , compute the effective number of parameters, the hyperparameters l mp and f mp . (b) Evaluate the level 3 posterior (51) for model comparison.</p><p>(3) Select the model M i with maximal evidence. If desired, refine the model tuning parameters K i ; r i ; I i to further optimize the classifier and go back to Step 2; else: go to step 4. (4) Given the optimal M H i , calculate a and b from (25), with kernel K i , parameter r i and input set I i . Calculate f H and select pþ and pÀ to evaluate (59).</p><p>For illustrative purposes, the design scheme is illustrated for a kernel function with one parameter r like the RBF-kernel. The design scheme is easily extended to other kernel functions or combinations of kernel functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Financial distress prediction for mid-cap firms in the benelux</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Data set description</head><p>The bankruptcy data, obtained from a major Benelux financial institution, were used to build an internal rating system <ref type="bibr" target="#b39">[40]</ref> for firms with middle-market capitalization (mid-cap firms) in the Benelux countries (Belgium, The Netherlands, Luxembourg) using linear modelling techniques. Firms in the mid-cap segment are defined as follows: they are not stocklisted, the book value of their total assets exceeds 10 mln euro, and they generate a turnover that is smaller than 0.25 bln euro. Note that more advanced methods like option based valuation models are not applicable since these companies are not listed. Together with small and medium enterprises, mid-cap firms represent a large proportion of the economy in the Benelux. The mid-cap market segment is especially important as it reflects an important business orientation of the bank.</p><p>The data set consists of N = 422 observations, n À D ¼ 74 bankrupt and n þ D ¼ 348 solvent companies. The data on the bankrupt firms were collected from 1991 to 1997, while the other data were extracted from the period 1997 only (for reasons of data retrieval difficulties). One out of five non-bankrupt observations of the 1997 database was used to train the model. Observe that a larger sample of solvent firms could have been selected, but involves training on an even more unbalanced <ref type="foot" target="#foot_8">8</ref> training set. A total number of 40 candidate input variables was selected from financial statement data, using standard liquidity, profitability and solvency measures. As can be seen from Table <ref type="table" target="#tab_3">2</ref>, both ratios as well as trends of ratios are considered.</p><p>The data were preprocessed as follows. Median imputation was applied to missing values. Outliers outside the interval ½ m À 2:5 Â s; m þ 2:5 Â s were put equal to the upper limit and lower limit, respectively; where m is the sample mean and s the sample standard deviation. A similar procedure is, e.g., used in the calculation of the Winsorized mean <ref type="bibr" target="#b40">[41]</ref>. The log transformation was applied to size variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Performance measures</head><p>The performance of all classifiers will be quantified using both the classification accuracy and the area under the receiver operating characteristic curve (AUROC). The classification accuracy simply measures the percentage of correctly classified (PCC) observations. Two closely related performance measures are the sensitivity which is the percentage of positive observations being classified as positive (PCC p ) and the specificity which is the percentage of negative observations being classified as negative (PCC n ). The receiver operating characteristic curve (ROC) is a two-dimensional graphical illustration of the sensitivity on the y-axis versus 1-specificity on the x-axis for various values of the classifier threshold <ref type="bibr" target="#b41">[42]</ref>. It basically illustrates the behaviour of a classifier without regard to class distribution or misclassification cost. The AUROC then provides a simple figure-of-merit for the performance of the constructed classifier. We will use McNemarÕs test to compare the PCC, PCC p and PCC n of different classifiers <ref type="bibr" target="#b42">[43]</ref> and the test of De Long et al. <ref type="bibr" target="#b43">[44]</ref> to compare the AUROCs. The ROC curve is also closely related to the Cumulative Accuracy Profile which is in turn related to the power statistic and Gini-coefficient <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Models with full candidate input set</head><p>The Bayesian framework was applied to infer the hyper-and kernel parameters. The kernel parameter r of the RBF kernel <ref type="foot" target="#foot_9">9</ref> was inferred on level 3 by selecting the parameter from the grid ffiffi ffi n p Â ½0:1; 0:5; 1; 1:2; 1:5; 2; 3; 4; 10. For each of these bandwidth parameters, the kernel matrix was constructed and its eigenvalue decomposition computed. The optimal hyperparameter c was determined from the scalar optimization problem <ref type="bibr" target="#b44">(45)</ref> and then, l, f, d eff and the level 3 cost were calculated. As the number of default data is low, no separate test data set was used. The generalization performance is assessed by means of the leaveone-out cross-validation error, which is a common measure in the bankruptcy prediction literature <ref type="bibr" target="#b21">[22]</ref>. In Table <ref type="table" target="#tab_4">3</ref>, we have contrasted the PCC, PCC p , PCC n and AUROC performance of the LS-SVM <ref type="bibr" target="#b25">(26)</ref> and the The inputs include various liquidity (L), solvency (S), profitability (P) and size (V) measures. Trends (Tr) are used to describe the evolution of the ratios (R). The results of backward input selection are presented by reporting the number of remaining inputs in the LDA, LOGIT and LS-SVM model when an input is removed. These ranking numbers are underlined when the corresponding input is used in the model having optimal leave-one-out cross-validation performance. Hence, inputs with low importance have a high number, while the most important input has rank 1.</p><p>Bayesian LS-SVM decision rule (59) classifier with the performance of the linear LDA and Logit classifiers.</p><p>The numbers between brackets represent the p-values of the tests between each classifier and the classifier scoring best on the particular performance measure. It is easily observed that both the LS-SVM and LS-SVM Bay classifiers yield very good performances when compared to the LDA and Logit classifiers. The corresponding ROC curves are depicted in the left pane of Fig. <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Models with optimized input set</head><p>Given the models with full candidate input set, a backward input selection procedure is applied to infer the most relevant inputs from the data. For the LDA and Logit classifiers, each time the input i was removed for which the coefficient had the highest p-value to test whether the coefficient is significantly different from zero. The procedure was stopped when all coefficients were significantly different from zero at the 1% level. A backward input selection procedure was applied with the LS-SVM model, computing each time the model probability (on level 3) with one of the inputs removed. The input that yielded the best decrease (or smallest increase) in the level 3 cost function was then selected. The procedure was stopped just before the difference with the optimal model became decisive according to Table <ref type="table" target="#tab_2">1</ref>. In order to reduce the numbers of inputs as much as possible, but still retain a liquidity ratio in the model, 11 inputs are selected, which is one before the limit of becoming decisively different. The level 3 cost function and the corresponding leaveone-out PCC are depicted in Fig. <ref type="figure" target="#fig_5">5</ref> with respect to the number of removed inputs. Notice the similarities between both curves during the input removal process. Table <ref type="table" target="#tab_5">4</ref> reports the performances of all classifiers using the optimally pruned set of inputs. Again it can be observed that the LS-SVM and LS-SVM Bay  classifiers yield very good performances when compared to the LDA and Logit classifiers. The ROC curves on the optimized input sets are reported in the right pane of Fig. <ref type="figure" target="#fig_3">4</ref>. The order of input removal is reported in Table <ref type="table" target="#tab_3">2</ref>. It can be seen that the pruned LS-SVM classifier has 11 inputs, the pruned LDA classifier 10 inputs and the pruned Logit classifier 6 inputs. Starting from a total set of 40 inputs, this clearly illustrates the efficiency of the suggested input selection procedure. All classifiers seem to agree on the importance of the turnover variable and the solvency variable. Consistent with prior studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, the inputs of the LS-SVM classifier consist of a mixture of profitability, solvency and liquidity ratios; but the exact ratios that are selected differ. Also, liquidity ratios seem to be less decisive as compared to prior bankruptcy studies. The number of days to customer credit is the only liquidity ratio that is withheld and only classifies as the 11th input; its trend is the second most important liquidity input in the backward input selection procedure. The three most important inputs for the LS-SVM classifier are the 2 solvency measures (solvency ratio, capital and reserves (percentage of total assets)), the size variable total assets and the profitability measures return on equity and turnover (percentage of total assets). Note that the five most important inputs for the LS-SVM classifier are also present in the optimally pruned LDA classifier.</p><p>The posterior class probabilities were computed for the evaluation of the decision rule (59) in a leaveone-out procedure, as mentioned above. These probabilities can also be used to identify the most difficult cases, which can be classified in an alternative way requiring e.g. human intervention. Referring the 10% most difficult cases to further analysis, the following classification performances were obtained on the remaining cases: PCC 93.12%, PCC p 99.69%, PCC n 52.83%. In the case of 25% removal, we obtained PCC 94.64%, PCC p 99.65%, PCC n 52.94%. These results clearly motivate the use of posterior class probabilities to allow the system to detect whether it should remark that its decision is too uncertain and needs further investigation.  In order to gain insight in the performance improvements of the different models, the full data sample was used, oversampling the non-defaults 7 times so as to obtain a more realistic sample because 7 years of defaults were combined with 1 year of non-defaults. The corresponding average default/bankruptcy rate is equal to 0.60% or 60 bps (basis points). The graph depicted in Fig. <ref type="figure" target="#fig_6">6</ref> reports the remaining default rate on the full portfolio as a function of the percentage of the ordered portfolio. In the ideal case, the curve would be a straight line from (0%, 60 bps) to (0.6%, 0 bps); a random scoring function that does not succeed in discriminating between weak and strong firms results into a diagonal line. The slope of the curve is a measure for the default rate at that point. Consider, e.g., the case where one decides not to grant credits to the 10% counterparts with the worst scores. The default rates on the full 100% portfolio (with 10% liquidities) are 26 bps (LDA), 27 bps (Logit) and 16 bps (LS-SVM), respectively. Taking into account the fact that the number of counterparts is reduced from 100% to 90%, the default rates on the invested part of the portfolio are obtained by multiplication with 1/0.90 and are equal to 29 bps (LDA), 30 bps (Logit) and 18 bps (LS-SVM), respectively, corresponding to the slope between the points at 10% and 100% (x-axis). From this graph, the better performance of the LS-SVM classifier becomes obvious from a practical perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>Prediction of business failure is becoming more and more a key component of risk management for financial institutions nowadays. In this paper, we illustrated and evaluated the added value of Bayesian LS-SVM classifiers in this context. We conducted experiments using a bankruptcy data set on the Benelux mid-cap market. The suggested Bayesian nonlinear kernel based classifiers yield better performances than the more traditional methods, such as logistic regression and linear discriminant analysis, in terms of classification accuracy and area under the receiver operating characteristic curve. The set of relevant explanatory variables was inferred from the data by applying Bayesian model comparison in a backward input-selection procedure. By adopting the Bayesian way of reasoning, one easily obtains posterior class probabilities that can be of high importance to credit managers for analysing the sensitivities of the classifier decisions with respect to the given inputs.  In matrix expressions, it is useful to express U T U À 1 N U T 11 T U as U T M c U with the idempotent centering matrix</p><formula xml:id="formula_80">M c ¼ I N À 1 N 11 T 2 R N ÂN having M c ¼ M 2 c . Given that F À1 11 ¼ ðlI nu þ fU T M c UÞ À1 , the inverse Hessian H À1 = Q is equal to Q ¼ ðlI nu þ fU T M c UÞ À1 À 1 N ðlI nu þ fU T M c UÞ À1 U T 1 À 1 N 1 T UðlI n u þ fU T M c UÞ À1 1 fN þ 1 N 2 1 T UðlI n þ fU T M c UÞ À1 U T 1 " # : A.2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Expression for the determinant</head><p>The determinant of H is obtained from (A.2) using the fact that the determinant of a product is equal to the product of the determinants and is thus equal to detðHÞ ¼ detðH 11 À h T 12 h À1 22 h 12 Þ Â detðh 22 Þ ¼ detðlI n u þ fU T M c UÞ Â ðfN Þ; ðA:4Þ</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>n À D the number of positive and negative training instances. The solution only differs in the choice of the bias term b and a scaling of the coefficients w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of linear SVM classification in a two dimensional input space: (a) separable case; (b) non-separable case. The margin of the SVM classifier is equal to 2/kwk 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5. 1 .</head><label>1</label><figDesc>Inference of model parameters (level 1) 5.1.1. Bayes' formula Applying BayesÕ formula on level 1, one obtains the posterior probability of the model parameters w and b: pðw; b j D; log l; log f; MÞ ¼ pðD j w; b; log l; log f; MÞpðw; b j log l; log f; MÞ pðD j log l; log f; MÞ / pðD j w; b; log l; log f; MÞpðw; b j log l; log f; MÞ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5. 4 .</head><label>4</label><figDesc>Moderated output of the classifier 5.4.1. Moderated output Based on the Bayesian interpretation, an expression is derived for the likelihood pðxjy; w; b; f; MÞ of observing x given the class label y and the parameters w; b; f; M. However, the parameters 5 w and b are multivariate normal distributed. Hence, the moderated likelihood is obtained as pðxjy; f; MÞ ¼ Z pðxjy; w; b; f; MÞpðw; bjy; l; f; MÞdw 1 Á Á Á dw nu db:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Receiver operating characteristic curves for the full input set (left) and pruned input set (right): LS-SVM (solid line), Logit (dashed-dotted line) and LDA (dashed line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Evolution of the level 3 cost function À log pðMjDÞ and the leave-one-out cross-validation classification performance. The dashed line denotes where the model becomes different from the optimal model in a decisive way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Default rates (leave-one-out) on the full portfolio as a function of the percentage of refused counterparts for the LDA (dotted line), Logit (dashed line) and LS-SVM (solid line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>into<ref type="bibr" target="#b27">(28)</ref>, one obtains pðDj log l; log f; MÞ ¼ pðw mp j log l; MÞpðDjw mp ; b mp ; log f; MÞ pðw mp ; b mp jD; log l; log f; MÞ :</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Evidence against H 0 (no improvement of M i over M j ) for different values of the Bayes factor B ij<ref type="bibr" target="#b38">[39]</ref> </figDesc><table><row><cell>2 ln B ij</cell><cell>B ij</cell><cell>Evidence against H 0</cell></row><row><cell>0-2</cell><cell>1-3</cell><cell>Not worth more than a bare mention</cell></row><row><cell>2-5</cell><cell>3-12</cell><cell>Positive</cell></row><row><cell>5-10</cell><cell>12-150</cell><cell>Strong</cell></row><row><cell>&gt;10</cell><cell>&gt;150</cell><cell>Decisive</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Benelux data set: description of the 40 candidate inputs</figDesc><table><row><cell>Input variable description</cell><cell>LDA</cell><cell>LOGIT</cell><cell>LS-SVM</cell></row><row><cell>L: Current ratio (R)</cell><cell>36</cell><cell>1</cell><cell>23</cell></row><row><cell>L: Current ratio (Tr)</cell><cell>34</cell><cell>27</cell><cell>28</cell></row><row><cell>L: Quick ratio (R)</cell><cell>22</cell><cell>26</cell><cell>24</cell></row><row><cell>L: Quick ratio (Tr)</cell><cell>35</cell><cell>30</cell><cell>29</cell></row><row><cell>L: Numbers of days to customer credit (R)</cell><cell>29</cell><cell>19</cell><cell>11</cell></row><row><cell>L: Numbers of days to customer credit (Tr)</cell><cell>6</cell><cell>14</cell><cell>19</cell></row><row><cell>L: Numbers of days of supplier credit (R)</cell><cell>21</cell><cell>21</cell><cell>27</cell></row><row><cell>L: Numbers of days of supplier credit (Tr)</cell><cell>25</cell><cell>33</cell><cell>21</cell></row><row><cell>S:Capital and reserves (% TA)</cell><cell>5</cell><cell>5</cell><cell>2</cell></row><row><cell>S: Capital and reserves (Tr)</cell><cell>20</cell><cell>18</cell><cell>35</cell></row><row><cell>S: Financial debt payable after one year (% TA)</cell><cell>37</cell><cell>37</cell><cell>31</cell></row><row><cell>S: Financial debt payable after one year (Tr)</cell><cell>40</cell><cell>39</cell><cell>8</cell></row><row><cell>S: Financial debt payable within one year (% TA)</cell><cell>38</cell><cell>38</cell><cell>18</cell></row><row><cell>S: Financial debt payable within one year (Tr)</cell><cell>39</cell><cell>40</cell><cell>17</cell></row><row><cell>S: Solvency Ratio (%)(R)</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell>S: Solvency Ratio (%)(Tr)</cell><cell>14</cell><cell>16</cell><cell>10</cell></row><row><cell>P: Turnover (% TA)</cell><cell>2</cell><cell>4</cell><cell>5</cell></row><row><cell>P: Turnover (Trend)</cell><cell>19</cell><cell>12</cell><cell>32</cell></row><row><cell>P: Added value (% TA)</cell><cell>18</cell><cell>28</cell><cell>13</cell></row><row><cell>P: Added value (Tr)</cell><cell>24</cell><cell>36</cell><cell>40</cell></row><row><cell>V: Total assets (Log)</cell><cell>4</cell><cell>6</cell><cell>3</cell></row><row><cell>P: Total assets (Tr)</cell><cell>7</cell><cell>11</cell><cell>20</cell></row><row><cell>P: Current profit/current loss before taxes (R)</cell><cell>28</cell><cell>25</cell><cell>38</cell></row><row><cell>P: Current profit/current loss before taxes (Tr)</cell><cell>33</cell><cell>31</cell><cell>30</cell></row><row><cell>P: Gross operation margin (%)(R)</cell><cell>32</cell><cell>3</cell><cell>25</cell></row><row><cell>P: Gross operation margin (%)(Tr)</cell><cell>15</cell><cell>23</cell><cell>7</cell></row><row><cell>P: Current profit/current loss (R)</cell><cell>27</cell><cell>35</cell><cell>36</cell></row><row><cell>P: Current profit/current loss (Tr)</cell><cell>30</cell><cell>34</cell><cell>37</cell></row><row><cell>P: Net operation margin (%)(R)</cell><cell>31</cell><cell>20</cell><cell>26</cell></row><row><cell>P: Net operation margin (%)(Tr)</cell><cell>26</cell><cell>32</cell><cell>15</cell></row><row><cell>P: Added value/sales (%)(R)</cell><cell>13</cell><cell>17</cell><cell>6</cell></row><row><cell>P: Added value/sales (%)(Tr)</cell><cell>10</cell><cell>9</cell><cell>9</cell></row><row><cell>P: Added value/pers. employed (R)</cell><cell>23</cell><cell>29</cell><cell>39</cell></row><row><cell>P: Added value/pers. employed (Tr)</cell><cell>17</cell><cell>10</cell><cell>34</cell></row><row><cell>P: Cash-flow/equity (%)(R)</cell><cell>16</cell><cell>8</cell><cell>33</cell></row><row><cell>P: Cash-flow/equity (%)(Tr)</cell><cell>11</cell><cell>24</cell><cell>14</cell></row><row><cell>P: Return on equity (%)(R)</cell><cell>8</cell><cell>7</cell><cell>4</cell></row><row><cell>P: Return on equity (%)(Tr)</cell><cell>9</cell><cell>22</cell><cell>12</cell></row><row><cell>P: Net return on total assets before taxes and debt charges (%)(R)</cell><cell>1</cell><cell>13</cell><cell>16</cell></row><row><cell>P: Net return on total assets before taxes and debt charges (%)(Tr)</cell><cell>12</cell><cell>15</cell><cell>22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Leave-one-out classification performances (percentages) for the LDA, Logit and LS-SVM model using the full candidate input set</figDesc><table><row><cell></cell><cell>LDA</cell><cell>LOGIT</cell><cell>LS-SVM</cell><cell>LS-SVM Bay</cell></row><row><cell>PCC</cell><cell>84.83% (0.13%)</cell><cell>85.78% (6.33%)</cell><cell>88.39% (100%)</cell><cell>88.39% (100%)</cell></row><row><cell>PCC p</cell><cell>95.98% (0.77%)</cell><cell>93.97% (0.02%)</cell><cell>98.56% (100%)</cell><cell>98.56% (100%)</cell></row><row><cell>PCC n</cell><cell>32.43% (0.01%)</cell><cell>47.30% (100%)</cell><cell>40.54% (26.7%)</cell><cell>40.54% (26.7%)</cell></row><row><cell>AUROC</cell><cell>79.51% (0.02%)</cell><cell>80.07% (0.36%)</cell><cell>86.58% (43.27%)</cell><cell>86.65% (100%)</cell></row><row><cell cols="3">The corresponding p-values (percentages) are denoted in parentheses.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Leave-one-out classification performances for the LDA, Logit and LS-SVM model using the optimized input sets</figDesc><table><row><cell></cell><cell>LDA</cell><cell>LOGIT</cell><cell>LS-SVM</cell><cell>LS-SVM Bay</cell></row><row><cell>PCC</cell><cell>86.49 (3.76)</cell><cell>86.49 (4.46)</cell><cell>89.34 (100)</cell><cell>89.34 (100)</cell></row><row><cell>PCC p</cell><cell>98.28% (100%)</cell><cell>97.13% (34.28%)</cell><cell>98.28% (100%)</cell><cell>98.28% (100%)</cell></row><row><cell>PCC n</cell><cell>31.08% (1.39%)</cell><cell>36.49% (9.90%)</cell><cell>47.30% (100%)</cell><cell>47.30% (100%)</cell></row><row><cell>AUROC</cell><cell>83.32% (0.81%)</cell><cell>83.13% (0.58%)</cell><cell>89.46% (100%)</cell><cell>89.35% (47.38%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>H ¼ H 11 h 12 h 21 h 22 ! ¼ lI nu þ fU T U fU T 1 with corresponding block matrices H 11 = lI n u + fU T U, h 12 ¼ h T 21 ¼ U T 1and h 22 = N. The inverse Hessian H À1 is then obtained via a Schur complement type argument: ¼ h 12 h À1 22 and F 11 ¼ H 11 À h 12 h À1 22 h T 12 .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">"</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">f1 T U</cell><cell></cell><cell></cell><cell>fN</cell><cell>ðA:1Þ</cell></row><row><cell>H À1 ¼</cell><cell></cell><cell>"</cell><cell>0 T 1 I nu X</cell><cell cols="3">0 T # I nu ÀX 1 "</cell><cell>#</cell><cell>"</cell><cell cols="2">h T 12 H 11 h 12 h 22</cell><cell>ÀX T 1 # I nu 0 "</cell><cell>X T 1 # I nu 0 "</cell><cell># ! À1</cell></row><row><cell>¼</cell><cell></cell><cell>"</cell><cell>I nu X 0 T 1</cell><cell># "</cell><cell cols="5">H 11 À h 12 h À1 22 h T 12 0 T</cell><cell>0 h 22</cell><cell># I nu 0 " X T 1</cell><cell># ! À1</cell><cell>ðA:2Þ</cell></row><row><cell>¼</cell><cell>"</cell><cell cols="4">ðH 11 À h 12 h À1 22 h T 12 Þ Àh À1 22 h T 12 F À1 11</cell><cell>À1</cell><cell cols="4">ÀF À1 11 h 12 h À1 22 22 þ h À1 h À1 22 h T 12 F À1 11 h 12 h À1 22</cell><cell>#</cell><cell>ðA:3Þ</cell></row><row><cell>with X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>T. Van Gestel et al. / European Journal of Operational Research 172 (2006) 979-1003</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>T. Van Gestel et al. / European Journal of Operational Research 172 (2006) 979-1003</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Matlab implementations for the dual space expressions are available from http://www.esat.kuleuven.ac.be/sista/lssvmlab. Practical examples on classification with LS-SVMs are given in the demo democlass.m. For classification, the basic routines are trainlssvm.m for training by solving<ref type="bibr" target="#b24">(25)</ref> and simlssvm.m for evaluating<ref type="bibr" target="#b25">(26)</ref>. For Bayesian learning the main routines are bay_lssvm.m for computation of the level 1, 2 and</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>cost functions<ref type="bibr" target="#b34">(35)</ref>, and (41), (51), respectively, bay_optimize.m for optimizing the hyperparameters with respect to the cost functions, bay_lssvmARD.m for input/ratio selection and bay_modout-Class.m for evaluation of the posterior class probabilities (58), (59). Initial estimates for the hyperparameters c and r 2 of, e.g., an LS-SVM with RBF-kernel, are obtained using bay_initlssvm.m. More details are found in the LS-SVMlab tutorial on the same website. T. Van Gestel et al. / European Journal of Operational Research 172 (2006) 979-1003</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>The notation [x; y] = [x, y] T is used here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>In this derivation, one uses that oðJ P;1 ðw mp ; b mp ÞÞ=ol ¼ dðJ P;1 ðw mp ; b mp ÞÞ=dl þ dðJ P;1 ðw mp ; b mp ÞÞ=d½w; bj ½wmp ;bmp Â dð½w mp ; b mp Þ=dl ¼ J w ðw mp Þ; since dðJ P;1 ðw mp ; b mp ÞÞ=d½w; bj ½wmp;bmp ¼ 0 [13,16,31]. T. Van Gestel et al. / European Journal of Operational Research 172 (2006) 979-1003</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>The uncertainty on f only has a minor influence in a limited number of directions<ref type="bibr" target="#b12">[13]</ref> and is neglected.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>Observe that this implies in each iteration step maximizing the level 1 posterior in w and b.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8"><p>In practice, one typically observes that the percentage of defaults in training databases varies from 50% to about 70% or 80%<ref type="bibr" target="#b28">[29]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9"><p>The use of an RBF-kernel is illustrated here because of its consistently good performance on 20 benchmark data sets<ref type="bibr" target="#b30">[31]</ref>. The other kernel functions can be applied in a similar way. T. Van Gestel et al. / European Journal of Operational Research 172 (2006) 979-1003</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by Dexia, Fortis, the K.U. Leuven, the Belgian federal government (IUAP V, GOA-Mefisto 666) and the national science foundation (FWO) with project G.0407.02. This research was initiated when TVG was at the K.U. Leuven and continued at Dexia. TVG is a honorary postdoctoral researcher with the FWO-Flanders. The authors wish to thank Peter Van Dijcke, Joao Garcia, Luc Leonard, Eric Hermann, Marc Itterbeek, Daniel Saks, Daniel Feremans, Geert Kindt, Thomas Alderweireld, Carine Brasseur and Jos De Brabanter for helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Primal-dual formulations for Bayesian inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Expression for the Hessian and covariance matrix</head><p>The level 1 posterior probability pð½w; bjD; l; f; MÞ is a multivariate normal distribution in R n u with mean [w mp ; b mp ] and covariance matrix Q = H À1 , where H is the Hessian of the least squares cost function <ref type="bibr" target="#b18">(19)</ref>. Defining the matrix of regressors U T = [u(x 1 ), . . . , u(x n u )], the identity matrix I and the vector with all ones 1 of appropriate dimension; the Hessian is equal to which is obtained as the product of fN and the eigenvalues k i (i = 1, . . . , n u ) of lI n u + fU T M c U, noted as k i (lI n u + fU T M c U). Because the matrix U T M c U 2 R nuÂnu is rank deficient with rank n e 6 N À 1, n u À n e eigenvalues are equal to l.</p><p>The dual space expressions can be obtained in terms of the singular value decomposition</p><p>ÀneÞ , with 0 6 n e 6 N À 1. Due to the orthonormality property we have</p><p>Hence, one obtains the primal and dual eigenvalue decompositions</p><p>ðA:7Þ</p><p>The n u eigenvalues of</p><p>where the non-zero eigenvalues s 2 i (i = 1, . . . , n e ) are obtained from the eigenvalue decomposition of M c UU T M c from (A.7). The expression for the determinant is equal to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Expression for the level 1 cost function</head><p>The dual space expression for <ref type="bibr" target="#b18">(19)</ref>. Applying a similar reasoning and algebra as for the calculation of the determinant, one obtains the dual space expression:</p><p>Given that M c XM c = VKV T , with K ¼ diagð½s 2 1 ; . . . ; s 2 ne ; 0; . . . ; 0Þ, one obtains that (48). In a similar way, one obtains (46) and (47).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Expression for the moderated likelihood</head><p>The primal space expression for the variance in the moderated output is obtained from (56) and is equal to</p><p>Substituting (A.5) into the expression for Q 11 from (A.3), one can write Q 11 as</p><p>Substituting (A.9) into (A.10), one obtains (57) given that UU T = X, u(x i ) T u(x j ) = K(x i , x j ) and Uu(x) = h(x).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Financial ratios, discriminant analysis and the prediction of corporate bankruptcy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="589" to="609" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
		<title level="m">Corporate Financial Distress and Bankruptcy: A Complete Guide to Predicting and Avoiding Distress and Profiting from Bankruptcy</title>
		<imprint>
			<publisher>Wiley Finance Edition</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Financial ratios as predictors of failure, empirical research in accounting selected studies</title>
		<author>
			<persName><forename type="first">W</forename><surname>Beaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Accounting Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="71" to="111" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
	<note>Suppl.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Corporate distress diagnosis: Comparisons using linear discriminant analysis and neural networks (the Italian experience)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Varetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Banking and Finance</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="505" to="529" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bankruptcy prediction for credit risk using neural networks: A survey and new results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="929" to="935" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D.-E</forename><surname>Baestaens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Network Solutions for Trading in Financial Markets</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Pitman, London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid neural network models for bankruptcy predictions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using feature construction to improve the performance of neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ragavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="416" to="430" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self organizing neural networks for financial diagnosis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Serrano</forename><surname>Cinca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="227" to="238" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural network applications in business: A review and analysis of the literature</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bodnovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Selvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="1988">1988-1995. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian interpolation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probable networks and plausible predictions-A review of practical Bayesian methods for supervised neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="469" to="505" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<title level="m">An Introduction to Support Vector Machines</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning with Kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scho ¨lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<title level="m">Least Squares Support Vector Machines</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized discriminant analysis using a kernel approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Baudat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Anouar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2385" to="2404" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Bayesian framework for least squares support vector machine classifiers, Gaussian processes and kernel Fisher discriminant analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lambrechts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1115" to="1147" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting financial time series using least squares support vector machines within the evidence framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-E</forename><surname>Baestaens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lambrechts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vandaele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks (Special Issue on Financial Engineering)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="809" to="821" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pitfalls in the application of discriminant analysis in business</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eisenbeis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="875" to="900" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Ripley</surname></persName>
		</author>
		<title level="m">Pattern Classification and Neural Networks</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
	<note>Generalized Linear Models</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Financial ratios and the probabilistic prediction of bankruptcy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ohlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Accounting Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="109" to="131" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using neural network rule extraction and decision tables for credit-risk evaluation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Setiono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="329" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmarking state of the art classification algorithms for credit scoring</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Viaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stepanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="627" to="635" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Developing intelligent systems for credit scoring using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Applied Economic Sciences, Katholieke Universiteit Leuven</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Benchmarking least squares support vector machine classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Viaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dedene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularization networks and support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A nonparametric approach to pricing and hedging derivative securities via learning networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="851" to="889" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pattern recognition using generalized portrait method</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automation and Remote Control</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="774" to="780" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the one class of the algorithms of pattern recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automation and Remote Control</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<title level="m">Practical Methods of Optimization</title>
		<imprint>
			<publisher>Chichester and New York</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Jeffreys</surname></persName>
		</author>
		<title level="m">Theory of Probability</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Credit risk modelling strategies: The road to serfdom</title>
		<author>
			<persName><forename type="first">D.-E</forename><surname>Baestaens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems in Accounting</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="225" to="235" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Finance &amp; Management</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><surname>Vaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asymptotic Statistics</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Signal Detection Theory and ROC analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Egan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Series in Cognition and Perception</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The Analysis of Contingency Tables</title>
		<author>
			<persName><forename type="first">B</forename><surname>Everitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Comparing the areas under two or more correlated receiver operating characteristic curves: A nonparametric approach</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Clarke-Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="837" to="845" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Validation methodologies for default risk models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Soberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keenan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Credit Magazine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="51" to="56" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
