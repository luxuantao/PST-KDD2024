<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">No-reference image and video quality estimation: Applications and human-motivated design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sheila</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amy</forename><forename type="middle">R</forename><surname>Reibman</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AT&amp;T Labs-Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">No-reference image and video quality estimation: Applications and human-motivated design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">865E74D214AC1A3D60EDA5C237580E32</idno>
					<idno type="DOI">10.1016/j.image.2010.05.009</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>No-reference Video quality Quality metrics Quality estimator Applications of quality metrics Blind quality assessment Image Communication 25 (2010) 469-481</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reviews the basic background knowledge necessary to design effective noreference (NR) quality estimators (QEs) for images and video. We describe a three-stage framework for NR QE that encompasses the range of potential use scenarios for the NR QE and allows knowledge of the human visual system to be incorporated throughout. We survey the measurement stage of the framework, considering methods that rely on bitstream, pixels, or both. By exploring both the accuracy requirements of potential uses as well as evaluation criteria to stress-test a QE, we set the stage for our community to make substantial future improvements to the challenging problem of NR quality estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Quality estimators (QE) for images and video have been the topics of numerous recent and not-so-recent surveys <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. This paper approaches the topic by providing a survey of what the authors consider to be basic background knowledge for the design of an effective QE. While a plethora of terms have been used in conjunction with quality to describe quality estimation and quality estimators (e.g., analysis, assessment, evaluation, measurement, metric, among others), we selected the verb estimate to reflect the statistical nature of the ground-truth subjective scores which a quality estimator strives to predict.</p><p>The goal of a QE is to characterize the quality of a test image or video, v ¼ v test , which is typically the output of a system. If Q subj ðÁÞ is the measured, perceived quality as estimated using an appropriate subjective experiment, then an ideal objective QE produces objective scores Q obj ðÁÞ which perfectly predict subjective scores Q subj ðÁÞ for all inputs. This is clearly challenging, as it requires not only that the QE be accurate for a wide range of input content and processing types, but also that the QE take into account a variety of environmental viewing conditions and a variety of viewers with disparate experience, expectations, and involvement.</p><p>To tackle this challenge, QE designers have taken a number of approaches to restrict the problem. One approach, taken by full-reference (FR) quality estimation, measures the quality of the test image or video v test relative to that of a reference v ref . A distorted image or video v test is written as the sum of an original v ref plus distortions d</p><formula xml:id="formula_0">v test ¼ v ref þ d:<label>ð1Þ</label></formula><p>FR QEs have the original signal v ref given as a priori information, so they are able to compute d exactly. However, a distortion as defined above is not necessarily visible. Therefore, the mere presence of d does not imply that subjective quality is degraded. FR QEs that use models of the human visual system (HVS) attempt to partition the distortions into those that are visible and those that are nonvisible as</p><formula xml:id="formula_1">v test ¼ v ref þðd nonvisible þd visible Þ,<label>ð2Þ</label></formula><p>Contents lists available at ScienceDirect In FR QEs, the original signal is considered to be a mask, and the goal is to determine how the mask affects the distortions that are introduced by the processing chain.</p><p>In practice, however, FR algorithms are only applicable when both v test and v ref can be made available at the same physical location. In addition, one fundamental assumption of most FR QEs is that the original v ref has maximum quality. This assumption may be violated when v ref represents the image or video prior to an enhancement algorithm (e.g., edge enhancement), or even prior to applying high-rate quantization, which may have a denoising effect.</p><p>In contrast, in no-reference (NR) quality estimation, neither v ref nor d is available. As such, NR QEs are the most broadly applicable type of QE, and are the focus of this paper. Without either v ref or d, NR QEs must distinguish the visible distortions from the rest of the signal:</p><formula xml:id="formula_2">v test ¼ ðv ref þ d nonvisible Þþd visible :<label>ð3Þ</label></formula><p>Thus, designers of NR QEs face additional challenges beyond those mentioned above. Using limited input information, NR QE must be able to distinguish signal from visible distortion, when varied processing (e.g., encoding, transmission) introduces different artifacts (e.g., blocking or blurring), into a wide range of source content. Further, they must achieve this despite the fact that many desired signals may ''look'' very similar to typical artifacts.</p><p>In this paper, we delineate three approaches that have been effectively used in the design of NR QEs to address these challenges. First, a NR QE can restrict the domain of the problem based on the desired use of the QE. Instead of striving for perfect accuracy, QEs can be designed for the more realistic performance goal: to achieve the required accuracy for its application over the set of input content and artifacts for which it was designed. Second, using knowledge of the expected processing and the expected signals, sophisticated signal and artifact models can be developed to improve NR QE design. Third, NR QEs gather as many sources of information as possible in addition to v test , including assumptions about the processing and information about the bitstream.</p><p>In addition, it is paramount for NR QE to incorporate as much information about the human visual system (HVS) as possible. A complete model of the human is not possible; for example, feelings or emotions evoked by content are extremely difficult to predict and quantify, and aesthetics are also highly observer-dependent <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. However, substantial modeling of the human observer has been performed by various communities, including psychology, vision, and photography, and this large body of work should inform QE design. While FR QEs have focused on including low-level psychophysics, NR quality estimation provides the opportunity to include work rooted in the photographic community, which is inherently no-reference.</p><p>We begin this paper by describing a three-stage framework for quality estimation in Section 2, which includes measurement, pooling, and mapping to quality.</p><p>Because NR QEs need only be as accurate as the application for which they are designed, we next describe in Section 3 a range of applications of NR QE, including algorithm optimization, benchmarking, and outage detection. Each application has different requirements for the set of input content V and the set of artifacts A over which it must be accurate. Section 4 briefly describes the variety of artifacts that may be introduced by different processing. Next, since models of human vision and perception should be integrated at all three proposed stages of a NR QE, in Section 5 we discuss three levels of human-based models, including subjective quality evaluation for the final crucial stage. However, since most of the attention to date has focused on the first measurement stage, we provide in Section 6 a brief survey of current approaches to measurements for NR quality estimation. We discuss appropriate performance evaluation for QEs in Section 7, and provide concluding thoughts in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A framework for no-reference quality estimation</head><p>A generic no reference quality estimator consists of three steps: measure, pool, and map to quality. In this framework, the input and the corresponding quality estimate can correspond to an entire video, several frames, a single image, or a segment of an image. Input data to the system consists of one or more sources of actual or estimated information, depending on the quality estimation application. The input includes the pixels corresponding to v test ; the bitstream corresponding to v test , including packet headers and data encoding parameters (e.g., quantizer step sizes, picture types); assumptions about the statistics of the original signal v ref or the class of signals V; and assumptions about the distortions and/or artifacts in v test .</p><p>Note that v test itself is not necessary; some techniques perform NR quality estimation without considering the actual pixel data.</p><p>Furthermore, since NR QEs attempt to estimate perceived quality as measured by an appropriate subjective experiment, models of and information about human perception, preferences, and ground-truth quality scores should be incorporated into each of the three components. Each of these issues will be discussed in greater detail in Section 5; here, we simply indicate which issues are appropriate for each component.</p><p>Measuring computes physical quantities (we will refer to them as features) using the input. Selection of specific quantities to compute can be guided by the previously mentioned assumptions as well as the ultimate application of the quality estimator. Both perception models and preference models can be incorporated into the measurement. An example of the former is identifying the presence of known artifacts and estimation of the visibility threshold for those artifacts, while an example of the latter is edge sharpness. Multiple measurements can be computed.</p><p>Pooling combines the possibly linearized measurements over an appropriate subset of space and/or time for the QE. For example, in an individual image, spatially local pooling over frequency and orientation results in a spatial map of responses. Subsequent pooling over space produces a single response estimate. In video, pooling can be performed over combinations of spatial frequency, orientation, space, and time. As described in Section 3, the subset of space and/or time will be defined by the application of the QE.</p><p>The pooling step includes an optional linearization process that consists of a nonlinear mapping for each measurement to rescale or renormalize the values to an appropriate scale (e.g., the same dynamic range or justnoticeable-differences (JNDs)) prior to pooling.</p><p>The pooling mechanism should be motivated by the measurements and known properties of human observers. Interactions among artifacts and between artifacts and the image itself should be accounted for in pooling, using characterizations of masking in human vision (cf. Section 5).</p><p>Linear pooling can be appropriate when the individual measurements satisfy the assumptions required by linear regression. Minkowski summation is more general and is motivated by additivity in low-level vision; its use requires selection of an appropriate exponent which should be based on measurements. Temporal pooling combines multiple frames into a score for each relevant time scale. The pooling may be linear, Minkowski summation, or a maximum-type operator, with the goal of incorporating both temporal masking and temporal summation. Statistical learning techniques can also been applied to combine measurements, but these require appropriate analysis to provide insight into whether all measurements are truly contributing and are in fact behaving as desired.</p><p>The last component, mapping to quality, applies a nonlinearity to map the output of the pooling component into an estimate of perceived quality. If the output of the pooling element is already linear, this stage may not be necessary. The exact form of the nonlinearity should be dictated by a best-fit of the output of the pooling component to ground-truth subjective data. As such, assumptions regarding the ''best'' and ''worst'' expected qualities in a system are implicitly included in the QE through the training data. It is extremely important that the training data be appropriate for the measurements and for the application (cf. Section 7 on evaluation of QEs).</p><p>This final component is required because even when the previous two components are typically sufficiently accurate to provide monotonicity and approximate rankorder preservation, they may still not accurately map to true quality scores (e.g., one end of the scale is often inappropriately compressed or expanded).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Use scenarios for quality estimators</head><p>Before content is finally displayed to a viewer or customer, it may undergo a wide variety of different types of processing, by different algorithms in different subsystems, many introducing distinct artifacts. In addition, it may pass from one owner to another; different companies may be responsible for different stages of the processing or different stages of the delivery. Quality estimation is appropriate whenever content is passed from one owner or entity, one piece of hardware, or one algorithm to another. However, a QE that is useful for one application may not be appropriate for another. In this section, we discuss and differentiate a variety of applications.</p><p>Each application is characterized by several aspects: the set V of sources (i.e., undistorted images or videos), the set A of artifacts for which the QE must be accurate, the degree of accuracy required, and (for video) the time scale at which quality values are required.</p><p>Algorithm optimization in processing employs QEs in a closed-loop during compression or other processing algorithms, to maximize quality of the output. Such QEs can be either FR (e.g., in compression, in which the original is obviously available) or NR (e.g., for enhancement at the decoder, or for transcoding applications in the network).</p><p>In such ''in the loop'' applications, the set A is limited to known artifacts resulting from the processing algorithm. The QE must correctly reflect increases or decreases in these known artifacts for a single source at a time, and hence the set V contains only one undistorted source. If the QE incorrectly characterizes quality increases or decreases, then incorrect decisions may be made during optimization that will result in worse visual quality than without optimization.</p><p>A QE for algorithm optimization may or may not contain a real-time requirement, depending on the application. Real-time video encoding clearly imposes both causality as well as computational restrictions on a QE, while image or off-line video processing does not.</p><p>Product benchmarking allows purchasers and marketers to compare the performance among different products or components. Product marketers use benchmarking with the goal of demonstrating superiority of their product over others. Benchmarking is typically applied to an individual algorithm (e.g., encoding) or a small bundle of algorithms (e.g., decoding and error concealment), although it could plausibly also be applied to a business owner's subsystem (to compare, for example, a cable TV offering to a DSL TV offering). A desirable benchmarking statement compares two products p 1 and p 2 , producing processed content p 1 (v) and p 2 (v), respectively, and may take either the form</p><formula xml:id="formula_3">Q subj ðp 1 ðvÞÞ 4T for a% of v 2 V,<label>ð4Þ</label></formula><formula xml:id="formula_4">or Q subj ðp 1 ðvÞÞ 4Q subj ðp 2 ðvÞÞ þ d for b% of v 2 V,<label>ð5Þ</label></formula><p>where T is a quality threshold. If p 1 is the marketer's product and p 2 belongs to a competitor, a marketer may carefully select the set of sources V for which a, b, and d are as large as possible. Selection could be content-based (e.g., sports and action) or even specific source-based (e.g., Monsters Inc. and Raiders of the Lost Ark). For statements of the form of ( <ref type="formula" target="#formula_3">4</ref>) and ( <ref type="formula" target="#formula_4">5</ref>), only one QE score per input is required regardless of sequence duration. An appropriate QE will be accurate for heterogeneous artifacts so that it can compare systems, for example, both with and without deblocking.</p><p>System provisioning occurs prior to deployment, and involves the design of an end-to-end system to achieve a target quality. A typical problem statement seeks a set of system parameters ffg (e.g., bit-rate, maximum packet loss rate, server capacity, or spatial resolution or temporal resolution) for a system s, according to Determineffg for which</p><formula xml:id="formula_5">Q subj ðsðvÞÞ 4T for a% of v 2 V,<label>ð6Þ</label></formula><p>where for video, Q subj ðÁÞ operates on multiple time scales. Here, V is representative of all content that will be handled by the system. To maximize system robustness, both the minimum quality over short time intervals and the average quality over longer time intervals are of interest.</p><p>While ( <ref type="formula" target="#formula_5">6</ref>) looks fairly similar to (4), several important aspects distinguish the applications. First, a QE for system provisioning needs only be accurate near the system design point threshold T. Second, a QE for system provisioning must be accurate across processing that results in different spatial (and for video) and/or temporal resolutions such that when resources are constrained, bandwidth-reducing decisions are made that retain the best perceived quality. Third, for system provisioning, the set V is large and must be inclusive of all possible types of content likely in the system, while product marketing focuses on choosing a subset of sources tailored to the product's strengths.</p><p>Content acquisition and delivery are important for their use in service level agreements (SLAs), which are contracts typically between business entities. For example, for video delivery to the home, it is common to have either implicit or explicit contracts between consumers, service providers, content providers, and network providers. SLAs and other legal contracts constrain the quality of both the incoming and outgoing material.</p><p>A QE for content acquisition and content delivery determines if either the incoming material (whether from a camera, e.g., <ref type="bibr" target="#b8">[9]</ref> or at the input of a large-scale content delivery network) or the outgoing material (i.e., to another network provider or to the viewer's end-system) has sufficient quality. For acquisition and delivery of images, a QE must Alarm when Q subj ðvÞ oT for more than a% of images:</p><p>Video acquisition and delivery require ongoing monitoring, as does outage detection, which considers substantially larger degradations including the loss of video entirely (termed blackout). For these applications, a video QE must Alarm when Q subj ðvÞ oT more than N times in t seconds: ð8Þ QEs for outage detection and content acquisition must be accurate for any content and any type of artifact, including those that represent acquisition failures. For example, the same end-user perception of outage occurs either when a video server fails or when, despite correct coding and transmission, video is received that was captured without removing the lens cap. QEs for these applications must minimize the instances of false alarm and of missed detection.</p><p>Transient quality failures can be seen neither by periodic assessment (i.e., checking once an hour) nor by time-averaged assessment (i.e., quality averaged over an hour). As such, the time scale at which video QE is performed for these applications includes seconds, minutes, hours, and days. Accurate operation over such a large range of time scales is challenging-while accuracy over second or minutes can be evaluated easily in a development environment, evaluating accuracy over hours and days may require more sophisticated approaches.</p><p>Troubleshooting occurs after outage detection, to pinpoint the cause of the problem, so that it can be fixed. The objective, identifying why Q subj ðvÞ o T, can be addressed using a set of artifact detectors. Troubleshooting requires artifact detectors that operate independently; a ringing artifact should not influence the output of a noise detector (e.g., <ref type="bibr" target="#b9">[10]</ref>).</p><p>Summary: QEs have a wide range of applications in both processing and transmission, differing in set V of sources, the set A of artifacts, the degree of accuracy required, and the time scale of operation. Designing a QE for a particular application should consider these requirements. Next, we present specific artifacts and describe how they are introduced in the processing chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Artifacts in the processing chain</head><p>The processing chain encompasses acquisition, compression, transmission or storage, decoding, and display, and artifacts can be introduced at various stages. At any point in the chain, the content can be repurposed, which can entail re-acquisition, re-compression, or additional transmission. In addition, at any point an enhancement algorithm can be applied.</p><p>Acquisition and display are inherently without reference. Compression of originals has a reference; transcoding does not. Transmission or storage can result in lost or errant packets or bits, which induce decoding errors later in the chain; quality estimation at the decoder is also most commonly without reference.</p><p>The interested reader is referred to the surveys of artifacts in <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> for additional information and visual examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image and video acquisition and display</head><p>The two ends of the processing chain are inherently without reference. An important aspect of both of these operations is the treatment of color data, both its appropriate interpretation during acquisition and its subsequent appropriate rendering during display. As such, many aspects of color image workflow are no-reference quality estimation problems, and these are approached with human-centric goals-to represent colors as they would have been perceived by a human observer, and to then display colors in a perceptually pleasing manner.</p><p>During acquisition, artifacts may be introduced due to the optical lens, the density and accuracy of the sensing elements, or the digitization process. Blurring can be introduced by defocus (focal blur) or due to camera or object motion with too slow a shutter (motion blur). Noise can be introduced in the sensing elements. Insufficient dynamic range in A/D conversion can lead to contouring. Insufficiently dense sampling results in aliasing, which has a variety of artifacts including jagginess, geometric distortions, and inhomogeneity of contrast <ref type="bibr" target="#b13">[14]</ref> as well as color artifacts. Failures in camera-based algorithms including white point selection and balancing and exposure adjustment, among others, can result in images which inaccurately represent colors and scene brightness as seen by a human observer.</p><p>Artifacts caused by the display are difficult to measure but can be estimated if appropriate display parameters are known. Such artifacts include LCD motion blur, overscan, and potential interlacing artifacts when interlaced video is displayed on progressive monitors. Inaccurate display characterization can cause problems in both tone mapping and gamut mapping, in which brightness and colors of the content are mapped to those of the display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Encoding and decoding</head><p>Compression with block-based coders (JPEG, MPEG-2, H.261, H.263, H.264) can introduce a number of artifacts. Blocking appears at deterministic locations and is caused by heavy quantization of the transform coefficients. Mosquito noise is temporal shimmering caused by timevarying blockiness. False edges, also called motioncompensated edge artifacts (MCEA), are the result of blocking artifacts that move away from the block boundaries due to motion compensation process <ref type="bibr" target="#b14">[15]</ref>. Flatness <ref type="bibr" target="#b15">[16]</ref> is a lack of resolution in fine detail.</p><p>Wavelet-based coders (including JPEG-2000) introduce a different set of artifacts, including blurring, ringing, and aliasing. Both wavelet and block-based coders may skip frames during compression, making video appear jerky.</p><p>Perfectly received data undergoes no decoder-induced artifacts. If bits or packets are damaged or lost in transmission or storage, artifacts introduced at the decoder are very dependent on the encoding strategy, the decoder design, and error concealment strategies. For JPEG and JPEG-2000 images, decoding artifacts can include DC shift caused by DPCM decoding errors, horizontal or vertical shifts of image data within an image, and loss of detail.</p><p>For motion-compensated video, artifacts can include motion jerkiness resulting from dropped frames, individual frames exhibiting concealment distortions, concealment distortions which propagate over time, ''missing'' blocks displayed as solid colors, and propagation of such missing blocks over time. Hardware faults may also occur in video decoders, introducing a range of artifacts considered in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Repurposing and enhancement</head><p>The most frequent type of repurposing is displaying at low resolution (for example, on a mobile device) content that was acquired at higher resolution. Scalable coding implicitly establishes a framework for repurposing; selective discard of scalably coded bitstreams during transmission is simply repurposing.</p><p>Processing for repurposing includes spatial resampling, temporal resampling and frame-rate conversion, recompression or transcoding. Thus, repurposing may introduce some artifacts already discussed. Additional examples include interlace artifacts in video frames treated as still images and a variety of artifacts that occur when converting video from high-definition (HD) to standard-definition (SD) or vice versa (incorrect aspect ratios, frame truncation, or color artifacts).</p><p>Digital video that has been reacquired from analog video or film can exhibit some unique artifacts. Demodulated analog NTSC or PAL video may have ''rainbow'' effects <ref type="bibr" target="#b18">[19]</ref> where color artifacts appear in regions of high luminance spatial frequency, or luminance artifacts appear where colors are saturated. Analog multipath transmission can result in ghosting. Film degradation introduces a wide variety of artifacts <ref type="bibr" target="#b19">[20]</ref> including flicker, a fluctuation of picture brightness.</p><p>Operator or equipment error during repurposing can cause additional artifacts after decoding, such as the two fields of a frame being presented in the wrong order. Visually, the entire frame appears to have interlacing artifacts.</p><p>Enhancement may also introduce artifacts that were not present previously. Sharpening can cause ringing; deblocking and denoising can cause blurriness; deinterlacing can cause ghosting or motion artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Modeling humans in quality estimator design</head><p>Because NR quality estimation attempts to estimate perceived quality by a human observer, models of and information about human perception, preferences, and ground-truth quality scores should all be incorporated into a NR QE. This data includes low-level psychophysical models, which can be used to estimate d visible ; measured sensitivities to particular artifacts, which can also be used to estimate d visible ; known preferences for ''perceptually pleasing content,'' including that on colorfulness, sharpness, degrees of blurring, addition of noise; and ground-truth subjective quality scores associated with a database of training content. This section first reviews fundamentals of low-level vision as they are applicable to quality estimation. Here, psychophysical experiments measure responses of the human visual system to simple stimuli such as sinusoids or spatially correlated bandlimited noise. The results provide a characterization of vision which can be applied in a ''bottom-up'' manner to complex stimuli such as processed images and video. Low-level vision is most commonly used in FR QEs, but is also clearly relevant to NR QEs.</p><p>Several alternative ''top-down'' approaches are next reviewed. Responses are measured to stimuli that consist of natural images or video processed to include one or more synthetic or actual artifacts. We include the study of preferences in this approach, which is rooted in the photographic community and is inherently no-reference.</p><p>This section concludes with a brief discussion on ground-truth data which is used for both training and validation of any QE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Low-level vision</head><p>Low-level vision is generally thought to perform a multichannel decomposition, where bandlimited channels process spatial frequency, temporal frequency, and color. With respect to quality estimation, not only are the responses of each channel relevant, but so are intra-and inter-channel interactions. The former permit an estimation of the HVS's response to bandlimited, simple stimuli, while the latter (more commonly known as masking) permit an estimation of its response to compound stimuli such as images and video. Masking is a general term that refers to the perceptual phenomenon in which the presence of masking signal (the masker) reduces a subject's ability to detect a given signal (the target). With respect to quality estimation, an estimate of masking is essential to separate distortions into visible distortions and those distortions that are masked (i.e., hidden) by the source.</p><p>While a thorough review of low-level vision is beyond the scope of this paper, this section provides a brief overview. Readers are encouraged to further explore not only the references in this section, but also several FR video quality estimators which provide different design decisions in implementing a HVS model <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Contrast</head><p>While digital pixels are stored as bits, luminance (measured in candelas/meter 2 ) represents the light entering the eye, and contrast contributes to the perceived luminance. Because perceived quality is experienced by a viewer, any quality estimator must include the display device in computing how the stored data is displayed to the viewer.</p><p>Contrast is qualitatively defined as luminance change divided by mean background luminance and can be computed globally or locally on a natural image. Many definitions of contrast exist (e.g., the Weber fraction, Michelson <ref type="bibr" target="#b24">[25]</ref>, bandlimited contrast <ref type="bibr" target="#b25">[26]</ref>, local bandlimited contrast <ref type="bibr" target="#b26">[27]</ref>, RMS <ref type="bibr" target="#b27">[28]</ref>; see <ref type="bibr" target="#b26">[27]</ref> for a review), leaving flexibility to select the appropriate definition based on the needs of a particular application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Spatial vision</head><p>The human contrast sensitivity function (CSF) is a wellaccepted description of spatial frequency perception; the HVS has band-pass characteristics. The multi-channel model postulates that the CSF represents the aggregate response of frequency-and orientation-tuned individual channels having increasing bandwidth with increasing frequency <ref type="bibr" target="#b28">[29]</ref>.</p><p>Current explanations of spatial masking can be divided into four paradigms: (1) Noise masking <ref type="bibr" target="#b29">[30]</ref>; (2) contrast masking <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>; (3) entropy masking <ref type="bibr" target="#b34">[35]</ref>; and (4) structural masking <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Temporal vision</head><p>While spatiotemporal <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> or spatiovelocity <ref type="bibr" target="#b38">[39]</ref> responses have been measured, HVS-based QEs typically apply a separate temporal frequency response. The HVS can be modeled as having both transient (i.e., bandpass) and sustained (i.e., lowpass) temporal response mechanisms <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>. For examples on how spatial and temporal frequency responses can be combined in QEs, the reader is directed to four examples <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>.</p><p>Temporal summation and temporal masking have also been measured and modeled <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">Color vision</head><p>The eye contains three cone types with different spectral sensitivities, colloquially known as red, green, and blue. Opponent color theory <ref type="bibr" target="#b44">[45]</ref> suggests and psychological experiments have demonstrated that the visual system has three color channels which are roughly independently processed at a low level (e.g., <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>). These channels represent achromatic vision and two chroma channels: red-green, and blue-yellow.</p><p>While CSFs have been measured for the red-green and blue-yellow channels <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>, color channel sensitivity has been substantially less studied than luma channel sensitivity. The chrominance CSFs differ from that of luminance in that they are low-pass rather than bandpass, and they fall off sooner than the luminance CSF. Temporal responses for red-green and blue-yellow have also been measured <ref type="bibr" target="#b49">[50]</ref>.</p><p>Interactions between color and luma channels are also not well understood, but color provides substantially more masking of luminance than the reverse (see <ref type="bibr" target="#b50">[51,</ref><ref type="bibr">Chapter 7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5.">Pooling</head><p>Estimates of responses in the spatial, temporal, and possibly color channels must be combined, or pooled, to provide an aggregate response estimate. A Minkowski sum is most commonly used in modeling low-level vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Top-down perception</head><p>Due to the challenges associated with applying a lowlevel characterization of vision to complex stimuli, especially when no reference is available, other approaches have been explored in which the images and video themselves are used as the stimuli. One benefit of this approach is avoidance of explicit masking (and sometimes pooling) models; unfortunately, the knowledge gained is limited to the specific preference or artifact under test and can also be limited to the specific source content used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Preferences</head><p>Preferences refer to characteristics of an image or video that can be computed by some measure and quantified as statistical functions of large groups of observers. They have a historical basis in photography, and are therefore inherently no-reference. Much work in color representation and reproduction is based on human preferences, including color scaling and color naturalness <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>; edge sharpness <ref type="bibr" target="#b53">[54]</ref>, color saturation, flesh tone preferences, and use of dynamic range <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Quantifying responses to specific artifacts and impairments</head><p>A second ''top-down'' approach directly measures observer responses to the artifacts likely to be encountered in a system or application, including all artifacts mentioned in Section 4. Single artifacts may be generated synthetically <ref type="bibr" target="#b55">[56]</ref> in an attempt to understand their impact in isolation. However, measuring human responses to individual artifacts does not provide information on cross-impairment masking or on how observers perceive two or more simultaneously presented artifacts.</p><p>While some experiments have inserted multiple synthetic artifacts and quantified the simultaneous response <ref type="bibr" target="#b56">[57]</ref>, it is more common for experiments to employ stimuli produced by systems, e.g., compressed MPEG video having undergone packet losses. Many processing techniques create impairments that are strongly correlated with the source content v ref . Often, such experiments are also designed to measure the impact of the content on the response, and hence are essentially quantifying masking. In contrast to masking experiments for low-level vision, however, these results are less generalizable.</p><p>As examples, responses have been measured to freeze frames <ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref>; synthetic blockiness, blurriness, noisiness and ringing <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>, MPEG-2 compression impairments localized in both space and time <ref type="bibr" target="#b60">[61]</ref>, packet loss <ref type="bibr" target="#b61">[62]</ref> and its visibility <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Another approach to understand human perception for a type of processing (e.g., image scaling or JPEG compression) that creates multiple artifacts with complicated interrelationships is to use naive viewers to label images in terms of their perceived quality and to have expert viewers label artifact strength <ref type="bibr" target="#b13">[14]</ref>. A regression analysis then determines the impact of the latter on the former. These results are also difficult to generalize beyond the particular experimental setup.</p><p>Lastly, some experiments vary parameter settings for the processor, and evaluate the subjective response. Many studies take this approach, including <ref type="bibr" target="#b64">[65]</ref> for compression at different bit-rates and <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref> for packet loss impact at different packet loss rates. It is difficult to generalize the results of these subjective tests to other processing, to other parameter settings, and most importantly, to different sources in V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Multidimensional scaling (MDS)</head><p>Multidimensional Scaling <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref> is a statistical technique for quantifying responses to multiple preferences and/or artifacts. With an input matrix of distances between stimuli (e.g., resulting from a perceptual experiment), it attempts to find a coordinate system in N-dimensional space which preserves the distances between the stimuli (N is userdefined). However, the dimensions themselves may not be perceptually meaningful. Examples include <ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Subjective estimation of quality-ground truth data</head><p>An ideal quality estimator predicts quality estimates as measured by an appropriate experiment with human subjects. As such, a quality estimator must be designed and evaluated using ground-truth subjective data gathered from observers. The subjective experiment is also critical for defining the scope of a estimator, and in particular for understanding both appropriate and potentially inappropriate uses of an estimator.</p><p>The performance of an estimator is limited by the nature of the data to which it has been tuned. While many estimators provide rank-ordered assessments that match those of human observers on images that contain differing amounts of a single artifact (e.g., JPEG compression) they are not as successful at rank-ordering degraded images from the same original that have different artifacts (e.g., comparing JPEG distortions with white noise). One reason for this weakness is a lack of actual observer scores for such comparisons and hence a lack of accurate training data. Use of a protocol such as SAMVIQ <ref type="bibr" target="#b73">[74]</ref>, in which observers simultaneously view and score all distorted versions of a source, avoids this problem. Comparisons between different distortions can also be made using the quality ruler protocol <ref type="bibr" target="#b74">[75]</ref>.</p><p>A full discussion of the design of subjective tests for gathering ground-truth subjective data for quality estimation is beyond the scope of this paper. The reader is referred to the VQEG committee documents which provide excellent ''case study'' discussions of subjective test design for three VQEG test phases <ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref>, as well as various international standards (e.g., <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref>) and other references <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref>.</p><p>We list below various issues for test design which should be carefully considered prior to beginning testing: selection of a testing protocol, including use of category (''excellent,'' ''good,'' etc.) or continuous (non-quantized) ratings, and whether evaluations are done with respect to a reference image/video (e.g., double stimulus protocols) or singly; collection of data at relevant (and potentially multiple) time scales for video; the required number of observers, and a methodology for determining the validity of particular observers; choice of subject matter, including use of gray-scale or color images; environmental viewing conditions, including background and room lighting, display calibration, and viewing distance; observer instructions and clarity of task wording; and human subjects approval of the protocol by an appropriate body at the researcher's institution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Overview of existing measurements for NR quality estimators</head><p>A large body of work addresses various aspects of NR QE design. Much of this work, however, does not in fact estimate quality. Rather, it stops at the measurement step, having computed a single feature, without any inclusion of human observer data. Other works compute a single feature and then map to quality, thus limiting the pooling stage to spatial or temporal averaging. Nevertheless, when considering this body of work, substantial progress has been made toward developing solutions to the first step of the framework for NR QE described in Section 2. Therefore, in this section we briefly review some of the literature studying this measurement step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Direct estimation of mean-squared error</head><p>We begin with a class of NR QEs whose measurement stage attempts to separate v test into v ref and distortions, d, statistically. While many of these types of NR QEs to date only consider this first stage, we also describe two examples where HVS models or subjective data are incorporated into subsequent processing.</p><p>The first methods in this class of NR QE were designed to predict the Mean-Squared-Error (MSE) caused by block-based compression like MPEG-2 <ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref>, JPEG <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b87">88]</ref>, or H.264 <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b91">92]</ref>. With the exception of <ref type="bibr" target="#b83">[84]</ref>, which uses the decoded pixels v test , these techniques use information only from the received bitstream. The basic approach is to model the DCT coefficients using a Laplacian distribution, and estimate the Laplacian parameter for each of the 8 Â 8 coefficients. However, this has been extended to generalized Gaussian <ref type="bibr" target="#b89">[90]</ref> and Cauchy distributions <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b91">92]</ref> as well.</p><p>These techniques have the common drawback that they only obtain an MSE estimate for each 8 Â 8 block; they are unable to predict neighboring pixel differences and hence be extended to estimate artifacts like blockiness. In addition, the accuracy of the estimated MSE for each of these methods is lower when the bit-rate is smaller, due to the presence of more coefficients quantized to zero. To improve the accuracy for low bit-rates, <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b87">88]</ref> rely on training data to obtain improved estimates of the coefficient distributions.</p><p>There are also several attempts to design NR QE to predict the Mean-Squared-Error (MSE) caused by packet loss errors. Bitstream-only approaches are designed in <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b93">94]</ref> for motion-compensated video compression with packet loss. Content-specific information is extracted from the parsed bitstream, to estimate local means, variances, and correlations. Together with extracted motion vectors, these are combined using a Gauss-Markov model to estimate initial MSE. Motion-compensated error propagation is incorporated into the overall estimate of MSE. In <ref type="bibr" target="#b94">[95]</ref>, both v test and information extracted from its bitstream are combined to estimate MSE due to packet loss for H.264. The initial error is estimated by separately considering the impact of missing motion vectors and missing prediction errors. Finally, <ref type="bibr" target="#b95">[96]</ref> estimates the MSE due to packet losses in motion-JPEG2000.</p><p>Noise estimation approaches estimate MSE using two basic methods <ref type="bibr" target="#b96">[97]</ref>. The first is to smooth v test and define any difference between v test and its smoothed version to be noise <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b97">98]</ref>. The second is to identify smooth areas in v test and assume that any variation within those smooth areas is noise <ref type="bibr" target="#b98">[99]</ref><ref type="bibr" target="#b99">[100]</ref><ref type="bibr" target="#b100">[101]</ref><ref type="bibr" target="#b101">[102]</ref>.</p><p>The strategy of estimating MSE is motivated by a desire to statistically estimate the distortion d in Eq. ( <ref type="formula" target="#formula_0">1</ref>). Unfortunately, most contributions in this area are limited in that they only estimate MSE; they do not further partition the estimated MSE into d visible and d nonvisible . However, Brandão and Queluz <ref type="bibr" target="#b87">[88]</ref> also incorporate their estimated error into a HVS-based NR QE relying on Watson's DCTbased perceptual model <ref type="bibr" target="#b102">[103]</ref>. Whereas <ref type="bibr" target="#b102">[103]</ref> uses the actual quantization error computed in a FR framework, <ref type="bibr" target="#b87">[88]</ref> uses the NR estimated quantization error. In addition, the results of <ref type="bibr" target="#b92">[93]</ref> were later incorporated by Kanumuri et al. <ref type="bibr" target="#b62">[63]</ref> into a NR estimator of the visibility of packet losses, whose pooling step uses subjective data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Feature-based approaches</head><p>Two strategies have been used to design features to be extracted in the measurement step. Both strategies assume that the statistics of v test differ from those of v ref and use features extracted from v test to evaluate model compliance. The first strategy is to develop a model for the specific artifacts that may contribute to d visible , focusing on those artifacts introduced by the processing chain. As such, NR measurements designed using this strategy may generalize to different classes of content V, but they are unlikely to be able to characterize quality degradations caused by different artifacts. This approach will fail if V contains v ref that mimic the artifacts (for example, periodic structure of vertical edges near block boundaries).</p><p>The second strategy is to model specific signal attributes that characterize v ref . The goal is then to find violations of the signal model. This strategy focuses on a specific class of V (for example, scenes without man-made structures, or scenes with consistent lighting), and is likely to be effective for a variety of artifact types. This approach will fail if the added distortions do not cause v test to violate the signal model.</p><p>Artifact and signal models can be developed in either the spatial and the transform domain, where the latter includes DCT, wavelet, and polynomial transforms. Complementary features extracted from each domain can be combined to improve overall QE accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">Spatial artifacts due to compression</head><p>Many blockiness features have been surveyed in <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b14">15]</ref>. Among the NR QE that consider some perceptual masking, <ref type="bibr" target="#b104">[105]</ref><ref type="bibr" target="#b105">[106]</ref><ref type="bibr" target="#b106">[107]</ref> compute a local gradient, <ref type="bibr" target="#b107">[108]</ref> estimates the power of an ideal blocking signal using the FFT, <ref type="bibr" target="#b108">[109,</ref><ref type="bibr" target="#b109">110]</ref> explore features of the DCT, and <ref type="bibr" target="#b110">[111]</ref> apply the first three coefficients of a polynomial transform. Most blockiness detectors assume the grid-location is known; <ref type="bibr" target="#b111">[112]</ref> detects the grid in case the image has been resized or cropped, and <ref type="bibr" target="#b14">[15]</ref> considers motioncompensated edge artifacts, which result when block edges are motion-compensated away from block boundaries.</p><p>A detailed overview of 13 QEs that measure blurriness (or sharpness) of images is presented in <ref type="bibr" target="#b112">[113]</ref>, which introduces the notion of just noticeable blur. Many approaches measure physical attributes of the edge profile, including acutance <ref type="bibr" target="#b113">[114]</ref>, horizontal and vertical edge extent <ref type="bibr" target="#b114">[115,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b115">116]</ref>, and diagonal edge extent <ref type="bibr" target="#b116">[117]</ref>. Using polynomial transforms, <ref type="bibr" target="#b117">[118]</ref> designs a multiscale blur estimation algorithm to estimate the spread of a Gaussian blurring kernel. A similar goal was addressed in <ref type="bibr" target="#b118">[119]</ref> using spatial gradients for both lines and edges. Blur has also been measured using features in the DCT domain <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b120">121]</ref> and using the phase of the Fourier transform <ref type="bibr" target="#b121">[122]</ref>. A combined spatial and frequency domain approach is presented in <ref type="bibr" target="#b122">[123]</ref>, where features about the edge profile are combined with the local frequency spectrum around image edges.</p><p>With the exception of <ref type="bibr" target="#b97">[98]</ref>, which operates in the frequency domain, ringing features are typically extracted from the pixel domain. Oguz et al. <ref type="bibr" target="#b123">[124]</ref> compute the variance of pixels in regions near strong edges to characterize ringing. Visible ringing regions are detected in smooth regions near edges in <ref type="bibr" target="#b124">[125]</ref>, which also incorporates luminance masking. To increase the accuracy of edge localization, <ref type="bibr" target="#b125">[126,</ref><ref type="bibr" target="#b126">127]</ref> apply a bilateral filter, before computing a ringing annoyance score which is a nonlinear function of the local variance of ringing artifacts.</p><p>Blockiness, blurriness, and ringing features have also been combined with other features, including bitstream features <ref type="bibr" target="#b127">[128,</ref><ref type="bibr" target="#b128">129]</ref> and edge gradient and luminance masking features <ref type="bibr" target="#b129">[130]</ref>.</p><p>Sheikh et al. <ref type="bibr" target="#b130">[131]</ref> characterize artifacts for JPEG-2000 by exploring deviations from a signal model. Using a recent model of wavelet coefficients for ''natural scenes'' <ref type="bibr" target="#b131">[132]</ref>, they calibrate deviations of this model in the presence of JPEG-2000 compression against human quality ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">Features for other spatial artifacts</head><p>A variety of other spatial artifacts have been considered. Chang et al. <ref type="bibr" target="#b18">[19]</ref> design a detector for rainbow artifacts using features that extract information about both high-frequency luminance and chrominance components. A method to identify aliasing energy in v test that contributes to visible jagginess d visible has recently been presented <ref type="bibr" target="#b132">[133]</ref> for integer downsampling. This method is only capable of identifying aliasing energy near strong directional edges that is not masked by the edge. Color features have been defined by <ref type="bibr" target="#b133">[134,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b134">135]</ref>, including others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3.">Temporal features</head><p>The simplest approach to quality estimation for video is to average the estimated quality of individual video frames. However, this approach ignores many known properties of temporal vision, as discussed in Section 5.1.3. More accurate QEs consider both additional temporal features and nonlinear temporal pooling.</p><p>The temporal consistency (or its opposite, temporal variability) of luminance levels measures the impact of flicker and blackout artifacts. While flicker-removal algorithms <ref type="bibr" target="#b135">[136,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b19">20]</ref> use sophisticated models that include motion, their performance is often evaluated using only intensity mean and variance. A no-reference flicker-score was proposed in <ref type="bibr" target="#b137">[138]</ref> with the goal of reducing flicker in H.264 encoded videos.</p><p>Frame freezes can be identified using several methods that differ based on the type of input information available. If the bitstream is available, frame freezes can be detected using picture time-stamps of received frames, and if only the pixels v test are available, frame freezes may be detected using inter-frame correlation <ref type="bibr" target="#b138">[139]</ref>. The duration and regularity of the frame freezes, as well as the intensity of the fluidity break, are incorporated in NR QE for frame freezes <ref type="bibr" target="#b138">[139]</ref><ref type="bibr" target="#b139">[140]</ref><ref type="bibr" target="#b140">[141]</ref><ref type="bibr" target="#b141">[142]</ref>. A dropping severity indicator, a scene boundary detector, and a motion activity estimator are combined in <ref type="bibr" target="#b141">[142]</ref> to design a NR QE that accounts for the perceptual impact of local quality fluctuations.</p><p>Strategies for NR QE given packet loss depend heavily on the type of available inputs. Those NR QE that rely solely on bitstream parameters <ref type="bibr" target="#b142">[143,</ref><ref type="bibr" target="#b143">144,</ref><ref type="bibr" target="#b2">3]</ref> are most widely applicable, but they rely on the strong assumption that all packet losses have equivalent perceptual impact. Packet loss rate (PLR) <ref type="bibr" target="#b142">[143,</ref><ref type="bibr" target="#b143">144]</ref>, quantizer step size <ref type="bibr" target="#b142">[143]</ref> and bit-rate and frame-rate <ref type="bibr" target="#b143">[144]</ref> have all been incorporated.</p><p>Those NR QE that can parse the video bitstream using a variable-length decoder (but do not use v test ) can obtain precise information about the location, spatial extent, and temporal extent of the packet loss artifacts <ref type="bibr" target="#b92">[93]</ref>, but must estimate the strength of the resulting error. A NR model of visibility of packet losses <ref type="bibr" target="#b62">[63]</ref> also considers motion predictability, spatial motion smoothness, and the estimated MSE due to packet loss <ref type="bibr" target="#b92">[93]</ref>. These methods must rely on assumptions about the error concealment strategy of the decoder.</p><p>Finally, those NR QE that rely solely on the video pixels v test no longer rely on assumptions about error concealment, but face the challenge of estimating the location and extent of the artifact. If a loss affects an entire frame, it can be detected as described above for frame freezes. Approaches that search for edges along macroblock boundaries <ref type="bibr" target="#b144">[145,</ref><ref type="bibr" target="#b145">146]</ref> assume the error does not affect the entire frame and may fail to identify artifacts caused by error propagation. The additional features of vertical gradients and edges in horizontal gradients are extracted in <ref type="bibr" target="#b146">[147]</ref> to detect artifacts of spatial error concealment.</p><p>Hybrid methods that use both the pixels v test and its bitstream avoid many of the challenges of using only one input, although they require both inputs to be available and additional processing <ref type="bibr" target="#b2">[3]</ref>. The effectiveness of error concealment is evaluated in <ref type="bibr" target="#b147">[148]</ref> which identifies corrupted macroblocks using bitstream information and combines bitstream-based motion information with and pixel-based vertical and horizontal luminance discontinuities. Pixel-based features are extracted from bitstream segments that have incorrect checksums in <ref type="bibr" target="#b148">[149]</ref> to detect bit-error artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Evaluation</head><p>Thorough evaluation of a QE can provide insight into potential improvements, identification of specific failure cases, and eventually more robust performance. In contrast, inadequate evaluation can lead to false performance claims and inevitable QE failure. Any QE should be provided with a complete discussion of evaluation, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Assumption and operation verification</head><p>Assumptions made at each step of the QE design should be validated or refuted. If a measurement evaluates violations of a signal model, it should be tested on a wide variety of undegraded inputs v ref . If a measurement evaluates conformance with an artifact model, it should be tested to verify not only that it does not unwittingly measure other artifacts <ref type="bibr" target="#b9">[10]</ref>, and also that it does not detect artifacts inside undegraded inputs. Monotonicity of the individual measurements should be verified. For example, blockiness should increase as quantization increases. The pooling step should be tested to verify correct operation across multiple artifacts. NR QEs which rely on restricted inputs (for example, only v test or only bitstream parameters) should be tested to understand how the input constraints limit performance.</p><p>Operation should be evaluated using synthetic inputs (or so-called ''toy examples''). Such inputs allow a QE to be stressed in carefully controlled directions. Artifacts can be added, amplified, or spatially and temporally distributed, for example. Inputs which either violate or exactly match the statistical signal models assumed in design can identify whether the QE adequately estimates desired statistical quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Classical numerical measures</head><p>Evaluating the performance of QEs nearly always begins with a quantification of the differences between Q subj ðVÞ and Q obj ðVÞ. Obviously, the ground truth data used to tune and/or train the QE should not be part of the test set. Pearson linear correlation, outlier ratio, and RMS error quantify performance based on how well the QE predicts individual subjective quality scores on an absolute scale. Spearman rank-order correlation quantifies how well the QE maintains the relative ranking of scores. These four parameters are the most commonly used quantities (see <ref type="bibr" target="#b76">[77]</ref> for a discussion of these quantities, along with several others).</p><p>While computation of these quantities over the entire test set provides performance information, evaluation should not stop with these simple computations. Computation of these quantities over meaningful subsets of the test set should also be performed. Such subsets can include classification based upon presence or absence of specific artifacts, sources with more or less observer variability in scores, observers, or any other subset which is reasonable for the particular QE.</p><p>Specific examination of outliers is important, as it can provide identification of particular failures within a QE or evidence that a QE will fail on some percentage of unforeseeable cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Resolving power and classification errors</head><p>A QE's accuracy regarding differences between pairs of scores in a test set Q subj ðv 1 Þ and Q subj ðv 2 Þ can be further quantified using resolving power and classification errors. Brill et al. define the resolving power of a QE <ref type="bibr" target="#b149">[150]</ref>, which computes a confidence in the difference between QE scores DQ obj ¼ Q obj ðv 1 ÞÀQ obj ðv 2 Þ and facilitates an understanding of whether a difference of a given size is meaningful. Resolving power is dependent on the subjective data; a QE can have different resolving powers for different data sets.</p><p>Classification errors occur when differences in subjective scores DQ subj ¼ Q subj ðv 1 ÞÀQ subj ðv 2 Þ and QE outputs DQ obj for two different sources disagree, in one of three different ways <ref type="bibr" target="#b149">[150,</ref><ref type="bibr" target="#b150">151]</ref>:</p><p>false ties occur when jDQ subj j 4 g but jDQ obj j og; false differences occur when jDQ subj j og but jDQ obj j 4g; false ranking occurs when</p><formula xml:id="formula_7">Q subj ðv 1 Þ 4 Q subj ðv 2 Þ but Q obj ðv 1 Þ o Q obj ðv 2 Þ.</formula><p>The threshold g may depend on the application (for example, in the stopping criterion for algorithm optimization), but it can be related to the minimum desired quality difference, which is often the JND. (Here, we have assumed that Q obj ðÁÞ and Q subj ðÁÞ have been normalized to exist on the same scale.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Application-specific evaluation</head><p>Lastly, the QE should be evaluated in the application for which it was designed. If a QE is designed for algorithm optimization, it should be placed in the loop of an actual algorithm to verify that the algorithm's outputs demonstrate superior quality to those generated without the QE in the loop. Such verification requires a subjective experiment. If a QE is designed for troubleshooting, it should be tested with a multi-component system in which various components are forced to fail. QE designed for other applications should be tested by verifying that Eqs. ( <ref type="formula" target="#formula_3">4</ref>)-( <ref type="formula">8</ref>) are correct when Q obj ðÁÞ is substituted for Q subj ðÁÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Concluding thoughts</head><p>We have introduced the reader to a variety of applications and a broad range of artifacts. We have described a three-stage framework for NR quality estimation that provides not only the opportunity for including a target application appropriately, but also substantial opportunity for incorporating characteristics of humans as viewers at multiple levels. We have provided a generous survey of approaches, primarily to the measurement stage of the framework, and have also enumerated a variety of performance metrics for evaluation of any proposed quality estimator.</p><p>An adoption of this framework by the community would facilitate collaborative effort toward effective solutions for the very challenging problem of NR quality estimation. We believe that ''the whole is greater than a sum of the parts'' and that through joint efforts, substantial progress can be made toward effective noreference quality estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>journal homepage: www.elsevier.com/locate/image Signal Processing: Image Communication 0923-5965/$ -see front matter &amp; 2010 Elsevier B.V. All rights reserved. doi:10.1016/j.image.2010.05.009 using experimental evidence of HVS sensory mechanisms.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Perceptual quality metrics applied to still image compression, Signal Process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="177" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Issues in vision modeling for perceptual video quality assessment, Signal Process</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="231" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The evolution of video quality measurement: from PSNR to hybrid metrics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mohandas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Broadcast</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="660" to="668" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mean squared error: love it or leave it? A new look at signal fidelity measures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation of image appeal in consumer photography</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Savakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Etz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="volume">3959</biblScope>
			<biblScope unit="page" from="111" to="120" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithmic inferencing of aesthetics and emotion in natural images: an exposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aesthetic visual quality assessment of paintings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="252" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image harmony for consumer images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neustaedter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="121" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Samadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berfanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bausk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representative image thumbnails: automatic and manual</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">6806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A methodology for designing no-reference video quality metrics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C Q</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>VPQM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A survey of hybrid MC/DPCM/DCT video coding distortions, Signal Process</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="247" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Artefacts in image and video systems: classification and mitigation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Punchihewa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Image and Vision Computing</title>
		<meeting>Image and Vision Computing<address><addrLine>New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="197" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Digital transport of video teleconferencing/ video telephony signals-performance terms, definitions, and examples</title>
		<idno>ANSI T1.801.02-1996</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>American National Standard for Telecommunication</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Design of a tool to benchmark scaling algorithms on LCD monitors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vicario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Heynderickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ferretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SID Digest of Technical Papers</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="704" to="707" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quality evaluation of motion-compensated edge artifacts in compressed video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Leontaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Reibman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="943" to="956" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Content-aware scalability-type selection for rate adaptation of scalable video</title>
		<author>
			<persName><forename type="first">E</forename><surname>Akyol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Civanlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="214" to="214" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">New quality metric for multimedia compression using faulty hardware</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Video Processing and Quality Metrics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A study of cognitive resilience in a JPEG compressor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nowroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Dependable Systems and Networks</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detection and removal of rainbow effects artifacts</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="297" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On missing data treatment for degraded video and film archives: a survey and a new Bayesian approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kokaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="415" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Color moving pictures quality metric</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branden</forename><surname>Lambrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="885" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Method and apparatus for assessing the visibility of differences between two image sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><surname>Finard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">159</biblScope>
			<date type="published" when="1999-10">October 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Toward a perceptual video quality metric, Human Vision and Electronic Imaging</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">3299</biblScope>
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A scalable wavelet-based video distortion metric and applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Masry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sermadevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="273" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Studies in Optics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Michelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1927">1927</date>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contrast-coding in amblyopia. I. Differences in the neural basis of human amblyopia</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Piotrowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. R. Soc. London Ser. B</title>
		<imprint>
			<biblScope unit="volume">217</biblScope>
			<biblScope unit="page" from="309" to="330" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contrast in complex images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Peli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2032" to="2040" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The standard deviation of luminance as a metric for contrast in random-dot images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moulden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A A</forename><surname>Kingdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Gatley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="79" to="101" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Visual Pattern Analyzers</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contrast masking in human vision</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Legge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Foley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1458" to="1470" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Normalization of cell responses in cat striate cortex, Visual Neurosci</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="181" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human luminance pattern mechanisms: masking experiments require a new model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Foley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1710" to="1719" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pattern detection in the presence of maskers that differ in spatial phase and temporal offset: threshold measurements and a model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3855" to="3872" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A model of visual contrast gain control and pattern masking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2379" to="2391" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image quality and entropy masking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Borthwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Vision, Visual Processing, and Digital Display VIII, Proceedings of SPIE</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">3016</biblScope>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Suprathreshold visual psychophysics and structure-based visual masking</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Moses</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Visual Communications and Image Processing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatial and temporal contrast-sensitivity functions of the visual system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Robson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1441" to="1442" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatial and temporal contrast-sensitivity functions of the visual system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiwatashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1245" to="1263" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Motion and vision. II. Stabilized spatio-temporal threshold surface</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1340" to="1349" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal properties of human visual filters: number shapes and spatial covariation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Snowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal detection in human vision: dependence on stimulus energy</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Fredericksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2557" to="2569" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Estimating multiple temporal mechanisms in human vision</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Fredericksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1023" to="1040" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatio-temporal integration in the human peripheral retina</title>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1011" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Time intensity reciprocity under various conditions of adaptation and backward masking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="543" to="549" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Hering</surname></persName>
		</author>
		<title level="m">Zur lehre vom Lichtsinne, Carl Gerolds and Sohn</title>
		<imprint>
			<date type="published" when="1878">1878</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Some quantitative aspects of an opponent-colors theory. I. chromatic responses and spectral saturation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jameson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Hurvich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="546" to="552" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An opponent-process theory of color vision</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Hurvich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jameson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="384" to="404" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatiotemporal chromaticity discrimination</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J C</forename><surname>Van Der Horst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1482" to="1488" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spatiotemporal chromaticity discrimination</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Heurtley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1173" to="1174" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Luminous and chromatic flickering patterns have opposite effects</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="issue">4186</biblScope>
			<biblScope unit="page" from="371" to="372" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Devalois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Devalois</surname></persName>
		</author>
		<title level="m">Spatial Vision</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Color preference and perceived color naturalness of digital videos</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="volume">6057</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Color preference, color naturalness, and annoyance of compressed and color scaled digital videos</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="volume">6492</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An investigation of perceived sharpness and sharpness metrics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pizlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the SPIE</title>
		<imprint>
			<biblScope unit="volume">5668</biblScope>
			<biblScope unit="page" from="98" to="110" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Handbook of Image Quality: Characterization and Prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Keelan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Marcel Dekker, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">ITU-T recommendation P.930: principles of a reference impairment system for video</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>International Telecommunication Union</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Detectability and annoyance of synthetic blocky, blurry, noisy, and ringing artifacts</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C Q</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2954" to="2964" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Pastrana-Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Gicquel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Colomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cherifi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sporadic frame dropping impact on quality perception</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5292</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal masking effect on dropped frames at video scene cuts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Pastrana-Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Gicquel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Colomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cherifi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="volume">5292</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal aspects of perceived quality in mobile video broadcasting</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huynh-Thu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Broadcast</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="641" to="651" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Defect visibility and content importance: effects on perceived impairment, Signal Process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Perceptual effects of packet loss on H.264/AVC encoded videos</title>
		<author>
			<persName><forename type="first">F</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parrein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Hands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Video Processing and Quality Metrics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Modeling packet-loss visibility in MPEG-2 video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanumuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Reibman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vaishampayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="355" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">264 packet loss visibility using a generalized linear model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanumuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Reibman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Predicting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2006-10-11">8-11 October 2006</date>
			<biblScope unit="page" from="2245" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Frame rate preferences in low bit rate video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yadavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Masry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2003-09-17">14-17 September 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="441" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Modeling and subjective assessment of cell discard in atm video</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghanbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seferidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="212" to="222" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Video quality evaluation for Internet streaming applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Campos</surname></persName>
		</author>
		<editor>SPIE,</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="104" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Multidimensional</forename><surname>Scaling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<pubPlace>Sage</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multidimensional modeling of image quality</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="133" to="153" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Multidimensional scaling of multiplyimpaired television pictures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="353" to="356" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Transmitted Picture Assessment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allnatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Wiley and Sons</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Perceptual characterization of images degraded by blur and noise: experiments</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kayargadde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1166" to="1177" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Effects of spatial correlations and global precedence on the visual fidelity of distorted images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Human Vision and Electronic Imaging</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Subjective quality of internet video codecs phase II evaluations using SAMVIQ</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kozamernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sunna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wyckens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Pettersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-01">January 2005</date>
		</imprint>
		<respStmt>
			<orgName>EBU Technical Review</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">ISO 20462, a psychological image quality measurement standard</title>
		<author>
			<persName><forename type="first">B</forename><surname>Keelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Urabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="volume">5294</biblScope>
			<biblScope unit="page" from="181" to="189" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Final report from the Video Quality Experts Group on the validation of objective models of video quality assessment</title>
		<author>
			<orgName type="collaboration">Video Quality Experts Group</orgName>
		</author>
		<imprint>
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Final report from the video quality experts group on the validation of objective models of video quality assessment</title>
		<author>
			<orgName type="collaboration">Video Quality Experts Group</orgName>
		</author>
		<imprint>
			<date type="published" when="2003-08">August 2003</date>
			<biblScope unit="volume">II</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Final report from the video quality experts group on the validation of objective models of multimedia quality assessment, phase I</title>
		<author>
			<orgName type="collaboration">Video Quality Experts Group</orgName>
		</author>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Recommendation 500-3: method for the subjective assessment of the quality of television pictures</title>
		<author>
			<persName><surname>Ccir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>International Telecommunication Union</publisher>
			<pubPlace>Geneva</pubPlace>
		</imprint>
	</monogr>
	<note>Recommendations and Reports of the CCIR</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Test plan for evaluation of quality models for IPTV services</title>
		<author>
			<persName><surname>Atis-0800025</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-10">October 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Corriveau</surname></persName>
		</author>
		<title level="m">Digital Video Image Quality and Perceptual Coding</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</editor>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="125" to="154" />
		</imprint>
	</monogr>
	<note>Video quality testing. Chapter 4</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comparison of various subjective video quality assessment methods</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6059</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The picture appraisal rating (PAR)-a single-ended picture quality measure for MPEG-2</title>
		<author>
			<persName><forename type="first">M</forename><surname>Knee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Broadcasting Convention</title>
		<meeting>International Broadcasting Convention</meeting>
		<imprint>
			<date type="published" when="2000-09">September 2000</date>
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">No reference PSNR estimation for compressed pictures, Signal Process</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caviedes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="173" to="184" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A method of estimating coding PSNR using quantized DCT coefficients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ichigaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurozumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nakasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="259" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Nonreference method for estimating PSNR of MPEG-2 coded video by using DCT coefficients and picture energy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ichigaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nakasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="817" to="826" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Blind PSNR estimation of video sequences using quantized DCT coefficient data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Queluz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Picture Coding Symposium</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">No-reference image quality assessment based on DCT domain statistics, Signal Process</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Queluz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="822" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">No-reference estimation of the coding PSNR for H.264-coded sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Consum. Electron</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="667" to="674" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Estimation of the peak signal-to-noise ratio for compressed video based on generalized Gaussian modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">No-reference PSNR estimation algorithm for H.264 encoded video sequences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Queluz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">PSNR estimation scheme using coefficient distribution of frequency domain in H.264 decoder</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Quality monitoring of video over a packet network</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Reibman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vaishampayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sermadevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="327" to="334" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Low complexity quality monitoring of MPEG-2 video in a network</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Reibman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaishampayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2003-09-17">14-17 September 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="261" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">No-reference modeling of the channel induced distortion at the decoder for H.264/AVC video coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naccari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">No-reference PSNR estimation for quality monitoring of motion JPEG2000 video over lossy packet networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Munadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="637" to="645" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Estimation of noise in images: an evaluation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVGIP: Graphical Models Image Processing</title>
		<imprint>
			<date type="published" when="1993-07">July 1993</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="319" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Blind image quality assessment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICIP02</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Fast and reliable structure-oriented video noise estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A real-time technique for spatiotemporal video noise estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghrayeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1690" to="1699" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">An objective measure for perceived noise, Signal Process</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kayargadde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Martens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="187" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Estimation of image noise variance, IEE Visual Image Signal Process</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lendl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Unbehauen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="80" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">DCT quantization matrices optimized for individual images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Human Vision, Visual Processing, and Digital Display IV</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Perceptual video quality and blockiness metrics for multimedia streaming applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcnally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Wireless Personal Multimedia Communications</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="547" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A generalized block-edge impairment metric for video coding</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="317" to="320" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A perceptually significant block-edge impairment metric for digital video coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suthaharan</surname></persName>
		</author>
		<idno>III-681-III-684</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">No-reference JPEG-image quality assessment using GAP-RBF, Signal Process</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perkis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1493" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Blind measurement of blocking artifacts in images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Efficient DCT-domain blind measurement and reduction of blocking artifacts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1139" to="1149" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">No-reference noticeable blockiness estimation in images, Signal Process</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="417" to="432" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">A single-ended blockiness measure for JPEG-coded images, Signal Process</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meesters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Martens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="369" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A no-reference block artifact measure for adaptive video processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Muijs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kirenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUSIPCO &apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">A no-reference objective image sharpness metric based on the notion of just noticeable blur JNB</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ferzli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="728" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Algorithm for the computation of region-based image edge profile acutance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rangayyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Elkadiki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="70" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Perceptual blur and ringing metrics: application to JPEG2000</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sig. Process. Image Commun</title>
		<imprint>
			<biblScope unit="page" from="163" to="172" />
			<date type="published" when="2004-02">February 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A no-reference perceptual image sharpness metric based on saliency-weighted foveal pooling</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Sadaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferzli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Abousleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008-10-15">12-15 October 2008</date>
			<biblScope unit="page" from="369" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">A noreference quality metric for measuring image blur</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moschetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Signal Processing and its Applications</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="469" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Estimation of edge parameters and imager blur using polynomial transforms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kayargadde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVGIP: Graphical Models Image Process</title>
		<imprint>
			<date type="published" when="1994-11">November 1994</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="442" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A new sharpness measure based on Gaussian lines and edges</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Grinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Van Asselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Van Vliet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis on Images and Patterns (CAIP)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2756</biblScope>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Blur determination in the compressed domain using DCT information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Marichal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="386" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Perceptual sharpness metric (PSM) for compressed video</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Guest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Rouge ´, Measuring the global phase coherence of an image</title>
		<author>
			<persName><forename type="first">G</forename><surname>Blanchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1176" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A new sharpness metric based on local kurtosis edge and energy information, Signal Process</title>
		<author>
			<persName><forename type="first">J</forename><surname>Caviedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Oberti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="147" to="161" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Image coding ringing artifact reduction using morphologicalpost-filtering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="628" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Measurement of ringing artifacts in JPEG images</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6076</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Perceptually relevant ringing region detection method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Klomp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Heynderickx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A no-reference metric for perceived ringing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Klomp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Heynderickx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Video Processing and Quality Metrics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Hybrid no-reference video quality prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bayart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Hands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Broadband Multimedia Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">No reference perceptual quality metrics: approaches and limitations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bayart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bourret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Human Vision and Electronic Imaging XIV</publisher>
			<biblScope unit="page">72400</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Pareto optimal weighting of structural impairments for wireless imaging quality assessment</title>
		<author>
			<persName><forename type="first">U</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zepernick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="373" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">No-reference quality assessment using natural scene statistics: JPEG 2000</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cormak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1918" to="1927" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Image compression via joint statistical characterization in the wavelet domain</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Buccigrossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1688" to="1701" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">A no-reference spatial aliasing measure for digital image resizing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Reibman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suthaharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
			<biblScope unit="page" from="1184" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Color image quality on the Internet</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Susstrunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="volume">5304</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Predicting the usefulness and naturalness of color reproductions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Blommaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Imaging Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Correction of intensity flicker in old film sequences</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M B</forename><surname>Van Roosmalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lagendijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Biemond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1013" to="1019" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Flicker correction for archived film sequences using a nonlinear model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="508" to="516" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Post-processing for flicker reduction in H.264/AVC</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuszpet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kletsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moshe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Picture Coding Symposium</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Objective quality evaluation of video services</title>
		<author>
			<persName><forename type="first">M</forename><surname>Montenovo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cicchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Video Processing and Quality Metrics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Automatic quality assessment of video fluidity impairments using a no-reference metric</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Pastrana-Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Gicquel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Video Processing and Quality Metrics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Objective video quality assessment method for evaluating effects of freeze distortion in arbitrary video scenes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">6494</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Perceptual temporal quality metric for compressed video</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Guest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>El-Maleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1528" to="1535" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">User-oriented QoS analysis in MPEG-2 video delivery</title>
		<author>
			<persName><forename type="first">O</forename><surname>Verscheure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Real-Time Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A study of real-time packet video quality using random neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rubino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1071" to="1083" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">No-reference metrics for video streaming applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bopardikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perkis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">I</forename><surname>Hillestad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Packet Video</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Evaluation of packet loss impairment on streaming video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Zhejiang University SCIENCE B</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2006-04">April. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Video packet loss concealment detection based on image content</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shabtay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moshe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">No-reference video quality estimation based on error-concealment effectiveness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoshihiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Serizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Packet Video</publisher>
			<biblScope unit="page" from="288" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">A robust error detection mechanism for H.264/AVC coded video sequences based on support vector machines</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Farrugia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Debono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1766" to="1770" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Accuracy and crosscalibration of video-quality metrics: new methods from ATIS/ T1A1, Signal Process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="101" to="107" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Data and sample program code to be used with the method specified in T1.TR.72-2001 for the calculation of resolving power of the video quality metrics in T1</title>
		<idno>TR.74-2001 and T1.TR.75-2001</idno>
		<imprint>
			<date type="published" when="2002-01">January 2002</date>
		</imprint>
	</monogr>
	<note type="report_type">ATIS Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
