<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sliced Score Matching: A Scalable Approach to Density and Score Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-27">27 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
							<email>&lt;yangsong@cs.stanford.edu&gt;</email>
						</author>
						<author>
							<persName><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sliced Score Matching: A Scalable Approach to Density and Score Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-27">27 Jun 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1905.07088v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Score matching is a popular method for estimating unnormalized statistical models. However, it has been so far limited to simple, shallow models or low-dimensional data, due to the difficulty of computing the Hessian of logdensity functions. We show this difficulty can be mitigated by projecting the scores onto random vectors before comparing them. This objective, called sliced score matching, only involves Hessian-vector products, which can be easily implemented using reverse-mode automatic differentiation. Therefore, sliced score matching is amenable to more complex models and higher dimensional data compared to score matching. Theoretically, we prove the consistency and asymptotic normality of sliced score matching estimators. Moreover, we demonstrate that sliced score matching can be used to learn deep score estimators for implicit distributions. In our experiments, we show sliced score matching can learn deep energy-based models effectively, and can produce accurate score estimates for applications such as variational inference with implicit distributions and training Wasserstein Auto-Encoders.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Score matching <ref type="bibr" target="#b12">(Hyvärinen, 2005)</ref> is particularly suitable for learning unnormalized statistical models, such as energy based ones. It is based on minimizing the distance between the derivatives of the log-density functions (a.k.a., scores) of the data and model distributions. Unlike maximum likelihood estimation (MLE), the objective of score matching only depends on the scores, which are oblivious to the (usually) intractable partition functions. However, score matching requires the computation of the diagonal elements of the Hessian of the model's log-density function. This Hessian trace computation is generally expensive <ref type="bibr" target="#b19">(Martens et al., 2012)</ref>, requiring a number of forward and backward propagations proportional to the data dimension. This severely limits its applicability to complex models parameterized by deep neural networks, such as deep energy-based models <ref type="bibr" target="#b15">(LeCun et al., 2006;</ref><ref type="bibr" target="#b35">Wenliang et al., 2019)</ref>.</p><p>Several approaches have been proposed to alleviate this difficulty: <ref type="bibr" target="#b13">Kingma &amp; LeCun (2010)</ref> propose approximate backpropagation for computing the trace of the Hessian; <ref type="bibr" target="#b19">Martens et al. (2012)</ref> develop curvature propagation, a fast stochastic estimator for the trace in score matching; and <ref type="bibr" target="#b34">Vincent (2011)</ref> transforms score matching to a denoising problem which avoids second-order derivatives. These methods have achieved some success, but may suffer from one or more of the following problems: inconsistent parameter estimation, large estimation variance, and cumbersome implementation.</p><p>To alleviate these problems, we propose sliced score matching, a variant of score matching that can scale to deep unnormalized models and high dimensional data. The key intuition is that instead of directly matching the high-dimensional scores, we match their projections along random directions. Theoretically, we show that under some regularity conditions, sliced score matching is a well-defined statistical estimation criterion that yields consistent and asymptotically normal parameter estimates. Moreover, compared to the methods of <ref type="bibr" target="#b13">Kingma &amp; LeCun (2010)</ref> and <ref type="bibr" target="#b19">Martens et al. (2012)</ref>, whose implementations require customized backpropagation for deep networks, sliced score matching only involves Hessian-vector products, thus can be easily and efficiently implemented in frameworks such as TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and PyTorch <ref type="bibr" target="#b1">(Adam et al., 2017)</ref>.</p><p>Beyond training unnormalized models, sliced score matching can also be naturally adapted as an objective for estimating the score function of a data generating distribution <ref type="bibr" target="#b24">(Sasaki et al., 2014;</ref><ref type="bibr" target="#b28">Strathmann et al., 2015)</ref> by training a score function model parameterized by deep neural networks. This observation enables many new applications of sliced score matching. For example, we show that it can be used to provide accurate score estimates needed for variational inference with implicit distributions <ref type="bibr" target="#b10">(Huszár, 2017)</ref> and learning Wasserstein Auto-Encoders (WAE, <ref type="bibr" target="#b31">Tolstikhin et al. (2018)</ref>).</p><p>Finally, we evaluate the performance of sliced score matching on learning unnormalized statistical models (density estimation) and estimating score functions of a data generating process (score estimation). For density estimation, experiments on deep kernel exponential families <ref type="bibr" target="#b35">(Wenliang et al., 2019)</ref> and NICE flow models <ref type="bibr" target="#b3">(Dinh et al., 2015)</ref> show that our method is either more scalable or more accurate than existing score matching variants.</p><p>For score estimation, our method improves the performance of variational auto-encoders (VAE) with implicit encoders, and can train WAEs without a discriminator or MMD loss by directly optimizing the KL divergence between aggregated posteriors and the prior. In both situations we outperformed kernel-based score estimators <ref type="bibr" target="#b16">(Li &amp; Turner, 2018;</ref><ref type="bibr" target="#b25">Shi et al., 2018)</ref> by achieving better test likelihoods and better sample quality in image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Given i.i.d. samples {x 1 , x 2 , • • • , x N } ⊂ R D from a data distribution p d (x), our task is to learn an unnormalized density, pm (x; θ), where θ is from some parameter space Θ. The model's partition function is denoted as Z θ , which is assumed to be existent but intractable. Let p m (x; θ) be the normalized density determined by our model, we have</p><formula xml:id="formula_0">p m (x; θ) = pm (x; θ) Z θ , Z θ = pm (x; θ)dx.</formula><p>For convenience, we denote the score functions of p m and p d as s m (x; θ) ∇ x log p m (x; θ) and s d (x) ∇ x log p d (x) respectively. Note that since log p m (x; θ) = log pm (x; θ) − log Z θ , we immediately conclude that s m (x; θ) does not depend on the intractable partition function Z θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SCORE MATCHING</head><p>Learning unnormalized models with maximum likelihood estimation (MLE) can be difficult due to the intractable partition function Z θ . To avoid this, score matching <ref type="bibr" target="#b12">(Hyvärinen, 2005)</ref> minimizes the Fisher divergence between p d and p m (•, θ), which is defined as</p><formula xml:id="formula_1">L(θ) 1 2 E p d [ s m (x; θ) − s d (x) 2 2 ].<label>(1)</label></formula><p>Since s m (x; θ) does not involve Z θ , the Fisher divergence does not depend on the intractable partition function. However, Eq. ( <ref type="formula" target="#formula_1">1</ref>) is still not readily usable for learning unnormalized models, as we only have samples and do not have access to the score function of the data s d (x).</p><p>By applying integration by parts, <ref type="bibr" target="#b12">Hyvärinen (2005)</ref> shows that L(θ) can be written as L(θ) = J(θ) + C (cf ., Theorem 1 in Hyvärinen ( <ref type="formula">2005</ref>)), where</p><formula xml:id="formula_2">J(θ) E p d tr(∇ x s m (x; θ)) + 1 2 s m (x; θ) 2 2 ,<label>(2)</label></formula><p>C is a constant that does not depend on θ, tr(•) denotes the trace of a matrix, and</p><formula xml:id="formula_3">∇ x s m (x; θ) = ∇ 2 x log pm (x; θ) (3)</formula><p>is the Hessian of the log-density function. The constant can be ignored and the following unbiased estimator of the remaining terms is used to train pm (x; θ):</p><formula xml:id="formula_4">Ĵ(θ; x N 1 ) 1 N N i=1 tr(∇ x s m (x i ; θ)) + 1 2 s m (x i ; θ) 2 2 ,</formula><p>where x N 1 is a shorthand used throughout the paper for a collection of N data points {x 1 , x 2 , • • • , x N } sampled i.i.d. from p d , and ∇ x s m (x i ; θ) denotes the Hessian of log pm (x; θ) evaluted at x i .</p><p>Computational Difficulty. While the score matching objective Ĵ(θ; x N 1 ) avoids the computation of Z θ for unnormalized models, it introduces a new computational difficulty: computing the trace of the Hessian of a logdensity function, ∇ 2</p><p>x log pm . A naïve approach of computing the trace of the Hessian requires D times more backward passes than computing the gradient s m = ∇ x log pm (see Alg. 2 in the appendix). For example, the trace could be computed by applying backpropogation D times to s m to get each diagonal term of ∇ 2</p><p>x log pm sequentially. In practice, D can be many thousands, which can render score matching too slow for practical purposes. Moreover, <ref type="bibr" target="#b19">Martens et al. (2012)</ref> argues from a theoretical perspective that it is unlikely that there exists an algorithm for computing the diagonal of the Hessian defined by an arbitrary computation graph within a constant number of forward and backward passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SCORE ESTIMATION FOR IMPLICIT DISTRIBUTIONS</head><p>Besides parameter estimation in unnormalized models, score matching can also be used to estimate scores of implicit distributions, which are distributions that have a tractable sampling process but without a tractable density. For example, the distribution of random samples from the generator of a GAN <ref type="bibr" target="#b5">(Goodfellow et al., 2014)</ref> is an implicit distribution. Implicit distributions can arise in many more situations such as the marginal distribution of a non-conjugate model <ref type="bibr" target="#b29">(Sun et al., 2019)</ref>, and models defined by complex simulation processes <ref type="bibr" target="#b32">(Tran et al., 2017)</ref>. In many cases learning and inference become intractable due to the need of optimizing an objective that involves the intractable density.</p><p>In these cases, directly estimating the score function s q (x) = ∇ x log q θ (x) based on i.i.d. samples from an implicit density q θ (x) can be useful. For example, suppose our learning problem involves optimizing the entropy H(q θ (x)) of an implicit distribution. This situation is common when dealing with variational free energies <ref type="bibr" target="#b14">(Kingma &amp; Welling, 2014)</ref>. Suppose x ∼ q θ can be reparameterized as x = g θ ( ), where is a simple random variable independent of θ, such as a standard normal, and g θ is a deterministic mapping. We can write the gradient of the entropy with respect to θ as</p><formula xml:id="formula_5">∇ θ H(q θ ) −∇ θ E q θ (x) [log q θ (x)] = −∇ θ E p( ) [log q θ (g θ ( ))] = −E p( ) [∇ x log q θ (x)| x=g θ ( ) ∇ θ g θ ( )],</formula><p>where ∇ θ g θ ( ) is usually easy to compute. The score ∇ x log q θ (x) is intractable but can be approximated by score estimation.</p><p>Score matching is an attractive solution for score estimation since (1) naturally serves as an objective to measure the difference between the a trainable score function and the score of a data generating process. We will discuss this in more detail in Section 3.2 and mention some other approaches of score estimation in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DENSITY AND SCORE ESTIMATION WITH SLICED SCORE MATCHING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SLICED SCORE MATCHING</head><p>We observe that one dimensional problems are usually much easier to solve than high dimensional ones. Inspired by the idea of Sliced Wasserstein distance <ref type="bibr" target="#b22">(Rabin et al., 2012)</ref>, we consider projecting s d (x) and s m (x; θ) onto some random direction v and propose to compare their average difference along that random direction. More specifically, we consider the following objective as a replacement of the Fisher divergence L(θ) in Eq. (1):</p><formula xml:id="formula_6">L(θ; p v ) 1 2 E pv E p d (v s m (x; θ) − v s d (x)) 2 .(<label>4</label></formula><formula xml:id="formula_7">)</formula><p>Here v ∼ p v and x ∼ p d are independent, and we require</p><formula xml:id="formula_8">E pv [vv ] 0 and E pv [ v 2 2 ] &lt; ∞.</formula><p>Many examples of p v satisfy these requirements. For instance, p v can be a multivariate standard normal (N (0, I D )), a multivariate Rademacher distribution (the uniform distribution over {±1} D ), or a uniform distribution on the hypersphere S D−1 (recall that D refers to the dimension of x).</p><p>To eliminate the dependence of L(θ; p v ) on s d (x), we use integration by parts as in score matching (cf ., the equivalence between Eq. ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula" target="#formula_2">2</ref>)). Defining</p><formula xml:id="formula_9">J(θ; p v ) E pv E p d v ∇ x s m (x; θ)v + 1 2 (v s m (x; θ)) 2 , (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>the equivalence is summarized in the following theorem.</p><p>Theorem 1. Under some regularity conditions (Assumption 1-3 in Appendix B.2), we have</p><formula xml:id="formula_11">L(θ; p v ) = J(θ; p v ) + C, (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>where C is a constant w.r.t. θ.</p><p>Other than our requirements on p v , the assumptions are exactly the same as in Theorem 1 of <ref type="bibr" target="#b12">Hyvärinen (2005)</ref>. We advise the interested readers to read Appendix B.2 for technical statements of the assumptions and a rigorous proof of the theorem.</p><p>In practice, given a dataset x N 1 , we draw M projection vectors independently for each point x i from p v . The collection of all such vectors {v ij } 1≤i≤N,1≤j≤M are abbreviated as v N M 11 .We then use the following unbiased estimator of J(θ; p v )</p><formula xml:id="formula_13">Ĵ(θ; x N 1 , v N M 11 ) 1 N 1 M N i=1 M j=1 v ij ∇ x s m (x i ; θ)v ij + 1 2 v ij s m (x i ; θ) 2 . (7)</formula><p>Note that when p v is a multivariate standard normal or multivariate Rademacher distribution, we have</p><formula xml:id="formula_14">E pv [(v s m (x; θ)) 2 ] = s m (x; θ) 2 2</formula><p>, in which case the second term of J(θ; p v ) can be integrated analytically, and may lead to an estimator with reduced variance:</p><formula xml:id="formula_15">Ĵvr (θ; x N 1 , v N M 11 ) 1 N 1 M N i=1 M j=1 v ij ∇ x s m (x i ; θ)v ij + 1 2 s m (x i ; θ) 2 2 . (8)</formula><p>Empirically, we do find that Ĵvr has better performance than Ĵ. We refer to this version as sliced score matching with variance reduction (SSM-VR). In fact, we can leverage E pv [(v s m (x; θ))<ref type="foot" target="#foot_0">2</ref> ] = s m (x; θ) 2 2 to create a control variate for guaranteed reduction of variance (Appendix D). Ĵvr is also closely related to Hutchinson's trace estimator <ref type="bibr" target="#b11">(Hutchinson, 1990</ref>), which we will analyze later in Section 4.3.</p><p>For sliced score matching, the second derivative term v ∇ x s m (x; θ)v is much less computationally expensive than tr(∇ x s m (x; θ)). Using auto-differentiation systems that support higher order gradients, we can compute it using two gradient operations for a single v, as shown in Alg. 1. Similarly, when there are M v's, the total number of gradient operations is M + 1. In contrast, assuming the dimension of x is D and D M , we typically need D + 1 gradient operations to compute tr(∇ x s m (x; θ)) because each diagonal entry of ∇ x s m (x; θ) needs to be computed separately (see Alg. 2 in the appendix).</p><formula xml:id="formula_16">Algorithm 1 Sliced Score Matching Input: pm (•; θ), x, v 1: s m (x; θ) ← grad(log pm (x; θ), x) 2: v ∇ x s m (x; θ) ← grad(v s m (x; θ), x) 3: J ← 1 2 (v s m (x; θ)) 2 (or J ← 1 2 s m (x; θ) 2 2 ) 4: J ← J + v ∇ x s m (x; θ)v return J</formula><p>In practice, we can tune M to trade off variance and computational cost. In our experiments, we find that oftentimes M = 1 is already a good choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SLICED SCORE ESTIMATION</head><p>As mentioned in section 2.2, the task of score estimation is to estimate ∇ x log q(x) at some test point x, given a set of samples x N 1 i.i.d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∼ q(x).</head><p>In what follows, we show how our sliced score matching objective Ĵ(θ; x N 1 , v N M 11 ) can be straightforwardly adapted for this task.</p><p>We propose to use a vector-valued deep neural network h(x; θ) : R D → R D as our score model. Then, substituting</p><formula xml:id="formula_17">h into Ĵ(θ; x N 1 , v N M 11 ) for s m , we get 1 N 1 M N i=1 M j=1 v ij ∇ x h(x i ; θ)v ij + 1 2 v ij h(x i ; θ)</formula><p>any scalar function. For h(x; θ) to represent a gradient, one necessary condition is ∇x × h(x; θ) = 0 for all x, which may not be satisfied by general networks. However, this is oblivious to the fact that h(x; θ) will be close to ∇x log q θ (x) in 2 norm. As will be shown later, our argument based on integration by parts does not require h(x; θ) to be a gradient.</p><p>Using a similar argument of integration by parts (cf ., Eq. ( <ref type="formula" target="#formula_6">4</ref>), ( <ref type="formula" target="#formula_9">5</ref>) and ( <ref type="formula" target="#formula_11">6</ref>)), we have</p><formula xml:id="formula_18">E pv E p d v ∇ x h(x; θ)v + 1 2 (v h(x; θ)) 2 = 1 2 E pv E p d [(v h(x; θ) − v ∇ x log q(x)) 2 ] + C, which implies that minimizing Ĵ(θ; x N 1 , v N M 11</formula><p>) with s m (x; θ) replaced by h(x; θ) is approximately minimizing the average projected difference between h(x; θ) and ∇ x log q(x). Hence, h(x; θ) should be close to ∇ x log q(x) and can serve as a score estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL ANALYSIS</head><p>In this section, we present several theoretical results to justify sliced score matching as a principled objective. We also discuss the connection of sliced score matching to other methods. For readability, we will defer rigorous statements of assumptions and theorems to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CONSISTENCY</head><p>One important question to ask for any statistical estimation objective is whether the estimated parameter is consistent under reasonable assumptions. Our results confirm that for any M , the objective Ĵ(θ;</p><formula xml:id="formula_19">x N 1 , v N M 11</formula><p>) is consistent under suitable assumptions as N → ∞.</p><p>We need several standard assumptions to prove the results rigorously. Let p m be the normalized density induced by our unnormalized model pm , which is assumed to be normalizable. First, we assume Θ is compact (Assumption 6), and our model family {p m (x; θ) : θ ∈ Θ} is well-specified and identifiable (Assumption 4). These are standard assumptions used for proving the consistency of <ref type="bibr">MLE (van der Vaart, 1998)</ref>. We also adopt the assumption in <ref type="bibr" target="#b12">Hyvärinen (2005)</ref> that all densities are strictly positive (Assumption 5). Finally, we assume that p m (x; θ) has some Lipschitz properties (Assumption 7), and p v has bounded higher-order moments (Assumption 2, true for all p v considered in the experiments). Then, we can prove the consistency of θN,M arg min θ∈Θ Ĵ(θ; x N 1 , v N M 11 ): Theorem 2 (Consistency). Assume the conditions of Theorem 1 are satisfied. Assume further that the assumptions discussed above hold. Let θ * be the true parameter of the data distribution. Then for every</p><formula xml:id="formula_20">M ∈ N + , θN,M p → θ * , N → ∞.</formula><p>Sketch of proof. We first prove that J(θ; p v ) = 0 ⇔ θ = θ * (Lemma 1 and Theorem 1). Then we prove the uniform convergence of Ĵ(θ;</p><formula xml:id="formula_21">x N 1 , v N M 11 ) (Lemma 3), which</formula><p>holds regardless of M . These two facts lead to consistency. For a complete proof, see Appendix B.3.</p><p>Remark 1. In <ref type="bibr" target="#b12">Hyvärinen (2005)</ref>, the authors only showed that J(θ) = 0 ⇔ θ = θ * , which leads to "local consistency" of score matching. This is a weaker notion of consistency compared to our settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ASYMPTOTIC NORMALITY</head><p>Asymptotic normality results can be very useful for approximate hypothesis testing and comparing different estimators. Below we show that θN,M is asymptotically normal when N → ∞.</p><p>In addition to the assumptions in Section 4.1, we need an extra assumption to prove asymptotic normality. We require p m (x; θ) to have a stronger Lipschitz property (Assumption 9).</p><p>For simplicity, we denote</p><formula xml:id="formula_22">∇ x h(x)| x=x as ∇ x h(x ),</formula><p>where h(•) is an arbitrary function. In the following, we will only show the asymptotic normality result for a specific p v . More general results are in Appendix B.4. Theorem 3 (Asymptotic normality, special case). Assume the assumptions discussed above hold. If p v is the multivariate Rademacher distribution, we have</p><formula xml:id="formula_23">√ N ( θN,M − θ * ) d → N (0, Σ),</formula><p>where</p><formula xml:id="formula_24">Σ [∇ 2 θ J(θ * )] −1 1≤i,j≤D V ij + 2 M 1≤i =j≤D W ij • [∇ 2 θ J(θ * )] −1 . (9)</formula><p>Here D is the dimension of data; V ij and W ij are p.s.d matrices depending on p m (x; θ * ), and their definitions can be found in Appendix B.4. Sketch of proof. Once we get the consistency (Theorem 2), the rest closely follows the proof of asymptotic normality of <ref type="bibr">MLE (van der Vaart, 1998)</ref>. A rigorous proof can be found in Appendix B.4.</p><p>Remark 2. As expected, larger M will lead to smaller asymptotic variance, as can be seen in Eq. (9). Remark 3. As far as we know, there is no published proof of asymptotic normality for the standard (not sliced) score matching. However, by using the same techniques in our proofs, and under similar assumptions, we can conclude that the asymptotic variance of the score matching estimator is</p><formula xml:id="formula_25">[∇ 2 θ J(θ * )] −1 ij V ij [∇ 2 θ J(θ * )] −1 (Corollary 1</formula><p>), which will always be smaller than sliced score matching with multivariate Rademacher projections. However, the gap reduces when M increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CONNECTION TO OTHER METHODS</head><p>Sliced score matching is widely connected to many other methods, and can be motivated from some different perspectives. Here we discuss a few of them.</p><p>Connection to NCE. Noise contrastive estimation (NCE), proposed by <ref type="bibr" target="#b7">Gutmann &amp; Hyvärinen (2010)</ref>, is another principle for training unnormalized statistical models. The method works by comparing p m (x; θ) with a noise distribution p n (x). We consider a special form of NCE which minimizes the following objective</p><formula xml:id="formula_26">−E p d [log h(x; θ)] − E pn [log(1 − h(x; θ))],<label>(10)</label></formula><p>where h(x; θ) pm(x;θ) pm(x;θ)+pm(x−v;θ) , and we choose p n (x) = p d (x+v). Assuming v 2 to be small, Eq. ( <ref type="formula" target="#formula_26">10</ref>) can be written as the following by Taylor expansion</p><formula xml:id="formula_27">1 4 E p d v ∇ x s m (x; θ)v + 1 2 (s m (x; θ) v) 2 + 2 log 2 + o( v 2 2 ).<label>(11)</label></formula><p>For derivation, see Proposition 1 in the appendix. A similar derivation can also be found in <ref type="bibr" target="#b8">Gutmann &amp; Hirayama (2011)</ref>. As a result of ( <ref type="formula" target="#formula_27">11</ref>), if we choose some p v and take the expectation of (10) w.r.t. p v , the objective will be equivalent to sliced score matching whenever v 2 ≈ 0.</p><p>Connection to Hutchinson's Trick. Hutchinson's trick <ref type="bibr" target="#b11">(Hutchinson, 1990</ref>) is a stochastic algorithm to approximate tr(A) for any square matrix A. The idea is to choose a distribution of a random vector v such that E pv [vv ] = I, and then we have</p><formula xml:id="formula_28">tr(A) = E pv [v Av].</formula><p>Hence, using Hutchinson's trick, we can replace tr(∇ x s m (x; θ)) with E pv [v ∇ x s m (x; θ)v] in the score matching objective J(θ). Then the finite sample objective of score matching becomes</p><formula xml:id="formula_29">1 N N i=1 1 M M j=1 v ij ∇ x s m (x i ; θ)v ij + 1 2 s m (x i ; θ) 2 2 ,</formula><p>which is exactly the sliced score matching objective with variance reduction Ĵvr (θ; x N 1 , v N M 11 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SCALABLE SCORE MATCHING</head><p>To the best of our knowledge, there are three existing methods that are able to scale up score matching to learning deep models on high dimensional data.</p><p>Denoising Score Matching. <ref type="bibr" target="#b34">Vincent (2011)</ref> proposes denoising score matching, a variant of score matching that completely circumvents the Hessian. Specifically, consider a noise distribution q σ (x | x), and let q σ (x) = q σ (x | x)p d (x)dx. Denoising score matching applies the original score matching to the noise-corrupted data distribution q σ (x), and the objective can be proven to be equivalent to the following up to a constant</p><formula xml:id="formula_30">1 2 E qσ(x|x)p d (x) [ s m (x; θ) − ∇ x log q σ (x | x) 2 2 ],</formula><p>which can be estimated without computing any Hessian.</p><p>Although denoising score matching is much faster than score matching, it has obvious drawbacks. First, it can only recover the noise corrupted data distribution. Furthermore, choosing the parameters of the noise distribution is highly non-trivial and in practice the performance can be very sensitive to σ, and heuristics have to be used in practice. For example, <ref type="bibr" target="#b23">Saremi et al. (2018)</ref> propose a heuristic for choosing σ based on Parzen windows.</p><p>Approximate Backpropagation. <ref type="bibr" target="#b13">Kingma &amp; LeCun (2010)</ref> propose a backpropagation method to approximately compute the trace of the Hessian. Because the backpropagation of the full Hessian scales quadratically w.r.t. the layer size, the authors limit backpropagation only to the diagonal so that it has the same cost as the usual backpropagation for computing loss gradients. However, there are no theoretical guarantees for the approximation errors. In fact, the authors only did experiments on networks with a single hidden layer, in which case the approximation is exact. Moreover, there is no direct support for the proposed approximate backpropagation method in modern automatic differentiation frameworks.</p><p>Curvature Propagation. <ref type="bibr" target="#b19">Martens et al. (2012)</ref> estimate the trace term in score matching by applying curvature propagation to compute an unbiased, complex-valued estimator of the diagonal of the Hessian. The work claims that curvature propagation will have the smallest variance among a class of estimators, which includes the Hutchinson's estimator. However, their proof evaluates the pseudo-variance of the complex-valued estimator instead of the variance. In practice, curvature propagation can have large variance when the number of nodes in the network is large, because it introduces noise for each node in the network. Finally, implementing curvature propagation also requires manually modifying the backpropagation code, handling complex numbers in neural networks, and will be inefficient for networks of more general structures, such as recurrent neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">KERNEL SCORE ESTIMATORS</head><p>Two prior methods for score estimation are based on a generalized form of Stein's identity <ref type="bibr" target="#b27">(Stein, 1981;</ref><ref type="bibr" target="#b6">Gorham &amp; Mackey, 2017</ref>):</p><formula xml:id="formula_31">E q(x) [h(x)∇ x log q(x) + ∇ x h(x)] = 0,<label>(12)</label></formula><p>where q(x) is a continuously differentiable density and h(x) is a function satisfying some regularity conditions. <ref type="bibr" target="#b16">Li &amp; Turner (2018)</ref> propose to set h(x) as the feature map of some kernel in the Stein class <ref type="bibr" target="#b17">(Liu et al., 2016)</ref> of q, and solve a finite-sample version of ( <ref type="formula" target="#formula_31">12</ref>) to obtain estimates of ∇ x log q(x) at the sample points. We refer to this method as Stein in the experiments. <ref type="bibr" target="#b25">Shi et al. (2018)</ref> adopt a different approach but also exploit (12). They build their estimator by expanding ∇ x log q(x) as a spectral series of eigenfunctions and solve for the coefficients using ( <ref type="formula" target="#formula_31">12</ref>). Compared to Stein, their method is argued to have theoretical guarantees and principled out-of-sample estimation at an unseen test point. We refer to their method as Spectral in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>Our experiments include two parts: (1) to test the effectiveness of sliced score matching (SSM) in learning deep models for density estimation, and (2) to test the ability of SSM in providing score estimates for applications such as VAEs with implicit encoders and WAEs. Unless specified explicitly, we choose M = 1 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">DENSITY ESTIMATION</head><p>We evaluate SSM and its variance-reduced version (SSM-VR) for density estimation and compare against score matching (SM) and its three scalable baselines: denoising score matching (DSM), approximate backpropagation (approx BP), and curvature propagation (CP). All SSM methods in this section use the multivariate Rademacher distribution as p v . Our results demonstrate that: (1) SSM is comparable in performance to SM, (2) SSM outperforms other computationally efficient approximations to SM, and (3) unlike SM, SSM scales effectively to high dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Deep Kernel Exponential Families</head><p>Model. Deep kernel exponential families (DKEF) are unnormalized density models trained using SM <ref type="bibr" target="#b35">(Wenliang et al., 2019)</ref>. DKEFs parameterize the unnormalized log density as log pf (x) = f (x) + log q 0 (x), where f is a mixture of Gaussian kernels evaluated at different inducing points:</p><formula xml:id="formula_32">f (x) = L l=1 α l k(x, z l ).</formula><p>The kernel is defined on the feature space of a neural network, and the  Setup. Following the settings of <ref type="bibr" target="#b35">Wenliang et al. (2019)</ref>, we trained models on three UCI datasets: Parkinsons, RedWine, and WhiteWine <ref type="bibr" target="#b4">(Dua &amp; Graff, 2017)</ref>, and used their original code for SM. To compute the trace term exactly, <ref type="bibr" target="#b35">Wenliang et al. (2019)</ref>'s manual implementation of backpropagation takes over one thousand lines for a model that is four layers deep, while the implementation of SSM only takes several lines. For DSM, the value of σ is chosen by grid search using the SM loss on a validation dataset. All models are trained with 15 different random seeds and training is stopped when validation loss does not improve for 200 steps.</p><p>Results. Results in Fig. <ref type="figure" target="#fig_0">1</ref> demonstrate that SSM-VR performs comparably to SM, when evaluated using the SM loss on a held-out test set. We observe that variance reduction substantially improves the performance of SSM. In addition, SSM outperforms other computationally efficient approaches. DSM can perform comparably to SSM on RedWine. However, it is challenging to select σ for DSM. Models trained using σ from the heuristic in Saremi  <ref type="formula">2018</ref>) are far from optimal (on both SM losses and likelihoods), and extensive grid search is needed to find the best σ. CP performs substantially worse, which is likely because it injects noise for each node in the computation graph, and the amount of noise introduced is too large for a neural-network-based kernel evaluated at 200 inducing points, which supports our hypothesis that CP does not work effectively for deep models. Results for approx BP are omitted because the losses exceeded 10 9 on all datasets. This is because approx BP provides a biased estimate of the Hessian without any error guarantees.</p><p>All the results are similar when evaluating according to log-likelihood metric (Appendix C.1).</p><p>Scalability to High Dimensional Data. We evaluate the computational efficiency of different losses on data of increasing dimensionality. We fit DKEF models to a multivariate standard normal in high dimensional spaces. The average running time per minibatch of 100 examples is reported in Fig. <ref type="figure" target="#fig_1">2</ref>. SM performance degrades linearly with the dimension of the input data due to the computation of the Hessian, and creates out of memory errors for a 12GB GPU after the dimension increases beyond 400. SSM performs similarly to DSM, approx BP and CP, and they are all scalable to large dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Deep Flow Models</head><p>Setup. As a sanity check, we also evaluate SSM by training a NICE flow model <ref type="bibr" target="#b3">(Dinh et al., 2015)</ref>, whose likelihood is tractable and can be compared to results obtained by MLE. The model we use has 20 hidden layers, and 1000 units per layer. Models are trained to fit MNIST handwritten digits, which are 784 dimensional images. Data are dequantized by adding uniform noise in the range [− 1 512 , 1 512 ], and transformed using a logit transformation, log(x) − log(1 − x). We provide additional details in Appendix C.2.</p><p>Training with SM is extremely computationally expensive in this case. Our SM implementation based on autodifferentiation takes around 7 hours to finish one epoch of training, and 12 GB GPU memory cannot hold a batch size larger than 24, so we do not include these results. Since NICE has tractable likelihoods, we also evaluate MLE as a surrogate objective for minimizing the SM loss. Notably, likelihoods and SM losses might be uncorrelated when the model is mis-specified, which is likely to be the case for a complex dataset like MNIST.</p><p>Results. SM losses and log-likelihoods on the test dataset are reported in Tab. 1, where models are evaluated using the best checkpoint in terms of the SM loss on a validation dataset over 100 epochs of training. SSM-VR greatly outperforms all the other methods on the SM loss. DSM performs worse than SSM-VR, and σ is still hard to tune. Specifically, following the heuristic in <ref type="bibr" target="#b23">Saremi et al. (2018)</ref> leads to σ = 1.74, which performed the worst (on both log-likelihood and SM loss) of the eight choices of σ in our grid search. Approx BP has more success on NICE than for training DKEF models. This might be because the Hessians of hidden layers of NICE are closer to a diagonal matrix, which results in a smaller approximation error for approx BP. As in the DKEF experiments, CP performs worse. This is likely due to injecting noise to all hidden units, which will lead to large variance for a network as big as NICE. Unlike the DKEF experiments, we find that good log-likelihoods are less correlated with good SM loss, probably due to model mis-specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">SCORE ESTIMATION</head><p>We consider two typical tasks that require accurate score estimations: (1) training VAEs with an implicit encoder and ( <ref type="formula" target="#formula_2">2</ref>) training Wasserstein Auto-Encoders. We show in both tasks SSM outperforms previous score estimators. Samples generated by various algorithms can be found in Appendix A.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">VAE with Implicit Encoders</head><p>Background. Consider a latent variable model p(x, z), where x and z denote observed and latent variables respectively. A variational auto-encoder (VAE) is composed of two parts: 1) an encoder p θ (x | z) modeling the conditional distribution of x given z; and a decoder q φ (z | x) that approximates the posterior distribution of the latent variable. A VAE is typically trained by maximizing the following evidence lower bound (ELBO)</p><formula xml:id="formula_33">E p d [E q φ (z|x) log p θ (x | z)p(z) − E q φ (z|x) log q φ (z | x)],</formula><p>where p(z) denotes a pre-specified prior distribution. The expressive power of q φ (z | x) is critical to the success of variational learning. Typically, q φ (z | x) is chosen to be a simple distribution with tractable densities so that H(q φ ) −E q φ (z|x) log q φ (z | x) is tractable. We call this traditional approach "ELBO" in the experiments. With score estimation techniques, we can directly compute ∇ φ H(q φ ) for implicit distributions, which enables more flexible options for q φ . We consider 3 score estimation techniques: SSM, Stein <ref type="bibr" target="#b16">(Li &amp; Turner, 2018)</ref> and Spectral <ref type="bibr" target="#b25">(Shi et al., 2018)</ref>.</p><p>For a single data point x, kernel score estimators need multiple samples from q φ (z | x) to estimate its score. On MNIST, we use 100 samples, as done in <ref type="bibr" target="#b25">Shi et al. (2018)</ref>. On CelebA, however, we can only take 20 samples due to GPU memory limitations. In contrast, SSM learns a score network h(z | x) along with q φ (z | x), which amortizes the cost of score estimation. Unless noted explicitly, we use one projection per data point (M = 1) for SSM, which is scalable to deep networks.</p><p>Setup. We consider VAE training on MNIST and CelebA <ref type="bibr" target="#b18">(Liu et al., 2015)</ref>. All images in CelebA are first cropped to a patch of 140 × 140 and then resized to 64 × 64. We report negative likelihoods on MNIST, as estimated by AIS <ref type="bibr" target="#b20">(Neal, 2001)</ref> with 1000 intermediate distributions. We evaluate sample quality on CelebA with FID scores <ref type="bibr" target="#b9">(Heusel et al., 2017)</ref>. For fast AIS evaluation on MNIST, we use shallow fully-connected networks with 3 hidden layers. For CelebA experiments we use deep convolutional networks. The architectures of implicit encoders and score networks are straightforward modifications to the encoders of ELBO. More details are provided in Appendix C.3.</p><p>Results. The negative likelihoods of different methods on MNIST are reported in the left part of Tab. 2. We note that SSM outperforms Stein and Spectral in all cases. When the latent dimension is 8, SSM outperforms ELBO, which indicates that the expressive power of implicit q φ (z | x) pays off. When the latent dimension is 32, the gaps between SSM and kernel methods are even larger, and the performance of SSM is still comparable to ELBO. Moreover, when M = 100 (matching the computation of kernel methods), SSM outperforms ELBO.</p><p>For CelebA, we provide FID scores of samples in the top part of Tab. 3. We observe that after 40k training iterations, SSM outperforms all other baselines. Kernel methods perform poorly in this case because only 20 samples per data point can be used due to limited amount of GPU memory. Early during training, SSM does not perform as well. Since the score network is trained along with the encoder and decoder, a moderate number of training steps is needed to give an accurate score estimation (and better learning of the VAE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">WAEs</head><p>Background. WAE is another method to learn latent variable models, which generally produces better samples than VAEs. Similar to a VAE, it contains an encoder q φ (z | x) and a decoder p θ (x | z) and both can be implicit distributions. Let p(z) be a pre-defined prior distribution, and q φ (z) q φ (z | x)p d (x)dx denote the aggregated posterior distribution. Using a metric function c(•, •) and KL divergence between q φ (z) and p(z), WAE minimizes the following objective</p><formula xml:id="formula_34">E p d [E q φ (z|x) [c(x, p θ (x | z)) − λ log p(z)]] − λH(q φ (z)).</formula><p>Compared to ∇ φ H(q φ (z | x)), it is easier to estimate ∇ φ H(q φ (z)) for kernel methods, because the samples of q φ (z) can be collected by first sampling</p><formula xml:id="formula_35">x 1 , x 2 , • • • , x N i.i.d.</formula><p>∼ p d (x) and then sample one z for each x i from q φ (z | x i ). In constrast, multiple z's need to be sampled for each x i when estimating ∇ φ H(q φ (z | x)) with kernel approaches. For SSM, we use a score network h(z) and train it alongside q φ (z | x).</p><p>Setup. The setup for WAE experiments is largely the same as VAE. The architectures are very similar to those used in the VAE experiments, and the only difference is that we made decoders implicit, as suggested in <ref type="bibr" target="#b31">Tolstikhin et al. (2018)</ref>. More details can be found in Appendix C.3.</p><p>Results. The negative likelihoods on MNIST are provided in the right part of Tab. 2. SSM outperforms both kernel methods, and achieves a larger performance gap when the latent dimension is higher. The likelihoods are lower than the VAE results as the WAE objective does not directly maximize likelihoods.</p><p>We show FID scores for CelebA experiments in the bottom part of Tab. 3. As expected, kernel methods perform much better than before, because it is faster to sample from q φ (z). The FID scores are generally lower than those in VAE experiments, which supports previous results that WAEs generally obtain better samples. SSM outperforms both kernel methods when the number of iterations is more than 40k. This shows the advantages of training a deep, expressive score network compared to using a fixed kernel in score estimation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We propose sliced score matching, a scalable method for learning unnormalized statistical models and estimating scores for implicit distributions. Compared to the original score matching and its variants, our estimator can scale to deep models on high dimensional data, while remaining easy to implement in modern automatic differentiation frameworks. Theoretically, our estimator is consistent and asymptotically normal under some regularity conditions. Experimental results demonstrate that our method outperforms competitors in learning deep energy-based models and provides more accurate estimates than kernel score estimators in training implicit VAEs and WAEs.</p><formula xml:id="formula_36">to denote N × M vectors {v 11 , v 12 , • • • , v 1M , v 21 , v 22 , • • • , v 2M , • • • , v N 1 , v N 2 , • • • , v N M }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 BASIC PROPERTIES</head><p>The following regularity conditions are needed for integration by parts and identifiability. Theorem 1. Assume s m (x; θ), s d (x) and p v satisfy some regularity conditions (Assumption 1, Assumption 2). Under proper boundary conditions (Assumption 3), we have</p><formula xml:id="formula_37">L(θ; p v ) 1 2 E pv E p d (v s m (x; θ) − v s d (x)) 2 = E pv E p d v ∇ x s m (x; θ)v + 1 2 (v s m (x; θ)) 2 + C, (<label>13</label></formula><formula xml:id="formula_38">)</formula><p>where C is a constant w.r.t. θ.</p><p>Proof. The basic idea of this proof is similar to that of Theorem 1 in <ref type="bibr" target="#b12">Hyvärinen (2005)</ref>. First, note that L(θ, p v ) can be expanded to</p><formula xml:id="formula_39">L(θ, p v ) = 1 2 E pv E p d (v s m (x; θ) − v s d (x)) 2 (i) = 1 2 E pv E p d [(v s m (x; θ)) 2 + (v s d (x)) 2 − 2(v s m (x; θ))(v s d (x; θ))] (14) = E pv E p d − (v s m (x; θ))(v s d (x; θ)) + 1 2 (v s m (x; θ)) 2 + C,<label>(15)</label></formula><p>where (i) is due to the assumptions of bounded expectations. We have absorbed the second term in the bracket of ( <ref type="formula">14</ref>) into C since it does not depend on θ. Now what we need to prove is</p><formula xml:id="formula_40">−E pv E p d [(v s m (x; θ))(v s d (x; θ))] = E pv E p d [v ∇ x s m (x; θ)v].<label>(16)</label></formula><p>This can be shown by first observing that</p><formula xml:id="formula_41">− E pv E p d [(v s m (x; θ))(v s d (x; θ))] = − E pv p d (x)(v s m (x; θ))(v s d (x; θ))dx = − E d (x)(v ∇ x log p m (x; θ))(v ∇ x log p d (x))dx = − E pv (v ∇ x log p m (x; θ))(v ∇ x p d (x))dx = − E pv D i=1 (v ∇ x log p m (x; θ))v i ∂p d (x) ∂x i dx,<label>(17)</label></formula><p>where we assume x ∈ R D . Then, applying multivariate integration by parts (cf ., Lemma 4 in Hyvärinen ( <ref type="formula">2005</ref>)), we obtain</p><formula xml:id="formula_42">E pv D i=1 (v s m (x; θ))v i ∂p d (x) ∂x i dx + E pv D i=1 v i p d (x)v ∂s m (x; θ) ∂x i dx = E pv D i=1 lim xi→∞ (v s m (x; θ))v i p d (x) − D i=1 lim xi→−∞ (v s m (x; θ))v i p d (x) ≤ D i=1 lim xi→∞ D j=1 E pv |v i v j ||s m,j (x; θ)p d (x)| + D i=1 lim xi→−∞ D j=1 E pv |v i v j ||s m,j (x; θ)p d (x)| (i) ≤ D i=1 lim xi→∞ D j=1 E pv v 2 i E pv v 2 j |s m,j (x; θ)p d (x)| + D i=1 lim xi→−∞ D j=1 E pv v 2 i E pv v 2 j |s m,j (x; θ)p d (x)| (ii) = 0,</formula><p>where s m,j (x; θ) denotes the j-th component of s m (x; θ). In the above derivation, (i) is due to Cauchy-Schwarz inequality and (ii) is from the assumption that E pv [ v 2 ] &lt; ∞ and s(x; θ)p d (x) vanishes at infinity. Now returning to (17), we have</p><formula xml:id="formula_43">−E pv D i=1 (v ∇ x log p m (x; θ))v i ∂p d (x) ∂x i dx = E pv D i=1 v i p d (x)v ∂s m (x; θ) ∂x i dx = E pv p d (x)v ∇ x s m (x; θ)vdx,</formula><p>which proves ( <ref type="formula" target="#formula_40">16</ref>) and the proof is completed.</p><p>Lemma 1. Assume our model family is well-specified and identifiable (Assumption 4). Assume further that the densities are all positive (Assumption 5). When p v satisfies some regularity conditions (Assumption 2), we have</p><formula xml:id="formula_44">L(θ; p v ) = 0 ⇔ θ = θ * . Proof. First, since p d (x) = p m (x; θ * ) &gt; 0, L(θ; p v ) = 0 implies 1 2 E pv (v (s m (x; θ) − s d (x))) 2 = 0 ⇔ E pv v (s m (x; θ) − s d (x))(s m (x; θ) − s d (x)) v = 0 ⇔ (s m (x; θ) − s d (x)) E pv [vv ](s m (x; θ) − s d (x)) = 0 (i) ⇔ s m (x; θ) − s d (x) = 0 ⇔ log p m (x; θ) = log p d (x) + C</formula><p>where (i) holds because E pv [vv ] is positive definite. Because p m (x; θ) and p d (x) are normalized probability density functions, we have p m (x; θ) = p d (x). The identifiability assumption gives θ = θ * . This concludes the left to right direction of the implication and the converse direction is trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 CONSISTENCY</head><p>In addition to the assumptions in Theorem 1 and Lemma 1, we need the following regularity conditions the consistency of θN,M arg min θ∈Θ Ĵ(θ; x N 1 , v N M 11 ). Assumption 6 (Compactness). The parameter space Θ is compact. Assumption 7 (Lipschitz continuity). Both ∇ x s m (x; θ) and s m (x; θ)s m (x; θ) are Lipschitz continuous in terms of Frobenious norm, i.e., ∀θ</p><formula xml:id="formula_45">1 ∈ Θ, θ 2 ∈ Θ, ∇ x s m (x; θ 1 ) − ∇ x s m (x; θ 2 ) F ≤ L 1 (x) θ 1 − θ 2 2 and s m (x; θ 1 )s m (x; θ 1 ) − s m (x; θ 2 )s m (x; θ 2 ) F ≤ L 2 (x) θ 1 − θ 2 2 . In addition, we require E p d [L 2 1 (x)] &lt; ∞ and E p d [L 2 2 (x)] &lt; ∞. Assumption 8 (Bounded moments of projection vectors). E pv [ vv 2 F ] &lt; ∞.</formula><p>Lemma 2. Suppose s m (x; θ) is sufficiently smooth (Assumption 7) and p v has bounded higher-order moments (Assumption 8). Let f</p><formula xml:id="formula_46">(θ; x, v) v ∇ x s m (x; θ)v + 1 2 (v s m (x; θ)) 2 . Then f (θ; x, v) is Lipschitz continuous with constant L(x, v) and E p d ,pv [L 2 (x, v)] &lt; ∞. Proof. Let A(θ) ∇ x s m (x; θ) and B(θ) s m (x; θ)s m (x; θ) . Consider θ 1 ∈ Θ, θ 2 ∈ Θ and let D be the dimension of v, we have |f (θ 1 ; x, v) − f (θ 2 ; x, v)| = D i=1 D j=1 v i v j (A(θ 1 ) ij − A(θ 2 ) ij ) + 1 2 v i v j (B(θ 1 ) ij − B(θ 2 ) ij ) (i) ≤ D i=1 D j=1 v 2 i v 2 j D i=1 D j=1 (A(θ 1 ) ij − A(θ 2 ) ij ) + 1 2 (B(θ 1 ) ij − B(θ 2 ) ij ) 2 (ii) ≤ D i=1 D j=1 v 2 i v 2 j D i=1 D j=1 2(A(θ 1 ) ij − A(θ 2 ) ij ) 2 + 1 2 (B(θ 1 ) ij − B(θ 2 ) ij ) 2 (iii) ≤ D i=1 D j=1 v 2 i v 2 j 2L 2 1 (x) θ 1 − θ 2 2 2 + 1 2 L 2 2 (x) θ 1 − θ 2 2 2 = D i=1 D j=1 v 2 i v 2 j 2L 2 1 (x) + 1 2 L 2 2 (x) θ 1 − θ 2 2 ,</formula><p>where (i) is Cauchy-Schwarz inequality, (ii) is Jensen's inequality, and (iii) is due to Assumption 7. Now let</p><formula xml:id="formula_47">L(x, v) D i=1 D j=1 v 2 i v 2 j 2L 2 1 (x) + 1 2 L 2 2 (x). Then E p d ,pv [L 2 (x, v)] (i) = E pv D i=1 D j=1 v 2 i v 2 j E p d 2L 2 1 (x) + 1 2 L 2 2 (x) (ii) &lt; ∞,</formula><p>where (i) results from the independence of v, x, and (ii) is due to Assumption 7 and Assumption 8.</p><p>Lemma 3 (Uniform convergence of the expected error). Under Assumption 6-8, we have</p><formula xml:id="formula_48">E pv,p d sup θ∈Θ Ĵ(θ; x N 1 , v N M 11 ) − J(θ; p v ) ≤ O diam(Θ) D N<label>(18)</label></formula><p>where diam(•) denotes the diameter and D is the dimension of Θ.</p><formula xml:id="formula_49">E p d [M 2 ij (x)] &lt; ∞, E p d [N 2 ij (x)] &lt; ∞, ∀i, j.</formula><p>Lemma 4. Suppose l m (x; θ) is sufficiently smooth (Assumption 9) and p v has bounded moments (Assumption 2 and Assumption 8). Let ∇</p><formula xml:id="formula_50">2 θ f (θ; x, v M 1 ) 1 M M i=1 ∇ 2 θ v i ∇ x s m (x; θ)v i + 1 2 ∇ 2 θ (v i s m (x; θ)) 2 . Then ∇ 2 θ f (θ;</formula><p>x, v) is Lipschitz continuous, i.e., for θ 1 and θ 2 close to θ * , there exists a Lipschitz constant L(x, v M 1 ) such that</p><formula xml:id="formula_51">∇ 2 θ f (θ 1 ; x, v M 1 ) − ∇ 2 θ f (θ 2 ; x, v M 1 ) F ≤ L(x, v M 1 ) θ 1 − θ 2 2 , and E p d ,pv [L 2 (x, v M 1 )] &lt; ∞. Proof. First, we write out ∇ 2 θ f (θ 1 ; x, v M 1 ) − ∇ 2 θ f (θ 2 ; x, v M 1 ) according to the definitions. Let A ij (θ) ∇ 2 θ ∂ i ∂ j l m (x; θ) and B ij (θ) ∇ 2 θ ∂ i l m (x; θ)∂ j l m (x; θ). Then, ∇ 2 θ f (θ 1 ; x, v M 1 ) − ∇ 2 θ f (θ 2 ; x, v M 1 ) = 1 M i,j,k v k,i v k,j A ij (θ 1 ) − A ij (θ 2 ) + 1 2 (B ij (θ 1 ) − B ij (θ 2 )) .</formula><p>Then, Cauchy-Schwarz and Jensen's inequality give</p><formula xml:id="formula_52">∇ 2 θ f (θ 1 ; x, v M 1 ) − ∇ 2 θ f (θ 2 ; x, v M 1 ) 2 F = l,m 1 M i,j,k v k,i v k,j A ij (θ 1 ) lm − A ij (θ 2 ) lm + 1 2 (B ij (θ 1 ) lm − B ij (θ 2 )) lm 2 ≤ l,m     1 M i,j k v k,i v k,j 2 • i,j A ij (θ 1 ) lm − A ij (θ 2 ) lm + 1 2 (B ij (θ 1 ) lm − B ij (θ 2 )) lm 2     2 ≤ l,m 1 M 2 i,j k v k,i v k,j 2 i,j 2 A ij (θ 1 ) lm − A ij (θ 2 ) lm 2 + 1 2 (B ij (θ 1 ) lm − B ij (θ 2 )) lm 2 = 1 M 2 i,j k v k,i v k,j 2 i,j l,m 2 A ij (θ 1 ) lm − A ij (θ 2 ) lm 2 + l,m 1 2 (B ij ) lm − B ij (θ 2 )) lm 2 = 1 M 2</formula><p>i,j,p,q v p,i v p,j v q,i v q,j i,j</p><formula xml:id="formula_53">2 A ij (θ 1 ) − A ij (θ 2 ) 2 F + 1 2 V ij (θ 1 ) − V ij (θ 2 ) 2 F ≤ 1 M 2</formula><p>i,j,p,q v p,i v p,j v q,i v q,j i,j</p><formula xml:id="formula_54">2M 2 ij + 1 2 N 2 ij L 2 (x,v M 1 ) θ 1 − θ 2 2 2</formula><p>Next, we bound the expectation</p><formula xml:id="formula_55">E p d ,pv [L 2 (x, v M 1 )] = 1 M 2 E pv i,j,p,q v p,i v p,j v q,i v q,j E p d ij 2M 2 ij (x) + 1 2 N 2 ij (x) (i)</formula><p>≤ O(1)E pv i,j,p,q v p,i v p,j v q,i v q,j</p><formula xml:id="formula_56">= O(1) ij M (M − 1)E pv [v i v j ] 2 + M E pv [(v i v j ) 2 ] = O(1) M (M − 1) E pv [vv ] 2 F + M E pv [ vv 2 F (ii) ≤ O(1) M (M − 1)E pv [ vv 2 F ] + M E pv [ vv 2 F (iii) &lt; ∞,</formula><p>where (i) is due to Assumption 9, (ii) is Jensen's, and (iii) is because of Assumption 2 and 8.</p><p>Lemma 5. Assume that conditions in Theorem 1 and Lemma 1 hold, and p v has bounded higher-order moments (Assumption 8). Then</p><p>Var p d ,pv ∇ θ f (θ * ; x, v M 1 ) = i,j,p,q</p><formula xml:id="formula_57">1 − 1 M Σ ij Σ pq + 1 M S ijpq V ijpq ,<label>(23)</label></formula><p>where ∇ θ f (θ * ; x, v M 1 ) = ∇ θ f (θ; x, v M 1 ) θ=θ * In particular, if p v ∼ N (0, I), we have</p><formula xml:id="formula_58">Var p d ,pv ∇ θ f (θ * ; x, v M 1 ) = ij V ij + 2 M i V ii + 2 M i =j W ij .</formula><p>If p v is the distribution of multivariate Rademacher random variables, we have</p><formula xml:id="formula_59">Var p d ,pv ∇ θ f (θ * ; x, v M 1 ) = ij V ij + 2 M i =j W ij .</formula><p>Proof. Since θ * is the true parameter of the data distribution, we have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL DETAILS OF EXPERIMENTS</head><p>C.1 KERNEL EXPONENTIAL FAMILIES Model. The kernel exponential family is a class of densities with unnormalized log density given by log pf (x) = f (x) + log q 0 (x). Here, q 0 is a fixed function and f belongs to a reproducing kernel Hilbert space H, with kernel k <ref type="bibr" target="#b2">(Canu &amp; Smola, 2006;</ref><ref type="bibr" target="#b26">Sriperumbudur et al., 2017)</ref>. We see this is a member of the exponential family by using reproducing property, f (x) = f, k(x, •) H ). Rewriting the density, the model has natural parameter f and sufficient statistic k(x, •): pf (x) = exp(f (x))q 0 (x) = exp( f, k(x, •) H ))q 0 (x)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SM loss after training DKEF models on UCI datasets with different loss functions; lower is better. Results for approximate backprapogation are not shown because losses were larger than 10 9 .</figDesc><graphic url="image-1.png" coords="7,94.16,72.00,421.20,95.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SM performance degrades linearly with the data dimension, while efficient approaches have relatively similar performance.</figDesc><graphic url="image-2.png" coords="7,93.25,214.49,180.00,135.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Assumption 1 (</head><label>1</label><figDesc>Regularity of score functions). The model score function s m (x) and data score function s d (x) are both differentiable. They additionally satisfy Ep d [ s m (x) 2 2 ] &lt; ∞ and E p d [ s d (x) 2 2 ] &lt; ∞. Assumption 2 (Regularity of projection vectors). The projection vectors satisfy E pv [ v 2 2 ] &lt; ∞, and E pv [vv ] 0. Assumption 3 (Boundary conditions). ∀θ ∈ Θ, lim x →∞ s m (x; θ)p d (x) = 0.Assumption 4 (Identifiability). The model family {p m (x; θ) | θ ∈ Θ} is well-specified, i.e., p d (x) = p m (x; θ * ). Furthermore, p m (x; θ) = p m (x; θ * ) whenever θ = θ * .Assumption 5 (Positiveness). p m (x; θ) &gt; 0, ∀θ ∈ Θ and ∀x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>EJJ</head><label></label><figDesc>p d ,pv [∇ θ f (θ * ; x, v M 1 )] = ∇ θ E p d ,pv [f (θ * ; x, v M 1 )] = ∇ θ J(θ * ; p v ) = 0.Therefore, (23) can be expanded asVar p d ,pv ∇ θ f (θ * ; x, v M 1 ) =E p d ,pv ∇ θ f (θ * ; x, v M 1 )∇ θ f (θ * ; x, v M 1 NCE (θ) −E p d [log h(x; θ)] − E pn [log(1 − h(x; θ))]whereh(x; θ) p m (x; θ) p m + p m (x − v; θ) p n (x) = p d (x + v).Then when v 2 → 0, we haveJ NCE (θ) = 2 log 2 + 1 4 E p d v ∇ 2 log p m (x; θ)v + 1 2 (∇ log p m (x; θ) v) 2 + o( v 2 2 )Proof. Using Taylor expansion, we can immediately getlog p m (x + v; θ) = log p m (x; θ) + ∇ log p m (x; θ) v + 1 2 v ∇ 2 log p m (x; θ)v + o( v 2 2 ).Next, observe thatlog(p m (x; θ) + p m (x + v; θ)) = log p m (x; θ) + log (1 + exp{log p m (x + v; θ) − log p m (x; θ)}) = log p m (x; θ) + log 1 + exp ∇ log p m (x; θ) v + 1 2 v ∇ 2 log p m (x; θ)v + o( p m (x; θ) v) 2 + o( m (x; θ) + p m (x − v; θ)) = log p m (x; θ) + log 2 + 1 2 −∇ log p m (x; θ) v + 1 2 v ∇ 2 log p m (x; θ)v + 1 8 (∇ log p m (x; θ) v) 2 + o( NCE (θ) = −E p d [log h(x; θ) + log(1 − h(x + v; θ))] = −E p d [log p m (x; θ) − log(p m (x; θ) + p m (x − v; θ))] − E p d [log p m (x; θ) − log(p m (x; θ) + p m (x + v; θ))] p m (x; θ) v) 2 + o(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Test SM Loss Test LL</cell></row><row><cell>MLE</cell><cell>-579</cell><cell>-791</cell></row><row><cell>SSM-VR</cell><cell>-8054</cell><cell>-3355</cell></row><row><cell>SSM</cell><cell>-2428</cell><cell>-2039</cell></row><row><cell>DSM (σ = 0.10)</cell><cell>-3035</cell><cell>-4363</cell></row><row><cell>DSM (σ = 1.74)</cell><cell>-97</cell><cell>-8082</cell></row><row><cell>CP</cell><cell>-1694</cell><cell>-1517</cell></row><row><cell>Approx BP</cell><cell>-48</cell><cell>-2288</cell></row></table><note>Score matching losses and log-likelihoods for NICE models on MNIST. σ = 0.1 is by grid search and σ = 1.74 is from the heuristic of Saremi et al. (2018). et al. (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Negative log-likelihoods on MNIST, estimated by AIS. † The result of SSM with M = 100, in which case SSM matches the computational cost of kernel methods, which used 100 samples for each data point.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>VAE</cell><cell cols="2">WAE</cell></row><row><cell cols="2">Latent Dim</cell><cell>8</cell><cell>32</cell><cell>8</cell><cell>32</cell></row><row><cell>ELBO</cell><cell></cell><cell>96.87</cell><cell>89.06</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>SSM</cell><cell></cell><cell cols="4">95.50 89.25 (88.29  † ) 98.24 90.37</cell></row><row><cell>Stein</cell><cell></cell><cell>96.71</cell><cell>91.84</cell><cell cols="2">99.05 91.70</cell></row><row><cell cols="2">Spectral</cell><cell>96.60</cell><cell>94.67</cell><cell cols="2">98.81 92.55</cell></row><row><cell cols="2">Method</cell><cell>Iteration</cell><cell>10k</cell><cell>40k</cell><cell>70k</cell><cell>100k</cell></row><row><cell cols="3">ELBO</cell><cell>96.20</cell><cell>73.70</cell><cell>69.42</cell><cell>66.32</cell></row><row><cell></cell><cell cols="2">SSM</cell><cell cols="2">108.52 70.28</cell><cell>66.52</cell><cell>62.50</cell></row><row><cell>VAE</cell><cell cols="2">Stein</cell><cell cols="4">126.60 118.87 120.51 126.76</cell></row><row><cell cols="3">Spectral</cell><cell cols="4">131.90 125.04 128.36 133.93</cell></row><row><cell></cell><cell cols="2">SSM</cell><cell>84.11</cell><cell>61.09</cell><cell>56.23</cell><cell>54.33</cell></row><row><cell>WAE</cell><cell cols="2">Stein</cell><cell>82.93</cell><cell>63.46</cell><cell>58.53</cell><cell>57.61</cell></row><row><cell cols="3">Spectral</cell><cell>82.30</cell><cell>62.47</cell><cell>58.03</cell><cell>55.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>FID scores of different methods versus number of training iterations on CelebA dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">, and we can optimize the objective to get θ. Afterwards, h(x; θ) can be used as an approximation to ∇ x log q(x). 1 1 Note that h(x; θ) may not correspond to the gradient of</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by Intel Corporation, TRI, NSF (#1651565, #1522054, #1733686 ), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Samples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOFS B.1 NOTATIONS</head><p>Below we provide a summary of the most commonly used notations used in the proofs. First, the data distribution as p d (x) and assume that the training/test data {x 1 , x 2 , • • • , x N } are i.i.d. samples of p d (x). The model is denoted as p m (x; θ), where θ is restricted to a parameter space Θ. Note that p m (x; v) can be an unnormalized energy-based model. We use v to represent a random vector with the same dimension of input x. This vector v is often called the projection vector, and we use p v to denote its distribution.</p><p>Next, we introduce several shorthand notations for quantities related to p m (x; θ) and p d (x). The log-likelihood log p m (x; θ) and log p d (x) are respectively denoted as l m (x; θ) and l d (x). The <ref type="bibr">(Stein)</ref> score function ∇ x log p m (x; θ) and ∇ x log p d (x) are written as s m (x; θ) and s d (x), and finally the Hessian of log p m (x; θ) w.r.t. x is denoted as ∇ x s m (x; θ).</p><p>We also adopt some convenient notations for collections.</p><p>In particular, we use x N 1 to denote a collection of N vectors {x 1 , x 2 , • • • , x N } and use v N M Proof. The proof consists of 3 steps. First, we use the symmetrization trick to get rid of the term J(θ; p v ) = E pv,p d [ Ĵ(θ; x N 1 , v N M 11 )]. Second, we use chaining to get an upper bound that involves integration of the metric entropy. Finally, we upper bound the metric entropy to obtain the uniform convergence bound.</p><p>Step 1: From Jensen's inequality, we obtain</p><p>11 are independent copies of x N 1 and v N M 11 . Let { i } N i=1 be a set of independent Rademacher random variables, we have</p><p>where (i) is because the quantity is symmetric about 0 in distribution, and (ii) is due to Jensen's inequality.</p><p>Step 2: First note that given</p><p>) is a zero-mean sub-Gaussian process w.r.t. θ. This can be observed from</p><p>where (i) holds because i is a 1-sub-Gaussian random variable, (ii) is from Cauchy-Schwarz inequality and (iii) is due to Lemma 2. As a result,</p><p>Since Θ is compact, the diameter of Θ with respect to the Euclidean norm • 2 is finite and we denote it as diam(Θ) &lt; ∞. Then, Dudley's entropy integral <ref type="bibr" target="#b4">(Dudley, 1967)</ref> gives</p><p>Here</p><p>Therefore, N (Θ, d, ) can be bounded by</p><p>Hence, the metric integral can be bounded</p><p>Finally <ref type="bibr">, combining (19), (20) and (21)</ref> gives us</p><p>where (i) is due to Jensen's inequality and (ii) results from E[L 2 (x, v)] &lt; ∞, and the compactness of Θ guarantees that the bound is 2. Suppose all the previous assumptions hold (Assumption 1-Assumption 8). Assume further the conditions of Theorem 1 and Lemma 1 are satisfied. Let θ * be the true parameter of the data distribution, and θN,M be the empirical estimator defined by</p><p>Proof. Note that Theorem 1 and Lemma 1 together imply that θ * = arg min θ∈Θ J(θ; p v ). Then, we will show</p><p>This can be done by noticing</p><p>We can easily conclude that ( <ref type="formula">22</ref>) is o p (1) with the help of Lemma 3, because</p><p>As shown by Theorem 1, this is the same as J( θN,M ;</p><p>Next, we show θN,M p → θ * . This can be inferred from J( θN,M ;</p><p>However, the fact that p(|J( θN,M ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ASYMPTOTIC NORMALITY</head><p>Notations. To simplify notations we use If p v ∼ N (0, I), S and Σ have the following simpler forms</p><p>Then, if we assume that the second derivatives of l m are continuous, we have</p><p>, and the variance ( <ref type="formula">23</ref>) can also be simplified to</p><p>Similarly, if p v ∼ U({±1} D ), (23) has the simplified form</p><p>Theorem 3. With the notations and assumptions in Lemma 4, Lemma 5 and Theorem 2, we have</p><p>In particular, if p v ∼ N (0, I), then the asymptotic variance is</p><p>.</p><p>If p v is the distribution of multivariate Rademacher random variables, the asymptotic variance is</p><p>Proof. To simplify notations, we use P N h(x)</p><p>, where h(x, •) is some arbitrary function. For example, Ĵ(θ; x N 1 , v N M 11 ) can be written as P N f (θ; x, v M 1 ). By Taylor expansion, we can approximate</p><p>where</p><p>2 from Lemma 4 and Taylor expansion of vector-valued functions. Combining with the law of large numbers, we have</p><p>,</p><p>) and the consistency of θN,M (Theorem 2). Now returning to (24), we get</p><p>But of course, the central limit theorem and Lemma 5 yield</p><p>Then, Slutsky's theorem gives the desired result</p><p>.</p><p>In particular, if p v ∼ N (0, I) or p v ∼ U({±1} D ), we have J(θ * ; p v ) = J(θ * ), and therefore ∇ 2 θ J(θ * ; p v ) = ∇ 2 θ J(θ * ). We can apply Lemma 5 to conclude the simplified expressions for the asymptotic variance.</p><p>Corollary 1 (Consistency and asymptotic normality of score matching). Under similar assumptions used in Theorem 2 and Theorem 3, we can also conclude that the score matching estimator θN arg min θ∈Θ Ĵ(θ; x) is consistent</p><p>and asymptotically normal</p><p>To improve computational cost of learning f, Sutherland et al. ( <ref type="formula">2018</ref>) use a Nyström-type lite approximation, selecting L inducing points z l , and f of the form:</p><p>We compare training using our objective against deep kernel exponential family (DKEF) models trained using exact score matching in <ref type="bibr" target="#b35">Wenliang et al. (2019)</ref>.</p><p>The kernel k(x, y) is a mixture of R = 3 Gaussian kernels, with features extracted by a neural network, φ wr (•), length scales σ r , and nonnegative mixture coefficients ρ r . The neural network is a three layer fully connected network with a skip connection from the input to output layers and softplus nonlinearities. Each hidden layer has 30 neurons. We have:</p><p>When training DKEF models, <ref type="bibr" target="#b35">Wenliang et al. (2019)</ref> note that it is possible to analytically minimize the score matching loss over α because the objective is quadratic in α. As a result, models are trained in a two step procedure: α is analytically minimized over a training minibatch, then the loss is computed over a validation minibatch. When training models, the analytically minimized α is treated as function of the other parameters. By doing this, <ref type="bibr" target="#b35">Wenliang et al. (2019)</ref> can also directly optimize the coefficient λ α of a 2 regularization loss on α. This regularization coefficient is initalized to 0.01, and is trained. (More details about the two-step optimization procedure can be found in <ref type="bibr" target="#b35">Wenliang et al. (2019)</ref>, which also includes a finalization stage for α).</p><p>A similar closed form for sliced score matching, denoising score matching, approximate backpropogation, and curvature propagation can be derived. The derivation for sliced score matching is presented below for completeness.</p><p>Proposition 2. Consider the loss</p><p>where</p><p>For fixed k, z, and λ α , as long as λ α &gt; 0 then the optimal α is</p><p>Proof. The derivation follows very similarly to Proposition 3 in <ref type="bibr" target="#b35">Wenliang et al. (2019)</ref>. We will show that the loss is quadratic in α. Note that</p><p>Thus the overall optimization problem is</p><p>Because λ α &gt; 0 and G is positive semidefinite, the matrix in parentheses is strictly positive definite, and the claimed result follows directly from standard vector calculus.</p><p>Hyperparameters. RedWine and WhiteWine are dequantized by adding uniform noise to each dimension in the range <ref type="bibr">[−d, d]</ref> where d is the median distance between two values for that dimension. For each dataset, 10% of the entire data was used as testing, and 10% of the remaining was used for validation. PCA whitening is applied to the data. Noise of standard deviation 0.05 is added as part of preprocessing.</p><p>The DKEF models have R = 3 Gaussian kernels. Each feature extractor is a 3-layer neural network with a skip connection from the input to output, with 30 hidden neurons per layer. Weights were initialized from a Gaussian distribution with standard deviation equal to 1 √ 30 . Length scales σ r were initialized to 1.0, 3.3 and 10.0. We use L = 200 trainable inducing points, which were initialized from training data. Models are trained using an Adam optimizer, with learning rate 10 −2 . A batch size of 200 is used, with 100 points for computing α, and 100 for computing the loss. Models are stopped after validation loss does not improve for 200 steps.</p><p>For denoising score matching, we perform a grid search with values [0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16, 0.20, 0.24, 0.28, 0.32, 0.40, 0.48, 0.56, 0.64, 1.28]. We train models for each value of σ using two random seeds, and pick the σ with the best average validation score matching loss. For curvature propagation, one noise sample is used to match the performance of sliced score matching.</p><p>Log-likelihoods. Log-likelihoods are presented below. They are estimated using AIS, using a proposal distribution N (0, 2I), using 1,000,000 samples.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 NICE</head><p>Hyperparameters and Model Architecture. The model has four coupling layers, each with five hidden layers, for a total of 20 hidden layers, as well as a final scale layer <ref type="bibr" target="#b3">(Dinh et al., 2015)</ref>. Softplus nonlinearities are used between hidden layers.</p><p>Models are trained using the Adam optimizer with learning rate 10 −3 for 100 epochs. The best checkpoint on exact score matching loss, evaluation every 100 iterations, is used to report test set performance. We use a batch size of 128.</p><p>Data are dequantized by adding uniform noise in the range [− 1 512 , 1 512 ], clipped to be in the range [−0.001, 0.001], and then transformed using a logit transformation log(x) − log(1 − x). 90% of the training set is used for training, and 10% for validation, and the standard test set is used.</p><p>For grid search for the optimal value of σ, eight values are used: <ref type="bibr">[0.01, 0.05, 0.10, 0.20, 0.28, 0.50, 1.00, 1.50]</ref>. We also evaluate σ = 1.74, chosen by the heuristic in <ref type="bibr" target="#b23">Saremi et al. (2018)</ref>. The model with the best performance on validation score matching loss is used. Only nine values of σ are evaluated because training each model takes approximately two hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 SCORE ESTIMATION FOR IMPLICIT DISTRIBUTIONS</head><p>Architectures. We put the architectures of all networks used in the MNIST and CelebA experiments in Tab. 8 and Tab. 9 respectively.</p><p>Training. For MNIST experiments, we use RMSProp optimizer with a learning rate of 0.001 for all methods. On CelebA, the learning rate is changed to 0.0001. All algorithms are trained for 100000 iterations with a batch size of 128.</p><p>Samples. All samples are generated after 100000 training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D VARIANCE REDUCTION</head><p>Below we discuss approaches to reduce the variance of Ĵ(θ; x N 1 , v N M 11 ), which can lead to better performance in practice. The most naïve approach, of course, is using a larger M to compute Ĵ(θ; x N 1 , v N M 11 ). However, this requires more computation and when M is close to the data dimension, sliced score matching will lose its computational advantage over score matching.</p><p>An alternative approach is to leverage control variates <ref type="bibr" target="#b21">(Owen, 2013)</ref>. A control variate is a random variable whose expectation is tractable, and is highly correlated with another random variable without a tractable expectation. Define which is easily computable. Now let β(x)c(θ; x, v) be our control variate, where β(x) is a function to be determined. Due to the structural similarity between β(x)c(θ; x, v) and Ĵ(θ; x N 1 , v N M 11 ), it is easy to believe that β(x)c(θ; x, v) can be a correlated control variate with an appropriate β(x). We thus consider the following objective</p><p>Note that E[ Ĵvr (θ; x N 1 , v N M 11 )] = J(θ; p v ). The theory of control variates guarantees the existence of β(x) that can reduce the variance. In practice, there can be many heuristics of choosing β(x), and we found that β(x) ≡ 1 can often be a good choice in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E PSEUDOCODE</head><p>Algorithm 2 Score Matching Input: pm (•; θ), x 1: s m (x; θ) ← grad(log pm (x; θ), x) 2: J ← 1 2 s m (x; θ) J ← J + (∇ x s m (x; θ)) d 6: end for return J</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">P</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soumith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zeming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kernel methods and the exponential family</title>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Issues in Neurocomputing: 13th European Symposium on Artificial Neural Networks</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="714" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NICE: Nonlinear independent components estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Learning Representations Workshop Track</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The sizes of compact subsets of Hilbert space and continuity of Gaussian processes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Dudley</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="290" to="330" />
			<date type="published" when="1967">2017. 1967</date>
		</imprint>
	</monogr>
	<note>UCI machine learning repository</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring sample quality with kernels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gorham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mackey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1292" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bregman divergence as general framework to estimate unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-I</forename><surname>Hirayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08235</idno>
		<title level="m">Variational inference using implicit distributions</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="450" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regularized estimation of image statistics by score matching</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1126" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting structured data</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient estimators for implicit models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A kernelized Stein discrepancy for goodness-of-fit tests</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
				<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
				<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating the Hessian by back-propagating curvature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
				<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Monte Carlo theory, methods and examples</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wasserstein barycenter and its application to texture mixing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scale Space and Variational Methods in Computer Vision</title>
				<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Ter Haar Romeny</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08306</idno>
		<title level="m">Deep energy estimator networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clustering via mode seeking by direct estimation of the gradient of a log-density</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A spectral approach to gradient estimation for implicit distributions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4651" to="4660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Density estimation in infinite dimensional exponential families</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">57</biblScope>
			<biblScope unit="page" from="1" to="59" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Estimation of the mean of a multivariate normal distribution. The annals of Statistics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="1135" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-free Hamiltonian monte carlo with efficient kernel exponential families</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Szabo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="955" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Functional variational Bayesian neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient and principled score estimation with Nyström kernel exponential families</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Twenty-First International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wasserstein auto-encoders</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5523" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Van Der Vaart</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511802256</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep kernels for exponential family densities</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wenliang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/wenliang19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6737" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
