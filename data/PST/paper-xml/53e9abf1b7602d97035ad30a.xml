<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast saliency-aware multi-modality image fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-02-04">4 February 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungonghan77@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Civolution Technology</orgName>
								<address>
									<addrLine>High Tech Campus 9</addrLine>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Pauwels</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centrum Wiskunde &amp; Informatica (CWI)</orgName>
								<address>
									<addrLine>Science Park 123</addrLine>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>De Zeeuw</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centrum Wiskunde &amp; Informatica (CWI)</orgName>
								<address>
									<addrLine>Science Park 123</addrLine>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast saliency-aware multi-modality image fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-02-04">4 February 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">528F8FED56247794F911E841260ECDEC</idno>
					<idno type="DOI">10.1016/j.neucom.2012.12.015</idno>
					<note type="submission">Received 12 August 2012 Received in revised form 21 November 2012 Accepted 18 December 2012 Communicated by Pingkun Yan</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Saliency detection Multi-modality fusion Co-occurrence Markov random fields (MRFs)</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a saliency-aware fusion algorithm for integrating infrared (IR) and visible light (ViS) images (or videos) with the aim to enhance the visualization of the latter. Our algorithm involves saliency detection followed by a biased fusion. The goal of the saliency detection is to generate a saliency map for the IR image, highlighting the co-occurrence of high brightness values (''hot spots'') and motion. Markov Random Fields (MRFs) are used to combine these two sources of information. The subsequent fusion step is employed to bias the end result in favor of the ViS image, except when a region shows clear IR saliency, in which case the IR image gains (local) dominance. By doing so, the fused image succeeds in depicting both the salient foreground object (gleaned from the IR image), against as an easily recognizable background as supplied by the ViS image. An evaluation of the proposed saliency detection method indicates improvements in detection accuracy when compared to state-of-the-art alternatives. Moreover, both objective and subjective assessments reveal the effectiveness of the proposed fusion algorithm in terms of visual context enhancement. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in imaging, networking, data processing and storage technology have resulted in tremendous expansion of the use of multi-modality image/video in a variety of fields. A typical application is surveillance imaging where people usually combine the advantages of different imaging sensors in order to enhance the capability of vision systems. At the core of such an application is multi-modality image fusion, which enables to combine multiple images captured by different modalities into a single representation. This single fused image provides comprehensive information about the scene such that the operator does not need to check each image separately. This is nicely illustrated in the case of fire monitoring based on the combination of IR and ViS images, where the system is expected to locate the fire at an early stage. While a ViS image may allow the operator to readily spot a billowing smoke plume, the actual location of the fire is more easily deduced by inspecting the corresponding hot spot in the IR image. If we combine two images properly, one can see both bright spot and smoke in the fused image, enabling the operator to quickly and precisely locate the fire.</p><p>Image fusion has been studied extensively <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Depending on the intended application, different fusion methods have been developed, but two basic research lines have gained prominence: pixel-based and region-based fusion. Pixel-based image fusion combines the images at the pixel level while region-based image fusion considers pixels constituting the same object as an entity. From the perceptual point of view, region-based fusion is often superior since meaningful objects always attract more attention than incoherent individual pixels. However, the usage of the region-based fusion is not straightforward due to several problems. Firstly, an assumption underlying region-based fusion is that segmented images from multiple modalities are similar in terms of region location and size. Unfortunately, this does not hold when two modalities are significantly different from each other, as is the case for long wavelength IR and ViS image. Secondly, image segmentation is computationally expensive so that the fusion based on it is not suited for applications, such as surveillance imaging, where real-time processing is paramount importance. Thirdly, the region-based fusion treats each segmented region on the same footing, irrespective of the region's saliency. For many applications, only a select few regions bear significance.</p><p>In this paper, an IR and ViS image/video fusion algorithm is proposed to enhance the visualization of a surveillance imaging system. The core idea is to take the saliency of the region into account during the fusion procedure. Our work differs from the existing work in three aspects. impression of the scene, thus reducing the cognitive load for the supervisor who has to recognize or locate the target object.</p><p>Instead of partitioning the image into seamless regions, we only extract regions that are salient in terms of the intended application. In this paper, we perform saliency detection on the IR image, as the thermography from the IR image can ''see the object'' without illumination.</p><p>We consider the saliency detection as a classification problem in which a Markov Random Field (MRF) is called upon to harness the co-occurrence of hot spots and motion. This model helps to generate a better saliency map, as consistency of neighboring pixels is adequately taken into consideration <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this context, we designate a region as salient if it consistently tends to attract attention from viewers. Obviously, it is difficult to design a generic saliency extraction algorithm that can be applied to a multitude of applications. However, we believe that it is possible to define saliency in operational terms, once the context of an application provided. Since in our application (wildfire surveillance and monitoring) regions of interesteither fire or humanstend to be hotter and moving when compared to the background, we define saliency in terms of high IR brightness and motion.</p><p>In the sequel, we first overview the literature in Section 2. In Section 3, we present our framework. We further introduce the idea of MRF-based saliency detection on IR image in Section 4. In Section 5, we describe our MR-based (wavelet) biased fusion algorithm. The experimental results are provided in Section 6. Finally, conclusions are drawn in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Prior work on image fusion</head><p>Several related survey papers <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> for image fusion have appeared over the years, providing a broad overview of over one hundred papers. In keeping with most of the literature, we divide existing techniques into two categories: pixel-based fusion and region-based fusion.</p><p>Pixel-based image fusion algorithm <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> is to combine images at the pixel level. The fusion schemes range from simple spatial pixel-value fusion to more complex transform fusion. The simplest form of the spatial pixel-value fusion is the weighted averaging of the registered input images, where the weight for each source image can be settled based on prior knowledge available for application at hand. In general, this type of method is applied to a single scale only (original image), so that the complexity of the algorithm is kept low. Through averaging, features from different source images are presented in the fused image. However, the contrast of the original features may be significantly reduced, especially in the case of fine-grained details.</p><p>Transform-based fusion methods first convert each input image into a more convenient representation by applying a multiresolution (MR) transform. Subsequently, fusion is carried out on the transformed images, where different rules are applied to different scales (resolutions). Finally, the fused image is obtained by performing the inverse transform process. The usage of an MR transform is motivated by the observation that the human visual system is primarily sensitive to local contrast changes (edges), and MR transform enables a perfect spatial-scale localization of those local changes. Well-known MR transform techniques that appear in a variety of image fusion approaches include pyramid transform, wavelet transform, contourlet transform <ref type="bibr" target="#b5">[6]</ref> and the multigrid transform <ref type="bibr" target="#b6">[7]</ref>. Since the pyramid transform and the wavelet transform are the more popular ones, we briefly discuss them here. For more details about MR-based image fusion algorithms, we refer to the overview paper <ref type="bibr" target="#b7">[8]</ref>.</p><p>A pyramid transform constructs image pyramids by successively filtering and down sampling the image. A variety of pyramid transforms exist, such as Laplacian pyramids <ref type="bibr" target="#b8">[9]</ref>, steerable pyramids <ref type="bibr" target="#b9">[10]</ref> and gradient pyramids <ref type="bibr" target="#b10">[11]</ref>. A common wavelet transform used for image fusion is the discrete wavelet transform (DWT) <ref type="bibr" target="#b11">[12]</ref> where two different fusion rules are used for approximation image and detail image, respectively. More specifically, for the approximation coefficient, it averages the coefficients of all input images. For detail coefficients however, the one that is largest in absolute value is selected (the so-called maximum selection rule). In general, it is difficult to determine which transform is better, because different schemes carry their own individual advantages and disadvantages. The DWTs (provided filters are properly chosen) have the advantage of perfect reconstruction. The Laplacian transform (also boasting perfect reconstruction) may be less edge-aware than the DWT, but the latter may show ringing effects in regions of high contrast. Gradient pyramids appear a safe bet but cannot boast perfect reconstruction. Which scheme is preferable will depend on the class of input images and the application-specific requirements imposed by the user.</p><p>Region-based image fusion refers to a fusion procedure that considers the pixels comprising a single object as an entity instead of combining the pixel values without reference to the object they belong to. In a region-based image fusion algorithm, the image is initially segmented into a set of regions. Various properties of these regions can be calculated and used to determine which features from which images are included in the fused image. It has advantages over pixel-based methods: less sensitive to noise, better contrast, and less affected by registration errors. Many region-level image fusion techniques are available in literature <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. Most publications focus on improving the algorithm either by means of a better region partition algorithm suited for multi-modality images, or by an intelligent decision rule based on the statistics of the region. For example, Zhang et al. <ref type="bibr" target="#b12">[13]</ref> combines Canny edge detection and region growing method to segment the image. The segmentation is only applied to the low-low band of the coefficient obtained from the wavelet transform. A gray-level clustering using a pyramid linking method is used for segmentation in <ref type="bibr" target="#b13">[14]</ref>. The regions are then fused based on a simple region property such as average activity. In <ref type="bibr" target="#b14">[15]</ref>, the authors produce a joint-segmentation map out of the input images. To determine the weight of each region in the fusion process, the algorithm measures priority using energy, variance, or entropy of the wavelet coefficients. In <ref type="bibr" target="#b15">[16]</ref>, instead of producing segmentation map for each input image, authors propose to generate a single segmentation map by using the correlation among images. This sort of joint segmentation can definitely improve the accuracy of the segmentation when treating multimodality images. Wan et al. <ref type="bibr" target="#b16">[17]</ref> propose a new region merging algorithm based on symmetric alpha-stable distribution. An intelligent decision method is introduced by considering the local statistical properties within the regions, which significantly improves the reliability of the fusion process. In a recent publication <ref type="bibr" target="#b17">[18]</ref>, graphbased normalized cut technique is utilized for the segmentation. Moreover, a new fusion rule, using mean max standard deviation, helps to guide the fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Prior work on saliency detection</head><p>Detecting saliency for a given image/video has been an active research topic for quite a long time. A pioneering work, called Itti model <ref type="bibr" target="#b19">[20]</ref>, attempts to develop a neuromorphic model that simulates which elements of a visual scene are likely to attract the attention of human observers. Basically, this model decomposes the input image into a set of multiscale ''feature maps'' which extract local spatial discontinuities of color, intensity and orientation.</p><p>All feature maps are then combined into a unique scalar ''saliency map'' which encodes for the saliency of a location in the scene. This model and its follow-up algorithms <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> have been successfully applied to various applications including video retrieval and image segmentation. Most work has been dedicated to saliency detection in ViS images, far less work has been devoted to saliency detection in IR images.</p><p>Most publications start from the assumption that bright regions in the IR spectrum (''hot spots'') are of particular interest. This assumption is mostly well-grounded as in many applications one is looking for humans or vehicles which tend to be significantly hotter than their surroundings. In <ref type="bibr" target="#b20">[21]</ref>, a simple intensity threshold followed by a probabilistic template is used to detect the hot spot. A similar approach is reported in <ref type="bibr" target="#b21">[22]</ref>, in which the hot spot detection is performed by a support vector machine with size-normalized pedestrian candidates. Generally speaking, most of algorithms can successfully detect the hot spot from the IR image. However, some regions marked as a hot spot, e.g., sunlight or glass, may not be meaningful considering the context of the application. In more recent publications, researchers point out that moving objects are likely the regionsof-interest (ROI) for most IR surveillance imaging systems. Therefore, the region incorporating moving pixels should be recognized as the salient region. To detect it, Benezeth et al. <ref type="bibr" target="#b22">[23]</ref> exploit the background subtraction method to extract the object candidates from the background model. Afterwards, the hypothesis that the human temperature is noticeably higher than the average of the environment helps to recognize which object among candidates corresponds to a human. In <ref type="bibr" target="#b23">[24]</ref>, the authors first use statistical background subtraction method in the thermal domain to identify ROIs. In order to obtain accurate object boundaries, the paper has designed tailor-made algorithms to remove the ''halo'' that appears around very hot and cold objects on the thermal image. Hot spot detection in IR. The top 5% pixels in terms of IR brightness are regarded as hot spot pixels. Based on this, a probabilistic map is generated where the probability of a hot spot pixel is proportional to its intensity value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The system overview</head><p>Motion detection in IR. We exploit background subtraction to extract the motion pixels, assuming the camera is fixed. Here, a mixture of Gaussians is used to model the background.</p><p>Co-occurrence detection. We notice that hot spot and the motion usually occur simultaneously, and their locations are normally near each other in our application. Therefore, the co-occurrence of these two features will help to infer saliency. Here, we use the distance between a hot spot region and a motion region to indicate the probability of the co-occurrence: the shorter the distance, the larger the probability.</p><p>MRF-based saliency classification. We consider saliency detection to be a type of pixel labeling problem (classification) where a given pixel is labeled as either the salient (1) or non-salient (0). We use the MRF technique to solve this classification problem, because this model determines the label of a pixel by jointly considering its own probability to be a salient pixel and the labels of its surrounding pixels.</p><p>Multi-resolution (MR) transform. In our system, we intend to have a combination of pixel-based and region-based fusion. For the region-based fusion, we only focus on extracting information from the salient regions. For the fusion at the pixel level, we adopt the popular multi-resolution (MR) framework based on the wavelet transform.</p><p>Biased fusion. Building on the above steps, we divide the pixels into two categories: normal versus salient. For the normal pixels, we perform a MR-based image fusion biased in favor of the ViS image. More specifically, when fusing the approximation coefficients of two images, we assign a significantly larger weight to the coefficients of ViS image. By doing so, we expect the fused image to be perceptually close to the ViS image. However, if a pixel is labeled as a salient pixel, it means that the information from the IR image at this pixel position is more important than the information from the ViS image. For this case, we duplicate the information from the IR image into the fused image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Saliency detection on IR image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Markov random fields (MRFs)-based saliency detection</head><p>Saliency detection can be interpreted as a pixel labeling problem, where each pixel in the image is labeled as either salient or non-salient. The pixel labeling is a typical classification problem, which can be treated by a Markovian-based Maximum A-Posterior (MAP) approximation. Given the observed features f and the configuration of labels l, the posterior probability of l is:</p><formula xml:id="formula_0">Pðl9f Þ ¼ Pðf 9lÞPðlÞ Pðf Þ :<label>ð1Þ</label></formula><p>Maximizing Pðl9f Þ amounts to maximizing the product of the class conditional probability Pðf 9lÞ and the prior probability P(l). In the MRF framework, we assume that Pðf 9lÞ and P(l) take the form of a Gibbs distribution, and the MAP problem can then be solved by minimizing a Gibbs energy E(l). For the saliency detection, only the saliency label S and the non-saliency label NS are considered. Energy E(l) is composed of a node energy E n and a smoothness energy E s <ref type="bibr" target="#b24">[25]</ref>, such that:</p><formula xml:id="formula_1">EðlÞ ¼ E n þ E s ¼ X V i ðl i Þþ X ði,jÞ A Cq V i,j ðl i ,l j Þ:<label>ð2Þ</label></formula><p>The node energy is simply the sum of a set of per-pixel node costs</p><formula xml:id="formula_2">V i ðl i Þ.</formula><p>It is a function derived from the observed data that measures the cost of assigning label l i to pixel i. The smoothness energy E s measures the cost of assigning the labels l i , l j to adjacent pixels i, j.</p><p>Here, C q is the clique at location q.</p><p>The core idea of the MRF model is that the label of a given pixel is jointly determined by a label-specific probability as well as the labels of its neighboring pixels. In the model, spatial coherence means that we can assume that neighboring image pixels will tend to have identical labels. This motivates us to use it for image fusion: label consistency of neighboring pixels in the fused image is beneficial, especially from the visualization perspective. The key issue of the MRF model is to define suitable energy functions in terms of the application. In our case, the definition of the node energy is more interesting, because it relies on the co-occurrence of hot spot and motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Co-occurrence of hot spots and motion</head><p>As we mentioned before, hot spots and motion are important clues to detect the IR saliency in our algorithm. We adopt a simple thresholding method to detect a hot spot, which can be formulated as:</p><formula xml:id="formula_3">HSðiÞ ¼ 1 if IðiÞ 4T h , 0 otherwise: (<label>ð3Þ</label></formula><p>Here, T h is an image-specific threshold value obtained by insisting that only 5% of the pixels exceed this value:</p><formula xml:id="formula_4">#fi : IðiÞ r T h g ¼ 0:95N,<label>ð4Þ</label></formula><p>where I(i) is the image gray value at pixel i, N is the total number of pixels, and # denotes the cardinality of the set. This can be recast as a probability map:</p><formula xml:id="formula_5">P HS ðiÞ ¼ HSðiÞ IðiÞ max i IðiÞ :<label>ð5Þ</label></formula><p>Fig. <ref type="figure">2</ref> shows an example, in which the left one is the original image, and the right one is the probability map (for the visualization purpose, the probability is rescaled to vary between 0 and 255).</p><p>In our system, we apply a background subtraction algorithm to sense the motion of the pixel, assuming a static camera. If a pixel is marked as a foreground pixel, this pixel is considered to ''move'' compared to the previous frames. Conversely, the pixel is supposed to be motionless if it is labeled as a background pixel. In a background subtraction algorithm, the generation of the background model is essential and important. Here, an adaptive Gaussian Mixture Model used in our previous work <ref type="bibr" target="#b25">[26]</ref> is employed to model the background. This algorithm maintains a Gaussian-mixture probability function for each pixel separately, where the parameters for each Gaussian distribution are updated in a recursive way.</p><p>Once we have obtained the hot spot pixels and the moving pixels, the next step is to calculate the co-occurrence of these two clues. The co-occurrence in this paper is defined as the probability that the hot spot and the motion are co-located. More specifically, if we detect one hot spot pixel and one moving pixel at the same time instance, the probability of the co-occurrence is measured in terms of the distance between them: the shorter the distance, the larger the probability of co-occurrence. In Fig. <ref type="figure" target="#fig_2">3a</ref>, we provide an example in which the probability that A and B occur together is larger than the probability that A and C occur together. In our implementation, we actually calculate the co-occurrence of regions instead of pixels to improve robustness against noise. The probability of co-occurrence of two pixels equals to the probability of co-occurrence of the two regions the pixels belong to. In Fig. <ref type="figure" target="#fig_2">3b</ref>, we can see the region map corresponding to Fig. <ref type="figure" target="#fig_2">3a</ref>. In this case, the white bounding-box indicates the location of the hot spot region while the black bounding-box delineates the region with motion. The region is formed by using a connected component labeling technique. Now, the problem reduces to computing the co-occurrence of these two regions. Similar to the co-occurrence of two pixels, the co-occurrence of two regions is inversely proportional to their distance, which can be described by:</p><formula xml:id="formula_6">Cooði,jÞ ¼ 1À dði,jÞ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi w 2 þh 2 p ,<label>ð6Þ</label></formula><p>Fig. <ref type="figure">2</ref>. Probabilistic hot spot map. Left: original images. Right: hot spot pixel map, in which the brighter the pixel, the higher the probability to be hot.</p><p>where dði,jÞ means the distance between the ith region and the jth region. It equals to the Euclidean distance between central points of two regions. The parameter ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi w 2 þ h 2 p actually refers to the length of the image diagonal: w and h are the width and height of the image respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation of IR saliency detection</head><p>In our work, we use the MRF model for saliency detection (no training procedure needed). In this model, the key issue is to define the node energy E n and the smoothness energy E s , where E n measures the cost of a pixel to be a salient pixel and E s measures the cost of assigning different labels to adjacent pixels. In our work, E n at the location i consists of two parts of equal importance. The first part is related to P HS (i) computed by Eq. ( <ref type="formula" target="#formula_5">5</ref>), and the second part is determined by the co-occurrence of the hot spot and motion at this location. Therefore, E n (i) can be specified by</p><formula xml:id="formula_7">E n ðiÞ ¼ P HS ðiÞþCðiÞ 2 :<label>ð7Þ</label></formula><p>Here, C(i) refers to the co-occurrence of a hot spot and motion at the location i. To compute it, we start by checking the hot spot map generated by Eq. (3). If the pixel i is labeled as a hot spot pixel, C(i) is the co-occurrence of the region that pixel i belongs to on the hot spot map and its closest region on the motion map, which can be computed via Eq. ( <ref type="formula" target="#formula_6">6</ref>). E s in Eq. ( <ref type="formula" target="#formula_1">2</ref>) is determined by V i,j ðl i ,l j Þ, which actually imposes spatial smoothness. The term V i,j ðl i ,l j Þ is defined as:</p><formula xml:id="formula_8">V i,j ðl i ,l j Þ ¼ 0 if l i ¼ l j , 1 if l i a l j : (<label>ð8Þ</label></formula><p>Here, i and j are neighboring pixels. Once we have defined E n and E s , the next step is to minimize E(l) formulated in Eq. ( <ref type="formula" target="#formula_1">2</ref>). In our system, the graph-cut technique is called upon to do the optimization task.</p><p>For the sake of completeness, we briefly outline how minimizing energy functions, such as Eq. ( <ref type="formula" target="#formula_1">2</ref>), is carried out by a graphbased min-cut/max-flow algorithm (for more details, we refer to <ref type="bibr" target="#b26">[27]</ref>). The graph here refers to a directed, weighted graph G ¼ ðE,VÞ, consisting of a set of nodes V and a set of directed edges E that connect them. Moreover, a graph also contains some additional labels that can be assigned to the nodes. In our case, the nodes correspond to image pixels, and the two labels are source s and sink t, respectively. Usually there are two types of edges in the graph: n-links and l-links, where n-links connect pairs of neighboring pixels while l-links connect pixels with labels. The cost of n-links corresponds to V i,j ðl i ,l j Þ in Eq. ( <ref type="formula" target="#formula_8">8</ref>), and the cost of l-links can be calculated by E n (i) in Eq. <ref type="bibr" target="#b6">(7)</ref>. Once the graph is constructed, a s=t cut can partition the nodes into disjoint groups, each containing exactly one label. Therefore, a cut actually assigns pixels to labels. If edge weights are appropriately set based on parameters of an energy, a minimum cut gives rise to a labeling with the minimum value for this energy. In <ref type="bibr" target="#b26">[27]</ref>, the authors provide an implementation of the min-cut algorithm, which is used in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Biased MR image fusion</head><p>We base our MR image fusion on the wavelet transform due to the resemblance between its filtering properties and the human vision process. Basically, our fusion consists of four steps: (1) Generate the saliency map from the IR image; (2) Carry out a wavelet transform for both the IR and ViS images; (3) Coefficient fusion at each scale using different fusion rules; (4) Perform the inverse wavelet transform and construct the fused image. Here, we will focus on explaining the coefficient fusion step.</p><p>Suppose that we have a pair of IR and ViS images denoted as x and y respectively. A x (i) and A y (i) are approximation coefficients of image x and y at the location i. D j</p><p>x ðiÞ and D j y ðiÞ refer to the detail coefficients of two images at the jth scale. According to the wavelet transform, D j</p><p>x ðiÞ is a composition of coefficients at three different bands, which are HL j</p><p>x ðiÞ, LH j x ðiÞ and HH j x ðiÞ respectively. In our system, we apply different rules for fusing approximation coefficients and detail coefficients. More specifically, the fusion rule for detail coefficients is simply named as selection. The basic idea can be formulated by:</p><formula xml:id="formula_9">D j f ðiÞ ¼ D j x ðiÞ if 9D j x ðiÞ9 49D j y ðiÞ9, D j y ðiÞ otherwise, 8 &lt; :<label>ð9Þ</label></formula><p>where,</p><formula xml:id="formula_10">9D j ðiÞ9 ¼ 9HL j ðiÞþLH j ðiÞþHH j ðiÞ9:<label>ð10Þ</label></formula><p>Here, D j f ðiÞ represents the detail coefficient of the fused image. Since the detail coefficient in the wavelet domain reflects the saliency of the image at the pixel level, the intuitive explanation of this fusion rule is that the fused image always preserves the pixel-level image saliency extracted from both IR and ViS images.</p><p>For the approximation coefficient, we apply a type of biased fusion which can be described by:</p><formula xml:id="formula_11">A f ðiÞ ¼ ð1ÀaÞA x ðiÞþaA y ðiÞ,<label>ð11Þ</label></formula><p>where the value of a depends on the saliency map obtained previously. If a pixel is labeled as a salient pixel on the map, a equals to 0. That is, the fusion fully relies on the IR image.</p><p>The rationale is that the region that the pixel belongs to is salient in the IR image, which therefore should be highlighted in the fused image. Conversely, if the pixel is not salient, a40:5. For this situation, the fusion bias is in favor of ViS image, because the ViS image normally has the best visualization properties. In practice, the precise value of the a-parameter is not critical, and can be set by users depending on the application at hand. Fig. <ref type="figure" target="#fig_3">4</ref> shows two fused images by setting a in Eq. ( <ref type="formula" target="#formula_11">11</ref>) differently. Obviously, the fused image is visibly poor when the fusion biases the IR image.</p><p>In passing we point out that when dealing with color images, each image is represented in the YUV-space and fusion is applied only to the Y component of the image. The color components are simply recycled from the original image:</p><formula xml:id="formula_12">Y f ¼ Y f ; U f ¼ U ViS ; and V f ¼ V ViS :<label>ð12Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head><p>Our proposed system is implemented in Cþþ on a Laptop PC platform (Dual core 2.53 GHz, 4 GB RAM) with a 64-bits operation system. We have tested our algorithm in two surveillance-related scenarios. In the first scenario, it is required to monitor an area within which a fire may occur. 2 For this situation, one may separately observe the hot spot (flame) and the smoke on the IR image and ViS image. If we can fuse two images properly, one is able to see both in the fused image, which helps the operator to quickly locate the fire. In the second scenario, the operator needs to be alerted when a human appears in a restricted area. They may find a moving bright spot with a blurring background on the IR image, but only observe clear background on the ViS image. Again, the fused image helps to locate the human accurately.</p><p>Our examination can be divided into two parts. First, we assess our MRF-based saliency detection algorithm based on IR videos only. Next, the saliency-aware fusion algorithm is tested. Both objective and subjective evaluation results are provided comparing it to a standard fusion algorithm that does not take saliency into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Evaluation of MRF-based saliency detection algorithm</head><p>We have tested our saliency detection algorithm with 15 pairs of IR and ViS videos ( 41 h), 3 which were recorded from 9 different locations. We also compared it with 4 recent algorithms, which are SR <ref type="bibr" target="#b27">[28]</ref>, FT <ref type="bibr" target="#b28">[29]</ref>, RC <ref type="bibr" target="#b29">[30]</ref> and Esaliency <ref type="bibr" target="#b30">[31]</ref>. They claim to outperform the original algorithm proposed by Itti <ref type="bibr" target="#b19">[20]</ref>. The first three algorithms mainly aim at detecting the saliency from the still RGB images, while the last one is used to extract salient regions from video, in which pixel motion is taken into consideration in the attention model. In general, our algorithm can successfully find all the ''moving'' hot spots except for the case when the spot is too small, e.g. in the very early stage of a flame.</p><p>In Fig. <ref type="figure" target="#fig_4">5</ref>, we show the visual comparison results of the saliency extraction by using various methods. Implementations of all existing algorithms are found on the authors' websites. They can generate a probabilistic-based saliency map except for Esaliency algorithm <ref type="bibr" target="#b30">[31]</ref>, which draws red bounding-boxes on the image to indicate the locations of salient objects. The saliency map provided by our method is actually a binary map, where the ''white'' region is the salient region. Clearly, the salient region attracting most attention in the fire monitoring scenario (video 1video 4) should be around the flame. Similarly, the definition of saliency for the second scenario (video 5-video 9) should be the moving objects. Seen from the first four images, the flame region is indeed marked as salient by most algorithms. However, all existing algorithms raise many false positive errors, e.g., the bright moon and black sky in the first two images. This may confuse the operator especially when he/she needs to take prompt decisions. Compared to existing algorithms, our method successfully detects all expected salient regions without generating any false alarms. For the second scenario, some existing algorithms even fail to detect the expected saliency, e.g., a moving car under the bridge on the fifth image and the moving person on the sixth image. By contrast, our method still provides accurate detection.</p><p>To evaluate the results in a qualitative way, precision and recall of the algorithm are calculated. The definitions <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref> for the precision and recall are given as:</p><formula xml:id="formula_13">precision ¼ TP TP þ FP ; recall ¼ TP TP þFN ,<label>ð13Þ</label></formula><p>where TP is true positive, FP refers to false positive and FN means false negative. It is noted from Fig. <ref type="figure" target="#fig_4">5</ref> that the SR algorithm <ref type="bibr" target="#b27">[28]</ref> and RC algorithm <ref type="bibr" target="#b29">[30]</ref> are slightly better than other existing algorithms. 4  Thus, we calculate the precision and the recall for our algorithm, as well as the SR and RC algorithms using two representative video sequences (video 1 and video 8). The ground truth data (see Fig. <ref type="figure" target="#fig_5">6</ref>) for each image is manually labeled with the help of an IR expert. Saliency maps generated by SR and RC are thresholded in order to obtain binary maps which are akin to the map provided by our method. Fig. <ref type="figure" target="#fig_6">7</ref> illustrates the comparison results, where the first two figures report the precision and recall obtained by different the methods. 2 This work is supported by EU-FP7 FIRESENSE project. 3 Two videos are from a public dataset and the others are provided by XenICs NV (Belgium). 4 Such a conclusion is made only based on our relative small dataset. It may not be true when a larger dataset is applied.</p><p>It can be seen that our method is superior to the other two methods.</p><p>The recalls of RC algorithm on both video sequences are acceptable. However, in both situations it raises too many false positives. In fact, it is not difficult to anticipate such behaviors by viewing the visual results in Fig. <ref type="figure" target="#fig_4">5</ref>.</p><p>In our algorithm, the key parameter is the T h in Eq. ( <ref type="formula" target="#formula_3">3</ref>). Earlier we mentioned that a change in T h does not significantly influence the performance of the saliency detection. The major reason is that we rely on the co-occurrence of motion and hot spots when determining the saliency, rather than considering the hot spots alone. Therefore, creating more hot spots by relaxing the threshold will normally not induce a significant increase in the number of salient pixels. To prove this, we measure the changes of precision and recall when T h is varying. It is noted from the bottom figure in Fig. <ref type="figure" target="#fig_6">7</ref> that the slope of the curve is rather shallow. This implies that our saliency detector is not very sensitive to the choice of the threshold. More specifically, the precision gradually decreases from 95% to 81%, and the recall increases from 91% to 98% when we change T h from 1% to 8%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation of biased MR fusion algorithm</head><p>We have also validated our fusion algorithm in both objective and subjective ways. The objective evaluation is carried out by using Liu's software <ref type="bibr" target="#b31">[32]</ref>. According to Liu's paper, metrics can be categorized into four groups:</p><p>1. information theory based metrics, 2. image feature based metrics, 3. image structural similarity based metrics, and 4. human perception inspired fusion metrics.</p><p>In our experiment, we select one representative from each category. They are normalized mutual information (Q MI ), gradient-based fusion performance (Q MI ), Yang's metric (Q Y ), and Chen-Blum metric (Q CB ), respectively. For more details concerning the metrics, we refer to the paper <ref type="bibr" target="#b31">[32]</ref>. We have applied our biased MR fusion algorithm to four video sequences, which correspond to Video 3, Video 4, Video 8 and Video 9 in Fig. <ref type="figure" target="#fig_4">5</ref>. Prior to the fusion, IR and ViS images are spatially registered with the aid of the software developed in our previous work <ref type="bibr" target="#b32">[33]</ref>. We randomly select some images ( Z 10) from each fused sequence, and calculate the average value of each metric. To prove the effectiveness of our algorithm, we compared our saliency-aware fusion with a standard fusion algorithm without saliency consideration. The result is given in Table <ref type="table">1</ref>. Note that all metrics are measured based on gray images. For both fusion algorithms, we use 3 levels 9/7 wavelet to decompose the image. For the standard  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Objective evaluation based on four metrics. fusion, we apply equal weight to IR and ViS channels when integrating low-frequency coefficients, which actually is an averaging procedure. All the metrics will assign a larger value to the better fusion result. Viewed from Table <ref type="table">1</ref>, metrics show a perfect consistency in the sense that our algorithm is better than the standard fusion on all metrics except for Q Y and Q CB for Video 9. We presume that the performance difference between two algorithms is substantial taking the fact that the saliency region is only a small part of image into account. The benefit of using saliency map for image fusion will be definitely more obvious if the salient region is bigger or there are multiple salient regions in the image. It is also noted that the results are not strongly dependent on the number of image samples, because the difference between two samples belonging to one sequence is subtle.</p><formula xml:id="formula_14">Q MI Q G Q Y Q CB Video<label>3</label></formula><p>To further investigate the performance of our fusion algorithm, we conducted a subjective assessment based on four samples of fused videos, which are given in Fig. <ref type="figure">8</ref>. To better demonstrate the advantage of fused image, we also provide the original IR and ViS images. The first two videos are related to the fire monitoring application, in which the main interest is the flame region. It is noticed that the fire on both videos is just at an early stage. Although the flame is invisible in the ViS image, it is rather obvious (hot spot) in the IR image due to its temperature difference with its surroundings. On the other hand, we can clearly observe the smoke plume in the ViS image, which is not apparent in the IR image. By integrating these two images, we can actually find both in the fused image, which not only confirms a fire but also helps to precisely locate the fire region in the real world.</p><p>A similar situation happens in the last two videos, in which cameras are used in a video surveillance application. In this case, the system should be aware of the presence of a person within the restricted area. Seen from both examples, the person is visible on the IR image, but its surrounding background is blurry, which may hamper the operator when trying to get the exact location of the intruder. On the ViS image, visual clarity of the background is better, but the person is hidden in the background. Fortunately, we have a better visualization for both foreground human and background environment in fused image.</p><p>We invited six subjects for our experiment. Four of the participants are working in the image processing field while the remaining two have little image processing experience. We asked them to watch fused images generated by standard fusion algorithm and our algorithm, and answer the following questions afterwards: Q1: Does the fused image visualize the surveillance application more effectively? Q2: Do the details from both channels appear on the fused image? Q3: Does the fused image help you to quickly find the object/ person of interest? Q4: Is the foreground clearly distinguishable from the background on the fused image? Q5: Can the combination of foreground and its surroundings help you to locate the target?</p><p>Fig. <ref type="figure">8</ref>. Image fusion comparison: our method vs standard fusion scheme.</p><p>Instead of only answering yes or no, subjects had to choose a score between 1 to 5, in which 5 corresponds to strongly accept and 1 represents strongly reject. The average scores from all subjects are given in Fig. <ref type="figure" target="#fig_7">9</ref>. Seen from the comparison results, the fused images generated by our algorithm are overwhelmingly better than the images generated by the standard fusion algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Evaluation of system efficiency</head><p>Our system is designed for surveillance applications. Therefore, a system that can provide the result promptly is desirable. To demonstrate the system efficiency, we calculate the time consumed for each frame when performing our algorithm to video sequences with different resolutions. There are two video sequences with 320 Â 256 (for both channels) and 360 Â 270 resolution, respectively. For the former video, the average execution time per pair of frames is 302 ms. For the video with larger resolution, we can obtain the fused image after 340 ms on average. To sum up, our system achieves a near real-time performance (3-4 pairs of frames per second) on a laptop environment when dealing with videos with CIF resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose a fast saliency-aware image fusion algorithm, which is inspired by the region-based image fusion concept. The major difference between traditional algorithms and our algorithm lies in the fact that our algorithm takes the saliency of the object/region into account. The saliency of the region drives the fusion procedure. In order to generate a consistent saliency map from IR image, we feed the co-occurrence of hot spots and motion into a MRF model. Both objective and subjective evaluations reveal that our saliency-aware fusion algorithm is better than the standard fusion algorithm. The fused image/video generated by our algorithm is able to enhance the visualization needed for typical surveillance applications. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 depicts the proposed system architecture with its main functional units and data flows. The functions of the key modules are as follows.</figDesc><graphic coords="3,38.23,531.06,240.12,182.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Architecture of the complete system. In addition to being processed via the regular multi-resolution (MR) transform, the IR video stream is also analysed for hot spots and motion, which together are indicative of saliency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Co-occurrence calculation. (a) Hot spot and motion map, where the white pixels represent the hot spot pixels and the red pixels refer to the moving pixels. (b) Hot spot regions and motion regions based on grouping pixels, where the white bounding-box indicates the location of the hot spot region and the black one delineates the region with motion. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</figDesc><graphic coords="5,112.77,58.64,360.00,142.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example of biased fusion. Left: Fused image when a ¼ 0:1. Right: Fused image when a ¼ 0:9.</figDesc><graphic coords="6,125.73,58.64,353.88,130.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visual comparison results of the saliency detection by using various methods.</figDesc><graphic coords="7,52.80,58.64,480.24,558.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Manually generated ground truth for video 1 and video 8.</figDesc><graphic coords="8,126.23,58.64,352.80,130.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Quantitative evaluation of the algorithm. Two upper figures: precision and recall for different algorithms. The bottom figure: precision-recall curve when Th is varying.</figDesc><graphic coords="8,92.53,219.05,420.12,243.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Subjective evaluation of our fusion algorithm, where the height of the bar indicates the average score of the corresponding question.</figDesc><graphic coords="10,50.78,58.64,234.72,140.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,123.93,460.90,357.48,269.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,53.81,355.12,478.08,375.12" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J. Han et al. / Neurocomputing 111 (2013) 70-80</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research leading to these results has received funding from the European Community's Seventh Framework Programme (FP7-ENV-2009-1) under Grant agreement no FP7-ENV-244088 ''FIRE-SENSE-Fire Detection and Management through a Multi-Sensor Network for the Protection of Cultural Heritage Areas from the Risk of Fire and Extreme Weather''.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Paul de Zeeuw is a numerical mathematician, affiliated at the CWI, Amsterdam (NL), since 1979. He studied mathematics and computer science at the University of Leiden and obtained his Ph.D. thesis from the University of Amsterdam. He authored and coauthored many papers on multigrid algorithms for the solution of partial differential equations. One paper in particular is much cited and the accompanying computer code is widely used. He has also been participating in image processing projects, as a spin-off thereof two Matlab toolboxes have been built and made available on the web. Further, he has been author at the Dutch Open University on the topic of numerical linear algebra, and was the secretary of the Dutch-Flemish Numerical Analysis Society from 1997 till 2002, including being editor of its newsletter. He has acted as a reviewer of project proposals. Present focal points are applications of multiresolution methods in image processing, including image fusion and contentbased image retrieval.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Review article multisensor image fusion in remote sensing: concepts, methods and applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Genderen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="823" to="854" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wavelet based image fusion techniques: an introduction, review and comparison</title>
		<author>
			<persName><forename type="first">-K</forename><surname>Amolins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="249" to="263" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image fusion using computational intelligence: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICECS</title>
		<meeting>the ICECS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="128" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Merging images through pattern decomposition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE</title>
		<meeting>the SPIE</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical image fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vision Appl</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A novel image fusion method using contourlet transform</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Communications, Circuits and Systems</title>
		<meeting>the IEEE Conference on Communications, Circuits and Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="548" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The multigrid image transform</title>
		<author>
			<persName><forename type="first">P</forename><surname>De Zeeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Image Processing Based on Partial Differential Equations</title>
		<meeting>the Image Processing Based on Partial Differential Equations</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="309" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A general framework for multiresolution image fusion: from pixels to regions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="259" to="280" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The steerable pyramid: a flexible architecture for multi-scale derivative computation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICIP</title>
		<meeting>the ICIP</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="444" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced image capture through fusion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision</title>
		<meeting>the Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multisensor image fusion using the wavelet transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models Image Process</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A region-based image fusion scheme for concealed weapon detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Conference on Information Sciences and Systems</title>
		<meeting>Conference on Information Sciences and Systems</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="168" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiresolution image fusion guided by a multimodal segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heijmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACIVS</title>
		<meeting>the ACIVS</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Region-based image fusion using complex wavelets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>O'callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Canagarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Conference on Information Fusion</title>
		<meeting>Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="555" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uni-modal versus joint segmentation for region-based image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Canagarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Conference on Information Fusion</title>
		<meeting>Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segmentation-driven image fusion based on alpha-stable modeling of wavelet coefficients</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="624" to="633" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel region based multimodality image fusion method</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zaveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pattern Recognition Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="140" to="153" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodality and multiresolution image fusion</title>
		<author>
			<persName><forename type="first">P</forename><surname>De Zeeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Theory and Applications</title>
		<meeting>the International Conference on Computer Vision Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic template based pedestrian detection in infrared videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intelligent Vehicles Symposium</title>
		<meeting>the IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pedestrian detection and tracking with night vision</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fujimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="71" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A real time human detection based on far infrared vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Emile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Image and Signal Processing</title>
		<meeting>the Conference on Image and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Background-subtraction using contour-based fusion of thermal and visible imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="162" to="182" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bayesian algorithms for adaptive change detection in image sequence using Markov random fields</title>
		<author>
			<persName><forename type="first">T</forename><surname>Aach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process. Image Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time multiple people tracking for automatic groupbehavior evaluation in delivery simulation training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De With</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="913" to="933" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency detection: a spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Esaliency (extended saliency): meaningful attention using stochastic image modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="693" to="708" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Objective assessment of multiresolution image fusion algorithms for context enhancement in night vision: a comparative study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laganiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="109" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visible and infrared image registration in man-made environment employing hybrid visual features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Zeeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
