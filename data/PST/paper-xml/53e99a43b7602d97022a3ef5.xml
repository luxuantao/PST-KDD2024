<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aggregating Crowdsourced Binary Ratings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nilesh</forename><surname>Dalvi</surname></persName>
							<email>nileshdalvi@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
							<email>anirban.dasgupta@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
							<email>ravi.k53@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Vibhor</forename><surname>Rastogi</surname></persName>
							<email>vibhor.rastogi@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Facebook, Inc</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Yahoo! Labs Sunnyvale</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Aggregating Crowdsourced Binary Ratings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0173DB6805462B39F006912F3F7E1232</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.4.m [Information Systems]: Miscellaneous Crowdsourcing</term>
					<term>mechanical turk</term>
					<term>spectral methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we analyze a crowdsourcing system consisting of a set of users and a set of binary choice questions. Each user has an unknown, fixed, reliability that determines the user's error rate in answering questions. The problem is to determine the truth values of the questions solely based on the user answers. Although this problem has been studied extensively, theoretical error bounds have been shown only for restricted settings: when the graph between users and questions is either random or complete. In this paper we consider a general setting of the problem where the user-question graph can be arbitrary. We obtain bounds on the error rate of our algorithm and show it is governed by the expansion of the graph. We demonstrate, using several synthetic and real datasets, that our algorithm outperforms the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Ever since Amazon launched its Mechanical Turk in 2005, crowdsourcing and human computing have become part and parcel of the World-Wide Web experience (en.wikipedia.org/wiki/ Crowdsourcing). The topic frequently hits popular media, ranging from plaudits 1 to all-round skepticism 2 . Crowdsourcing has also attracted the attention of the research community at large, as evinced by the number of workshops and tutorials in many recent conferences dedicated to this topic: WWW 3 , WSDM 4 , SIGIR 5 , CHI 6 , KDD/AAAI 7 .</p><p>As its name suggests, crowdsourcing taps into the wisdom of crowds. In its most basic version, it involves posing a presumably hard question to a set of users and aggregating their individual responses in order to deduce the answer to the question. This simple paradigm is useful in two scenarios where human labeling offers some version of the ground truth. First, it can be used to generate large quantities of labeled examples for algorithms that are based on machine learning. Second, it can be used for large-scale human evaluation and comparison of different algorithms for a problem.</p><p>Even this simplest version of crowdsourcing already poses an interesting research challenge: how to aggregate the responses of the users in order to obtain the true answer to each question? Meticulous users can be more accurate than the others in answering the questions, whereas unreliable/lazy (or spammy) users can provide random (or even adversarial) answers. To further complicate the problem, in many such systems, the reliability of a user may not be known a priori; indeed, a large fraction of the users may even be new recruits. These issues entail a holistic approach to the problem: rather than aggregate the answers for each question in isolation, it becomes necessary to look at the global matrix of user provided answers to all the questions in order to simultaneously elicit both the user reliabilities and the true answers.</p><p>There have been several approaches <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b11">11]</ref> to formalizing this problem. These approaches posit a set of items with binary qualities, and a set of users indicating the qualities of items. Not all users necessarily rate all items. A bipartite graph G between items and users captures the set of items rated by each user. Typically, a simple model is assumed for users: each user is associated with a reliability measure, which is used to independently "corrupt" her perception of the true quality of the item. Given a set of user ratings, the problem is to collectively determine the reliability of each user and the true quality of each item. These approaches fall into two broad categories: machine-learning based and linearalgebraic based. The machine-learning approaches are based on variants of EM, which work on any graph G, but offer no guarantees as to how well they perform (see <ref type="bibr">Section 2)</ref>.</p><p>Algebraic approaches, on the other hand, can provide theoretical guarantees on the error in estimating item qualities, but so far have been limited to either complete assignment graphs (when each user rates all items) or to random graphs (when the assignment of users to items is random). One of the first algebraic approaches was proposed by Ghosh et al. <ref type="bibr" target="#b5">[5]</ref>, who present an algorithm with the following guarantee: for a random user-item assignment graph with n users, where in expectation each user rates D items and each item receives ∆ ratings, the fraction of incorrectly estimated items bounded by O( n D 3 ). This bound is vacuous for sparse graphs where each user rates o(n 1/3 ) items. Karger et al. <ref type="bibr" target="#b10">[10]</ref> show that for random graphs, in the limit when number of items is going to infinity, the error in item qualities can be asymptotically bounded by e -O(∆) , where ∆ is again the expected number of users rating an item. Thus their bound is stronger than <ref type="bibr" target="#b5">[5]</ref> and holds for sparse random graphs as well, but only asymptotically.</p><p>Our work is motivated by the fact that the user-item graph G is in practice neither random nor regular. Often, users determine both the number as well as the set of items they want to rate. The former is a function of their motivation level while the latter is determined by their expertise and familiarity with the items. Under such circumstances, it is not obvious how the techniques developed in <ref type="bibr" target="#b5">[5]</ref> and <ref type="bibr" target="#b10">[10]</ref> generalize-e.g., <ref type="bibr" target="#b5">[5]</ref> depends crucially on the fact that the "expected" item-item agreement matrix is low-rank and hence recoverable under random perturbations, which the assumed generative mechanism posits as the model for user mistakes. Similarly, the performance of <ref type="bibr" target="#b10">[10]</ref> depends crucially on whether belief propagation converges in arbitrary graphs. Thus it remains an open question to develop a strategy for aggregating user ratings when we do not have too much control in deciding which users choose what set of items to rate-whether there are characteristics of the user-item rating graph that make it amenable to good aggregation.</p><p>Main results. Our main contribution is an eigenvector-based technique to estimate both the user reliabilities and the item qualities that works for arbitrary user-item assignment graphs G. We bound the error rate as a function of the expansion gap, i.e., the gap between the first and second eigenvalues of the graph G t G. The essence of our technique is to look at the user-user agreement matrices-measuring agreement between pairs of users-that are normalized by the number of items they decided to co-rate, and to then extract its topmost eigenvector. A key element of our approach is to show a concentration result for structured random matrices, using the matrix version of McDiarmid's inequality. We then present two algorithms that are based on matrix completion; for each of the algorithms, we prove that the estimated user reliabilities are close to the truth if the graph G t G has some expansion properties. If the assignment graph is random, our estimate for user reliabilities translates into an approximation for the item qualities as well.</p><p>In particular, for a (D, ∆)-regular graph with a large eigengap, our bounds translate into a user reliability estimation error of Õ( 1</p><formula xml:id="formula_0">√ D + 1 ∆</formula><p>). On the other hand, even if we knew the true answer to each of the D questions that a user responds to, the estimated user reliability would still have a variance of 1/D, resulting in an estimation error of Ω(1/D) in the user reliabilities. Our error bound of O(D -1/2 ) is not too far off from this lower bound. For (D, ∆)random assignment graph our algorithm makes mistakes on only e -O(∆) fraction of the items. Our bound generalizes both the results of <ref type="bibr" target="#b5">[5]</ref> and <ref type="bibr" target="#b10">[10]</ref>, since this result holds for sparse graphs (unlike <ref type="bibr" target="#b5">[5]</ref>) without requiring the asymptotic argument of the number of items going to infinity (unlike <ref type="bibr" target="#b10">[10]</ref>).</p><p>Finally we also demonstrate our algorithms on real world datasets and show how they improve upon the state of the art in terms of accuracy of estimates in both item qualities and user reliabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Crowdsourcing, using the global marketplace to perform microtasks in a scalable way, is a topic that has generated much excitement <ref type="bibr">[8,</ref><ref type="bibr" target="#b12">12]</ref>-labeling and rating items consists of a large fraction of such tasks. A key problem in here is to decide how to aggregate the labels from multiple labelers of varying reliabilities such that the effect of the underlying noise is mitigated. The extensive empirical work by Sheng et al. <ref type="bibr" target="#b15">[15]</ref> shows that getting more noisy labels per item and then aggregating them is more accurate than getting more expensive, and hence allegedly more "accurate", labels; their work uses only majority voting to aggregate labels from multiple users, and is primarily concerned with identifying the items that will benefit from more labels. Dekel et al. <ref type="bibr" target="#b4">[4]</ref> show that such aggregation can be improved if the bad raters are pruned.</p><p>A more general analysis of the user reliabilities was done by Dawid et al. <ref type="bibr" target="#b3">[3]</ref>, who are the first to model the obfuscation of labels by judges, and use the EM algorithm in order to derive the true labels. Unfortunately, the EM technique suffers from lack of theoretical guarantees and has issues regarding convergence and initialization. Since then, there has been a host of followup work modifying this approach using a Bayesian technique <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b11">11]</ref>, studying it in the context of learning a specific classifier <ref type="bibr" target="#b14">[14]</ref>, and modifying it by finding out spammers, i.e., labelers deliberately giving incorrect responses <ref type="bibr" target="#b13">[13]</ref>. Other related results in applying machine learning techniques to cleaning user labels include <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b18">18]</ref>.</p><p>Much of the above work does not come with theoretical guarantees on the inferred user reliabilities or the item labels. Both Ghosh et al. <ref type="bibr" target="#b5">[5]</ref> and Karger et al. <ref type="bibr" target="#b10">[10]</ref> study this problem independently in the same generative setting, where each user rates a random set of items, and has an inherent probability of identifying the correct label, or flipping it. Our model is essentially a generalization of their setting to arbitrary user-item assignment graphs. Ghosh et al. <ref type="bibr" target="#b5">[5]</ref> present a spectral algorithm that provably learns the true item qualities, with bounded error. However, as pointed out, these bounds are useful only when each user performs a large number of ratings. Karger et al. <ref type="bibr" target="#b10">[10]</ref> uses belief propagation<ref type="foot" target="#foot_0">8</ref> to derive both a set of user reliabilities and an estimate for item qualities for a sparse random graph. Their convergence analysis uses techniques from density evolution and hence critically depends on the fact the graph is both sparse and random. Liu et al. <ref type="bibr" target="#b11">[11]</ref> extend the BP algorithm of <ref type="bibr" target="#b10">[10]</ref> via a Bayesian approach by choosing a suitable prior for item qualities and user reliabilities, and uses clever techniques to make the message passing more efficient.</p><p>An orthogonal question to ours, and one that has received much attention, is how to design incentives such that each user performs to the best of his abilities and provides truthful ratings <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b9">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM DESCRIPTION</head><p>Let m be the number of items and n the number of users. Let qi ∈ {-1, 1} denote the quality of the ith item. Let q denote the column vector of length m with qi as the ith entry. Each user rates a subset of items. Let G ∈ {0, 1} m×n denote the item-user assignment matrix, i.e., Gij = 1 if item i is rated by user j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rating generation model</head><p>The ratings given by n users on m items is represented by a stochastic matrix U generated by the following random process (similar to <ref type="bibr" target="#b5">[5]</ref>). Each user j is associated with a probability pj ∈ [0, 1] that captures how correct is her rating. Independently, for each item i she rates (as dictated by G), she tosses a coin with bias pj: with probability pj, she rates item i (correctly) as qi and with probability 1 -pj, she rates item i (incorrectly) as -qi. Thus, the random matrix U ∈ {-1, 0, 1} m×n can be described as</p><formula xml:id="formula_1">Uij =      qi if Gij = 1, w.p. pj, -qi if Gij = 1, w.p. 1 -pj, 0 if Gij = 0.<label>(1)</label></formula><p>We call this random process as rating generation. Let wj = 2pj -1; we call wj the reliability of user j. Thus, the user reliabilities are in the range [-1, 1], where a reliability of 1 indicates a user who always answers correctly, a reliability of -1 indicates one who always answers incorrectly, and a reliability of 0 indicates one who answers uniformly at random. Let w ∈ n denote the vector of user reliabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem definition</head><p>The algorithm is given as input a realization of the stochastic rating matrix U , assumed to be generated from the set of latent parameters q and w, which are unknown. The aim is to estimate both the user reliabilities and the item qualities simultaneously, i.e., an estimate ŵ ∈ [-1, 1] n for the user reliabilities and an estimate q ∈ {-1, 1} m for the item qualities. The performance of the algorithm is measured by the distance to the underlying reliability vector and the quality vector. The errors for the estimates ŵ and q provided by the algorithm will thus be defined by the following quantities:</p><formula xml:id="formula_2">error( ŵ) = 1 n E(|| ŵ -w|| 2 2 ) and error(q) = 1 m E(||q -q|| 2 2 ) = 4 m 1[qi = qi].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TECHNIQUES</head><p>We review some definitions from linear algebra before presenting our algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Background</head><p>Throughout the paper, we represent (column) vectors using lowercase letters (a, b, w, . . .) and matrices by uppercase letters (M, N, . . .). Let x • y denote the innerproduct of x and y and let x t denote the transpose of x. For a matrix M , the spectral and Frobenius norms are denoted by</p><formula xml:id="formula_3">M = M 2 = max x 2 =1 M x 2 and M F = ( ij M 2 ij ) 1/2 respectively.</formula><p>For two matrices M and N of matching dimensions, we define the following Hadamard products:</p><formula xml:id="formula_4">(M ⊗ N )ij = MijNij; (M N )ij = Mij/Nij if Nij = 0, 0 otherwise.</formula><p>For any matrix M ∈ m×n , we define the indicator matrix</p><formula xml:id="formula_5">I(M ) ∈ {0, 1} m×n such that I(M )ij = 1[Mij = 0], i.e., I(M )ij is 1 if</formula><p>and only if the corresponding entry of M is nonzero. We will also denote the (scaled) top eigenvector of a matrix M as</p><formula xml:id="formula_6">v1(M ) = arg minx M -xx t 2, x<label>(1)</label></formula><p>≥ 0. We use the convention that indices i always denote items and indices j, k, . . . denote users. Let δi denote the number of ratings that item i gets, i.e., the number of non-zero entries in row i in G. Similarly, let dj denote the number of ratings that user j supplies. Define D = maxj dj and ∆ = maxi δi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithms</head><p>Recall that we are only presented with a realization of the ratings matrix U (and hence its indicator, the assignment matrix G as well) and we need to estimate both the item qualities and the user reliabilities. Before describing the algorithms, we present the intuition behind them.</p><p>The main idea is to work with the two user-user matrices U t U and G t G. The entry (G t G) jk is the number of common items rated by users j and k. The entry (U t U ) jk is the difference between the number of agreements and disagreements of the users j and k. Let E denote the matrix which contains itemwise expected values of the random matrix</p><formula xml:id="formula_7">U , i.e., Eij = E[Uij] = (pjqi + (1 -pj)(-qi))Gij = qiGijwj. 1: Input: U ∈ {-1, 0, 1} m×n , G ∈ {0, 1} m×n . 2: Output: w ∈ n , q ∈ m . 3: ŵ = v1(U t U ) v1(G t G) 4: Define wj = sgn( wj) max(| ŵj|, 1) 5: Define qi = sgn( j Uij wj). 6: Output ŵ, q.</formula><p>Algorithm 1: Ratio of eigenvectors.</p><p>It is easy to see that</p><formula xml:id="formula_8">E t E = (G t G) ⊗ (ww t ), E t E G t G = I(G t G) ⊗ ww t .</formula><p>(</p><formula xml:id="formula_9">)<label>2</label></formula><p>Suppose we knew the expected matrix E. We could then estimate the user reliabilities by solving the following problem</p><formula xml:id="formula_10">F (A, B) = arg min w A -B ⊗ (ww t ) F , s.t. ∀j, w 2 j ≤ 1.<label>(3)</label></formula><p>with (A, B) as either</p><formula xml:id="formula_11">(E t E, G t G) or (E t E G t G, I(G t G)).</formula><p>It is easy to see why this approach works if the graph G is complete: the expected matrix E = qw t and solving (3) would give us back the user reliabilities w exactly. This approach, however, has a few problems when the graph G is arbitrary. First, the above program is computationally intractable for arbitrary G (e.g., <ref type="bibr" target="#b7">[7]</ref>). But more importantly, we show that for arbitrary assignment graphs the matrix E t E might not be informative, as shown in the following example. Suppose there were two disjoint user groups A and B, and a user x ∈ A ∪ B. All users in A have reliability 1, those in B have reliability -1, and user x has reliability 0. The items have two disjoint groups S and T of size m/2. All users in A rate all items in S, all users in B rate all in T , and user x rates all items in S ∪ T . It is clear that by looking only at the matrix E t E, it is not possible to distinguish the highly reliable users from the non-reliable ones. It is easy to extend this construction to k + 2 user partitions such that we cannot distinguish the high and low proficiency users even if we are explicitly given, in addition to E t E, the names of k users who answer all the questions. Thus, we want to characterize the class of graphs G that allows us to recover w with small errors <ref type="foot" target="#foot_1">9</ref> .</p><p>One of our main contributions is to identify the expansion of the graph G as a sufficient property that enables us to estimate w both efficiently and with low errors-the resulting algorithms are presented in Algorithm 1 and 2. Since the matrix E is not observable, we instead work with the matrix U . Algorithm 1 is inspired by the observation that when G t G has rank one, (3) has an exact solution ŵ where ŵ ⊗ v1(G t G) = v1(E t E) and hence ŵ = v1(E t E) v1(G t G). We will show that when the graph G has sufficiently high expansion, this solution, even when using U t U in place of E t E, is a reasonable approximation. Algorithm 2 is inspired by ( <ref type="formula" target="#formula_9">2</ref>) and uses the same intuition that (3) is approximable when I(G t G) is close to a rank one matrix. Hence, in this case, we first compute the rank one approximation v1(I(G t G)) and then use it to compute the final estimate ŵ.</p><p>We next show an error bound on the estimate ŵ for user reliability obtained from Algorithm 1. (Similar bounds can be shown for Algorithm 2, which we defer to the full version.) Our error bound holds for arbitrary graphs having expansion properties. However,</p><formula xml:id="formula_12">1: Input: U ∈ {-1, 0, 1} m×n , G ∈ {0, 1} m×n . 2: Output: w ∈ n , q ∈ m . 3: Define ŵ = v1(U t U G t G v1(I(G t G))</formula><p>4: Define wj = sgn( ŵj) max(| ŵj|, 1) 5: Define qi = sgn( j Uij wj). 6: Output ŵ, q.</p><p>Algorithm 2: Eigenvectors of ratio. in order to illustrate our bounds, we state the results for (D, ∆)regular graphs. The more general result is stated and proved in Section 5 (See Theorem 5.12). Let w = i w 2 i n denote the average reliability of users.</p><formula xml:id="formula_13">THEOREM 4.1 (USER ERROR BOUND). Let , δ &lt; 1 be a fixed positive constants. If G is a (D, ∆)-regular graph such that ∆ &gt; 1 8 w2 , D &gt; 256 log(n/δ) 2 w2</formula><p>and the second eigenvalue of G t G, denoted by µ2, satisfies the condition µ2 &lt; w2 D∆ 16</p><p>, then with probability 1 -δ, Algorithm 1 returns an estimate ŵ, such that</p><formula xml:id="formula_14">error( ŵ) = Õ 1 ∆ + 1 √ D .</formula><p>When the item-user assignment graph is random, this error bound translates into a bound for error in item estimates. The question of whether such a bound holds for fixed graphs, under some assumptions, remains open. THEOREM 4.2. Let G be a random (D, ∆)-regular graph. With high probability, Algorithm 1 returns estimates q, such that</p><formula xml:id="formula_15">error(q) ≤ exp -O ∆ w2 - 1 ∆ - 1 √ D 2 .</formula><p>When the average reliability w is some constant bounded away from 0 (i.e., users are good on average), then error(q) scales as exp(-O(∆)). This matches the bound in <ref type="bibr" target="#b10">[10]</ref>. However, the bound in <ref type="bibr" target="#b10">[10]</ref> requires that the limit of number of items goes to infinity, an assumption we no longer require.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Alternate projections</head><p>So far we have considered the case when G has a large expansion gap, i.e., when the second eigenvalue is much smaller than the first. We propose a heuristic, without any theoretical guarantees, that improves the performance of both Algorithms 1 and 2 for low expansion graphs. This heuristic is based on the standard alternating projections technique <ref type="bibr" target="#b1">[1]</ref> for solving the weighted low rank approximation problem.</p><p>Recall that we are trying to find a user reliability vector w as a solution to the problem F (A, B) = arg minw A-B ⊗(ww t ) F . When B and I(B) satisfy expansion properties, Algorithm 1 and 2 both give good approximations to this problem. Consider a slight generalization of this problem that instead finds two vectors u and v to minimize arg minu,v A -B ⊗ (uv t ) F . When one of the vectors, say u, is known, the other can be computed by solving a simple least squares problem. Thus, this gives an EM-style alternating projections algorithm to iteratively compute u and v. On convergence, we are guaranteed to achieve a local optimum, which for symmetric matrices A and B implies that u = v. This common converged value can thus be used instead of w.</p><p>One problem with this approach is that since the original problem is not convex, the convergence can happen at a local minima. Thus, both the rate of convergence and the quality of converged solution depends on the initialization for u and v. In practice we observed that when u = v = ŵ, where ŵ is the estimate obtained by either Algorithm 1 or 2, then both rate of convergence and quality of converged solution is good. Intuitively, this is because Algorithm 1 and 2 already try to minimize the objective function (at least in the case of graphs with good expansions) and hence provide a very good seed for the alternating projections heuristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ANALYSIS</head><p>In this section we prove guarantees on the performance of our algorithms both in terms of the error incurred in estimating user reliabilities as well as for item qualities. The underlying intuition behind the proof is as follows. First we show that the response matrix U t U is close to the expectation matrix E t E. In order to prove this concentration bound, we need to use machinery aimed towards giving Chernoff-like tail bounds for sums of random matrices. We then use the expansion (and corresponding eigenvalue gap) of the user-user co-rating graph G t G to show that the gap between the first and second eigenvalues of G t G translates to a corresponding gap between the first and second eigenvalue of E t E as well. Using this, we then characterize the top eigenvector of E t E in terms of the top eigenvector of G t G and the reliability vector w; the error in this characterization depends, among other quantities, on the ratio between the top two eigenvalues of the graph G t G. This enables us to use the eigenvalues of G t G and U t U to create ŵ, an estimate of w. After creating an estimate ŵ of the user reliabilities, we can then use it to create an estimate of the item qualities q-the error in q will depend on the error in ŵ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Matrix tail bounds</head><p>We start with a statement of the matrix McDiarmid inequality that we will use as a tool. The underlying intuition behind this concentration result from <ref type="bibr" target="#b17">[17]</ref> is that a random matrix is close to its expectation in terms of the spectral norm, if it can be expressed as the output of a function having bounded sensitivity over its input variables. Note that A B denotes the usual semidefinite ordering, i.e., B -A is semidefinite. THEOREM 5.1 (MATRIX BOUNDED DIFFERENCE <ref type="bibr" target="#b17">[17]</ref>). Let {Z k } n k=1 be an independent family of random variables, and let H be a function that maps n variables to a self-adjoint matrix of dimension d. Consider a sequence {A k } of fixed self-adjoint matrices that satisfies (H(z1, . . . , z k , . . . , zn) -H(z1, . . . , z k , . . . , zn)</p><formula xml:id="formula_16">) 2 A k 2 ,</formula><p>where zi and z i range over all possible values of Zi for each index i. Compute the variance parameter σ = k A k 2 2. Denote the random vector z = (Z1, . . . , Zn). Then, for all t ≥ 0,</p><formula xml:id="formula_17">Pr[ H(z) -E[H(z)] &gt; t] ≤ d • e -t 2 /8σ 2 .</formula><p>We will use Theorem 5.1 to show that the user-user agreement matrix U t U is close to its expectation E t E in the following sense. For a user j, denote ρj = m i=1 Gijδ 2 i , i.e., ρj is the sum of squared degrees of items that j responds to, and denote ρ = max n j=1 ρj. We first define the function H(•). Lemma 5.2 then characterizes the structure of the difference matrices when any of the random variables is perturbed. Using this structural characterization Lemma 5.3 shows that function H(•) satisfies the sensitivity conditions of Theorem 5.1, and Lemma 5.4 shows the final bound that we get using the sensitivity condition derived in Lemma 5.3.</p><p>We abuse notation and define the sequence of random variables U = {U11, . . . , U1n, U21, . . . , U2n, . . . , Um1, . . . , Umn}.</p><p>The function H(•) is then defined as H(U ) = U t U , which is a selfadjoint matrix in n×n . We also define the sequence of self-adjoint matrices {Aij ∈ n×n , i ∈ [m], j ∈ [n]} where each Aij is a diagonal matrix with kth diagonal entry as 8G ik Gij(δi -1) for all k ∈ [n]. Lastly, we define column vectors ej and rij of length n as following: ej is the unit vector with 1 as the jth element, and rij[k] = -2UijU ik if k = j, and 0 otherwise. The following Lemma shows the structure of the sensitivity matrices.</p><p>LEMMA 5.2. For any response matrix U , denote ∆ij = H(U )-H(U ), where U is the response matrix identical to U in all entries except with the (i, j)th entry switched, i.e., U ij = -Uij and U kl = U kl for (k, l) = (i, j). Then ∆ 2 ij = rijr t ij + 4(δi -1)Gijeje t j . PROOF. Recall that H(U ) = U t U is an n × n matrix with the (j, j)th diagonal entry dj, where dj is the number of items rated by user j. Also H(U ) kl = a kl -b kl where (b kl ) a kl denotes the number of (dis-) agreements between users k and l in rating the items that they have in common.</p><p>Now since ∆ij = H(U ) -H(U ), where U differs from U only in the (i, j)th entry, ∆ij is again an n × n matrix with all but the jth row and column as 0. To see why, consider (k, l)th entry of ∆ij such that k = j and l = j. Both users k and l have same responses in both U and U . Thus the number of agreements and disagreements between k and l is same in U and U . Hence the (k, l)th entry of ∆ij is zero.</p><p>Since H(U ) = U t U and H(U ) = U t U are symmetric matrices, so is their difference ∆ij. Thus, the jth row and column for ∆ij are identical. We will show that the column is precisely the vector rij (and hence row is r t ij ). Consider the kth element of this row. If user k has rated item i, and k and j agree according to U , then they will disagree according to U . Similarly, if they disagree according to U , then they will agree according to U . Thus, kth element of rij, which is the difference in agreements and disagreements of users k and j will change by either 2 or -2. These cases can be summarized succinctly as -2UijU ik = rij[k], by definition. Only exception is rij[j], which is always 0, since no user disagrees with himself on the same item i. Thus the jth column of ∆ij is precisely rij.</p><p>The fact that ∆ij is the matrix with jth row and column equal to rij and rest elements as zero can be written as ∆ij = rije t j + ejr t ij , which yields that</p><formula xml:id="formula_18">∆ 2 ij = (rije t j )(rije t j ) + (rije t j )(ejr t ij ) +(ejr t ij )(rije t j ) + (ejr t ij )(ejr t ij ) = rij(e t j rij)e t j + rij(e t j ej)r t ij +ej(r t ij rij)e t j + ej(r t ij ej)r t ij = 0 + (1)rijr t ij + 0 + 4(δi -1)Gijeje t j .</formula><p>Here, the last equation follows from using the following values of the four innerproducts highlighted in penultimate equation: r t ij ej = e t j rij is 0 (since ej has only jth entry as non-zero, which is zero in rij), e t j ej is 1, and</p><formula xml:id="formula_19">r t ij rij is 4Gij(δi -1) (since k rij[k] 2 = k =j (-2U ik Uij) 2 = k =j 4GijG ik = 4(δi -1)Gij). The proof follows.</formula><p>Let Aij ∈ n×n be defined as a diagonal matrix where the kth entry equals 8G ik Gij(δi -1). Using the above lemma, we can show that ∆ 2 ij is bounded by the matrix Aij.</p><p>LEMMA </p><formula xml:id="formula_20">R kl = (-2UijU ik )(-2UijU il ) = 4GijU ik U il ,</formula><p>and hence</p><formula xml:id="formula_21">|R kl | = 4Gij|U ik ||Uij| = 4GijG ik G il .</formula><p>If either k = j or l = j, then the (k, l)th element is 0. Hence for the kth row, the sum of the absolute values of (k, l)th entries is</p><formula xml:id="formula_22">l |R kl | = l 4GijG ik G il = 4GijG ik (δi -1),</formula><p>since each user l = j who rated item i contributes exactly 1 to the sum.</p><p>Thus the diagonal matrix with 4GijG ik (δi -1) as the kth diagonal entry, diagonally dominates rijr t ij . Now A 2 ij /2 by definition is precisely such a diagonal matrix. Hence rijr</p><formula xml:id="formula_23">t ij A 2 ij /2.</formula><p>The next statement shows that U t U is close to the expectation matrix E t E. Recall that ρ = maxj m i=1 Gijδ 2 i .</p><p>LEMMA 5.4. Suppose the matrix U is generated by the rating generation process described above. Then, for every δ ∈ (0, 1),</p><formula xml:id="formula_24">Pr U t U -E[U t U ] 2 ≤ 8 ρ log (n/δ) ≥ 1 -δ.</formula><p>PROOF. Using the statement of Lemma 5.2, we get that the sensitivity of H(•) with respect to each variable Uij is bounded by A 2 ij . Thus, from the statement of Theorem 5.1, the variance parameter σ is given by</p><formula xml:id="formula_25">σ 2 = m i=1 n j=1 A 2 ij .</formula><p>Since each A 2 ij is diagonal, so is this sum. The kth diagonal entry of A 2 ij is 8G ik Gij(δi -1) and hence the kth diagonal entry of the sum is given by</p><formula xml:id="formula_26">m i=1 n j=1 8G ik Gij(δi -1) = m i=1 8(δi -1)G ik n j=1 Gij = m i=1 8(δi -1)G ik δi ≤ 8 m i=1 G ik δ 2 i = 8ρ k .</formula><p>Hence the spectral norm, which is the largest diagonal entry for a diagonal matrix, is simply 8 max k ρ k = 8ρ and hence σ 2 = 8ρ. Using this value for σ, setting d = n, and t 2 = 8σ 2 log(n/δ) = 64ρ log(n/δ) in Theorem 5.1 completes the proof.</p><p>Finally, this implies the following result.</p><p>LEMMA 5.5. For a matrix U generated by the random rating generation process, with probability 1-δ, and E = E[U ], U t U -E t E ≤ 8 ρ log (n/δ) + D, where D is the maximum number of ratings done by a person.</p><p>PROOF. Assuming the result of Lemma 5.4 holds, we only need to bound the norm of E t E -E[U t U ]. This is a diagonal matrix, with the jth diagonal entry to be dj(1 -w 2 j ). Hence,</p><formula xml:id="formula_27">E t E - E[U t U ] ≤ maxj d 2 j = D.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of estimators</head><p>In this section we show that the estimators for user reliabilities and item qualities have a small error. For notational simplicity, we assume that the event in Lemma 5.4 holds, i.e., the matrix U t U is close to its expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Algorithm 1: Ratio of eigenvectors</head><p>We first show the proof for Algorithm 1, which takes the ratio of the top eigenvectors of U t U and G t G. The proof strategy is to first show that under a suitable set of assumptions for G, the matrix E t E has a large gap between the first and second eigenvalues, and hence can be represented accurately using only the topmost eigenvectorthis will ensure that the eigenvector-based Algorithm 1 has small error.</p><p>Let the first and second eigenvalues of G t G be denoted by µ1 and µ2 respectively, and the top two eigenvalues of E t E be denoted by λ1 and λ2. Let g denote the first eigenvector of G t G, and e be that of E t E. Let gmin denote the minimum entry of g; by Perron-Frobenius theorem, gmin &gt; 0. Recall w2 = 1 n j w 2 j . Define κ = U t U -E t E 2 and W ∈ n × n to be the diagonal matrix with wj for the jth diagonal entry. LEMMA 5.6. λ1 ≥ µ1 W g 2 -µ2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF.</head><p>Recall that E t E = (G t G) ⊗ (ww t ). Since µ1 and g are the first eigenvalue and vector of G t G, we have that G t G = µ1gg t +A, where A is the matrix defined as the difference between G t G and µ1gg t . Thus, A = µ2.</p><formula xml:id="formula_28">E t E = (G t G) ⊗ (ww t ) = (µ1gg t + A) ⊗ (ww t ) (4) = µ1(W g)(W g) t + A ⊗ (ww t ).</formula><p>Hence we can write using the triangle inequality:</p><formula xml:id="formula_29">E t E ≥ µ1(W g)(W g) t -A ⊗ (ww t ) ≥ µ1 W g 2 -A = µ1 W g 2 -µ2,</formula><p>where we use A ⊗ (ww t ) = W AW ≤ W 2 A ≤ A . This completes the proof.</p><p>LEMMA 5.7. (e t W g) 2 ≥ W g 2 -2µ 2 µ 1 .</p><p>PROOF. From (4), we know that E t E = µ1(W g)(W g) t + A ⊗ (ww t ), where A ≤ µ2. Also eE t Ee t = λ1 and e t E t Ee = e t (µ1(W g) t (W g) + A ⊗ (ww t ))e = µ1(e t W g) 2 + e t (A ⊗ (ww t ))e ≤ µ1(e t W g) 2 + µ2, where the last inequality again follows from A ⊗ (ww t ) ≤ A W 2 ≤ µ2 maxj w 2 j ≤ µ2. Thus, we have λ1 = e t E t Ee ≤ µ1(e t W g) 2 + µ2.</p><p>LEMMA 5.8. λ2 ≤ 3µ2.</p><p>PROOF. Let x be the second eigenvector of E t E. Then xE t Ex t = λ2. Also x is perpendicular to the largest eigenvector e of E t E. So, we know (e t W g) 2 + (x t W g) 2 ≤ W g 2 . From Lemma 5.7, we know (e t W g) 2 ≥ W g 2 -2µ2/µ1. Hence, (x t W g) 2 ≤ 2µ2/µ1. Thus, we can write</p><formula xml:id="formula_30">λ2 = x t E t Ex = x t µ1(W g)(W g) t + A ⊗ (ww t ) = µ1(x t W g) 2 + x(A ⊗ (ww t ))x ≤ µ1 • 2µ2 µ1 + µ2 = 3µ2. LEMMA 5.9. Let κ = U t U -E t E 2 and τ = W g . If λ 2 +3κ λ 1 &lt; 1 and 2µ 2 µ 1 τ 2 &lt; 1, then u - W g τ ≤ 2 λ2 + 3κ λ1 + 4µ2 µ1τ 2 .</formula><p>PROOF. Since u and e are the top eigenvector of U t U and E t E respectively, and κ = U t U -E t E , by applying a standard matrix perturbation bound <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">Lemma 3.2]</ref>,</p><formula xml:id="formula_31">(e • u) 2 ≥ 1 - λ2 + 3κ λ1 .</formula><p>We write the bound derived in Lemma 5.7 as follows:</p><formula xml:id="formula_32">(e t W g τ ) 2 ≥ 1 -2µ 2 µ 1 τ 2 .</formula><p>From the condition stated in the Lemma, since 2µ2 ≤ µ1τ 2 , and</p><formula xml:id="formula_33">√ 1 -x ≥ 1 -x for 0 &lt; x &lt; 1, we have e t W g τ ≥ 1 -2µ 2 µ 1 τ 2 ≥ 1-2µ 2 µ 1 τ 2 . Hence e-W g τ 2 = 2-2 e t W g τ ≤ 4µ 2 µ 1 τ 2</formula><p>Similarly, e t u ≥ 1 -</p><formula xml:id="formula_34">λ 2 +3κ λ 1 ≥ 1 -λ 2 +3κ λ 1</formula><p>and thus e -u 2 ≤ 2 λ 2 +3κ λ 1 . The proof follows from the triangle inequality.</p><p>LEMMA 5.10. Denote τ = W g and let ŵ be the vector with the ith element τ ui/gi.</p><formula xml:id="formula_35">If λ 2 +3κ λ 1 &lt; 1 and 2µ 2 µ 1 τ 2 &lt; 1, then error( ŵ) = ŵ -w 2 n ≤ τ 2 ng 2 min 2 λ2 + 3κ λ1 + 4µ2 µ1τ 2 .</formula><p>PROOF. From Lemma 5.9, we know that</p><formula xml:id="formula_36">u - W g τ ≤ 2 λ2 + 3κ λ1 + 4µ2 µ1τ 2 .</formula><p>Hence</p><formula xml:id="formula_37">ŵ -w 2 = (τ u -W g) g 2 ≤ τ 2 u -W g/τ 2 g 2 min . Since ( √ x + √ y) 2 ≤ 2(x + y), we have ŵ -w 2 ≤ τ 2 g 2 min 2 λ2 + 3κ λ1 + 4µ2 µ1τ 2 ,</formula><p>and hence the proof. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF. This follows from considering the weighted graph corresponding to</head><formula xml:id="formula_38">G t G. Then i w 2 i g 2 i ≥ n w2 g 2 min ≥ w2 n(g 2 max /r 2 ) ≥ w2 /r 2 ,</formula><p>which completes the proof.</p><p>Combining the above lemmas, we get the final theorem about the error bounds. THEOREM 5.12. For a fixed assignment graph G and a rating matrix U that is generated by the random rating generating process, if the graph G satisfies µ2 &lt; µ1 w2 4r -6 ρ log (n/δ) -D</p><p>then with probability 1 -δ, Algorithm 1 returns estimates ŵ, such that error( ŵ) &lt; 10 µ1ng 2 min µ2 + D + 5 ρ log (n/δ) . PROOF. From Lemma 5.5, with probability 1 -δ,</p><formula xml:id="formula_40">U t U -E t E 2 ≤ 8 ρ log (n/δ) + D.</formula><p>Assume that the above event holds. Also, for Lemma 5.10, we need the following bounds:</p><formula xml:id="formula_41">λ2 + 3κ λ1 &lt; 1, 2µ2 µ1τ 2 &lt; 1.<label>(6)</label></formula><p>Using the bounds on λ1, λ2 and κ, the above bounds are satisfied if</p><formula xml:id="formula_42">µ2 &lt; µ1τ 2 4 -6 ρ log (n/δ) -D &lt; µ1 w2 4r -6 ρ log (n/δ) -D.<label>(7)</label></formula><p>Conditioned on this and Lemma 5.10, we have that</p><formula xml:id="formula_43">error( ŵ) = ŵ -w 2 n ≤ τ 2 ng 2 min 2 λ2 + 3κ λ1 + 4µ2 µ1τ 2 , where κ = U t U -E t E 2.</formula><p>Plugging in this value, and the bounds on λ2 and λ1 from Lemma 5.6 and Lemma 5.8, we have that</p><formula xml:id="formula_44">error( ŵ) ≤ τ 2 ng 2 min 3µ2 + 3D + 24 ρ log (n/δ) µ1τ 2 -µ2 + 4µ2 µ1τ 2 .</formula><p>We simplify this by using µ1τ 2 -µ2 ≥ µ1τ 2 /2 to give</p><formula xml:id="formula_45">error( ŵ) ≤ τ 2 ng 2 min 6µ2 + 6D + 48 ρ log (n/δ) µ1τ 2 + 4µ2 µ1τ 2 ≤ 10τ 2 ng 2 min 1 µ1τ 2 µ2 + D + 5 ρ log (n/δ) .</formula><p>In order to illustrate our bounds better, we also state a corollary for (D, ∆)-regular graphs. This is also a restatement of Theorem 4.1 and thus completes its proof. and the second eigenvalue µ2 satisfies the condition µ2 &lt; w2 D∆ 16 , then with probability 1 -δ, Algorithm 1 returns estimate ŵ, such that</p><formula xml:id="formula_46">error( ŵ) = O µ2 D∆ + 1 ∆ + log(n/δ) √ D = O( ).</formula><p>The proof is straightforward, after noting that gmin = 1 √ n and nD = m∆, and using a bound on µ1 ≥ m∆ 2 n , the average degree in G t G.</p><p>Asymptotically, this gives error( ŵ</p><formula xml:id="formula_47">) = Õ( 1 ∆ + 1 √ D )</formula><p>. Finally, we show that estimating the set of user reliabilities accurately enables us to estimate the quality of each item with small error. We show that for a random (D, graph the total error in estimating item quality falls exponentially with the maximum item-degree, as well as with the average reliability. This is also a restatement of Theorem 4.2 and thus completes its proof. THEOREM 5.14 (THEOREM 4.2). Let G be a random (D, ∆)-</p><formula xml:id="formula_48">regular graph. Let w = i w 2 i n</formula><p>. Let ŵ be an estimate with error(w, ŵ) ≤ . Then, error(q) ≤ e -∆( w2 -) 2 /64 . In particular, for q obtained by Algorithm 1, error(q) ≤ e</p><formula xml:id="formula_49">-O(∆( w2 -1 ∆ -1 √ D ) 2 ) .</formula><p>The proof of this theorem is based on the following lemma.</p><p>LEMMA 5.15. Denote α = ∆ n (w • ŵ). Then (i) ∆ ≥ α ≥ ∆( w2 -)/2 and (ii) if w2 &gt; , the probability that the ith item is wrong is at most e -α 2 /16∆ ≤ e -∆( w2 -) 2 /64 . PROOF. For (i), note that = error(w, ŵ) = w -</p><formula xml:id="formula_50">ŵ 2 /n = |w| 2 +| ŵ| 2 -2w• ŵ n . Thus w • ŵ/n = (|w| 2 + | ŵ| 2 -n )/2n ≥ ( w2 -)/2,</formula><p>which yields the result.</p><p>For (ii), define zi = j Uij ŵj. Then</p><formula xml:id="formula_51">E[zi] = j qiE[Gij]wj ŵj = qi(∆/n)w • ŵ = qiα.</formula><p>Then from (i) and assuming w2 &gt; , we get α &gt; 0. Thus,</p><formula xml:id="formula_52">sgn(E[zi]) is same as qi. Thus sgn(zi) = qi implies that |zi - E[zi]| &gt; E[zi]. Thus the probability that sgn(zi) = qi is at most Pr[|zi -E[zi]| &gt; E[zi]].</formula><p>For computing this probability, we will use Bernstein's inequality. Define yij = Uij ŵj.</p><formula xml:id="formula_53">Then zi = j yij. Also E[yij] = qi(∆/n)wj ŵj. Denote xij = yij -E[yij]</formula><p>. Now we will apply Bernstein's inequality over xij for a fixed i but j from</p><formula xml:id="formula_54">1 to n. Note that -1 -|E[yij]| ≤ xij ≤ 1 + E[yij]. Thus, it is safe to say that -2 ≤ xij ≤ 2. Also E[x 2 ij ] = E[y 2 ij ] -E[yij] 2 = (∆/n) ŵ2 j -(∆/n) 2 w 2 j ŵ2 j . Thus, E[x 2 ij ] ≤ (∆/n) ŵ2 j (1 -(∆/n)w 2 j ) ≤ ∆/n. Applying Bernstein's inequality for t = α/2, we get Pr j xij ≥ α/2 ≤ e -α 2 /8 j E[x 2 ij ]+2(α/2)(1/3) ≤ e -α 2 /8 ∆+α/3 ≤ e -α 2 /16∆ . Now j xij = j yij-E[ j yij] = j yij-E[zi] = j yij- qiα Thus | j yij| ≥ |qiα| -| j xij| ≥ α -α/2</formula><p>with probability e -α 2 /16∆ , which yields the result.</p><p>Analysis for Algorithm 2. The proof for Algorithm 2 follows a similar route. We first show a similar matrix concentration inequality and then use it to follow the the proof outline in Section 5. We postpone the details to the final version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>In this section we experimentally analyze the accuracy of the proposed algorithms in estimating both item ratings and user reliabilities. We implemented both Algorithm 1 and 2, which we denote by ALGORITHM 1 and ALGORITHM 2 respectively. We compare them with the following algorithms: the simple majority voting algorithm denoted by MAJORITY, the iterative EM algorithm denoted by EM, the spectral algorithm from Ghosh et al. <ref type="bibr" target="#b5">[5]</ref> denoted by GKM, and the belief propagation algorithm from Karger et al. <ref type="bibr" target="#b10">[10]</ref> denoted by KOS. We also implement LOWERBOUND which uses ground truth to compute the user reliabilities, and then uses the reliabilities to infer item ratings. Since it uses ground truth, it is not a true algorithm, but provides a benchmark to compare the performance of other algorithms.</p><p>Our implementation of ALGORITHM 1 and ALGORITHM 2 include the alternating projections heuristic described in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>To illustrate the properties of our algorithms we use both synthetic and real datasets as described below.  (d) measure error in user reliabilities using correlation coefficient. Lower % means better item estimates, while higher correlation coefficient means better user estimates. In all cases, ALGORITHM 2 is either best or second best. In terms of aggregate error, ALGORITHM 2 is best in both the item rating estimates and one user reliability estimate. (1) TREC 10 : this dataset is a collection of topic-document pairs labeled as relevant or non-relevant by mechanical turks. Several of the labels have ground truth assigned as well. There are three distinct datasets corresponding to different competitions of the workshop: namely, TREC.stage2, TREC.task1, and TREC.task2. The number of items, labeled items, users, and user responses for these datasets have been summarized in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>(2) NLP: this dataset <ref type="bibr" target="#b16">[16]</ref> is a collection of three human judged 10 sites.google.com/site/treccrowd/home datasets, all having ground truth labels, as summarized in Table <ref type="table" target="#tab_1">1</ref>.</p><p>(3) Synth: this is a synthetically generated dataset to help us analyze various algorithms in a controlled setting as a function of the numbers of responses by users and user reliabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Real datasets</head><p>We compare the different algorithms over the TREC and NLP datasets. We evaluate both item rating estimates and user reliability estimates. Error in item ratings is measured in terms of % of incorrect item rating. Thus lower the value, better is the estimate.</p><p>Figure <ref type="figure" target="#fig_3">1</ref>(a) shows the error for the three TREC datasets. We also show the overall aggregate error, which is the % of total items incorrectly predicted over the three datasets. For the first two datasets, the best algorithms are ALGORITHM 2 and EM, with MAJORITY much worst than the rest. This is perhaps because as we will see in synthetic datasets, MAJORITY is very sensitive to presence of spammers. In the third dataset, MAJORITY is in fact the best, along with ALGORITHM 2. Thus overall, ALGORITHM 2 is the most robust algorithm and has lowest aggregate error for the TREC dataset.   <ref type="figure">2</ref>: Item errors on synthetic data. User degrees are drawn according to a power law. First row considers equal number of spammers, hammers, and random users, while second considers only hammers and random users. First two columns consider user reliabilities positively correlated with their degrees, while third and fourth considers negative correlation. We break each scenario into two graphs to better visualize the differences. In all graphs as max degree increases, so does the degree skew, and then ALGORITHM 2 performs consistently better than GKM and KOS. In presence of spammers (first row), MAJORITY and EM deteriorate. ALGORITHM 2 performs well across the spectrum. Again we see a similar story here, ALGORITHM 2 is best in two out of the three datasets. For the third, MAJORITY is best, with ALGORITHM 2 not far behind. Overall, ALGORITHM 2 has the lowest aggregate error in NLP.</p><p>Next we analyze the error in user reliability estimates. Since some of the algorithms like KOS give user reliabilities only up to a constant normalization factor, we cannot directly measure user reliability estimates by comparing them to the ground truth (as they could be off by a constant factor). Thus we use Pearson's correlation coefficient to measure the accuracy of user reliability estimates, which is always a number between -1 and 1, and measures the correlation between two vector quantities. A value of 1 means complete positive correlation (up to some affine transformation), 0 means the two quantities are independent of each other, and -1 means they are negatively correlated. Larger the value, more the positive correlation, and therefore lower the error.</p><p>Figure <ref type="figure" target="#fig_4">1(c</ref>) and 1(d) show that ALGORITHM 2 is either the best or close to the best in estimating user reliabilities for all the datasets, while other algorithms significantly underperform in at least one of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Synthetic data</head><p>To better understand the performance of the algorithms with respect to the different parameters, we perform experiments over synthetic datasets. We generate synthetic datasets using the following steps. The number of items and the number of users is fixed to 1000 and 100 respectively. For the items, their binary ratings are generated as i.i.d. Bernoulli variables with p = 1/2. algorithms. This is because many of the users are close to random, as evident in high % errors for this dataset. Thus having a true estimate for these random users is not helpful, and LOWERBOUND is in fact worse than some of the other algorithms.</p><p>For generating the bipartite graph between the items and users, we use powerlaw sequences for user degrees, where the number of items rated by users follow a powerlaw distribution with an exponent of 2.5. In each case, we generate a random graph satisfying the given degree sequence. We study the accuracy of different algorithms as a function of the maximum degree.</p><p>We define three types of users: hammers, which have reliability 0.8, spammers, who have reliability -0.8, and random, who have reliability of 0. We study the performance of algorithms as a function of the fraction of spammers, hammers and random users in the dataset. We consider two configurations: equal spammers, consisting of equal number of hammers, spammers and random users, and no spammers, consisting of equal number of hammers and random users.</p><p>To model real-life scenarios we consider cases when the user reliabilities are correlated with degrees. For e.g., reliable users could be more expensive, and hence offer less number of labels. Thus we consider the case of negative correlation where reliabilities are negatively correlated to the user degrees. For the sake of completeness, we also consider the case of positive correlation where reliabilities are positively correlated to user degrees. This gives us four combinations: equal vs. no spammers and positive vs. negative correlations. Figure <ref type="figure">2</ref> shows the performance of all the algorithms for the four combinations. We explain the results below.</p><p>Figures <ref type="figure">2(a</ref>) and 2(b) contains the results of the dataset with equal spammers and positive correlations. We break the graph into two parts to focus on the low and high degree parts separately. Because of a large number of spammers, MAJORITY has an error rate close to 50%, which is so large that it does not even appear in the plot. EM also has a very large error for low max degree, but becomes competitive for high max degree. As the maximum user degrees become larger, the skew in degrees also becomes larger, and we notice that ALGORITHM 2 performs consistently better than the spectral methods of GKM and KOS for high maximum degree. This difference, although slight in synthetic data, manifests as a large one in real datasets, where the degree sequences are even more non-uniform. We see a very similar trend in Figures <ref type="figure">2(c</ref>) and 2(d).</p><p>For Figures 2(e) and 2(f), which have no spammers and positive correlation, MAJORITY and EM do better than before. In fact, EM does slightly better than the spectral algorithms. Among the spectral algorithms, ALGORITHM 2 outperforms everyone else because of the non-uniform degree sequence. Figures <ref type="figure">2(g</ref>) and 2(h) show as a similar trend for negative correlations as in the case of positive correlation, but the effect is less pronounced with all the algorithms bunched together more closely.</p><p>In summary, KOS and GKM perform well when the degrees are uniform (maximum degree is small and close to the minimum), but deteriorate when there is a skew in the degrees. EM performs well when there are no spammers, but deteriorates with the introduction of spammers. ALGORITHM 2 works well across the spectrum, and is robust to spammers and non-uniform degree sequences. This helps ALGORITHM 2 perform well on most synthetic and real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>We studied the problem of aggregating user ratings when the user-item rating graph is arbitrary. We formulated a matrix completion problem and presented two eigenvector-based algorithms that have guaranteed error bounds when the resulting user-user corating graph satisfies expansion properties. It would be interesting to see if one can say anything directly about the alternate-projection based technique under a similar set of assumptions. In practice not all items need similar effort to rate; incorporating this difficulty is also an interesting open direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>LEMMA 5 . 11 .</head><label>511</label><figDesc>Let w = i w 2 i n be the average reliability of users; let τ = W g and r = gmax/gmin. Then, τ ≥ w/r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>THEOREM 5 .</head><label>5</label><figDesc>13 (THEOREM 4.1). If G is a (D, ∆)-regular graph such that ∆ &gt; 1 8 w2 , D &gt; 256 log(n/δ) 2 w2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Error analysis on real datasets: (a) and (b) measure error in item ratings estimates as % of incorrect items, and (c) and (d) measure error in user reliabilities using correlation coefficient. Lower % means better item estimates, while higher correlation coefficient means better user estimates. In all cases, ALGORITHM 2 is either best or second best. In terms of aggregate error, ALGORITHM 2 is best in both the item rating estimates and one user reliability estimate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1(b) shows the item errors for the three NLP datasets.Again we see a similar story here, ALGORITHM 2 is best in two out of the three datasets. For the third, MAJORITY is best, with ALGORITHM 2 not far behind. Overall, ALGORITHM 2 has the lowest aggregate error in NLP.Next we analyze the error in user reliability estimates. Since some of the algorithms like KOS give user reliabilities only up to a constant normalization factor, we cannot directly measure user reliability estimates by comparing them to the ground truth (as they could be off by a constant factor). Thus we use Pearson's correlation coefficient to measure the accuracy of user reliability estimates, which is always a number between -1 and 1, and measures the correlation between two vector quantities. A value of 1 means complete positive correlation (up to some affine transformation), 0 means the two quantities are independent of each other, and -1 means they are negatively correlated. Larger the value, more the positive correlation, and therefore lower the error.Figure1(c) and 1(d) show that ALGORITHM 2 is either the best or close to the best in estimating user reliabilities for all the datasets, while other algorithms significantly underperform in at least one of the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>5.3. ∆ 2 consider the (k, l)th element, denoted by R kl , of rijr t ij . If k, l = j, then we have</figDesc><table><row><cell></cell><cell>ij</cell><cell>A 2 ij .</cell></row><row><cell cols="3">PROOF. From Lemma 5.2, ∆ 2 ij = rijr t ij + 4(δi -1)Gijeje t j .</cell></row><row><cell cols="3">Now if we show that rijr t ij</cell><cell>A 2 ij /2, then the proof of lemma is</cell></row><row><cell cols="3">complete, since trivially, 4(δi -1)Gijeje t j</cell><cell>A 2 ij /2.</cell></row><row><cell>To show rijr t ij</cell><cell cols="2">A 2 ij /2,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics for the real datasets used in our experiments.</figDesc><table><row><cell></cell><cell>m</cell><cell>labels</cell><cell>n</cell><cell>responses</cell></row><row><cell>TREC.stage2</cell><cell>3568</cell><cell cols="2">3568 181</cell><cell>10,751</cell></row><row><cell>TREC.task1</cell><cell>3297</cell><cell cols="2">3297 120</cell><cell>12000</cell></row><row><cell>TREC.task2</cell><cell cols="3">19033 2275 762</cell><cell>88385</cell></row><row><cell>NLP.rte</cell><cell>800</cell><cell>800</cell><cell>164</cell><cell>8000</cell></row><row><cell>NLP.temp</cell><cell>462</cell><cell>800</cell><cell>76</cell><cell>4620</cell></row><row><cell>NLP.emotions</cell><cell>600</cell><cell>600</cell><cell>228</cell><cell>6000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>11  </figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_0"><p>Belief Propagation (BP) operates on the user-item bipartite graph, and like any standard BP algorithm, excludes the message from the node when computing the outgoing message to that node-if this message is included, then the algorithm reduces to that of<ref type="bibr" target="#b5">[5]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_1"><p>Previous approaches have looked at the matrix U U t (as a proxy for EE t )<ref type="bibr" target="#b5">[5]</ref>; by augmenting the above construction it is possible to show that such approaches will also incur a constant fraction error for arbitrary assignment graphs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_2"><p>Surprisingly, LOWERBOUND for TREC.task1 is worse than some</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Alternating projections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dattorro</surname></persName>
		</author>
		<ptr target="www.stanford.edu/class/ee392o/altproj.pdf" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian model of crowdsourced relevance coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prof. 12th TREC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vox populi: Collecting high-quality labels from a crowd</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd COLT</title>
		<meeting>22nd COLT</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Who moderates the moderators?: Crowdsourcing abuse detection in user-generated content</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Mcafee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th EC</title>
		<meeting>12th EC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Crowdsourcing with endogenous entry</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Mcafee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st WWW</title>
		<meeting>21st WWW</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="999" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low-rank matrix approximation with weights or missing data is NP-hard</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Glineur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Analysis Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1149" to="1165" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Managing crowdsourced human computation: a tutorial</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Paritosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th WWW (Companion Volume)</title>
		<meeting>20th WWW (Companion Volume)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incentives for truthful reporting in crowdsourcing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1329" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative learning for reliable crowdsourcing systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th NIPS</title>
		<meeting>25th NIPS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1953" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Variational inference for crowdsourcing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ihler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th NIPS</title>
		<meeting>26th NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The computer is the new sewing machine: benefits and perils of crowdsourcing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th WWW (Companion Volume)</title>
		<meeting>20th WWW (Companion Volume)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="325" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eliminating spammers and ranking annotators for crowdsourced labeling tasks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="491" to="518" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Get another label? Improving data quality and data mining using multiple, noisy labelers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th KDD</title>
		<meeting>14th KDD</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good? Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">User-friendly tail bounds for sums of random matrices</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="389" to="434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The multidimensional wisdom of crowds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th NIPS</title>
		<meeting>24th NIPS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2424" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning from the wisdom of crowds by minimax entropy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th NIPS</title>
		<meeting>26th NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
