<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intrusion detection in computer networks by a modular ensemble of one-class classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-12-01">1 December 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
							<email>giacinto@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<addrLine>Piazza d&apos;Armi</addrLine>
									<postCode>09123</postCode>
									<settlement>Cagliari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Perdisci</surname></persName>
							<email>roberto.perdisci@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<addrLine>Piazza d&apos;Armi</addrLine>
									<postCode>09123</postCode>
									<settlement>Cagliari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mauro</forename><forename type="middle">Del</forename><surname>Rio</surname></persName>
							<email>mdrio@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<addrLine>Piazza d&apos;Armi</addrLine>
									<postCode>09123</postCode>
									<settlement>Cagliari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
							<email>roli@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<addrLine>Piazza d&apos;Armi</addrLine>
									<postCode>09123</postCode>
									<settlement>Cagliari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intrusion detection in computer networks by a modular ensemble of one-class classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-12-01">1 December 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">BB0A5C3817FCAE2B94A7F545362F3DB6</idno>
					<idno type="DOI">10.1016/j.inffus.2006.10.002</idno>
					<note type="submission">Received 14 December 2005; received in revised form 12 October 2006; accepted 13 October 2006</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Intrusion detection</term>
					<term>Computer networks</term>
					<term>Multiple classifier systems</term>
					<term>One-class classifiers</term>
					<term>Modular systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the early days of research on intrusion detection, anomaly-based approaches have been proposed to detect intrusion attempts. Attacks are detected as anomalies when compared to a model of normal (legitimate) events. Anomaly-based approaches typically produce a relatively large number of false alarms compared to signature-based IDS. However, anomaly-based IDS are able to detect neverbefore-seen attacks. As new types of attacks are generated at an increasing pace and the process of signature generation is slow, it turns out that signature-based IDS can be easily evaded by new attacks. The ability of anomaly-based IDS to detect attacks never observed in the wild has stirred up a renewed interest in anomaly detection. In particular, recent work focused on unsupervised or unlabeled anomaly detection, due to the fact that it is very hard and expensive to obtain a labeled dataset containing only pure normal events.</p><p>The unlabeled approaches proposed so far for network IDS focused on modeling the normal network traffic considered as a whole. As network traffic related to different protocols or services exhibits different characteristics, this paper proposes an unlabeled Network Anomaly IDS based on a modular Multiple Classifier System (MCS). Each module is designed to model a particular group of similar protocols or network services. The use of a modular MCS allows the designer to choose a different model and decision threshold for different (groups of) network services. This also allows the designer to tune the false alarm rate and detection rate produced by each module to optimize the overall performance of the ensemble. Experimental results on the KDD-Cup 1999 dataset show that the proposed anomaly IDS achieves high attack detection rate and low false alarm rate at the same time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Intrusion detection systems (IDS) are tools placed inside computer networks that analyze the network traffic (referred as network-based IDS) or audit data recorded by network hosts (referred as host-based IDS). IDS look for known or potential threats and raise an alarm whenever an intrusion attempt is detected. Two main approaches to intrusion detection are currently used, namely misuse detection and anomaly detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. Misuse detectors are based on models of known attacks, i.e., a detailed description of attack patterns. Most misuse-based detectors model the attack patterns by a set of rules often called attack signatures. This approach allows the IDS to precisely detect intrusive events that perfectly match the signatures. As precise signatures can only be written to describe well-known attacks, the effectiveness of signature-based IDS is strictly related to the extent to which the IDS are updated with the signatures that describe the latest attacks. Conversely, most anomaly-based detectors are based on statistical models of normal or legitimate 1 events, i.e., a statistical profile of what constitutes normal network activities. The main 1566-2535/$ -see front matter Ó 2006 Elsevier B.V. All rights reserved. doi:10.1016/j.inffus.2006. <ref type="bibr" target="#b9">10</ref>.002 assumption at the base of anomaly detection is the fact that manifestations of intrusive activities usually deviates from legitimate activities and then can be detected comparing them to the model of normal events.</p><p>Anomaly-based detection has been the first approach to be developed, in account of its theoretical ability to detect intrusions regardless of the specific system vulnerability and the type of attack <ref type="bibr" target="#b4">[5]</ref>. Even though anomaly-based approaches are promising, they usually produce a relatively high number of false alarms, due to the difficulties in modeling the normal events. For this reason, signature-based detectors are usually chosen to be deployed in many organizations, instead, thanks to their ability to reliably detect known attacks while producing a relatively low number of false alarms. Nevertheless, attackers are constantly developing new attack tools designed to evade signaturebased IDS. For example, techniques based on metamorphism and polymorphism are used to generate instances of the same attack that look syntactically different from each other, yet retaining the same semantic and therefore the same effect on the victim <ref type="bibr" target="#b28">[29]</ref>. In principle this problem could be solved by designing vulnerability-specific signatures <ref type="bibr" target="#b29">[30]</ref> that capture the ''root-cause'' of an attack, thus allowing the IDS to detect all the attacks that try to exploit the same vulnerability. This type of signatures usually works well when implemented as part of host-based IDS. However, it is difficult to implement vulnerability-specific signatures for network-based IDS due to computational complexity problems related to the high volume of events to be analyzed. Attempts to accomplish this task using approximate solutions usually cause the IDS to produce a high number of false positives, i.e., legitimate events that match an attack signature <ref type="bibr" target="#b22">[23]</ref>. Pattern recognition techniques for misuse-based IDS have also been explored <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. The main motivation in using pattern recognition techniques to develop misuse-based detectors is their generalization ability, which may support the recognition of new ''variants'' of known attacks that cannot be reliably detected by signature-based IDS. In order to apply these techniques a dataset containing examples of attack patters as well as normal traffic is needed. However, it is very difficult and expensive to create such a dataset and previous works in this field funded by DARPA and developed by the MIT Lincoln Laboratory group <ref type="bibr" target="#b17">[18]</ref> have been largely criticized <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>The reasons mentioned above motivate the renewed interest in network-based anomaly detection techniques, and, in particular, in unsupervised or unlabeled anomaly detection. Because it is very hard and expensive to obtain a labeled dataset, clustering and outlier detection techniques are applied on completely unlabeled traffic samples extracted from a real network. Alternatively, when a (small) set of labeled data samples is available in addition to unlabeled data, semi-supervised techniques may be used <ref type="bibr" target="#b1">[2]</ref>. However, semi-supervised techniques are not guaranteed to provide better solutions than unsupervised techniques, because semi-supervised techniques require the labeled data to be representative and accurate <ref type="bibr" target="#b1">[2]</ref>. Unfortunately, in the intrusion detection field it is hard to meet these requirements. For this reason, in this paper we propose an unsupervised method, where the only a priori knowledge about the data is represented by the following assumptions: (i) the extracted dataset contains two classes of data, normal and anomalous traffic; (ii) the numerosity of the anomalous traffic class is by far less than the numerosity of the normal traffic class. The first assumption comes from the consideration that pure legitimate traffic can only be generated by simulations in an isolated environment. However, this simulation process cannot reproduce traffic patterns of a real network <ref type="bibr" target="#b18">[19]</ref>. In turn, an anomaly detector derived from such an artificial dataset of normal traffic may easily produce a large number of false alarms during the operational phase, because real network traffic is in general different from the one artificially generated. On the other hand, it is very hard, time-consuming and expensive to produce a dataset of pure legitimate traffic by thoroughly ''cleaning'' real network traffic traces. As a result, traffic traces collected from real networks are not guaranteed to be attack free. The second assumption is due to the fact that in general the majority of the network traffic is legitimate <ref type="bibr" target="#b21">[22]</ref>. Moreover, it is possible to use existing signature-based IDS to eliminate the know attacks from the collected traffic, thus further reducing the percentage of attack events in the dataset.</p><p>Recently proposed anomaly detectors use clustering and outlier detection techniques on unlabeled data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref>. These techniques try both to cope with the possible presence of outliers attack patterns in the training data and to detect anomalous network events during the operational phase. They are based on the common assumption that in a suitable feature space attack traffic is statistically different from normal traffic <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>. Reported results show that the proposed approaches can be used to design effective anomaly detectors based on unlabeled traffic traces.</p><p>However, the unlabeled techniques proposed so far in the literature are based on ''monolithic'' approaches, i.e., one model of normal traffic is designed that covers all the protocols and services offered by the network. Besides, different models of normal traffic have been proposed and compared, but their combination has not been explored. This paper proposes an unlabeled approach for Network Anomaly IDS based on a modular Multiple Classifier System (MCS), whereby (i) A module is designed for each group of protocols and services, so that they fit the characteristics of the normal traffic related to that specific group of services <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref>. (ii) Each module can be implemented by using an individual classifier as well as a combination of different classifiers.</p><p>The proposed modular architecture allows the designer to choose the rejection threshold of each module, so that the overall attack detection rate can be optimized given a desired total false alarm rate for the ensemble.</p><p>Experiments have been carried out on the KDD-Cup 1999 Dataset, available at the KDD-UCI repository. Even though this dataset has been criticized and cannot be deemed accurate to test the effectiveness of IDS in real scenarios, it still has the capability to allow researchers to compare different techniques on a common dataset <ref type="bibr" target="#b20">[21]</ref>. Experimental results show that the anomaly IDS proposed in this paper achieves high attack detection rate and low false alarm rate at the same time. Besides, the proposed IDS is able to detect attacks that were included in the training set as ''noise'' mixed with normal traffic, as well as ''new'' attacks.</p><p>The paper is organized as follows. Related work is presented in Section 2. Section 3 describes the proposed MCS architecture. In particular, a heuristic for choosing the rejection threshold of each module is proposed. The implemented detection algorithms and the combining techniques are described in Section 4. Section 5 illustrates the experimental results on the KDD-Cup 1999 dataset and compares them to the results obtained by applying other approaches reported in the literature. Conclusions are drawn in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>One of the first works on unsupervised network anomaly detection was presented in <ref type="bibr" target="#b21">[22]</ref>, where a variant of the single-linkage clustering algorithm is used to discover outliers in the training dataset. Once the normal patterns are separated from outlier patterns, the clusters of normal data are used to construct a supervised detection model. In <ref type="bibr" target="#b7">[8]</ref>, Eskin et al. presented a geometric framework to perform anomaly detection. The patterns are mapped from a feature space F to a new feature space F 0 and anomalies are detected by searching for patterns that lie in sparse regions of F 0 . Three different classification technique are used, a clustering algorithm, a k-NN algorithm and a SVM-based one-class classifier. Experiments are performed on the KDD-UCI dataset, both on the portion containing network traffic, and on the portion containing sequences of system-calls.</p><p>In <ref type="bibr" target="#b31">[32]</ref>, Yang et al. proposed an anomaly detection measure called EWIND and an unsupervised pattern learning algorithm that uses EWIND to find outliers in the data, whereas Leung et al. <ref type="bibr" target="#b16">[17]</ref> presented a new density-based and grid-based clustering algorithm to perform unsupervised network anomaly detection. In <ref type="bibr" target="#b32">[33]</ref> a two-layer IDS is presented. The first layer uses a clustering algorithm to ''compress'' the information extracted for network packets, whereas the second layer implements an anomaly detection algorithm to classify the patterns received from the first layer.</p><p>In <ref type="bibr" target="#b30">[31]</ref>, Wang et al. proposed a payload-based anomaly IDS. A model is trained for each different service running on different server hosts in the protected network. For each packet, the frequency of the byte values in the payload (i.e., the data portion of the packet) is measured and a simple Gaussian model is trained. The detection of anomalous packets is performed by using a simplified Mahalanobis distance between the packets and the model of normal traffic.</p><p>Recently the paradigm of Multiple Classifier Systems (MCS) has been proposed for intrusion detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3]</ref>. In <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> classifiers trained on different feature subsets are combined to attain better performance than those attained by classifiers trained on the large feature space made of the union of subsets. A different approach is proposed in <ref type="bibr" target="#b2">[3]</ref> where a serial combination of classifiers is proposed. Network traffic is serially processed by different classifiers. At each stage a classifier may either decide for one attack class or send the pattern to another stage, which is trained on difficult cases. Reported results show that MCS improve the performance of IDS based on statistical pattern recognition techniques.</p><p>The work presented in this paper was mainly inspired by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Our work is new in that we propose a service-specific approach to address the unlabeled anomaly detection problem by applying multiple one-class classification techniques, which are often referred to also as outlier detection techniques. In particular, an heuristic to tune the false alarm rate produced by each anomaly detection module is proposed so that, given a fixed tolerable false alarm rate for the IDS, the overall detection rate is optimized. Besides, the combination of one-class classifiers is a new and not completely explored research area. A heuristic approach to combine multiple one-class classifiers' output was proposed by Tax et al. in <ref type="bibr" target="#b24">[25]</ref>. Nevertheless, the proposed heuristic may present some problems, in particular when density-based and ''distance-based'' one-class classifiers are combined. We propose a new heuristic to map the output of two different ''distance-based'' one-class classifiers to a class-conditional probability, which aims at overcoming the mentioned problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Modular MCS architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem definition</head><p>The traffic over a TCP/IP network consists of packets related to communications between hosts. The exchange of packets between hosts usually fits in the client-server paradigm, whereby a client host requests some information offered by a service running on a server host. The set of packets related to the communication established between the client and (the service running on) the server forms a connection. Each connection can be viewed as a pattern to be classified and the network-based anomaly detection problem can be formulated as follows <ref type="bibr" target="#b9">[10]</ref>:</p><p>Given the information about connections between pairs of hosts, assign each connection to the class of either normal or anomalous traffic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modular architecture</head><p>As mentioned in Section 3.1, each connection is related to a particular service. Different services are characterized by different peculiarities, e.g., the traffic related to the HTTP service is different from the traffic related to the SMTP service. Besides, as different services involve different software applications, attacks launched against different services manifest different characteristics. We propose to divide the network services into m groups, each one containing a number of ''similar'' services <ref type="bibr" target="#b8">[9]</ref>. Therefore, m modules are used, each one modeling the normal traffic related to one group of services. The intuitive advantage given by the modular approach is supported by the results in <ref type="bibr" target="#b15">[16]</ref>, where Lee et al. used information theory to measure the ''complexity'' of the classification task. The subdivision of services into groups turns into a decrease of the entropy of each subset of data, which in general coincides to the ability to construct a more precise model of the normal traffic. An example of how the services can be grouped is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, where the groupings refer to the network from which the KDD-Cup 1999 dataset was derived (see Section 5). In Fig. <ref type="figure" target="#fig_0">1</ref>, a ''miscellaneous'' group is used to aggregate different services that are rarely used in the computer network at hand. It is worth noting that the number of groups and the type of services in each group depend on the network to be protected, as different networks may provide different services with different characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overall vs. service-specific false alarm rate</head><p>Anomaly detection requires setting an acceptance threshold t, so that a traffic pattern x is labeled as anomalous if its similarity s(x, M) to the normal model M is less then t. The similarity measure s depends on the particular technique chosen to implement the model of normal traffic M. As we use different modules (i.e., different models) for different services, a method to tune the acceptance threshold for each module is necessary. In order to solve this task, we propose an heuristic approach whereby given a fixed tolerable false alarm rate for the IDS, the overall detection rate is optimized.</p><p>Let m be the number of service-specific modules of the IDS; FAR be the overall tolerable false alarm rate; FAR i be the false alarm rate related to the ith module; t i be the acceptance threshold for the ith module; P(M i ) = n i /n be the prior distribution of the patterns related to the ith group of services (i.e., the module) M i in the training data, where n i is the number of patterns related to the services for which the module M i is responsible and n is the total number of patterns in the training dataset. Accordingly, FAR is defined as</p><formula xml:id="formula_0">FAR ¼ X m i¼1 P ðM i Þ Á FAR i<label>ð1Þ</label></formula><p>Given a fixed value of the tolerable false alarm rate FAR for the IDS, there are many possible ways to ''distribute'' the overall FAR on the m modules. A value of FAR i has to be chosen for each module M i so that Eq. ( <ref type="formula" target="#formula_0">1</ref>) is satisfied. Once a FAR i has been set for each module M i , the thresholds t i can be chosen accordingly. As a first choice, we could set FAR i = FAR for each module M i . This choice satisfies Eq. ( <ref type="formula" target="#formula_0">1</ref>) and appears to be reasonable, given that no service is seemingly penalized. Nevertheless, this choice presents two drawbacks. One drawback is related to the actual number of false positives generated by each service. As the number of false positives is proportional to P(M i ), the group of services (i.e., the module) accounting for the largest portion of the traffic produces a number of false alarms that is by far larger than the one produced by poorly represented services (i.e., those services which are rarely, or not so often used in the network). This behavior is not adequate as the modules of the IDS that produce an overwhelming number of false alarms could be ''turned off'' by the network administrator. The other drawback is related to the relation between FAR i and the detection rate of the ith service, DR i . We observed experimentally that for a fixed value of FAR i , the corresponding value of DR i strongly depends on P(M i ). In particular, the larger P(M i ) the larger DR i . This effect can be explained as follows. Small values of P(M i ) are related to services rarely used in the network, whereby a smaller training set for M i can be extracted and the corresponding classifier(s) in general will not be able to adequately model the normal traffic.</p><p>According to the considerations reported above, given a fixed FAR we propose to compute FAR i as</p><formula xml:id="formula_1">FAR i ¼ 1 m Á P ðM i Þ FAR<label>ð2Þ</label></formula><p>This choice satisfies Eq. ( <ref type="formula" target="#formula_0">1</ref>) and allows us to attain an higher overall detection rate DR than that attained by choosing a fixed value FAR i = FAR for each module.</p><p>In order to set an acceptance threshold t i for the module M i , to obtain the false alarm rate FAR i computed as in Eq. ( <ref type="formula" target="#formula_1">2</ref>), we propose the following heuristic. Let us first note that for a given value of t i , the fraction p r i ðt i Þ of patterns rejected by M i may contain both patterns related to attacks and false alarms. Let us denote with p ai ðt i Þ the fraction of rejected attack patterns using the threshold t i , and with far i (t i ) the related fraction of false alarms. It is easy to see that the following relation holds:</p><formula xml:id="formula_2">p ri ðt i Þ ¼ p ai ðt i Þ þ far i ðt i Þ ð<label>3Þ</label></formula><p>We want to set t i so that far i (t i ) is equal to the desired false alarm rate FAR i (computed by using (2)). As for a given value of t i the only measurable quantity in Eq. ( <ref type="formula" target="#formula_2">3</ref>) is the rejection rate p ri ðt i Þ, we need some hypothesis on p ai ðt i Þ so that we can estimate far i ðt i Þ ¼ p ri ðt i Þ À p ai ðt i Þ, and therefore we can chose t i in order to obtain far i (t i ) = FAR i . We propose to assume p ai ðt i Þ ¼ P ai , where P ai is the expected attack probability for the ith service. <ref type="foot" target="#foot_1">2</ref> In other words, we assume that for a given threshold value, the rejected patterns are made up of all the attacks related to that service contained in the training set, plus a certain number of normal patterns. Thus, having fixed the value of p ai ðt i Þ ¼ P ai , we can tune t i in order to obtain</p><formula xml:id="formula_3">far i (t i ) = FAR i .</formula><p>It is easy to see that the computed thresholds t i (estimated according to the heuristic described above) produce the required overall FAR (see Eq. ( <ref type="formula" target="#formula_0">1</ref>)) only if the fraction of patterns rejected by each module actually contains all the attacks n i Á P ai , where n i is the total number of training patterns for the module M i . If this is not the case and the rejection rate p ri includes just a portion of the attacks, a larger number of false alarms far i (t i ) &gt; FAR i will occur. However, if the training dataset is a good sample of the real network traffic, we expect most of the attacks will ''look'' different from normal traffic and will be likely deemed outliers and rejected by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Service-specific MCS</head><p>Lee et al. <ref type="bibr" target="#b14">[15]</ref> proposed a framework for constructing the features used to describe the connections (the patterns). The derived set of features can be subdivided into two groups: (i) features describing each single connection; (ii) features related to statistical measures on ''correlated'' connections, namely different connections that have in common either the type of service they refer to or the destination host (i.e., the server host). The latter subset of features is usually referred as traffic features. On the other hand, the first group of features can be further subdivided into two subsets, namely intrinsic features and content features. The intrinsic features are extracted from the headers of the packets related to the connection, whereas the content features are extracted from the payload (i.e., the data portion of the packets). We call F the entire set of features and I, C and T the subsets of intrinsic, content and traffic features respectively, so that</p><formula xml:id="formula_4">F = I [ C [ T.</formula><p>As explained in Section 3.2, our IDS is subdivided into a number of modules. Each module implements a model of the normal traffic related to a group of services, so that a module can be viewed as a service-specific IDS. The problem of modeling the normal traffic for each module of the IDS can be formulated essentially in two different ways: (i) a ''monolithic'' classifier can be trained using all the available features to describe a pattern; (ii) subsets of features from the three groups described above can be used separately to train different classifiers whose outputs can be combined. Depending on the dimensionality of the feature space d and the size of the training set, one approach can outperform the other. In particular, a multiple classifier approach can be effective when the use of a ''monolithic'' classifier suffers from the ''curse of dimensionality'' problem, i.e., the training set n i is too small with respect to d <ref type="bibr" target="#b5">[6]</ref>. In this paper we propose to use, when needed, a MCS that consists of either two or three classifiers, depending on the module M i we consider. When a two-classifiers MCS is used, the module is implemented by training two classifiers on two different features subsets, namely I [ C and I [ T. On the other hand, when a three-classifiers MCS is used, the module is implemented by training a classifier on each single subset of features, namely one classifier is trained by using the subset I, one by using C and one by using T (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ensembles of unsupervised intrusion detection techniques</head><p>This section is divided into two parts. The first part introduces one-class classification concepts and outlines the techniques used to perform unlabeled intrusion detection, whereas the second part describes the techniques used to combine one-class classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">One-class classification</head><p>One-class classification (also referred to as outlier detection) techniques are particularly useful in those two-class problems where one of the classes of objects is well-sampled, whereas the other one is severely undersampled due to the fact that it is too difficult or expensive to obtain a significant number of training patterns. The goal of one-class classification is to distinguish between a set of target objects and all the other possible objects, referred as outliers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. A number of one-class classification techniques have been proposed in the literature. Following the categorization of one-class classifiers proposed by Tax <ref type="bibr" target="#b25">[26]</ref>, they can be subdivided into three groups, namely density methods, boundary methods and reconstruction methods.</p><p>We decided to use one classification method from each category to implement the service-specific MCS modules described in Section 3.4 in order to compare different approaches that showed good results in other applications. In particular, we chose the Parzen density estimation <ref type="bibr" target="#b5">[6]</ref> from the density methods, the m-SVC <ref type="bibr" target="#b23">[24]</ref> from the boundary methods and the k-means algorithm <ref type="bibr" target="#b10">[11]</ref> from the reconstruction methods. These one-class classifiers exhibited good performance on a number of applications <ref type="bibr" target="#b25">[26]</ref>. Besides, the output of the k-means and m-SVC classifiers can be redefined as class-conditional probability density functions, so that they can be correctly combined with the output of the Parzen classifier (see Section 4.2). We also trained the clustering technique proposed by Eskin et al. <ref type="bibr" target="#b7">[8]</ref> in order to compare the results of the combination of ''standard'' pattern recognition techniques with an algorithm tailored to the unlabeled intrusion detection problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Parzen density estimation</head><p>The Parzen-window approach <ref type="bibr" target="#b5">[6]</ref> can be used to estimate the density of the target objects distribution</p><formula xml:id="formula_5">pðxjx t Þ ¼ 1 n X n i¼1 1 h d u x À x i h<label>ð4Þ</label></formula><p>where n is the total number of training patterns belonging to the target class x t , x i is the ith training pattern, u is a kernel function, h is the width of the Parzen-window and p(xjx t ) is the estimated class-conditional probability density distribution. When the Gaussian kernel</p><formula xml:id="formula_6">uðxÞ ¼ 1<label>ð2pÞ</label></formula><formula xml:id="formula_7">d 2 exp À 1 2 kxk 2<label>ð5Þ</label></formula><p>is used, p(xjx t ) can be written as</p><formula xml:id="formula_8">pðxjx t Þ ¼ 1 n X n i¼1 1<label>ð2psÞ</label></formula><formula xml:id="formula_9">d=2 exp À kx À x i k 2 2s ! ; s ¼ h 2<label>ð6Þ</label></formula><p>and the one-class Parzen classifier can be obtained by simply setting a threshold h whereby a pattern z is rejected (i.e., deemed an outlier) if p(zjx t ) &lt; h <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">k-means</head><p>The k-means classifier is based on the well-known kmeans clustering algorithm <ref type="bibr" target="#b10">[11]</ref>. The algorithm identifies k clusters in the data by iteratively assigning each pattern to the nearest cluster. This algorithm can be used as a one-class classifier by clustering the training set and then computing the distance d(z, x t ) of a test pattern z from the target distribution x t as dðz;</p><formula xml:id="formula_10">x t Þ ¼ min i¼1...k kz À l i k ð<label>7Þ</label></formula><p>where l i represents the ith cluster center. If the distance is larger than a threshold h the pattern will be rejected <ref type="bibr" target="#b25">[26]</ref>. It is hard to map the distance d(z, x t ) into a probability density distribution and thus the combination of the k-means one-class classifier with density-based classifiers (e.g., the Parzen classifier) may produce unreliable results, as will be explained in Section 4.2. In order to allow this algorithm to produce an output that can be interpreted as a probability density function, we propose to use all the k distances between the test pattern z and the centroids l i as follows: In other words, we model the distribution of the target class by a mixture of k normal densities, each one centred on a centroid l i . An heuristic is used to compute s as the average distance between the k centroids. As for the oneclass Parzen classifier, the one-class k-means classifier based on Eq. ( <ref type="formula" target="#formula_12">8</ref>) can be obtained by setting a threshold h whereby a pattern z is rejected if p(zjx t ) &lt; h. It is worth noting that the same number of distances kz À l i k have to be computed both in <ref type="bibr" target="#b6">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>. Besides, the number of centroids is in general chosen to be low, therefore s can be efficiently computed. This means that the proposed probability density estimate does not add appreciable complexity to the classifier.</p><formula xml:id="formula_11">pðxjx t Þ ¼ 1 k X k i¼1 1<label>ð2psÞ</label></formula><formula xml:id="formula_12">d=2 exp À kx À l i k 2 2s ! s ¼ avg i;j kl i À l j k; i; j ¼ 1; 2; . . . ; k<label>ð8Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">m-SVC</head><p>The m-SVC classifier was proposed by Scho ¨lkopf et al. in <ref type="bibr" target="#b23">[24]</ref> and is inspired by the Support Vector Machine classifier proposed by Vapnik <ref type="bibr" target="#b27">[28]</ref>. The one-class classification problem is formulated to find an hyperplane that separates a desired fraction of the training patterns from the origin of the feature space F. This hyperplane cannot always be found in the original feature space, thus a mapping function U : F ! F 0 , from F to a kernel space F 0 , is used. In particular, it can be proven that when the Gaussian kernel</p><formula xml:id="formula_13">Kðx; yÞ ¼ UðxÞ Á UðyÞ ¼ exp À kx À yk 2 2s !<label>ð9Þ</label></formula><p>is used, it is always possible to find a hyperplane that solves the separation problem. The problem is formulated as follows:</p><formula xml:id="formula_14">min w;n;q 1 2 kwk 2 À q þ 1 ml X i n i ! w Á /ðx i Þ P q À n i ; n i P 0; 8i ¼ 1; . . . ; l<label>ð10Þ</label></formula><p>where w is a vector orthogonal to the hyperplane, m represents the fraction of training patterns that are allowed to be rejected (i.e., that are not separated from the origin by the hyperplane), x i is the ith training pattern, l is the total number of training patterns, n = [n 1 , . . . , n l ] T is a vector of slack variables used to ''penalize'' the rejected patterns, q represents the margin, i.e., the distance of the hyperplane from the origin. The solution of (10) brings to the decision function, for a generic test pattern z, formulated as</p><formula xml:id="formula_15">f svc ðzÞ ¼ I X i a i Kðx i ; zÞ P q ! ; X l i¼1 a i ¼ 1<label>ð11Þ</label></formula><p>where I is the indicator function <ref type="foot" target="#foot_2">3</ref> and the parameters a i and q are provided by the solution of <ref type="bibr" target="#b9">(10)</ref>. According to <ref type="bibr" target="#b10">(11)</ref>, a pattern z is either rejected if f svc (z) = 0 or accepted as target object if f svc (z) = 1. When the Gaussian kernel ( <ref type="formula" target="#formula_13">9</ref>) is used, the output of the m-SVC can be formulated in terms of a class-conditional probability by</p><formula xml:id="formula_16">pðxjx t Þ ¼ 1 ð2p Á sÞ d 2 X n i¼1 a i Á Kðx; x i Þ ¼ X n i¼1 a i 1 ð2p Á sÞ d 2 Á e À 1 2 kx À x i k 2 s<label>ð12Þ</label></formula><formula xml:id="formula_17">which respects the constraint R R d pðxjx t Þ dx ¼ 1.</formula><p>It is worth noting that in general only a small number of coefficients a i will be different from zero, thus p(xjx t ) can be efficiently computed. The training patterns x i whereby the related a i 5 0 represent the support vectors for the m-SVC. The acceptance threshold can be rewritten as</p><formula xml:id="formula_18">q 0 ¼ q ð2p Á sÞ d 2<label>ð13Þ</label></formula><p>so that a pattern z will be considered an outlier if p(zjx t ) &lt; q 0 . It is worth noting that Tax et al. <ref type="bibr" target="#b26">[27]</ref> independently formulated a SVM-based one-class classifier whose solution is identical to the one of the m-SVC when the Gaussian kernel is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Combining one-class classifiers</head><p>Traditional pattern classifiers can be combined by using many different combination rules and methods <ref type="bibr" target="#b13">[14]</ref>. Among the combination rules, the min, max, mean and product rules <ref type="bibr" target="#b12">[13]</ref> are some of the most commonly used. These combination rules can be easily applied when the output of the classifiers can be viewed as an a posteriori probability P i (x j jx), where p i refers to the output of the i-classifier, whereas x j is the j-class of objects. In case of a two-class problem, the a posteriori probability can be written as</p><formula xml:id="formula_19">P i ðx j jxÞ ¼ p i ðxjx j ÞP ðx j Þ p i ðxÞ ¼ p i ðxjx j ÞP ðx j Þ p i ðxjx 1 ÞP ðx 1 Þ þ p i ðxjx 2 ÞP ðx 2 Þ ; j ¼ 1; 2; i ¼ 1; . . . ; L<label>ð14Þ</label></formula><p>where L is the number of classifiers. Unfortunately, in case of one-class classifiers in general it is not possible to reliably estimate the probability distribution of one of the two classes, namely the probability density of the outlier objects (i.e., one of the terms in the denominator in ( <ref type="formula" target="#formula_19">14</ref>)). Tax et al. <ref type="bibr" target="#b24">[25]</ref> proposed to consider the distribution of the outlier to be constant in a suitable region of the feature set, so that the a posteriori probability for the target class, for example, can be approximated as</p><formula xml:id="formula_20">P i ðx t jxÞ ¼ p i ðxjx t ÞP ðx t Þ p i ðxjx t ÞP ðx t Þ þ h i Á P ðx o Þ ; i ¼ 1; . . . ; L<label>ð15Þ</label></formula><p>where x t represents the target class, x o represent the outlier class and h i is the uniform density distribution assumed for the outlier patterns. Let's consider now the traditional mean combination rule. We need to compute</p><formula xml:id="formula_21">lðx t jxÞ ¼ 1 L X L i¼1 P i ðx t jxÞ lðx o jxÞ ¼ 1 L X L i¼1 P i ðx o jxÞ<label>ð16Þ</label></formula><p>and the decision criterion is</p><formula xml:id="formula_22">lðx t jxÞ &lt; lðx o jxÞ ) x is an outlier<label>ð17Þ</label></formula><p>If we assume p i (x) ' p(x) "i, we can write</p><formula xml:id="formula_23">lðx j jxÞ ¼ 1 L X L i¼1 p i ðxjx j Þ Á P ðx j Þ pðxÞ ¼ 1 L P ðx j Þ pðxÞ X L i¼1 p i ðxjx j Þ ð<label>18Þ</label></formula><p>where j = t, o (i.e., ( <ref type="formula" target="#formula_23">18</ref>) is applied to both the target and the outlier class). In this case we can compute</p><formula xml:id="formula_24">y avg ðxÞ ¼ 1 L X L i¼1 p i ðxjx t Þ ð<label>19Þ</label></formula><formula xml:id="formula_25">h 0 ¼ P ðx o Þ P ðx t Þ Á 1 L X L i¼1 h i<label>ð20Þ</label></formula><p>and the decision criterion (17) becomes simply</p><formula xml:id="formula_26">y avg ðxÞ &lt; h 0 ) x is an outlier<label>ð21Þ</label></formula><p>which means that we can combine the class-conditional probability density functions, instead of the a posteriori probabilities estimated by each classifier. The obtained y avg (x) can be used as a standard one-class classifier output and the threshold h 0 can be independently tuned to attain the desired trade-off between false positives (i.e., target objects classified as outliers) and false negatives (i.e., outliers classified as belonging to the target class). This approach is (almost) exactly like the one proposed in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> and can be extended to the min, max and product rules. Another approach is to estimate P i (x t jx) and P i (x o jx) so that the decision criterion ( <ref type="formula" target="#formula_22">17</ref>) can be used directly. For each one-class classifier i we have</p><formula xml:id="formula_27">P i ðx t jxÞ ¼ p i ðxjx t ÞP ðx t Þ p i ðxjx t ÞP ðx t Þ þ h i Á P ðx o Þ P i ðx o jxÞ ¼ h i Á P ðx o Þ p i ðxjx t ÞP ðx t Þ þ h i Á P ðx o Þ<label>ð22Þ</label></formula><p>and, setting s i ¼ h i Á P ðxoÞ P ðxtÞ , the decision criterion for the classifier i can be written as</p><formula xml:id="formula_28">p i ðxjx t Þ &lt; s i ) x is an outlier<label>ð23Þ</label></formula><p>It is worth noting that s i represents the decision threshold applied on the output of classifier i. According to <ref type="bibr" target="#b21">(22)</ref> we can write</p><formula xml:id="formula_29">P i ðx t jxÞ ¼ p i ðxjx t Þ p i ðxjx t Þ þ s i ; i ¼ 1; . . . ; L<label>ð24Þ</label></formula><formula xml:id="formula_30">P i ðx o jxÞ ¼ s i p i ðxjx t Þ þ s i ; i ¼ 1; . . . ; L<label>ð25Þ</label></formula><p>In practice, we can set the thresholds s i so that a given rejection rate is produced by each single one-class classifier. Once the thresholds s i , i = 1,. . . , L, have been set, the posterior probabilities can be estimated using ( <ref type="formula" target="#formula_29">24</ref>) and ( <ref type="formula" target="#formula_30">25</ref>), and the rule ( <ref type="formula" target="#formula_22">17</ref>) can be applied. This approach can be extended to the min, max and product rules by computing l(x t jx) and l(x o jx) according to the new rule and then applying <ref type="bibr" target="#b16">(17)</ref>.</p><p>As mentioned in Section 4.1, it is not possible to directly make use of the output of one-class classifiers that implement boundary or reconstruction methods in <ref type="bibr" target="#b18">(19)</ref>, <ref type="bibr" target="#b23">(24)</ref> and <ref type="bibr" target="#b24">(25)</ref>. In order to solve this problem, Tax et al. <ref type="bibr" target="#b24">[25]</ref> proposed an heuristic approach to map the output of ''distance-based'' classifiers to a probability estimate</p><formula xml:id="formula_31">e P ðxjx t Þ ¼ exp À qðxjx t Þ s<label>ð26Þ</label></formula><p>where q(xjx t ) is the output to be mapped (e.g., q(xjx t ) = min i=1. . .k kx À l i k, if the k-means classifier is considered). However, in general e P does not respect the integral constraint for a density probability distribution, whereby Z</p><formula xml:id="formula_32">R d e P ðxjx t Þ dx 6 ¼ 1<label>ð27Þ</label></formula><p>This fact may produce some problems, especially when the output of ''distance-based'' one-class classifiers is combined with density-based classifiers (e.g., the Parzen classifier described in Section 4.1.1), which respect the integral constraint by definition. On the other hand, the methods proposed in Section 4.1 to compute the output of the k-means and m-SVC classifiers do not suffer from this problem and the decision criterion ( <ref type="formula" target="#formula_26">21</ref>) can be used without further output transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>Experiments were carried out on a subset of the DARPA 1998 dataset distributed as part of the UCI KDD Archive (http://kdd.ics.uci.edu/databases/kdd-cup99/kddcup99.html). The DARPA 1998 dataset was created by the MIT Lincoln Laboratory group in the framework of the 1998 Intrusion Detection Evaluation Program (http://www.ll.mit.edu/IST/ideval). This dataset was obtained from the network traffic produced by simulating the computer network of an air-force base. It consists of seven weeks of traffic data for training purposes, and two weeks of data for testing. A subset of the traffic of the DARPA 1998 dataset has been mapped to a pattern recognition problem and distributed as the KDD-Cup 1999 dataset. The training set is made of 494,020 patterns, and the test set contains 311,029 patterns. Each pattern represents a network connection described by a 41-dimensional feature vector according to the set of features illustrated in Section 3. In particular, 9 features were of the intrinsic type, 13 features were of the content type, and the remaining 19 features were of the traffic type. Each pattern of the data set is labeled as belonging to one out of five classes, namely normal traffic and four different classes of attack: Probe, Denial of Service (DoS), Remote to Local (R2L), and User to Root (U2R). The attacks belonging to a certain attack class are designed to attain the same effect by exploiting different vulnerabilities of the computer network.</p><p>The DARPA dataset has been widely criticized <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. The main criticism is related to the fact that the traffic traces reported in the dataset are not representative of a real network scenario. In particular, it is worth noting that the prior probabilities of the attack classes included in the DARPA 1998 dataset (and thus in the KDD-Cup 1999) cannot be considered representative of the traffic in a real network. This fact has been clearly pointed out in a critique of the DARPA corpus of data by McHugh <ref type="bibr" target="#b20">[21]</ref>. Although this dataset has been criticized, it is currently used by researchers because it is the only reference dataset that allows the designers to compare results obtained using different intrusion detection techniques.</p><p>In order to perform experiments with unlabeled intrusion detection techniques, we removed the labels of all the training patterns to simulate the unlabeled collection of network traffic. Besides, we relabeled patterns of the test set as belonging either to the normal traffic class or to the ''generic'' attack class, thus discarding the different labels assigned to attack patterns related to different classes of attack. Nominal features have been converted into numerical values according to the procedure in <ref type="bibr" target="#b7">[8]</ref>. Given a nominal feature with N possible distinct values, it is mapped to a vector of length N, i.e., the vector contains one coordinate for every possible value of the feature. When a particular value of the feature is mapped on the vector, the coordinate corresponding to the value of the feature is set to 1/N, whereas the other coordinates (i.e., the ones corresponding to the other N À 1 possible values for the feature) are set to zero.</p><p>According to the description of the modular architecture presented in Section 3, we divided the traffic of the data set into six subsets, each one related to ''similar'' services: HTTP, containing the traffic related to the HTTP protocol; FTP, containing the traffic related to the control flow and data flow for the FTP protocol, and the traffic related to the TFTP protocol; Mail, containing the traffic related to the SMTP, POP2, POP3, NNTP, and IMAP4 protocols; ICMP, containing the traffic related to the ICMP protocol; Private &amp; Other, containing the traffic related to TCP/UDP ports higher than 49,152; Miscellaneous, containing all the remaining traffic. For each module, the features taking a constant value for all patterns have been discarded, provided that these features have a constant value by ''definition'' for that service, and not by chance. For example, the intrinsic feature ''protocol_type'' is always constant and equal to the value ''TCP'' for the http, and mail services, thus for those services it can be discarded. As a result, for each module we used a subset of the 41 available features, namely: 29 features for the HTTP module; 34 features for the FTP module; 16 features for the ICMP module (in particular, the content features were discarded as they have no meaning for the ICMP traffic); 31 features for the Mail module; 37 features for the Miscellaneous module; 29 features for the Private &amp; Other module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training set undersampling</head><p>As mentioned above, the prior probabilities of the attack classes in the training portion of the KDD-Cup 1999 dataset cannot be considered representative of the traffic in a real network. The analysis of the training set confirmed that it contained a large fraction of attacks compared to normal traffic patterns, as show in Table <ref type="table" target="#tab_0">1</ref>. This violates the first assumption behind unlabeled techniques, i.e., connections containing attacks should account for a small portion of the network traffic. As typical network traffic satisfies this assumption <ref type="bibr" target="#b21">[22]</ref>, we filtered the training set so that the selected data satisfied this assumption. To this end, for each service we retained all the normal connections, while we sampled the attack patterns so that they accounted for 1.5% of the total traffic. This sampling procedure is similar to the one performed by other researchers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref>. Let us recall that each attack class is made up of connections related to a number of different attack types, each attack type designed to produce the same effect of all the attacks in the same class. For each type of attack, a different number of patterns is available because each attack type produces a different number of connections, and because of the simulations carried out during the DARPA programme. A number of techniques can be used to sample a set of data such that the resulting subset is representative of the whole data <ref type="bibr" target="#b0">[1]</ref>.</p><p>In the experiments reported in this paper we reduced the percentage of attacks by reducing the number of those attacks accounting for a number of connections larger than 973, which is 1% of the total normal connections. In particular we proceeded as follows:</p><p>(a) 10 subsets, each one containing 101 patterns, are extracted randomly from each attack type (this ''magic'' number was chosen in order to attain a total percentage of attacks equal to 1.5%); (b) for each subset, we trained a m-SVC classifier, and computed the error attained by using the remaining patterns of that attack type as a test set; (c) the subset with the smallest error is selected, as it can be considered representative of the entire set of available connections for that attack type.</p><p>Table <ref type="table">2</ref> shows the composition of the training set obtained after the preprocessing phase. It can be observed that attacks are not distributed uniformly among different services. While the overall percentage of attacks has been reduced so that it is equal to 1.5% of all the training traffic, the percentages of attacks related to different services range from the 0.17% of the HTTP and Mail traffic, to the 30.34% of the ICMP traffic. The high percentage of attacks in the ICMP traffic can be explained by observing that the available training set contained a very small number of normal ICMP connections compared to attacks, so that the proposed reduction of the number of attack patterns left the ICMP traffic data unbalanced.</p><p>It is worth noting that the distribution of traffic reported in Table <ref type="table">2</ref> was used to compute the prior probabilities related to the different modules of the IDS, according to the discussion in Section 3.3.</p><p>Table <ref type="table">3</ref> shows the composition of the test set. As shown in the table, the test set contains a very large fraction of attacks, as it was designed to test the performance of IDS and not to be representative of a realistic network traffic. It is worth noting that we did not apply any changes on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance evaluation</head><p>We divided the performance evaluation experiments into two phases. In the first phase, we evaluated the performance of one module of the IDS at a time. In particular, for each module the performance of a ''monolithic'' classifier is compared to the performance attained by combining classifiers trained on distinct feature subsets (see Section 3.4). In the second phase, the modules related to different services are combined, and the performance of the overall IDS is evaluated. Performance evaluation has been carried out by ROC curve analysis, i.e., by computing the detection rate as a function of the false alarm rate. Different ROC can be compared by computing the Area Under the Curve (AUC). AUC measures the average performance of the related classifier, so that the larger the value of AUC of a classifier the higher the performance <ref type="bibr" target="#b25">[26]</ref>. It is worth noting that AUC usually measures the average performance of classifiers considering the entire range of variation of the false positive rate. For some ranges of the false alarm rate the classifier with the smallest AUC value may provide the highest detection rate. Therefore, it may be better to measure the AUC in the interval [0, a], where a &lt; 1 represents the maximum expected false positive rate. However, it is not always possible to know in advance the working point (or the set of possible working points) on the ROC curve that will be actually used during the operational phase. Moreover, in our application the overall false positive rate is ''distributed'' in different percentages on different modules in order to optimize the performance of the IDS (see Section 3.2). In these cases of unknown a the AUC measured in the interval [0, 1] is a valuable indicator of the performance of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Evaluation of service-specific modules</head><p>The first phase of the performance evaluation consisted of three experiments for each of the six modules. The first experiment was designed to assess the performance of individual one-class classifiers, i.e., the m-SVC, the k-means, and the Parzen classifier when the patterns are described by using the entire set of available features F. The performance of the clustering algorithm described in <ref type="bibr" target="#b7">[8]</ref> have been also computed for comparison purposes. The second experiment was designed to assess the performance attained by combining classifiers trained on two distinct feature subsets, i.e., the subset of intrinsic and traffic features I [ T, and the subset made of intrinsic and content features I [ C (see Section 3.4). In particular, each classifier has been trained using the two feature subsets, and then they have been combined by using four different combination rules, i.e., the max rule, the min rule, the mean rule, and the product rule. The third experiment was designed to assess the performance attained by combining classifiers trained on three distinct feature subsets, i.e., the intrinsic features I, the traffic features T, and the content features C (see Section 3.4) by using again four different combination rules.</p><p>When combining classifiers trained on different feature spaces, we used both the combination approaches described in Section 4.2. We noted that for the m-SVC, the k-means, and the clustering algorithm proposed in <ref type="bibr" target="#b7">[8]</ref>, the best performance was obtained by estimating the posterior probabilities for the target class as in <ref type="bibr" target="#b23">(24)</ref> and then comparing these probabilities to a varying threshold in order to compute the ROC curves. For the Parzen classifier, the combination of class-conditional probabilities, using <ref type="bibr" target="#b18">(19)</ref> and the decision criteria <ref type="bibr" target="#b20">(21)</ref>, produced the best results.</p><p>In the following, we discuss the results obtained by applying the best combination approach for each single classifier. Therefore, we present the results obtained by combining the posterior probabilities for the m-SVC, the k-means, and the clustering algorithm, and the results obtained by combining the class-conditional probabilities for the Parzen classifier.</p><p>Tables <ref type="table" target="#tab_1">4</ref><ref type="table">5</ref><ref type="table">6</ref><ref type="table" target="#tab_2">7</ref>summarize the performance results on the test set in terms of AUC, for the m-SVC, the k-means, the Parzen classifier, and the clustering algorithm proposed in <ref type="bibr" target="#b7">[8]</ref>, respectively. For each algorithm, the parameters have been tuned on the training set. It is worth noting that in the case of the ICMP protocol only intrinsic and traffic features were available, thus only the third kind of experiment could be performed by combining two one-class classifiers trained on intrinsic and traffic features, respectively. The obtained results are discussed in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Evaluation of overall IDS</head><p>In order to analyze the performance of the overall IDS, we built three systems:</p><p>(1) An ''optimal'' system made up, for each module, of the classification techniques that provided the highest value of AUC, according to Table <ref type="table" target="#tab_3">8</ref>. (2) A system made up of one ''monolithic'' m-SVC for each module. We chose to use m-SVC classifiers because on average they provide better results than the other considered classifiers, as discussed in Section 5.3. (3) As in the second system, we chose to use m-SVC classifiers. Then, for each module we chose between a ''monolithic'' versus a MCS approach, according to best performance results reported in Table <ref type="table" target="#tab_1">4</ref>. It is worth noting that for the Miscellaneous module the performance of the ''monolithic'' classifier is really close to the best performance result. Therefore, it is difficult to conclude which approach really performs better than the other. We chose to construct a system made up of one ''monolithic'' m-SVC for the HTTP, Mail, Miscellaneous and Private &amp; Other modules, and a MCS for the FTP and ICMP modules (we will further discuss the motivation for this choice in Section 5.3). For the FTP module we used an MCS constructed by using three m-SVC classifiers, namely one trained on the subset of features I, one on the subset C and one on the subset T. For the ICMP module we constructed a MCS using two m-SVC classifiers, namely one trained on the subset of features I and one on the subset T. In particular, for the FTP module, the min rule was used, whereas the max rule was used for the ICMP module.</p><p>In order to evaluate the performance of the three IDS systems, we computed some working points according to the heuristic proposed in Section 3.3. The attained results are reported in Table <ref type="table">9</ref>. The motivation for the choice of the three proposed IDS systems and the attained results are further discussed in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Discussion</head><p>The results reported in Section 5.2.1 clearly show that the m-SVC algorithm provides the highest AUC value for all services, when classifiers are trained using all the available features. The difference between the performance of m-SVC and that of the other algorithms is very small in the case of the HTTP and the Miscellaneous traffic, while it is larger for the other services.</p><p>Tables <ref type="table" target="#tab_1">4</ref><ref type="table">5</ref><ref type="table">6</ref><ref type="table" target="#tab_2">7</ref>show that combining classifiers trained on distinct feature sets does not always improve performance, with respect to those attained by classifiers trained on the entire feature set. In particular, it can be seen that for the m-SVC, k-means, and Parzen classifiers, the use of distinct feature sets clearly outperforms the use of the entire feature set F only for the FTP, and ICMP modules. In the case of the clustering algorithm, the use of distinct feature sets clearly outperforms the use of the entire feature set F only for the Mail, ICMP, and Private &amp; Others modules. In all other cases the differences in performance are small, thus the superiority of one technique against the others cannot be concluded. Unfortunately, results show no regularity. For this reason, it is difficult to explain the behavior of different classifiers and combination rules on different modules. On the other hand, results clearly show that each module should be carefully and independently designed by making a decision about the classification algorithm to be used, and by choosing between an individual classification technique and the MCS approach.</p><p>Summing up, reported results allow us to conclude that the m-SVC algorithm performs better than the other ones, on average. Further, it is easy to see that the combination of distinct feature representations usually provides significantly higher performance, with respect to just one classifier trained on the entire feature set, only for the FTP and ICMP modules. These observations have been used in Section 5.2.2, where three different overall IDS made up of six modules are described.</p><p>In order to compare the performance of the modular systems proposed in Section 5.2.2 to the approach used by Eskin et al. <ref type="bibr" target="#b7">[8]</ref>, we trained the clustering algorithm proposed in <ref type="bibr" target="#b7">[8]</ref> and the m-SVC on the entire training set obtained after subsampling. It is worth noting that this approach is the same used in <ref type="bibr" target="#b7">[8]</ref>. Besides, our test set is the same as the one used in <ref type="bibr" target="#b7">[8]</ref>, and we also used an approach similar to the one proposed in <ref type="bibr" target="#b7">[8]</ref> to adjust the training dataset. The performance results obtained on the test set are reported in Tables <ref type="table" target="#tab_4">10</ref> and<ref type="table" target="#tab_0">11</ref>, respectively. It is easy to see that if the false alarm rate is set to 1%, the algorithms trained on the entire training set provide a detection rate near 18%, while the proposed modular approaches provide detection rates from 67% to 79% (see Table <ref type="table">9</ref>). As the effectiveness of IDS depends on the capa-bility of providing high detection rates at small false alarms rates, the proposed modular approaches are very effective compared to the ''monolithic'' approaches. At 4% false alarm rate, the ''monolithic'' clustering algorithm provides better results than the modular approaches, in terms of detection rates. However, for higher false positive rates, the clustering algorithm does not provide performance improvements, whereas the proposed modular IDS reaches definitely better detection rates with respect to the ones obtained at low false positive rates. It is worth noting that, from a practical point of view, the working point of anomaly detectors are usually tuned to produce a low false alarm rate (e.g., equal to 1% or lower). Reported results clearly show that the proposed modular approach outperforms the ''monolithic'' approaches in the range of low false positive rates, due to its capability of allowing different false positive rates on different modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper we have proposed a modular solution to the Unsupervised Anomaly Detection problem. In particular, one-class classifiers have been considered for designing each module. In addition, the combination of one-class classifiers trained on distinct features have been also considered. As the combination of one-class classifier is a new and not completely explored research area, we proposed a technique aimed at mapping the outputs of one-class classifiers to density functions. The proposed framework allowed us to effectively combine the outputs of different classifiers using well-known fixed combination techniques developed for multi-class classifiers. We also proposed a heuristic designed to compute the false alarm rate for each module, given the maximum allowed overall false alarm rate. Attained results clearly show that subdividing the problem into different modules allows us to accurately model the traffic of each service, as for small values of false alarm rates the proposed systems attained high detection rates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Modular architecture.</figDesc><graphic coords="4,109.24,536.59,368.55,194.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Feature subsets for service-specific MCS.</figDesc><graphic coords="6,78.06,70.12,425.25,121.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Composition of the training set before the undersampling phase</figDesc><table><row><cell></cell><cell>HTTP</cell><cell>FTP</cell><cell>Mail</cell><cell>ICMP</cell><cell>Private &amp; Others</cell><cell>Miscellaneous</cell></row><row><cell>Normal</cell><cell>61,885 (96.55%)</cell><cell>4172 (78.16%)</cell><cell>9677 (99.83%)</cell><cell>1288 (0.46%)</cell><cell>12,998 (81.27%)</cell><cell>7257 (98.33%)</cell></row><row><cell>Attacks</cell><cell>2208 (3.45%)</cell><cell>1166 (21.84%)</cell><cell>16 (0.17%)</cell><cell>28,1250 (99.54%)</cell><cell>2996 (18.73%)</cell><cell>123 (1.67%)</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Composition of the training set after the undersampling phase</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>HTTP</cell><cell>FTP</cell><cell>Mail</cell><cell>ICMP</cell><cell>Private &amp; Others</cell><cell>Miscellaneous</cell></row><row><cell>Normal</cell><cell>61,885 (99.83%)</cell><cell>4172 (96.60%)</cell><cell>9677 (99.83%)</cell><cell>1288 (69.66%)</cell><cell>12,998 (96.09%)</cell><cell>7257 (98.33%)</cell></row><row><cell>Attacks</cell><cell>106 (0.17%)</cell><cell>147 (3.40%)</cell><cell>16 (0.17%)</cell><cell>561 (30.34%)</cell><cell>529 (3.91%)</cell><cell>123 (1.67%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4</head><label>4</label><figDesc>Performance attained by the m-SVC classifier on the six modules in terms of AUC</figDesc><table><row><cell></cell><cell></cell><cell>HTTP</cell><cell>FTP</cell><cell>Mail</cell><cell>ICMP</cell><cell>Private &amp; Others</cell><cell>Miscellaneous</cell></row><row><cell>m-SVC</cell><cell>F</cell><cell>0.995</cell><cell>0.894</cell><cell>0.971</cell><cell>0.862</cell><cell>0.992</cell><cell>0.987</cell></row><row><cell>m-SVC -max rule</cell><cell>I [ C + I [ T</cell><cell>0.956</cell><cell>0.918</cell><cell>0.960</cell><cell>-</cell><cell>0.911</cell><cell>0.975</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.807</cell><cell>0.566</cell><cell>0.956</cell><cell>0.929</cell><cell>0.918</cell><cell>0.939</cell></row><row><cell>m-SVC -min rule</cell><cell>I [ C + I [ T</cell><cell>0.948</cell><cell>0.967</cell><cell>0.855</cell><cell>-</cell><cell>0.921</cell><cell>0.953</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.773</cell><cell>0.973</cell><cell>0.954</cell><cell>0.913</cell><cell>0.904</cell><cell>0.944</cell></row><row><cell>m-SVC -mean rule</cell><cell>I [ C + I [ T</cell><cell>0.952</cell><cell>0.962</cell><cell>0.970</cell><cell>-</cell><cell>0.957</cell><cell>0.965</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.865</cell><cell>0.972</cell><cell>0.953</cell><cell>0.879</cell><cell>0.921</cell><cell>0.988</cell></row><row><cell>m-SVC -product rule</cell><cell>I [ C + I [ T</cell><cell>0.951</cell><cell>0.961</cell><cell>0.857</cell><cell>-</cell><cell>0.919</cell><cell>0.963</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.865</cell><cell>0.971</cell><cell>0.953</cell><cell>0.879</cell><cell>0.921</cell><cell>0.945</cell></row><row><cell cols="3">For each module, the best performance is reported in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Performance attained by the k-means classifier on the six modules in terms of AUC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>HTTP</cell><cell>FTP</cell><cell>Mail</cell><cell>ICMP</cell><cell>Private &amp; Others</cell><cell>Miscellaneous</cell></row><row><cell>k-means</cell><cell>F</cell><cell>0.978</cell><cell>0.820</cell><cell>0.899</cell><cell>0.736</cell><cell>0.918</cell><cell>0.955</cell></row><row><cell>k-means -max rule</cell><cell>I [ C + I [ T</cell><cell>0.864</cell><cell>0.874</cell><cell>0.926</cell><cell>-</cell><cell>0.917</cell><cell>0.974</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.872</cell><cell>0.335</cell><cell>0.930</cell><cell>0.913</cell><cell>0.917</cell><cell>0.889</cell></row><row><cell>k-means -min rule</cell><cell>I [ C + I [ T</cell><cell>0.353</cell><cell>0.830</cell><cell>0.826</cell><cell>-</cell><cell>0.903</cell><cell>0.909</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.814</cell><cell>0.926</cell><cell>0.630</cell><cell>0.750</cell><cell>0.907</cell><cell>0.284</cell></row><row><cell>k-means -mean rule</cell><cell>I [ C + I [ T</cell><cell>0.859</cell><cell>0.778</cell><cell>0.913</cell><cell>-</cell><cell>0.965</cell><cell>0.932</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.961</cell><cell>0.850</cell><cell>0.929</cell><cell>0.740</cell><cell>0.920</cell><cell>0.947</cell></row><row><cell>k-means -product rule</cell><cell>I [ C + I [ T</cell><cell>0.858</cell><cell>0.777</cell><cell>0.913</cell><cell>-</cell><cell>0.965</cell><cell>0.932</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.965</cell><cell>0.851</cell><cell>0.929</cell><cell>0.740</cell><cell>0.920</cell><cell>0.951</cell></row><row><cell cols="3">For each module, the best performance is reported in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Performance attained by the Parzen classifier on the six modules in terms of AUC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>HTTP</cell><cell>FTP</cell><cell>Mail</cell><cell>ICMP</cell><cell>Private &amp; Others</cell><cell>Miscellaneous</cell></row><row><cell>Parzen</cell><cell>F</cell><cell>0.977</cell><cell>0.878</cell><cell>0.932</cell><cell>0.743</cell><cell>0.921</cell><cell>0.982</cell></row><row><cell>Parzen -max rule</cell><cell>I [ C + I [ T</cell><cell>0.854</cell><cell>0.904</cell><cell>0.568</cell><cell>-</cell><cell>0.905</cell><cell>0.900</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.858</cell><cell>0.368</cell><cell>0.581</cell><cell>0.872</cell><cell>0.903</cell><cell>0.909</cell></row><row><cell>Parzen -min rule</cell><cell>I [ C + I [ T</cell><cell>0.987</cell><cell>0.868</cell><cell>0.940</cell><cell>-</cell><cell>0.921</cell><cell>0.974</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.982</cell><cell>0.914</cell><cell>0.940</cell><cell>0.704</cell><cell>0.864</cell><cell>0.698</cell></row><row><cell>Parzen -mean rule</cell><cell>I [ C + I [ T</cell><cell>0.854</cell><cell>0.904</cell><cell>0.828</cell><cell>-</cell><cell>0.991</cell><cell>0.900</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.858</cell><cell>0.867</cell><cell>0.582</cell><cell>0.872</cell><cell>0.910</cell><cell>0.909</cell></row><row><cell>Parzen -product rule</cell><cell>I [ C + I [ T</cell><cell>0.857</cell><cell>0.913</cell><cell>0.839</cell><cell>-</cell><cell>0.977</cell><cell>0.906</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.959</cell><cell>0.924</cell><cell>0.941</cell><cell>0.725</cell><cell>0.888</cell><cell>0.898</cell></row></table><note><p>For each module, the best performance is reported in bold.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 7</head><label>7</label><figDesc>Performance attained by the clustering algorithm proposed in<ref type="bibr" target="#b7">[8]</ref> on the six modules in terms of AUC</figDesc><table><row><cell></cell><cell></cell><cell>HTTP</cell><cell>FTP</cell><cell>Mail</cell><cell>ICMP</cell><cell>Private &amp; Others</cell><cell>Miscellaneous</cell></row><row><cell>Cluster</cell><cell>F</cell><cell>0.967</cell><cell>0.839</cell><cell>0.891</cell><cell>0.739</cell><cell>0.847</cell><cell>0.973</cell></row><row><cell>Cluster -max rule</cell><cell>I [ C + I [ T</cell><cell>0.965</cell><cell>0.705</cell><cell>0.949</cell><cell>-</cell><cell>0.843</cell><cell>0.253</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.740</cell><cell>0.478</cell><cell>0.949</cell><cell>0.918</cell><cell>0.390</cell><cell>0.141</cell></row><row><cell>Cluster -min rule</cell><cell>I [ C + I [ T</cell><cell>0.922</cell><cell>0.782</cell><cell>0.802</cell><cell>-</cell><cell>0.903</cell><cell>0.875</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.970</cell><cell>0.809</cell><cell>0.814</cell><cell>0.856</cell><cell>0.848</cell><cell>0.936</cell></row><row><cell>Cluster -mean rule</cell><cell>I [ C + I [ T</cell><cell>0.932</cell><cell>0.829</cell><cell>0.962</cell><cell>-</cell><cell>0.915</cell><cell>0.876</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.983</cell><cell>0.874</cell><cell>0.970</cell><cell>0.872</cell><cell>0.847</cell><cell>0.958</cell></row><row><cell>Cluster -product rule</cell><cell>I [ C + I [ T</cell><cell>0.924</cell><cell>0.802</cell><cell>0.802</cell><cell>-</cell><cell>0.903</cell><cell>0.875</cell></row><row><cell></cell><cell>I + C + T</cell><cell>0.980</cell><cell>0.809</cell><cell>0.814</cell><cell>0.872</cell><cell>0.947</cell><cell>0.943</cell></row><row><cell cols="3">For each module, the best performance is reported in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 8</head><label>8</label><figDesc>Summary of the best results in terms of AUC attained for each module.</figDesc><table><row><cell></cell><cell>HTTP</cell><cell>FTP</cell><cell>Mail</cell><cell>ICMP</cell><cell>Private &amp; Others</cell><cell>Miscellaneous</cell></row><row><cell>Best</cell><cell>m-SVC</cell><cell>m-SVC min rule</cell><cell>m-SVC</cell><cell>m-SVC max rule</cell><cell>Parzen min rule</cell><cell>m-SVC mean rule</cell></row><row><cell></cell><cell>F</cell><cell>I + C + T</cell><cell>F</cell><cell>I + T</cell><cell>I [ C + I [ T</cell><cell>I + C + T</cell></row><row><cell>Table 9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Results attained by the proposed three modular systems</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Best modules in terms of AUC</cell><cell>m-SVC</cell><cell></cell><cell cols="2">Best m-SVC modules in terms of AUC</cell></row><row><cell cols="2">False alarm rate (%)</cell><cell>Detection rate (%)</cell><cell>False alarm rate (%)</cell><cell>Detection rate (%)</cell><cell>False alarm rate (%)</cell><cell>Detection rate (%)</cell></row><row><cell>0.87</cell><cell></cell><cell>75.34</cell><cell>0.91</cell><cell>67.31</cell><cell>0.88</cell><cell>79.27</cell></row><row><cell>2.10</cell><cell></cell><cell>80.35</cell><cell>2.06</cell><cell>75.61</cell><cell>2.07</cell><cell>89.45</cell></row><row><cell>2.64</cell><cell></cell><cell>80.80</cell><cell>2.65</cell><cell>77.10</cell><cell>2.66</cell><cell>89.67</cell></row><row><cell>4.00</cell><cell></cell><cell>85.67</cell><cell>3.20</cell><cell>86.31</cell><cell>3.28</cell><cell>89.92</cell></row><row><cell>5.49</cell><cell></cell><cell>94.12</cell><cell>4.51</cell><cell>92.25</cell><cell>4.82</cell><cell>93.02</cell></row><row><cell>6.86</cell><cell></cell><cell>94.27</cell><cell>6.72</cell><cell>93.91</cell><cell>6.49</cell><cell>94.16</cell></row><row><cell>8.25</cell><cell></cell><cell>94.32</cell><cell>8.09</cell><cell>94.12</cell><cell>8.05</cell><cell>94.26</cell></row><row><cell>10.44</cell><cell></cell><cell>94.38</cell><cell>9.62</cell><cell>94.25</cell><cell>9.49</cell><cell>94.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 10</head><label>10</label><figDesc>Results attained by applying the ''monolithic'' approach using the clustering algorithm proposed in<ref type="bibr" target="#b7">[8]</ref> </figDesc><table><row><cell>Clustering</cell><cell></cell></row><row><cell>False alarm rate (%)</cell><cell>Detection rate (%)</cell></row><row><cell>1</cell><cell>18.37</cell></row><row><cell>2</cell><cell>26.80</cell></row><row><cell>3</cell><cell>27.21</cell></row><row><cell>4</cell><cell>92.21</cell></row><row><cell>5</cell><cell>92.24</cell></row><row><cell>6</cell><cell>92.25</cell></row><row><cell>7</cell><cell>92.25</cell></row><row><cell>8</cell><cell>92.29</cell></row><row><cell>9</cell><cell>92.29</cell></row><row><cell>10</cell><cell>92.68</cell></row><row><cell>Table 11</cell><cell></cell></row><row><cell cols="2">Results attained by applying the ''monolithic'' approach using the m-SVC</cell></row><row><cell>classifier</cell><cell></cell></row><row><cell>m-SVC</cell><cell></cell></row><row><cell>False alarm rate (%)</cell><cell>Detection rate (%)</cell></row><row><cell>1</cell><cell>17.91</cell></row><row><cell>2</cell><cell>66.44</cell></row><row><cell>3</cell><cell>78.40</cell></row><row><cell>4</cell><cell>78.85</cell></row><row><cell>5</cell><cell>86.07</cell></row><row><cell>6</cell><cell>92.53</cell></row><row><cell>7</cell><cell>92.57</cell></row><row><cell>8</cell><cell>92.60</cell></row><row><cell>9</cell><cell>92.63</cell></row><row><cell>10</cell><cell>92.91</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>G. Giacinto et al. / Information Fusion 9 (2008) 69-82</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In practice, if the network is already protected by ''standard'' security devices (e.g., firewall, signature-based IDS, etc.), we may be able to estimate P ai from historical data related to attacks to the network service i that occurred in the past. G. Giacinto et al. / Information Fusion 9 (2008) 69-82</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>I(x) = 1 if x is true, otherwise I(x) = 0.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study of the behavior of several methods for balancing machine learning training data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Monard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semisupervised learning of classifiers: theory, algorithms and their applications to human-computer interaction</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Cirelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1553" to="1567" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network intrusion detection by a multi-stage classification system</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Limongiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple Classifier Systems</title>
		<editor>
			<persName><surname>Roli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Windeatt</forename><surname>Kittler</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3077</biblScope>
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural network component for an intrusion detection system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Debar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Siboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Research in Security and Privacy</title>
		<meeting>the IEEE Symposium on Research in Security and Privacy<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="240" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An intrusion-detection model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Denning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="232" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Results of the KDD&apos;99 Classifier Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A geometric framework for unsupervised anomaly detection: detecting intrusions in unlabeled data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prerau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Portnoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Data Mining in Computer Security</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Barbara</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Jajodia</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A modular multiple classifier system for the detection of intrusions in computer networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Didaci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Workshop on Multiple Classifier Systems (MCS 2003)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Windeatt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</editor>
		<meeting><address><addrLine>Guildford, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="volume">2709</biblScope>
			<biblScope unit="page" from="346" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fusion of multiple classifiers for intrusion detection in computer networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Didaci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1795" to="1803" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The NIDES statistical component: Description and justification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Javits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valdes</surname></persName>
		</author>
		<idno>A010</idno>
		<imprint>
			<date type="published" when="1993-03">March 1993</date>
		</imprint>
		<respStmt>
			<orgName>SRI International, Computer Science Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">SRI Annual Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="229" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<title level="m">Combining Pattern Classifiers: Methods and Algorithms</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Framework for Constructing Features and Models for Intrusion Detection Systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information and System Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Information-theoretic measures for anomaly detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection in network intrusion detection using clusters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leckie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Australasian Computer Science Conference</title>
		<meeting>the 28th Australasian Computer Science Conference<address><addrLine>ACSC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The 1999 DARPA off-line intrusion detection evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Korba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="595" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An analysis of the 1999 DARPA/Lincoln Laboratory evaluation data for network anomaly detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Symposium on Recent Advances in Intrusion Detection</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Defending yourself: the role of intrusion detection systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="2000-09">2000. September/ October</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Testing intrusion detection systems: a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln Laboratory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mchugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information and System Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="262" to="294" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Intrusion detection with unlabeled data using clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Portnoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CSS Workshop on Data Mining</title>
		<meeting>ACM CSS Workshop on Data Mining</meeting>
		<imprint/>
	</monogr>
	<note>Applied to Security, DMSA-2001</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Practical Intrusion Detection Handbook</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Proctor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scho ¨lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining one-class classifiers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Multiple Classifier Systems</title>
		<meeting>Multiple Classifier Systems</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">One-class classification, concept learning in the absence of counter examples</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Delft, Netherland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Delft University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Testing network-based intrusion detection signatures using mutant exploits</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vigna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balzarotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the 11th ACM Conference on Computer and Communications Security (CCS)</title>
		<meeting><address><addrLine>Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-10">October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shield: vulnerability-driven network filters for preventing known vulnerability exploits</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zugenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM Conference</title>
		<meeting>the ACM SIGCOMM Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anomalous payload-based network intrusion detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Recent Advances on Intrusion Detection</title>
		<meeting>the International Symposium on Recent Advances on Intrusion Detection</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An unsupervised anomaly detection patterns learning algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCT 2003</title>
		<meeting>ICCT 2003</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning techniques for an intrusion detection system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zanero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Savaresi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM symposium on Applied computing</title>
		<meeting>the ACM symposium on Applied computing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
