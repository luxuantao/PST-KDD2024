<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrating Both Visual and Audio Cues for Enhanced Video Caption</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wangli</forename><surname>Hao</surname></persName>
							<email>haowangli2015@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Brain-inspired Intelligence</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
							<email>zhaoxiang.zhang@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Brain-inspired Intelligence</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">He</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Brain-inspired Intelligence</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Integrating Both Visual and Audio Cues for Enhanced Video Caption</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video caption refers to generating a descriptive sentence for a specific short video clip automatically, which has achieved remarkable success recently. However, most of the existing methods focus more on visual information while ignoring the synchronized audio cues. We propose three multimodal deep fusion strategies to maximize the benefits of visual-audio resonance information. The first one explores the impact on cross-modalities feature fusion from low to high order. The second establishes the visual-audio short-term dependency by sharing weights of corresponding front-end networks. The third extends the temporal dependency to long-term through sharing multimodal memory across visual and audio modalities. Extensive experiments have validated the effectiveness of our three cross-modalities fusion strategies on two benchmark datasets, including Microsoft Research Video to Text (MSRVTT) and Microsoft Video Description (MSVD). It is worth mentioning that sharing weight can coordinate visualaudio feature fusion effectively and achieve the state-of-art performance on both BELU and METEOR metrics. Furthermore, we first propose a dynamic multimodal feature fusion framework to deal with the part modalities missing case. Experimental results demonstrate that even in the audio absence mode, we can still obtain comparable results with the aid of the additional audio modality inference module.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Automatically describing video with natural sentences has potential applications in many fields, such as humanrobot interaction, video retrieval. Recently, benefiting from extraordinary abilities of convolutional neural networks (CNN) <ref type="bibr" target="#b14">(Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b16">Szegedy et al. 2015;</ref><ref type="bibr">2016)</ref>, recurrent neural networks (RNN) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997)</ref> and large paired video language description datasets <ref type="bibr" target="#b25">(Xu et al. 2016)</ref>, video caption has achieved promising successes.</p><p>Most video caption frameworks can be simply splitted into a encoder stage and a decoder stage respectively. Conditioned on a fixed length of visual feature representation offered by encoder, decoder can generate a corresponding video description recurrently. To generate a fixed length video representation, several methods are proposed, such as pooling over frames <ref type="bibr" target="#b19">(Venugopalan et al. 2014)</ref>, holistic video representations (Gua ; <ref type="bibr">Rohrbach et al. 2015;</ref><ref type="bibr">2013)</ref>, sub-sampling on a fixed number of input frames <ref type="bibr" target="#b26">(Yao et al. 2015)</ref> and extracting the last hidden state of recurrent visual feature encoder <ref type="bibr" target="#b20">(Venugopalan et al. 2015)</ref>.</p><p>Those feature encoding methods mentioned above are only based on visual cues. However, videos contain the visual modality and the audio modality. The resonance information underlying them is essential for video caption generation. We believe that the lack of arbitrary modality will result in the loss of information. For example, when a person is lying on the bed and singing a song, traditional video caption methods may generate an incomplete sentence, like "a person is lying on the bed", which may due to the loss of resonance information underling audio modality. If audio features can be integrated into video caption framework, precise sentence "a person is lying on the bed and singing" will be expected to generate.</p><p>To thoroughly utilize both visual and audio information, we propose and analyze three multimodal deep fusion strategies to maximize the benefits of visual-audio resonance information. The first one explores the impact on crossmodalities feature fusion from low to high order. The second establishes the visual-audio short-term dependency by sharing weights of corresponding front-end networks. The third extends the temporal dependency to long-term through sharing multimodal memory across visual and audio modalities. Furthermore, a dynamic multimodal feature fusion framework is also proposed to deal with audio modality absent problem during video caption generation.</p><p>The contributions of our paper include: a. We present three multimodal feature fusion strategies, to efficiently integrate audio cues into video caption.</p><p>b. We propose an audio modality inference module to handle audio modality absent problem, through generating audio feature based on the corresponding visual feature of the video.</p><p>c. Our experimental results based on Microsoft Research-Video to Text (MSR-VTT) and Microsoft Video Description (MSVD) datasets show that our multimodal feature fusion frameworks lead to the improved results in video caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related works Video Caption</head><p>Early works concerning on video caption can be classified into three groups.</p><p>The first category is template-based methods. They first identified the semantic attributes hidden in videos and then derived a sentence structure based on some predefined sentence templates <ref type="bibr" target="#b6">(Krishnamoorthy et al. 2013;</ref><ref type="bibr" target="#b18">Thomason et al. 2014)</ref>. Then, probabilistic graphical model was utilized to collect the most relevant contents in videos to generate the corresponding sentence. Although sentences generated by these models seemed to be grammatically correct, they were lack of richness and flexibility.</p><p>The second category treat video caption as a retrieval problem. They tagged videos with metadata <ref type="bibr" target="#b0">(Aradhye, Toderici, and Yagnik 2009)</ref> and then clustered videos and captions based on these tags. Although the generated sentences were more naturally compared to the first group, they were subject to the metadata seriously.</p><p>The third category of video caption methods directly map visual representation into specific provided sentences <ref type="bibr" target="#b19">(Venugopalan et al. 2014;</ref><ref type="bibr" target="#b26">Yao et al. 2015;</ref><ref type="bibr" target="#b8">Pan et al. 2016a;</ref><ref type="bibr" target="#b9">2016b)</ref>, which take inspiration from image caption <ref type="bibr" target="#b21">(Vinyals et al. 2015;</ref><ref type="bibr" target="#b3">Donahue et al. 2015)</ref>.</p><p>We argue that these video caption methods only rely on visual information while ignoring audio cues, which will restrict the performance of video caption. To handle this problem, we explore to incorporate audio information into video caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploiting Audio Information from Videos</head><p>Audio sequence underlying videos always carry meaningful information. Recently, many researchers have tried to incorporate audio information into their specific applications. In <ref type="bibr" target="#b7">(Owens et al. 2016)</ref>, Owens et al. adopted ambient sounds as a supervisory signal for training visual models, their experiments showed that units of trained network supervised by sound signals carried semantic meaningful information about objects and scenes. <ref type="bibr" target="#b10">Ren et al. (Ren et al. 2016)</ref> proposed a multimodal Long Short-Term Memory (LSTM) for speaker identification, which referred to locating a person who has the same identity with the ongoing sound in a certain video. Their key point was sharing weights across face and voice to model the temporal dependency over these two different modalities.</p><p>Inspired by <ref type="bibr" target="#b10">(Ren et al. 2016)</ref>, we propose to build temporal dependency across visual and audio modalities through sharing weights for video caption, aiming at exploring whether temporal dependency across visual and audio modalities can capture the resonance information among them or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Extended Recurrent Neural Network</head><p>Internal memory in RNN can preserve valuable information for specific tasks. However, it cannot well handle the tasks which need long-term temporal dependency.</p><p>To enhance the memory ability of RNN, an external memory has been utilized to extend RNN in some works, such as Neural Turing Machine(NTM) <ref type="bibr" target="#b4">(Graves, Wayne, and Danihelka 2014)</ref>, memory network <ref type="bibr" target="#b23">(Weston, Chopra, and Bordes 2014)</ref>, which is simply dubbed as memory enhanced RNN (ME-RNN).</p><p>ME-RNNs have been widely applied in many tasks. Besides handling single task which needs long temporal dependency, such as visual question answering <ref type="bibr" target="#b24">(Xiong, Merity, and Socher 2016)</ref> and dialog systems <ref type="bibr" target="#b2">(Dodge et al. 2015)</ref>, ME-RNNs have been adopted for multi-tasks to model long temporal dependency across different tasks <ref type="bibr" target="#b7">(Liu, Qiu, and Huang 2016)</ref>.</p><p>To explore whether long visual-audio temporal dependency can capture the resonance information among two modalities, we first try to build a visual-audio shared memory across visual and audio modalities for video caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In this section, we first introduce the basic video caption framework that our work is based on. Then, three multimodal feature fusion strategies are depicted for video caption respectively. Meanwhile, dynamic multimodal feature fusion framework and its core component AMIN are also presented.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic Video Caption Framework</head><p>Our basic video caption framework is extended from S2VT (sequence to sequence: video to text) model <ref type="bibr" target="#b20">(Venugopalan et al. 2015)</ref> and M 3 (multimodal memory modeling) model <ref type="bibr" target="#b22">(Wang et al. 2016)</ref>, which is shown in Figure <ref type="figure" target="#fig_0">1</ref>. As in Figure <ref type="figure" target="#fig_0">1</ref>, encoding stage encodes visual feature and decoding stage generates visual description. Specifically, visual feature inputs are constructed by the top LSTM layer (colored green). Intermediate mulitimodal memory (colored cyan) layer is shared by visual and textual modalities. Language is modeled by the bottom multimodal memory extended LSTM (colored red), which is conditioned on text sequence input and information reading from mulitimodal memory. &lt;BOS&gt; and &lt;EOS&gt; tags in Figure <ref type="figure" target="#fig_0">1</ref> indicate the begin-of-sentence and end-of-sentence respectively. &lt;pad&gt; hints that there is no input at the corresponding time step. In addition, the colored blue/organge lines denote writing/reading information into/from memory. Sharing Weights across Visual-Audio Modalities Although concatenation is effective for visual and audio feature fusion, it can not capture the resonance information across them well. To handle this problem, we propose a multimodal LSTM via sharing weights across visual and audio modalities for video caption. Framework of this multimodal LSTM encoder is shown in Figure <ref type="figure" target="#fig_3">3</ref> (a) and formulated as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal Feature Fusion strategies</head><formula xml:id="formula_0">i t s = σ(W i s x s t−1 + U i h s t−1 + b i s ), s = 0, 1 (1) f t s = σ(W f s x s t−1 + U f h s t−1 + b f s ), s = 0, 1 (2) o t s = σ(W o s x s t−1 + U o h s t−1 + b o s ), s = 0, 1 (3) cs t = tanh(W c s x s t−1 + U c h s t−1 + b c s ), s = 0, 1 (4) c t s = f t s c s t−1 + i t s cs t , s = 0, 1 (5) h t s = o t s c t s , s = 0, 1<label>(6)</label></formula><p>where i t , f t , o t and ct are the input gate, forget gate, output gate and the updated memory content separately, the superscript s indexes visual and audio input sequences respectively. When s = 0, (1)-( <ref type="formula" target="#formula_0">6</ref>) denotes the LSTM-based visual feature encoder and x s t is the visual feature extracted by CNNs. When s = 1, (1)-( <ref type="formula" target="#formula_0">6</ref>) indicates the LSTMbased audio feature encoder and x s t is the audio MFCC (mel-frequency cepstral coefficients) feature. In addition, W s (s = 0, 1) are weight matrices for inputting visual and audio features respectively. U is the weight matrix shared by hidden states of visual and audio encoders. b s (s = 0, 1) are the corresponding biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sharing Memory between Visual-Audio Modalities</head><p>To see whether long temporal dependency across visual and audio modalities is beneficial to video caption, we first build memory across visual and audio modalities. Concretely, an external memory is attached between visual and audio LSTM encoders. Framework of this multimodal memory encoder is presented in Figure <ref type="figure" target="#fig_3">3 (b)</ref>.</p><p>Basic procedures of our multimodal memory interactions between visual and audio modalities can be realized through the following steps: (1) read information from external memory. ( <ref type="formula">2</ref>  a. External Memory External memory adopted in our paper is defined as a matrix M ∈ R K×D , where K is the number of memory elements, and D is the dimension of each memory element.</p><p>At each time step t, an output h s t and three vectors, including key value key s t , erase vector e s t and add vector a s t are simultaneously emitted by visual and audio LSTM encoders respectively. They can be computed by ⎡</p><p>⎣</p><formula xml:id="formula_1">key s t e s t a s t ⎤ ⎦ = ⎡ ⎣ tanh σ tanh ⎤ ⎦ (W s e h s t + b s e ), s = 0, 1<label>(7)</label></formula><p>where W s e , b s e (s = 0, 1) are the weights and bias for corresponding terms respectively. b. Reading Information from External Memory We define the procedure as:</p><formula xml:id="formula_2">r s t = α s t M t−1 , s = 0, 1<label>(8)</label></formula><p>where superscript s indexes the visual and audio input sequences, r s t ∈ R D (s = 0, 1) indicate reading vectors for visual or audio streams respectively, α t ∈ R K denotes attention distribution over the elements of memory M t−1 , which decides how much information will be read from the external memory.</p><p>Each element α t,k in α t can be obtained via the following calculation:</p><formula xml:id="formula_3">α s t,k = sof tmax(g(M t−1,k , key s t−1 )</formula><p>), s = 0, 1 (9) where g(.) is the similarity measure function, which is utilized to calculate the similarity between each element of memory and the key value key s t at time t. Here, we apply cosine similarity metric function.</p><p>c. Fusing Information of External and Internal Memories After we obtain information from external memory r t , deep fusion strategy proposed in paper <ref type="bibr" target="#b7">(Liu, Qiu, and Huang 2016</ref>) is utilized to comprehensively integrate r t into internal memories of visual and audio LSTMs respectively. In detail, states h s t of visual and audio LSTM encoders at step t conditioned not only on internal memory c s t , but also on information r s t reading from external memory, which can be computed via</p><formula xml:id="formula_4">h t s = o t s (c t s + g t s (W l s r s t )), s = 0, 1<label>(10</label></formula><p>) where W l denotes the parameter matrix, g t indicates the fusion gate, which controls how much information will flow from external memory into fused memory and can be obtained via</p><formula xml:id="formula_5">g t s = σ(W p s c s t + W q s r s t ), s = 0, 1<label>(11)</label></formula><p>where W p and W q are the corresponding parameter matrices.</p><p>d. Updating Memory Memory is updated through the following procedures:</p><formula xml:id="formula_6">M t 0 = M 0 t−1 [1 − α 0 t e 0 t ] + α 0 t a 0 t (<label>12</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">M t 1 = M 1 t−1 [1 − α 1 t e 1 t ] + α 1 t a 1 t (13) M t = P M t 0 + (1 − P )M t 1 (<label>14</label></formula><p>) where e 0 t /e 1 t and a 0 t /a 1 t are the erase and add vectors emitted by visual/audio encoder respectively. Final updating of memory is the combination of updated memory from visual and audio streams respectively. Parameter P is tuned on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Modality Inference Framework</head><p>To still get benefits from audio features even when this modality is absent, we develop an audio modality inference framework (AMIN). AMIN is presented in Figure <ref type="figure" target="#fig_5">4 (a)</ref>.</p><p>AMIN can be formulated as follows: We utilize 2 constraint as the training loss for AMIN model, which is dubbed as L AM IN and formulated:</p><formula xml:id="formula_9">ŷ = D AM IN (E AM IN (x, θ), ϑ) (<label>15</label></formula><formula xml:id="formula_10">L AM IN = y − ŷ 2</formula><p>(16) where y is the ground truth audio MFCC feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Feature Fusion Framework</head><p>When AMIN is trained well, a dynamic feature fusion framework can be obtained by combining AMIN with our proposed feature fusion strategies, which is presented in <ref type="bibr">Figure 4 (b)</ref>.</p><p>Concerning videos which own both visual and audio sequences, they can be directly sent to multimodal feature fusion framework perform video caption (solid arrows). If offered videos have only visual sequence, AMIN model is adopted to generate audio features based on the corresponding video clip, then the visual and generated audio features are sent to multimodal feature fusion framework to perform video caption (dotted arrows). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Inference</head><p>Assume the number of training video caption pairs (x i , y i ) are N and the length of caption y i is l i , the averaged loglikelihood over the whole training dataset integrates a regularization term is treated as our objective function.</p><formula xml:id="formula_11">L (θ) = 1 N N i=1 li j=1 log ρ(y i j |y i 1:j−1 , x i , θ) + λ θ 2 2 (17)</formula><p>where y i j is adopted to represent the input word, λ indicates the regularization coefficient and θ denotes all parameters needed to be optimized in the model.</p><p>Just as most LSTM language models, a softmax layer is employed to model the probability distribution over the whole vocabulary of the next generated word.</p><formula xml:id="formula_12">m t = tanh(W x X t + W h h t + W y y t−1 + b h ) (<label>18</label></formula><formula xml:id="formula_13">)</formula><formula xml:id="formula_14">η t = sof max(U p m t + b η )<label>(19)</label></formula><p>where W x , W h , W y , U p , b h and b η are the parameters needed to be optimized. Depending on the probability distribution η t , word sequence y t can be recursively sampled until encountering the end of symbol in the vocabulary. Concerning caption generation, a beam search strategy is chosen to generate word sequence <ref type="bibr" target="#b27">(Yu et al. 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Data Representation and Video Caption Datasets</head><p>For a given video, we first sample it with a fixed number of frames/clips, then the pre-trained 3D CNNs are utilized to extract frame features. Meanwhile, MFCC features of each audio clip are extracted. Visual and audio feature representations can be denoted as x s = {x 1 , x 2 , ..., x n }, s = 0, 1, where s indexes visual and audio feature representations respectively and n is the number of sampled frames/clips.</p><p>To validate the performance of our model, we utilize the Microsoft Research-Video to Text Dataset (MSR-VTT) and Microsoft Video Description Dataset (MSVD) <ref type="bibr" target="#b1">(Chen and Dolan 2011)</ref>. Their split method can be found in <ref type="bibr" target="#b25">(Xu et al. 2016</ref>) and <ref type="bibr" target="#b26">(Yao et al. 2015)</ref> respectively. In addition, gradients are clipped into range <ref type="bibr">[-10,10]</ref> to prevent gradient explosion. Optimization algorithm utilized for our deep feature fusion frameworks is ADADELTA <ref type="bibr" target="#b28">(Zeiler 2012)</ref>.</p><p>Concerning auditory modality supplemental network, it contains 3 fully connected layers for encoder and decoder respectively. Units' numbers of encoder hidden layers are 1024, 512 and 256 separately and 256, 512, 1024 for decoder hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of Multimodal Feature Fusion Models</head><p>Evaluation the Performance of Various Multimodal Feature Fusion Frameworks To validate the effectiveness of integrating audio modality into video caption framework, we develop several feature fusion models and denote them as follows: V-CatL-A/V-CatH-A: Concatenating features of visual and audio modalities before/after encoder LSTM. V-ShaWei-A/V-ShaMem-A: Sharing weights/memeory across visual and audio modalities during encoding stage.</p><p>We compare our feature fusion models with several video caption models, including M 3 <ref type="bibr" target="#b22">(Wang et al. 2016)</ref>, Visual model (our basic video caption model), Audio model (our basic video caption model with audio features instead of visual features) respectively. Comparison results based on C3D visual features are shown in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Table <ref type="table" target="#tab_1">1</ref> reveals that performances of our visual and audio feature fusion models, including V-CatL-A, V-CatH-A, V-ShaMem-A and V-ShaWei-A models, are uniformly better than those of models which only conditioned on visual or audio features. Moreover, performance of V-CatH-A is better than that of V-CatL-A, which indicates concatenating visual and audio features in higher layer is more efficient than that in low layer. In addition, results of V-ShaMem-A and V-ShaWei-A models are superior to those of V-CatL-A and V-CatH-A models, which hints the temporal dependency across visual and audio modalities can further boost the performance of video caption. Moreover, performances of V-ShaWei-A model surpass those of V-ShaMem-A model, demonstrating short temporal dependency is more efficient. It may be because that short temporal dependency can capture the resonance information among visual and audio modalities more efficiently.</p><p>Our best model can make a great improvement over M 3 by 38.3−35.1   35.1 = 9.1% in BLUE@4 score and by 26.1−25.7   25.7 = 1.5% in METEOR score based on C3D feature.</p><p>Evaluation of the Generated Sentences of Various Multimodal Feature Fusion Strategies Figure <ref type="figure">5</ref> presents some sentences generated by different models and humanannotated ground truth based on the test set of MSR-VTT. We can see that audio model always generates wrong sentences, which may be because the absence of visual modality leads to serious information loss. On the other hand, V-ShaWei-A model can generate sentences with more related objects, actions and targets.</p><p>Concerning the first video, sentence generated by Visual model focuses more on visual cues while ignores audio information. As a result, it generates wrong content ("to a man" vs. "news"). V-Cat-A model generates accurate object "a man" and action "talking" while lossing the content "news". It is because that directly concatenating visual and audio features may lead to the collapse of information. Both V-ShaMem-A and V-ShaWei-A models can generate more related sentences with the help of audio cues. Concerning V-ShaMem-A model, it focuses more on the summation of longer period of information, which blurs the resonance among visual and audio modalities and offers a more abstract word "something". Concerning V-ShaWei-A model, it pays more attention to the event in a finer granularity which real matters, indicating it can capture the resonance information among two modalities effectively.</p><p>Concerning the second video, all models can generate the related action "swimming" and target "in the water". While only V-ShaWei-A model generates precise object ("fish" vs. "man" and "person"). Reason is that V-ShaWei-A model can capture both motion and sound sensitive object (the resonance information among visual and audio modalities), other than static object which looks like a man.</p><p>Concerning the third video, only V-ShaWei-A model generates more related action ("showing" vs. "playing"), which indicates V-ShaWei-A model can capture the nature of an action.</p><p>Concerning the forth video, V-Cat-A and V-ShaWei-A model can generate more related actions ("knocking on a wall", "using a phone") with the aid of audio information. However, V-ShaMem-A model focus more on global event and generates a sentence "lying on bed". Moreover, Visual model pays more attention on visual information and also generates description "lying on bed".</p><p>Concerning the fifth video, event happened in this video is more related to visual information. Consequently, Visual, V-Generated Sentence:</p><p>Reference Sentence:</p><p>1. a man with a blue shirt is talking 2. a man talks about matters of science 3. a man in a suit is talking about psychological Aduio:</p><p>a man is showing how to make a dish Visual:</p><p>a man in a suit is talking to a man V-Cat-A: a man is talking V-ShaMem-A: a man in a suit is talking about something V-ShaWei-A: a man in a suit talks about the news 1. gold fishes are swimming in the blue water 2. fish swimming in the fish tank 3. some red and white fish are swimming in a tank Aduio:</p><p>a man is swimming Visual:</p><p>a person is swimming in the water V-Cat-A:</p><p>a man is swimming in the water V-ShaMem-A: a man is swimming in the water V-ShaWei-A: a fish is swimming in the water ShaMem-A and V-ShaWei-A models all generate more precise actions ("dancing" vs. "playing" and "singing"). Moreover, V-ShaMem-A and V-ShaWei-A models offer more precise number of objects ("a group of" vs. "a girl", "a cartoon character" and "someone"), indicating temporal dependency across visual and audio modalities is helpful for the identification of the object. Moreover, V-ShaWei-A model provides more accurate object ("cartoon characters" vs. "people"), which validates short temporal dependency is more effective in capturing resonance information among two modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of Dynamic Multimodal Feature Fusion</head><p>Evaluation of Supplemental Audio Modality based on MSR-VTT To validate whether the supplemental audio modality has comparable effects with the original one, we compare models V-ShaMem-GA (similar with V-ShaMem-A model except utilizing generated audio features instead of original audio features), V-ShaMem-Zero (similar with V-ShaMem-A model except utilizing zeros to replace audio features) and Visual model based on MSR-VTT dataset. V-ShaWei-GA, V-ShaCatH-GA, V-ShaWei-Zero, V-ShaCatHzero share the similar meanings with corresponding terms.</p><p>Comparison results are shown in Table <ref type="table" target="#tab_2">2</ref>. Models with visual and generated audio features (V-CatH-GA, V-ShaMem-GA and V-ShaWei-GA), are superior to corresponding models with visual and zero filled audio features (V-CatH-Zero, V-ShaMem-Zero and V-ShaWei-Zero) and Visual model, which indicates supplemental audio features convey useful information. To validate whether supplemental audio features contain useful information or not, we compare models V-ShaWei-GA, V-ShaMem-GA with Visual model. In addition, to verify whether pretraining based on a big dataset MSR-VTT dataset will further boost the performance or not, V-ShaWei-GA-Pre, V-ShaMem-GA-Pre models (similar with V-ShaWei-GA and V-ShaMem-GA models respectively, except that before training on MSVD dataset, models are first a girl is riding a horse Visual:</p><p>a man is riding a horse V-ShaWei-GA: a girl is riding a horse  <ref type="table" target="#tab_3">3</ref>.</p><p>Among Table <ref type="table" target="#tab_3">3</ref>, performances of V-ShaWei-GA models are better than those of Visual model (M 3 , state-of-art Visual model), which again verifies that supplemental audio features carry meaningful information for video caption. In addition, models with pretraining obtain best performance, which demonstrates knowledge learned from other big dataset can further enhance our specific task.</p><p>Evaluation of the Generated Sentences of Dynamic Multimodal Feature Fusion Framework Figure <ref type="figure" target="#fig_9">6</ref> presents some sentences generated by GA (models with only generated audio features), M 3 <ref type="bibr" target="#b22">(Wang et al. 2016</ref>), V-ShaWei-GA models and human-annotated ground truth based on the test set of MSVD.</p><p>Concerning the first video, sentence generated by Visual model focuses more on visual cues. Consequently, it generates wrong content "piano", which is because the object behind the boy is like a piano and takes a large space in the image. V-ShaMem-GA model equipped with generated audio cues captures more related object "violin", which further verifies that the supplemental audio modality is useful and the V-ShaWei-GA model can capture temporal related visual and audio cues. GA model generates more similar term "guitar" than "piano", compared to the precise term "violin", which validates the effectiveness of generated audio cues.</p><p>Concerning the second video, V-ShaWei-GA model can generates more accurate action ("pouring sauce into a pot" vs. "cooking the something"), which reveals that the V-ShaWei-GA model can capture the resonance information underlying visual and audio modalities effectively. Similar with V-ShaWei-GA model, GA model also generates precise action "pouring", further demonstrating that the generated audio features is meaningful.</p><p>Concerning the third video, V-ShaWei-GA and GA model can generate more related object ("girl" vs. "man").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we propose three multimodal feature fusion strategies to integrate audio information into models for en- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Basic pipeline of our video caption framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>ConcatenatingFigure 2 :</head><label>2</label><figDesc>Figure 2: Visual and audio feature concatenation before and after LSTM encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) fuse information from external memory into internal memories of visual and audio encoder LSTMs respectively. (3) update external memory. visual and audio features by sharing weights. (b) Fusing visual and audio features by sharing weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visual and audio feature fusion via sharing weights and memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) where x indicates the visual feature and ŷ denotes the generated corresponding audio feature. E AM IN /D AM IN demonstrates the encoding/decoding function and θ/ϑ is the parameter set of the encodering/deconding stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Dynamic multimodal feature fusion framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 5: Descriptions generated by Visual, audio, V-Cat-A, V-ShaMem-A, V-ShaWei-A models and human-annotated ground truth based on the test set of MSR-VTT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>is pouring tomato sauce from a can into a saucepan containing meat pieces 2. a person pours tomato sauce in a pot 3. a man pours tomato sauce into a pan with meat GA: a man is pouring water Visual:the person is cooking the something V-ShaWei-GA: a man is pouring sauce into a pot 1. a girl is horseback riding through a course 2. a girl is riding a horse 3. a woman is riding a horse GA:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Descriptions generated by Visual, Generated audio (GA), V-ShaWei-GA models and human-annotated ground truth based on the test set of MSVD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; M0 M1 M2 M3 M4 M5 A Woman is Singing a Song &lt;EOS&gt; &lt;BOS&gt; Encoding Stage Decoding Stage</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison results of different models for video caption with C3D frame features. Srivastava et al. 2014) with 0.5 rate are utilized on both the output of fully connected layer and the output layers of LSTM, but not on the intermediate recurrent transitions.</figDesc><table><row><cell>Models</cell><cell cols="2">B@3 B@4 METEOR</cell></row><row><cell>M 3</cell><cell>0.472 0.351</cell><cell>0.257</cell></row><row><cell>audio</cell><cell>0.370 0.268</cell><cell>0.192</cell></row><row><cell>Visual</cell><cell>0.473 0.363</cell><cell>0.257</cell></row><row><cell>V-CatL-A</cell><cell>0.480 0.369</cell><cell>0.258</cell></row><row><cell>V-CatH-A</cell><cell>0.485 0.374</cell><cell>0.258</cell></row><row><cell cols="2">V-ShaMem-A 0.493 0.375</cell><cell>0.259</cell></row><row><cell>V-ShaWei-A</cell><cell>0.494 0.383</cell><cell>0.261</cell></row><row><cell>Experimental Setup</cell><cell></cell><cell></cell></row><row><cell cols="3">During model training, start and end tags are added to each</cell></row><row><cell cols="3">sentence respectively. Words that not existed in vocabu-</cell></row><row><cell cols="3">lary are replaced with UNK token. Furthermore, masks are</cell></row><row><cell cols="3">added to sentences, visual and auditory features separately</cell></row><row><cell cols="3">for better batch training. Parameters are set as follows, beam</cell></row><row><cell cols="3">search size, word embedding dimension and LSTM hid-</cell></row><row><cell cols="3">den state dimension are 5, 468 and 512 respectively. Size</cell></row><row><cell cols="3">of visual-auditory and visual-textual shared memories are</cell></row><row><cell cols="3">64 × 128 and 128 × 512 respectively. To avoid overfitting,</cell></row><row><cell>dropout (</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison results of different models for video caption with C3D frame features based on MSR-VTT.</figDesc><table><row><cell>Models</cell><cell cols="2">B@3 B@4 METEOR</cell></row><row><cell>Visual</cell><cell>0.473 0.363</cell><cell>0.257</cell></row><row><cell>V-CatH-Zero</cell><cell>0.447 0.343</cell><cell>0.241</cell></row><row><cell>V-CatH-GA</cell><cell>0.479 0.372</cell><cell>0.255</cell></row><row><cell cols="2">V-ShaMem-Zero 0.450 0.338</cell><cell>0.251</cell></row><row><cell>V-ShaMem-GA</cell><cell>0.479 0.374</cell><cell>0.256</cell></row><row><cell>V-ShaWei-Zero</cell><cell>0.463 0.354</cell><cell>0.252</cell></row><row><cell>V-ShaWei-GA</cell><cell>0.487 0.379</cell><cell>0.259</cell></row><row><cell cols="3">Evaluation of Supplemental Audio Modality based on</cell></row><row><cell cols="3">MSVD To further verify the effectiveness of supplemental</cell></row><row><cell cols="3">audio features, we evaluate video caption based on MSVD</cell></row><row><cell cols="3">dataset which has no audio cues. Audio features are first gen-</cell></row><row><cell cols="3">erated by audio modality inference network (AMIN), then</cell></row><row><cell cols="3">these features are fused with visual information through our</cell></row><row><cell cols="3">multimodal featuree fusion frameworks for video caption.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison results of different models for video caption with C3D frame features based on MSVD. Each of these three strategies can uniformly boost the performance of video caption, which denotes the valuableness of audio cues underlying videos. In addition, fusion models via sharing weights across visual and audio modalities can well model the reason information among them and obtains the best results. Moreover, based on our multimodal feature fusion model, we propose a dynamic multimodal feature fusion framework to handle audio modality absent problem. It can generate promising audio features based on the corresponding visual features when the audio modality is missing.</figDesc><table><row><cell>Models</cell><cell cols="2">B@3 B@4 METEOR</cell></row><row><cell cols="2">M 3 (Wang et al. 2016) 0.563 0.455</cell><cell>0.299</cell></row><row><cell>GA</cell><cell>0.482 0.381</cell><cell>0.281</cell></row><row><cell>V-ShaMem-GA</cell><cell>0.569 0.467</cell><cell>0.304</cell></row><row><cell>V-ShaMem-GA-Pre</cell><cell>0.570 0.471</cell><cell>0.307</cell></row><row><cell>V-ShaWei-GA</cell><cell>0.571 0.468</cell><cell>0.307</cell></row><row><cell>V-ShaWei-GA-Pre</cell><cell>0.584 0.479</cell><cell>0.309</cell></row><row><cell>hanced video caption.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">The Thirty-Second AAAI Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Natural Science Foundation of China (No. 61773375, No. 61375036,  No. 61602481, No. 61702510), and in part by the Microsoft Collaborative Research Project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video2text: Learning to annotate video content</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining Workshops, 2009. ICDMW&apos;09. IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06931</idno>
		<title level="m">Evaluating prerequisite qualities for learning end-to-end dialog systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating naturallanguage video descriptions using text-mined knowledge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07222</idno>
	</analytic>
	<monogr>
		<title level="m">Deep multi-task learning with shared memory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="801" to="816" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="4594" to="4602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look, listen and learn-a multimodal lstm for speaker identification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3581" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="page" from="3202" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequencevideo to text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international</title>
				<meeting>the IEEE international</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multimodal memory modelling for video captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05592</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4584" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
