<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contents lists available at ScienceDirect Pattern Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-22">22 September 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Lixiang</forename><surname>Xu</surname></persName>
							<email>xulixianghf@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Artificial Intelligence and Big Data</orgName>
								<orgName type="institution">Hefei University</orgName>
								<address>
									<postCode>230601</postCode>
									<settlement>Hefei Anhui</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Central University of Finance and Economics</orgName>
								<address>
									<addrLine>10 0 081</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyi</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Münster</orgName>
								<address>
									<addrLine>Einsteinstrasse 62</addrLine>
									<postCode>48149</postCode>
									<settlement>Münster</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Artificial Intelligence and Big Data</orgName>
								<orgName type="institution">Hefei University</orgName>
								<address>
									<postCode>230601</postCode>
									<settlement>Hefei Anhui</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daoqiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing Jiangsu</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Luo</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<postCode>230601</postCode>
									<settlement>Hefei Anhui</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contents lists available at ScienceDirect Pattern Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-22">22 September 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.patcog.2020.107668</idno>
					<note type="submission">Received 12 December 2019 Revised 13 June 2020 Accepted 18 September 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Shannon entropy Rényi entropy Deep representation Graph kernel Graph classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph kernels are applied heavily for the classification of structured data. In this paper, we propose a deep Rényi entropy graph kernel for this purpose. We gauge the deep information through a family of h -layer expansion subgraphs rooted at a vertex, and define a h -layer depth-based second-order Rényi entropy representation for each vertex. The second-order Rényi entropy representation is used together with Euclidean distance to build a deep second-order Rényi entropy graph kernel (SREGK). For graphs with n vertices, the time complexity for our kernel is O ( n 3 ). This low-order polynomial complexity enables our subgraph kernels to easily scale up to graphs of reasonably large sizes and thus overcome the size limits arising in state-of-the-art graph kernels. Experimental results on fourteen real world graph datasets are shown to demonstrate the overall superior performance of our approach over a number of state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph-based representations have already gained considerable popularity thanks to their competence of characterizing structured data in general. Over the last decades the research in structural pattern recognition has mainly focused on the solution of the correspondence problems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> as the essential means of assessment of structural similarity. For the sake of the analysis and comprehension of these data, it is necessary to have some graph similarity measure methods.</p><p>The famous kernel trick <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> has switched the problem from the vectorial representation of data to a representation of similarity. Then, it became possible to use standard statistical learning techniques for data with no easy vectorial representation. With the similarity being at hand, similarity-based pattern recognition techniques such as graph kernel can be adopted to execute the tasks of recognition and classification, or graphs may be embedded in a high-dimensional pattern space using kernel mapping techniques. So, in last years the pattern recognition community has been increasingly interested in structural learning utilizing graph kernels. Nontheless, owing to the ample expressiveness of graphs, this task has been demonstrated to be hard as well.</p><p>Graph kernels are used heavily for the classification of structured data. Images could be regarded as graphs if components in an image are considered as node and their semantic relations as edges. A great deal of work has been witnessed for image classification by adopting methods like graph edit distance <ref type="bibr" target="#b4">[5]</ref> or marginalized kernels <ref type="bibr" target="#b5">[6]</ref> . Other application fields include bioinformatics (graph models of molecular structures <ref type="bibr" target="#b6">[7]</ref> in molecular biology), chemoinformatics (modelling molecular compounds in chemistry, prediction of features of molecules from their graph structures, e.g. toxicity or effectiveness as a drug <ref type="bibr" target="#b7">[8]</ref> ), financial data analysis <ref type="bibr" target="#b8">[9]</ref> , social network analysis <ref type="bibr" target="#b9">[10]</ref> , and HTML, XML, Internet for graph models <ref type="bibr" target="#b10">[11]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Entropy and applications in pattern recognition</head><p>Entropy theory was put forward by German physicist Rudolf Clausius in 1865 <ref type="bibr" target="#b11">[12]</ref> . He gave the entropy in a reversible thermodynamic system: ds = ( d q /t ) r , where r indicates the reversible process, d s symbolizes the entropy change of the system, d q indicates the heat change of the system, and t is the absolute temperature of the system. Clausius associated entropy with the heat exchange of the system from a macro perspective, and found that entropy is a state function, independent of the thermodynamic path experienced by the research object, only related to the initial state and the final state.</p><p>In 1896, Boltzmann, an Austrian physicist, connected entropy with the number of microscopic states and used statistical theory to explain thermodynamics <ref type="bibr" target="#b12">[13]</ref> . In a system composed of a large number of particles, entropy is the disorder degree of the system, which can be used to express the disorder degree of particle motion. According to Boltzmann the function of entropy can be expressed as: S = −k ln p, where k is Boltzmann constant, p is thermodynamic probability. Boltzmann's microscopic study makes the entropy theory popularized. By the 1950s, entropy was found to be able to describe information. In 1948, Shannon defined the uncertainty of information in the communication process as information entropy, which was used to measure the uncertainty of signal source and laid the theoretical foundation of modern information theory <ref type="bibr" target="#b13">[14]</ref> .</p><p>Recently, entropy is widely used in pattern recognition. For example, George et al. <ref type="bibr" target="#b14">[15]</ref> provided a comprehensive analysis, making the most of the entropy rate created by a Markov chain over a connected graph of order n and subject to a prescribed stationary distribution. Aggarwal et al. <ref type="bibr" target="#b15">[16]</ref> presented an entropy-based method to model a decision-maker's subjective utility for a criterion value. Zhou et al. <ref type="bibr" target="#b16">[17]</ref> showed an ave-entropy based weight assignment process considering the risk preference of decision making to deal with manifold attribute decision making problems, where indefinite subjective judgments such as belief distributions are included. Moreover, other approaches for information entropy are such as: Min-entropy latent model <ref type="bibr" target="#b17">[18]</ref> , Multi-layer entropyguided pooling <ref type="bibr" target="#b18">[19]</ref> , Semi-supervised multi-view maximum entropy discrimination <ref type="bibr" target="#b19">[20]</ref> , and so on.</p><p>It is worth noting that in recent years, there has been a lot of research literature about the applications of Rényi entropy on graphs. For example, Pál <ref type="bibr" target="#b20">[21]</ref> presented simple and computationally efficient nonparametric estimation of Rényi entropy and mutual information based on generalized nearest-neighbor graphs. Livi <ref type="bibr" target="#b21">[22]</ref> presented different variants of the improved optimized dissimilarity space embedding graph classification system by exploiting the α-order Rényi entropy of the dissimilarity representation to execute the classification on labeled graphs. Moreover, Ran <ref type="bibr" target="#b22">[23]</ref> showed the connections between Renyi entropy Principal Component Analysis, kernel learning and graph embedding. This theoretic development enables a close relationship between information theoretic learning, kernel learning and graph embedding. In this paper, we will explore the utility of a h -layer deep Rényi entropy graph kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Related work</head><p>Graph kernel methods are strong learning algorithms that can be effortlessly used for every input domain and are supported from statistical learning theory that provides powerful theoretical guarantees on the output hypothesis. One of the common ideas of graph kernel methods is that of deconstructing two distinct objects and comparing some simpler substructures. Examples include Random Walk Kernels <ref type="bibr" target="#b23">[24]</ref> , Marginalized Graph Kernels <ref type="bibr" target="#b5">[6]</ref> , Graph Hopper Kernels <ref type="bibr" target="#b24">[25]</ref> , Deep Graph Kernels <ref type="bibr" target="#b25">[26]</ref> , and Multiscale Laplacian Graph Kernel <ref type="bibr" target="#b26">[27]</ref> . There are other convolution kernel functions: Shervashidze et al. <ref type="bibr" target="#b27">[28]</ref> gave a feature extraction scheme rooted in the Weisfeiler-Lehman test of isomorphism on graphs. It establishes the mapping of the initial graph to a sequence of graphs whose node attributes capture topological and label information. Another significant graph kernel is studied in recent years <ref type="bibr" target="#b28">[29]</ref> . Zhao et al. <ref type="bibr" target="#b26">[27]</ref> proposed a labeled graph kernel for behavior analysis, Xu et al. <ref type="bibr" target="#b29">[30]</ref> defined two reproducing graph kernels. They further proposed a hybrid reproducing graph kernel, and used it as a measure for establishment of the similarity resemblance between a couple of graphs. Formerly suggested graph kernels that undergo implicit computation typically back up assigning arbitrary kernels for vertex annotations, but do not scale to large graphs and datasets. Even when people know approximative explicit feature maps of the kernel on vertex annotations, it is still unclear how to gain feature maps for the graph kernel. Recently, Bai et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref> built a graph kernel based on multilayer representation as well as the entropy-based isomorphism test. The applied strategy has a significant implication upon the operation time of the graph kernel method at the higher level. The running time of graph kernel methods discussed above holding up implicit kernel computation is often longer for training support vector machines. Therefore, it is necessary to find more efficient and faster graph kernel functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Contributions and paper organization</head><p>In contract to previous works we use the Rényi entropy to structure a deep representation of graphs, and focus on the combination of Rényi entropy and probability of the steady state random walks on graphs visit vertex to generate a novel graph kernel. In detail the primary contributions of our work are highlighted as follows:</p><p>(1) We use the L'Hospital's rule to derive the Shannon entropy of graph by generalized entropy with q = 1 , and deduce the expression of second-order Rényi entropy by generalized entropy with q = 2 . We further give some properties about Our experimental outcomes upon fourteen benchmark datasets demonstrate that our SREGK outperforms or matches twelve stateof-the-art graph classification algorithms including graph kernels and representations of graphs through deep learning networks.</p><p>The remainer of the paper is organized as follows. In Section 2 we discuss the fundamentals including graph kernel and Shannon entropy. The Rényi entropy is reviewed and deduced in Section 3 . It contains the generalized entropy, first-order entropy, second-order entropy and their properties. In Section 4 we show deep Rényi entropy representation and graph matching, and further propose deep Rényi entropy graph kernel. The experimen-tal evaluation is given in Section 5 . Finally, some discussion in Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Fundamentals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph kernel</head><p>The kernel-based approach to predictive graph mining demands a positive definite kernel function between graphs. Graphs, which consist of vertices and edges, probably enjoy enrichment with attributes. One overall strategy for designing graph kernels is to deconstruct graphs into smaller substructures among which kernels are defined conforming to the concept of convolution kernels owing to Haussler <ref type="bibr" target="#b31">[32]</ref> . The graph kernel itself is thus the combined kernels between the likely overlapping parts. Therefore, the diverse graph kernels suggested in the literature primarily differ in the way the parts are composed and in the similarity measure employed to make comparison of them.</p><p>Basic facts on graph kernels, which exert essential effects upon computational aspects, are recalled. A kernel on a non-empty set X is a positive definite function k : X × X → R . Likewise, a function k is a kernel if there is a feature map φ: X → H to a Hilbert space H with inner product, such that k ( x, y ) = φ( x ) , φ( y ) for all x and y in X .</p><p>In the graph domain, let G be a finite or infinite set of graphs. There is a Hilbert space (of possibly infinite dimension) H and a mapping φ, for all G 1 , G 2 ∈ G , and we define a function K called graph kernel as follows</p><formula xml:id="formula_0">K ( G 1 , G 2 ) = φ( G 1 ) , φ( G 2 ) (1) where &lt; • , • &gt; is inner product on Hilbert space H .</formula><p>This equivalence generates two algorithmic strategies for computing graph kernels on graphs: (i) One strategy is functional computation, e.g., from closed-form expressions. On this occasion the feature map is not inevitably known and the feature space might be of infinite dimension. Hence, this approach based on the famous kernel trick is referred to as implicit computation. (ii) The other is to make a computation of the feature map φ( G ) for each graph G explicitly to acquire the kernel values from the dot product between pairs of graph feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Shannon entropy</head><p>The graph G ( V, E ) has a substitute entropy which utilizes steady state random walks on G ( V, E ). d v i is the degree of node v i . The probability that the steady state random walks on</p><formula xml:id="formula_1">G ( V, E ) visit ver- tex v i is P G (i ) = d v i / v j ∈ V d v j .</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>The Shannon entropy for</p><formula xml:id="formula_3">G ( V, E ) is H S (G ) = − | V | i =1 P G (i ) log (P G (i ) ) .</formula><p>(</p><formula xml:id="formula_4">)<label>3</label></formula><p>The time complexity O (| V | 2 ) is necessary for compting the Shannon entropy H S ( G ) <ref type="bibr" target="#b29">[30]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Rényi entropy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generalized entropy</head><p>Entropy is not only a concept in physics, but also a mathematical function. Shannon associated entropy with statistical concept and explained the functional relationship between probability and information redundancy in mathematical language.</p><p>For the random variable X of continuous distribution, there is also corresponding entropy function</p><formula xml:id="formula_5">H(X ) = − + ∞ −∞ P (x ) ln P (x ) dx (4)</formula><p>Moreover, the entropy of discrete random variable X is defined as follows</p><formula xml:id="formula_6">H(X ) = − n i =1 P (x i ) ln P (x i ) . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where X = (x 1 , x 2 , . . . , x n ) , P ( x i ) is the probability corresponding to x i . The formula <ref type="bibr" target="#b3">(4)</ref> shows that the information entropy H ( X ) is equal to the weighted average of the corresponding probability P ( x i ) of the information entropy in all cases of the source. In other words, information entropy shows the average uncertainty of random variables. The greater the average uncertainty of variables is, the greater the information entropy is. Therefore, the size of information entropy is adopted to measure the average uncertainty of random variables.</p><p>Shannon made information entropy an important part of information theory. From then on, the concept of entropy stepped out of physics and crossed over to other scientific fields. However, one needs to know the specific probability distribution to solve its entropy function, which limits its application range. So many scholars studied and generalized Shannon entropy. For example, Jaynes proposed the principle of maximum entropy, Rényi, Cressie, Chernoff and Tsallis proposed different forms of popularizing entropy functions. These popularizations make entropy theory generalized to other fields. After continuous development, entropy theory has been paid more and more attention by scholars, and has been applied to almost all disciplines.</p><p>Generalized entropy introduces a parameter α into the definition. The change in α can affect the final weighted average. Consequently, the measurement of generalized entropy to information is more universal and flexible. There are many kinds of generalized entropy, which can be widely used in the fields of economic system, agricultural system, life system and information system. Next, we will focus on two classical forms of generalized entropy that are applicable to graph kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">First-order entropy</head><p>In 1970, Rényi proposed α-order generalized entropy in phase space, which is defined as</p><formula xml:id="formula_8">H α (p) = 1 1 − α ln n i =1 p α i . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where P i is the probability of the system state falling into the i th phase lattices, P i = 1 , N is the number of phase lattices. Next, we give the connection between first-order generalized entropy and Shannon entropy <ref type="bibr" target="#b32">[33]</ref> .</p><p>Theorem 1. The first-order generalized entropy is Shannon information entropy.</p><p>Proof. Take the limit of Eq. ( <ref type="formula" target="#formula_8">6</ref>) with q → 1, we have lim</p><formula xml:id="formula_10">α→ 1 H α (p) = lim α→ 1 1 1 − α ln n i =1 p α i (7)</formula><p>Further, according to L'Hospital's rule for Eq. <ref type="bibr" target="#b6">(7)</ref> and</p><formula xml:id="formula_11">P i = 1 , we obtain lim α→ 1 1 1 − α ln n i =1 P α i = lim a → 1 d dα ln n i =1 P α i d dα ( 1 − α) = − lim a → 1 1 n i =1 P α i n i =1 P α i lnP i = − n i =1 P i lnP i</formula><p>= H S (P ) <ref type="bibr" target="#b7">(8)</ref> where H S ( P ) is the Shannon information entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Second-order entropy</head><p>The second-order generalized entropy is Rényi's second-order correlation entropy, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H R (P )</head><formula xml:id="formula_12">= − ln N i =1 P 2 i (9)</formula><p>In the formula, N i =1 P 2 i is called the second-order correlation function. The significance of Rényi entropy H R ( P ) is the logarithmic measure of two unrelated phase points.</p><p>In information theory, Rényi entropy is an important index of diversity in ecology and statistics. Rényi entropy is also very important in quantum information. It can be used to measure entanglement. For a random variable, its value is uncertain. Before the random experiment is conducted, we only know the probability distribution of each value. After the random experiment is completed, we know the value exactly, and the uncertainty disappears completely. In this way, we obtain information through random experiments, and the amount of information is exactly equal to the entropy of random variables. In this sense, we can use entropy as a measurement of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Shannon entropy and Rényi entropy</head><p>In this section, we give a Theorem that is important to compare Shannon entropy with second-order Rényi entropy <ref type="bibr" target="#b32">[33]</ref> . </p><formula xml:id="formula_13">Theorem 2. Suppose P i (G ) = d v i / v j ∈ V d v j is</formula><formula xml:id="formula_14">H S (G ) = − n i =1 P i ln P i (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>Since the probability 0</p><formula xml:id="formula_16">&lt; P i (G ) = d v i / v j ∈ V d v j &lt; 1 , then we have ln P i &lt; 0 , (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>so H S ( P ) &gt; 0, that is the Shannon entropy of graph H S ( G ) is nonnegative.</p><p>(2) According to formula (9), we get the second-order Rényi entropy</p><formula xml:id="formula_18">H R (G ) = − ln N i =1 P 2 i (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>Since the probability 0</p><formula xml:id="formula_20">&lt; P i (G ) = d v i / v j ∈ V d v j &lt; 1 , then we have 0 &lt; N i =1 P 2 i &lt; N i =1 P i = 1 , (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>So we obtain ln</p><formula xml:id="formula_22">N i =1 P 2 i &lt; 0 , (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>and then we have H R ( G ) &gt; 0, that is the second-order Rényi entropy H R ( G ) of graph is nonnegative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Rényi entropy graph kernel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Deep Rényi entropy representation</head><p>In this subsection, we introduce the concept of the deep Rényi entropy representation by replacing the required entropy measure of the depth-based complexity trace with the novel Rényi entropy. As for an undirected graph G ( V, E ), we can compute the shortest path S G ( v, u ) between a pair of vertex v and vertex u if we adopt the Dijkstra algorithm. The matrix S G with element S G ( v, u ) standing for the shortest path length between vertex v and vertex u is denoted as the shortest path matrix for G ( V, E ).</p><p>Suppose</p><formula xml:id="formula_24">N K v ⊂ V and N K v = { u ∈ V | S G (v , u ) ≤ K} . For G ( V, E ), the K -layer expansion subgraph ζ K v (V K v ; E K v ) around vertex v is V K v = { u ∈ N K v }; E K v = { (u, v ) ⊂ N K v | (u, v ) ∈ E} . (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>Suppose L max be the largest length of the shortest path distances from vertex v to the rest of vertexes of</p><formula xml:id="formula_26">G ( V, E ). If L v ≥ L max , the L v -layer expansion subgraph is G ( V, E ) itself.</formula><p>Definition 1 (Deep Rényi entropy Representation) . As for a graph</p><formula xml:id="formula_27">G ( V, E ) and a vertex v ∈ V , the h -layer deep Rényi entropy repre- sentation around v is a h -dimensional vector [7] D h G (v ) = [ H I (ζ 1 v ) , • • • , H I (ζ K v ) , • • • , H I (ζ h v )] T ,<label>(16)</label></formula><p>where</p><formula xml:id="formula_28">h ( h ≤ L v ) is the length of the shortest paths from vertex v to other vertexes in G ( V, E ), ζ K v (V K v ; E K v )(K ≤ h ) is the K -layer ex- pansion subgraph of G ( V, E ) around v , and H I (ζ K v ) is the infor- mation entropy of ζ K v .</formula><p>In this paper, our information entropy of ζ K v uses the Shannon entropy and second-order Rényi entropy in Eqs. ( <ref type="formula">8</ref>) and (9) , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Graph matching</head><p>How the deep Rényi entropy representation could be utilized for graph matching is introduced in this subsection. Furthermore, a novel entropy graph kernel is defined according to the proposed matching method.</p><p>A matching approach like the one presented in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> for point set matching is created and it can make an computation of an affinity matrix with regard to the distances between points. For a vertex v of G ( V, E ), the h -layer deep Rényi entropy representations D h G (v ) is regarded as the point coordinate linked to v . Further, the Euclidean distance between the deep Rényi entropy representations</p><formula xml:id="formula_29">D h G 1 (v i ) and D h G 2</formula><p>(u j ) is adopted for the length measurement of the pair-wise vertexes v i and u j of graphs G 1 ( V 1 , E 1 ) and G 2 ( V 2 , E 2 ). The affinity matrix element R ( i, j ) has the definition as</p><formula xml:id="formula_30">R (i, j) = [ A − B ] T [ A − B ] . (<label>17</label></formula><formula xml:id="formula_31">)</formula><p>where <ref type="formula" target="#formula_30">17</ref>) is the Euclidean distance.</p><formula xml:id="formula_32">A = D h G 1 (v i ) , B = D h G 2 (u j ) , R is a | V 1 | × | V 2 |, Eq. (</formula><p>The element R ( i, j ) stands for the unsimilarity between the vertex v i in G 1 ( V 1 , E 1 ) and the vertex u j in G 2 ( V 2 , E 2 ). The rows of R ( i, j ) are used to make indexing the vertexes of G 1 ( V 1 , E 1 ), and the columns are used to make indexing of the vertexes of G 2 ( V 2 , E 2 ). Supposing R ( i, j ) is the minimum element that is smaller than any other one not only in row i but also in column j , there ought to exist a one-by-one match between the vertex v i of G 1 and the vertex u j of G 2 . The corresponding condition is recorded by exploiting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref> C(i, j) = 1 if R (i, j) is the smallest element both in row i and in column j; 0 otherwise . <ref type="bibr" target="#b17">(18)</ref> Eq. ( <ref type="formula">18</ref>) suggests that if C(i, j) = 1 , the vertexes v i and v j are matched. There may be two or over two elements that meet Eq. ( <ref type="formula">18</ref>) in row i or column j . In other words, for each pair of graphs, a vertex of one graph may be aligned to more than one vertex of the other graph, and vice versa. To assign each vertex an unique matched vertex, one can adopt the well-known Hungarian algorithm to update the matrix C . However, employing the Hungarian algorithm may require extra expensive computational complexity, and thus lead to computational inefficiency. To overcome this problem, we follow the alternative strategy proposed in <ref type="bibr" target="#b35">[36]</ref> , and randomly assign each vertex an unique matched vertex through the correspondence matrix C . Note that, this strategy will not influence the effectiveness of the proposed kernel.</p><formula xml:id="formula_33">the correspondence matrix C ∈ { 0 , 1 } | V 1 | | V 2 | satisfying</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Deep Rényi entropy graph kernel</head><p>A Rényi entropy graph kernel is defined by utilizing the graph matching strategy in this subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 (Deep Rényi Entropy Graph Kernel</head><formula xml:id="formula_34">) . Regard G 1 ( V 1 , E 1 )</formula><p>as well as G 2 ( V 2 , E 2 ) as a pair of sample graphs. The correspondence matrix C is computed in accordance with the terms defined in Eqs. ( <ref type="formula" target="#formula_27">16</ref>) -( <ref type="formula">18</ref>) and the Hungarian algorithm. The deep Rényi entropy graph kernel k (h )  DRE adopting the h -layer deep Rényi matching entropy representations of the graphs is</p><formula xml:id="formula_35">k (h ) DRE (G 1 , G 2 ) = | V 1 | i =1 | V 2 | j=1 C(i, j) , (<label>19</label></formula><formula xml:id="formula_36">)</formula><p>which counts the quantity of matched vertex pairs between</p><formula xml:id="formula_37">G 1 ( V 1 , E 1 ) and G 2 ( V 2 , E 2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3. The deep Rényi entropy graph kernel k (h )</head><p>DRE is positive definite.</p><p>Proof. In an intuitive manner, the proposed deep Rényi entropy graph kernel is positive definite for the reason that it calculates pairs of matched vertexes (i.e., the minimum subgraphs). Further, suppose the base kernel k be a function which counts the quantity of matched vertexes pairs in the pair of graphs</p><formula xml:id="formula_38">G 1 ( V 1 , E 1 ) and G 2 ( V 2 , E 2 ) k (G 1 , G 2 ) = k (h ) DRE (G 1 , G 2 ) = v i ∈ V p u j ∈ V q δ(v i , u j ) ,<label>(20)</label></formula><p>where</p><formula xml:id="formula_39">δ(v i , u j ) = 1 i f C(i, j) = 1 ; 0 otherwise. (<label>21</label></formula><formula xml:id="formula_40">)</formula><p>δ is the Dirac kernel, that is, it is 1 if a pair of vertexes are matched and 0 otherwise. Therefore the defined kernel function k (h ) DRE is the total of several positive definite Dirac kernels, which is hence positive definite.</p><p>The time complexity O ( hn 3 ) is necessary for the deep Rényi entropy graph kernel k (h )  DRE on a pair of graphs</p><formula xml:id="formula_41">G 1 ( V 1 , E 1 ) and G 2 ( V 2 , E 2 ) ( | V 1 | = | V 2 | = n ) .</formula><p>Because a time complexity O ( hn 3 ) is neces- sary for computing of their correspondence matrix C with regard to the h -layer deep Rényi entropy representations, and it is necessary for us to itemize all the n 2 pairs of elements in C in order to count the number of matched vertex pairs from the matrix C . Therefore, the whole time complexity of the deep Rényi entropy graph kernel k (h ) DRE is O ( hn 3 ). This shows that our deep Rényi entropy graph kernel k (h )  DRE can be computed in low-order polynomial time.</p><p>The deep Rényi entropy graph kernel is connected with the layered representation defined in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref> . Nevertheless, there exists two distinctions. Firstly, the graphs in <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b30">[31]</ref> were decomposed into Shannon entropy substructures with growing layer for every vertex of graph as a point coordinate. By contrast, in our method, a graph is decomposed into deep Rényi entropy substructures with growing layer. Second, we figure out the Euclidean distance between the deep Rényi entropy representations to measure the distance of the pairwise vertexes, and develop a method of point set matching. However, Bai et al. <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b30">[31]</ref> adopted the Euclidean distance between the multilayer Shannon entropy representations to measure the pairwise vertexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>The performance of our novel graph kernel is demonstrated on fourteen normative graph based datasets. These datasets are from bioinformatics and computer vision <ref type="bibr" target="#b2">[3]</ref> , and include MUTAG, D &amp; D, ENZYMES, BAR31, BSPHERE31, GEOD31, CATH2, NCI1, NCI109, COIL5, Shock, PPIs, GATORBait and PTC. More detailed contents about the datasets are displayed in Table <ref type="table" target="#tab_2">1</ref> . Moreover, in order to better make out these datasets, the distributions of the sample quantity changes with node quantities are shown in Fig. <ref type="figure" target="#fig_0">1</ref> .</p><p>Here, merely the MUTAG, D &amp; D, ENZYMES, NCI1, NCI109, COIL5, Shock, PPIs, PTC datsets are chosen, while the others are similar to these datasets. For the D &amp; D dataset, the distributions of the sample number within node quantities from 0 to 2500 only are displayed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments setup</head><p>The performance of our proposed deep second-order Rényi entropy graph kernel (SREGK) on several standard graph datasets are evaluated, and then we contrast it with several alternative state of the art the methods of depth-based representation of graph and graph kernels. These methods employed for comparison include (1) the depth-based representations of graphs through deep learning networks (DDBR) <ref type="bibr" target="#b36">[37]</ref> , (2) the depth-based complexity trace of graphs (DBCT) <ref type="bibr" target="#b6">[7]</ref> , (3) the graph matching kernel with Shannon entropy (GMKS) <ref type="bibr" target="#b30">[31]</ref> , (4) the graph matching kernel with approximated von Neumann entropy (GMKV) <ref type="bibr" target="#b30">[31]</ref> , <ref type="bibr" target="#b4">(5)</ref> the Jensen-Shannon subgraph kernel (JSSK) <ref type="bibr" target="#b2">[3]</ref> , (6) the entropic isomorphism kernel (ISK) <ref type="bibr" target="#b2">[3]</ref> , <ref type="bibr" target="#b6">(7)</ref> the attributed graph kernel from the Jensen-Tsallis q −differences connected with q = 2 (JTQK) <ref type="bibr" target="#b37">[38]</ref> , <ref type="bibr" target="#b7">(8)</ref> the unaligned quantum Jensen-Shannon kernel (UQJS) <ref type="bibr" target="#b38">[39]</ref> , (9) the backtraceless random walk kernel employing the Ihara zeta function based cycles (BRWK) <ref type="bibr" target="#b39">[40]</ref> , <ref type="bibr" target="#b9">(10)</ref> the Weisfeiler-Lehman subtree kernel (WL) <ref type="bibr" target="#b27">[28]</ref> , <ref type="bibr" target="#b10">(11)</ref> the shortest path graph kernel (SPGK) <ref type="bibr" target="#b40">[41]</ref> , and (12) the graphlet count graph kernels with graphlet of size 3 (GCGK) <ref type="bibr" target="#b41">[42]</ref> , For the ISK, SPGK, JTQK and WL kernel, we use the same setup as <ref type="bibr" target="#b2">[3]</ref> .</p><p>In our experiments, the 10-fold cross-validation is performed applying the C−Support Vector Machine ( C−SVM) classification, and the classification accuracy is computed applying LIBSVM. Nine out of ten samples were used for training and one-tenth of the samples were used for testing. We performed each classification with its parameters which were optimized on each dataset. The average classification accuracies from the 10-fold cross-validation for each graph kernel are reported in Table <ref type="table">2</ref> . Furthermore, the runtime of each graph kernel and graph embedding method on different datasets is displayed in Table <ref type="table">3</ref> , which is measured under Matlab R2013a operating on a 2.5 GHz Intel 2-Core processor (i.e., i5-3210m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Kernel matrix</head><p>In this paper, for the MUTAG, D &amp; D, BAR31, BSPHERE31, GEOD31, CATH2, COIL5 and GatorBait datasets, our method consistently  </p><p>, GEOD31 (e), CATH2 (e) and COIL5 (f) datasets, respectively. Our method is mediocre on some datasets, so we do not show these kernel matrices. Our SREGK matrix plots are merely displayed in the 50 × 50 square matrices of upper-left corner of the SREGK matrix. Besides, it is clear to see that the checker-board (i.e., the SREGK matrix) is strictly symmetric in Fig. <ref type="figure" target="#fig_2">3</ref> (a)-(f). It is worth noting that the SREGK matrix plots are similar for Fig. <ref type="figure" target="#fig_2">3 (c</ref>)-(d), since the two datasets are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Classification accuracies</head><p>The classification accuracies are exhibited in Table <ref type="table">2</ref> . (a) On the MUTAG dataset, the accuracy of our SREGK kernel surpasses the alternative graph kernels obviously, and the DDBR method is competitive with our SREGK kernel method. (b) On the D &amp; D dataset, the accuracy of our SREGK kernel overcomes the alternative kernels, and is one percentage point higher than the best of the other kernels selected. The DDBR, DBCT, GMKS and GMKV graph kernel methods are similar. However, the UQJS, BRWK and SPGK graph kernels are not able to finish the demanded compu-  tations on the D &amp; D dataset due to the graphs in this dataset being rather large (e.g., there are more than thousands vertices in some graphs). (c) On the ENZYMES dataset, the accuracy of the DDBR method is the best. The accuracy of our SREGK kernel has a competitive edge against that of the DBCT, UQJS and GMKS kernels. (d) On the BAR31, BSPHERE31 and GEOD31 datasets, the accuracy of our SREGK kernel evidently overcomes those of the rest of kernels, in particular, on the BAR31 dataset, our graph kernel is four percentage points higher than the best of the other kernels selected with the exception of the GMKS graph kernel. On the BSPHERE31 dataset, our graph kernel is two percentage points higher than the best of the other kernels selected except the GMKS graph kernel. On the GEOD31 dataset, our SREGK method is competitive with the DDBR and GMKS, and slightly surpasses them. (e) On CATH2 dataset, the classification accuracy of our SREGK kernel surpasses the other kernels selected with the exception of the 36.58 <ref type="bibr" target="#b8">(9)</ref> 78.91 <ref type="bibr" target="#b5">(6)</ref> 68.02 <ref type="bibr" target="#b6">(7)</ref> 67.92 <ref type="bibr" target="#b7">(8)</ref> 67.93 <ref type="bibr" target="#b6">(7)</ref> DDBR method, and is competitive with the DDBR method. Here, our method is slightly lower than the DDBR method. The BRWK cannot fulfil the computing task on BAR31, BSPHERE31, GEOD31 and CATH2 datasets. This is due to the fact that the BRWK kernel counts on the cycle structures of the graphs. The graphs are sparsely distributed on these datasets. Accordingly, the BRWK is not able to catch any cycle on these datasets. (f) Overall, on the NCI1, NCI109 and PPIs datasets, the accuracies of the JTQK and WL kernels are clearly higher than the SREGK kernel, our graph kernel method is mediocre. For PPIs dataset, our SREGK method may outperform that of all the rest of kernels except for the JTQK and WL kernels. But then, our method may surpass or have the competitive edge against alternative most of the alternative kernels on these datasets. (g) On the COIL5 dataset, the advantages of our SREGK method are significant. The accuracy of our SREGK kernel obviously surpasses that of all the alternative kernels, and our method is five percentage points higher than the best of the other kernels being compared except for the GMKS kernel. The GMKS kernel is competitive to our SREGK kernel, and they have similar classification ability. (h) On the Shock dataset, our method is commonplace. Most of the methods go beyond our graph kernel. The DBCT method is best. The DDBR and ISK are similar, and they are competitive with DBCT method. (i) In the end, on the GatorBait dataset, the accuracy of our SREGK kernel clearly surpasses those of all the alternative kernels. On the PTC dataset, the accuracy of our SREGK kernel surpasses those of all the alternative kernels except for the DDBR, ISK, JTQK, WL and GMKV methods. The DDBR method is clearly superior to that of all the rest of kernels including our SREGK kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Runtime analysis</head><p>It is obvious that our SREGK kernel is an efficient one with low-order polynomial time. It can fulfil the computing task on all datasets with efficiency. There are four reasons to account for this efficiency. First, for the efficiency of our SREGK kernel is lower than that of the JSSK kernel. The corresponding reason is that the SREGK kernel considers the deep Rényi entropy representations (i.e., the h −layer deep Rényi entropy representation around each vertex) originated from all the vertexes. By comparison, the JSSK kernel simply takes into account the layered representation obtained from the centroid vertex. Hence, more expansion subgraphs need to be measured by our SREGK kernel. Second, the efficiency of the SREGK kernel is lower than the WL and GCGK kernels on some datasets as well. Being different from some kernels which can simply fulfil the computing task on datasets having small graphs, it is worth mentioning that our SREGK kernel can also fulfil the computing rask on datasets owning large graphs in polynomial time. The corresponding reason is that the demanded Rényi entropy connected with the steady state random walk for the SREGK kernel can compute with efficiency (i.e., the computation is quadratic vertex number), respectively. Third, the efficiency of our SREGK kernel is similar to the GMKS and GMKV kernels, because they all explore how to break down a graph into layered deep substructures with growing size, and then to measure the information content of these substructures. Fourth, although our classification accuracy is not as good as the DDBR and JTQK methods on some datasets, the runtime of our method is much faster than theirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Statistic analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1.">Random weighted borda sorting</head><p>Borda count is a voting system. Voters rank the candidates according to their preferences. If the candidate ranks first on the ballot, he will get a certain score; if he ranks second, he will obtain In order to validate the decision results of the Borda sorting, a weighted operation is used to distinguish the importance and authority of each expert. Here we assign the weight w k for the k th expert, so the Borda Count of the evaluation object y i can have the definition as:</p><formula xml:id="formula_43">b i = m k =1 w k b k i j , where n k =1 w k = 1 , 0 ≤ w k ≤ 1.</formula><p>Here, we randomly generate the weights within a range of 0-1 to implement random weighting Borda sorting, and the sum of the random weights is equal to 1. We show the result of random weighting on the last line of Table <ref type="table" target="#tab_4">4</ref> . Moveover, in Table <ref type="table" target="#tab_4">4</ref> , we show the result of Borda sorting for the fourteen methods. In particular, we give the mean value of Borda Count and the mean value of random weighting Borda Count on the penultimate column and last column of Table <ref type="table" target="#tab_4">4</ref> , respectively. Table <ref type="table" target="#tab_4">4</ref> reveals that our SREGK kernel method is the most preferred for all methods, while BRWK is the worst method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2.">Comparison with different methods</head><p>We summarize the sorting results of different methods in Table <ref type="table" target="#tab_5">5</ref> . These methods are the mean value sorting of classification accuracies, mean rank sorting and random weighted Broda sorting, respectively, where the mean value sorting of classification accuracies and mean rank sorting are from the last two rows of Table <ref type="table">2</ref> . Table <ref type="table" target="#tab_5">5</ref> shows that our SREGK method has the best result in all alternative sorting methods. The DDBR and GMKS methods are second and third on the mean value of classification accuracies and mean rank sorting, respectively. However, their order is reversed for the random weighted Broda sorting. For all sorting methods, the second-to-last is GCGK method. The worst is BRWK method in our experiments. For the JTQK, WL, SPGK and JSSK methods, their sorting is similar and is between fifth and tenth consistently in classification performance. The WL method consistently ranks eighth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">SREGK vs. GMKS</head><p>Here we compare our SREGK method with graph kernel from the deep representation based on Shannon entropy (GMKS) <ref type="bibr" target="#b30">[31]</ref> , since they are both graph kernels based on the depth-based complexity traces of graphs. For the SREGK and GMKS, the SREGK has better classification capacity and efficiency, and is superior to or comparable to the GMKS kernel on most of datasets. The reason is twofold: Firstly, according to Theorem 2 , we obtain that the Shannon entropy and second-order Rényi entropy are non-negative. However, the Shannon entropy and second-order Rényi entropy are different. For the MUTAG, D &amp; D, BAR31, BSPHERE31, GEOD31, CATH2 and COIL5 datasets, our method consistently beats all other alternative methods. Therefore, we discuss and compare the complexity traces of the Shannon entropy and second-order Rényi entropy on these datasets. In Fig. <ref type="figure" target="#fig_2">3</ref> (a)-(f), we show, when h is small, the complexity traces of Shannon entropy and second-order Rényi entropy are similar, but the complexity trace of second-order Rényi entropy is bigger and bigger than Shannon entropy with layer number h increasing. Secondly, the SREGK and GMKS rely on the degree statistics of the nodes, but they are two different depiction of the graph. As a matter of fact, a number of graphs have the identical degrees d(v i ) , i = 1 , 2 , . . . , | V | , yet they may have diverse structures. In other words, there are same Shannon entropy but different secondorder Rényi entropies for these graphs (Maybe, there are also same second-order Rényi entropies for some of the graphs). It is significant that we select different information entropies to discuss deep entropy representation and deep entropy graph kernel of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we define a deep Rényi entropy representation for a graph based on the second-order Rényi entropy. The purpose is that it expands the information entropy character representation in graph matching learning field. Our SREGK is composed of secondorder Rényi entropy, deep representation of graph and Euclidean distance measure. It enjoys competitive advantages in the light of accuracy with other alternative graph kernels on several classification benchmark graph datasets. Hereafter, we intend to consider the SREGK on real-world graphs acquired from larger datasets and assess their efficient computation. A definition based on secondorder Rényi entropy is planned to build a deep graph kernel on graph datasets with unlabeled or nodes labeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Competing Interest</head><p>None.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. a-f. The quantitative distribution of sample experiences its change with node numbers for MUTAG (a), D &amp; D (b), ENZYMES (c), NCI1 (d), NCI109 (e), COIL5 (f), Shock (g), PPIs (h) and PTC (i) datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. a-f. The SREGK matrix for MUTAG (a), D &amp; D (b), BAR31 (c), BSPHERE31 (d), GEOD31 (e) and COIL5 (f) datasets, respectively.</figDesc><graphic url="image-7.png" coords="7,482.06,68.66,72.28,253.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. a-f. Comparison of complexity traces of the SREGK and GMKS methods for MUTAG (a), D &amp; D (b), BAR31 (c), BSPHERE31 (d), GEOD31 (e), CATH2 (e) and COIL5 (f) datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the probability that the steady state random walks on G ( V, E ) visit vertex v i , then<ref type="bibr" target="#b0">(1)</ref> the Shannon entropy H S ( G ) of graph is nonnegative.<ref type="bibr" target="#b1">(2)</ref> the second-order Rényi entropy H R ( G ) of graph is nonnegative.</figDesc><table><row><cell>Proof. (1) According to formula (8), we get the Shannon entropy</cell></row><row><cell>expression</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Key information about the datasets used in this work.</figDesc><table><row><cell>Datasets</cell><cell>MUTAG</cell><cell>D &amp; D</cell><cell>ENZYMES</cell><cell>BAR31</cell><cell>BSPHERE31</cell><cell>GEOD31</cell><cell>CATH2</cell></row><row><cell>Max # vertexes</cell><cell>28</cell><cell>5748</cell><cell>126</cell><cell>220</cell><cell>227</cell><cell>380</cell><cell>568</cell></row><row><cell>Min # vertexes</cell><cell>10</cell><cell>30</cell><cell>2</cell><cell>41</cell><cell>43</cell><cell>29</cell><cell>143</cell></row><row><cell>Mean # vertexes</cell><cell>17.93</cell><cell>284.3</cell><cell>32.63</cell><cell>95.42</cell><cell>99.83</cell><cell>57.42</cell><cell>308.03</cell></row><row><cell># graphs</cell><cell>188</cell><cell>1178</cell><cell>600</cell><cell>300</cell><cell>300</cell><cell>300</cell><cell>190</cell></row><row><cell># classes</cell><cell>2</cell><cell>2</cell><cell>6</cell><cell>15</cell><cell>15</cell><cell>15</cell><cell>2</cell></row><row><cell>Datasets</cell><cell>NCI1</cell><cell>NCI109</cell><cell>COIL5</cell><cell>Shock</cell><cell>PPIs</cell><cell>GatorBait</cell><cell>PTC</cell></row><row><cell>Max # vertexes</cell><cell>111</cell><cell>111</cell><cell>241</cell><cell>33</cell><cell>218</cell><cell>545</cell><cell>109</cell></row><row><cell>Min # vertexes</cell><cell>3</cell><cell>4</cell><cell>72</cell><cell>4</cell><cell>3</cell><cell>239</cell><cell>2</cell></row><row><cell>Mean # vertexes</cell><cell>29.87</cell><cell>29.68</cell><cell>144.90</cell><cell>13.16</cell><cell>109.63</cell><cell>348.70</cell><cell>25.60</cell></row><row><cell># graphs</cell><cell>4110</cell><cell>4127</cell><cell>360</cell><cell>150</cell><cell>219</cell><cell>100</cell><cell>344</cell></row><row><cell># classes</cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>5</cell><cell>30</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>L. Xu, L. Bai, X. Jiang et al.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pattern Recognition 111 (2021) 107668</cell></row><row><cell></cell><cell></cell><cell>Average rank</cell><cell>3.43</cell><cell>3.79</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell>61.05(1)</cell><cell>60.83(2)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>PTC</cell><cell>56.79(5)</cell><cell>62.57(1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>GatorBait</cell><cell>14.4 (1)</cell><cell>10.42(6)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>PPIs</cell><cell>84.13(3)</cell><cell>75.98(7)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Shock</cell><cell>33.24(9)</cell><cell>39.34(4)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>COIL5</cell><cell>74.41 (1)</cell><cell>69.36(5)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>NCI109</cell><cell>68.17(7)</cell><cell>72.45(5)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>NCI1</cell><cell>67.59(9)</cell><cell>71.27(5)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CATH2</cell><cell>83.58(2)</cell><cell>84.97(1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>GEOD31</cell><cell>43.57 (1)</cell><cell>42.51(3)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>BSPHERE31</cell><cell>57.36 (1)</cell><cell>54.76(4)</cell><cell>47.53(7)</cell></row><row><cell></cell><cell></cell><cell>BAR31</cell><cell>70.08 (1)</cell><cell>63.21(4)</cell><cell>56.36(8)</cell></row><row><cell>Table 2</cell><cell>Prediction accuracy (Rank) on graph classification.</cell><cell>Datasets MUTAG D&amp;D ENZYMES</cell><cell>SREGK 86.35 (1) 78.19 (1) 36.85(6)</cell><cell>DDBR 86.12(2) 77.12(5) 41.47(1)</cell><cell>DBCT 84.82(4) 77.85(2) 37.13(5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Result of Borda Count and random weighted Borda Count for the thirteen methods. , and so on. The candidate with the highest cumulative score wins. Many people in history have proposed the use of Borda count method. It was one of the voting systems adopted by the Roman Parliament. In 1770, Jean Charles de Borda proposed to use this method for the election of the French Academy of Sciences, and later this counting method was named after him.</figDesc><table><row><cell cols="13">Datasets MUTAG D&amp;D ENZYMES BAR31 BSPHERE31 GEOD31 CATH2 NCI1 NCI109 COIL5 Shock PPIs</cell><cell cols="2">GatorBait PTC</cell><cell cols="2">Mean Random</cell></row><row><cell>SREGK</cell><cell>12</cell><cell>12</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>12</cell><cell>11</cell><cell>4</cell><cell>6</cell><cell>12</cell><cell>4</cell><cell>10</cell><cell>12</cell><cell>8</cell><cell>9.57</cell><cell>10.16(1)</cell></row><row><cell>DDBR</cell><cell>11</cell><cell>8</cell><cell>12</cell><cell>9</cell><cell>9</cell><cell>10</cell><cell>12</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>9</cell><cell>6</cell><cell>7</cell><cell>12</cell><cell>9.21</cell><cell>8.57(3)</cell></row><row><cell>DBCT</cell><cell>9</cell><cell>11</cell><cell>8</cell><cell>5</cell><cell>6</cell><cell>4</cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>6</cell><cell>12</cell><cell>5</cell><cell>9</cell><cell>4</cell><cell>6.93</cell><cell>6.83(6)</cell></row><row><cell>GMKS</cell><cell>10</cell><cell>11</cell><cell>5</cell><cell>11</cell><cell>11</cell><cell>11</cell><cell>10</cell><cell>5</cell><cell>4</cell><cell>11</cell><cell>2</cell><cell>9</cell><cell>11</cell><cell>3</cell><cell>8.14</cell><cell>8.83(2)</cell></row><row><cell>GMKV</cell><cell>1</cell><cell>9</cell><cell>4</cell><cell>10</cell><cell>10</cell><cell>9</cell><cell>8</cell><cell>3</cell><cell>3</cell><cell>7</cell><cell>1</cell><cell>7</cell><cell>10</cell><cell>9</cell><cell>6.50</cell><cell>6.83(7)</cell></row><row><cell>JSSK</cell><cell>7</cell><cell>7</cell><cell>1</cell><cell>3</cell><cell>4</cell><cell>3</cell><cell>6</cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>7</cell><cell>1</cell><cell>4</cell><cell>7</cell><cell>4.21</cell><cell>4.15(11)</cell></row><row><cell>ISK</cell><cell>8</cell><cell>6</cell><cell>11</cell><cell>8</cell><cell>8</cell><cell>7</cell><cell>2</cell><cell>10</cell><cell>10</cell><cell>3</cell><cell>10</cell><cell>8</cell><cell>8</cell><cell>11</cell><cell>7.86</cell><cell>7.63(4)</cell></row><row><cell>JTQK</cell><cell>5</cell><cell>4</cell><cell>10</cell><cell>7</cell><cell>5</cell><cell>8</cell><cell>3</cell><cell>12</cell><cell>12</cell><cell>1</cell><cell>6</cell><cell>12</cell><cell>5</cell><cell>10</cell><cell>7.14</cell><cell>6.97(5)</cell></row><row><cell>UQJS</cell><cell>4</cell><cell>2</cell><cell>6</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>7</cell><cell>7</cell><cell>10</cell><cell>11</cell><cell>4</cell><cell>2</cell><cell>6</cell><cell>4.93</cell><cell>4.64(10)</cell></row><row><cell>BRWK</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.14</cell><cell>0.21(13)</cell></row><row><cell>WL</cell><cell>3</cell><cell>3</cell><cell>9</cell><cell>6</cell><cell>3</cell><cell>5</cell><cell>1</cell><cell>11</cell><cell>11</cell><cell>2</cell><cell>5</cell><cell>11</cell><cell>6</cell><cell>5</cell><cell>5.79</cell><cell>5.97(8)</cell></row><row><cell>SPGK</cell><cell>6</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>7</cell><cell>6</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>8</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>5.71</cell><cell>5.50(9)</cell></row><row><cell>GCGK</cell><cell>3</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>2.21</cell><cell>2.21(12)</cell></row><row><cell>Weight</cell><cell>0.09</cell><cell>0.10</cell><cell>0.01</cell><cell>0.10</cell><cell>0.07</cell><cell>0.01</cell><cell>0.03</cell><cell>0.06</cell><cell>0.11</cell><cell>0.11</cell><cell>0.02</cell><cell cols="2">0.11 0.11</cell><cell>0.06</cell><cell></cell><cell></cell></row><row><cell cols="2">a smaller score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Comparison of three sorting methods.</figDesc><table><row><cell>Method</cell><cell>1th</cell><cell>2th</cell><cell>3th</cell><cell>4th</cell><cell>5th</cell><cell>6th</cell><cell>7th</cell><cell>8th</cell><cell>9th</cell><cell>10th</cell><cell>11th</cell><cell>12th</cell><cell>13th</cell></row><row><cell>Mean</cell><cell>SREGK</cell><cell>DDBR</cell><cell>GMKS</cell><cell>GMKV</cell><cell>DBCT</cell><cell>ISK</cell><cell>JTQK</cell><cell>WL</cell><cell>JSSK</cell><cell>SPGK</cell><cell>UQJS</cell><cell>GCGK</cell><cell>BRWK</cell></row><row><cell>Average rank</cell><cell>SREGK</cell><cell>DDBR</cell><cell>GMKS</cell><cell>ISK</cell><cell>JTQK</cell><cell>DBCT</cell><cell>GMKV</cell><cell>WL</cell><cell>SPGK</cell><cell>UQJS</cell><cell>JSSK</cell><cell>GCGK</cell><cell>BRWK</cell></row><row><cell>Weight Broda</cell><cell>SREGK</cell><cell>GMKS</cell><cell>DDBR</cell><cell>ISK</cell><cell>JTQK</cell><cell>DBCT</cell><cell>GMKV</cell><cell>WL</cell><cell>SPGK</cell><cell>UQJS</cell><cell>JSSK</cell><cell>GCGK</cell><cell>BRWK</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Pattern Recognition 111 (2021) 107668</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This work was financially supported by</head><p>Key International Cooperation Projects of National Natural Science Foundation of China ( 61860206004 ), National Natural Science Foundation of China ( 61976235 ), Nature Science Research Project of Anhui province (1908085MF185), Major Scientific Research Projects of Universities of Anhui Province (KJ2019ZD61), Key Projects of Excellent Young Talents Support Program (gxyq2019113).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern Recognition 111 (2021) 107668</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph matching and similarity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Systems and Interfaces</title>
				<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000">20 0 0</date>
			<biblScope unit="page" from="281" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exact and inexact graph matching -methodology and applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<editor>C.C. Aggarwal, H. Wang</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="217" to="247" />
		</imprint>
	</monogr>
	<note>Managing and Mining Graph Data</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast depth-based subgraph kernels for unattributed graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="233" to="245" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multiple attributes convolution kernel with reproducing property</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximate graph edit distance computation by means of bipartite graph matching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="950" to="959" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03)</title>
				<meeting>the 20th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth-based complexity traces of graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1172" to="1186" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast neighborhood subgraph pairwise distance kernel</title>
		<author>
			<persName><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning</title>
				<meeting>the 27th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">lending analysis using the most relevant graph-based features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Heat diffusion: thermodynamic depth complexity of networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Escolano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36206</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Method for node ranking in a linked database</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">999</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Laidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gerischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The World of Physical Chemistry</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1995">1995</date>
			<publisher>Oxford University Press Oxford</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zermelo, Boltzmann, and the recurrence paradox</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Steckline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Phys</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="894" to="897" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Markov chains with maximum entropy for robotic surveillance</title>
		<author>
			<persName><forename type="first">M</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jafarpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bullo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1566" to="1580" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decision aiding model with entropy-based subjective utility</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">501</biblScope>
			<biblScope unit="page" from="558" to="572" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evidential reasoning approach with multiple kinds of attributes and entropy-based weight assignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="358" to="375" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Min-entropy latent model for weakly supervised object detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1297" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">REMAP: Multi-layer entropy-guided pooling of dense CNN features for image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-view maximum entropy discrimination with expectation Laplacian regularization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="296" to="306" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Estimation of Rényi entropy and mutual information based on generalized nearest-neighbor graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Designing labeled graph classifiers by exploiting the Ré Ȩ nyi entropy of the dissimilarity representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">216</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On connections between Rényi entropy principal component analysis, kernel learning and graph embedding</title>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="125" to="130" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>SEP.1)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On graph kernels: hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The multiscale Laplacian graph kernel</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2990" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A combined Weisfeiler-Lehman graph kernel for structured data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Wavelets Multiresolution Inf. Process</title>
		<imprint>
			<biblScope unit="page">1850039</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A hybrid reproducing graph kernel based on information entropy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="89" to="98" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A graph kernel from the depth-based representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<title level="m">Convolution Kernels on Discrete Structures</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A brief review of generalized entropies</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Amigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Balogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">813</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Longuet-Higgins , An algorithm for associating the features of two images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. R. Soc. Lond. B</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="21" to="26" />
			<date type="published" when="1309">1309. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A generative model for graph matching and embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="777" to="789" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A graph kernel based on the Jensen-Shannon representation alignment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
				<editor>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wooldridge</surname></persName>
		</editor>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">July 25-31, 2015. 2015</date>
			<biblScope unit="page" from="3322" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep depth-based representations of graphs through deep learning networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">336</biblScope>
			<biblScope unit="page" from="3" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attributed graph kernels using the Jensen-Tsallis q-differences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="99" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A quantum Jensen-Shannon graph kernel for unattributed graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="344" to="355" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Backtrackless walks on a graph</title>
		<author>
			<persName><forename type="first">F</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="977" to="989" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note>Data Mining</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="4" to="88" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">He has been awarded a scholarship to pursue his study in Germany as a joint Ph.D. student from 2015 to 2017. Presently, he is doing postdoctoral research at the University of Science and Technology of China. His current research interests include structural pattern recognition, machine learning, graph spectral analysis, image and graph matching</title>
	</analytic>
	<monogr>
		<title level="m">Lixiang Xu received the B.Sc. degree and M.Sc. degree in applied mathematics in 2005 and 2008, respectively</title>
				<meeting><address><addrLine>Hefei, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2017</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology, Anhui University</orgName>
		</respStmt>
	</monogr>
	<note>before joining Hefei University in the following year. He received his Ph.D. degree from the School of</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">His current research interests include structural pattern recognition, machine learning, quantum walks on graphs, and graph matching</title>
		<imprint>
			<pubPlace>Macau SAR, China; Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Lu Bai received the Ph.D. degree from the University of York, York, UK, and both the B.Sc. and M.Sc degrees from Faculty of Information Technology, Macau University of Science and Technology ; Central University of Finance and Economics</orgName>
		</respStmt>
	</monogr>
	<note>He is now an Asistant Professor in School of Information. especially in kernel methods and complexity analysis on (hyper) graphs and networks</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">where he is currently the Dean of the Faculty of Mathematics and Computer Science. His current research interests include biomedical imaging, 3-D image analysis, and structural pattern recognition. Dr. Jiang is the Editor-in-Chief of the International Journal of Pattern Recognition and Artificial Intelligence. He also serves on the Advisory Board and the Editorial Board of several journals</title>
		<author>
			<persName><forename type="first">China</forename><surname>Beijing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venia</forename><forename type="middle">D</forename><surname>The Ph</surname></persName>
		</author>
		<author>
			<persName><surname>Docendi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging and International Journal of Neural Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Xiaoyi Jiang received the bachelor&apos;s degree from Peking University ; University of Bern, Bern, Switzerland, all in Computer Science. He was an Associate Professor with the Technical University of Berlin ; Professor of Computer Science with the University of Münster</orgName>
		</respStmt>
	</monogr>
	<note>He is a Senior Member of IEEE and a fellow of IAPR</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">His current research interests include structural pattern recognition, machine learning, image and graph matching</title>
		<imprint>
			<pubPlace>Hefei, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Hefei University</orgName>
		</respStmt>
	</monogr>
	<note>He received the M.Sc. degree in computer science from Hefei University of Technology of China in 2003. object detection</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bin Luo was born in Anhui province of China in 1963. He received his BEng. degree in electronics and MEng. degree in computer science from Anhui university of China in 1984 and 1991, respectively. From 1996 to 1997, he worked as a British Council visiting scholar at the University of York under the Sino-British Friendship Scholarship Scheme (SBFSS)</title>
	</analytic>
	<monogr>
		<title level="m">He has published some 200 papers in journals, edited books and refereed conferences. Some papers were published on the IEEE T-PAMI, Pattern Recognition journal, CVPR, IJCAI, and AAAI conferences</title>
				<meeting><address><addrLine>Nanjing, China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999. 2004. 2002</date>
		</imprint>
		<respStmt>
			<orgName>Daoqiang Zhang received the B.Sc. and Ph.D. degrees in computer science from Nanjing University of Aeronautics and Astronautics ; Department of Computer Science and Engineering, Nanjing University of Aeronautics and Astronautics</orgName>
		</respStmt>
	</monogr>
	<note>He served as a visiting fellow of the University of New South Wales, Australia in 2008. He was a TCT Exchange Fellow at the Nanyang Technological University. He is at present a professor at Anhui University of China. His current research interests include graph spectral analysis, image and graph matching, statistical pattern recognition. At present, he chairs the IEEE Hefei Subsection</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
