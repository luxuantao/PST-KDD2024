<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Heterogeneous Information for Personalized Tag Recommendation in Social Tagging Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Feng</surname></persName>
							<email>feng-w10@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
							<email>jianyong@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Heterogeneous Information for Personalized Tag Recommendation in Social Tagging Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AAA596C4E376892D097536C774B336B4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Information Filtering</term>
					<term>Retrieval Models</term>
					<term>Selection Process Recommender System</term>
					<term>Social Tagging System</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A social tagging system provides users an effective way to collaboratively annotate and organize items with their own tags. A social tagging system contains heterogeneous information like users' tagging behaviors, social networks, tag semantics and item profiles. All the heterogeneous information helps alleviate the cold start problem due to data sparsity. In this paper, we model a social tagging system as a multi-type graph. To learn the weights of different types of nodes and edges, we propose an optimization framework, called OptRank. OptRank can be characterized as follows:(1) Edges and nodes are represented by features. Different types of edges and nodes have different set of features.</p><p>(2) OptRank learns the best feature weights by maximizing the average AUC (Area Under the ROC Curve) of the tag recommender. We conducted experiments on two publicly available datasets, i.e., Delicious and Last.fm. Experimental results show that: (1) OptRank outperforms the existing graph based methods when only &lt;user, tag, item&gt; relation is available. (2) OptRank successfully improves the results by incorporating social network, tag semantics and item profiles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In social tagging systems, users can annotate and organize items with their own tags for future search and sharing. For  Personalized tag recommendation is the key part of a social tagging system. When a user wants to annotate an item, the user may have his/her own vocabulary to organize items. Personalized tag recommendation tries to find the tags that can precisely describe the item with the user's vocabulary.</p><p>A social tagging system, as shown in Figure <ref type="figure" target="#fig_1">1</ref>, contains heterogeneous information and can be modeled as a graph:</p><p>• Users(U), tags(T) and items(I) co-exist in the graph.</p><p>• Inter-relation. Edges between users, tags and items can be derived from annotation behaviors &lt;user, tag, item&gt;. Suppose we have u ∈ U and t ∈ T , the weight of &lt;u, t&gt; is the times of tag t being used by user u.</p><p>The same rule applies to &lt;u, i&gt; and &lt;i, t&gt; (i ∈ I).</p><p>• Intra-relation. While the inter-relation has been well studied in previous work <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>, few work tries to incorporate all the intra-relation into a unified model. Incorporating the intra-relation may solve the cold start problem due to data sparsity. Users in a social network may influence each other by sharing some annotated items. Semantically related tags may co-occur to describe an item. Items that have similar contents may be annotated with the same tag. When a user u wants to annotate an item i, the recommended tags should meet two requirements: (1) Highly relevant to user u because users have their own way to organize items. <ref type="bibr" target="#b2">(2)</ref> Highly relevant to item i because tags should precisely describe the item. To rank the tags, we can perform a random walk with restart at user u and item i to assign each tag a visiting probability, which is used as the ranking score. Only tags that are both relevant to u and i can get high scores. However, two problems arise when the random walk is performed on the multi-type graph:</p><p>• Different types of edges have different meanings and thus are measured in different metrics. For example, the edge weights of a social network may be binary and they have completely different meanings from other types of edges, such as the edges formed by tagging behaviors &lt;user, tag, item&gt;. To perform a random walk, they need to be measured under the same metric.</p><p>• The random walker can either restart from the user u or the item i. The probabilities of restart at u and at i should be estimated.</p><p>To solve the above two problems, we propose an optimization framework called OptRank. OptRank can be characterized as follows:</p><p>• Edges are represented by features. Different types of edges have different set of features. For example, &lt;u1, u2&gt; (u1, u2 ∈ U ) in a social network is represented by the feature set {the number of common tags, the number of common items}. The edge &lt;t, u&gt; (u ∈ U, t ∈ T ) is represented by the feature {the times of t being used by u}. Each feature has a feature weight. The edge weight is decided by both the features and the feature weights.</p><p>• User u and item i for recommendation are represented by a constant feature but their feature weights are learned separately.</p><p>• OptRank learns the feature weights by maximizing the average AUC (Area under the ROC Curve) of the tag recommender.</p><p>Although graph based methods have been studied in the field of personalized tag recommendation by many researchers <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b17">17]</ref>, most of them belong to the unsupervised approach, in which the edge weights and restart probabilities of u and i are empirically assigned. Inspired by the recent development of semi-supervised learning <ref type="bibr" target="#b3">[3]</ref> and graph-based learning <ref type="bibr" target="#b1">[1]</ref>, we are able to turn the existing unsupervised graph-based methods into supervised methods. More specifically, we extend the supervised random walk proposed in <ref type="bibr" target="#b1">[1]</ref> for link prediction into the setting of personalized tag recommendation. This paper has two major differences from <ref type="bibr" target="#b1">[1]</ref> : <ref type="bibr" target="#b1">(1)</ref> The graph in our setting contains different types of edges, each of them has their own set of features and the corresponding feature weights are learned separately. (2) Since we have two nodes for restart, we further introduce node features.</p><p>To summarize, our contributions are as follows:</p><p>• To solve the cold start problem due to data sparsity, we are among the first to explore the three new relations: social network, tag semantic relatedness and item content similarities. The remainder of this paper is organized as follows. The problem we addressed is formulated in Section 2. Graph model and random walk with restart are introduced in Section 3. Our optimization framework OptRank is introduced in Section3. Experimental study is described in Section 5. Related work is introduced in Section 6. We conclude the paper and discuss the future work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM STATEMENT AND BASIC FR-AMEWORK</head><p>Personalized Tag Recommendation. Given a user u and an item i, personalized tag recommendation tries to find tags to describe or classify the item i precisely according to u's vocabulary. Inter-relations and intra-relations among users, items and tags are considered, which makes the graph as a multi-type graph (as shown in Figure <ref type="figure" target="#fig_1">1</ref>). Highly ranked tags should be relevant to both u and i. To achieve this goal, a random walk with restart is performed on the multi-type graph with restart at user u and item i. Only tags that are both near to u and item i can get a high visiting probability. Formally, the random walk with restart is performed according to the following equation:</p><formula xml:id="formula_0">  p U p T p I   (t+1) = (1 -α)A   p U p T p I   (t) + α   q U 0 q I  <label>(1)</label></formula><p>where • α is the restart probability. (1 -α) means that the random walker has the probability of (1-α) to perform a random jump based on his current state.</p><formula xml:id="formula_1">• p T = (p T U , p T T , p T I</formula><p>) is a vector of visiting probabilities of all nodes. p T contains the ranking scores for each tag.</p><p>• A is the transition matrix that stores graph structure information. A is obtained by normalizing each column of the adjacency matrix A to sum to 1. • q T = (q T U , 0 T , q T I ) is the preference vector that contains the restart probabilities of each node. q is obtained by normalizing the node weight vector q to sum to 1. The transition matrix A and the preference vector q will be introduced in detail in Section 3.</p><p>Optimization Framework To get a good ranking by following Equation 1, the transition matrix A and the preference vector q need to be carefully assigned. Thus we develop an optimization framework called OptRank.</p><p>Given a user u and an item i for personalized tag recommendation, suppose u has finally annotated i with tags (t1, t2, ...t k ), these tags are defined to be positive tags, denoted by P T . The rest tags are defined to be negative tags, denoted by N T . In other words, the whole tag set T is divided into two parts, i.e. T = P T ∪ N T .</p><p>A good ranking function defined by Equation 1 should rank all the positive tags higher than the negative tags. For a randomly picked positive tag t1 and a negative tag t2, a good ranking function has a high probability of ranking t1 higher than t2. This is the idea of AU C (Area Under the ROC Curve) metric. Formally, AU C is defined by the following equation:</p><formula xml:id="formula_2">AU C = i∈P T j∈NT I(p T (i) -p T (j)) |P T ||N T |<label>(2)</label></formula><p>where I(x) is 1 when x &gt; 0. Otherwise I(x) is 0. Our goal is to find the best transition matrix A and the preference vector q to maximize the AU C. To achieve this, edges are represented by features X and nodes u and i are represented by features Y. To better illustrate the idea, we can assume the adjacency matrix A only contains edges of the same type. A with different types of edges will be introduced in Section 3.1.</p><p>• Each edge &lt;v, u&gt; (u,v∈ U ∪ T ∪ I) is represented by a feature vector X(u, v). Let θ represent the vector of feature weights, the edge weight</p><formula xml:id="formula_3">A(u, v) is computed by A(u, v) = f edge (θ T X(u, v))</formula><p>, where f edge :R → R + . • User u and item i are respectively represented by feature vector<ref type="foot" target="#foot_4">5</ref> YU = (1) and YI = (1). Let ξ denote the feature weights. The node weights q U (u) and q I (i) are computed by q U (u)=f node (ξ T U YU ) and q I (i)=f node (ξ T I YI ), where f node :R → R + . Other entries of q U and q I are all 0.</p><p>According to the above representation, the transition matrix A and the adjacency matrix A can be rewritten to A(θ) and A(θ). q and q can be rewritten to q(ξ) and q(ξ). This means they are respectively decided by parameters θ and ξ. Since the random walk is defined by A(θ) and q(ξ) according to Equation 1, we know that p can be rewritten to p(θ, ξ), which means the final ranking scores are parameterized by θ and ξ. However, to make the following formulae more clear, we will not rewrite the above notations with parameters θ and ξ.</p><p>With edges and nodes parameterized by θ and ξ, we give a formal description of our optimization framework. Given a user u and an item i for tag recommendation and the positive tag set, the optimization problem is max</p><formula xml:id="formula_4">θ,ξ AU C(θ, ξ) = i∈P T j∈NT I(p T (i) -p T (j))</formula><p>|P T ||N T | However, the above equation only considers a single training instance. When m instances {&lt; u k , i k , P T k &gt;} m k=1 are considered, the cost function J(θ, ξ) is defined as the average AU C:</p><formula xml:id="formula_5">max θ,ξ J(θ, ξ) = 1 m m k=1 i∈P T k j∈NT k I(p T (i) -p T (j)) |P T k ||N T k | (3) where N T k = T -P T k .</formula><p>The optimization framework OptRank and its solution will be introduced in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GRAPH MODEL</head><p>Before introducing the optimization problem, we first introduce more details about Equation 1. Section 3.1 introduces the transition matrix. Section 3.2 describes the preference vector. Section 3.3 introduces more intuitions and details of the random walk with restart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transition Matrix</head><p>Transition matrix stores the graph structure information. Before defining the transition matrix, we first introduce how to construct a graph from a social tagging system. The graph shown in Figure <ref type="figure" target="#fig_1">1</ref> is constructed with three steps: (1) Users, tags and items are mapped as the nodes. (2) All the binary relations, i.e., social network, tag semantic relatedness and item content similarities are mapped to edges. (3) For ternary relation &lt;user, tag, item&gt; where three nodes are involved, binary relations can be derived by projections on each dimension. For example, suppose we have &lt;u, t, i&gt; (u ∈ U , t ∈ T , i ∈ I), &lt;i, t&gt; can be derived by projecting on the user dimention. &lt;i, t&gt; is described by the feature which is the times of i annotated with t.</p><p>Now we define the adjacency matrix. Let G denote the whole graph as shown in Figure <ref type="figure" target="#fig_1">1</ref> and A denote its adjacency matrix. Let GMN (M, N ∈ {U, T, I}) denote the each sub-graph made up by relation &lt;m,n&gt; (m ∈ M, n ∈ N ) and AMN denote its adjacency matrix. We have G= M,N∈{U,T,I} GMN and A is composed of sub-matrices AMN :</p><formula xml:id="formula_6">A =   AUU AUT AUI AT U AT T AT I AIU AIT AII  <label>(4)</label></formula><p>Recall that edges are represented by features. In Section 2, the edge feature set is denoted by X and the feature weights is denoted by θ. Since different types of edges have different features and feature weights. We have X={XMN | M, N ∈{U, T, I}} and θ={θMN |M, N ∈ {U, T, I}}. Given an edge &lt;m,n&gt;∈ {M, N }, AMN (m,n) is defined by</p><formula xml:id="formula_7">AMN (m, n) = f edge (θ T M N XMN (m, n))<label>(5)</label></formula><p>Note that XMN (m, n) is a vector and XMN is an array of three dimensions. In this paper, f edge : R → R + is the sigmoid function:</p><formula xml:id="formula_8">f edge (x) = 1 1 + e -x<label>(6)</label></formula><p>Transition matrix A is obtained by normalizing each column of A:</p><formula xml:id="formula_9">A =   AUU D -1 U AUT D -1 T AUI D -1 I AT U D -1 U AT T D -1 T AT I D -1 I AIU D -1 U AIT D -1 T AII D -1 I  <label>(7)</label></formula><p>where DU , DT and DI are diagonal matrices. The i-th entry in the diagonal of DU is the out-degree of the i-th user. For u ∈ U , we have</p><formula xml:id="formula_10">D -1 U (u, u) = 1 M ∈{U,T,I} |M | k=1 AMU (k, u)<label>(8)</label></formula><p>DT and DI are defined in the same way. Following this definition, each column of A will be normalized to sum to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preference Vector</head><p>Given a user u and an item i for tag recommendation, the preference vector q T = (q T U , 0 T , q T I ) specifies the restart probability at u and i. As introduced in Section 2, User u and item i are respectively represented by feature vector YU = (1) and YI = (1). Let ξ = {ξ U , ξ I } denote the feature weights. Node weight</p><formula xml:id="formula_11">q M (m) (M ∈ {U, I}, m ∈ {u, i}) is computed by q M (m) = f node (ξ T M YM )<label>(9)</label></formula><p>The other entries of q U and q I are all set to 0. f node : R → R + is the sigmoid function in this paper:</p><formula xml:id="formula_12">f node (x) = 1 1 + e -x (10) u 1 u 2 u 3 1 t 2 t 3 i 2 i 1 i 3 t 4 Figure 2:</formula><p>The random walker restart at u1 and i1 in no more than 2-hops</p><p>The preference vector q T = (q T U , 0 T , q T I ) is obtained by normalizing q T = (q T U , 0, q T I ) to sum to 1:</p><formula xml:id="formula_13">q =   q U D -1 q 0 q I D -1 q   (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>where Dq is the summation of each entry in q U and q I . Formally, D -1 q is defined as the following equation:</p><formula xml:id="formula_15">D -1 q = 1 M ∈{U,I} |M | k=1 q M (k)<label>(12)</label></formula><p>Equation 11 ensures that q sums to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Random Walk With Restart</head><p>In this section, we introduce more intuitions of the random walk with restart for personalized tag recommendation.</p><p>As we introduced in Section 2, the random walker can frequently restart at u and i to rank the tags. We illustrate this idea with an example shown in Figure <ref type="figure">2</ref>. In Figure <ref type="figure">2</ref>, we want to recommend tags for user u1 to annotate i1, so the random walker restarts frequently from u1 and i1. The edges indicate how the random walker jumps from node to node. u1 has the history that she/he has annotated i2 before. i1 has the history that it has been annotated by u3. Besides annotation relation, u2 is a friend of u1, i3 has similar contents with i1, and t4 has high semantic relatedness with t1. Now we discuss how the random walker behaves in no more than two hops from u1 and i1:</p><p>• When the random walker is only allowed to jump one hop from user u1 and item i1, the recommended tags either have been used by user u1 or have been annotated on item i1 by other users. As we can see from Figure <ref type="figure">2</ref>, t1 is such a tag. When u1 has annotated many items and i1 has been annotated by many users, the random walker will find the best common tags in both sets of u1's tags and i1's tags.</p><p>• When the random walker is allowed to jump within two hops, the recommended tags come from different sources: (1) Items annotated by u1. For example, i2 has been annotated by u1 and i2 has a tag t2. t2 may reflect the interests of u1. (2) Users that have annotated item i1. Since u3 has annotated i1, the tags annotated by u3 may reflect the content of i1.</p><p>(3) Friends of u1. u2 is a friend of u1 and his/her tags may also be adopted by u1. (4) Similar items. Since i3 and i1 have similar content, the tags of i3 may also be the tags of i1.</p><p>(5)Semantically related tags. t4 and t1 are semantically related, which means that they may co-occur in the annotation. When data is sparse, i.e., u1 and i1 are both inactive, more information can be taken into account by jumping more than two-hops away. Now we introduce another intuition behind the random walk. With the transition matrix A defined by Equation <ref type="formula" target="#formula_9">7</ref>, we can rewrite Equation 1 as follows:</p><formula xml:id="formula_16">p U = (1 -α)(AUU p U + AUT p T + AUI p I ) + αq U (13) p T = (1 -α)(AT U p U + AT T p T + AT I p I ) (<label>14</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">p I = (1 -α)(AIU p U + AIT p T + AII p I ) + αq I (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>where AMN =AMN D -1 N (M, N ∈ {U, T, I}). AMN p N (M, N ∈ {U, T, I}) means that p N is spread to its neighbor nodes through the transition matrix AMN .</p><p>First we discuss the extreme case that α equals to 0. Taking p T as an example, p T receives scores from p U through AT U , p T through AT T and p I through AT I . For t ∈ T , p T (t) will have a high score if t has highly ranked user neighbors, tag neighbors and item neighbors. The same rule applies to p U and p I . In other words, users, tags and items reinforce each other iteratively until a stable state is reached. However, there is no personalized information considered. Given a user u and an item i for tag recommendation, when α is greater than 0, the random walker will restart at u and i. Besides reinforcement rule, p U , p T and p I are also influenced by the distance from u and i. Nodes that are near to u and i will get a higher ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">OPTIMIZATION BASED FRAMEWORK</head><p>In this section, we focus on how to find the best feature weights to achieve an optimal random walk with restart. Section 4.1 describes the objective function for optimization. Section 4.2 introduces how to solve the optimization problem. Section 4.3 introduces the derivatives of the random walk with respect to the feature weights, which belongs to the details in solving the optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Objective Function</head><p>As we introduced in Section 2, we want to maximize the average AUC of the tag recommender according to Equation 3. To convert this problem into a minimization problem, we can rewrite Equation 2 to an equivalent form:</p><formula xml:id="formula_20">AU C = 1 - i∈P T j∈NT I(p T (j) -p T (i)) |P T ||N T |<label>(16)</label></formula><p>This equation tells us that to maximize AUC is equivalent to minimize</p><formula xml:id="formula_21">i∈P T j∈NT I(p T (j) -p T (i))/|P T ||N T |.</formula><p>We propose an equivalent minimization problem of Equation <ref type="formula">3</ref>:</p><formula xml:id="formula_22">min θ,ξ J(θ, ξ) = 1 m m k=1 i∈P T k j∈NT k I(p T (j) -p T (i)) |P T k ||N T k |<label>(17</label></formula><p>) Since J(θ, ξ) is not differentiable, we can use the sigmoid function with parameter β as a differentiable approximation:</p><formula xml:id="formula_23">S(x; β) = 1 1 + e -βx<label>(18)</label></formula><p>The bigger the β is, the smaller the approximate error is. However, when β is big, the steep gradient will cause a numerical problem. β is empirically assigned. Now we have a new objective function:</p><formula xml:id="formula_24">min θ,ξ J(θ, ξ) = 1 m m k=1 i∈P T k j∈NT k S(p T (j) -p T (i)) |P T k ||N T k | (19)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Solving the Optimization Problem</head><p>We use gradient descent to solve the optimization problem. The basic idea of gradient descent is to find the direction (gradient) that the objective function drops down and make a small step towards the direction to update and ξ. However, the cost function defined in Equation 19 requires to sum up all the training instances to perform one update, which is too costly. So we update θ and ξ based on each training instance, which is called stochastic gradient descent. The algorithm is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Stochastic Gradient Descent</head><p>Input: m training instances lr: learning rate Output: optimal θ and ξ 1 t=0; 2 initialize θ (0) and ξ (0) ; 3 while J(θ, ξ) has not converged do 4</p><p>Randomly shuffle the m training instances;</p><formula xml:id="formula_25">foreach training instance k do 5 θ (t+1) = θ (t) -lr ∂J k (θ (t) ,ξ (t) ) ∂θ 6 ξ (t+1) = ξ (t) -lr ∂J k (θ (t) ,ξ (t) ) ∂ξ 7 t = t + 1;</formula><p>where J k (θ, ξ) is the cost based on the k-th instance:</p><formula xml:id="formula_26">J k (θ, ξ) = i∈P T k j∈NT k S(p T (j) -(p T (i)) |P T k ||N T k |<label>(20)</label></formula><p>Learning rate lr decides the step size towards the dropping direction. The random shuffle at Line 4 is required by stochastic descent for convergence. The updating rules for θ and ξ are shown in Lines 5 and 6. We will discuss how to compute ∂J k (θ, ξ)/∂θ and ∂J k (θ, ξ)/∂ξ in detail in the following.</p><formula xml:id="formula_27">∂J k (θ, ξ) ∂θ = i∈P T k ∧j∈NT k ∂S(δ ji ) ∂δ ji ∂p T (j) ∂θ -∂p T (i) ∂θ |P T k ||N T k | (21) ∂J k (θ, ξ) ∂ξ = i∈P T k ∧j∈NT k ∂S(δ ji ) ∂δ ji ∂p T (j) ∂ξ - ∂p T (i) ∂ξ |P T k ||N T k |<label>(22)</label></formula><p>where δji = p T (j) -p T (i). ∂S(δji)/∂δji is easy to compute. According to Equation <ref type="formula" target="#formula_23">18</ref>, we can derive that ∂S(δji)/∂δji = βS(δji)(1 -S(δji)). The remaining question is how to compute ∂p T (j)/∂θ and ∂p T (i)/∂ξ, which will be discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Derivatives of the Random Walk</head><p>In this section, we will discuss how to compute the derivatives of the random walk. Suppose p T = (p T U , p T T , p T I ) T , we want to compute ∂p/∂θ and ∂p/∂ξ. The basic idea is that we can derive similar iterative to compute derivatives from the definition of random walk. Derivatives with respect to θ. Since ∂p/∂θ is composed of ∂p/∂θMN (M, N ∈ {U, T, I}), without loss of generality, we introduce how to compute ∂p/∂θUU . Taking the derivatives with respect to θUU on both sides of the Equations 13, 14, 15, we can get</p><formula xml:id="formula_28">∂p U ∂θUU = (1 -α) N∈{U,T,I} AUN ∂p N ∂θUU + ∂AUN ∂θUU p N (23) ∂p T ∂θUU = (1-α) N∈{U,T,I} AT N ∂p N ∂θUU + ∂AT N ∂θUU p N (24) ∂p I ∂θUU = (1 -α) N∈{U,T,I} AIN ∂p N ∂θUU + ∂AIN ∂θUU p N (25)</formula><p>Following the same rule, we can compute the derivatives with respect to any θMN (M, N ∈ {U, T, I}), which all lead to the same form with the above three equations. To better illustrate the connections between computing p and computing ∂p/∂θMN , we can rewrite the above three equations with θUU replaced by θMN in the matrix form:</p><formula xml:id="formula_29">   ∂p U ∂θ M N ∂p T ∂θ M N ∂p I ∂θ M N    = (1-α)A    ∂p U ∂θ M N ∂p T ∂θ M N ∂p I ∂θ M N   +(1-α) ∂A ∂θMN   p U p T p I   (<label>26</label></formula><formula xml:id="formula_30">)</formula><p>where A is the transition matrix defined in the original random walk. Comparing the above equation with Equation 1 for computing p, we can find two differences: (1) p is replaced by ∂p/∂θMN . (2) The last term on the right side is totally changed. However, only the first term (1α)A∂p/∂θMN decides whether Equation 26 will converge to a stable state. More details about the convergence are discussed in the appendix.</p><p>The last detail is how to compute ∂A/∂θMN . Without loss of generality, we discuss how to compute ∂A/∂θUU . Recall that A is composed of sub-matrices {AMN |M, N ∈ {U, T, I}} and not all AMN are related with ∂θUU . According to Equation 7, only AUU , AT U and AIU can be influenced by θUU . So we only need to compute ∂AUU /∂θUU , ∂AT U /∂θUU , ∂AIU /∂θUU . Take ∂AUU /∂θUU for example, we can get</p><formula xml:id="formula_31">∂AUU ∂θUU = ∂AUU ∂θUU D -1 U + AUU ∂D -1 U θUU<label>(27)</label></formula><p>Each entry of AUU is defined according to Equation <ref type="formula" target="#formula_7">5</ref>. For u1, u2 ∈ U , we have</p><formula xml:id="formula_32">∂AUU (u1, u2) ∂θUU = ∂f edge (θ T U U XUU (u1, u2)) ∂θUU<label>(28)</label></formula><p>Each entry in the diagonal of D -1 U is the out-degree of a user. According to Equation 8, for u ∈ U , the derivative is </p><formula xml:id="formula_33">∂D -1 U (u, u) ∂θUU = - |U | k=1 ∂A U U (k,u) ∂θ U U ( M ∈{U,T,I} |M | k=1 AMU (k, u)) 2<label>(</label></formula><formula xml:id="formula_34">∂p U ∂ξ U = (1 -α) N∈{U,T,I} AUN ∂p N ∂ξ U + α ∂q U ∂ξ U (30) ∂p T ∂ξ U = (1 -α) N∈{U,T,I} AT N ∂p N ∂ξ U + α ∂q T ∂ξ U<label>(31)</label></formula><formula xml:id="formula_35">I ∂ξ U = (1 -α) N∈{U,T,I} AIN ∂p N ∂ξ U + α ∂q I ∂ξ U<label>(32)</label></formula><p>Following the same rule, ∂p/∂ξ I can also be obtained. Replacing ξ I with ξ M (M ∈ {U, I}), we can rewrite the above three equations to a single equation in the matrix form:</p><formula xml:id="formula_36">   ∂p U ∂ξ M ∂p T ∂ξ M ∂p I ∂ξ M    = (1 -α)A    ∂p U ∂ξ M ∂p T ∂ξ M ∂p I ∂ξ M    + α    ∂q U ∂ξ M ∂q T ∂ξ M ∂q I ∂ξ M    (33)</formula><p>From the above equation, we can see that computing ∂p/∂ξ M also has the same form with Equation <ref type="formula" target="#formula_0">1</ref>. More details on the convergence will be discussed in the appendix. The last detail is how to compute ∂q/∂ξ M (M ∈ {U, I}). Without loss of generality, suppose M is U, according to Equation <ref type="formula" target="#formula_13">11</ref>, we have</p><formula xml:id="formula_37">∂q ∂ξ U =     ∂q U ∂ξ U D -1 q + q U ∂D -1 q ∂ξ U 0 q I ∂D -1 q ∂ξ U    <label>(34)</label></formula><p>Each entry of q U is defined according to Equation <ref type="formula" target="#formula_11">9</ref>. For u ∈ U , we have</p><formula xml:id="formula_38">∂q(u) ∂ξ U = ∂f node (ξ T U YU ) ∂ξ U (35)</formula><p>When f node is the sigmoid function, we know that df node (x)</p><formula xml:id="formula_39">/ dx = [f node (x)][1 -f node (x)]. D -1</formula><p>q is defined according to Equation 12 and the derivative is</p><formula xml:id="formula_40">∂D -1 q ∂ξ U = - |M | k=1 ∂q U (k) ∂ξ U ( M ∈{U,I} |M | k=1 q M (k)) 2<label>(36)</label></formula><p>So far we have described how to compute ∂q/∂ξ U . The same process can be performed to compute ∂q/∂ξ I To sum up, we have introduced how to compute ∂p/∂θ and ∂p/∂ξ, which can be summarized by Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We test OptRank on two publicly available datasets 6 : Delicious and Last.fm, which are published by <ref type="bibr" target="#b2">[2]</ref>  Inter-relation. For edge &lt;m,n&gt; (m ∈ M ∧n ∈ N ∧M, N ∈ {U, T, I} ∧ M = N ), the feature vector X M N (m, n) = (the times of m co-occurred with n in the posts). For example, suppose we have &lt;u, t&gt; (u ∈ U ∧ t ∈ T ), XUT (u, t) = (the times of u co-occurred with t in the posts), which means the times of t used by u. In our experiments, we use the same feature set to denote &lt;m,n&gt; and &lt;n,m&gt;. This means that AMN and ANM are both decided by ξ M N and θMN .</p><p>User Relation. User relations are formed by the social network. Each relation is bi-direction and binary weighted. To find the strength of a user relation, we check their items and tags in common. More formally, user u can be represented by an item vector AIU (•, u) and a tag vector AT U (•, u). Each entry of AIU and AT U is re-weighted by TF-IDF. Users and items can be viewed as documents and words in the information retrieval. Let AMN (M, N ∈ {U, T, I}) denote the AMN re-weighted by TF-IDF. For edge &lt;u1, u2&gt; (u1, u2 ∈ U ), the feature vector</p><formula xml:id="formula_41">XUU (u1, u2) = [cos( AT U (•, u1), AT U (•, u2)), cos( AIU (•, u1), AIU (•, u2))]</formula><p>Tag Relation. Tag semantic relatedness is computed with the help of Wikipedia<ref type="foot" target="#foot_5">7</ref> . To be more specific, 47% tags are article titles in Wikipedia. Articles link to each other by anchor texts. Semantic relatedness of tag pairs can be inferred from the the number of links between article pairs. We use WikipediaMiner <ref type="bibr" target="#b10">[10]</ref>, which is an off-the-shelf tool, to calculate semantic relatedness. Only tag pairs that have semantic relatedness larger than 0.25 are retained. To refine the edge weights, tags are also represented by user vectors and item vectors. We perform the same TF-IDF weighting technique to AUT and AIT . Let AUT and AIT denote the TF-IDF weighted matrix. For edge (t1, t2) (t1, t2 ∈ T ), edge</p><formula xml:id="formula_42">feature XT T (t1, t2) = [semantic relatedness, cos( AUT (•, t1), AUT (•, t2)), cos( AIT (•, t1), AIT (•, t2))].</formula><p>Item Relation. We calculate item similarities based on Web page titles in Delicious. A title is a vector of words with TF-IDF weighting on each entry. Besides content similarities, we refine edge weight with TF-IDF weighted AUI and</p><formula xml:id="formula_43">AT I . For &lt;i1, i2&gt; (i1, i2 ∈ I), XII (i1, i2) = [cos(title1, title2), AUI (•, i1), AUI (•, i2)), cos( AT I (•, i1), AT I (•,<label>i2</label></formula><p>))] Like logistic regression, we add a constant feature 1 to each feature set XMN and all the features are normalized to have mean 0 and standard deviation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>Since OptRank is an extension of existing graph-based methods, we want to prove two points: (1) OptRank outperforms existing graph-based methods when only &lt;user, tag, item&gt; is available. (2) OptRank further improves the performance by incorporating social networks, tag semantic relatedness, item content similarities. we choose two graph based methods as our baselines.</p><p>Random Walk with Restart. Random Walk with Restart, called RWR for short, is the unsupervised version of Op-tRank. RWR performs on the graph defined by &lt;user, tag, item&gt;. The weight of the edge &lt;m,n&gt;</p><formula xml:id="formula_44">(m ∈ M ∧ n ∈ N ∧ M, N ∈ {U, T, I} ∧ M = N )</formula><p>is the times of m cooccurred with n in the posts. Given a user u and an item i for tag recommendation, when the random walker decides to restart, it has the probability of 0.5 to restart at u and 0.5 to restart at i. RWR has been adopted in <ref type="bibr" target="#b6">[6]</ref> to incorporate social networks, but the different types of edges are normalized empirically and are hard to reproduce. In our experiments, we set the damping factor to 0.7, which achieves the best performance for FolkRank. In our experiments, FolkRank is denoted by 'FR'.</p><p>We are aware that there are many methods based on tensor factorization <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15]</ref>. However, tensor factorization needs to learn a low rank approximation vector for each user, item and tag. In OptRank, a user can even not exist in the training set but can still get recommendation if she/he has neighbors in the test set. OptRank only needs about 3000 training instances to reach its best performance. However, tensor factorization would fail with such a small training set, which is unfair. For this reason, we did not choose these methods as baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Methodology</head><p>Performance Measurements. We use average precision, precision-recall curve and average AUC (Area under the ROC Curve) to measure the performance. We are aware that the optimal of AUC is not necessarily the optimal of average precision/recall. To trade-off between best AUC and best precision, we choose the model that has both high AUC and precision in the cross validation set. Then the model is evaluated on the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Parameters</head><p>β and learning rate. β in Equation 18 controls the error of approximating I(x). The bigger β is, the smaller the approximate error is. However, when β gets too big, the derivative at x = 0 will also get too steep and will cause a numeric problem. When β gets too small, minimizing J(θ, ξ) would fail to maximize AUC. From Equations 21 and 22, we can know that the summation of the derivatives is divided by</p><formula xml:id="formula_45">|P T k ||N T k |. Since large dataset has big |P T k ||N T k |,</formula><p>we can use a big β. In our experiments, β is 10 9 in Delicious and 10 6 in Last.fm. Learning rate lr is strongly related to β. When lr gets too big, stochastic gradient descent would fail to converge. lr is set to 10 in both datasets. Restart Probability α. α controls how frequently the random walker chooses to restart. We evaluate how AUC and precision change by differing α from 0.2 to 0.8 in Delicious. The results are shown in Figure <ref type="figure" target="#fig_4">3</ref>. OptRank was run on the inter-relation formed by &lt;user, tag, item&gt;. When precision and AUC are both considered, α ∈[0.6, 0.8] seems to be a good choice. Finally, we set α to 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experimental Results</head><p>Results on Delicious. Results on Delicious are shown in Table <ref type="table" target="#tab_3">1</ref> and Figure <ref type="figure" target="#fig_5">4</ref>. FR denote FolkRank. OptRank Edge, OptRank Node and OptRank EN denote OptRank with only edge features enabled, only node features enabled , and both features enabled, respectively. Firstly, we compare the algorithms that are only performed on inter-relations formed by &lt;user, tag, item&gt;. Since RWR always performs better than FR, we only compare OptRank with RWR in the following. When only edge features are enabled, OptRank Edge has comparable performance with RWR. This indicates that the original transition matrix of RWR and FR is nearly optimal. When only node features are enabled, OptRank Node learns the best weights for u and i, which improves the top-1 precision by 3.3% compared with RWR. This indicates the original node weight is not optimal. When edge features and node features are both enabled, OptRank EN futher improves the top-1 precision by 2.4% based on OptRank Node. From Figure <ref type="figure" target="#fig_5">4</ref> we can know that the OptRank EN outperforms RWR at top-5 but the advantage disappears at top-10. However, since a user usually annotates an item with less than 5 tags, top-5 performance is considered more important than top-10 performance. In terms of AUC, FolkRank  In contrast, RWR has a much better average AUC. This is probably because FolkRank is an empirically designed algorithm and relies too much on the global information. We can see that a high precision does not indicate a high AUC. Now we discuss how OptRank performs when extra user relations, tag relations and item relations exist. When each type of relation is considered separately, OptRank U, Op-tRank T and OptRank I improve the top-1 precision by around 1% based on OptRank EN, which is not very significant compared with previous improvement. However, as we can see from Figure <ref type="figure" target="#fig_5">4</ref>, the top-10 performance of OptRank I is significantly improved compared with OptRank EN. Since OptRank U, OptRank T and OptRank I are comparable, only OptRank I is shown in Figure <ref type="figure" target="#fig_5">4</ref>. When all the relations are combined, we can see that OptRank UTI achieves the best performance at all top-k performance. In terms of AUC, OptRank UTI also achieves the best performance.</p><p>Results on Last.fm. The results are shown in Figure <ref type="figure" target="#fig_6">5</ref> and Table <ref type="table" target="#tab_4">2</ref>. The results are significantly better than the results on Delicious. This can be explained in terms of data sparsity. When only inter-relations are considered, a post can be viewed as an entry in the three-dimension array spanned by users, tags and items. 1.05 × 10 (-7) and 0.83 × 10 (-7) of the entries in Last.fm and Delicious are known, respectively. Thus Last.fm is less sparse and more predictable than Delicious. In Last.fm, all algorithms have comparable performance at top-10. So we mainly focus on the top-5 performance in this experiment and this is reasonable since users usually annotate an item within 5 tags. From Figure <ref type="figure" target="#fig_6">5</ref> we can know that FolkRank and RWR have comparable performances in term of precision, which is different from the results on Delicious. When node features and edge features are both considered, OptRank EN improves P@1, P@2 and P@3 by 1.2%, 1.5%, 1.5% respectively compared with FR and RWR. Although Last.fm is less sparse than Delicious,   <ref type="formula" target="#formula_2">2</ref>) OptRank successfully combined extra relations to improve the performance. Now we discuss some details about the training process. Since Delicious is bigger and takes more time, the training size and running time are reported according to Delicious. Over-fitting Issues. Over-fitting does not seem to be a problem in our model since we only have 18 parameters when all the relations are combined<ref type="foot" target="#foot_6">8</ref> . OptRank UTI achieves nearly the same top-1 precision in the cross validation set and test set. Training Size. The training size is really small compared with tensor factorization. OptRank EN achieves its best performance when 600 training instances are passed. Op-tRank UTI achieves its best performance when 1200 training instances are passed. Running Time. The experiments were conducted on a single PC with a 2-core 3.2GHz CPU and 2G main memory. We implemented the algorithm in Matlab with full vectorization. When all the relations are combined, each training instance takes nearly 3.5 seconds. Prediction takes around 0.1 seconds per instance and most of the time is spent on computing the gradients. Training with 5000 instances would take 4.8 hours at most. However, all the algorithms in our experiments achieve their best performance within 2000 training instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>There are mainly three approaches for personalized recommendation in social tagging systems: (1) Graph-based approach <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b17">17]</ref>. <ref type="bibr" target="#b2">(2)</ref> Tensor decomposition <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15]</ref>. The annotation relation is modeled as a cube with many unknown entries. After performing tensor decomposition, we can predict the unknown entries by low-rank approximations. (3) User/Item based collaborative filtering <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">18]</ref>. The original user-item matrix is extended by including tag information so that we can apply user/item based collaborative filtering methods.</p><p>Besides annotation behaviors, user space, tag space and item space have also been explored. <ref type="bibr">[9]</ref> has studied trust networks and proposed a factor analysis approach based on probabilistic matrix factorization. <ref type="bibr" target="#b6">[6]</ref> incorporates social network for item recommendation, but fails to improve the performance significantly. <ref type="bibr" target="#b14">[14]</ref> links social tags from Flickr into WordNet. <ref type="bibr" target="#b7">[7]</ref> introduces item taxonomies into recommender systems.</p><p>This paper is mainly inspired by two recent work on graphbased learning <ref type="bibr" target="#b1">[1]</ref> and semi-supervised learning <ref type="bibr" target="#b3">[3]</ref>. <ref type="bibr" target="#b1">[1]</ref> proposes supervised random walks to learn the edge weights for link prediction in homogenous graph. This paper extends <ref type="bibr" target="#b1">[1]</ref> with multi-type edges and nodes. <ref type="bibr" target="#b3">[3]</ref> has proposed similar idea to learn edge weights and node weights with an inductive learning framework in homogenous graph. Since a recommender should have the ability to predict for future events, our framework is different from <ref type="bibr" target="#b3">[3]</ref> in that ours belongs to transductive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose an optimization-based graph method for personalized tag recommendation. To alleviate data sparsity, different sources of information are incorporated into the optimization framework. There are some problems unsolved for future work: (1) Reducing the graph size. Since the random walker frequently restarts at u and i, nodes that are far away from u and i may be cut without influencing the final ranking. (2) Comparing with tensor factorization methods under a suitable experiment setting.</p><p>(3) More features can be explored to further improve the results, such as the temporal factors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Social Tagging System example, users can annotate and share Web pages in Delicious 1 . Besides Delicious, there are many other social tagging system like Last.fm 2 and YouTube 3 in entertainment domain and CiteULike 4 in the research domain.Personalized tag recommendation is the key part of a social tagging system. When a user wants to annotate an item, the user may have his/her own vocabulary to organize items. Personalized tag recommendation tries to find the tags that can precisely describe the item with the user's vocabulary.A social tagging system, as shown in Figure1, contains heterogeneous information and can be modeled as a graph:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 1 )</head><label>1</label><figDesc>Social network among users.(2) Tag semantic network based on semantic relatedness. (3) Item network based on content similarities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FolkRank.</head><label></label><figDesc>FolkRank is a state-of-the-art graph-based algorithm. The graph is defined in the same way with RWR. FolkRank can be summarized as three steps: (1) Calculate a global PageRank score p global for each node. (2) Calculate a personalized PageRank score p pref with special preference to u and i for each node. (3) Calculate FolkRank score as the wins and loses between the personalized PageRank p pref and the global PageRank p global , i.e., score = p pref -p global .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The effect of α Training/Cross Validation/Test Set. Posts are aggregated into records &lt;u, i, PT&gt; (u ∈ U , i ∈ I). For each dataset, we randomly picked 5000, 3000, and 3000 records as the training set, cross validation set, and test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision-Recall Curve on Delicious</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Precision-Recall Curve on Last.fm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>how to compute ∂p/∂ξ U . Taking the partial derivatives with respect to ξ U on both sides of Equations 13, 14 and 15, we can get</figDesc><table><row><cell>29)</cell></row><row><cell>So far we have explained how to compute ∂AUU /∂θUU . The</cell></row><row><cell>same process can be used for computing ∂AT</cell></row></table><note><p>U /∂θUU and ∂AIU /∂θUU . Derivatives with respect to ξ. Computing ∂p/∂ξ is analogous to computing ∂p/∂θ. Since ∂p/∂ξ is composed of ∂p/∂ξM (M ∈ {U, I}), without loss of generality, we first focus on</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>as benchmarks. Delicious contains 437593 posts involving 1867 users, 40678 tags, 69223 items, 15328 user relations, 197438 tag relations and 151971 item relations. All types of intrarelations we studied are included in Delicious. Posts are represented by &lt;user, tag, item&gt;. Last.fm contains 24164 posts involving 1892 users, 9749 tags, 12523 items and 25434</figDesc><table><row><cell cols="4">Algorithm 2: Derivatives of the random walk</cell></row><row><cell></cell><cell cols="3">Input: Transition matrix A and preference</cell></row><row><cell></cell><cell cols="3">vector q Output: ∂p ∂θ and ∂p ∂ξ</cell></row><row><cell cols="2">1 t=0;</cell><cell></cell></row><row><cell cols="3">2 Initialize p (t)</cell></row><row><cell cols="4">3 while p has not converged do</cell></row><row><cell>4</cell><cell cols="3">p (t+1) = (1 -α)Ap (t) + α q</cell></row><row><cell>5</cell><cell cols="2">t = t + 1;</cell></row><row><cell cols="2">6 t=0;</cell><cell></cell></row><row><cell cols="4">7 Initialize ∂p ∂θ 8 while ∂p ∂θ has not converged do (0) ; 9 Computing ∂p (t+1) according to Equation 26 ∂θ 10 t = t + 1;</cell></row><row><cell cols="2">11 t=0;</cell><cell></cell></row><row><cell cols="2">12 Initialize ∂p ∂ξ</cell><cell>(0) ;</cell></row><row><cell cols="4">13 while ∂p ∂ξ has not converged do</cell></row><row><cell>14</cell><cell cols="2">Computing ∂p ∂ξ</cell><cell>(t+1) according to Equation 33</cell></row><row><cell>15</cell><cell cols="2">t = t + 1;</cell></row><row><cell>6 http://www.grouplens.org/node/462</cell><cell></cell><cell></cell></row></table><note><p>user relations. Last.fm is a smaller dataset and only user relations are available. We introduce each type of relation and its features as follows.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Precision and AUC on Delicious</figDesc><table><row><cell>Algorithm</cell><cell>P@1</cell><cell>P@2</cell><cell>P@3</cell><cell>AUC</cell></row><row><cell>FR</cell><cell>0.219</cell><cell>0.180</cell><cell>0.163</cell><cell>0.6851</cell></row><row><cell>RWR</cell><cell>0.233</cell><cell>0.197</cell><cell>0.179</cell><cell>0.9812</cell></row><row><cell>OptRank Edge</cell><cell>0.234</cell><cell>0.194</cell><cell>0.171</cell><cell>0.9851</cell></row><row><cell>OptRank Node</cell><cell>0.266</cell><cell>0.204</cell><cell>0.179</cell><cell>0.9833</cell></row><row><cell>OptRank EN</cell><cell>0.290</cell><cell>0.239</cell><cell>0.201</cell><cell>0.9862</cell></row><row><cell>OptRank U</cell><cell>0.297</cell><cell>0.247</cell><cell>0.215</cell><cell>0.9862</cell></row><row><cell>OptRank T</cell><cell>0.302</cell><cell>0.245</cell><cell>0.213</cell><cell>0.9862</cell></row><row><cell>OptRank I</cell><cell>0.303</cell><cell>0.242</cell><cell>0.210</cell><cell>0.9863</cell></row><row><cell>OptRank UTI</cell><cell cols="4">0.316 0.262 0.223 0.9869</cell></row><row><cell cols="5">has relatively poor performance, worse than the precision.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Precision and AUC on Last.fm When only &lt;user, tag, item&gt; is available, OptRank outperforms RWR and FolkRank. (</figDesc><table><row><cell>Algorithm</cell><cell>P@1</cell><cell>P@2</cell><cell>P@3</cell><cell>AUC</cell></row><row><cell>FR</cell><cell>0.495</cell><cell>0.410</cell><cell>0.355</cell><cell>0.9509</cell></row><row><cell>RWR</cell><cell>0.495</cell><cell>0.410</cell><cell>0.355</cell><cell>0.9969</cell></row><row><cell>OptRank EN</cell><cell>0.507</cell><cell>0.425</cell><cell>0.369</cell><cell>0.9973</cell></row><row><cell>OptRank U</cell><cell cols="4">0.532 0.436 0.373 0.9975</cell></row><row><cell cols="5">when the social network is combined, OptRank U success-</cell></row><row><cell cols="5">fully improves the top-1 precision by 3.7% compared with</cell></row><row><cell cols="5">the two baselines. In terms of AUC, empirically designed</cell></row><row><cell cols="5">FR still falls behind other methods. OptRank U achieves</cell></row><row><cell>the highest AUC.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">To sum up, we have two conclusions from the experiments:</cell></row><row><cell>(1)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://delicious.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.last.fm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.youtube.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://www.citeulike.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Nodes are allowed to have more than one feature, so YU and YI are still in bold to represent vectors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>http://www.wikipedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Each inter relation has 2 parameters, user relation has 3 parameters, tag relation and item relation has 4 parameters, respectively.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>This work was supported in part by National Basic Research Program of China (973 Program) under Grant No. 2011CB302206, and National Natural Science Foundation of China under Grant No. 60833003.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We prove the convergence of Equations 26 and 33. Both the equations can be rewritten to a more general form:</p><p>where 0≤ λ, µ ≤1, A is a transition matrix with each column summing to 1 and q can be any vector with the same dimension of p. Suppose p (0) = π, we have p (1) = λAπ + µq, p (2) = (λA) 2 π + λAµq + µq, ..., p (n) = (λA) n π + n-1 k=0 (λA) k µq. Since 0≤ λ, µ ≤1 and the eigenvalues of the transition matrix A are in [-1, 1], we have limn→∞(λA) n = 0 and limn→∞ n-1 k=0 (λA) k = (I -λA) -1 . So p (n) finally converges to p * = (I -λA) -1 µq.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supervised random walks: predicting and recommending links in social networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Workshop hetrec RecSys 2011</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cantador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuflik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised ranking on very large graphs with rich metadata</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personalized tag recommendation using graph-based ranking on multi-type interrelated objects</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="540" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tag recommendations in folksonomies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jäschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hotho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stumme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On social networks and collaborative recommendation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nayak</surname></persName>
		</author>
		<title level="m">Personalized recommender system based on item taxonomy and folksonomy. CIKM &apos;10</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1641" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">Connecting users and items with weighted tags for personalized item recommendations. HT &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving recommender systems by incorporating social contextual information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<title level="m">An Open-Source Toolkit for Mining Wikipedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2889</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collaborative filtering in social tagging systems based on joint item-tag recommendations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="809" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning optimal ranking with tensor factorization for tag recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="727" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pairwise interaction tensor factorization for personalized tag recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Flickr tag recommendation based on collective knowledge. WWW &apos;08</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tag recommendations based on tensor dimensionality reduction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Symeonidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified framework for providing recommendations in social tagging systems based on ternary semantic analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Symeonidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A random walk method for alleviating the sparsity problem in collaborative filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<title level="m">Tagicofi: tag informed collaborative filtering. RecSys &apos;09</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
