<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BOLT: BRIDGING THE GAP BETWEEN AUTO-TUNERS AND HARDWARE-NATIVE PERFORMANCE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-25">25 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiarong</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jack</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
						</author>
						<title level="a" type="main">BOLT: BRIDGING THE GAP BETWEEN AUTO-TUNERS AND HARDWARE-NATIVE PERFORMANCE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-25">25 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.15238v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today's auto-tuners (e.g., AutoTVM, Ansor) generate efficient tensor programs by navigating a large search space to identify effective implementations, but they do so with opaque hardware details. Thus, their performance could fall behind that of hardware-native libraries (e.g., cuBLAS, cuDNN), which are hand-optimized by device vendors to extract high performance. On the other hand, these vendor libraries have a fixed set of supported functions and lack the customization and automation support afforded by auto-tuners. Bolt is based on the recent trend that vendor libraries are increasingly modularized and reconfigurable via declarative control (e.g., CUTLASS). It enables a novel approach that bridges this gap and achieves the best of both worlds, via hardware-native templated search. Bolt provides new opportunities to rethink end-to-end tensor optimizations at the graph, operator, and model levels. Bolt demonstrates this concept by prototyping on a popular auto-tuner in TVM and a class of widely-used platforms (i.e., NVIDIA GPUs)-both in large deployment in our production environment. Bolt improves the inference speed of common convolutional neural networks by 2.5x on average over the state of the art, and it auto-tunes these models within 20 minutes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Auto-tuning engines <ref type="bibr" target="#b5">(Chen et al., 2018b;</ref><ref type="bibr">Zheng et al., 2020a;</ref><ref type="bibr" target="#b2">Adams et al., 2019;</ref><ref type="bibr">Zheng et al., 2020b)</ref> are at the heart of a variety of DNN compilers and frameworks <ref type="bibr">(Chen et al., 2018a;</ref><ref type="bibr" target="#b16">Leary &amp; Wang, 2018;</ref><ref type="bibr" target="#b0">Abadi et al., 2016;</ref><ref type="bibr" target="#b24">Paszke et al., 2019)</ref>. Example auto-tuners like AutoTVM <ref type="bibr" target="#b5">(Chen et al., 2018b)</ref> and Ansor <ref type="bibr">(Zheng et al., 2020a)</ref> infer hardware cost models from afar, by executing sample implementations on a particular device and observing their performance. Building on the inferred cost models, auto-tuners take tensor programs as inputs, and navigates a large search space to select effective transformations for high performance. Operating with opaque device models affords generality, as the same approach can be applied to different hardware, without requiring hardware details.</p><p>As a downside, treating devices as opaque models comes with performance implications-for any specific device, it is likely that hardware-native performance as delivered by lowlevel vendor libraries is hard to come by. Traditional vendor libraries (e.g., cuBLAS (NVIDIA, a), cuDNN <ref type="bibr" target="#b7">(Chetlur et al., 2014)</ref>) expose a fixed set of primitives that are heavily hand-optimized for the underlying hardware. For workloads that fit into their library primitives, and for users with ex-pert device knowledge, directly using such libraries extracts hardware-native performance. For instance, auto-tuners like AutoTVM and Ansor do not achieve competitive performance compared to cuBLAS and cuDNN for non-FP32 compute-intensive operators on NVIDIA GPUs because of their inefficient usage of tensor cores. In our benchmarks, Ansor <ref type="bibr">(Zheng et al., 2020a)</ref> only achieves 20% of cuBLAS performance for FP16 GEMMs on NVIDIA Tesla T4 GPUs (see Figure <ref type="figure">1</ref> for more details).</p><p>Related, opaque device models also lead to a prolonged auto-tuning time, as the search process is less informed by hardware details. For instance, it takes AutoTVM <ref type="bibr" target="#b5">(Chen et al., 2018b)</ref> 10 hours on x86 CPUs and 7 days on NVIDIA GPUs to tune all workloads in the ResNet-50 model <ref type="bibr" target="#b30">(Yu et al., 2021)</ref>. This has led to the development of special databases (SAMPL) that cache and reuse tuning logs, but this approach only goes so far. Models have increasing dynamism, not only in terms of dynamic data structures <ref type="bibr" target="#b17">(Liang et al., 2016)</ref> but also dynamic shapes <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>, making caching much less effective. Maintaining these databases also incurs substantial costs.</p><p>Can we achieve the best of both worlds, combining the flexibility of auto-tuners and the hardware-native performance as afforded by vendor implementations? Bolt bridges this gap leveraging the trend that vendor libraries are increasingly templated, reconfigurable with declarative parameters to suit different scenarios, but exposing concise interfaces that are potentially amenable to auto-tuning. An exemplar of a templated design, NVIDIA CUTLASS (NVIDIA, b), encodes efficient design patterns but is not committed to a fixed set of primitives. Users can parameterize the templates to suit their workloads, and they can extend and compose templated primitives for new functionality. In a similar vein, Intel OneDNN (Intel) and AMD ROCm (AMD) also share this emerging trend for their platforms.</p><p>We propose to enable end-to-end DNN model optimizations via hardware-native templated search utilizing the above trend. Bolt operates on hardware-native templates which interposes a thin layer between the high-level computational graph and the low-level CUDA code, opening up opportunities for joint optimization. First, it generates tensor programs with hardware-native performance via efficient templated search. Second, by composing and extending template primitives, Bolt enables novel computational graphlevel optimizations. Combined, Bolt enables auto-tuners to achieve both graph-level and operator-level optimization and generates the implementations with hardware-native performance using a significantly shortened turnaround time. Furthermore, Bolt also enables model-level optimizations by proposing system-model codesign principles. If models are designed in a system-friendly manner, they can fully utilize the benefits of Bolt and achieves more efficient inference.</p><p>We prototype Bolt in TVM <ref type="bibr">(Chen et al., 2018a)</ref> for NVIDIA GPUs utilized its open-sourced CUTLASS library, while noting that the new design approach generalizes beyond this scenario. Compared to Ansor, Bolt achieves 2.5x inference speedup on widely-used convolutional neural networks; it auto-tunes these workloads within 20 minutes. Our new computational graph level optimization-persistent kernel fusion-leads to a performance improvement up to 1.3x and 2.0x on GEMMs and Conv2Ds. Finally, we validate our model-level optimization-system-model codesign-by augmenting RepVGG models <ref type="bibr" target="#b10">(Ding et al., 2021)</ref>, which effectively improves model accuracy with less speed sacrifice. Bolt is deployed in our production setting, serving real models and workloads, and will be released in open source to the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Auto-tuners have a performance gap</head><p>State-of-the-art DNN compilers and frameworks <ref type="bibr" target="#b5">(Chen et al., 2018b;</ref><ref type="bibr">Zheng et al., 2020a)</ref> leverage auto-tuning to identify effective tensor implementations. Auto-tuners transform a tensor program into an equivalent but structurally different one, which delivers higher performance on the target. This is achieved by constructing a cost model of the hardware via learning-e.g., by building a training set with sample programs and their performance on the target, and by predicting which implementations are likely to be  <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>, a widely adopted NLP model, when the batch size is 32 and the sequence length is 40.</p><p>performant when navigating the search space. Operating afar from the hardware delivers benefits such as platform generality, but it also leads to two performance implications.</p><p>Lack of hardware-native performance. With opaque device models, tensor code generated by existing auto-tuners usually has a performance gap for certain workloads (e.g., non-FP32 GEMM/Conv) as compared to hardware-native performance, as delivered by vendor-tuned libraries, such as cuBLAS and cuDNN. As concrete evidence, Figure <ref type="figure">1</ref> benchmarks the FP16 GEMM speed of tensor programs generated by Ansor <ref type="bibr">(Zheng et al., 2020a)</ref>, a state-of-the-art auto-tuner, against hardware-native speeds as achieved by cuBLAS. The auto-tuned program achieves less than 20% of the library performance. The reason is that NVIDIA GPUs have special hardware architecture, tensor cores, to accelerate FP16 computation, but they cannot be efficiently utilized by Ansor that uses an opaque hardware model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inefficient program search.</head><p>Opaque device models and inferred hardware execution costs also lead to a less informed tuning process. Existing auto-tuners spend days or weeks when models have many different workloads, e.g., ResNet-152 and Inception-V3 <ref type="bibr" target="#b30">(Yu et al., 2021)</ref>. Caching and reusing previous tuning logs (SAMPL)) works well for static models, but not those with dynamic data structures <ref type="bibr" target="#b17">(Liang et al., 2016)</ref> or shapes <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>, where the exact workloads are only determined at runtime. In contrast, with hardware-native templated search, Bolt reduces the tuning time to tens of minutes for common models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Emerging trend: Templated libraries</head><p>The optimizations in Bolt are made possible by an emerging trend: vendor libraries are escaping the earlier generation design with fixed primitives and becoming modularized and composable. Controlled by a set of declarative parameters, templates can be instantiated to suit different hardware and workloads. New primitives can be composed from existing ones, and creating new templates also has a lower barrier. In addition, the templated libraries are efficient design patterns that take into account device details, and extract hardware performance at a level impossible from opaque auto-tuning.</p><p>Example: NVIDIA CUTLASS. Of particular interest to us is CUTLASS, an example templated library from NVIDIA. CUTLASS provides reusable software components in C++ templates for every layer of the CUDA programming model for GEMM. With the right parameters, it achieves high performance for thread-wide, warp-wide, block-wide, and device-wide primitives. Such templates leverage intricate device knowledge, specifically tensor cores as integrated in NVIDIA Volta, Turing, and Ampere GPUs, and optimize for a wide range of mixed-precision computations including B1, INT4, INT8, FP16, BF16, FP32, TF32, FP64, complex, and quaternion. By plugging in the right tile size, data type, and other parameters, users can tap into device-level performance for their workloads. Beyond this example, Intel and AMD also exhibit similar trends in their designtemplated libraries with parameterized control. This design principle, therefore, is generalizable to other platforms.</p><p>CUTLASS leverages GPU tiling structures for efficient GEMM implementation by decomposing GEMMs into a hierarchy of threadblocks and warp tiles. It optimizes data movement for locality and carefully controls movement from global to shared memory to the register files. Figure <ref type="figure" target="#fig_0">2</ref> illustrates the hierarchy and data movement from slower to faster storage for GEMM operation</p><formula xml:id="formula_0">C = A ? B. Figure 2(a)</formula><p>shows the inputs A, B and result C in global memory and their threadblock tiles in color (inputs A, B in pink and yellow and the result C in green). Threadblock tiles can be divided into warp tiles in shared memory as shown in Figure <ref type="figure" target="#fig_0">2</ref>(b). In this example, a threadblock tile can be split into eight warp tiles which can be further partitioned into thread tiles in the register file as shown in Figure <ref type="figure" target="#fig_0">2</ref>(c). From global memory to shared memory and to register files, the memory size is decreasing but the read/write speed is increasing. Tensor cores on NVIDIA GPUs take thread tiles as input and store the output into register files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bolt: The best of both worlds</head><p>Bolt enables end-to-end optimizations that bridge the gap between auto-tuners and hardware-native performance.</p><p>Graph level: Enabling deeper operator fusion. Leveraging the templated design, Bolt opens up new opportunities for operator optimizations. This is because new optimizations can be introduced to the device libraries via template customization. Bolt develops a new operator fusion technique that is called persistent kernel fusion for improved performance. Operator fusion computes multiple operators using only one kernel, reducing data shuffling to memory to improve locality <ref type="bibr" target="#b1">(Abdolrashidi et al., 2019)</ref>, but existing auto-tuner fusions <ref type="bibr">(Chen et al., 2018a;</ref><ref type="bibr" target="#b16">Leary &amp; Wang, 2018;</ref><ref type="bibr" target="#b26">Roesch et al., 2019;</ref><ref type="bibr" target="#b0">Abadi et al., 2016;</ref><ref type="bibr" target="#b24">Paszke et al., 2019;</ref><ref type="bibr" target="#b15">Jia et al., 2019;</ref><ref type="bibr" target="#b1">Abdolrashidi et al., 2019)</ref> do not interact well with performant, device libraries. For instance, computing Conv2D+BiasAdd+Hardswish in a single kernel improves performance, but the resulting operator may not be supported by fixed-function libraries like cuDNN. Via a templated design, Bolt enables new search space that considers deeper fusion, thus opening up graph-level optimizations.</p><p>Operator level: Automating templated code generation. Templated libraries by themselves, however, are too lowlevel for common users. Precisely instantiating the parameters to govern tiling sizes and data types creates a high burden. Also, the templated primitives are simply building blocks, and they need to be assembled into complete DNN models for execution. Bolt conquers their difficulty of use by combining the library primitives and auto-tuners. It designs a light-weight performance profiler to search for the best template parameter automatically. By efficiently using the hardware details, the profiler significantly shortens the search time. The search results are later used to instantiates templates and generate the low-level tensor code with hardware-native performance.</p><p>Model level: System-friendly models. The end-to-end optimizations in Bolt also shed light on efficient model design.</p><p>We propose to design models in a system-friendly manner so that they can efficiently use the optimization provided by the underlying systems to achieve better inference performance. In Bolt, we have summarized three system-model codesign principles and validated them by augmenting several RepVGG models <ref type="bibr" target="#b10">(Ding et al., 2021)</ref>. the model into a relay graph. On this graph, it invokes computational graph optimizations (e.g., deeper fusion) and performs graph partition over the optimized graph. Bolt next performs hardware-native profiling to search for the best kernels for each operator in the Bolt subgraph. Finally, Bolt generates high-performance CUDA code which will be compiled together with the code generated by TVM into a single runtime file. In the ensuing discussion, we start with the graph-level, deeper fusion opportunities enabled by Bolt, and move down to the automated code generation including the light-weight performance profiler and templated code generator, and finally discuss the system-friendly model design principles distilled from Bolt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BOLT DESIGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Enabling deeper operator fusion</head><p>Bolt enables novel graph-level optimizations by extending hardware-native templates. Specifically, Bolt introduces persistent kernels which enable novel deeper operator fusion. As shown in Figure <ref type="figure">4</ref>(a), it works on epilogue fusion as a basis, and further fuses two or more sequential GEMMs/Convs. Fusing multiples GEMMs or Convs into a single operator improves performance in the following ways: (i) eliminating memory traffic for storing and loading inter-layer activations; (ii) eliminating launch latency which is especially beneficial for short kernels with small batch sizes; (iii) enlarging optimization scope for the compiler to explore more instruction scheduling options <ref type="bibr" target="#b29">(Wu et al., 2012)</ref>. Figure <ref type="figure">4</ref>(b) shows the kernel view of persistent kernel fusion.</p><p>Prerequisite: Epilogue fusion. As a prerequisite for persistent kernel fusion, Bolt first integrates the epilogue fusion provided in CUTLASS, which fuses a GEMM/Conv kernel with its following epilogues all together into one operator, so that we can further leverage persistent kernels. The epilogue fusion patterns in CUTLASS include: (i) element-wise operators, (ii) data type conversion, (iii) broadcast vector over columns, and (iv) partial reduction over columns. Bolt iden-  tifies these patterns in the computational graph and generates corresponding algorithmic policy automatically. Bolt takes epilogue fusion as a starting point, and develops deeper fusions using persistent kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Persistent kernel (GEMM/Conv) fusion</head><p>Persistent kernels allow fusing multiple GEMMs or Convs into one operator to improve performance. As illustrated in <ref type="bibr">Figure. 4(b)</ref>, when two GEMM/Conv operations are fused together, the main loops of math computation for the two operators run back-to-back in a single fused kernel. The output activation for the first GEMM/Conv stays in faster GPU memory. This eliminates the need for storing GEMM0/Conv0 output activation back to global memory, launching GEMM1/Conv1 kernel, and loading GEMM1/Conv1 input activation from global memory. Bolt automatically identifies the opportunity to use persistent kernels and generates CUDA code by creating new templates in CUTLASS. We describe the back-to-back GEMM fusion in detail, and convolution fusion works similarly.</p><p>A back-to-back GEMM is defined as:</p><formula xml:id="formula_1">D0 = ? 0 A0 ? W 0 + ? 0 C0, (1) D1 = ? 1 D0 ? W 1 + ? 1 C1,<label>(2)</label></formula><p>with A0, W 0 and W 1 as matrix inputs, ?s and ?s as scalar inputs, and C0, C1 as pre-existing matrices (bias), which will be overwritten by the output. In order to fuse backto-back GEMMs, output activation D0 of the first GEMM layer must be used as input activation of the second GEMM layer. This requires that the M dimension of the GEMM stays the same for all layers. For back-to-back Convs, this requires that all subsequent Convs (from the 2nd) must use 1 ? 1 filter with no padding and a stride of one.</p><p>Key property: Threadblock residence.</p><p>The key challenge of persistent kernels is to compute the 2nd GEMM/Conv without loading its input activation from the global memory. This requires each output threadblock of the 1st GEMM/Conv to remain within the same threadblock memory (either in the shared memory or register files) as its respective input threadblock. We call this threadblock residence. If it does not hold, the 2nd GEMM/Conv has to fetch data from the global memory, eliminating the benefits of persistent kernels. For GEMM fusion, threadblock residence requires ThreadBlock N = GEMM N for each operator. As for Conv fusion, the requirement is Thread-Block N = Conv output channel. Figure <ref type="figure" target="#fig_3">5</ref> visualizes this requirement. With threadblock residence, we develop two designs for different scenarios.</p><p>RF-resident fusion. When the weight matrix W 1 can be completely streamed into a warp tile in its 'N ' dimension (as indicated in Figure . 6), threadblock-residence can be satisfied by storing the output activation for each threadblock entirely in the register file (RF). By doing so, the 2nd GEMM/Conv can compute without touching other warps for W 1. We call this RF-resident fusion which requires that the warp size has to follow Warp N = ThreadBlock N = GEMM N for each layer. In RF-resident fusion, each warp will own a chunk of the accumulator data in the RF (referred to as accumulator fragment) produced by the current layer. This will be used entirely as the input for the next layer computed by the same warp. We develop a CUT-LASS warp fragment iterator to extract the data from the accumulator fragment and feed it into warp-level MMA operations. RF-resident fusion incorporates back-to-back MMA pipelines by extending the threadblock-level GEMM design in CUTLASS. Our design has no interference across GEMM operations. The only extra operation for the 2nd GEMM is to get warp fragments from the previous accumulator and perform epilogue computation all in the RF. Shared memory-resident fusion. RF-resident GEMM fusion creates higher RF pressure especially when GEMM N is large, which will potentially harm the kernel performance and limit the applicable scenarios. To solve the problem, we propose shared memory-resident fusion to relax the warp size restriction. In this design, when the 2nd GEMM/Conv requires data sharing between warps, the data can be staged into shared memory instead of RF. Figure <ref type="figure">7</ref> shows an example where the computation for D1 has to stream W 1 fragments from multiple warp tiles in the 'N ' dimension. Thus, the accumulator data produced in GEMM0 must be transferred from RF to shared memory in order to be loaded by GEMM1. The data chunk owned by each warp will be shared in M dimension for the next layer. By doing so, the warp size restriction of Warp N in RF-resident fusion can be relaxed. To enable shared memory-resident fusion, we introduce a smem fragment iterator as the mechanism to store the accumulator tile into shared memory, and then fetch fragment from shared memory for the 2nd GEMM. In order to achieve higher performance, we carefully design the shared memory layout to avoid any shared memory bank conflict when storing the accumulators of the 1st kernel and loading it for the 2nd one.</p><p>Summary. RF-resident and shared memory-resident fusion enables deeper fusion of sequential GEMMs/Convs. Based on the back-to-back fusion, Bolt can support fusing multiple GEMMs/Conv by extending the persistent kernel templates and duplicating the GEMM pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automating templated code generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Challenges in code generation</head><p>Templated libraries pose new challenges for end-to-end tensor program optimization. Foremost, these templates usually do not provide complete functionality for end-to-end models, but only support a subset of operators. One na?ve solution is to develop a full compiler stack from scratch for each hardware, but this does not scale. Bolt addresses this challenge by employing a BYOC (Bring Your Own Codegen) <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> approach. It enables us to reuse the existing compiler stacks (e.g., TVM) as much as possible and focus only on the optimization and code generation using templated device libraries.</p><p>A na?ve application of BYOC does not solve the entire problem. First, templated device libraries by themselves are not directly runnable. They require users to instantiate the template with well-tuned parameters to achieve good performance, but BYOC does not support such performance profiling. Bolt addresses the problem by proposing a light-weight hardware-native performance profiler that can search for the best parameters for an operator with a specific workload within minutes. In addition, conventional BYOC regards device libraries as agnostic external functions and generates hardware code with hooks to invoke them at runtime. This design makes it difficult to customize the hardware library and support new optimizations, such as layout transformation and kernel padding. Bolt resolves the problem by viewing the library as a whitebox and generating code in its convention directly. In the following, we will describe our detailed design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Light-weight performance profiler</head><p>Bolt designs a light-weight performance profiler to search for the best template parameters. Traditional auto-tuners, assuming no hardware information, infer a cost model via generating sample implementations and measuring their speeds, which requires auto-tuners to explore a large search space and leads to long tuning time. Bolt greatly reduces the search time by separating the time-consuming sample program generation from performance measurement, and by effectively using the hardware details to accelerate the former. Specifically, the performance-related parameters in CUTLASS templates include threadblock, warp, and instruction shapes, swizzling functor, stages, etc. Bolt determines their possible values according to the GPU architecture as well as tuning guidelines that are specific to each hardware, thanks to the whitebox approach. For example, within the capacity of register files, Bolt prefers large warp tile sizes to achieve a higher compute-memory ratio; four or eight warps per threadblock tends to have better performance when running on modern NVIDIA GPUs; small problem sizes need small threadblock sizes to launch enough thredablocks to keep more SMs busy. For each GPU architecture, Bolt produces tens of best parameter combinations and generates the corresponding sample programs by initiating the template. Note that these sample programs are reusable across models and workloads by given different inputs. Therefore, at runtime, Bolt can profile the performance by calling the pre-generated sample programs with concrete inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Templated code generation</head><p>Traditional BYOC systems <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> cannot target code generation in templated format; they treat such libraries as external functions at runtime. In contrast, Bolt produces low-level tensor implementations in the CUTLASS convention by instantiating the templates with the best parameters identified by the profiler. Our approach has two advantages. First, the generated code delivers superior performance, e.g., can reach 300 TFLOPS throughput for FP16 GEMM on Ampere A100 which is more than 95% of the hardware theoretic limit. Second, it provides full flexibility to add novel optimizations in the generated code. In Bolt, we develop the following two optimizations.</p><p>Layout transformation. CUTLASS supports only NHWC layout for Convs because it is faster than NCHW layout (NVIDIA, c). But not all models are written in the desired layout-e.g., all Pytorch models use NCHW. To enable more optimization opportunities, Bolt provides automated layout transformation. Note that this is different from the layout transformation provided in TVM that achieves the function by modifying the relay graph. Rather, Bolt implements the transformation in the generated CUDA code of the model's first and last layer directly to save extra kernel launch overhead. The transformation requires a new tensor to hold the data with the new layout. Allocating and imitating the new tensor within the kernel will create significant memory overhead. Instead, we pre-allocate the memory by adding a new variable in the model's parameters that can be used by the kernel directly.</p><p>Kernel padding. Although CUTLASS supports alignments 8, 4, 2, 1 to cover all different workloads, the performance varies significantly across different alignments. The largest vectorized load and store supported by NVIDIA GPUs is 128 bits, so the most efficient way to use it for FP16 data type is alignment 8 (128/16). Using alignment 8 in this case can reduce the load and store instruction counts, as well as the number of predicates needed by every load and store instruction. Tensor shapes with a dimension that cannot be divided by 8 will have to use smaller alignments. For instance, the first layer of convolutional neural networks usually has three input channels, which has to use alignment 1. This will suffer from non-coalesced memory access and shared memory bank conflicts. Therefore, Bolt automatically pads unaligned tensors to use alignment 8. It allows us to not only fully utilize tensor core acceleration, but also to reduce memory loading time. Similar to layout transformation, we also pre-allocate the aligned tensor memory in models' parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Designing system-friendly models</head><p>The graph-level optimization (e.g., persistent kernel fusion) and operator-level optimization (e.g., automated padding) in Bolt also shed light on model-level optimization opportunities. Models that are designed in a way that effectively makes use of the system strengths can lead to more efficient inference. We call this system-model codesign, which can help build system-friendly models running more efficiently. Bolt identifies the following principles for this codesign.</p><p>Exploring different activation functions with epilogue fusion. The selection of activation functions has a notable influence on the accuracy of DNN models <ref type="bibr" target="#b25">(Prajit et al., 2017)</ref>. Over the years, a line of activation functions have been designed, such as ReLU <ref type="bibr" target="#b19">(Nair &amp; Hinton, 2010)</ref> and its variants, GELU <ref type="bibr" target="#b12">(Hendrycks &amp; Gimpel, 2016)</ref>, Softplus <ref type="bibr" target="#b31">(Zheng et al., 2015)</ref>, and Hardswish <ref type="bibr" target="#b13">(Howard et al., 2019)</ref>. In Bolt, the epilogue fusion will fuse activations with the leading GEMM/Conv to reduce the overhead of activations. Therefore, model designs could explore different activation functions in their models and identify the most effective one.</p><p>Deepening models with 1?1 Convs. Deepening neural networks to achieve higher accuracy is a commonly-used model design technique. For instance, ResNet <ref type="bibr" target="#b11">(He et al., 2016)</ref> has different depths from 18 layers to 151 layers with increasing accuracy. However, the inference speed will drop quickly as the depth increases. Deepening models with 1?1 Convs, on the other hand, only incurs low computation overhead in Bolt. This is because of the persistent kernel fusion optimization. Therefore, although deepening models with 1?1 Convs does not increase accuracy to the same extent as larger kernels, one can still add 1?1 Convs to improve the accuracy with reduced speed loss.</p><p>Aligning tensor shapes to use GPUs more efficiently. As we discussed in Section 3.2.3, tensor shapes have significant impacts on the efficiency of models running on GPUs. Although Bolt will automatically perform padding over unaligned tensors, the padding itself will incur extra overhead, as shown in Table <ref type="table" target="#tab_3">3</ref>. As a result, one could design models with aligned tensor shapes to achieve higher efficiency, avoiding additional padding that will be needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>Our evaluation of Bolt focuses on the following aspects. First, we perform microbenchmarks to evaluate the performance of Bolt in terms of GEMM/Conv2D computation, epilogue fusion, persistent kernel fusion, and kernel padding. Second, we evaluate the end-to-end performance of Bolt on widely-used convolutional neural networks. Finally, we apply our system-model codesign principles to the RepVGG models <ref type="bibr" target="#b10">(Ding et al., 2021)</ref> as a case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup.</head><p>Our experiments are performed on a single NVIDIA Tesla T4 GPU. We use Ansor <ref type="bibr">(Zheng et al., 2020a)</ref>, the stateof-the-art auto-tuner in TVM as our baseline. All inference computations in the evaluation use the FP16 data type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Microbenchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">GEMM/Conv2D performance</head><p>We first evaluate the performance of Bolt-generated GEMM and Conv2D implementations. For GEMMs, we evaluate a) typical GEMMs in BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> where the batch size is 32 and sequence length is 40 and b) two square GEMMs. For the Ansor baseline, we tune each workload for 2000 trials for performance optimization, following the TVM official example. We run each workload 1000 times and compute the average speed; results are shown in Figure <ref type="figure">8a</ref>. Ansor adopts a strategy that aggressively consumes all register files to achieve higher performance. However, this greedy approach is only effective for less computeintensive workloads. Therefore, Bolt is 6.1-9.5x faster than Ansor on compute-intensive workloads and achieves 1.9x speedup on the one that is less compute-intensive. Similarly, we measure the speed of Conv2Ds in Figure <ref type="figure">8b</ref>. The workloads are extracted from ResNet-50 using 32 as the batch size. All Conv2Ds in the table are using (3, 3) kernels and (1, 1) zero padding. For all cases, Bolt is 2.7-3.5x faster than Ansor. Overall, Bolt achieves significantly higher performance as the tuning strategy based on hardware-native templates extracts native performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Epilogue fusion performance</head><p>We then evaluate the effectiveness of epilogue fusion on element-wise operators.</p><p>We choose one workload for GEMM and Conv2D from Figure <ref type="figure">8</ref> respectively and measure the performance of the pattern GEMM/Conv2D+BiasAdd+Activation. We experiment on four different activation functions: ReLU, GELU, Hardswish, and Softplus, and the results are shown in Figure 9. Our baseline here is Bolt without epilogue fusion, in which Bolt only computes the GEMM/Conv2D and TVM will fuse the BiasAdd and activation and compute them as one operator. As we can see, epilogue fusion improves the computation speeds for both GEMMs and Conv2Ds. The average speedup for GEMMs and Conv2D is 1.45x and 1.38x respectively. We have observed similar performance gains on other workloads (not shown).  <ref type="table" target="#tab_5">562 ,</ref><ref type="table" target="#tab_5">6 4 2 ,</ref><ref type="table" target="#tab_2">(1 ,</ref><ref type="table" target="#tab_5">1 )  562 ,</ref><ref type="table" target="#tab_2">1 282 ,</ref><ref type="table">(2 ,</ref><ref type="table">2 )  282 ,</ref><ref type="table" target="#tab_2">1 282 ,</ref><ref type="table" target="#tab_2">(1 ,</ref><ref type="table" target="#tab_2">1 )  282 ,</ref><ref type="table" target="#tab_5">2 562 ,</ref><ref type="table">(2 ,</ref><ref type="table" target="#tab_4">2 )  142 ,</ref><ref type="table" target="#tab_5">2 562 ,</ref><ref type="table" target="#tab_2">(1 ,</ref><ref type="table" target="#tab_4">1 )  142 ,</ref><ref type="table" target="#tab_2">5 122 ,</ref><ref type="table">(2 ,</ref><ref type="table">2 )  7 2 ,</ref><ref type="table" target="#tab_2">5 122 ,</ref><ref type="table" target="#tab_2">(1 ,</ref><ref type="table" target="#tab_2">1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Persistent kernel fusion performance</head><p>We next evaluate the performance of persistent kernel fusion. First, we use Bolt to fuse two GEMMs with the pattern GEMM1+ReLU+GEMM2+ReLU into one GEMM operator using RF-based or shared-memory based persistent kernels, depending on their performance. The baseline is Bolt with only epilogue fusion that computes the two GEMMs sequentially. Results are presented in Table <ref type="table" target="#tab_2">1</ref>. Workloads are extracted from real recommendation models, e.g., DCNv2 <ref type="bibr" target="#b28">(Wang et al., 2021)</ref>, DLRM <ref type="bibr" target="#b20">(Naumov et al., 2019)</ref>. As we can see, the persistent kernel fusion accelerates the computation by 1.2x-1.5x. For Convs, we extract the 3?3 Conv2Ds from the first a few layers in the RepVGG models and create a 1 ? 1 Conv2D for each of them. As shown in Table <ref type="table">2</ref>, our persistent kernel fusion can improve the computation speed by 1.1x-2.0x. Note that our persistent kernels can fuse more than two GEMMs/Convs, which can further improve the performance by saving intermediate memory access and kernel launch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Padding performance and overhead</head><p>We now evaluate the performance benefits of automated padding and its extra overhead in Bolt. In Table <ref type="table" target="#tab_3">3</ref>, we choose a few Conv2D workloads in our production of which the input channels are not divisible by eight. Without padding,   <ref type="bibr">32 20, 26 174, 64 (3, 3)</ref> (1, 1) 1.00 1.60 24% 32 <ref type="bibr">20, 26 174, 64 (5, 5)</ref> (2, 2) 1.00 1.99 12% overhead: in this benchmark, the average time spent on padding over the total computation time (padding+Conv2D computation) is 16%. This is further evidence of our 3rd system-friendly model design principle-models should be designed with aligned tensor shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">End-to-end optimization</head><p>We evaluate the performance of Bolt on end-to-end model optimization by experimenting on six widely-used convolutional neural networks. Our baseline is Ansor which performs auto-tuning to optimize performance. We configure Ansor following the official example and set the tuning trials to the recommended 900 ? the number of tasks. We use batch size=32 and FP16 data type for all models. The inference speed and tuning time are shown in Figure <ref type="figure" target="#fig_9">10</ref>. As we can see, Bolt has significant better inference performance compared to Ansor. In particular, Bolt is 4.2x faster on VGG models, 1.5x faster on ResNet models, and 2.6x faster on RepVGG models. On average, Bolt improves the inference speed by 2.8x compared to Ansor. In terms of tuning time, as shown in Figure <ref type="figure" target="#fig_9">10b</ref>, Bolt can complete the tuning much faster than Ansor because Bolt uses hardware-native templated search which greatly reduces the searching space. Concretely, Bolt can finish the tuning within 20 minutes for all models while Ansor takes 12 hours on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">System-friendly models: RepVGG case study</head><p>Finally, we perform a case study on RepVGG <ref type="bibr" target="#b10">(Ding et al., 2021)</ref> to show the effectiveness of our system-model codesign principles. RepVGG is a VGG-like convolution neural network which only uses 3?3 Conv2Ds to achieve higher accuracy and faster inference speed. The key idea of RepVGG is to train high-accuracy models with branches and remove those branches by re-parameterization to accelerate the inference. We apply our system-friendly model principles to augment RepVGG. In our experiments, models are trained on ImageNet with FP32 using the Swin Transformer training codebase <ref type="bibr">(Liu et al.)</ref>, but they are quantized to FP16 for inference without accuracy loss. The inference runs on an NVIDIA Tesla T4 GPU using 32 as the batch size. Changing activation functions.</p><p>We first augment RepVGG by trying different activation functions. The original RepVGG model selects ReLU as its activation function, but we also experiment with GELU <ref type="bibr" target="#b12">(Hendrycks &amp; Gimpel, 2016)</ref>, Hardswish <ref type="bibr" target="#b13">(Howard et al., 2019)</ref>, and Softplus <ref type="bibr" target="#b31">(Zheng et al., 2015)</ref>. The top-1 accuracy and inference speed of RepVGG-A0 with different activation functions is shown in Table <ref type="table" target="#tab_4">4</ref>. We have found that activation functions do affect the accuracy-RepVGG-A0 with Hardswish achieves 0.67% higher accuracy. Meanwhile, the inference speed does not show too much difference. Even with Softplus that has complex computation, the speed only drops by 7.7%.</p><p>Deepening the model with 1?1 Conv2Ds. We apply our 2nd codesign principle by adding 1 ? 1 Conv2Ds after each 3 ? 3 Conv2D (except for the last one which has too many output channels). The 1 ? 1 Conv2Ds have the same input and output channels, with strides of (1, 1) and no padding. Bolt will fuse adjacent 3 ? 3 and 1 ? 1 Conv2Ds using persistent kernels if the fusion is beneficial. To verify the effectiveness of each individual principle, we do not change the activation function in this experiment. As shown in Table <ref type="table">5</ref>, adding 1 ? 1 Conv2Ds can improve the accuracy with minimal speed loss. The accuracy is increased by 0.82%, 0.77%, and 0.74% for RepVGGAug-A0, A1, and B0 respectively. Their speed drops by 15.3% on average. Combined effect. Finally, we combine the above two techniques and train the model with advanced augmentation, label smoothing, and mixup in 300 epochs. For RepVGG-A0, we train it for 300 epochs with only simple augmentation, which has better accuracy. As shown in Table <ref type="table" target="#tab_5">6</ref>, designing models in a system-friendly manner can improve accuracy more efficiently. For example, in original RepVGG models, B0 is built by augmenting A1 with more 3 ? 3 Conv2Ds,   which has 1% higher accuracy and 21.8% lower speed compared to A1. In our augmentation, however, RepVGGAug-A1 is augmented by adding 1 ? 1 Conv2Ds which has similar speed overhead, but the accuracy is improved by 1.83% than RepVGG-A1. Note that designers have the flexibility to make a trade off between accuracy and speed. For instance, by adding only 1 ? 1 Conv2Ds to the first three layers of RepVGG-A0 and using Hardswish, we can get a RepVGAug-A0 model with 74.02% Top-1 accuracy and 7288 images/sec speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Other platforms. Although we use NVIDIA CUTLASS and GPUs to demonstrate the design of Bolt, our approach is not bound to any specific devices or libraries. Templated libraries are a design trend for other devices as well. We expect that for other libraries, a Bolt-like approach would also result in new end-to-end optimizations.</p><p>Persistent kernel fusion limitations. Although persistent kernels can fuse any sequential GEMMs/Conv2Ds following the threadblock residence, we design it specifically for memory-bound operators, which is consistent with the motivation of general operator fusion. That is, Bolt can improve the performance for sequential GEMMs with small N and K dimensions but large M dimensions and Conv2Ds with small channels. Fusing compute-bound operators could lead to performance drops because of the threadblock residence requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Auto-tuners. Many DNN compilers and frameworks employ auto-tuning strategies to search for the tensor implementation with optimal performance <ref type="bibr" target="#b5">(Chen et al., 2018b;</ref><ref type="bibr">Zheng et al., 2020a;</ref><ref type="bibr" target="#b2">Adams et al., 2019;</ref><ref type="bibr">Zheng et al., 2020b)</ref>.</p><p>As they infer the hardware cost model by trying different tensor implementations and measuring their performance, this takes hours to days. Moreover, the generated tensor programs cannot achieve hardware-native performance. Bolt bridges the gap between auto-tuners and hardware-native performance.</p><p>Operator fusion. Operator fusion is an important graphlevel optimization <ref type="bibr">(Chen et al., 2018a;</ref><ref type="bibr" target="#b0">Abadi et al., 2016;</ref><ref type="bibr" target="#b16">Leary &amp; Wang, 2018;</ref><ref type="bibr" target="#b24">Paszke et al., 2019;</ref><ref type="bibr" target="#b1">Abdolrashidi et al., 2019;</ref><ref type="bibr" target="#b15">Jia et al., 2019)</ref>. However, existing operator fusion only considers one GEMM/Conv and its adjacent operators, e.g., BiasAdd, ReLU, and the fusion is not well supported by vendor libraries. Bolt enables new operator fusion with high performance. For instance, the proposed persistent kernel fusion can fuse a sequence of GEMMs and Convs, further improving the performance. Our persistent kernel is different from Persistent RNNs <ref type="bibr" target="#b9">(Diamos et al., 2016)</ref> which is manually designed specifically for RNNs without using tensor cores.</p><p>System-friendly model design. RepVGG <ref type="bibr" target="#b10">(Ding et al., 2021)</ref> designs system-friendly models by employing multibranch architectures in training models to pursue high accuracy and by removing the branches via re-parameterization for the inference. Also, RepVGG uses only 3 ? 3 Conv2Ds which are well-supported by the hardware. Bolt further extends the idea by proposing system-model codesign principles, and uses RepVGG as a concrete case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper presents Bolt, which bridges the gap between auto-tuners and device library performance. Bolt utilizes the emerging trend that vendor libraries are becoming modularized and composable. It combines the flexibility of auto-tuners and the hardware-native performance of templated device libraries to achieve the best of both worlds. Our design enables new tensor-level and graph-level optimizations, and inspires system-friendly model design insights.</p><p>Our experiments show that Bolt can achieve 2.5? speedup on widely-used convolutional neural networks compared against the state of the art. Moreover, it finishes its autotuning within 20 minutes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The GEMM hierarchy in CUTLASS and the data movement in threadblock and warp tiles.</figDesc><graphic url="image-1.png" coords="3,60.64,67.06,221.10,101.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 Figure 3 .</head><label>33</label><figDesc>Figure 3 illustrates the workflow of Bolt. It follows a BYOC (Bring Your Own Codegen) (Chen et al., 2021) approach, carving out a suitable subgraph of the tensor program and offloading it to Bolt for optimization. Starting from DNN models written in popular frameworks (e.g., TensorFlow, PyTorch, MXNet), Bolt reuses the TVM frontend to parse</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4. The graph view and kernel view of persistent kernel fusion for back-to-back GEMMs/Convs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustration of threadblock-residence of GEMM fusion. Colored boxes represent one single threadblock. This requires ThreadBlock0 N = N0, ThreadBlock1 N = N1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. RF-resident fusion in a threadblock of back-to-back GEMMs. The threadblock and warp size requirements are: Warp0 N=ThreadBlock0 N=N0, Warp1 N=ThreadBlock1 N=N1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure8. The performance of Bolt on GEMMs and Conv2Ds. Figure8ashows the speed of GEMMs in BERT with batch size=32 and sequence length=40 and two square GEMMs. Figure8bshows the speed of 3 ? 3 Conv2Ds in ResNet-50. The batch size=32 and all Conv2Ds use (1, 1) zero padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. The normalized inference speed and tuning time for widely used convolutional neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The performance of fusing two back-to-back GEMMs using persistent kernels. Each GEMMs is followed by a ReLU epilogue and all of them will be fused into one kernel.</figDesc><table><row><cell cols="2">1st GEMM</cell><cell></cell><cell cols="4">2nd GEMM</cell><cell cols="2">Normalized speed</cell></row><row><cell>M</cell><cell cols="2">N K</cell><cell>M</cell><cell></cell><cell cols="2">N K</cell><cell cols="2">w/o fuse. w/ fuse.</cell></row><row><cell>2464</cell><cell>1</cell><cell>4</cell><cell>2464</cell><cell></cell><cell>4</cell><cell>1</cell><cell>1.00</cell><cell>1.24</cell></row><row><cell>16384</cell><cell cols="4">64 256 16384</cell><cell cols="2">16 64</cell><cell>1.00</cell><cell>1.34</cell></row><row><cell>32768</cell><cell cols="4">128 576 32768</cell><cell cols="2">64 128</cell><cell>1.00</cell><cell>1.28</cell></row><row><cell cols="3">128320 32 96</cell><cell cols="4">128320 96 32</cell><cell>1.00</cell><cell>1.46</cell></row><row><cell cols="9">Table 2. The performance of fusing two back-to-back Conv2Ds</cell></row><row><cell cols="9">using persistent kernels. Each Conv2D is followed by a BiasAdd</cell></row><row><cell cols="9">and a ReLU epilogue. The 3 ? 3 Conv2D uses (1, 1) padding and</cell></row><row><cell cols="9">the 1 ? 1 Conv2D uses (1, 1) strides and does not have padding.</cell></row><row><cell cols="3">3?3 Conv2D</cell><cell></cell><cell cols="3">1?1 Conv2D</cell><cell cols="2">Normalized speed</cell></row><row><cell cols="9">H, W IC, OC strides H, W IC, OC w/o fuse. w/ fuse.</cell></row><row><cell>224 2</cell><cell>3, 48</cell><cell cols="2">(2, 2)</cell><cell>112 2</cell><cell></cell><cell>48, 48</cell><cell>1.00</cell><cell>1.10</cell></row><row><cell>112 2</cell><cell>48, 48</cell><cell cols="2">(2, 2)</cell><cell>56 2</cell><cell></cell><cell>48, 48</cell><cell>1.00</cell><cell>1.41</cell></row><row><cell>56 2</cell><cell>48, 48</cell><cell cols="2">(1, 1)</cell><cell>56 2</cell><cell></cell><cell>48, 48</cell><cell>1.00</cell><cell>1.87</cell></row><row><cell>224 2</cell><cell>3, 64</cell><cell cols="2">(2, 2)</cell><cell>112 2</cell><cell></cell><cell>64, 64</cell><cell>1.00</cell><cell>1.24</cell></row><row><cell>112 2</cell><cell>64, 64</cell><cell cols="2">(2, 2)</cell><cell>56 2</cell><cell></cell><cell>64, 64</cell><cell>1.00</cell><cell>1.12</cell></row><row><cell>56 2</cell><cell>64, 64</cell><cell cols="2">(1, 1)</cell><cell>56 2</cell><cell></cell><cell>64, 64</cell><cell>1.00</cell><cell>2.02</cell></row><row><cell cols="9">these workloads can only compute with alignment 2. Bolt</cell></row><row><cell cols="9">will automatically pad them to the closest 8-divisible sizes,</cell></row><row><cell cols="9">thus leveraging alignment 8. We measure the normalized</cell></row><row><cell cols="9">computation speed of Bolt with and without padding. As</cell></row><row><cell cols="9">we can see, after padding, the speed can be improved by</cell></row><row><cell cols="9">1.8x on average. However, the padding itself will incur extra</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The performance and overhead of Bolt's automated padding. Unpadded Conv2Ds are computed with alignment=2; after being padded, alignement=8 can be used. The cost of padding is the time spent on the padding over the total computation time (padding+Conv2D).</figDesc><table><row><cell cols="3">N H, W IC, OC kernel padding</cell><cell cols="2">Norm. speed Cost unpad pad</cell></row><row><cell>32 20, 26 46, 32</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>1.00 1.62</cell><cell>18%</cell></row><row><cell>32 20, 26 46, 32</cell><cell>(5, 5)</cell><cell>(2, 2)</cell><cell>1.00 1.95</cell><cell>9%</cell></row><row><cell>128 14, 19 46, 32</cell><cell>(5, 7)</cell><cell>(0, 0)</cell><cell>1.00 1.77</cell><cell>15%</cell></row><row><cell>288 11, 15 46, 32</cell><cell>(5, 7)</cell><cell>(0, 0)</cell><cell>1.00 1.71</cell><cell>18%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The top-1 accuracy and speed of RepVGG-A0 using different activation functions (120 epochs + simple data augmentation).</figDesc><table><row><cell>Activation</cell><cell>Top-1 accuracy</cell><cell cols="2">Speed (images/sec)</cell></row><row><cell>ReLU</cell><cell>72.31</cell><cell>5909</cell><cell></cell></row><row><cell>GELU</cell><cell>72.38</cell><cell>5645</cell><cell></cell></row><row><cell>Hardswish</cell><cell>72.98</cell><cell>5713</cell><cell></cell></row><row><cell>Softplus</cell><cell>72.57</cell><cell>5453</cell><cell></cell></row><row><cell cols="4">Table 5. The top-1 accuracy and speed of original RepVGG models</cell></row><row><cell cols="4">and their augmentation with 1 ? 1 Conv2Ds (200 epochs + simple</cell></row><row><cell>data augmentation).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Top-1 accuracy Speed Params</cell></row><row><cell>RepVGG-A0</cell><cell>73.05</cell><cell>7861</cell><cell>8.31</cell></row><row><cell>RepVGG-A1</cell><cell>74.75</cell><cell>6253</cell><cell>12.79</cell></row><row><cell>RepVGG-B0</cell><cell>75.28</cell><cell>4888</cell><cell>14.34</cell></row><row><cell>RepVGGAug-A0</cell><cell>73.87</cell><cell>6716</cell><cell>13.35</cell></row><row><cell>RepVGGAug-A1</cell><cell>75.52</cell><cell>5241</cell><cell>21.7</cell></row><row><cell>RepVGGAug-B0</cell><cell>76.02</cell><cell>4145</cell><cell>24.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The top-1 accuracy and speed of original RepVGG models and their augmentation with 1 ? 1 Conv2Ds+Hardswish (300 epochs + advanced augmentation, label smoothing, and mixup).</figDesc><table><row><cell>Model</cell><cell cols="2">Top-1 accuracy Speed (images/sec)</cell></row><row><cell>RepVGG-A0</cell><cell>73.41</cell><cell>7861</cell></row><row><cell>RepVGG-A1</cell><cell>74.89</cell><cell>6253</cell></row><row><cell>RepVGG-B0</cell><cell>75.89</cell><cell>4888</cell></row><row><cell>RepVGGAug-A0</cell><cell>74.54</cell><cell>6338</cell></row><row><cell>RepVGGAug-A1</cell><cell>76.72</cell><cell>4868</cell></row><row><cell>RepVGGAug-B0</cell><cell>77.22</cell><cell>3842</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>ByteDance</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Rice Univer-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>sity 3 NVIDIA. Correspondence to: Leyuan Wang &lt;leyuan.wang@bytedance.com&gt;.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to fuse</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdolrashidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS ML for Systems Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to optimize halide with tree search and random programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Amd</surname></persName>
		</author>
		<author>
			<persName><surname>Rocm</surname></persName>
		</author>
		<ptr target="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08166</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Delaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03215</idno>
		<title level="m">Bring your own codegen to deep learning compiler</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><surname>Cudnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Persistent rnns: Stashing recurrent weights on-chip</title>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making vgg-style convnets great again</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Repvgg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13733" to="13742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="https://github.com/oneapi-src/oneDNN" />
		<title level="m">Intel. oneAPI deep neural network library (oneDNN)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Taso: optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">XLA -tensorflow, compiled</title>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://github.com/microsoft/Swin-Transformer" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<idno>CoRR, abs/1906.00091</idno>
		<ptr target="https://arxiv.org/abs/1906.00091" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Cublas</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/cublas" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">CUTLASS: CUDA templates for linear algebra subroutines</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/cutlass" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Tensor layouts in memory: NCHW vs NHWC</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tensor-layout" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Swish: a self-gated activation function</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prajit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Quoc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.059417</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirisame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tatlock</surname></persName>
		</author>
		<author>
			<persName><surname>Relay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08368</idno>
		<title level="m">A high-level compiler for deep learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tophub autotvm log collections</title>
		<author>
			<persName><surname>Sampl</surname></persName>
		</author>
		<ptr target="https://github.com/tlc-pack/tophub" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dcn v2: Improved deep &amp; cross network and practical lessons for web-scale learning to rank systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shivanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1785" to="1797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kernel weaver: Automatically fusing database primitives for efficient gpu computation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yalamanchili</surname></persName>
		</author>
		<idno>MICRO-45 &apos;12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 45th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lorien: Efficient deep learning workloads delivery</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh ACM Symposium on Cloud Computing</title>
		<meeting>the Seventh ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving deep neural networks using softplus units</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ansor: Generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="859" to="873" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
