<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LinkBERT: Pretraining Language Models with Document Links</title>
				<funder ref="#_j2VGtVD #_PZbPkRs">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">UnitedHealth Group</orgName>
				</funder>
				<funder ref="#_E2FspZ8">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">NEC</orgName>
				</funder>
				<funder ref="#_V3qQwXM #_qxRUVgV">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-29">29 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LinkBERT: Pretraining Language Models with Document Links</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-29">29 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.15827v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language model (LM) pretraining can learn various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data. 1 * Equal senior authorship.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models (LMs), like BERT and GPTs <ref type="bibr" target="#b18">(Devlin et al., 2019;</ref><ref type="bibr" target="#b10">Brown et al., 2020)</ref>, have shown remarkable performance on many natural language processing (NLP) tasks, such as text classification and question answering, becoming the foundation of modern NLP systems <ref type="bibr">(Bommasani et al., 2021)</ref>. By performing self-supervised learning, such as masked language modeling <ref type="bibr" target="#b18">(Devlin et al., 2019)</ref>, LMs learn to encode various knowledge from text corpora and produce informative representations for downstream tasks <ref type="bibr" target="#b55">(Petroni et al., 2019;</ref><ref type="bibr" target="#b8">Bosselut et al., 2019;</ref><ref type="bibr" target="#b58">Raffel et al., 2020)</ref>.</p><p>Figure <ref type="figure">1</ref>: Document links (e.g. hyperlinks) can provide salient multi-hop knowledge. For instance, the Wikipedia article "Tidal Basin" (left) describes that the basin hosts "National Cherry Blossom Festival". The hyperlinked article (right) reveals that the festival celebrates "Japanese cherry trees". Taken together, the link suggests new knowledge not available in a single document (e.g. "Tidal Basin has Japanese cherry trees"), which can be useful for various applications, including answering a question "What trees can you see at Tidal Basin?". We aim to leverage document links to incorporate more knowledge into language model pretraining.</p><p>However, existing LM pretraining methods typically consider text from a single document in each input context <ref type="bibr" target="#b48">(Liu et al., 2019;</ref><ref type="bibr" target="#b35">Joshi et al., 2020)</ref> and do not model links between documents. This can pose limitations because documents often have rich dependencies (e.g. hyperlinks, references), and knowledge can span across documents. As an example, in Figure <ref type="figure">1</ref>, the Wikipedia article "Tidal Basin, Washington D.C." (left) describes that the basin hosts "National Cherry Blossom Festival", and the hyperlinked article (right) reveals the background that the festival celebrates "Japanese cherry trees". Taken together, the hyperlink offers new, multi-hop knowledge "Tidal Basin has Japanese cherry trees", which is not available in the single article "Tidal Basin" alone. Acquiring such multi-hop knowledge in pretraining could be useful for various applications including question answering. In fact, document links like hyperlinks and references are ubiquitous (e.g. web, books, scientific literature), and guide how we humans acquire knowledge and Figure <ref type="figure">2</ref>: Overview of our approach, LinkBERT. Given a pretraining corpus, we view it as a graph of documents, with links such as hyperlinks ( ?4.1). To incorporate the document link knowledge into LM pretraining, we create LM inputs by placing a pair of linked documents in the same context (linked), besides the existing options of placing a single document (contiguous) or a pair of random documents (random) as in BERT. We then train the LM with two self-supervised objectives: masked language modeling (MLM), which predicts masked tokens in the input, and document relation prediction (DRP), which classifies the relation of the two text segments in the input (contiguous, random, or linked) ( ?4.2). even make discoveries <ref type="bibr" target="#b51">(Margolis et al., 1999)</ref>.</p><p>In this work, we propose LinkBERT, an effective language model pretraining method that incorporates document link knowledge. Given a text corpus, we obtain links between documents such as hyperlinks, and create LM inputs by placing linked documents in the same context, besides the existing option of placing a single document or random documents as in BERT. Specifically, as in Figure <ref type="figure">2</ref>, after sampling an anchor text segment, we place either (1) the contiguous segment from the same document, (2) a random document, or (3) a document linked from anchor segment, as the next segment in the input. We then train the LM with two joint objectives: We use masked language modeling (MLM) to encourage learning multi-hop knowledge of concepts brought into the same context by document links (e.g. "Tidal Basin" and "Japanese cherry" in Figure <ref type="figure">1</ref>). Simultaneously, we propose a Document Relation Prediction (DRP) objective, which classifies the relation of the second segment to the first segment (contiguous, random, or linked). DRP encourages learning the relevance and bridging concepts (e.g. "National Cherry Blossom Festival") between documents, beyond the ability learned in the vanilla next sentence prediction objective in BERT.</p><p>Viewing the pretraining corpus as a graph of documents, LinkBERT is also motivated as self-supervised learning on the graph, where DRP and MLM correspond to link prediction and node feature prediction in graph machine learning <ref type="bibr" target="#b74">(Yang et al., 2015;</ref><ref type="bibr" target="#b32">Hu et al., 2020)</ref>. Our modeling approach thus provides a natural fusion of language-based and graph-based self-supervised learning.</p><p>We train LinkBERT in two domains: the general domain, using Wikipedia articles with hyperlinks ( ?4), and the biomedical domain, using PubMed articles with citation links ( ?6). We then evaluate the pretrained models on a wide range of downstream tasks such as question answering, in both domains.</p><p>LinkBERT consistently improves on baseline LMs across domains and tasks. For the general domain, LinkBERT outperforms BERT on MRQA benchmark (+4% absolute in F1-score) as well as GLUE benchmark. For the biomedical domain, LinkBERT exceeds PubmedBERT <ref type="bibr" target="#b25">(Gu et al., 2020)</ref> and sets new states of the art on BLURB biomedical NLP benchmark (+3% absolute in BLURB score) and MedQA-USMLE reasoning task (+7% absolute in accuracy). Overall, LinkBERT attains notably large gains for multi-hop reasoning, multi-document understanding, and few-shot question answering, suggesting that LinkBERT internalizes significantly more knowledge than existing LMs by pretraining with document link information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Retrieval-augmented LMs. Several works <ref type="bibr">(Lewis et al., 2020b;</ref><ref type="bibr" target="#b37">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b54">Oguz et al., 2020;</ref><ref type="bibr" target="#b73">Xie et al., 2022)</ref> introduce a retrieval module for LMs, where given an anchor text (e.g. question), retrieved text is added to the same LM context to improve model inference (e.g. answer prediction). These works show the promise of placing related documents in the same LM context at inference time, but they do not study the effect of doing so in pretraining. <ref type="bibr" target="#b26">Guu et al. (2020)</ref> pretrain an LM with a retriever that learns to retrieve text for answering masked tokens in the anchor text. In contrast, our focus is not on retrieval, but on pretraining a general-purpose LM that internalizes knowledge that spans across documents, which is orthogonal to the above works (e.g., our pretrained LM could be used to initialize the LM component of these works). Additionally, we focus on incorporating document links such as hyperlinks, which can offer salient knowledge that common lexical retrieval methods may not provide <ref type="bibr" target="#b2">(Asai et al., 2020)</ref>.</p><p>Pretrain LMs with related documents. Several concurrent works use multiple related documents to pretrain LMs. <ref type="bibr" target="#b11">Caciularu et al. (2021)</ref> place documents (news articles) about the same topic into the same LM context, and <ref type="bibr" target="#b44">Levine et al. (2021)</ref> place sentences of high lexical similarity into the same context. Our work provides a general method to incorporate document links into LM pretraining, where lexical or topical similarity can be one instance of document links, besides hyperlinks. We focus on hyperlinks in this work, because we find they can bring in salient knowledge that may not be obvious via lexical similarity, and yield a more performant LM ( ?5.5). Additionally, we propose the DRP objective, which improves modeling multiple documents and relations between them in LMs ( ?5.5).</p><p>Hyperlinks and citation links for NLP. Hyperlinks are often used to learn better retrieval models. <ref type="bibr" target="#b14">Chang et al. (2020)</ref>; <ref type="bibr" target="#b2">Asai et al. (2020)</ref>; <ref type="bibr" target="#b60">Seonwoo et al. (2021)</ref> use Wikipedia hyperlinks to train retrievers for open-domain question answering. <ref type="bibr" target="#b50">Ma et al. (2021)</ref> study various hyperlink-aware pretraining tasks for retrieval. While these works use hyperlinks to learn retrievers, we focus on using hyperlinks to create better context for learning general-purpose LMs. Separately, <ref type="bibr" target="#b12">Calixto et al. (2021)</ref> use Wikipedia hyperlinks to learn multilingual LMs. Citation links are often used to improve summarization and recommendation of academic papers <ref type="bibr" target="#b57">(Qazvinian and Radev, 2008;</ref><ref type="bibr" target="#b76">Yasunaga et al., 2019;</ref><ref type="bibr" target="#b5">Bhagavatula et al., 2018;</ref><ref type="bibr" target="#b38">Khadka et al., 2020;</ref><ref type="bibr" target="#b16">Cohan et al., 2020)</ref>. Here we leverage citation networks to improve pretraining general-purpose LMs.</p><p>Graph-augmented LMs. Several works augment LMs with graphs, typically, knowledge graphs (KGs) where the nodes capture entities and edges their relations. Zhang et al. ( <ref type="formula">2019</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>A language model (LM) can be pretrained from a corpus of documents, X = {X (i) }. An LM is a composition of two functions, f head (f enc (X)), where the encoder f enc takes in a sequence of tokens X = (x 1 ,x 2 ,...,x n ) and produces a contextualized vector representation for each token, (h 1 ,h 2 ,...,h n ). The head f head uses these representations to perform selfsupervised tasks in the pretraining step and to perform downstream tasks in the fine-tuning step. We build on BERT <ref type="bibr" target="#b18">(Devlin et al., 2019)</ref>, which pretrains an LM with the following two self-supervised tasks. Masked language modeling (MLM). Given a sequence of tokens X, a subset of tokens Y ? X is masked, and the task is to predict the original tokens from the modified input. Y accounts for 15% of the tokens in X; of those, 80% are replaced with [MASK], 10% with a random token, and 10% are kept unchanged. Next sentence prediction (NSP). The NSP task takes two text segments<ref type="foot" target="#foot_0">2</ref> (X A ,X B ) as input, and predicts whether X B is the direct continuation of X A . Specifically, BERT first samples X A from the corpus, and then either (1) takes the next segment X B from the same document, or (2) samples X B from a random document in the corpus. The two segments are joined via special tokens to form an input instance,</p><formula xml:id="formula_0">[CLS] X A [SEP] X B [SEP],</formula><p>where the prediction target of [CLS] is whether X B indeed follows X A (contiguous or random).</p><p>In this work, we will further incorporate document link information into LM pretraining. Our approach ( ?4) will build on MLM and NSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LinkBERT</head><p>We present LinkBERT, a self-supervised pretraining approach that aims to internalize more knowledge into LMs using document link information. Specifically, as shown in Figure <ref type="figure">2</ref>, instead of viewing the pretraining corpus as a set of documents X = {X (i) }, we view it as a graph of documents, G = (X , E), where E = {(X (i) , X (j) )} denotes links between documents ( ?4.1). The links can be existing hyperlinks, or could be built by other methods that capture document relevance. We then consider pretraining tasks for learning from document links ( ?4.2): We create LM inputs by placing linked documents in the same context window, besides the existing options of a single document or random documents. We use the MLM task to learn concepts brought together in the context by document links, and we also introduce the Document Relation Prediction (DRP) task to learn relations between documents. Finally, we discuss strategies for obtaining informative pairs of linked documents to feed into LM pretraining ( ?4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document graph</head><p>Given a pretraining corpus, we link related documents so that the links can bring together knowledge that is not available in single documents. We focus on hyperlinks, e.g., hyperlinks of Wikipedia articles ( ?5) and citation links of academic articles ( ?6). Hyperlinks have a number of advantages. They provide background knowledge about concepts that the document writers deemed useful-the links are likely to have high precision of relevance, and can also bring in relevant documents that may not be obvious via lexical similarity alone (e.g., in Figure <ref type="figure">1</ref>, while the hyperlinked article mentions "Japanese" and "Yoshino" cherry trees, these words do not appear in the anchor article). Hyperlinks are also ubiquitous on the web and easily gathered at scale <ref type="bibr" target="#b0">(Aghajanyan et al., 2021)</ref>. To construct the document graph, we simply make a directed edge (X (i) ,X (j) ) if there is a hyperlink from document X (i) to document X (j) .</p><p>For comparison, we also experiment with a document graph built by lexical similarity between documents. For each document X (i) , we use the common TF-IDF cosine similarity metric <ref type="bibr" target="#b15">(Chen et al., 2017;</ref><ref type="bibr">Yasunaga et al., 2017)</ref> to obtain top-k documents X (j) 's and make edges (X (i) ,X (j) ). We use k = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pretraining tasks</head><p>Creating input instances. Several works <ref type="bibr" target="#b23">(Gao et al., 2021;</ref><ref type="bibr" target="#b44">Levine et al., 2021)</ref> find that LMs can learn stronger dependencies between words that were shown together in the same context during training, than words that were not. To effectively learn knowledge that spans across documents, we create LM inputs by placing linked documents in the same context window, besides the existing option of a single document or random documents. Specifically, we first sample an anchor text segment from the corpus (Segment A; X A ? X (i) ). For the next segment (Segment B; X B ), we either (1) use the contiguous segment from the same document (X B ? X (i) ), (2) sample a segment from a random document (X B ? X (j) where j = i), or (3) sample a segment from one of the documents linked from Segment A (X B ? X (j) where (X (i) ,X (j) ) ? E). We then join the two segments via special tokens to form an input instance:</p><formula xml:id="formula_1">[CLS] X A [SEP] X B [SEP].</formula><p>Training objectives. To train the LM, we use two objectives. The first is the MLM objective to encourage the LM to learn multi-hop knowledge of concepts brought into the same context by document links. The second objective, which we propose, is Document Relation Prediction (DPR), which classifies the relation r of segment X B to segment X A (r ? {contiguous,random,linked}). By distinguishing linked from contiguous and random, DRP encourages the LM to learn the relevance and existence of bridging concepts between documents, besides the capability learned in the vanilla NSP objective.</p><p>To predict r, we use the representation of [CLS] token, as in NSP. Taken together, we optimize:</p><formula xml:id="formula_2">L = L MLM +L DRP (1) = - i log p(x i | h i )-log p(r | h [CLS] )<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">x i is each token of the input instance, [CLS] X A [SEP] X B [SEP]</formula><p>, and h i is its representation.</p><p>Graph machine learning perspective. Our two pretraining tasks, MLM and DRP, are also motivated as graph self-supervised learning on the document graph. In graph self-supervised learning, two types of tasks, node feature prediction and link prediction, are commonly used to learn the content and structure of a graph. In node feature prediction <ref type="bibr" target="#b32">(Hu et al., 2020)</ref>, some features of a node are masked, and the task is to predict them using neighbor nodes. This corresponds to our MLM task, where masked tokens in Segment A can be predicted using Segment B (a linked document on the graph), and vice versa. In link prediction <ref type="bibr" target="#b7">(Bordes et al., 2013;</ref><ref type="bibr">Wang et al., 2021a)</ref>, the task is to predict the existence or type of an edge between two nodes. This corresponds to our DRP task, where we predict if the given pair of text segments are linked (edge), contiguous (self-loop edge), or random (no edge). Our approach can be viewed as a natural fusion of language-based (e.g. BERT) and graph-based self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Strategy to obtain linked documents</head><p>As described in ?4.1, ?4.2, our method builds links between documents, and for each anchor segment, samples a linked document to put together in the LM input. Here we discuss three key axes to consider to obtain useful linked documents in this process.</p><p>Relevance. Semantic relevance is a requisite when building links between documents. If links were randomly built without relevance, LinkBERT would be same as BERT, with simply two options of LM inputs (contiguous or random). Relevance can be achieved by using hyperlinks or lexical similarity metrics, and both methods yield substantially better performance than using random links ( ?5.5).</p><p>Salience. Besides relevance, another factor to consider (salience) is whether the linked document can offer new, useful knowledge that may not be obvious to the current LM. Hyperlinks are potentially more advantageous than lexical similarity links in this regard: LMs are shown to be good at recognizing lexical similarity <ref type="bibr" target="#b65">(Zhang et al., 2020)</ref>, and hyperlinks can bring in useful background knowledge that may not be obvious via lexical similarity alone <ref type="bibr" target="#b2">(Asai et al., 2020)</ref>. Indeed, we empirically find that using hyperlinks yields a more performant LM ( ?5.5).</p><p>Diversity. In the document graph, some documents may have a very high in-degree (e.g., many incoming hyperlinks, like the "United States" page of Wikipedia), and others a low in-degree. If we uniformly sample from the linked documents for each anchor segment, we may include documents of high in-degree too often in the overall training data, losing diversity. To adjust so that all documents appear with a similar frequency in training, we sample a linked document with probability inversely proportional to its in-degree, as done in graph data mining literature <ref type="bibr" target="#b30">(Henzinger et al., 2000)</ref>. We find that this technique yields a better LM performance ( ?5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We experiment with our proposed approach in the general domain first, where we pretrain LinkBERT on Wikipedia articles with hyperlinks ( ?5.1) and evaluate on a suite of downstream tasks ( ?5.2). We compare with BERT <ref type="bibr" target="#b18">(Devlin et al., 2019)</ref> as our baseline. We experiment in the biomedical domain in ?6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pretraining setup</head><p>Data. We use the same pretraining corpus used by BERT: Wikipedia and BookCorpus <ref type="bibr">(Zhu et al., 2015)</ref>. For Wikipedia, we use the WikiExtractor 3 to extract hyperlinks between Wiki articles. We then create training instances by sampling contiguous, random, or linked segments as described in ?4, with the three options appearing uniformly (33%, 33%, 33%). For BookCorpus, we create training instance by sampling contiguous or random segments (50%, 50%) as in BERT. We then combine the training instances from Wikipedia and BookCorpus to train LinkBERT. In summary, our pretraining data is the same as BERT, except that we have hyperlinks between Wikipedia articles.</p><p>Implementation. We pretrain LinkBERT of three sizes, -tiny, -base and -large, following the configurations of BERT tiny (4.4M parameters), BERT base (110M params), and BERT large (340M params) <ref type="bibr" target="#b18">(Devlin et al., 2019;</ref><ref type="bibr" target="#b67">Turc et al., 2019)</ref>. We use -tiny mainly for ablation studies.</p><p>For -tiny, we pretrain from scratch with random weight initialization. We use the AdamW <ref type="bibr" target="#b49">(Loshchilov and Hutter, 2019)</ref> optimizer with (? 1 , ? 2 ) = (0.9, 0.98), warm up the learning rate for the first 5,000 steps and then linearly decay it.</p><p>3 https://github.com/attardi/wikiextractor</p><p>We train for 10,000 steps with a peak learning rate 5e-3, weight decay 0.01, and batch size of 2,048 sequences with 512 tokens. Training took 1 day on two GeForce RTX 2080 Ti GPUs with fp16.</p><p>For -base, we initialize LinkBERT with the BERT base checkpoint released by <ref type="bibr" target="#b18">Devlin et al. (2019)</ref> and continue pretraining. We use a peak learning rate 3e-4 and train for 40,000 steps. Other training hyperparameters are the same as -tiny.</p><p>Training took 4 days on four A100 GPUs with fp16.</p><p>For -large, we follow the same procedure as -base, except that we use a peak learning rate of 2e-4. Training took 7 days on eight A100 GPUs with fp16.</p><p>Baselines. We compare LinkBERT with BERT. Specifically, for the -tiny scale, we compare with BERT tiny , which we pretrain from scratch with the same hyperparameters as LinkBERT tiny . The only difference is that LinkBERT uses document links to create LM inputs, while BERT does not.</p><p>For -base scale, we compare with BERT base , for which we take the BERT base release by <ref type="bibr" target="#b18">Devlin et al. (2019)</ref> and continue pretraining it with the vanilla BERT objectives on the same corpus for the same number of steps as LinkBERT base .</p><p>For -large, we follow the same procedure as -base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation tasks</head><p>We fine-tune and evaluate LinkBERT on a suite of downstream tasks.</p><p>Extractive question answering (QA). Given a document (or set of documents) and a question as input, the task is to identify an answer span from the document. We evaluate on six popular datasets from the MRQA shared task <ref type="bibr" target="#b22">(Fisch et al., 2019)</ref>:</p><p>HotpotQA <ref type="bibr" target="#b75">(Yang et al., 2018)</ref>, TriviaQA <ref type="bibr" target="#b36">(Joshi et al., 2017)</ref>, NaturalQ <ref type="bibr" target="#b42">(Kwiatkowski et al., 2019)</ref>, SearchQA <ref type="bibr" target="#b21">(Dunn et al., 2017)</ref>, NewsQA <ref type="bibr" target="#b66">(Trischler et al., 2017)</ref>, and SQuAD <ref type="bibr" target="#b59">(Rajpurkar et al., 2016)</ref>.</p><p>As the MRQA shared task does not have a public test set, we split the dev set in half to make new dev and test sets. We follow the fine-tuning method BERT <ref type="bibr" target="#b18">(Devlin et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the performance (F1 score) on MRQA datasets. LinkBERT substantially outperforms BERT on all datasets. On average, the gain is +4.1% absolute for the BERT tiny scale, +2.6% for the BERT base scale, and +2.5% for the BERT large scale. Table <ref type="table">2</ref> shows the results on GLUE, where LinkBERT performs moderately better than BERT. These results suggest that LinkBERT is especially effective at learning knowledge useful for QA tasks (e.g. world knowledge), while keeping performance on sentence-level language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>We further study when LinkBERT is especially useful in downstream tasks.</p><p>Improved multi-hop reasoning. In Table <ref type="table" target="#tab_0">1</ref>, we find that LinkBERT obtains notably large gains on QA datasets that require reasoning with multiple documents, such as HotpotQA (+5% over BERT tiny ), TriviaQA (+6%) and SearchQA (+8%), as opposed to SQuAD (+1.4%) which just has a single document per question. To further gain qualitative insights, we studied in what QA examples LinkBERT succeeds but BERT fails. Figure <ref type="figure">3</ref> shows a representative example from HotpotQA.</p><p>Answering the question needs 2-hop reasoning: identify "Roden Brothers were taken over by Birks Group" from the first document, and then "Birks Group is headquartered in Montreal" from the second document. While BERT tends to simply predict an entity near the question entity ("Toronto" in the first document, which is just 1-hop), LinkBERT correctly predicts the answer in the second document ("Montreal"). Our intuition is that because LinkBERT is pretrained with pairs of linked documents rather than purely single documents, it better learns how to flow information (e.g., do attention) across tokens when multiple related documents are given in the context. In summary, these results suggest that pretraining with linked documents helps for multi-hop reasoning on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improved understanding of document relations.</head><p>While the MRQA datasets typically use groundtruth documents as context for answering questions, in open-domain QA, QA systems need to use documents obtained by a retriever, which may include noisy documents besides gold ones <ref type="bibr" target="#b15">(Chen et al., 2017;</ref><ref type="bibr" target="#b21">Dunn et al., 2017)</ref>. In such cases, QA systems need to understand the document relations to perform well <ref type="bibr" target="#b75">(Yang et al., 2018)</ref>. To simulate this setting, we modify the SQuAD dataset by prepending or appending 1-2 distracting documents to the original document given to each question. Table <ref type="table" target="#tab_1">3</ref> shows the result. While BERT incurs a large performance drop (-2.8%), LinkBERT is robust to distracting documents (-0.5%). This result suggests that pretraining with document links improves the ability to understand document relations and LinkBERT prediction: "Montreal" (?) BERT prediction: "Toronto" (?)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HotpotQA example</head><p>LinkBERT predicts: "Montreal" (?) BERT predicts: "Toronto" (?)</p><p>Figure <ref type="figure">3</ref>: Case study of multi-hop reasoning on HotpotQA.</p><p>Answering the question needs to identify "Roden Brothers were taken over by Birks Group" from the first document, and then "Birks Group is headquartered in Montreal" from the second document. While BERT tends to simply predict an entity near the question entity ("Toronto" in the first document), LinkBERT correctly predicts the answer in the second document ("Montreal").</p><p>relevance. In particular, our intuition is that the DRP objective helps the LM to better recognize document relations like (anchor document, linked document) in pretraining, which helps to recognize relations like (question, right document) in downstream QA tasks. We indeed find that ablating the DRP objective from LinkBERT hurts performance ( ?5.5). The strength of understanding document relations also suggests the promise of applying LinkBERT to various retrieval-augmented methods and tasks (e.g. <ref type="bibr">Lewis et al. 2020b)</ref>, either as the main LM or the dense retriever component.</p><p>Improved few-shot QA performance. We also find that LinkBERT is notably good at few-shot learning. Concretely, for each MRQA dataset, we fine-tune with only 10% of the available training data, and report the performance in Table <ref type="table" target="#tab_2">4</ref>. In this few-shot regime, LinkBERT attains more significant gains over BERT, compared to the full-resource regime in Table <ref type="table" target="#tab_0">1</ref> (on NaturalQ, 5.4% vs 1.8% absolute in F1, or 15% vs 7% in relative error reduction). This result suggests that LinkBERT internalizes more knowledge than BERT during pretraining, which supports our core idea that document links can bring in new, useful knowledge for LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation studies</head><p>We conduct ablation studies on the key design choices of LinkBERT.</p><p>What linked documents to feed into LMs? We study the strategies discussed in ?4.3 for obtaining linked documents: relevance, salience, and diversity.</p><p>Table <ref type="table">5</ref> shows the ablation result on MRQA datasets. First, if we ignore relevance and use random document links instead of hyperlinks, we get the same performance as BERT (-4.1% on average; "random" in Table <ref type="table">5</ref>). Second, using lexical similarity links instead of hyperlinks leads to 1.8% performance drop ("TF-IDF"). Our intuition is that hyperlinks can provide more salient knowledge that may not be obvious from lexical similarity alone. Nevertheless, using lexical similarity links is substantially better than BERT (+2.3%), confirming the efficacy of placing relevant documents together in the input for LM pretraining. Finally, removing the diversity adjustment in document sampling leads to 1% performance drop ("No diversity"). In summary, our insight is that to create informative inputs for LM pretraining, the linked documents must be semantically relevant and ideally be salient and diverse.</p><p>Effect of the DRP objective. Table <ref type="table">6</ref> shows the ablation result on the DRP objective ( ?4.2). Removing DRP in pretraining hurts downstream QA performance. The drop is large on tasks with multiple documents (HotpotQA, TriviaQA, and SQuAD with distracting documents). This suggests that DRP facilitates LMs to learn document relations.</p><p>6 Biomedical LinkBERT (BioLinkBERT)</p><p>Pretraining LMs on biomedical text is shown to boost performance on biomedical NLP tasks <ref type="bibr" target="#b4">(Beltagy et al., 2019;</ref><ref type="bibr" target="#b43">Lee et al., 2020;</ref><ref type="bibr">Lewis et al., 2020a;</ref><ref type="bibr" target="#b25">Gu et al., 2020)</ref>. Biomedical LMs are typically trained on PubMed, which contains abstracts and citations of biomedical papers. While prior works only use their raw text for pretraining, academic papers have rich dependencies with each other via citations (references). We hypothesize that incorporating citation links can help LMs learn dependencies between papers and knowledge that spans across them.</p><p>With this motivation, we pretrain LinkBERT on PubMed with citation links ( ?6.1), which we term BioLinkBERT, and evaluate on biomedical downstream tasks ( ?6.2). As our baseline, we follow and compare with the state-of-the-art biomedical LM, PubmedBERT <ref type="bibr" target="#b25">(Gu et al., 2020)</ref>, which has the same architecture as BERT and is trained on PubMed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Pretraining setup</head><p>Data. We use the same pretraining corpus used by PubmedBERT: PubMed abstracts (21GB). <ref type="foot" target="#foot_1">4</ref> We use the Pubmed Parser 5 to extract citation links between articles. We then create training instances by sampling contiguous, random, or linked segments as described in ?4, with the three options appearing uniformly (33%, 33%, 33%). In summary, our pretraining data is the same as PubmedBERT, except that we have citation links between PubMed articles.</p><p>Implementation. We pretrain BioLinkBERT of -base size (110M params) from scratch, following the same hyperparamters as the PubmedBERT base <ref type="bibr" target="#b25">(Gu et al., 2020)</ref>. Specifically, we use a peak learning rate 6e-4, batch size 8,192, and train for 62,500 steps. We warm up the learning rate in the first 10% of steps and then linearly decay it. Training took 7 days on eight A100 GPUs with fp16.</p><p>Additionally, while the original PubmedBERT release did not include the -large size, we pretrain BioLinkBERT of the -large size (340M params) from scratch, following the same procedure as -base, except that we use a peak learning rate of 4e-4 and warm up steps of 20%. Training took 21 days on eight A100 GPUs with fp16.</p><p>Baselines. We compare BioLinkBERT with PubmedBERT released by <ref type="bibr" target="#b25">Gu et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation tasks</head><p>For downstream tasks, we evaluate on the BLURB benchmark <ref type="bibr" target="#b25">(Gu et al., 2020)</ref>, a diverse set of biomedical NLP datasets, and MedQA-USMLE <ref type="bibr" target="#b33">(Jin et al., 2021)</ref>, a challenging biomedical QA dataset.</p><p>BLURB consists of five named entity recognition tasks, a PICO (population, intervention, comparison, and outcome) extraction task, three relation extraction tasks, a sentence similarity task, a document classification task, and two question answering tasks, as summarized in Table <ref type="table" target="#tab_4">7</ref>. We follow the same fine-tuning method and evaluation metric used by PubmedBERT <ref type="bibr" target="#b25">(Gu et al., 2020)</ref>.</p><p>MedQA-USMLE is a 4-way multi-choice QA task that tests biomedical and clinical knowledge. The questions are from practice tests for the US Medical License Exams (USMLE). The questions typically require multi-hop reasoning, e.g., given patient symptoms, infer the likely cause, and then answer the appropriate diagnosis procedure (Figure <ref type="figure" target="#fig_1">4</ref>). We follow the fine-tuning method in <ref type="bibr" target="#b33">Jin et al. (2021)</ref>. More details are provided in Appendix B.</p><p>MMLU-professional medicine is a multichoice QA task that tests biomedical knowledge and reasoning, and is part of the popular MMLU 5 https://github.com/titipata/pubmed_parser</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PubMed-BERTbase</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioLink-BERTbase</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioLink-BERTlarge</head><p>Named entity recognition BC5-chem <ref type="bibr" target="#b47">(Li et al., 2016)</ref> 93.33 93.75 94.04 BC5-disease (Li et al., 2016)  85.62 86.10 86.39 NCBI-disease <ref type="bibr" target="#b19">(Dogan et al., 2014)</ref> 87.82 88.18 88.76 BC2GM (Smith et al., 2008)  84.52 84.90 85.18 JNLPBA (Kim et al., 2004)  80.06 79.03 80.06 PICO extraction EBM PICO (Nye et al., 2018)  73.38 73.97 74.19</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation extraction</head><p>ChemProt <ref type="bibr" target="#b41">(Krallinger et al., 2017)</ref> 77.24 77.57 79.98 DDI (Herrero-Zazo et al., 2013)  82.36 82.72 83.35 GAD <ref type="bibr" target="#b9">(Bravo et al., 2015)</ref> 82.34 84.39 84.90</p><p>Sentence similarity BIOSSES (Soganc?oglu et al., 2017)  92.30 93.25 93.63</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document classification</head><p>HoC <ref type="bibr" target="#b3">(Baker et al., 2016)</ref> 82.32 84.35 84.87</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question answering</head><p>PubMedQA (Jin et al., 2019)  55.84 70.20 72.18 BioASQ <ref type="bibr" target="#b52">(Nentidis et al., 2019)</ref> 87 BioBERTlarge <ref type="bibr" target="#b43">(Lee et al., 2020)</ref> 36.7 QAGNN (Yasunaga et al., 2021)  38.0 GreaseLM <ref type="bibr" target="#b73">(Zhang et al., 2022)</ref> 38.5</p><p>PubmedBERTbase <ref type="bibr" target="#b25">(Gu et al., 2020)</ref> 38 GPT-3 (175B params) (Brown et al., 2020)  38.7 UnifiedQA (11B params) <ref type="bibr" target="#b39">(Khashabi et al., 2020)</ref> 43.2 BioLinkBERTlarge (Ours) 50.7</p><p>Table <ref type="table">9</ref>: Performance on MMLU-professional medicine.</p><p>BioLinkBERT significantly outperforms the largest generaldomain LM or QA model, despite having just 340M parameters.</p><p>benchmark <ref type="bibr" target="#b29">(Hendrycks et al., 2021)</ref> that is used to evaluate massive language models. We take the BioLinkBERT fine-tuned on the above MedQA-USMLE task, and evaluate on this task without further adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>BLURB. LinkBERT prediction: "Montreal" (?) BERT prediction: "Toronto" (?)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HotpotQA example</head><p>LinkBERT predicts: "Montreal" (?) BERT predicts: "Toronto" (?) from the patient symptoms described in the question (leg swelling, pancreatic cancer), infer the cause (deep vein thrombosis), and then infer the appropriate diagnosis procedure (compression ultrasonography). While the existing PubmedBERT tends to simply predict a choice that contains a word appearing in the question ("blood" for choice D), BioLinkBERT correctly predicts the answer (B). Our intuition is that citation links bring relevant documents together in the same context in pretraining (right), which readily provides the multi-hop knowledge needed for the reasoning (center).</p><p>tasks such as question answering (+7% on BioASQ and PubMedQA). This result is consistent with the general domain ( ?5.3) and confirms that LinkBERT helps to learn document dependencies better.  <ref type="figure" target="#fig_1">4</ref> shows a representative example. Answering the question (left) needs 2-hop reasoning (center): from the patient symptoms described in the question (leg swelling, pancreatic cancer), infer the cause (deep vein thrombosis), and then infer the appropriate diagnosis procedure (compression ultrasonography). We find that while the existing PubmedBERT tends to simply predict a choice that contains a word appearing in the question ("blood" for choice D), BioLinkBERT correctly predicts the answer (B). Our intuition is that citation links bring relevant documents and concepts together in the same context in pretraining (right), 6 which readily provides the multi-hop knowledge needed for the reasoning (center). Combined with the analysis on HotpotQA ( ?5.4), our results suggest that pretraining with document links consistently helps for multi-hop reasoning across domains (e.g., general documents with hyperlinks and biomedical articles with citation links). 6 For instance, as in Figure <ref type="figure" target="#fig_1">4</ref> (right), <ref type="bibr" target="#b1">Ansari et al. (2015)</ref> in PubMed mention that pancreatic cancer can induce deep vein thrombosis in leg, and it cites a paper in PubMed, <ref type="bibr" target="#b56">Piovella et al. (2002)</ref>, which mention that deep vein thrombosis is tested by compression ultrasonography. Placing these two documents in the same context yields the complete multi-hop knowledge needed to answer the question ("pancreatic cancer" ? "deep vein thrombosis" ? "compression ultrasonography").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MedQA-USMLE.</head><p>MMLU-professional medicine. Table <ref type="table">9</ref> shows the performance. Despite having just 340M parameters, BioLinkBERT large achieves 50% accuracy on this QA task, significantly outperforming the largest general-domain LM or QA models such as GPT-3 175B params (39% accuracy) and UnifiedQA 11B params (43% accuracy). This result shows that with an effective pretraining approach, a small domain-specialized LM can outperform orders of magnitude larger language models on QA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented LinkBERT, a new language model (LM) pretraining method that incorporates document link knowledge such as hyperlinks. In both the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links), LinkBERT outperforms previous BERT models across a wide range of downstream tasks. The gains are notably large for multi-hop reasoning, multi-document understanding and few-shot question answering, suggesting that LinkBERT effectively internalizes salient knowledge through document links. Our results suggest that LinkBERT can be a strong pretrained LM to be applied to various knowledge-intensive tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>); He et al. (2020); Wang et al. (2021b) combine LM training with KG embeddings. Sun et al. (2020); Yasunaga et al. (2021); Zhang et al. (2022) combine LMs and graph neural networks (GNNs) to jointly train on text and KGs. Different from KGs, we use document graphs to learn knowledge that spans across documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Case study of multi-hop reasoning on MedQA-USMLE. Answering the question (left) needs 2-hop reasoning (center):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>uses for extractive QA. More details are provided in Appendix B. Performance (F1) on MRQA question answering datasets. LinkBERT consistently outperforms BERT on all datasets across the -tiny, -base, and -large scales. The gain is especially large on datasets that require reasoning with multiple documents in the context, such as HotpotQA, TriviaQA, SearchQA.</figDesc><table><row><cell>GLUE. The General Language Understanding</cell></row><row><cell>Evaluation (GLUE) benchmark (Wang et al., 2018)</cell></row><row><cell>is a popular suite of sentence-level classification</cell></row><row><cell>tasks. Following BERT, we evaluate on CoLA</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p><ref type="bibr" target="#b71">(Warstadt et al., 2019)</ref></p>, SST-2</p><ref type="bibr" target="#b63">(Socher et al., 2013)</ref></p>,</p>MRPC (Dolan and Brockett, 2005)</p>, QQP, STS-B</p><ref type="bibr" target="#b13">(Cer et al., 2017)</ref></p>, MNLI</p><ref type="bibr" target="#b72">(Williams et al., 2017)</ref></p>, QNLI</p><ref type="bibr" target="#b59">(Rajpurkar et al., 2016)</ref></p>, and RTE</p><ref type="bibr" target="#b17">(Dagan et al., 2005;</ref><ref type="bibr" target="#b27">Haim et al., 2006;</ref> Giampiccolo    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Performance (F1) on SQuAD when distracting documents are added to the context. While BERT incurs a large drop in F1, LinkBERT does not, suggesting its robustness in understanding document relations.</figDesc><table><row><cell></cell><cell cols="4">HotpotQA TriviaQA NaturalQ SQuAD</cell></row><row><cell>BERTbase</cell><cell>64.8</cell><cell>59.2</cell><cell>64.8</cell><cell>79.6</cell></row><row><cell>LinkBERTbase</cell><cell>70.5</cell><cell>66.0</cell><cell>70.2</cell><cell>82.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Few-shot QA performance (F1) when 10% of finetuning data is used. LinkBERT attains large gains, suggesting that it internalizes more knowledge than BERT in pretraining.</figDesc><table><row><cell></cell><cell cols="5">HotpotQA TriviaQA NaturalQ SQuAD</cell></row><row><cell>LinkBERTtiny</cell><cell></cell><cell>54.6</cell><cell>50.0</cell><cell>60.3</cell><cell>58.0</cell></row><row><cell>No diversity</cell><cell></cell><cell>53.5</cell><cell>48.0</cell><cell>60.0</cell><cell>57.8</cell></row><row><cell cols="2">Change hyperlink to TF-IDF</cell><cell>50.0</cell><cell>48.2</cell><cell>59.6</cell><cell>57.6</cell></row><row><cell cols="2">Change hyperlink to random</cell><cell>49.8</cell><cell>43.4</cell><cell>58.9</cell><cell>56.6</cell></row><row><cell cols="6">Table 5: Ablation study on what linked documents to feed</cell></row><row><cell cols="2">into LM pretraining ( ?4.3).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">HotpotQA TriviaQA NaturalQ SQuAD SQuAD distract</cell></row><row><cell>LinkBERTbase</cell><cell>78.2</cell><cell>73.9</cell><cell>78.3</cell><cell>90.1</cell><cell>89.6</cell></row><row><cell>No DRP</cell><cell>76.5</cell><cell>72.5</cell><cell>77.0</cell><cell>89.3</cell><cell>87.0</cell></row><row><cell cols="6">Table 6: Ablation study on the document relation prediction</cell></row><row><cell cols="4">(DRP) objective in LM pretraining ( ?4.2).</cell><cell></cell><cell></cell></row></table><note><p><p>et al., 2007)</p>, and report the average score. More fine-tuning details are provided in Appendix B.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city? Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden. In the 1910s the firm became known as Roden Bros. Ltd. and were later taken over by Henry Birks and Sons in 1953. ... In 1974 Roden Bros. Ltd. published the book, "Rich Cut Glass" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden. In the 1910s the firm became known as Roden Bros. Ltd. and were later taken over by HenryBirks and Sons  in 1953. ... In 1974 Roden Bros.  Ltd. published the book, "Rich Cut Glass" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto.</figDesc><table><row><cell>Festival held each spring. The Jefferson Memorial, the Martin Luther King Jr. Memorial, the</cell><cell>varieties of 3,020 trees, the Yoshino Cherry now dominates. ...</cell></row><row><cell>Franklin Delano Roosevelt</cell><cell></cell></row><row><cell>Memorial, and the George Mason</cell><cell></cell></row><row><cell>Memorial are situated adjacent</cell><cell></cell></row><row><cell>to the Tidal Basin.</cell><cell></cell></row><row><cell>HotpotQA example</cell><cell></cell></row><row><cell></cell><cell>Question: Roden Brothers were taken over in 1953 by a group</cell></row><row><cell></cell><cell>headquartered in which Canadian city?</cell></row><row><cell></cell><cell>Doc B: Birks Group (formerly Birks &amp; Mayors) is a designer,</cell></row><row><cell>Doc B: Birks Group (formerly Birks &amp; Mayors) is a designer,</cell><cell>manufacturer and retailer of jewellery, timepieces, silverware and gifts,</cell></row><row><cell>manufacturer and retailer of jewellery, timepieces, silverware and gifts,</cell><cell>with stores and manufacturing facilities located in Canada and the United</cell></row><row><cell>with stores and manufacturing facilities located in Canada and the</cell><cell>States. As of June 30, 2015, it operates stores under three different retail</cell></row><row><cell>United States. As of June 30, 2015, it operates stores under three</cell><cell>banners: ... The company is headquartered in Montreal, Quebec, with</cell></row><row><cell>different retail banners: ? The company is headquartered in Montreal,</cell><cell>American corporate offices located in Tamarac, Florida.</cell></row><row><cell>Quebec, with American corporate offices located in Tamarac, Florida.</cell><cell></cell></row><row><cell>LinkBERT prediction: "Montreal" (?)</cell><cell></cell></row><row><cell>BERT prediction: "Toronto" (?)</cell><cell></cell></row></table><note><p>Question:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>Performance on BLURB benchmark. BioLinkBERT attains improvement on all tasks, establishing new state of the art on BLURB. Gains are notably large on document-level tasks such as PubMedQA and BioASQ.</figDesc><table><row><cell></cell><cell>.56</cell><cell>91.43</cell><cell>94.82</cell></row><row><cell>BLURB score</cell><cell>81.10</cell><cell>83.39</cell><cell>84.30</cell></row><row><cell>Methods</cell><cell></cell><cell>Acc. (%)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Performance on MedQA-USMLE. BioLinkBERT outperforms all previous biomedical LMs.</figDesc><table><row><cell></cell><cell>.1</cell></row><row><cell>BioLinkBERTbase (Ours)</cell><cell>40.0</cell></row><row><cell>BioLinkBERTlarge (Ours)</cell><cell>44.6</cell></row><row><cell>Methods</cell><cell>Acc. (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table 7 shows the results on BLURB. BioLinkBERT base outperforms PubmedBERT base on all task categories, attaining a performance boost of +2% absolute on average. Moreover, BioLinkBERT large provides a further boost of +1%. In total, BioLinkBERT outperforms the previous best by +3% absolute, establishing a new state of the art on the BLURB leaderboard. We see a trend that gains are notably large on document-level Three days after undergoing a laparoscopic Whipple's procedure, a 43-year-old woman has swelling of her right leg. ... She was diagnosed with pancreatic cancer 1 month ago. ... Her temperature is 38?C (100.4?F ), pulse is 90/min, and blood pressure is 118/78 mm Hg. Examination shows mild swelling of the right thigh to the ankle; there is no erythema or pitting edema. ... Which of the following is the most appropriate next step in management? Doc A: ... Pancreatic cancer can induce deep vein thrombosis in leg ... (e.g. Ansari et al. 2015) Doc B: ... Deep vein thrombosis is tested by compression ultrasonography ... Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden. In the 1910s the firm became known as Roden Bros. Ltd. and were later taken over by Henry Birks and Sons in 1953. ... In 1974 Roden Bros. Ltd. published the book, "Rich Cut Glass" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden. In the 1910s the firm became known as Roden Bros. Ltd. and were later taken over by Henry Birks and Sons in 1953. ... In 1974 Roden Bros. Ltd. published the book, "Rich Cut Glass" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto.</figDesc><table><row><cell cols="2">MedQA-USMLE example</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Need multi-hop reasoning</cell><cell>Knowledge learned via document links</cell></row><row><cell></cell><cell></cell><cell cols="2">Leg swelling, pancreatic cancer</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(symptom)</cell></row><row><cell></cell><cell></cell><cell cols="2">Deep vein thrombosis</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(possible cause)</cell></row><row><cell cols="2">(A) CT pulmonary angiography (B) Compression ultrasonography (C) D-dimer level (D) 2 sets of blood cultures</cell><cell cols="2">Compression ultrasonography (next step for diagnosis)</cell><cell>(e.g. Piovella et al. 2002)</cell></row><row><cell>LinkBERT predicts: B (?)</cell><cell>PubmedBERT predicts: D (?)</cell><cell></cell></row><row><cell></cell><cell cols="2">[Tidal Basin, Washington D.C.]</cell></row><row><cell></cell><cell cols="2">The Tidal Basin is a man-made</cell></row><row><cell></cell><cell cols="2">reservoir located between the</cell></row><row><cell></cell><cell>Potomac River and the</cell><cell></cell></row><row><cell></cell><cell cols="2">Washington Channel in Washington, D.C. It is part of West Potomac Park, is near the National Mall and is a focal point of the National Cherry Blossom Festival held each spring. The Jefferson Memorial, the Martin Luther King Jr. Memorial, the</cell><cell>[The National Cherry Blossom Festival] ? It is a spring celebration commemorating the March 27, 1912, gift of Japanese cherry trees from Mayor of Tokyo City to the city of Washington, D.C. ... Of the initial gift of 12 varieties of 3,020 trees, the Yoshino Cherry now dominates. ...</cell></row><row><cell></cell><cell>Franklin Delano Roosevelt</cell><cell></cell></row><row><cell></cell><cell cols="2">Memorial, and the George Mason</cell></row><row><cell></cell><cell cols="2">Memorial are situated adjacent</cell></row><row><cell></cell><cell>to the Tidal Basin.</cell><cell></cell></row><row><cell></cell><cell>HotpotQA example</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Question: Roden Brothers were taken over in 1953 by a group</cell></row><row><cell></cell><cell></cell><cell></cell><cell>headquartered in which Canadian city?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Doc B: Birks Group (formerly Birks &amp; Mayors) is a designer,</cell></row><row><cell cols="2">Doc B: Birks Group (formerly Birks &amp; Mayors) is a designer,</cell><cell></cell><cell>manufacturer and retailer of jewellery, timepieces, silverware and gifts,</cell></row><row><cell cols="2">manufacturer and retailer of jewellery, timepieces, silverware and gifts,</cell><cell></cell><cell>with stores and manufacturing facilities located in Canada and the United</cell></row><row><cell cols="2">with stores and manufacturing facilities located in Canada and the</cell><cell></cell><cell>States. As of June 30, 2015, it operates stores under three different retail</cell></row><row><cell cols="2">United States. As of June 30, 2015, it operates stores under three</cell><cell></cell><cell>banners: ... The company is headquartered in Montreal, Quebec, with</cell></row><row><cell cols="2">different retail banners: ? The company is headquartered in Montreal,</cell><cell></cell><cell>American corporate offices located in Tamarac, Florida.</cell></row><row><cell cols="2">Quebec, with American corporate offices located in Tamarac, Florida.</cell><cell></cell></row><row><cell cols="2">LinkBERT prediction: "Montreal" (?)</cell><cell></cell></row><row><cell cols="2">BERT prediction: "Toronto" (?)</cell><cell></cell></row></table><note><p><p>Reference</p>Question: Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city? Doc A:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 8 shows the results. BioLinkBERT base obtains a 2% accuracy boost over PubmedBERT base , and BioLinkBERT large provides an additional +5% boost. In total, Bi-oLinkBERT outperforms the previous best by +7% absolute, setting a new state of the art. To further gain qualitative insights, we studied in what QA examples BioLinkBERT succeeds but the baseline PubmedBERT fails. Figure</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>A segment is typically a sentence or a paragraph.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://pubmed.ncbi.nlm.nih.gov. We use papers published before Feb. 2020 as in PubmedBERT.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Pretrained models, code and data are available at https://github.com/michiyasunaga/ LinkBERT. Experiments are available at https://worksheets. codalab.org/worksheets/ 0x7a6ab9c8d06a41d191335b270da2902e.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We thank <rs type="person">Siddharth Karamcheti</rs>, members of the <rs type="institution">Stanford P-Lambda</rs>, SNAP and NLP groups, as well as our anonymous reviewers for valuable feedback. We gratefully acknowledge the support of a <rs type="grantName">PECASE Award</rs>; DARPA under Nos. <rs type="grantNumber">HR00112190039</rs> (TAMI), N660011924033 (MCS); Funai Foundation Fellowship; Microsoft Research PhD Fellowship; ARO under Nos. <rs type="grantNumber">W911NF-16-1-0342</rs> (MURI), <rs type="grantNumber">W911NF-16-1-0171</rs> (DURIP); <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">OAC-1835598</rs> (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID), <rs type="funder">NIH</rs> under No. <rs type="grantNumber">R56LM013365</rs>; <rs type="affiliation">Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMorgan Chase, Docomo, Hitachi, Intel, KDDI, Toshiba</rs>, <rs type="funder">NEC</rs>, <rs type="person">Juniper</rs>, and <rs type="funder">UnitedHealth Group</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_V3qQwXM">
					<idno type="grant-number">HR00112190039</idno>
					<orgName type="grant-name">PECASE Award</orgName>
				</org>
				<org type="funding" xml:id="_qxRUVgV">
					<idno type="grant-number">W911NF-16-1-0342</idno>
				</org>
				<org type="funding" xml:id="_E2FspZ8">
					<idno type="grant-number">W911NF-16-1-0171</idno>
				</org>
				<org type="funding" xml:id="_j2VGtVD">
					<idno type="grant-number">OAC-1835598</idno>
				</org>
				<org type="funding" xml:id="_PZbPkRs">
					<idno type="grant-number">R56LM013365</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ethics, limitations and risks</head><p>We outline potential ethical issues with our work below. First, LinkBERT is trained on the same text corpora (e.g., Wikipedia, Books, PubMed) as in existing language models. Consequently, LinkBERT could reflect the same biases and toxic behaviors exhibited by language models, such as biases about race, gender, and other demographic attributes <ref type="bibr" target="#b61">(Sheng et al., 2020)</ref>. Another source of ethical concern is the use of the MedQA-USMLE evaluation <ref type="bibr" target="#b33">(Jin et al., 2021)</ref>. While we find this clinical reasoning task to be an interesting testbed for LinkBERT and for multi-hop reasoning in general, we do not encourage users to use the current models for real world clinical prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Fine-tuning details</head><p>We apply the following fine-tuning hyperparameters to all models, including the baselines.</p><p>MRQA. For all the extractive question answering datasets, we use max_seq_length = 384 and a sliding window of size 128 if the lengths are longer than max_seq_length.</p><p>For the -tiny scale (BERT tiny , LinkBERT tiny ), we choose learning rates from {5e-5, 1e-4, 3e-4}, batch sizes from {16, 32, 64}, and fine-tuning epochs from {5, 10}.</p><p>For -base (BERT base , LinkBERT base ), we choose learning rates from {2e-5, 3e-5}, batch sizes from {12, 24}, and fine-tuning epochs from {2, 4}.</p><p>For -large (BERT large , LinkBERT large ), we choose learning rates from {1e-5, 2e-5}, batch sizes from {16, 32}, and fine-tuning epochs from {2, 4}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE.</head><p>We use max_seq_length = 128.</p><p>For the -tiny scale (BERT tiny , LinkBERT tiny ), we choose learning rates from {5e-5, 1e-4, 3e-4}, batch sizes from {16, 32, 64}, and fine-tuning epochs from {5, 10}.</p><p>For -base and -large (BERT base , LinkBERT base , BERT large , LinkBERT large ), we choose learning rates from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5}, batch sizes from {16, 32, 64} and fine-tuning epochs from 3-10.</p><p>BLURB. We use max_seq_length = 512 and choose learning rates from {1e-5, 2e-5, 3e-5, 5e-5, 6e-5}, batch sizes from {16, 32, 64} and fine-tuning epochs from 1-120.</p><p>MedQA-USMLE. We use max_seq_length = 512 and choose learning rates from {1e-5, 2e-5, 3e-5}, batch sizes from {16, 32, 64} and fine-tuning epochs from 1-6.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06955</idno>
		<title level="m">Htlm: Hyper-text pre-training and prompting of language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pancreatic cancer and thromboembolic disease, 150 years after trousseau. Hepatobiliary surgery and nutrition</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ke</forename><surname>Andr?n-Sandberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic semantic classification of scientific literature according to the hallmarks of cancer</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilona</forename><surname>Silins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>H?gberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulla</forename><surname>Stenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scibert: Pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Content-based citation recommendation</title>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamal</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladri</forename><surname>Castellon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Creel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dora</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moussa</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Doumbouya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Etchemendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Ethayarajh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelby</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyusha</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fereshte</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohith</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kuditipudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Levent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvir</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mirchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zanele</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Munyikwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avanika</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Nilforoshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giray</forename><surname>Nyarko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ogut ; Krishnan Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><forename type="middle">W</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><forename type="middle">E</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<editor>Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R?, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih,</editor>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>?elikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research</title>
		<author>
			<persName><forename type="first">?lex</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><surname>Pi?ero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N?ria</forename><surname>Queralt-Rosinach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rautschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">I</forename><surname>Furlong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language models are fewshot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Avi</forename><surname>Caciularu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arie</forename><surname>Cattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00406</idno>
		<title level="m">Crossdocument language modeling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wikipedia entities as rendezvous across languages: Grounding multilingual language models by predicting wikipedia hyperlinks</title>
		<author>
			<persName><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluation (SemEval)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pre-training tasks for embedding-based large-scale retrieval</title>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Specter: Document-level representation learning using citation-informed transformers</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName><forename type="first">Rezarta</forename><surname>Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ugur Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mrqa 2019 shared task: Evaluating generalization in reading comprehension</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Reading for Question Answering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better fewshot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
		<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15779</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Realm: Retrievalaugmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Bar Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integrating graph contextualized knowledge into pre-trained language models</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On near-uniform url sampling</title>
		<author>
			<persName><forename type="first">Allan</forename><surname>Monika R Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heydon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName><surname>Najork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Computer Networks</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The ddi corpus: An annotated corpus with pharmacological substances and drug-drug interactions</title>
		<author>
			<persName><forename type="first">Mar?a</forename><surname>Herrero-Zazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paloma</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Applied Sciences</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>In Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting citation knowledge in personalised recommendation of recent scientific publications</title>
		<author>
			<persName><forename type="first">Anita</forename><surname>Khadka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Cantador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation Conference (LREC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unifiedqa: Crossing format boundaries with a single qa system</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Introduction to the bio-entity recognition task at jnlpba</title>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international joint workshop on natural language processing in biomedicine and its applications</title>
		<meeting>the international joint workshop on natural language processing in biomedicine and its applications</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gael P?rez Rodr?guez, Georgios Tsatsaronis, and Ander Intxaurrondo</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Obdulia</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jes?s</forename><surname>P?rez P?rez</surname></persName>
		</author>
		<author>
			<persName><surname>Santamar?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth BioCreative challenge evaluation workshop</title>
		<meeting>the sixth BioCreative challenge evaluation workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Overview of the biocreative vi chemical-protein interaction track</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
	<note>TACL</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jannai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Navon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04541</idno>
		<title level="m">The inductive bias of in-context learning: Rethinking pretraining example design</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Clinical Natural Language Processing Workshop</title>
		<meeting>the 3rd Clinical Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint/>
	</monogr>
	<note>et al. 2020b</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pretraining for ad-hoc retrieval: Hyperlink is also you need</title>
		<author>
			<persName><forename type="first">Zhengyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Margolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Laurence</surname></persName>
		</author>
		<title level="m">Concepts: core readings</title>
		<imprint>
			<publisher>Mit Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Results of the seventh edition of the bioasq challenge</title>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Junyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roma</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference</title>
		<meeting>the conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unified open-domain question answering with structured and unstructured knowledge</title>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Peshterliev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14610</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Normalization rates of compression ultrasonography in patients with a first episode of deep vein thrombosis of the lower limbs: association with recurrence and new thrombosis</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Piovella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Crippa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marisa</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vigano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Galli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Beltrametti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armando D'</forename><surname>Angelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Haematologica</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="515" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scientific paper summarization using citation summary networks</title>
		<author>
			<persName><forename type="first">Vahed</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Weakly supervised pretraining for multi-hop retriever</title>
		<author>
			<persName><forename type="first">Yeon</forename><surname>Seonwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Hoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Towards controllable biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Overview of biocreative ii gene mention recognition</title>
		<author>
			<persName><forename type="first">Larry</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><forename type="middle">K</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson Nee Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Ju</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Fang</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Nan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Shi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Biosses: a semantic sentence similarity estimation system for the biomedical domain</title>
		<author>
			<persName><forename type="first">Gizem</forename><surname>Soganc?oglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakime</forename><surname>?zt?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzucan</forename><surname>?zg?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Colake: Contextualized language and knowledge embedding</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Representation Learning for NLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<author>
			<persName><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">2021a. Relational message passing for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Kepler: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Scholak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengzu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansong</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05966</idno>
		<title level="m">Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">QA-GNN: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
