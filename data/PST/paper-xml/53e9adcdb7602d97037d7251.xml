<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Naturalness and Intuitiveness in Gesture Production: Insights for Touchless Gestural Interfaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sukeshini</forename><forename type="middle">A</forename><surname>Grandhi</surname></persName>
							<email>grandhi@humtec.rwth-aachen.de</email>
						</author>
						<author>
							<persName><forename type="first">Gina</forename><surname>Joue</surname></persName>
							<email>joue@humtec.rwth-aachen.de</email>
						</author>
						<author>
							<persName><forename type="first">Irene</forename><surname>Mittelberg</surname></persName>
							<email>mittelberg@humtec.rwth-aachen.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Natural Media and Engineering</orgName>
								<orgName type="institution" key="instit2">HumTec</orgName>
								<orgName type="institution" key="instit3">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>May 7-12</addrLine>
									<postCode>2011 â€¢</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Naturalness and Intuitiveness in Gesture Production: Insights for Touchless Gestural Interfaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A1926103E0425F012C5352B7A4754C22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gestures</term>
					<term>intuitiveness</term>
					<term>naturalness</term>
					<term>user centric interaction design</term>
					<term>cognitive semiotic principles</term>
					<term>metonymy H.5.2 Information Interfaces and Presentation: User Interfaces Design</term>
					<term>Human Factors</term>
					<term>Experimentation CHI 2011</term>
					<term>Session: Gestures</term>
					<term>Body &amp; Touch</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores how interaction with systems using touchless gestures can be made intuitive and natural. Analysis of 912 video clips of gesture production from a user study of 16 subjects communicating transitive actions (manipulation of objects with or without external tools) indicated that 1) dynamic pantomimic gestures where imagined tool/object is explicitly held are performed more intuitively and easily than gestures where a body part is used to represent the tool/object or compared to static hand poses and 2) gesturing while communicating the transitive action as how the user habitually performs the action (pantomimic action) is perceived to be easier and more natural than gesturing while communicating it as an instruction. These findings provide guidelines for the characteristics of gestures and user mental models one must consciously be concerned with when designing and implementing gesture vocabularies of touchless interaction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Touchless gestural interfaces have many potential applications such as in sterile/clean room environments, collocated shared technology and robotics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. However, unlike touch gestures, touchless gestures remain largely a notion developed in science fiction (as depicted in the popular sci-fi movie Minority Report) and have only been implemented to a limited degree in a few proof-ofconcept research applications (e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>) and video games (e.g. Sony Eye Toy, Microsoft Kinect). This limited implementation of touchless gestures is due to challenges in 1) achieving accurate and meaningful gesture recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and 2) identifying natural, intuitive and meaningful gesture vocabularies appropriate for the tasks in question <ref type="bibr" target="#b9">[10]</ref>. While computer vision researchers have long been working on gesture recognition, the challenge of generating natural and intuitive touchless gesture vocabulary while keeping user experience in mind, has received relatively very little research attention. The gesture vocabulary used has often been an ad-hoc choice made by the designer to trigger certain pre-assigned actions (e.g. the gesture of clapping one's hands to turn on the computer). Mostly chosen for their ease of implementation to facilitate distinctive recognition and segmentation, such gestures have arbitrary mappings that require users to learn a set of gestures and the associated commands they trigger. This often puts a strain on user memory and defeats the purpose of using gestures as a way to facilitate intuitive and natural interaction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>. The aim of the work presented in this paper is to take a small step towards understanding what makes touchless gesture production natural and intuitive to provide design guidelines for such interfaces. We adopt a vernacular definition of "natural" as being marked by spontaneity and "intuitive" as coming naturally without excessive deliberation. While several classifications of gestures and their functions exist <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>, we adopt a semiotic or communicative approach to gestures <ref type="bibr" target="#b11">[12]</ref> for HCI, which corresponds to Caldoz's classification of semiotic hand movements <ref type="bibr" target="#b3">[4]</ref>, or movements that communicate information from shared common ground. Since 90% of semiotic gestures are accompanied by speech <ref type="bibr" target="#b10">[11]</ref>, we look at gestures in association with speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESEARCH QUESTIONS</head><p>In particular, this paper focuses on the communication of transitive actions, or actions involving the manipulation of objects with or without external tools. For example, how do we communicate to the system using gestures that a screen be erased? Specifically we view gesture forms in terms of its relationship to what is being communicated or represented. This was motivated by neuropsychology, psychiatry and developmental psychology literature which all suggest that for transitive gestures (e.g. brushing teeth with a toothbrush), it is more normal for the hand itself to hold an imagined object (e.g. holding an imaginary toothbrush) rather than representing a body part as the object itself (e.g. finger representing the toothbrush itself). Using the hand to represent the object itself is termed as "body-part-as-object" in neuropsychology, and is an example of "object substitution" in developmental psychology. In neurological examinations, this type of action is considered an error associated with movement planning disorders <ref type="bibr" target="#b6">[7]</ref>. The ability to perform hand actions where the hand holds an imagined object, referred to as pantomimed action with an imagined object in neuropsychology, is considered the proper developmental trend in children <ref type="bibr" target="#b2">[3]</ref>. Based on semiotic theory, "body-partas-object" gestures are referred to as exhibiting "internal metonymy", and "holding-imagined-object" gestures are referred to as "external metonymy" <ref type="bibr" target="#b11">[12]</ref>. Therefore, we hypothesize that body-part-as-object gestures, i.e. reflecting internal metonymy, are generally not as intuitive as hand actions holding-imagined-objects, i.e. reflecting external metonymy. In order to develop design principles for effective communication of transitive actions to a system using gestures, we considered the following two research questions:</p><p>1. aspects of a gesture (such as motion, hand shape and form) are natural and intuitive when communicating a transitive action? In particular, are gestures using hand actions holding-imagined-objects more intuitive and natural than using body-part-as-object?</p><p>2. Is it easier to gesture a transitive action when the user communicates as how s/he habitually performs the action or when the user communicates it as an instruction? That is, should one gesture as "this is how I do it" or "this is how you should do it"?</p><p>USER STUDY</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>In order to address the above research questions we adopted a user-centric approach <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Participants were told that the study explored people's intuitive preferences and natural tendencies in verbal and non-verbal expressions. Sixteen (8 female) native or near-native American English speakers of different U.S. regions were recruited from a university visitor pool. All except one (over 60 years old) were between 20-30 years old, and 68.8% were strongly right-handed based on the Edinburgh Handedness Survey.</p><p>Participants were presented pairs of pictures showing a "before" and "after" scenario. These scenarios (Table <ref type="table" target="#tab_0">1</ref>) reflected simple computer tasks (e.g. cut, erase), but were camouflaged as everyday non-computer scenarios to minimize the influence of conceptual models of performing these tasks on pre-existing input devices. Our definitions of intuitive and natural translate into the assumption that spontaneous and more frequently produced gestures in common tasks can indicate what is more "natural and intuitive," and how well people can maintain a so-called unintuitive gesture, can shed further light on naturalness.</p><p>As soon as the picture pair disappeared, participants were asked to speak aloud while performing a gesture to "explain and show" the experimenter (seated in front of them in a position where the pictures could not be seen), what needed to be done to achieve what was in the after-picture from the before-picture. Each picture pair was shown for 3 seconds on a 24Ê¹â€²Ê¹â€² monitor at a resolution of 1024 x 768 (stimulus visual angle of 7.15Â°). A total of 19 pairs of before-after pictures were used. The scenarios were divided into three categories: Transitive actions usually done 1) with an external tool (e.g. slicing an apple with a knife), 2) without an external tool (e.g. laying plates on a table) and 3) with or without an external tool (e.g. wiping a surface with bare hands or a cloth). Participants performed the "explain and show" scenario for the above tasks under three conditions to distinguish between the styles of communication (Instructional vs. Habitual) and to understand naturalness and intuitiveness of performing gestures with internal metonymy (body-part-as-tool).  The order of the three conditions as well as the 19 scenarios within each condition was randomized for each participant to minimize order effects. At the end of the study, participants were interviewed on their perspectives on the naturalness and intuitiveness of the communicating styles and tool/object representation in their gestures (internal or external metonymy). All sessions were recorded using two high-speed video cameras from two visual perspectives. the annotations were verified for consistency by the authors, and a second annotator blindly coded 25% of the data. The inter-rater reliability for the annotators was good (Cohen's Îº = 0.64, p&lt;0.001). Annotations consisted of 1) right-or left-hand use; 2) number of hands used; 3) use of body-part as tool/object vs. pantomimed action of imagined tool/object use; 4) dynamic (moving gesture) vs. static gesture poses; 5) distinctions between gestures involving a tool (device used to perform the dominant action, e.g. knife used for cutting) or object (entity upon which the dominant action is performed, e.g. the apple that is cut); and 6) distinctions between gestures that set up the context vs. execute the dominant action of the transitive action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gestures exhibited motion than static poses</head><p>Gestures representing the dominant action were seldom shown as static hand forms or poses (e.g. palm held sideways with extended fingers to represent cutting), but were almost always (95.7%) dynamic motions demonstrating the "use of tool".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pantomimed actions of holding-imagined-tool were more intuitive and easier than body-part-as-tool</head><p>In conditions 1 and 2 where participants were not told how to represent tools in their gestures, participants gestured holding an imaginary tool significantly more often than representing a part of their hand as a tool (Ï‡ 2 (4) = 54.5, p &lt; 0.001). Figure <ref type="figure" target="#fig_0">1</ref> and Video Clip 1 (video file uploaded with the paper) show a subject holding an imaginary knife in the right hand to cut an imaginary apple held in the left. Figure <ref type="figure">2</ref> and Video Clip 2 show the right palm to represent a knife.</p><p>In gestures where no external tool was used and the dominant action was pantomimed, the object, by default, was manipulated (Video Clip 3). In condition 3 where participants were explicitly asked to use body-part-as-tool to represent any tool used in the action, they failed to do so 77.5% of the time when an external tool was necessary. Only 3.7% of these participants "corrected" themselves to the instructions of the condition without prompting (Video Clip 4). The intuitiveness and ease of pantomiming with an imaginary tool in hand is further corroborated by participants' comments such as the following that were typical: "most difficult was using my hand as a tool. That really didn't come naturally", "It felt weird to use my finger as the tool because I don't do that. It's just more usual to use the toolâ€¦it's kind of awkward."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gestures represented objects explicitly rather implicitly</head><p>In all the three conditions, bimanual gestures were performed three times more often than one-handed gestures. Objects on which an action was performed, be it with an external tool or the hand itself, were depicted explicitly 45% of the time, as opposed to implicitly assuming its presence in one of three forms (illustrated in Communicating as "actor" easier than as commander"</p><p>Almost all participants reported that gesturing how they habitually perform an action was easier and more intuitive than gesturing an instruction. Comments such as the following were typical from the participants: "it was easier to visualize and pantomime what I would normally do than relay that information to youâ€¦", "I found showing how I do it easier because it was more natural because I know how to do it and instructing you I don't know if you will understand my hand gestures". Video Clip 5 exemplifies the struggle and awkwardness in terms of body orientation of a participant gesturing while instructing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION AND IMPLICATIONS</head><p>Our findings suggest that communicating with a system through gestures may be easier when designers adopt an embodied approach <ref type="bibr" target="#b5">[6]</ref>. Namely, user experience can be enhanced when the gesture vocabulary is developed based on the understanding that actions are embodied, i.e. situated in real-life social and physical scenarios. In particular, our findings provide the following key guiding principles for the design of touchless gestures involving transitive actions.</p><p>Firstly our findings have implications for how designers should communicate the presence and use of touchless gestural interactions in a system, be it in the form of text, symbols or illustrations and demonstrations <ref type="bibr" target="#b14">[15]</ref>. Gestures triggering manipulation of objects should be dynamic iconic representations of the motion required for the manipulation, rather than static iconic hand poses. For example, a gesture to trigger "delete" should be a "wiping" hand movement rather than a static hand sign. Instructions for use of such gestures may also be helpful as illustrations of the pantomimed action. For example, rather than showing an image of a knife to indicate the gesture to use, an illustration of hand holding a knife performing the cutting act could be shown.</p><p>Secondly, gestures to trigger tasks that suggest the use of a tool should not impose body-part-as-tool compositions because such gestures do not seem to come naturally and intuitively. Instead designers should consider selecting gestures that pantomime the actual action with imagined tool in hand. For example, a gesture to slice could be a pantomiming motion of cutting with an imaginary knife in hand rather than the gesture, open palm moving vertically up and down. However, it must be noted that when the hand shape can represent the tool or object, or draw attention to an important aspect of the tool/object being communicated, in an unambiguous way due to anatomy <ref type="bibr" target="#b12">[13]</ref>, using bodypart-as-tool may make more sense. This may be why certain body-part-as-object gestures have become emblematic, e.g. index and middle fingers showing scissors for cutting.</p><p>Thirdly, gestures in space to trigger manipulation of objects should be two-handed, as the non-dominant hand often appears to provide a reference frame while the dominant hand performs the transitive gesture. This is consistent with principles of Guiard's kinematic chain model of asymmetric bimanual actions <ref type="bibr" target="#b7">[8]</ref> and other research, which suggests that two hands together provide a better sense of virtual space than one hand for task execution <ref type="bibr" target="#b8">[9]</ref>.</p><p>Finally, since gestures performed in habitual conditions were reported to be easier and more intuitive, adopting an egocentric rather than an allocentric (viewer's) frame of reference to gesture may ease the awkwardness of interacting with a system for the user, which is often a major concern in human robotic communication <ref type="bibr" target="#b13">[14]</ref>. That is, instilling the mental model of an actor egocentrically pantomiming the action, i.e. with a communicative perspective of "I would like to do Task X", reduces the propensity of users to re-orient their reference frames to position gestures in space when communicating the perspective of "You do Task X for me".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIMITATIONS AND FUTURE WORK</head><p>In this study we concentrated on hand forms, motion and communication style. However, many other characteristics such as the intensity of hold and other gesture kinematics may also afford insights on what comes naturally and intuitively. Consequently, promising directions for future work include using motion capture data to identify other characteristics of gestures which may help define intuitive and natural interaction, such as distance, speed, trajectory, image schemas and gestural space.</p><p>Although learned behavior can become common and even "natural", we wanted to avoid the preconceived assumptions of how one is familiar with interacting with existing system interfaces and disguised our tasks as everyday tasks. Of course, this also gives rise to limitations as to how translatable these tasks are to specific computer applications. We acknowledge that a good gesture vocabulary is typically task/context-specific, and it is a challenge to develop generic principles that can be applied across different contexts <ref type="bibr" target="#b12">[13]</ref>. However, this challenge can be met if researchers collectively bring together their findings from diverse tasks/contexts. Thus, work presented here makes a small yet significant contribution towards the understanding of the core design principles to create intuitive and natural user experience for touchless gestural interfaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>Instructional: Participants instructed the experimenter on what needed to be done and began their instructions with the sentence frame, "You need toâ€¦" 2. Habitual: Participants explained how they normally would perform what needed to be done, beginning their explanations with the sentence frame, "This is how Iâ€¦" 3. Instructional using body-part-as-tool: Participants repeated the Instructional condition, but they additionally were required to use their hands to represent any tool they might need to use (internal metonymy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FINDINGS</head><label></label><figDesc>Data from 912 video clips (16 participants x 19 scenarios x 3 conditions) were blindly annotated by a coder. Parts of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figures 1-3 by the left hand for an apple being cut): pantomimed action of holding-imagined-object (52.5% in all conditions, Fig 1), using a hand to situate the space in which an object is placed (34.0% in all conditions, Fig 3) and part of hand as object when not explicitly asked to do so (18.1% in conditions 1 and 2, Fig 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 Figure 2 Figure 3</head><label>123</label><figDesc>Figure 1 Figure 2 Figure 3</figDesc><graphic coords="3,346.80,81.12,54.72,55.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The 19 scenarios and corresponding computing task (CT)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>May 7-12, 2011 â€¢ Vancouver, BC, Canada</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work has been funded by the Excellence Initiative of the German Research Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Remote Control of Objects Using Free-Hand Gestures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beaudouin-Lafon</surname></persName>
		</author>
		<author>
			<persName><surname>Charade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="28" to="35" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Put-that-there&quot;: Voice and gesture at the graphics interface</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 7th annual conference on Computer graphics and interactive techniques<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Preschool children&apos;s symbolic representation of objects through gestures</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Boyatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="729" to="735" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Cadoz</surname></persName>
		</author>
		<title level="m">Les realites virtuelles. Dominos, Flammarion</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Framework For Gesture Generation and Interpretation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision in Human-Machine Interaction</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="191" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Where the Action Is: The Foundations of Embodied Interaction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dourish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Disturbance of gesture and pantomine in aphasia</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goodglass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="703" to="720" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Asymmetric Division of Labor in Human Skilled Bimanual Action: The Kinematic Chain as a Model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guiard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Motor Behavior</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="486" to="517" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention and visual feedback: the bimanual frame of reference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pausch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Proffitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium on Interactive 3D Graphics</title>
		<meeting>Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In search of a natural gesture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">XRDS: Crossroads, The ACM Magazine for Students</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="9" to="12" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hand and mind: What gestures reveal about thought</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Metonymy first, metaphor second: A cognitive-semiotic approach to multimodal figures of speech in co-speech gesture</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mittelberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Waugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Metaphor</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Forceville</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Urios-Aparisi</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Mouton de Gruyter</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="329" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A procedure for developing intuitive and ergonomic gesture interfaces for HCI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Storring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gesture-Based Communication in Human-Computer Interaction: 5th International Gesture Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2915</biblScope>
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
	<note>GW 2003</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What Robotics Can Learn from HCI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Interactions Magazine</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="67" to="69" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Designing Gestural Interfaces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saffer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Userdefined gestures for surface computing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th Int. Conf. on Human Factors in Computing Systems</title>
		<meeting>of the 27th Int. Conf. on Human Factors in Computing Systems<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
