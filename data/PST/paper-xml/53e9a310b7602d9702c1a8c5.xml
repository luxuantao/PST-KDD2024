<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Semi-Supervision for Pairwise Constrained Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sugato</forename><surname>Basu</surname></persName>
							<email>sugato@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Univ. of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Computer</forename><surname>Sciences</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univ. of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
							<email>abanerje@ece.utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Univ. of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Computer Sciences,</roleName><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
							<email>mooney@cs.utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Univ. of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Active Semi-Supervision for Pairwise Constrained Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EE899C27C978856872067F722D79D7AB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised clustering uses a small amount of supervised data to aid unsupervised learning. One typical approach specifies a limited number of must-link and cannotlink constraints between pairs of examples. This paper presents a pairwise constrained clustering framework and a new method for actively selecting informative pairwise constraints to get improved clustering performance. The clustering and active learning methods are both easily scalable to large datasets, and can handle very high dimensional data. Experimental and theoretical results confirm that this active querying of pairwise constraints significantly improves the accuracy of clustering when given a relatively small amount of supervision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many data mining and machine learning tasks, there is a large supply of unlabeled data but limited labeled data, since labeled data can be expensive to generate. Consequently, semi-supervised learning, learning from a combination of both labeled and unlabeled data, has become a topic of significant recent interest <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. More specifically, semisupervised clustering, the use of class labels or pairwise constraints on some examples to aid unsupervised clustering, has been the focus of several recent projects <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>In a semi-supervised clustering setting, the focus is on clustering large amounts of unlabeled data in the presence of a small amount of supervised data. In this setting, we consider a framework that has pairwise must-link and cannotlink constraints between points in a dataset (with an associated cost of violating each constraint), in addition to having distances between the points. These constraints specify that two examples must be in the same cluster (must-link) or different clusters (cannot-link) <ref type="bibr" target="#b32">[33]</ref>. In real-world unsupervised learning tasks, e.g., clustering for speaker identification in a conversation <ref type="bibr" target="#b16">[17]</ref>, visual correspondence in multiview image processing <ref type="bibr" target="#b6">[7]</ref>, clustering multi-spectral information from Mars images <ref type="bibr" target="#b31">[32]</ref>, etc., considering supervision in the form of constraints is generally more practical than providing class labels, since true labels may be unknown a priori, while it can be easier to specify whether pairs of points belong to the same cluster or different clusters.</p><p>We propose a cost function for pairwise constrained clustering (PCC) that can be shown to be the configuration energy of a Hidden Markov Random Field (HMRF) over the data with a well-defined potential function and noise model. Then, the pairwise-constrained clustering problem becomes equivalent to finding the HMRF configuration with the highest posterior probability, i.e., minimizing its energy. We present an algorithm for solving this problem.</p><p>Further, in order to maximize the utility of the limited supervised data available in a semi-supervised setting, supervised training examples should be actively selected as maximally informative ones rather than chosen at random, if possible <ref type="bibr" target="#b26">[27]</ref>. In that case, fewer constraints will be required to significantly improve the clustering accuracy. To this end, we present a new method for actively selecting good pairwise constraints for semi-supervised clustering.</p><p>Both our active learning and pairwise constrained clustering algorithms are linear in the size of the data, and hence easily scalable to large datasets. Our formulation can also handle very high dimensional data, as our experiments on text datasets demonstrate.</p><p>Section 2 outlines the pairwise constrained clustering framework, and Section 3 presents a refinement of KMeans clustering <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, called PCKMeans, that utilizes pairwise constraints. In Section 4, we present a method for actively picking good constraints by asking queries of the form "Are these two examples in same or different classes?". Experimental results on clustering high-dimensional text data and UCI data demonstrate that (1) PCKMeans clustering with constraints performs considerably better than unconstrained KMeans clustering, and (2) active PCKMeans achieves significantly steeper learning curves compared to PCKMeans with random pairwise queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pairwise Constrained Clustering</head><p>Centroid-based partitional clustering algorithms (e.g., KMeans) find a disjoint partitioning ½ (with each partition having a centroid ) of a dataset Ü Ò ½ such that the total distance between the points and the cluster centroids is (locally) minimized. We introduce a framework for pairwise constrained clustering (PCC) that has pairwise must-link and cannot-link constraints <ref type="bibr" target="#b32">[33]</ref> between a subset of points in the dataset (with a cost of violating each constraint), in addition to distances between points. Since centroid-based clustering cannot handle pairwise constraints explicitly, we formulate the goal of clustering in the PCC framework as minimizing a combined objective function: the sum of the total distance between the points and their cluster centroids and the cost of violating the pairwise constraints.</p><p>For the PCC framework with both must-link and cannotlink constraints, let Å be the set of must-link pairs such that ´Ü Ü µ ¾ Å implies Ü and Ü should be assigned to the same cluster, and be the set of cannot-link pairs such that ´Ü Ü µ ¾ implies Ü and Ü should be assigned to different clusters. Note that we consider the tuples in Å and to be order-independent, i.e., ´Ü Ü µ ¾ µ ´Ü Ü µ ¾ , and so also for Å. Let Ï Û and Ï Û be two sets that give the weights corresponding to the must-link constraints in Å and the cannot-link constraints in respectively. Let Ð be the cluster assignment of a point Ü , where Ð ¾ ½ . The cost of violating mustlink and cannot-link constraints is typically quantified by metrics <ref type="bibr" target="#b22">[23]</ref>. We restrict our attention to the uniform metric (also known as the generalized Potts metric), for which the cost of violating a must-link ´Ü Ü µ ¾ Å is given by Û ½ Ð Ð ℄, i.e., if the must-linked points are assigned to two different clusters, the incurred cost is Û . Similarly, the cost of violating a cannot-link ´Ü Ü µ ¾ is given by Û ½ Ð Ð ℄, i.e., if the cannot-linked points are assigned to the same cluster, the incurred cost is Û . Note that here ½ is the indicator function, with ½ ØÖÙ ℄ = 1 and ½ Ð× ℄ = 0. Using this model, the problem of PCC under must-link and cannot-link constraints is formulated as minimizing the following objective function, where point Ü is assigned to the partition Ð with centroid Ð :</p><formula xml:id="formula_0">Â pckm ½ ¾ Ü ¾ Ü Ð ¾ (2.1) • ´Ü Ü µ¾Å Û ½ Ð Ð ℄ • ´Ü Ü µ¾ Û ½ Ð Ð ℄</formula><p>Minimizing this objective function can be shown to be equivalent to maximizing the posterior configuration probability of a Hidden Markov Random Field, details of which are given in Appendix A.1. The mathematical formulation of this framework was motivated by the metric labeling problem and the generalized Potts model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>, for which Kleinberg et al. <ref type="bibr" target="#b22">[23]</ref> proposed an approximation algorithm. Their formulation only considers the set Å of must-link constraints, which we extended to the PCC framework by adding the set of cannot-link constraints. Our proposed pairwise constrained KMeans (PCKMeans) algorithm greedily optimizes Â pckm using a KMeans-type iter- ation with a modified cluster-assignment step. For experiments with text documents, we used a variant of KMeans called spherical KMeans (SPKMeans) <ref type="bibr" target="#b10">[11]</ref> that has computational advantages for sparse high dimensional text data vectors. We will present our algorithm and its motivation based on KMeans in Section 3, but all of it can be easily extended for SPKMeans. In the domains that we will be considering, e.g., text clustering, different costs for different pairwise constraints are not available in general, so for simplicity we will be assuming all elements of Ï and Ï to have the same constant value Û in (2.1). We will make a detailed study of the effect of the choice of Û in Section 5.</p><p>Note that KMeans has a running time of Ç´Ò µ, where Ò is the number of data points, is the number of dimensions and is the number of clusters. SPKMeans has a running time of Ç´Ð µ, where Ð is the number non-zero entries in the Ò ¢ input data matrix. So they are both linear in the size of the input, making our PCKMeans algorithm for the PCC framework quite efficient. PCKMeans can also handle sparse high-dimensional data (e.g. text, gene micro-array), since it has the computational advantage of SPKMeans in these domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Clustering Algorithm</head><p>Given a set of data points , a set of must-link constraints Å, a set of cannot-link constraints , the weight of the constraints Û and the number of clusters to form , we propose an algorithm PCKMeans that finds a disjoint partitioning ½ of (with each partition having a centroid ) such that Â pckm is (locally) minimized.</p><p>In our previous work <ref type="bibr" target="#b3">[4]</ref>, we had observed that proper initialization of centroid-based algorithms like KMeans using the provided semi-supervision in the form of labeled points greatly improves clustering performance. Here, instead of labeled points, we are given supervision in the form of constraints on pairs of points -in this case also, our goal in the initialization step will be to get good estimates of the cluster centroids from the pairwise constraints.</p><p>In the initialization step of PCKMeans, we take the transitive closure of the must-link constraints <ref type="bibr" target="#b32">[33]</ref> and augment the set Å by adding these entailed constraints. 1 Note that our current model considers consistent (non-noisy) constraints, but it can also be extended to handle inconsistent (noisy) constraints, as discussed in Section 7. Let the number of connected components in the augmented set Å be , which are used to create neighborhood sets AE Ô Ô ½ . 1 A note on complexity: the transitive closure and constraint augmentation takes Ç´ Å • µ operations.</p><p>For every pair of neighborhoods AE Ô and AE Ô ¼ that have at least one cannot-link between them, we add cannot-link constraints between every pair of points in AE Ô and AE Ô ¼ and augment the cannot-link set by these entailed constraints, again assuming consistency of constraints. From now on, we will refer to the augmented must-link and cannot-link sets as Å and respectively.</p><p>Note that the neighborhood sets AE Ô , which contain the neighborhood information inferred from the must-link constraints and are unchanged during the iterations of the algorithm, are different from the partition sets , which contain the cluster partitioning information and get updated at each iteration of the algorithm.</p><p>After this preprocessing step we get neighborhood sets AE Ô Ô ½ , which are used to initialize the cluster centroids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If</head><p>, where is the required number of clusters, we select the neighborhood sets of largest size and initialize the cluster centers with the centroids of these sets. If , we initialize cluster centers with the centroids of the neighborhood sets. We then look for a point Ü that is connected by cannot-links to every neighborhood set. If such a point exists, it is used to initialize the ´ • ½µ Ø cluster.</p><p>If there are any more cluster centroids left uninitialized, we initialize them by random perturbations of the global centroid of <ref type="bibr" target="#b13">[14]</ref>. The algorithm PCKMeans alternates between the cluster assignment and centroid estimation steps (see Figure <ref type="figure" target="#fig_0">1</ref>). In the cluster assignment step of PCKMeans, every point Ü is assigned to a cluster such that it minimizes the sum of the distance of Ü to the cluster centroid and the cost of constraint violations incurred by that cluster assignment (by equivalently satisfying as many must-links and cannot-links as possible by the assignment). Note that the cluster assignment step is order-dependent, since the subsets of Å and associated with each cluster may change with the assignment of a point. For our experiments, we consider a random ordering of the points in this assignment step. The centroid re-estimation step is the same as KMeans, i.e., each cluster centroid is calculated by taking the mean of the points assigned to that cluster. </p><formula xml:id="formula_1">Ð ÓÖ Ø Ñ È ÃÅ Ò× ÁÒÔÙØ Ë Ø Ó Ø ÔÓ ÒØ× Ü Ò ½ × Ø Ó ÑÙ×Ø¹Ð Ò ÓÒ×ØÖ ÒØ× Å ´Ü Ü µ × Ø Ó ÒÒÓØ¹Ð Ò ÓÒ×ØÖ ÒØ× ´Ü Ü µ ÒÙÑ Ö Ó ÐÙ×Ø Ö× ¸Û Ø Ó ÓÒ×ØÖ ÒØ× Ûº ÇÙØÔÙØ × Ó ÒØ Ô ÖØ Ø ÓÒ Ò ½ Ó ×Ù Ø Ø Ó Ø Ú ÙÒØ ÓÒ Â Ô Ñ × ´ÐÓ ÐÐÝµ Ñ Ò Ñ Þ º Å Ø Ó ½º ÁÒ Ø Ð Þ ÐÙ×Ø Ö× ½ º Ö Ø Ø Ò ÓÖ ÓÓ × AE Ô Ô ½ ÖÓÑ Å Ò ½ º ×ÓÖØ Ø Ò × Ô Ò Ö × Ò × Þ Ó AE Ô ½º Ò Ø Ð Þ ´¼µ ½ Û Ø ÒØÖÓ × Ó AE Ô Ô ½ Ð× Ò Ø Ð Þ ´¼µ ½ Û Ø ÒØÖÓ × Ó AE Ô Ô ½ ÔÓ ÒØ Ü ÒÒÓØ¹Ð Ò ØÓ ÐÐ Ò ÓÖ ÓÓ × AE Ô Ô ½ Ò Ø Ð Þ ´¼µ •½ Û Ø Ü Ò Ø Ð Þ Ö Ñ Ò Ò ÐÙ×Ø Ö× Ø Ö Ò ÓÑ ¾º Ê Ô Ø ÙÒØ Ð ÓÒÚ Ö Ò ¾ º ×× Ò ÐÙ×Ø Ö ×× Ò Ø ÔÓ ÒØ Ü ØÓ Ø ÐÙ×Ø Ö £ ´ º º × Ø ´Ø•½µ £ µ¸ ÓÖ £ Ö Ñ Ò ´½ ¾ Ü ´Øµ ¾ •Û È ´Ü Ü µ¾Å ½ Ð ℄ • Û È ´Ü Ü µ¾ ½ Ð ℄µ ¾ º ×Ø Ñ Ø Ñ Ò× ´Ø•½µ ½ ½ ´Ø•½µ È Ü¾ ´Ø•½µ Ü ½ ¾º Ø ´Ø • ½µ</formula><formula xml:id="formula_2">½ ¾ Ü ¾ • Û È ´Ü Ü µ¾Å ½ Ð ℄ • Û È ´Ü Ü µ¾ ½ Ð ℄) of Â pckm ,</formula><p>contributed by the point Ü, decreases. So when all points are given their new assignment, Â pckm will decrease or remain the same.</p><p>For analyzing the centroid re-estimation step, let us consider an equivalent form of (2.1):</p><formula xml:id="formula_3">Â pckm ½ ¾ ½ Ü ¾ Ü ¾ (3.2) • ´Ü Ü µ¾Å Û ½ Ð Ð ℄ • ´Ü Ü µ¾ Û ½ Ð Ð ℄</formula><p>Each cluster centroid is re-estimated by taking the mean of the points in the partition , which minimizes the component (</p><formula xml:id="formula_4">½ ¾ È Ü ¾ Ü ¾ ) of Â pckm in (3.2)</formula><p>contributed by the partition . The pairwise constraints do not take in part in this step because the constraints are not an explicit function of the centroid. As a result only the first term (</p><formula xml:id="formula_5">½ ¾ È ½ È Ü ¾ Ü ¾ ) of Â pckm in (3.2) is minimized in this step.</formula><p>Hence the objective function decreases after every cluster assignment and centroid re-estimation step till convergence, implying that the PCKMeans algorithm will converge to a local minimum of Â pckm .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Active Learning Algorithm</head><p>In the semi-supervised setting, getting labels on data point pairs may be expensive. In this section, we discuss an active learning scheme in the PCC setting in order to improve clustering performance with as few queries as possible. Formally, the scheme has access to a noiseless oracle that can assign a must-link or cannot-link label on a given pair ´Ü Ü µ, and it can pose a constant number of queries to the oracle. <ref type="foot" target="#foot_0">2</ref>In order to get pairwise constraints that are more informative than random in the PCC model, we have developed an active learning scheme for selecting pairwise constraints using the farthest-first traversal scheme. The basic idea of farthest-first traversal of a set of points is to find points such that they are far from each other. In farthest-first traversal, we first select a starting point at random, choose the next point to be farthest from it and add it to the traversed set, then pick the following point farthest from the traversed set (using the standard notion of distance from a set:</p><p>´Ü Ëµ Ñ Ò Ý¾Ë ´Ü Ýµ), and so on. Farthest-first traversal gives an efficient approximation of the -ÒØ Ö problem <ref type="bibr" target="#b17">[18]</ref>, and has also been used to construct hierarchical clusterings with performance guarantees at each level of the hierarchy <ref type="bibr" target="#b9">[10]</ref>. For our data model (see Appendix A.2), we prove another interesting property of farthest-first traversal (see Appendix A.4) that justifies its use for active learning.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, it was observed that initializing KMeans with centroids estimated from a set of labeled examples for each cluster gives significant performance improvements. Under certain generative model-based assumptions, one can connect the mixture of Gaussians model to the KMeans model <ref type="bibr" target="#b20">[21]</ref>. A direct calculation using Chernoff bounds shows that if a particular cluster (with an underlying Gaussian model) with true centroid is seeded with Ñ points (drawn independently at random from the corresponding Gaussian distribution) and the estimated centroid is , then</p><formula xml:id="formula_6">(4.3) ÈÖ´ AEµ AE ¾ Ñ ¾</formula><p>where AE ¾ Ê•. Thus, the deviation of the centroid estimates falls exponentially with the number of seeds, and hence seeding results in good initial centroids. Since good initial centroids are very critical for the success of greedy algorithms such as KMeans, we follow the same principle for the pairwise case: we will try to get as many points (proportional to the actual cluster size) as possible per cluster by asking pairwise queries, so that PCKMeans is initialized from a very good set of centroids.</p><p>The proposed active learning scheme has two phases. The first phase explores the given data to get pairwise disjoint non-null neighborhoods, each belonging to a different cluster in the underlying clustering of the data, as fast as possible. Note that even if there is only one point per neighborhood, this neighborhood structure defines a correct skeleton of the underlying cluster. For this phase, we propose an algorithm Explore that essentially uses the farthest-first scheme to form appropriate queries for getting the required pairwise disjoint neighborhoods.</p><p>At the end of Explore, at least one point has been obtained per cluster. The remaining queries are used to consolidate this structure. The cluster skeleton obtained from Explore is used to initialize pairwise disjoint nonnull neighborhoods AE Ô Ô ½ . Then, given any point Ü not in any of the existing neighborhoods, we will have to ask at most ´ ½µ queries by pairing Ü up with a member from each of the disjoint neighborhoods AE Ô to find out the neighborhood to which Ü belongs. This principle forms the second phase of our active learning algorithm, and we call the algorithm for this phase Consolidate. In this phase, we are able to get the correct cluster label of Ü by asking at most ´ ½µ queries. So, ´ ½µ pairwise labels are equivalent to a single pointwise label in the worst case. Now, we present the details of the algorithms for performing the exploration and the consolidation.</p><formula xml:id="formula_7">Ð ÓÖ Ø Ñ ÜÔÐÓÖ ÁÒÔÙØ Ë Ø Ó Ø ÔÓ ÒØ× Ü Ò ½ ¸ ×× ØÓ Ò ÓÖ Ð Ø Ø Ò×Û Ö× Ô ÖÛ × ÕÙ Ö ×¸ÒÙÑ Ö Ó ÐÙ×Ø Ö× ¸ØÓØ Ð ÒÙÑ Ö Ó ÕÙ Ö × Éº ÇÙØÔÙØ × Ó ÒØ Ò ÓÖ ÓÓ × AE Ô Ô ½ ÓÖÖ ×ÔÓÒ Ò ØÓ Ø ØÖÙ ÐÙ×Ø Ö Ò Ó Û Ø Ø Ð ×Ø ÓÒ ÔÓ ÒØ Ô Ö Ò ÓÖ ÓÓ º Å Ø Ó ½º ÁÒ Ø Ð Þ × Ø ÐÐ Ò ÓÖ ÓÓ × AE Ô Ô ½ ØÓ ÒÙÐÐ ¾º È Ø ¬Ö×Ø ÔÓ ÒØ Ü Ø Ö Ò ÓÑ¸ ØÓ AE ½ ¸ ½ ¿º Ï Ð ÕÙ Ö × Ö ÐÐÓÛ Ò Ü ÔÓ ÒØ ÖØ ×Ø ÖÓÑ Ü ×Ø Ò Ò ÓÖ ÓÓ × AE Ô Ô ½ ¸ Ý ÕÙ ÖÝ Ò ¸Ü × ÒÒÓØ¹Ð Ò ØÓ ÐÐ Ü ×Ø Ò Ò ÓÖ ÓÓ × • ½¸×Ø ÖØ Ò Û Ò ÓÖ ÓÓ AE Û Ø Ü Ð× Ü ØÓ Ø Ò ÓÖ ÓÓ Û Ø Û Ø × ÑÙ×Ø¹Ð Ò Figure 2: Algorithm Explore</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Explore</head><p>In the exploration phase, we use a very interesting property of the farthest-first traversal. Given a set of disjoint balls of unequal size in a metric space, we show that the farthest-first scheme is sure to get one point from each of the balls in a reasonably small number of attempts (see Appendix A.4). Hence, our algorithm Explore (see Figure <ref type="figure">2</ref>) uses farthest-first traversal for getting a skeleton structure of the neighborhoods. In Explore, while queries are still allowed and pairwise disjoint neighborhoods have not yet been found, the point Ü farthest from all the existing neighborhoods is chosen as a candidate for starting a new neighborhood. Queries are posed by pairing Ü with a random point from each of the existing neighborhoods. If Ü is cannot-linked to all the existing neighborhoods, a new neighborhood is started with Ü. If a must-link is obtained for a particular neighborhood, Ü is added to that neighborhood. This continues till the algorithm runs out of queries or pairwise disjoint neighborhoods have been found. In the latter case, the active learning scheme enters the consolidation phase.</p><formula xml:id="formula_8">Ð ÓÖ Ø Ñ ÓÒ×ÓÐ Ø ÁÒÔÙØ Ë Ø Ó Ø ÔÓ ÒØ× Ü Ò ½ ¸ ×× ØÓ Ò ÓÖ Ð Ø Ø Ò×Û Ö× Ô ÖÛ × ÕÙ Ö ×¸ÒÙÑ Ö Ó ÐÙ×Ø Ö× ¸ØÓØ Ð ÒÙÑ Ö Ó ÕÙ Ö × É¸ × Ó ÒØ Ò ÓÖ ÓÓ × ÓÖÖ ×ÔÓÒ Ò ØÓ ØÖÙ ÐÙ×Ø Ö Ò Ó Û Ø Ø Ð ×Ø ÓÒ ÔÓ ÒØ Ô Ö Ò ÓÖ ÓÓ º ÇÙØÔÙØ × Ó ÒØ Ò ÓÖ ÓÓ × ÓÖÖ ×ÔÓÒ Ò ØÓ Ø ØÖÙ ÐÙ×Ø Ö Ò Ó Û Ø Ö ÒÙÑ Ö Ó ÔÓ ÒØ× Ô Ö Ò ÓÖ ÓÓ º Å Ø Ó ½º ×Ø Ñ Ø ÒØÖÓ × ½ Ó Ó Ø Ò ÓÖ ÓÓ × ¾º Ï Ð ÕÙ Ö × Ö ÐÐÓÛ ¾ º Ö Ò ÓÑÐÝ Ô ÔÓ ÒØ Ü ÒÓØ Ò Ø Ü ×Ø Ò Ò ÓÖ ÓÓ × ¾ º ×ÓÖØ Ø Ò × Û Ø ÒÖ × Ò ×Ø Ò × Ü ¾ ¾º ÓÖ ½ ØÓ ÕÙ ÖÝ Ü Û Ø Ó Ø Ò ÓÖ ÓÓ × Ò ×ÓÖØ ÓÖ Ö Ø ÐÐ ÑÙ×Ø¹Ð Ò × Ó Ø Ò ¸ Ü ØÓ Ø Ø Ò ÓÖ ÓÓ Figure 3: Algorithm Consolidate</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Consolidation</head><p>The consolidation phase starts when at least one point has been obtained from each of the clusters. The basic idea in the consolidation phase is that since we now have points from all the clusters, the proper neighborhood of any random point Ü can be determined within a maximum of ´ ½µ queries. The queries will be formed by taking a point Ý from each of the neighborhoods in turn and asking for the label on the pair ´Ü Ýµ till a must-link has been obtained. We will either get a must-link reply in</p><p>´ ½µ queries, else if we get cannot-link replies for the</p><p>´ ½µ queries to the ´ ½µ neighborhoods, we can infer that the point is must-linked to the remaining neighborhood. Note that it is practical to sort the neighborhoods in increasing order of the distance of their centroids from Ü so that the correct must-link neighborhood for Ü is encountered sooner in the querying process. The outline of the algorithm Consolidate is given in Figure <ref type="figure">3</ref>.</p><p>Both Explore and Consolidate add points to the clusters at a good rate. It can be shown using the result in Appendix A.4 that the Explore phase gets at least one point from each of the underlying clusters in maximum ¾ ¡ queries. When the active scheme has finished running Explore and is running Consolidate, it can also be shown using a generalization of the coupon collector's problem (Appendix A.4) that with high probability it will get one new point from each cluster in approximately ¾ ÐÓ queries. Consolidate therefore adds points to clusters at a faster rate than Explore by a factor of Ç´ ÐÓ µ, which is validated by our experiments in Section 5. Note that this analysis is for balanced clusters, but a similar analysis with unbalanced clusters gives the same improvement factor.</p><p>Finally, we briefly address the case when the right number of clusters is not known to the clustering algorithm, so that is also unknown to the active learning scheme. In this case, only Explore is used while queries are allowed. Explore will keep discovering new clusters as fast as it can. When it has obtained all the clusters, it will not have any way of knowing this. However, from this point onwards, for every farthest-first Ü it draws from the dataset, it will always find a neighborhood that is must-linked to it. Hence, after discovering all the clusters, Explore will essentially consolidate the clusters too. However, when is known, it makes sense to invoke Consolidate since (1) it adds points to clusters at a faster rate than Explore, and (2) it picks random samples following the underlying data distribution, which have certain nice properties in terms of estimating good centroids (e.g., Chernoff bounds on the centroid estimates, as shown in (4.3)), that the samples obtained using farthest-first traversal need not have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we outline the details of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>In our experiments with high-dimensional text documents, we used datasets created from the 20 Newsgroups collection. <ref type="foot" target="#foot_1">3</ref> It has messages collected from 20 different Usenet newsgroups, 1000 messages from each newsgroup. From the original dataset, a reduced dataset News-all20 was created by taking a random subsample of 100 documents from each of the 20 newsgroups -this subsample is a more difficult dataset to cluster than the original 20 Newsgroups, since each cluster has fewer documents. News-all20 has 2000 points in 16089 dimensions. By selecting 3 categories from the reduced dataset News-all20, two other datasets were created: News-sim3 that consists of 3 newsgroups on similar topics (comp.graphics, comp.os.ms-windows, comp.windows.x) with significant cluster overlap, and News-diff3 that consists of 3 newsgroups on different topics (alt.atheism, rec.sport.baseball, sci.space) with well-separated clusters. News-sim3 has 300 points in 3225 dimensions, while News-diff3 had 300 points in 3251 dimensions. Another dataset we used in our experiments is a subset of Classic3 <ref type="bibr" target="#b10">[11]</ref> containing 400 documents -100 Cranfield documents from aeronautical system papers, 100 Medline documents from medical journals, and 200 Cisi documents from information retrieval papers. This Classic3-subset dataset was specifically designed to create clusters of unequal size, and has 400 points in 2897 dimensions. Similarities between data points in the text datasets were computed using cosine similarity, following SPKMeans <ref type="bibr" target="#b10">[11]</ref>. SPKMeans maximizes the average cosine similarity between points and cluster centroids, so that the objective function monotonically increases with every iteration till convergence. All the text datasets were preprocessed following the methodology of Dhillon et al. <ref type="bibr" target="#b10">[11]</ref>.</p><p>For experiments on low-dimensional data, we selected the UCI dataset Iris <ref type="bibr" target="#b4">[5]</ref>, which has 150 points in 4 dimen-sions. The Euclidean metric was used for computing distances between points in this dataset, following KMeans. In this case, the objective function, which is the average squared Euclidean distance between points and cluster centroids, decreases at every iteration till convergence. The Iris dataset was not pre-processed in any way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Clustering evaluation</head><p>We have used three metrics for cluster evaluation: normalized mutual information (NMI), pairwise F-measure, and objective function.</p><p>Normalized mutual information (NMI) determines the amount of statistical information shared by the random variables representing the cluster assignments and the userlabeled class assignments of the data points. We computed NMI following the methodology of Strehl et al. <ref type="bibr" target="#b30">[31]</ref>. NMI measures how closely the clustering algorithm could reconstruct the underlying label distribution in the data. If is the random variable denoting the cluster assignments of the points, and Ã is the random variable denoting the underlying class labels on the points, then the NMI measure is defined as <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_9">AEÅÁ Á´ Ãµ ´À ´ µ • À´Ãµµ ¾</formula><p>where Á´ µ À´ µ À´ µ is the mutual information between the random variables and , À´ µ is the Shannon entropy of , and À´ µ is the conditional entropy of given .</p><p>Pairwise F-measure is defined as the harmonic mean of pairwise precision and recall, the traditional information retrieval measures adapted for evaluating clustering by considering pairs of points -for every pair of points that do not have explicit constraints between them, the decision to cluster this pair into same or different clusters is considered to be correct if it matches with the underlying class labeling available for the points. Pairwise F-measure is defined as:</p><formula xml:id="formula_10">ÈÖ × ÓÒ È Ö× ÓÖÖ ØÐÝÈÖ Ø ÁÒË Ñ ÐÙ×Ø Ö ÌÓØ ÐÈ Ö×ÈÖ Ø ÁÒË Ñ ÐÙ×Ø Ö Ê ÐÐ È Ö× ÓÖÖ ØÐÝÈÖ Ø ÁÒË Ñ ÐÙ×Ø Ö ÌÓØ ÐÈ Ö× ØÙ ÐÐÝÁÒË Ñ ÐÙ×Ø Ö Ñ ×ÙÖ ¾ ¢ ÈÖ × ÓÒ ¢ Ê ÐÐ ÈÖ × ÓÒ • Ê ÐÐ</formula><p>Measures like modified Rand Index <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> that are frequently used for evaluation of clustering in the PCC framework are very similar to pairwise F-measure. NMI has also become a popular clustering evaluation metric <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>. We present results using both these evaluation measures and observe from the results that they are strongly correlated.</p><p>We also show results for the objective function Â pckm .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Methodology</head><p>For all the algorithms, on each dataset, we have generated learning curves with 10fold cross-validation, where the x-axis represents the number of pairwise constraints given as input to the algorithms. For non-active PCKMeans the pairwise constraints are selected at random, while for active PCKMeans the pairwise constraints are selected using our active learning scheme. For studying the effect of pairwise constraints and active learning, 10% of the dataset is set aside as the test set at any particular fold. The training sets at different points of the learning curve are pairwise constraints obtained from the remaining 90% of the data, with increasing number of pairwise constraints being given as input to the clustering along the learning curve. The clustering algorithm is run on the whole dataset, and the corresponding objective function is reported. Following the methodology of Basu et al. <ref type="bibr" target="#b3">[4]</ref>, NMI and pairwise F-measure are calculated only on the test set, from which no constraints were supplied. The results at each point on the learning curve are obtained by averaging over 10 folds. We did not continue the learning curve beyond 1000 queries (5000 for News-all20), since the general nature of the results was evident in this range. Moreover, in practical active learning applications, it is unrealistic to expect the user to answer even 1000 queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>The results of the experiments are shown in Figures <ref type="figure" target="#fig_2">4</ref><ref type="figure" target="#fig_3">5</ref><ref type="figure" target="#fig_5">6</ref><ref type="figure">7</ref><ref type="figure">8</ref><ref type="figure">9</ref><ref type="figure" target="#fig_1">10</ref><ref type="figure" target="#fig_1">11</ref>. Since the standard deviations of NMI, pairwise F-measure and objective function values in the plots were small for all the datasets, they have not been shown in the plots to reduce clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of Û:</head><p>We experimented with different values of the constraint weight parameter Û. If Û is set to 0, the algorithm is initialized with neighborhoods derived from the given constraints and then normal KMeans iterations are run till convergence. This is similar to the SeededKMeans algorithm outlined in <ref type="bibr" target="#b3">[4]</ref>, where the labeled data (seeds) are used to only initialize the KMeans algorithm and are not used in the following steps of the algorithm.</p><p>If Û is set to a very high value, the algorithm is initialized with neighborhoods derived from the given constraints and the constraints become hard constraints, since the constraint cost violation component of the Â pckm objective func- tion far supersedes its distance component. This is similar to the ConstrainedKMeans algorithm outlined in <ref type="bibr" target="#b3">[4]</ref>. In this algorithm, the seeds are also used to initialize the KMeans algorithm. However, in the subsequent steps, the cluster labels of the seed data are kept unchanged and only the labels of the non-seed data are re-estimated.</p><p>If Û is set to an intermediate value, the algorithm gives a tradeoff between minimizing the total distance between points and cluster centroids and the cost of violating the constraints. In the result plots in Figures <ref type="figure" target="#fig_2">4</ref><ref type="figure" target="#fig_3">5</ref><ref type="figure" target="#fig_5">6</ref><ref type="figure">7</ref><ref type="figure">8</ref><ref type="figure">9</ref><ref type="figure" target="#fig_1">10</ref><ref type="figure" target="#fig_1">11</ref><ref type="figure" target="#fig_8">12</ref>, PCKMeans refers to running the algorithm with the intermediate value of Û. The parameter Û can be chosen by the user according to the degree of confidence in the constraints, or chosen to be a constant of the same order as the average similarity (for Spherical KMeans) or distance (for Euclidean KMeans) between pairs of points in the dataset. We set Û to be 0.001 for the text-datasets and 1 for Iris dataset.</p><p>Thus, the Û parameter acts as a tuning knob, giving us the continuum between a SeededKMeans-like algorithm on one extreme, where there is no guarantee of the constraint satisfaction in the clustering, and a ConstrainedKMeans-like algorithm on the other extreme, where the clustering process is forced to respect all the given constraints. Note that we can selectively guarantee that any particular constraint is satisfied throughout the clustering iterations, by selecting a very high corresponding cost of constraint violation.</p><p>The comparative results of active and non-active algo-     <ref type="figure" target="#fig_10">4</ref> and<ref type="figure" target="#fig_3">5</ref>). This leads us to conclude that proper initialization (by active learning) using the constraints gives much more benefit than satisfying the constraints during the algorithm, which validates the observation in <ref type="bibr" target="#b3">[4]</ref> that proper initialization is crucial for good performance of centroid-based clustering algorithms. This point is explained in more detail in the discussion below. In Figures <ref type="figure" target="#fig_5">6</ref><ref type="figure">7</ref><ref type="figure">8</ref><ref type="figure">9</ref><ref type="figure" target="#fig_1">10</ref><ref type="figure" target="#fig_1">11</ref>, we only present the results for the intermediate value of Û for clarity of the plots. The curves for NMI and Pairwise F-measure were very similar for the datasets we considered (see Figures <ref type="figure" target="#fig_10">4</ref> and<ref type="figure" target="#fig_3">5</ref>), so we only presenting the NMI results.</p><p>Objective function results: We show a representative objective function plot for a text dataset clustered using SPKMeans (Figure <ref type="figure">7</ref>), for which the objective function increases along the learning curve. For Figure <ref type="figure" target="#fig_1">11</ref>, the objective function is decreasing along the learning curve since simple KMeans with Euclidean distance was used for this dataset.</p><p>Note that for each objective function plot, the active and non-active schemes have the same number of constraints in the Å and sets at any point on the learning curve, but the actual constraints they have may be different. The active and the non-active schemes are allowed to both choose their own sets of constraints, and the objective function value after running PCKMeans clustering depends on this choice. For active PCKMeans, the constraints it chooses give it a better initialization (which is discussed in detail below), resulting in better value of the objective function after running the clustering algorithm.</p><p>Non-active schemes: As shown in Appendix A.3, if the number of random pairwise constraints is low, the probability that the largest neighborhoods are in fact from different clusters is very low. Until this point on the learning curve, some of the neighborhoods used to initialize PCKMeans can actually belong to the same cluster, so that we may not get representatives from all the clusters. This gives a poor initialization of PCKMeans that may cause the algorithm to converge to bad local minima. Consequently the clustering produced by PCKMeans can be unstable, resulting in varying pairwise F-measure and NMI values on the test set. This initial jitter can be observed in all the Figures <ref type="figure" target="#fig_2">4</ref><ref type="figure" target="#fig_3">5</ref><ref type="figure" target="#fig_5">6</ref><ref type="figure">7</ref><ref type="figure">8</ref><ref type="figure">9</ref><ref type="figure" target="#fig_1">10</ref><ref type="figure" target="#fig_1">11</ref>. Beyond this point on the learning curve, non-active PCKMeans will most likely be initialized with points from each cluster. So after the initial jitter, the performance of non-active PCKMeans improves steadily along the learning curve with respect to objective function, NMI and pairwise F-measure, showing the benefit of incorporating supervised data (constraints) in the clustering process.</p><p>Active schemes: For the active algorithms, we consistently get significant improvements over the non-active algorithms, for all datasets we have considered. Firstly, we see the jitter only in the very early part of the learning curve. This is because the Explore phase creates only one neighborhood from each cluster and continues until pairwise disjoint neighborhoods are found, creating all the neighborhoods within a small number of queries (see Appendix A.4). The jitter is so early in the learning curve that it cannot be even observed in the plots. In Figure <ref type="figure">8</ref>, the jitter disappears after about the first 20 queries. The Explore phase of the active selection algorithm guarantees that the pairwise disjoint neighborhoods inferred from the constraints belong to different clusters in the actual underlying clustering, and so these neighborhoods would give us good initializations for the clustering algorithm. The Consolidate phase grows the pairwise disjoint neighborhoods already created, so that when the active learning scheme runs out of queries, PCKMeans is initialized using centroids constructed from good neighborhoods. The improvement of the active scheme is more pronounced for the difficult high-dimensional text datasets, e.g., Figure <ref type="figure" target="#fig_2">4</ref><ref type="figure" target="#fig_3">5</ref><ref type="figure" target="#fig_5">6</ref><ref type="figure">7</ref><ref type="figure">8</ref><ref type="figure">9</ref>.</p><p>From the above results, we conclude:</p><p>¯Semi-supervised clustering with constraints performs considerably better than unsupervised clustering for the datasets we have considered (note that unsupervised clustering corresponds to semi-supervised clustering with 0 constraints). For both the active and non-active algorithms, the clustering evaluation measures (NMI and pairwise F-measure) and the objective function improve with increasing number of pairwise constraints provided along the learning curve.</p><p>¯Active selection of pairwise constraints, using our twophase active learning algorithm, significantly outperforms random selection of constraints. Explore Vs Consolidate: We also ran some ablation experiments, comparing the performance of the active PCKMeans scheme with both Explore and Consolidate with active PCKMeans with Explore only. We ran the ablation experiment on the News-diff3 dataset. From the NMI result shown in Figure <ref type="figure" target="#fig_8">12</ref>, we can see that running Explore only in the active learning phase gives improvement over random choice of constraints, but running both Explore and Consolidate gives even better results. So, both Explore and Consolidate are useful phases of the active learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>COP-KMeans is another algorithm in the pairwise constrained clustering model <ref type="bibr" target="#b32">[33]</ref>, but it does not handle softconstraints, i.e., constraints that can be violated with an associated violation cost, which PCKMeans does. A softconstrained algorithm SCOP-KMeans has been recently proposed <ref type="bibr" target="#b31">[32]</ref>, whose performance would be interesting to compare with PCKMeans. Bansal et al. <ref type="bibr" target="#b2">[3]</ref> proposed a theoretical model where they performed clustering using only pairwise constraints, which is different from our model since we consider both constraints and an underlying metric between the points while clustering. Other work with the pairwise constrained clustering model includes learning distance metrics for clustering from pairwise constraints <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34]</ref>. In this domain, Cohn. et al. <ref type="bibr" target="#b7">[8]</ref> have proposed iterative userfeedback to acquire constraints, but it was not an active learning algorithm.</p><p>Active learning in the classification framework is a longstudied problem, where different principles of query selection have been studied, e.g., reduction of the version space size <ref type="bibr" target="#b15">[16]</ref>, reduction of uncertainty in predicted label <ref type="bibr" target="#b23">[24]</ref>, maximizing the margin on training data <ref type="bibr" target="#b0">[1]</ref>, finding high variance data points by density-weighted pool-based sampling <ref type="bibr" target="#b26">[27]</ref>, etc. However, active learning techniques in classification are not applicable in the clustering framework, since the basic underlying concept of reduction of classification error and variance over the distribution of examples <ref type="bibr" target="#b8">[9]</ref> is not well-defined for clustering. In the unsupervised setting, Hofmann et al. <ref type="bibr" target="#b18">[19]</ref> consider a model of active learning which is different from ours -they have incomplete pairwise similarities between points, and their active learning goal is to select new data, using expected value of information estimated from the existing data, such that the risk of making wrong estimates about the true underlying clustering from the existing incomplete data is minimized. In contrast, our model assumes that we have complete similarity information between all pairs of points, along with pairwise constraints whose violation cost is a component of the objective function (2.1), and the active learning goal is to select pairwise constraints which are most informative about the underlying clustering. Klein et al. <ref type="bibr" target="#b21">[22]</ref> also consider active learning in semi-supervised clustering, but instead of making examplelevel queries they make cluster level queries, i.e., they ask the user whether or not two whole clusters should be merged. Answering example-level queries rather than cluster-level queries is a much easier task for a user, making our model more practical in a real-world active learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we have presented a pairwise constrained clustering framework and a new theoretically well-motivated method for actively selecting good pairwise constraints for semi-supervised clustering. Experiments on text and UCI data show that (1) PCKMeans with constraints performs considerably better than unconstrained KMeans, and (2) our active learning scheme performs quite well, giving significantly steeper learning curves compared to random pairwise queries. The active learning and pairwise constrained clustering algorithms are both linear and hence suitable for realworld clustering applications, as they can be easily scaled to large datasets and can handle very high-dimensional data.</p><p>The Explore stage of the active learning scheme is currently sensitive to outliers in the data. Note however that it is as sensitive to outliers as the baseline algorithm KMeans. Outlier sensitivity can be handled by densityweighted point selection in Explore, where we could have a modified farthest-first traversal that selects distant points from dense regions of the data space <ref type="bibr" target="#b26">[27]</ref>. Such a formulation of active learning would be more robust to outliers, and can be used with more outlier-robust clustering algorithms, e.g., KMedian <ref type="bibr" target="#b27">[28]</ref>.</p><p>Our current clustering model assumes that the constraints are consistent, i.e., there is no noise in the constraints. We are working on incorporating a noise model into our PCC framework, so that we will be able to handle noisy constraints. This would involve some changes to the algorithms, e.g., not adding the inferred constraints between neighborhoods in the initialization step of PCKMeans, selectively rejecting points using a noise model in the Explore stage of the active learning algorithm, etc. The clustering model also assumes that the right number of clusters is given as an input -in the future, we want to select the number of clusters automatically, by incorporating a model selection criterion into the PCC objective function.</p><p>The cluster assignment step in the PCKMeans algorithm is an incremental step, dependent on the order in which the points are assigned to the cluster. Currently, we used a random ordering for cluster assignment for our experiments. We plan to investigate other cluster assignment schemes in future, and look into an online version of PCKMeans.</p><p>On the theoretical side, we want to explore the correlation between NMI and F-measure that we have empirically observed in our experiments, and come up with better guarantees for improvement of PCC with actively selected con-straints over PCC with randomly selected constraints.</p><p>Finally, we also want to apply our active learning algorithm in the PCC framework to other domains, especially gene micro-array data analysis in bioinformatics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>This research was supported in part by IBM PhD Fellowship, by Intel and by NSF grant IIS-0117308.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Hidden Markov Random Field The PCKMeans objective function in (2.1) tries to minimize the total distance between points and their cluster centroids such that the minimum number of specified constraints between the points are violated. This mathematical formulation can be motivated by considering a Markov Random Field (MRF) <ref type="bibr" target="#b6">[7]</ref>  </p><formula xml:id="formula_11">Ð is È ´Ðµ » ÜÔ´ È È Î ´ µ ´Ð Ð µµ, where Î ´ µ ´Ð Ð µ Û ½ Ð Ð ℄ if ´Ü Ü µ ¾ Å Û ½ Ð Ð ℄ if ´Ü Ü µ ¾ ¼ otherwise</formula><p>We assume an identity covariance Gaussian noise model for the observed data (von-Mises Fisher distribution <ref type="bibr" target="#b25">[26]</ref> was considered as the noise model for high-dimensional text data) <ref type="foot" target="#foot_2">4</ref> , and also assume that the observed (noisy) data points have been drawn independently of each other following this model. If</p><p>½ denote the true representatives corresponding to the labels ½ , the conditional probability of the observation for a given configuration Ð is</p><formula xml:id="formula_12">È ´ Ðµ » ÜÔ´ ½ ¾ È Ü Ü Ð ¾ µ.</formula><p>Since the MRF is defined over the hidden true labels Ð Ò ½ of the observed points Ü Ò ½ , this model is called a Hidden Markov Random Field (HMRF) <ref type="bibr" target="#b34">[35]</ref>, which is a direct generalization of a Hidden Markov Model.</p><p>Since the posterior probability of a configuration Ð is È ´Ð µ » È ´ÐµÈ ´ Ðµ, the PCC objective function is proportional to the negative logarithm of the posterior probability of the specified HMRF. Hence, finding the MAP configuration of the HMRF becomes equivalent to minimizing the objective function in (2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Model Assumptions</head><p>First of all, we present the formal model of the dataset based on which all analysis will be done. The data is assumed to be coming from disjoint uniform density balls of unequal size in a metric space.</p><p>The balls are defined in terms of the metric. All data points inside any particular ball are assumed to be in the same cluster, and points from different balls are assumed to be from different clusters. The oracle is assumed to know this model. Let Ò be the total number of points under consideration. Let </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Analysis of random initialization</head><p>In PCKMeans, initialization is done using the largest sized neighborhoods. We argue that within a small number of queries, the probability of getting even a 3-point neighborhood from any cluster is very low. Given É pairs at random, on average there will be one must-link in every ´½ • «µ pairs. Hence, there will be a total of É ´½ • «µ must-link pairs in the expected behavior. Then, for the -th cluster, there will be Ö É ´½ • «µ Ñ must-link pairs on average. We focus on a particular cluster on which Ö pairs have been selected at random. The size of the cluster is Ñ Ò .</p><p>We will not get a 3-point neighborhood from if none of the points Ü ¾ gets drawn more than once in the random , then, by an extension of the coupon collector's problem <ref type="bibr" target="#b28">[29]</ref>, one can show that points of all colors will be drawn with high probability within Ð ÐÒ • Ç´Ðµ draws. We claim that the farthest first scheme gets points of all colors within Ð attempts with probability 1.</p><p>In the worst case, if the disjoint balls are placed by an adversary, the adversary will try to place the balls such that getting a point from at least one ball is very difficult. One can show that the optimum strategy for the adversary will be to make the smallest ball the most difficult to reach. Using a packing argument, we show that irrespective of the placement of the balls, the farthest first traversal cannot avoid any particular ball for long. Consider two balls with probabilities . Let Ö Ö be the radii of the two balls, and Î Î be the volumes of the two balls. Further, let ´ µ denote the packing number of with ballsthe maximum number of disjoint balls that can be packed inside the ball . Now, if there are just these two balls in the universe and if farthest-first traversal starts in , the points obtained from before entering must have pairwise distances (between their centers) of at least ¾Ö , because otherwise the traversal would have picked the farthest point from and got a distance of at least ¾Ö . Hence, the traversal cannot stay in for more that ´ µ farthest-first jumps because there are exactly these many points inside that can be at a distance of at least ¾Ö from each other. Now, the packing number ´ µ Î Î , the ratio of their probabilities. This argument can be extended to the general case of balls. In the general case, the number of times the farthest first traversal can continue without entering the ball is</p><formula xml:id="formula_13">Ò ½ ´ µ ½</formula><p>Clearly, this number is largest for the smallest ball ½ . So, the maximum number of farthest-first jumps before reaching ½ is given by</p><formula xml:id="formula_14">Ò ½ ¾ ½ ´½ ½ µ ½ ½ ½ ½ Ð ½</formula><p>In the next jump, farthest-first gets a point from ½ . Hence, the farthest first traversal will find points of all the colors in ´Ð ½µ • ½ Ð attempts. Note that this is a significant ÐÒ factor improvement over the random scheme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PCKMeans algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>LEMMA 1 .</head><label>1</label><figDesc>The algorithm PCKMeans converges to a local minimum of Â Ô Ñ . Proof. For analyzing the cluster assignment step, let us consider (2.1). Each point Ü moves to a new cluster only if the component (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of NMI values on News-sim3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of pairwise F-measure values on News-sim3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of NMI values on News-diff3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 Figure 8 :</head><label>78</label><figDesc>Figure 7: Comparison of objective function on News-diff3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Figure 9: Comparison of NMI values on Classic400</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Comparison of Explore and Consolidate phases w.r.t. NMI on News-diff3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>½</head><label></label><figDesc>be the probabilities of drawing a point randomly from the -th ball . Without loss of generality, we assume ½ the number of possible cannot-links is È Ð Ð Ñ Ñ Ð and the number of must-links is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>A. 4</head><label>4</label><figDesc>pair sampling. If the sampling of Ö pairs is replaced by the sampling of ¾Ö vertices, the probability of getting a vertex twice is increased. Hence, the probability Ô of not getting a 3-point neighborhood is lower bounded by the probability of not getting a vertex twice in the vertex sampling setting. So, • «µ ¾ which is close to 1 for small values of É. Hence, the probability of getting 3-point neighborhoods is very low. Therefore, the initialization is essentially done by random draws from a set of approximately É ´½ • «µ 2-point neighborhoods. In this setting, the probability of getting exactly one neighborhood from each cluster is µµ using the AM-GM inequality and the Stirling's formula. Clearly, the probability is quite low. This results in significant variance in the initializing neighborhoods and explains the initial jitter for the non-active algorithms for low values of É. Analysis of Explore We shall refer to points from the same cluster as having the same color. If the probability of drawing points of different colors is given by ½ Ð</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Restricting the model to MRFs whose clique potentials involve pairwise points, the prior probability of a configuration</figDesc><table><row><cell>defined over ½ of random vari-½ with Ð ¾ Ò . ½ Let a configuration Ð denote the joint event such that the field (or set) ables over can take values Ð Ò Ð Ð Ò</cell></row></table><note><p>½</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The oracle can also give a don't-know response to a query, in which case that response is ignored (pair not considered as a constraint) and that query is not posed again later.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://www.ai.mit.edu/people/jrennie/20Newsgroups</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The framework can be shown to hold for arbitrary exponential noise models.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Query learning strategies using boosting and bagging</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mamitsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 15th Intl. Conf. on Machine Learning (ICML-98)</title>
		<meeting>of 15th Intl. Conf. on Machine Learning (ICML-98)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative model-based clustering of directional data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 9th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD-2003)</title>
		<meeting>of 9th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlation clustering</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchi</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. on Foundations of Comp. Sci</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised clustering by seeding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 19th Intl. Conf. on Machine Learning (ICML-2002)</title>
		<meeting>of 19th Intl. Conf. on Machine Learning (ICML-2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/˜mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Annual Conf. on Computational Learning Theory</title>
		<meeting>of the 11th Annual Conf. on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Markov random fields with efficient approximations</title>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition Conf</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semisupervised clustering with user feedback</title>
		<author>
			<persName><forename type="first">David</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>TR2003-1892</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Active learning with statistical models</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Performance guarantees for hierarchical clustering</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computational Learning Theory</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="351" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Concept decompositions for large sparse text data using clustering</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="143" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An information-theoretic external clustervalidity measure</title>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">E</forename><surname>Dom</surname></persName>
		</author>
		<idno>RJ 10219</idno>
	</analytic>
	<monogr>
		<title level="j">IBM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Initialization of iterative refinement clustering algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Usama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cory</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 4th Intl. Conf. on Knowledge Discovery and Data Mining (KDD-98)</title>
		<meeting>of 4th Intl. Conf. on Knowledge Discovery and Data Mining (KDD-98)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="194" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random projection for high dimensional data clustering: A cluster ensemble approach</title>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 20th Intl. Conf. on Machine Learning (ICML-2003)</title>
		<meeting>of 20th Intl. Conf. on Machine Learning (ICML-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selective sampling using the query by committee algorithm</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Sebastian</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="133" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning distance functions using equivalence relations</title>
		<author>
			<persName><forename type="first">Aharon</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hillel</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shental</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 20th Intl. Conf. on Machine Learning (ICML-2003)</title>
		<meeting>of 20th Intl. Conf. on Machine Learning (ICML-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A best possible heuristic for the -center problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shmoys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="184" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Active data clustering</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 16th Intl. Conf. on Machine Learning (ICML-99)</title>
		<meeting>of 16th Intl. Conf. on Machine Learning (ICML-99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An information-theoretic analysis of hard and soft assignment methods for clustering</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 13th Conf. on Uncertainty in Artificial Intelligence (UAI-97)</title>
		<meeting>of 13th Conf. on Uncertainty in Artificial Intelligence (UAI-97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepandar</forename><forename type="middle">D</forename><surname>Kamvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 19th Intl. Conf. on Machine Learning (ICML-2002)</title>
		<meeting>of 19th Intl. Conf. on Machine Learning (ICML-2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Approximation algorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. on Foundations of Comp. Sci</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 17th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</title>
		<meeting>of 17th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 5th Berkeley Symp. on Mathematical Statistics and Probability</title>
		<meeting>of 5th Berkeley Symp. on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jupp</surname></persName>
		</author>
		<title level="m">Directional Statistics</title>
		<imprint>
			<publisher>John Wiley and Sons Ltd</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Employing EM and pool-based active learning for text classification</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 15th Intl. Conf. on Machine Learning (ICML-98)</title>
		<meeting>of 15th Intl. Conf. on Machine Learning (ICML-98)<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mirchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<title level="m">Discrete Location Theory</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Randomized Algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using EM</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Impact of similarity measures on web-page clustering</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Artificial Intelligence for Web Search (AAAI 2000)</title>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
			<biblScope unit="page" from="58" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Intelligent Clustering with Instance-Level Constraints</title>
		<author>
			<persName><forename type="first">Kiri</forename><surname>Wagstaff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Constrained K-Means clustering with background knowledge</title>
		<author>
			<persName><forename type="first">Kiri</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schroedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 18th Intl. Conf. on Machine Learning (ICML-2001)</title>
		<meeting>of 18th Intl. Conf. on Machine Learning (ICML-2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distance metric learning, with application to clustering with side-information</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Hidden Markov random field model and segmentation of brain MR images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<idno>TR00YZ1</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>John Radcliffe Hospital</publisher>
			<pubPlace>Oxford Center for FMRIB</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
