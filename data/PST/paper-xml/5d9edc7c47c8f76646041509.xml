<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Sentence Matching with Densely-Connected Recurrent and Co-Attentive Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Seonhoon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Search&amp;Clova</orgName>
								<orgName type="institution" key="instit2">Naver Corp</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Inho</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Search&amp;Clova</orgName>
								<orgName type="institution" key="instit2">Naver Corp</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
							<email>nojunk@snu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Sentence Matching with Densely-Connected Recurrent and Co-Attentive Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentence matching is widely used in various natural language tasks such as natural language inference, paraphrase identification, and question answering. For these tasks, understanding logical and semantic relationship between two sentences is required but it is yet challenging. Although attention mechanism is useful to capture the semantic relationship and to properly align the elements of two sentences, previous methods of attention mechanism simply use a summation operation which does not retain original features enough. Inspired by DenseNet, a densely connected convolutional network, we propose a densely-connected co-attentive recurrent neural network, each layer of which uses concatenated information of attentive features as well as hidden features of all the preceding recurrent layers. It enables preserving the original and the co-attentive feature information from the bottommost word embedding layer to the uppermost recurrent layer. To alleviate the problem of an ever-increasing size of feature vectors due to dense concatenation operations, we also propose to use an autoencoder after dense concatenation. We evaluate our proposed architecture on highly competitive benchmark datasets related to sentence matching. Experimental results show that our architecture, which retains recurrent and attentive features, achieves state-of-the-art performances for most of the tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Semantic sentence matching, a fundamental technology in natural language processing, requires lexical and compositional semantics. In paraphrase identification, sentence matching is utilized to identify whether two sentences have identical meaning or not. In natural language inference also known as recognizing textual entailment, it determines whether a hypothesis sentence can reasonably be inferred from a given premise sentence. In question answering, sentence matching is required to determine the degree of matching 1) between a query and a question for question retrieval, and 2) between a question and an answer for answer selection. However identifying logical and semantic relationship between two sentences is not trivial due to the problem of the semantic gap <ref type="bibr" target="#b14">(Liu et al. 2016)</ref>.</p><p>Recent advances of deep neural network enable to learn textual semantics for sentence matching. Large amount of annotated data such as Quora <ref type="bibr" target="#b6">(Csernai 2017)</ref>, SNLI <ref type="bibr" target="#b1">(Bowman et al. 2015)</ref>, and MultiNLI <ref type="bibr" target="#b31">(Williams, Nangia, and Bowman 2017)</ref> have contributed significantly to learning semantics as well. In the conventional methods, a matching model can be trained in two different ways <ref type="bibr" target="#b8">(Gong, Luo, and Zhang 2018)</ref>. The first methods are sentence-encodingbased ones where each sentence is encoded to a fixed-sized vector in a complete isolated manner and the two vectors for the corresponding sentences are used in predicting the degree of matching. The others are joint methods that allow to utilize interactive features like attentive information between the sentences.</p><p>In the former paradigm, because two sentences have no interaction, they can not utilize interactive information during the encoding procedure. In our work, we adopted a joint method which enables capturing interactive information for performance improvements. Furthermore, we employ a substantially deeper recurrent network for sentence matching like deep neural machine translator (NMT) <ref type="bibr" target="#b31">(Wu et al. 2016)</ref>. Deep recurrent models are more advantageous for learning long sequences and outperform the shallower architectures. However, the attention mechanism is unstable in deeper models with the well-known vanishing gradient problem. Though <ref type="bibr">GNMT (Wu et al. 2016</ref>) uses residual connection between recurrent layers to allow better information and gradient flow, there are some limitations. The recurrent hidden or attentive features are not preserved intact through residual connection because the summation operation may impede the information flow in deep networks.</p><p>Inspired by Densenet <ref type="bibr" target="#b12">(Huang et al. 2017)</ref>, we propose a densely-connected recurrent network where the recurrent hidden features are retained to the uppermost layer. In addition, instead of the conventional summation operation, the concatenation operation is used in combination with the attention mechanism to preserve co-attentive information better. The proposed architecture shown in Figure <ref type="figure" target="#fig_0">1</ref> is called DRCN which is an abbreviation for Densely-connected Recurrent and Co-attentive neural Network. The proposed DRCN can utilize the increased representational power of deeper recurrent networks and attentive information. Furthermore, to alleviate the problem of an ever-increasing feature vector size due to concatenation operations, we adopted an autoencoder and forwarded a fixed length vector to the higher layer recurrent module as shown in the figure <ref type="figure">.</ref> DRCN is, to our best knowledge, the first generalized version of DenseRNN which is expandable to deeper layers with the property of controllable feature sizes by the use of an autoencoder.</p><p>We evaluate our model on three sentence matching tasks: natural language inference, paraphrase identification and answer sentence selection. Experimental results on five highly competitive benchmark datasets (SNLI, MultiNLI, QUORA, TrecQA and SelQA) show that our model significantly outperforms the current state-of-the-art results on most of the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Earlier approaches of sentence matching mainly relied on conventional methods such as syntactic features, transformations or relation extraction <ref type="bibr" target="#b21">(Romano et al. 2006;</ref><ref type="bibr" target="#b30">Wang, Smith, and Mitamura 2007)</ref>. These are restrictive in that they work only on very specific tasks.</p><p>The developments of large-scale annotated datasets <ref type="bibr" target="#b1">(Bowman et al. 2015;</ref><ref type="bibr" target="#b31">Williams, Nangia, and Bowman 2017)</ref> and deep learning algorithms have led a big progress on matching natural language sentences. Furthermore, the wellestablished attention mechanisms endowed richer information for sentence matching by providing alignment and dependency relationship between two sentences. The release of the large-scale datasets also has encouraged the developments of the learning-centered approaches to semantic representation. The first type of these approaches is sentence-encoding-based methods <ref type="bibr" target="#b5">(Conneau et al. 2017;</ref><ref type="bibr" target="#b4">Choi, Yoo, and goo Lee 2017;</ref><ref type="bibr" target="#b17">Nie and Bansal 2017;</ref><ref type="bibr" target="#b24">Shen et al. 2018)</ref> where sentences are encoded into their own sentence representation without any cross-interaction. Then, a classifier such as a neural network is applied to decide the relationship based on these independent sentence represen-tations. These sentence-encoding-based methods are simple to extract sentence representation and are able to be used for transfer learning to other natural language tasks <ref type="bibr" target="#b5">(Conneau et al. 2017</ref>). On the other hand, the joint methods, which make up for the lack of interaction in the former methods, use cross-features as an attention mechanism to express the word-or phrase-level alignments for performance improvements <ref type="bibr" target="#b29">(Wang, Hamza, and Florian 2017;</ref><ref type="bibr" target="#b3">Chen et al. 2017b;</ref><ref type="bibr" target="#b8">Gong, Luo, and Zhang 2018;</ref><ref type="bibr" target="#b32">Yang et al. 2016)</ref>.</p><p>Recently, the architectural developments using deeper layers have led more progress in performance. The residual connection is widely and commonly used to increase the depth of a network stably <ref type="bibr">(He et al. 2016;</ref><ref type="bibr" target="#b31">Wu et al. 2016</ref>). More recently, Huang et al. <ref type="bibr" target="#b12">(Huang et al. 2017)</ref> enable the features to be connected from lower to upper layers using the concatenation operation without any loss of information on lower-layer features.</p><p>External resources are also used for sentence matching. Chen et al. <ref type="bibr" target="#b2">(Chen et al. 2017a;</ref><ref type="bibr" target="#b3">2017b)</ref> used syntactic parse trees or lexical databases like WordNet to measure the semantic relationship among the words and Pavlick et al. <ref type="bibr" target="#b17">(Pavlick et al. 2015)</ref> added interpretable semantics to the paraphrase database. Unlike these, in this paper, we do not use any such external resources. Our work belongs to the joint approaches which uses densely-connected recurrent and co-attentive information to enhance representation power for semantic sentence matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In this section, we describe our sentence matching architecture DRCN which is composed of the following three components: (1) word representation layer, (2) attentively connected RNN and (3) interaction and prediction layer. We denote two input sentences as P = {p 1 , p 2 , • • • , p I } and Q = {q 1 , q 2 , • • • , q J } where p i /q j is the i th /j th word of the sentence P /Q and I/J is the word length of P /Q. The overall architecture of the proposed DRCN is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Representation Layer</head><p>To construct the word representation layer, we concatenate word embedding, character representation and the exact matched flag which was used in <ref type="bibr" target="#b8">(Gong, Luo, and Zhang 2018)</ref>.</p><p>In word embedding, each word is represented as a ddimensional vector by using a pre-trained word embedding method such as GloVe <ref type="bibr" target="#b18">(Pennington, Socher, and Manning 2014)</ref> or Word2vec <ref type="bibr" target="#b16">(Mikolov et al. 2013</ref>). In our model, a word embedding vector can be updated or fixed during training. The strategy whether to make the pre-trained word embedding be trainable or not is heavily task-dependent. Trainable word embeddings capture the characteristics of the training data well but can result in overfitting. On the other hand, fixed (non-trainable) word embeddings lack flexibility on task-specific data, while it can be robust for overfitting, especially for less frequent words. We use both the trainable embedding e tr pi and the fixed (non-trainable) embedding e f ix pi to let them play complementary roles in enhancing the performance of our model. This technique of mixing trainable and non-trainable word embeddings is simple but yet effective.</p><p>The character representation c pi is calculated by feeding randomly initialized character embeddings into a convolutional neural network with the max-pooling operation. The character embeddings and convolutional weights are jointly learned during training.</p><p>Like <ref type="bibr" target="#b8">(Gong, Luo, and Zhang 2018)</ref>, the exact match flag f pi is activated if the same word is found in the other sentence.</p><p>Our final word representational feature p w i for the word p i is composed of four components as follows:</p><formula xml:id="formula_0">e tr pi = E tr (p i ), e f ix pi = E f ix (p i ) c pi = Char-Conv(p i ) p w i = [e tr pi ;e f ix pi ; c pi ; f pi ].<label>(1)</label></formula><p>Here, E tr and E f ix are the trainable and non-trainable (fixed) word embeddings respectively. Char-Conv is the character-level convolutional operation and [• ; •] is the concatenation operator. For each word in both sentences, the same above procedure is used to extract word features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Densely connected Recurrent Networks</head><p>The ordinal stacked RNNs (Recurrent Neural Networks) are composed of multiple RNN layers on top of each other, with the output sequence of previous layer forming the input sequence for the next. More concretely, let H l be the l th RNN layer in a stacked RNN. Note that in our implementation, we employ the bidirectional LSTM (BiLSTM) as a base block of H l . At the time step t, an ordinal stacked RNN is expressed as follows:</p><formula xml:id="formula_1">h l t = H l (x l t , h l t−1 ), x l t = h l−1 t .<label>(2)</label></formula><p>While this architecture enables us to build up higher level representation, deeper networks have difficulties in training due to the exploding or vanishing gradient problem.</p><p>To encourage gradient to flow in the backward pass, residual connection <ref type="bibr">(He et al. 2016</ref>) is introduced which bypasses the non-linear transformations with an identity mapping. Incorporating this into (2), it becomes</p><formula xml:id="formula_2">h l t = H l (x l t , h l t−1 ), x l t = h l−1 t + x l−1 t .<label>(3)</label></formula><p>However, the summation operation in the residual connection may impede the information flow in the network <ref type="bibr" target="#b12">(Huang et al. 2017)</ref>. Motivated by Densenet <ref type="bibr" target="#b12">(Huang et al. 2017)</ref>, we employ direct connections using the concatenation operation from any layer to all the subsequent layers so that the features of previous layers are not to be modified but to be retained as they are as depicted in Figure <ref type="figure" target="#fig_0">1</ref>. The densely connected recurrent neural networks can be described as</p><formula xml:id="formula_3">h l t = H l (x l t , h l t−1 ), x l t = [h l−1 t ; x l−1 t ].<label>(4)</label></formula><p>The concatenation operation enables the hidden features to be preserved until they reach to the uppermost layer and all the previous features work for prediction as collective knowledge <ref type="bibr" target="#b12">(Huang et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Densely-connected Co-attentive networks</head><p>Attention mechanism, which has largely succeeded in many domains <ref type="bibr" target="#b31">(Wu et al. 2016;</ref><ref type="bibr" target="#b28">Vaswani et al. 2017)</ref>, is a technique to learn effectively where a context vector is matched conditioned on a specific sequence.</p><p>Given two sentences, a context vector is calculated based on an attention mechanism focusing on the relevant part of the two sentences at each RNN layer. The calculated attentive information represents soft-alignment between two sentences. In this work, we also use an attention mechanism. We incorporate co-attentive information into densely connected recurrent features using the concatenation operation, so as not to lose any information (Fig. <ref type="figure" target="#fig_0">1</ref>). This concatenated recurrent and co-attentive features which are obtained by densely connecting the features from the undermost to the uppermost layers, enrich the collective knowledge for lexical and compositional semantics.</p><p>The attentive information a pi of the i th word p i ∈ P against the sentence Q is calculated as a weighted sum of h qj 's which are weighted by the softmax weights as follows:</p><formula xml:id="formula_4">a pi = J j=1 α i,j h qj α i,j = exp(e i,j ) J k=1 exp(e i,k ) e i,j = cos(h pi , h qj ) (5)</formula><p>Similar to the densely connected RNN hidden features, we concatenate the attentive context vector a pi with triggered vector h pi so as to retain attentive information as an input to the next layer:</p><formula xml:id="formula_5">h l t = H l (x l t , h l t−1 ), x l t = [h l−1 t ; a l−1 t ; x l−1 t ].<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottleneck component</head><p>Our network uses all layers' outputs as a community of semantic knowledge. However, this network is a structure with increasing input features as layers get deeper, and has a large number of parameters especially in the fully-connected layer. To address this issue, we employ an autoencoder as a bottleneck component. Autoencoder is a compression technique that reduces the number of features while retaining the original information, which can be used as a distilled semantic knowledge in our model. Furthermore, this component increased the test performance by working as a regularizer in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction and Prediction Layer</head><p>To extract a proper representation for each sentence, we apply the step-wise max-pooling operation over densely connected recurrent and co-attentive features (pooling in Fig. <ref type="figure" target="#fig_0">1</ref>). More specifically, if the output of the final RNN layer is a 100d vector for a sentence with 30 words, a 30 × 100 matrix is obtained which is max-pooled column-wise such that the size of the resultant vector p or q is 100. Then, we aggregate these representations p and q for the two sentences P and Q in various ways in the interaction layer and the final feature vector v for semantic sentence matching is obtained as follows: v = [p; q; p + q; p − q; |p − q|].</p><p>(7) Here, the operations +, − and | • | are performed elementwise to infer the relationship between two sentences. The element-wise subtraction p − q is an asymmetric operator for one-way type tasks such as natural language inference or answer sentence selection.</p><p>Finally, based on previously aggregated features v, we use two fully-connected layers with ReLU activation followed by one fully-connected output layer. Then, the softmax function is applied to obtain a probability distribution of each class. The model is trained end-to-end by minimizing the multi-class cross entropy loss and the reconstruction loss of autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate our matching model on five popular and wellstudied benchmark datasets for three challenging sentence matching tasks: (i) SNLI and MultiNLI for natural language inference; (ii) Quora Question Pair for paraphrase identification; and (iii) TrecQA and SelQA for answer sentence selection in question answering. Additional details about the above datasets can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We initialized word embedding with 300d GloVe vectors pre-trained from the 840B Common Crawl corpus (Pennington, Socher, and Manning 2014), while the word embeddings for the out-of-vocabulary words were initialized randomly. We also randomly initialized character embedding with a 16d vector and extracted 32d character representation with a convolutional network. For the densely-connected recurrent layers, we stacked 5 layers each of which have 100</p><p>Premise two bicyclists in spandex and helmets in a race pedaling uphill. Hypothesis A pair of humans are riding their bicycle with tight clothing, competing with each other. Label {entailment; neutral; contradiction} Premise Several men in front of a white building. Hypothesis Several people in front of a gray building. Label {entailment; neutral; contradiction} Table <ref type="table">1</ref>: Examples of natural language inference. hidden units. We set 1000 hidden units with respect to the fully-connected layers. The dropout was applied after the word and character embedding layers with a keep rate of 0.5. It was also applied before the fully-connected layers with a keep rate of 0.8. For the bottleneck component, we set 200 hidden units as encoded features of the autoencoder with a dropout rate of 0.2. The batch normalization was applied on the fully-connected layers, only for the one-way type datasets. The RMSProp optimizer with an initial learning rate of 0.001 was applied. The learning rate was decreased by a factor of 0.85 when the dev accuracy does not improve. All weights except embedding matrices are constrained by L2 regularization with a regularization constant λ = 10 −6 . The sequence lengths of the sentence are all different for each dataset: 35 for SNLI, 55 for MultiNLI, 25 for Quora question pair and 50 for TrecQA. The learning parameters were selected based on the best performance on the dev set. We employed 8 different randomly initialized sets of parameters with the same model for our ensemble approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI and MultiNLI</head><p>We evaluated our model on the natural language inference task over SNLI and MultiNLI datasets. Table <ref type="table" target="#tab_1">2</ref> shows the results on SNLI dataset of our model with other published models. Among them, ESIM+ELMo and LM-Transformer are the current stateof-the-art models. However, they use additional contextualized word representations from language models as an externel knowledge. The proposed DRCN obtains an accuracy of 88.9% which is a competitive score although we do not use any external knowledge like ESIM+ELMo and LM-Transformer. The ensemble model achieves an accuracy of 90.1%, which sets the new state-of-the-art performance. Our ensemble model with 53m parameters (6.7m×8) outperforms the LM-Transformer whose the number of parameters is 85m. Furthermore, in case of the encoding-based method, we obtain the best performance of 86.5% without the co-attention and exact match flag.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows the results on MATCHED and MIS-MATCHED problems of MultiNLI dataset. Our plain DRCN has a competitive performance without any contextualized knowledge. And, by combining DRCN with the ELMo, one of the contextualized embeddings from language models, our model outperforms the LM-Transformer which has 85m parameters with fewer parameters of 61m. From this point of view, the combination of our model with a contextualized knowledge is a good option to enhance the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Acc.</p><p>|θ| Sentence encoding-based method BiLSTM-Max <ref type="bibr" target="#b5">(Conneau et al. 2017)</ref> 84.5 40m Gumbel TreeLSTM <ref type="bibr" target="#b4">(Choi, Yoo, and goo Lee 2017)</ref> 85.6 2.9m CAFE <ref type="bibr" target="#b26">(Tay, Tuan, and Hui 2017)</ref> 85.9 3.7m Gumbel TreeLSTM <ref type="bibr" target="#b4">(Choi, Yoo, and goo Lee 2017)</ref> 86.0 10m Residual stacked <ref type="bibr" target="#b17">(Nie and Bansal 2017)</ref> 86.0 29m Reinforced SAN <ref type="bibr" target="#b24">(Shen et al. 2018)</ref> 86.3 3.1m Distance SAN <ref type="bibr" target="#b13">(Im and Cho 2017)</ref> 86.3 3.1m DRCN (-Attn, -Flag) 86.5 5.6m Joint method (cross-features available) DIIN <ref type="bibr" target="#b8">(Gong, Luo, and Zhang 2018)</ref> 88.0 / 88.9 4.4m ESIM <ref type="bibr" target="#b3">(Chen et al. 2017b)</ref> 88.0 / 88.6 4.3m BCN+CoVe+Char <ref type="bibr" target="#b15">(McCann et al. 2017)</ref> 88.1 / -22m DR-BiLSTM <ref type="bibr" target="#b7">(Ghaeini et al. 2018)</ref> 88.5 / 89.3 7.5m CAFE <ref type="bibr" target="#b26">(Tay, Tuan, and Hui 2017)</ref> 88.5 / 89.3 4.7m KIM <ref type="bibr">(Chen et</ref>   Quora Question Pair Table <ref type="table" target="#tab_3">4</ref> shows our results on the Quora question pair dataset. BiMPM using the multiperspective matching technique between two sentences reports baseline performance of a L.D.C. network <ref type="bibr" target="#b29">(Wang, Hamza, and Florian 2017)</ref>. We obtained accuracies of 90.15% and 91.30% in single and ensemble methods, respectively, surpassing the previous state-of-the-art model of DIIN.</p><p>TrecQA and SelQA Table <ref type="table" target="#tab_4">5</ref> shows the performance of different models on TrecQA and SelQA datasets for answer sentence selection task that aims to select a set of candidate answer sentences given a question. Most competitive models <ref type="bibr" target="#b24">(Shen, Yang, and Deng 2017;</ref><ref type="bibr" target="#b0">Bian et al. 2017;</ref><ref type="bibr" target="#b29">Wang, Hamza, and Florian 2017;</ref><ref type="bibr">Shen et al. 2017</ref>) also use attention methods for words alignment between question and candidate answer sentences. However, the proposed DRCN using collective attentions over multiple layers, achieves the new state-of-the-art performance, exceeding the current state-of-the-art performance significantly on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Ablation study We conducted an ablation study on the SNLI dev set as shown in Table <ref type="table">6</ref>, where we aim to ex-Models Accuracy (%) L.D.C. <ref type="bibr" target="#b29">(Wang, Hamza, and Florian 2017)</ref> 85.55 BiMPM <ref type="bibr" target="#b29">(Wang, Hamza, and Florian 2017)</ref> 88.17 pt-DecAttchar.c <ref type="bibr" target="#b27">(Tomar et al. 2017)</ref> 88.40 DIIN <ref type="bibr">(Gong, Luo, and</ref>   amine the effectiveness of our word embedding technique as well as the proposed densely-connected recurrent and coattentive features. Firstly, we verified the effectiveness of the autoencoder as a bottleneck component in (2). Although the number of parameters in the DRCN significantly decreased as shown in Table <ref type="table" target="#tab_1">2</ref>, we could see that the performance was rather higher because of the regularization effect. Secondly, we study how the technique of mixing trainable and fixed word embeddings contributes to the performance in models (3-4). After removing E tr or E f ix in eq. ( <ref type="formula" target="#formula_0">1</ref>), the performance degraded, slightly. The trainable embedding E tr seems more effective than the fixed embedding E f ix . Next, the effectiveness of dense connections was tested in models (5-9). In (5-6), we removed dense connections only over coattentive or recurrent features, respectively. The result shows that the dense connections over attentive features are more effective. In (7), we removed dense connections over both co-attentive and recurrent features, and the performance degraded to 88.5%. In ( <ref type="formula">8</ref>  The results of (8-9) demonstrate that the dense connection using concatenation operation over deeper layers, has more powerful capability retaining collective knowledge to learn textual semantics. The model ( <ref type="formula">10</ref>) is the basic 5-layer RNN with attention and ( <ref type="formula">11</ref>) is the one without attention. The result of (10) shows that the connections among the layers are important to help gradient flow. And, the result of ( <ref type="formula">11</ref>) shows that the attentive information functioning as a soft-alignment is significantly effective in semantic sentence matching.</p><p>The performances of models having different number of recurrent layers are also reported in Fig. <ref type="figure" target="#fig_2">2</ref>. The models (5-9) which have connections between layers, are more robust to the increased depth of network, however, the performances of (10-11) tend to degrade as layers get deeper. In addition, the models with dense connections rather than residual connections, have higher performance in general. Figure <ref type="figure" target="#fig_2">2</ref> shows that the connection between layers is essential, especially in deep models, endowing more representational power, and the dense connection is more effective than the residual connection. to the classification layer through the max pooling operation such that all max-valued features of every layer affect the loss function and perform a kind of deep supervision <ref type="bibr" target="#b12">(Huang et al. 2017</ref>). Thus, we could cautiously interpret the classification results using our attentive weights and maxpooled positions. The attentive weights contain information on how two sentences are aligned and the numbers of max-pooled positions in each dimension play an important role in classification.</p><p>Figure <ref type="figure" target="#fig_4">3</ref> shows the attention map (α i,j in eq. ( <ref type="formula">5</ref>)) on each layer of the samples in Table <ref type="table">1</ref>. The Avg(Layers) is the average of attentive weights over 5 layers and the gray heatmap right above the Avg(Layers) is the rate of max-pooled positions. The darker indicates the higher importance in classification. In the figure, we can see that tight, competing and bicycle are more important words than others in classifying the label. The word tight clothing in the hypothesis can be inferred from spandex in the premise. And competing is also inferred from race. Other than that, the riding is matched with pedaling, and pair is matched with two. Judging by the matched terms, the model is undoubtedly able to classify the label as an entailment, correctly.</p><p>In Figure <ref type="figure" target="#fig_4">3</ref> (b), most of words in both the premise and the hypothesis coexist except white and gray. In attention map of layer 1, the same or similar words in each sentence have a high correspondence (gray and white are not exactly  matched but have a linguistic relevance). However, as the layers get deeper, the relevance between white building and gray building is only maintained as a clue of classification (See layer 5). Because white is clearly different from gray, our model determines the label as a contradiction.</p><p>The densely connected recurrent and co-attentive features are well-semanticized over multiple layers as collective knowledge. And the max pooling operation selects the soft-positions that may extract the clues on inference correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic Error Analysis</head><p>We conducted a linguistic error analysis on MultiNLI, and compared DRCN with the ESIM, DIIN and CAFE. We used annotated subset provided by the MultiNLI dataset, and each sample belongs to one of the 13 linguistic categories. The results in table <ref type="table" target="#tab_5">7</ref> show that our model generally has a good performance than others on most categories. Especially, we can see that ours outperforms much better on the Quantity/Time category which is one of the most difficult problems. Furthermore, our DRCN shows the highest mean and the lowest stddev for both MATCHED and MISMATCHED problems, which indicates that it not only results in a competitive performance but also has a consistent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we introduce a densely-connected recurrent and co-attentive network (DRCN) for semantic sentence matching. We connect the recurrent and co-attentive features from the bottom to the top layer without any deformation. These intact features over multiple layers compose a community of semantic knowledge and outperform the previous deep RNN models using residual connections. In doing so, bottleneck components are inserted to reduce the size of the network. Our proposed model is the first generalized version of DenseRNN which can be expanded to deeper layers with the property of controllable feature sizes by the use of an autoencoder. We additionally show the interpretability of our model using the attentive weights and the rate of max-pooled positions. Our model achieves the state-of-theart performance on most of the datasets of three highly challenging natural language tasks. Our proposed method using the collective semantic knowledge is expected to be applied to the various other natural language tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General architecture of our Densely-connected Recurrent and Co-attentive neural Network (DRCN). Dashed arrows indicate that a group of RNN-layer, concatenation and AE can be repeated multiple (N ) times (like a repeat mark in a music score). The bottleneck component denoted as AE, inserted to prevent the ever-growing size of a feature vector, is optional for each repetition. The upper right diagram is our specific architecture for experiments with 5 RNN layers (N = 4).</figDesc><graphic url="image-1.png" coords="2,79.20,54.00,453.60,191.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>), we replace dense connection with residual connection only over recurrent and co-attentive features. It means that only the word embedding features are Models − dense(Rec. &amp; Attn.) 88.7 + res(Rec. &amp; Attn.) (9) − dense(Rec. &amp; Attn. &amp; Emb) 88.4 + res(Rec. &amp; Attn.) (10) − dense(Rec. &amp; Attn. &amp; Emb) 87.8 (11) − dense(Rec. &amp; Attn. &amp; Emb) -Attn. 85.3Table6: Ablation study results on the SNLI dev sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of models on every layer in ablation study. (best viewed in color)</figDesc><graphic url="image-2.png" coords="6,54.00,221.33,238.50,133.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of attentive weights and the rate of max-pooled position. The darker, the higher. See supplementary materials for a comparison with other models that use the residual connections.</figDesc><graphic url="image-4.png" coords="7,107.06,194.10,400.06,119.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy (%) for natural language inference on SNLI test set. |θ| denotes the number of parameters in each model.</figDesc><table><row><cell>Models</cell><cell cols="2">Accuracy (%) MATCHED MISMATCHED</cell></row><row><cell>ESIM (Williams, Nangia, and Bowman 2017)</cell><cell>72.3</cell><cell>72.1</cell></row><row><cell>DIIN (Gong, Luo, and Zhang 2018)</cell><cell>78.8</cell><cell>77.8</cell></row><row><cell>CAFE (Tay, Tuan, and Hui 2017)</cell><cell>78.7</cell><cell>77.9</cell></row><row><cell>LM-Transformer (Radford et al. 2018)</cell><cell>82.1</cell><cell>81.4</cell></row><row><cell>DRCN</cell><cell>79.1</cell><cell>78.4</cell></row><row><cell>DIIN* (Gong, Luo, and Zhang 2018)</cell><cell>80.0</cell><cell>78.7</cell></row><row><cell>CAFE* (Tay, Tuan, and Hui 2017)</cell><cell>80.2</cell><cell>79.0</cell></row><row><cell>DRCN*</cell><cell>80.6</cell><cell>79.5</cell></row><row><cell>DRCN+ELMo*</cell><cell>82.3</cell><cell>81.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy for natural language inference on MultiNLI test set. * denotes ensemble methods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy for paraphrase identification on Quora question pair test set. * denotes ensemble methods.</figDesc><table><row><cell>Zhang 2018)</cell><cell>89.06</cell></row><row><cell>DRCN</cell><cell>90.15</cell></row><row><cell>DIIN* (Gong, Luo, and Zhang 2018)</cell><cell>89.84</cell></row><row><cell>DRCN*</cell><cell>91.30</cell></row><row><cell>Models</cell><cell>MAP MRR</cell></row><row><cell>Raw version</cell><cell></cell></row><row><cell>aNMM (Yang et al. 2016)</cell><cell>0.750 0.811</cell></row><row><cell>PWIM (He and Lin 2016)</cell><cell>0.758 0.822</cell></row><row><cell>MP CNN (He, Gimpel, and Lin 2015)</cell><cell>0.762 0.830</cell></row><row><cell>HyperQA (Tay, Luu, and Hui 2017)</cell><cell>0.770 0.825</cell></row><row><cell>PR+CNN (Rao, He, and Lin 2016)</cell><cell>0.780 0.834</cell></row><row><cell>DRCN</cell><cell>0.804 0.862</cell></row><row><cell>clean version</cell><cell></cell></row><row><cell>HyperQA (Tay, Luu, and Hui 2017)</cell><cell>0.801 0.877</cell></row><row><cell cols="2">BiMPM (Wang, Hamza, and Florian 2017) 0.802 0.875</cell></row><row><cell>Comp.-Aggr. (Bian et al. 2017)</cell><cell>0.821 0.899</cell></row><row><cell>IWAN (Shen, Yang, and Deng 2017)</cell><cell>0.822 0.889</cell></row><row><cell>DRCN</cell><cell>0.830 0.908</cell></row><row><cell>(a) TrecQA: raw and clean</cell><cell></cell></row><row><cell>Models</cell><cell>MAP MRR</cell></row><row><cell cols="2">CNN-DAN (Santos, Wadhawan, and Zhou 2017) 0.866 0.873</cell></row><row><cell cols="2">CNN-hinge (Santos, Wadhawan, and Zhou 2017) 0.876 0.881</cell></row><row><cell>ACNN (Shen et al. 2017)</cell><cell>0.874 0.880</cell></row><row><cell>AdaQA (Shen et al. 2017)</cell><cell>0.891 0.898</cell></row><row><cell>DRCN</cell><cell>0.925 0.930</cell></row><row><cell>(b) SelQA</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance for answer sentence selection on TrecQA and selQA test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Word Alignment and Importance Our denselyconnected recurrent and co-attentive features are connected</figDesc><table><row><cell>Category</cell><cell cols="4">ESIM DIIN CAFE DRCN</cell></row><row><cell></cell><cell cols="2">Matched</cell><cell></cell><cell></cell></row><row><cell>Conditional</cell><cell>100</cell><cell>57</cell><cell>70</cell><cell>65</cell></row><row><cell>Word overlap</cell><cell>50</cell><cell>79</cell><cell>82</cell><cell>89</cell></row><row><cell>Negation</cell><cell>76</cell><cell>78</cell><cell>76</cell><cell>80</cell></row><row><cell>Antonym</cell><cell>67</cell><cell>82</cell><cell>82</cell><cell>82</cell></row><row><cell>Long Sentence</cell><cell>75</cell><cell>81</cell><cell>79</cell><cell>83</cell></row><row><cell>Tense Difference</cell><cell>73</cell><cell>84</cell><cell>82</cell><cell>82</cell></row><row><cell>Active/Passive</cell><cell>88</cell><cell>93</cell><cell>100</cell><cell>87</cell></row><row><cell>Paraphrase</cell><cell>89</cell><cell>88</cell><cell>88</cell><cell>92</cell></row><row><cell>Quantity/Time</cell><cell>33</cell><cell>53</cell><cell>53</cell><cell>73</cell></row><row><cell>Coreference</cell><cell>83</cell><cell>77</cell><cell>80</cell><cell>80</cell></row><row><cell>Quantifier</cell><cell>69</cell><cell>74</cell><cell>75</cell><cell>78</cell></row><row><cell>Modal</cell><cell>78</cell><cell>84</cell><cell>81</cell><cell>81</cell></row><row><cell>Belief</cell><cell>65</cell><cell>77</cell><cell>77</cell><cell>76</cell></row><row><cell>Mean</cell><cell>72.8</cell><cell>77.46</cell><cell>78.9</cell><cell>80.6</cell></row><row><cell>Stddev</cell><cell>16.6</cell><cell>10.75</cell><cell>10.2</cell><cell>6.7</cell></row><row><cell></cell><cell cols="2">Mismatched</cell><cell></cell><cell></cell></row><row><cell>Conditional</cell><cell>60</cell><cell>69</cell><cell>85</cell><cell>89</cell></row><row><cell>Word overlap</cell><cell>62</cell><cell>92</cell><cell>87</cell><cell>89</cell></row><row><cell>Negation</cell><cell>71</cell><cell>77</cell><cell>80</cell><cell>78</cell></row><row><cell>Antonym</cell><cell>58</cell><cell>80</cell><cell>80</cell><cell>80</cell></row><row><cell>Long Sentence</cell><cell>69</cell><cell>73</cell><cell>77</cell><cell>84</cell></row><row><cell>Tense Difference</cell><cell>79</cell><cell>78</cell><cell>89</cell><cell>83</cell></row><row><cell>Active/Passive</cell><cell>91</cell><cell>70</cell><cell>90</cell><cell>100</cell></row><row><cell>Paraphrase</cell><cell>84</cell><cell>100</cell><cell>95</cell><cell>90</cell></row><row><cell>Quantity/Time</cell><cell>54</cell><cell>69</cell><cell>62</cell><cell>80</cell></row><row><cell>Coreference</cell><cell>75</cell><cell>79</cell><cell>83</cell><cell>87</cell></row><row><cell>Quantifier</cell><cell>72</cell><cell>78</cell><cell>80</cell><cell>82</cell></row><row><cell>Modal</cell><cell>76</cell><cell>75</cell><cell>81</cell><cell>87</cell></row><row><cell>Belief</cell><cell>67</cell><cell>81</cell><cell>83</cell><cell>85</cell></row><row><cell>Mean</cell><cell>70.6</cell><cell>78.53</cell><cell>82.5</cell><cell>85.7</cell></row><row><cell>Stddev</cell><cell>10.2</cell><cell>8.55</cell><cell>7.6</cell><cell>5.5</cell></row></table><note>Accuracy (%) of Linguistic correctness on MultiNLI dev sets.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) (2017M3C4A7077582).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A compareaggregate model with dynamic-clip attention for answer selection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1987">2017. 1987-1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04289</idno>
		<title level="m">Natural language inference with external knowledge</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning to compose task-specific tree structures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02364</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Quora question pair dataset</title>
		<author>
			<persName><forename type="first">K</forename><surname>Csernai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05577</idno>
		<title level="m">Dr-bilstm: Dependent reading bidirectional lstm for natural language inference</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language inference over interaction space</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
				<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-perspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Distance-based self-attention network for natural language inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02047</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep fusion lstms for text semantic matching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1034" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shortcut-stacked sentence encoders for multi-domain inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02312</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
				<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2017. 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1512" to="1522" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adding semantics to data-driven paraphrasing</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2014">2014. 2018</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Proc. of NAACL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation for answer selection with deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1913" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Investigating a generic paraphrase-based approach for relation extraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kouylekov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Conference of the European Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning loss functions for semi-supervised learning via discriminative adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02198</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adaptive convolutional filter generation for natural language understanding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08294</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10296</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2018. 2017</date>
			<biblScope unit="page" from="1179" to="1189" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Inter-weighted alignment network for sentence pair modeling</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Enabling efficient question answer retrieval via hyperbolic neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<idno>CoRR abs/1707.07847</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A compare-propagate architecture with alignment factorization for natural language inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00102</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural paraphrase identification of questions with noisy pretraining</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04565</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<title level="m">Bilateral multiperspective matching for natural language sentences</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What is the jeopardy model? a quasi-synchronous grammar for qa</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A broadcoverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<idno>arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2017. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">anmm: Ranking short answer texts with attention-based neural matching model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
