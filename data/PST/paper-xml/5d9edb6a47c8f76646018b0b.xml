<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixture Models, Robustness, and Sum of Squares Proofs *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samuel</forename><forename type="middle">B</forename><surname>Hopkins</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
							<email>jerryzli@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mixture Models, Robustness, and Sum of Squares Proofs *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7568BEA2B01BFFAB0E54B5BA24E4C83D</idno>
					<idno type="DOI">10.1145/3188745.3188748</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised learning</term>
					<term>clustering</term>
					<term>mixture models</term>
					<term>mixture of Gaussians</term>
					<term>robust statistics</term>
					<term>high-dimensional statistics</term>
					<term>sum of squares method</term>
					<term>semidefinite programming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use the Sum of Squares method to develop new efficient algorithms for learning well-separated mixtures of Gaussians and robust mean estimation, both in high dimensions, that substantially improve upon the statistical guarantees achieved by previous efficient algorithms. Our contributions are:</p><p>Mixture models with separated means: We study mixtures of k distributions in d dimensions, where the means of every pair of distributions are separated by at least k ε . In the special case of spherical Gaussian mixtures, we give a (dk) O (1/ε 2 ) -time algorithm that learns the means assuming separation at least k ε , for any ε &gt; 0. This is the first algorithm to improve on greedy ("single-linkage") and spectral clustering, breaking a long-standing barrier for efficient algorithms at separation k 1/4 .</p><p>Robust estimation: When an unknown (1 -ε)-fraction of X 1 , . . . , X n are chosen from a sub-Gaussian distribution with mean µ but the remaining points are chosen adversarially, we give an algorithm recovering µ to error ε 1-1/t in time d O (t 2 ) , so long as sub-Gaussian-ness up to O(t) moments can be certified by a Sum of Squares proof. This is the first polynomial-time algorithm with guarantees approaching the information-theoretic limit for non-Gaussian distributions. Previous algorithms could not achieve error better than ε 1/2 .</p><p>Both of these results are based on a unified technique. Inspired by recent algorithms of Diakonikolas et al. in robust statistics, we devise an SDP based on the Sum of Squares method for the following setting: given X 1 , . . . , X n ∈ R d for large d and n = poly(d) with the promise that a subset of X 1 , . . . , X n were sampled from a probability distribution with bounded moments, recover some information about that distribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We propose and analyze a family of new algorithms for some fundamental high-dimensional statistical estimation problems. In particular, we give new algorithms for the following problems.</p><p>(1) Learning ∆-separated mixture models: Given n samples X 1 , . . . , X n ∈ R d from a mixture of k probability distributions D 1 , . . . , D k on R d with means µ 1 , . . . , µ k ∈ R d and covariances Σ 1 , . . . , Σ k ⪯ Id, where ∥µ i -µ j ∥ ≥ ∆, estimate µ 1 , . . . , µ k . <ref type="foot" target="#foot_0">1</ref>(2) Robust mean estimation: Given n vectors X 1 , . . . , X n ∈ R d , of which a (1 -ε)-fraction are samples from a probability distribution D with mean µ and covariance Σ ⪯ Id and the remaining ε-fraction are arbitrary vectors (which may depend on the (1 -ε)n samples from D), estimate µ.</p><p>Mixture models, and especially Gaussian mixture models (where D 1 , . . . , D k are Gaussian distributions) have been studied since Pearson in 1894 <ref type="bibr" target="#b55">[55]</ref>. Work in theoretical computer science dates at least to the pioneering algorithm of Dasgupta in 1999 <ref type="bibr" target="#b18">[19]</ref>, which has been followed by numerous other algorithms and lower bounds <ref type="bibr">[1-3, 12, 14, 20-22, 29, 30, 33, 37, 40, 45, 48, 52, 58, 62, 67-69]</ref>.</p><p>Robust estimation in the form we study here is a more recent transplant to theoretical computer science <ref type="bibr">[17, 18, 23-25, 27, 46, 61]</ref>, but statisticians have long sought outlier-robust estimators. Formal study of arbitrarily-bad/adversarially-chosen outliers originates in the 1960s with the advent of "breakdown points" in statistics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b64">64]</ref>.</p><p>Though outwardly rather different, mixture model learning and robust estimation share some underlying structure. An algorithm for either must identify or otherwise recover information about one or several structured subsets of a number of samples X 1 , . . . , X n ∈ R d . In the mixture model case, each collection of all the samples from each distribution D i is a structured subset. In the robust estimation case there is just one structured subset: the (1 -ε)n samples drawn from the distribution D. 2 Our algorithms are based on new techniques for identifying such structured subsets of points in large data sets.</p><p>For mixture models, a special case of our main result yields the first progress in more than 15 years on efficiently clustering mixtures of separated spherical Gaussians. The question here is: if D 1 , . . . , D k are all Gaussian with covariance identity, what is the minimum cluster separation ∆ which allows for a polynomialtime algorithm to estimate µ 1 , . . . , µ k from poly(k, d) samples from the mixture model? When k = d, the guarantees of the previous best algorithms for this problem, which require ∆ ≥ O(k 1/<ref type="foot" target="#foot_1">4</ref> ), are captured by a simple greedy clustering algorithm, sometimes called single-linkage clustering: when ∆ ≥ O(k 1/4 ), with high probability every pair of samples from the same cluster is closer in Euclidean distance than every pair of samples from differing clusters.</p><p>We break this single-linkage clustering barrier: for every γ &gt; 0 we give a poly(k, d)-time algorithm for this problem when ∆ &gt; k γ . Our results extend to any k and d. In this more general setting the previous-best algorithms combine spectral dimension reduction (by projecting the samples to the top eigenvectors of an empirical covariance matrix) with single-linkage clustering <ref type="bibr" target="#b67">[67]</ref>. These algorithms require separation ∆ ≥ O(min(d, k) 1/4 ), while our algorithms continue to tolerate separation ∆ &gt; k γ for any γ &gt; 0. 3  Our algorithm relies on novel use of higher moments (in fact, O(1/γ ) moments) of the underlying distributions D i . Our main technical contribution is a new algorithmic technique for finding either a structured subset of data points or the empirical mean of such a subset when the subset consists of independent samples from a distribution D which has bounded higher-order moments and there is a simple certificate of this boundedness. This technique leverages the Sum of Squares (SoS) hierarchy of semidefinite programs (SDPs), and in particular a powerful approach for designing SoS-based algorithms in machine learning settings, developed and used in <ref type="bibr">[7-9, 31, 36, 49, 56]</ref>.</p><p>This SoS approach to unsupervised learning rests on a notion of simple identifiability proofs: the main step in designing an algorithm using SoS to recover some parameters θ from samples x 1 , . . . , x n ∼ p(x | θ ) is to prove in a restricted proof system that θ is likely to be uniquely identifiable from x 1 , . . . , x n . We develop this thoroughly later on, but roughly speaking one may think of this as requiring the identifiability proof to use only simple inequalities, such as Cauchy-Schwarz and Hölder's inequality, applied to low-degree polynomials. The simple identifiability proofs we construct for both the mixture models and robust estimation settings are heavily inspired by the robust estimation algorithms of Diakonikolas et al. <ref type="bibr" target="#b22">[23]</ref>. 2 The recent work <ref type="bibr" target="#b16">[17]</ref> codifies this similarity by unifying both these problems into what they call a list-decodable learning setting. 3 In the years since an algorithm obtaining ∆ ≥ O (min(d, k) 1/4 ) was achieved by <ref type="bibr" target="#b67">[67]</ref> there has been progress in extending similar results for more general clustering settings. In fact, the algorithm of <ref type="bibr" target="#b67">[67]</ref> already tolerates isotropic, log-concave distributions, and allows for each component to have a distinct variance σ 2 i ∈ R, with the separation condition becoming ∥µ iµ j ∥ &gt; max(σ i , σ j ) min(d, k) 1/4 . Later works such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">45]</ref> continued to generalize these results to broader clustering settings. Most related to the present work are spectral algorithms which weaken log-concavity to a bounded-covariance assumption, at the cost of requiring separation ∆ &gt; √ k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Results</head><p>Both of the problems we study have a long history; for now we just note some highlights and state our main results.</p><p>Mixture models. The problem of learning mixture models dates to Pearson in 1894, who invented the method of moments in order to separate a mixture of two Gaussians <ref type="bibr" target="#b55">[55]</ref>. Mixture models have since become ubiquitous in data analysis across many disciplines <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b63">63]</ref>. In recent years, computer scientists have devised many ingenious algorithms for learning mixture models as it became clear that classical statistical methods (e.g. maximum likelihood estimation) often suffer from computational intractability, especially when there are many mixture components or the components are high dimensional.</p><p>A highlight of this work is a series of algorithmic results when the components of the mixture model are Gaussian <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b67">67]</ref>.</p><p>Here the main question is: how small can the cluster separation ∆ be such that there exists an algorithm to estimate µ 1 , . . . , µ k from samples x 1 , . . . , x n in poly(k, d) time (hence also using n = poly(k, d) samples)? Focusing for simplicity on spherical Gaussian components (i.e. with covariance equal to the identity matrix Id) and with number of components similar to the ambient dimension of the data (i.e. k = d) and uniform mixing weights (i.e. every cluster has roughly the same representation among the samples), the best result in previous work gives a poly(k)-time algorithm when ∆ ≥ k 1/4 .</p><p>Separation ∆ = k 1/4 represents a natural algorithmic barrier: when ∆ ≥ k 1/4 , every pair of samples from the same cluster are closer to each other in Euclidean distance than are every pair of samples from distinct clusters (with high probability), while this is no longer true if ∆ &lt; k 1/4 . Thus, when ∆ ≥ k 1/4 , a simple greedy algorithm correctly clusters the samples into their components (this algorithm is sometimes called single-linkage clustering). On the other hand, standard information-theoretic arguments show that the means remain approximately identifiable from poly(k, d) samples when ∆ is as small as O( log k), but these methods yield only exponentialtime algorithms. 4 Nonetheless, despite substantial attention, this ∆ = k 1/4 barrier representing the breakdown of single-linkage clustering has stood for nearly 20 years.</p><p>We prove the following main theorem, breaking the singlelinkage clustering barrier. Theorem 1.1 (Informal, special case for uniform mixture of spherical Gaussians (see full version)). For every γ &gt; 0 there is an algorithm with running time (dk</p><formula xml:id="formula_0">) O (1/γ 2 ) using at most n ≤ k O (1) d O (1/γ ) samples which, given samples x 1 , . . . , x n from a uniform mixture of k spherical Gaussians N (µ i , Id) in d dimensions with means µ 1 , . . . , µ k ∈ R d satisfying ∥µ i -µ j ∥ ≥ k γ for each i j, returns estimators μ1 , . . . , μk ∈ R d such that ∥ μi -µ i ∥ ≤ 1/poly(k) (with high probability).</formula><p>We pause here to make several remarks about this theorem. Our algorithm makes novel use of higher order moments of Gaussian (and sub-Gaussian) distributions. Most previous work for efficiently learning well-separated mixtures either used only second-order moment information, and required separation ∆ ≥ Ω( √ k), or made mild use of log-concavity to improve this to k 1/4 , whereas we use O(1/γ ) moments.</p><p>The guarantees of our theorem hold well beyond the Gaussian setting; the theorem applies to any mixture model with k γ separation and whose component distributions D 1 , . . . , D k are what we term O(1/γ )-explicitly bounded. We define this notion formally below, but roughly speaking, a t-explicitly bounded distribution D has t-th moments obeying a subgaussian-type bound-that is, for every unit vector u ∈ R d one has E Y ∼D |⟨Y , u⟩| t ≤ t t /2 -and there is a certain kind of simple certificate of this fact, namely a lowdegree Sum of Squares proof. Among other things, this means the theorem also applies to mixtures of symmetric product distributions with bounded moments.</p><p>For mixtures of distributions with sufficiently-many bounded moments (such as Gaussians), our guarantees go even further. We show that using d O (log k ) 2 time and d O (log k ) samples, we can recover the means to error 1/poly(k) even if the separation is only C log k for some universal constant C. Strikingly, <ref type="bibr" target="#b58">[58]</ref> show that any algorithm that can learn the means nontrivially given separation o( log k) must require super-polynomial samples and time. Our results show that just above this threshold, it is possible to learn with just quasipolynomially many samples and time.</p><p>Finally, throughout the paper we state error guarantees roughly in terms of obtaining μi with ∥ μi -µ i ∥ ≤ 1/poly(k) ≪ k γ , meaning that we get ℓ 2 error which is much less than the true separation. In the special case of spherical Gaussians, we note that we can use our algorithm as a warm-start to recent algorithms due to <ref type="bibr" target="#b58">[58]</ref>, and achieve error δ using poly(m, k, 1/δ ) additional runtime and samples for some polynomial independent of γ . Robust mean estimation. Estimators which are robust to outlying or corrupted samples have been studied in statistics at least since the 1960s <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b65">65]</ref>. The model we consider in this paper is a slight generalization of Hüber's contamination model <ref type="bibr" target="#b38">[38]</ref>. We are given X 1 , . . . , X n , originally drawn iid from some unknown distribution D, but an adversary has changed an ε fraction of these points adversarially. We call such a set of points ε-corrupted. <ref type="foot" target="#foot_2">5</ref> The goal of robust statistics is to recover statistics of D such as mean and covariance, given ε-corrupted samples from D.</p><p>In classical robust statistics, the robust mean estimation problem is known as robust estimation of location, and robust covariance estimation is known as robust estimation of scale. Classical works consider a measure known as breakdown point, which is (informally) the fraction of samples that an adversary must corrupt before the estimator has no provable guarantees. They often design robust estimators for mean and covariance that achieve optimal error in many fundamental settings. For instance, given samples from a symmetric sub-Gaussian distribution in k dimensions such that an ε-fraction are arbitrarily corrupted, an estimator known as the Tukey median <ref type="bibr" target="#b65">[65]</ref> achieves error O(ε), which is information theoretically optimal. However, these estimators are all N P-hard to compute <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">39]</ref> and the best known algorithms require exp(d) time in general.</p><p>For a long time, all known computationally efficient robust statistics for the mean or covariance of a d-dimensional Gaussian had error degrading polynomially with the dimension. <ref type="foot" target="#foot_3">6</ref> In recent work, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">46]</ref> gave efficient and robust estimators for these statistics which achieve substantially better error. In particular, <ref type="bibr" target="#b22">[23]</ref> achieve error O(ε log 1/ε) for estimating the mean of a Gaussian with identity covariance, and error O(ε log 3/2 1/ε) for robustly estimating the mean of a Gaussian with unknown variance Σ ⪯ I .</p><p>Unfortunately, these results are somewhat tailored to Gaussian distributions, or require covariance very close to identity. For general sub-Gaussian distributions with unknown variance Σ ⪯ I , the best known efficient algorithms achieve only O(ε 1/2 ) error <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b61">61]</ref>. We substantially improve this, under a slightly stronger condition than sub-Gaussianity. Recall that a distribution D with mean µ over R d is sub-Gaussian if for every unit vector u and every t ∈ N even, the following moment bound holds:</p><formula xml:id="formula_1">E X ∼D ⟨u, X -µ⟩ t ≤ t t /2 .</formula><p>Informally stated, our algorithms will work under the condition that this moment bound can be certified by a low degree SoS proof, for all s ≤ t. We call such distributions t-explicitly bounded (we are ignoring some parameters, see full version for a formal definition). This class captures many natural sub-Gaussian distributions, such as Gaussians, product distributions of sub-Gaussians, and rotations thereof (see full version). For such distributions, we show: Theorem 1.2 (Informal, see full version). Fix ε &gt; 0 sufficiently small and let t ≥ 4. Let D be a O(t)-explicitly bounded distribution over R d with mean µ * . There is an algorithm with sample complexity d O (t ) (1/ε) O (1) running time (d t ε) O (t ) such that given an ε-corrupted set of samples of sufficiently large size from D, outputs µ so that with high probability ∥µ -µ * ∥ ≤ O(ε 1-1/t ).</p><p>As with mixture models, we can push our statistical rates further, if we are willing to tolerate quasipolynomial runtime and sample complexity. In particular, we can obtain error O(ε log 1/ε) with d O (log 1/ε ) samples and d O (log 1/ε ) 2 time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>Mixture models. The literature on mixture models is vast so we cannot attempt a full survey here. The most directly related line of work to our results studies mixtures models under mean-separation conditions, and especially mixtures of Gaussians, where the number k of components of the mixture grows with the dimension d <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b67">67]</ref>. The culmination of these works is the algorithm of Vempala and Wang, which used spectral dimension reduction to improve on the d 1/4 separation required by previous works to k 1/4 in ℓ 2 distance for k ≤ d spherical Gaussians in d dimensions. Concretely, they show the following:</p><formula xml:id="formula_2">Theorem 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">([67], informal).</head><p>There is a constant C &gt; 0 and an algorithm with running time poly(k, d) such that for every µ 1 , . . . , µ k ∈ R d and σ 1 , . . . , σ k &gt; 0, satisfying</p><formula xml:id="formula_3">∥µ i -µ j ∥ &gt; C max(σ i , σ j )k 1/4 log 1/4 (d)</formula><p>with high probability the algorithm produces estimates μ1 , . . . , μk with ∥µ iμi ∥ ≤ 1/poly(k), given poly(k, d) samples from a mixture</p><formula xml:id="formula_4">1 k i ≤k N (µ i , σ i I ).</formula><p>The theorem extends naturally to isotropic log-concave distributions; our main theorem generalizes to distributions with explicitly bounded moments. These families of distributions are not strictly comparable.</p><p>Other works have relaxed the requirement that the underlying distributions be Gaussian <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">45]</ref>; to second-moment moment boundedness instead of to log-concavity; these algorithms typically tolerate separation of order √ k rather than k 1/4 . Our work can be thought of as a generalization of these algorithms to use boundedness of higher moments. One recent work in this spirit uses SDPs to cluster mixture models under separation assumptions <ref type="bibr" target="#b51">[51]</ref>; the authors show that a standard SDP relaxation of k-means achieves guarantees comparable to previously-known specially-tailored mixture model algorithms.</p><p>Information-theoretic sample complexity: Recent work of <ref type="bibr" target="#b58">[58]</ref> considers the Gaussian mixtures problem in an information-theoretic setting: they show that there is some constant C so that if the means are pairwise separated by at least C log k, then the means can be recovered to arbitrary accuracy (given enough samples). They give an efficient algorithm which, warm-started with sufficientlygood estimates of the means, improves the accuracy to δ using poly(1/δ, d, k) additional samples. However, their algorithm for providing this warm start requires time exponential in the dimension d. Our algorithm requires somewhat larger separation but runs in polynomial time. Thus by combining the techniques in the spherical Gaussian setting we can estimate the means with ℓ 2 error δ in polynomial time using an extra poly(1/δ, d, k) samples, when the separation is at least k γ , for any γ &gt; 0.</p><p>Fixed number of Gaussians in many dimensions: Other works address parameter estimation for mixtures of k ≪ d Gaussians (generally k = O(1) and d grows) under weak identifiability assumptions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b52">52]</ref>. In these works the only assumptions are that the component Gaussians are statistically distinguishable; the goal is to recover their parameters of the underlying Gaussians. It was shown in <ref type="bibr" target="#b52">[52]</ref> that algorithms in this setting provably require exp(k) samples and running time. The question addressed in our paper is whether this lower bound is avoidable under stronger identifiability assumptions. A related line of work addresses proper learning of mixtures of Gaussians <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b62">62]</ref>, where the goal is to output a mixture of Gaussians which is close to the unknown mixture in total-variation distance, avoiding the exp(k) parameterlearning sample-complexity lower bound. These algorithms achieve poly(k, d) sample complexity, but they all require exp(k) running time, and moreover, do not provide any guarantee that the parameters of the distributions output are close to those for the true mixture.</p><p>Tensor-decomposition methods: Another line of algorithms focus on settings where the means satisfy algebraic non-degeneracy conditions, which is the case for instance in smoothed analysis settings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">37]</ref>. These algorithms are typically based on finding a rank-one decomposition of the empirical 3rd or 4th moment tensor of the mixture; they heavily use the special structure of these moments for Gaussian mixtures. One paper we highlight is <ref type="bibr" target="#b13">[14]</ref>, which also uses much higher moments of the distribution. They show that in the smoothed analysis setting, the ℓth moment tensor of the distribution has algebraic structure which can be algorithmically exploited to recover the means. Their main structural result holds only in the smoothed analysis setting, where samples from a mixture model on perturbed means are available.</p><p>In contrast, we do not assume any non-degeneracy conditions and use moment information only about the individual components rather than the full mixture, which always hold under separation conditions. Moreover, our algorithms do not need to know the exact structure of the 3rd or 4th moments. In general, clustering-based algorithms like ours seem more robust to modelling errors than algebraic or tensor-decomposition methods.</p><p>Expectation-maximization (EM): EM is the most popular algorithm for Gaussian mixtures in practice, but it is notoriously difficult to analyze theoretically. The works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b69">69]</ref> offer some theoretical guarantees for EM, but non-convergence results are a barrier to strong theoretical guarantees <ref type="bibr" target="#b68">[68]</ref>.</p><p>Robust statistics. The literature on robust estimation is too large to do justice to here. There has been a long line of work on making algorithms tolerant to error in supervised settings <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b66">66]</ref>, especially for learning halfspaces <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b60">60]</ref>, and for problems such as PCA <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b70">70]</ref>. See <ref type="bibr" target="#b22">[23]</ref> for a more detailed discussion on the relationship between these questions (and others) and the model we consider here.</p><p>We consider the classical statistical notion of robustness against corruption, introduced back in the 70's in seminal works of <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b64">64]</ref>. Even for the mean of a Gaussian distribution, essentially all classical robust estimators are hard in the worst case to compute ( <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">39]</ref>). However, a recent flurry of work <ref type="bibr">([17, 23, 27, 46, 61]</ref>) has given new, computationally efficient, nearly optimal robust estimators for the mean and covariance of a high dimensional Gaussian distribution. Given sufficiently-many samples from a sub-Gaussian distribution with identity covariance, where an ε-fraction are arbitrarily corrupted, these algorithms can output mean estimates which achieve error at most O(ε log 1/ε) in ℓ 2 , which is information-theoretically optimal up to the log 1/ε factor. However, these mean estimation algorithms heavily rely on knowing that the covariance is equal (or very close) to the identity. When the distribution is a general sub-Gaussian distribution with unknown covariance, the best known error achieved by an efficient algorithm is O(ε 1/2 ) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b61">61]</ref>. Under a slightly stronger assumption, our algorithm is able to achieve O(ε 1-1/t ) error in polynomial time, for arbitrarily large t ∈ N, and error O(ε log 1/ε) in quasipolynomial time for distributions with O(log 1/ε) bounded moments.</p><p>SoS algorithms for unsupervised learning. SoS algorithms for unsupervised learning obtain the best known polynomial-time guarantees for many problems, including dictionary learning, tensor completion, and others <ref type="bibr">[7-9, 31, 36, 49, 56]</ref>. While the running times of such algorithms are often large polynomials, due to the need to solve large SDPs, insights from the SoS algorithms have often been used in later works obtaining fast polynomial running times <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b59">59]</ref>. This lends hope that in light of our results there is a practical algorithm to learn mixture models under separation k 1/4-ε for some ε &gt; 0.</p><p>Concurrent work. Finally, we note that concurrent and independent works by several groups <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref> have either obtained results or developed techniques similar to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TECHNIQUES</head><p>In this section we give a high-level overview of the main ideas in our algorithms. First, we describe the proofs-to-algorithms methodology developed in recent work on SoS algorithms for unsupervised learning problems. Then we describe the core of our algorithms for mixture models and robust estimation: a simple proof of identifiability of the mean of a distribution D on R d from samples X 1 , . . . , X n when some fraction of the samples may not be from D at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proofs to Algorithms for Machine Learning: the SoS Method</head><p>The Sum of Squares (SoS) hierarchy is a powerful tool in optimization, originally designed to approximately solve systems of polynomial equations via a hierarchy of increasingly strong but increasingly large semidefinite programming (SDP) relaxations (see <ref type="bibr" target="#b9">[10]</ref> and the references therein). There has been much recent interest in using the SoS method to solve unsupervised learning problems in generative models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b56">56]</ref>. . By now there is an established method for desgining such SoSbased algorithms, which we employ in this paper. Consider a generic statistical estimation setting: there is a vector θ * ∈ R k of parameters, and given some samples x 1 , . . . , x n ∈ R d sampled iid according to p(x | θ * ), one wants to recover some θ (x 1 , . . . , x n ) such that ∥θ * -θ ∥ ≤ δ (for some appropriate norm ∥ • ∥ and δ ≥ 0). One says that θ * is identifiable from x 1 , . . . , x n if, for any θ with ∥θ * -θ ∥ &gt; δ , one has Pr(x 1 , . . . , x n | θ ) ≪ Pr(x 1 , . . . , x n | θ * ). Often mathematical arguments for identifiability proceed via concentration of measure arguments culminating in a union bound over every possible θ with ∥θ * -θ ∥ &gt; δ . Though this would imply θ could be recovered via brute-force search, this type of argument generally has no implications for efficient algorithms.</p><p>The SoS proofs-to-algorithms method prescribes designing a simple proof of identifiability of θ from samples x 1 , . . . , x n . Here "simple" has a formal meaning: the proof should be captured by the low-degree SoS proof system. The SoS proof system can reason about equations and inequalities among low-degree polynomials. Briefly, if p(y 1 , . . . , y m ) and q(y 1 , . . . , y m ) are polynomials with real coefficients, and for every y ∈ R m with p(y) ≥ 0 it holds also that q(y) ≥ 0, the SoS proof system can deduce that p(y) ≥ 0 implies q(y) ≥ 0 if there is a simple certificate of this implication: polynomials r (y), s(y) which are sums-of-squares, such that q(y) = r (y) • q(y) + s(y). (Then r, s form an SoS proof that p(y) ≥ 0 implies q(y) ≥ 0.) Remarkably, many useful polynomial inequalities have such certificates. For example, the usual proof of the Cauchy-Schwarz inequality ⟨y, z⟩ 2 ≤ ∥y∥ 2 ∥z ∥ 2 , where y, z are m-dimensional vectors, actually shows that the polynomial ∥y ∥ 2 ∥z ∥ 2 -⟨y, z⟩ 2 is a sum-ofsquares in y and z. The simplicity of the certificate is measured by the degree of the polynomials r and s; when these polynomials have small (usually constant) degree there is hope of transforming SoS proofs into polynomial-time algorithms. This transformation is possible because (under mild assumptions on p and q) the set of low-degree SoS proofs is in fact captured by a polynomial-size semidefinite program.</p><p>Returning to unsupervised learning, the concentration/unionbound style of identifiability proofs described above are almost never captured by low-degree SoS proofs. Instead, the goal is to design (1) A system of constant-degree polynomial equations and inequalties</p><formula xml:id="formula_5">A = {p 1 (θ ) = 0, . . . , p m (θ ) = 0, q 1 (θ ) ≥ 0, . . . , q m (θ ) ≥ 0},</formula><p>where the polynomials p and q depend on the samples x 1 , . . . , x n such that with high probability θ * satisfies all the equations and inequalities. (2) A low-degree SoS proof that A implies ∥θ -θ * ∥ ≤ δ for some small δ and appropriate norm ∥ • ∥.</p><p>Clearly these imply that any solution θ of A also solves the unsupervised learning problem. It is in general NP-hard to find a solution to a system of low-degree polynomial equations and inequalities. However, the SoS proof ( <ref type="formula" target="#formula_8">2</ref>) means that such a search can be avoided. Instead, we will relax the set of solutions θ to A to a simple(er) convex set: the set of pseudodistributions satisfying A. We define pseudodistributions formally later, for now saying only that they are the convex duals of SoS proofs which use the axioms A. By this duality, the SoS proof (2) implies not only that any solution θ to A is a good choice of parameters but also that a good choice of parameters can be extracted from any pseudodistribution satisfying A. (We are glossing over for now that this last step requires some SDP rounding algorithm, since we use only standard rounding algorithms in this paper.) Thus, the final SoS algorithms from this method take the form: solve an SDP to find a pseudodistribution which satisfies A and round it to obtain a estimate θ of θ * . To analyze the algorithm, use the SoS proof (2) to prove that ∥ θθ * ∥ ≤ δ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hölder's Inequality and Identifiability from</head><p>Higher Moments</p><p>Now we discuss the core ideas in our simple SoS identifiability proofs. We have not yet formally defined SoS proofs, so our goal will just be to construct identifiability proofs which are (a) phrased in terms of inequalities of low-degree polynomials and (b) provable using only simple inequalities, like Cauchy-Schwarz and Hölder's inequalities, leaving the formalities for later.</p><p>We consider an idealized version of situations we encounter in both the mixture model and robust estimation settings. Let µ * ∈ R d . Let X 1 , . . . , X n ∈ R d have the guarantee that for some T ⊆ [n] of size |T | = αn, the vectors {X i } i ∈T are iid samples from N (µ * , Id), a spherical Gaussian centered at µ * ; for the other vectors we make no assumption. The goal is to estimate the mean µ * .</p><p>The system A of polynomial equations and inequalities we employ will be designed so that a solution to A corresponds to a subset of samples S ⊆ [n] of size |S | = |T | = αn. We accomplish this by identifying S with its 0/1 indicator vector in R n (this is standard). The inequalities in A will enforce the following crucial moment property on solutions: if µ = 1 |S | i ∈S X i is the empirical mean of samples in S and t ∈ N, then</p><formula xml:id="formula_6">1 |S | i ∈S ⟨X i -µ, u⟩ t ≤ 2 • t t /2 • ∥u ∥ t for all u ∈ R d . (1)</formula><p>This inequality says that every one-dimensional projection u of the samples in S, centered around their empirical mean, has a sub-Gaussian empirical t-th moment. (The factor 2 accounts for deviations in the t-th moments of the samples.) By standard concentration of measure, if αn ≫ d t the inequality holds for S = T . It turns out that this property can be enforced by polynomials of degree t.</p><p>(Actually our final construction of A will need to use inequalities of matrix-valued polynomials but this can be safely ignored here.) Intuitively, we would like to show that any S which satisfies A has empirical mean close to µ * using a low-degree SoS proof,. This is in fact true when α = 1 -ε for small ε, which is at the core of our robust estimation algorithm. However, in the mixture model setting, when α = 1/(# of components), for each component j there is a subset T j ⊆ [n] of samples from component j which provides a valid solution S = T j to A. The empirical mean of T j is close to µ j and hence not close to µ i for any i j.</p><p>We will prove something slightly weaker, which still demonstrates the main idea in our identifiability proof. Lemma 2.1. With high probability, for every S ⊆ [n] which satisfies</p><formula xml:id="formula_7">(1), if µ = 1 |S | i ∈S X i is the empirical mean of samples in S, then ∥µ -µ * ∥ ≤ 4t 1/2 • (|T |/|S ∩ T |) 1/t .</formula><p>Notice that a random S ⊆ [n] of size αn will have |S ∩ T | ≈ α 2 n. In this case the lemma would yield the bound ∥µ -µ * ∥ ≤ 4t 1/2 α 1/t . Thinking of α ≪ 1/t, this bound improves exponentially as t grows. In the d-dimensional k-component mixture model setting, one has 1/α = poly(k), and thus the bound becomes ∥µ -µ * ∥ ≤ 4t 1/2 • k O (1/t ) . In a mixture model where components are separated by k ε , such an estimate is nontrivial when ∥µ -µ * ∥ ≪ k ε , which requires t = O(1/ε). This is the origin of the quantitative bounds in our mixture model algorithm.</p><p>We turn to the proof of Lemma 2.1. As we have already emphasized, the crucial point is that this proof will be accomplished using only simple inequalities, avoiding any union bound over all possible subsets S.</p><p>Proof of Lemma 2.1. Let w i be the 0/1 indicator of i ∈ S. To start the argument, we expand in terms of samples:</p><formula xml:id="formula_8">|S ∩ T | • ∥µ -µ * ∥ 2 = i ∈T w i ∥µ -µ * ∥ 2 = i ∈T w i ⟨µ * -µ, µ * -µ⟩<label>(2)</label></formula><formula xml:id="formula_9">= i ∈T w i ⟨X i -µ, µ * -µ⟩ + ⟨µ * -X i , µ * -µ⟩ .<label>(3)</label></formula><p>The key term to bound is the first one; the second amounts to a deviation term. By Hölder's inequality and for even t,</p><formula xml:id="formula_10">i ∈T w i ⟨X i -µ, µ * -µ⟩ ≤ i ∈T w i t -1 t • i ∈T w i ⟨X i -µ, µ * -µ⟩ t 1/t ≤ i ∈T w i t -1 t • i ∈[n] w i ⟨X i -µ, µ * -µ⟩ t 1/t ≤ i ∈T w i t -1 t • 2t 1/2 • ∥µ * -µ ∥ = |S ∩ T | t -1 t • 2t 1/2 • ∥µ * -µ ∥ .</formula><p>The second line follows by adding the samples from [n] \ T to the sum; since t is even this only increases its value. The third line uses the moment inequality (1). The last line just uses the definition of w.</p><p>For the second, deviation term, we use Hölder's inequality again:</p><formula xml:id="formula_11">i ∈T w i ⟨µ * -X i , µ * -µ⟩ ≤ i ∈T w i t -1 t • i ∈T ⟨µ * -X i , µ * -µ⟩ t 1/t . The distribution of µ * -X i for i ∈ T is N (0, Id). By standard matrix concentration, if |T | = αn ≫ d t , i ∈T (X i -µ * ) ⊗t /2 (X i -µ * ) ⊗t /2 ⊤ ⪯ 2|T | E Y ∼N(0,Id) Y ⊗t /2 Y ⊗t /2 ⊤</formula><p>with high probability and hence, using the quadratic form at</p><formula xml:id="formula_12">(µ * - µ) ⊗t /2 , i ∈T ⟨µ * -X i , µ * -µ⟩ t ≤ 2|T |t t /2 • ∥µ * -µ ∥ t .</formula><p>Putting these together and simplifying constants, we have obtained that with high probability,</p><formula xml:id="formula_13">|S ∩ T | • ∥µ -µ * ∥ 2 ≤ 4t 1/2 |T | 1/t • |S ∩ T | (t -1)/t • ∥µ -µ * ∥ which simplifies to |S ∩ T | 1/t • ∥µ -µ * ∥ ≤ 4t 1/2 |T | 1/t . □</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">From Identifiability to Algorithms</head><p>We now discuss how to use the ideas described above algorithmically for learning well-separated mixture models. The high level idea for robust estimation is similar. Given Lemma 2.1, a naive algorithm for learning mixture models would be the following: find a set of points T of size roughly n/k that satisfy the moment bounds described, and simply output their empirical mean. Since by a simple counting argument this set must have nontrivial overlap with the points from some mixture component, Lemma 2.1 guarantees that the empirical mean is close to mean of this component. However, in general finding such a set of points is algorithmically difficult. In fact, it would suffice to find a distribution over such sets of points (since then one could simply sample from this distribution), however, this is just as computationally difficult. The critical insight is that because of the proof of Lemma 2.1 only uses facts about low degree polynomials, it suffices to find an object which is indistinguishable from such a distribution, considered as a functional on low-degree polynomials.</p><p>The natural object in this setting is a pseudo-distribution. Pseudodistributions form a convex set, and for a set of low-degree polynomial equations and inequalities A, it is possible to find a pseudodistribution which is indistinguishable from a distribution over solutions to A (as such a functional) in polynomial time via semidefinite programming (under mild assumptions on A). More specifically, the set of SoS proofs using axioms A is a semidefinite program (SDP), and the above pseudodistributions form the dual SDP. (We will make these ideas more precise in the next two sections.)</p><p>Our algorithm then proceeds via the following general framework: find an appropriate pseudodistribution via convex optimization, then leverage our low-degree sum of squares proofs to show that information about the true clusters can be extracted from this object by a standard SDP rounding procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>Throughout the paper we let d be the dimensionality of the data, and we will be interested in the regime where d is at least a large constant. We also let ∥v ∥ denote the ℓ 2 norm of a vector v, and ∥M ∥ F to denote the Frobenius norm of a matrix M; often we just write ∥M ∥. We will also give randomized algorithms for our problems that succeed with probability 1poly(1/k, 1/d); by standard techniques this probability can be boosted to 1 -ξ by increasing the sample and runtime complexity by a mulitplicative log 1/ξ .</p><p>We now formally define the class of distributions we will consider throughout this paper. At a high level, we will consider distributions which have bounded moments, for which there exists a low degree SoS proof of this moment bound. Formally: Definition 3.1. Let D be a distribution over R d with mean µ. For c ≥ 1, t ∈ N, we say that D is t-explicitly bounded with variance proxy σ if for every even s ≤ t there is a degree s SoS proof (see Section 3.1 for a formal definition) of</p><formula xml:id="formula_14">⊢ s E Y ∼D k ⟨(Y -µ) , u⟩ s ≤ (σs) s/2 ∥u ∥ s .</formula><p>Equivalently, the polynomial p(u)</p><formula xml:id="formula_15">= (σs) s/2 ∥u ∥ s - E Y ∼D k ⟨(Y -µ)</formula><p>, u⟩ s should be a sum-of-squares. In our typical use case, σ = 1, we will omit it and call the distribution t-explicitly bounded.</p><p>Throughout this paper, since all of our problems are scale invariant, we will assume without loss of generality that σ = 1. This class of distributions captures a number of natural classes of distributions. Intuitively, if u were truly a vector in R k (rather than a vector of indeterminants), then this exactly captures sub-Gaussian type moment. Our requirement is simply that these types of moment bounds not only hold, but also have a SoS proof.</p><p>We remark that our results also hold for somewhat more general settings. It is not particularly important that the s-th moment bound has a degree s proof; our techniques can tolerate degree O(s) proofs.</p><p>Our techniques also generally apply for weaker moment bounds. For instance, our techniques naturally extend to explicitly bounded sub-exponential type distributions in the obvious way. We omit these details for simplicity.</p><p>As we show in the full version of this paper, this class still captures many interesting types of nice distributions, including Gaussians, product distributions with sub-Gaussian components, and rotations therof. With this definition in mind, we can now formally state the problems we consider in this paper:</p><p>Learning well-separated mixture models. We first define the class of mixture models for which our algorithm works:</p><formula xml:id="formula_16">Definition 3.2 (t-explicitly bounded mixture model with separa- tion ∆). Let µ 1 , . . . , µ k ∈ R d satisfy ∥µ i -µ j ∥ &gt; ∆ for every i j, and let D 1 , . . . , D k have means µ 1 , . . . , µ k , so that each D i is t-explicitly bounded. Let λ 1 , . . . , λ k ≥ 0 satisfy i ∈[k ] λ i = 1.</formula><p>Together these define a mixture distribution on R d by first sampling i ∼ λ, then sampling x ∼ D i .</p><p>The problem is then: Problem 3.1. Let D be a t-explicitly bounded mixture model in R d with separation ∆ with k components. Given k, ∆, and n independent samples from D, output µ 1 , . . . , µ m so that with probability at least 0.99, there exists a permutation π :</p><formula xml:id="formula_17">[k] → [k] so that ∥µ i -µ π (i) ∥ ≤ δ for all i = 1, . . . , k.</formula><p>Robust mean estimation. We consider the same basic model of corruption introduced in <ref type="bibr" target="#b22">[23]</ref>. Definition 3.3 (ε-corruption). We say a set of samples X 1 , . . . , X n is ε-corrupted from a distribution D if they are generated via the following process. First, n independent samples are drawn from D. Then, an adversary changes εn of these points arbitrarily, and the altered set of points is then returned to us in an arbitrary order.</p><p>The problem we consider in this setting is the following: Problem 3.2 (Robust mean estimation). Let D be an O(t)-explicitly bounded distribution over R d wih mean µ. Given t, ε, and an εcorrupted set of samples from D, output µ satisfying ∥µ -µ ∥ ≤ O(ε 1-1/t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The SoS Proof System</head><p>We refer the reader to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b54">54]</ref> and the references therein for a thorough exposition of the SoS algorithm and proof system; here we only define what we need. <ref type="foot" target="#foot_4">7</ref>Let x 1 , . . . , x n be indeterminates and A be the set of polynomial equations and inequalities {p 1 (x) ≥ 0, . . . , p m (x) ≥ 0, q 1 (x) = 0, . . . , q m (x) = 0}. We say that the statement p(x) ≥ 0 has an SoS proof if there are polynomials {r α } α ⊆[m] (where α may be a multiset) and</p><formula xml:id="formula_18">{s i } i ∈[m] such that p(x) = α r α (x) • i ∈α p i (x) + i ∈[m] s i (x)q i (x)</formula><p>and each polynomial r α (x) is a sum of squares.</p><p>(4) (Booleanness) A includes the equations w<ref type="foot" target="#foot_5">2</ref> i = w i for all i ∈ [n].</p><p>(5) (Size) A includes the inequalities (1 -τ )αn ≤ w i ≤ (1 + τ )αn.</p><p>(6) (Empirical mean) A includes the equation µ</p><formula xml:id="formula_19">• i ∈[n] w i = i ∈[n] w i X i .</formula><p>In particular this implies that A ⊢ O (t ) A.</p><p>The proof of Lemma 4.1 can be found in the full version of this work.</p><p>Remark 4.1 (Numerical accuracy, semidefinite programming, and other monsters). We pause here to address issues of numerical accuracy. Our final algorithms use point 2 in Lemma 4.1 (itself implemented using semidefinite programming) to obtain a pseudodistribution Ẽ satisfying A approximately, up to error η = 2 -n in the following sense: for every r a sum of squares and f 1 , . . . ,</p><formula xml:id="formula_20">f ℓ ∈ A with deg [r • f i ≤ Ct], one has Ẽ r • i ∈A f ≥ -η • ∥r ∥,</formula><p>where ∥r ∥ is ℓ 2 norm of the coefficients of r . Our main analyses of this pseudodistribution employ the implication A ⊢ B for another family of inequalities B to conclude that if Ẽ satisfies A then it satisfies B, then use the latter to analyze our rounding algorithms. Because all of the polynomials eventually involved in the SoS proof A ⊢ B have coefficients bounded by n B for some large constant B, it may be inferred that if Ẽ approximately satisfies A in the sense above, it also approximately satisfies B, with some error η ′ ≤ 2 -Ω(n) . The latter is a sufficient for all of our rounding algorithms.</p><p>Aside from mentioning at a couple key points why our SoS proofs have bounded coefficients, we henceforth ignore all numerical issues. For further discussion of numerical accuracy and well-conditioned-ness issues in SoS, see <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b57">57]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MIXTURE MODELS: ALGORITHM AND ANALYSIS</head><p>In this section we formally describe and analyze our algorithm for mixture models. We prove the following theorem.</p><p>Theorem 5.1 (Main theorem on mixture models). For every large-enough t ∈ N there is an algorithm with the following guarantees. Let µ 1 , . . . , 1) samples from the mixture model given by λ 1 , . . . , λ k , D 1 , . . . , D k , the algorithm runs in time n O (t ) and with high probability returns { μ1 , . . . , μk } (not necessarily in that order) such that</p><formula xml:id="formula_21">µ k ∈ R d , satisfy ∥µ i -µ j ∥ ≥ ∆. Let D 1 , . . . , D k be 10t-explicitly bounded, with means µ 1 , . . . , µ k . Let λ 1 , . . . , λ k ≥ 0 satisfy λ i = 1. Given n ≥ (d t k) O (1) • (max i ∈[m] 1/λ i ) O (</formula><formula xml:id="formula_22">∥µ i -μi ∥ ≤ 2 Ct m C t t /2 ∆ t -1</formula><p>for some universal constant C.</p><p>In particular, we note two regimes: if ∆ = k γ for a constant γ &gt; 0, choosing t = O(1/γ ) we get that the ℓ 2 error of our estimator is poly(1/k) for any O(1/γ )-explicitly bounded distribution, and our estimator requires only (dk) O (1) samples and time. This matches the guarantees of Theorem 1.1.</p><p>On the other hand, if ∆ = C ′ log k (for some universal C ′ ) then taking t = O(log k) gives error </p><formula xml:id="formula_23">∥µ i -μi ∥ ≤ k O (1) • √ t ∆ t which,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Algorithm and Main Analysis</head><p>We formally describe our mixture model algorithm now. We use the following lemma, which we prove in Section 5.6. The lemma says that given a matrix which is very close, in Frobenious norm, to the 0/1 indicator matrix of a partition of [n] it is possible to approximately recover the partition. (The proof is standard.)  By semidefinite programming (see Lemma 4.1, item 2), find a pseudoexpectation of degree O(t) which satisfies the structured subset polynomials from Lemma 4.1, with α = n/m such that ∥ Ẽ ww ⊤ ∥ F is minimized among all such pseudoexpectations.  Run the algorithm EstimateMean from the full version of this paper on each cluster C i , with ε = 2 Ct t t /2 m 4 /∆ t for some universal constant C to obtain a list of mean estimates μ1 , . . . , μm . </p><formula xml:id="formula_24">t ∥a -u ∥ t + ∥b -u ∥ t ≥ 2 -t • ∆ t .</formula><p>The last lemma helps put the previous two together. Although we have phrased this lemma to concorde with the mixture model setting, we note that the proof uses nothing about mixture models and consists only of generic manipulations of pseudodistributions. Lemma 5.7. Let µ 1 , . . . , µ k , D 1 , . . . , D k , X 1 , . . . , X n be as in Theorem 5.1. Let a j be the 0/1 indicator for the set of samples drawn from D j . Suppose Ẽ is a degree-O(t) pseudodistribution which satisfies</p><formula xml:id="formula_25">⟨a j , w⟩ ≤ n ⟨a ℓ , w⟩ ≤ n ∥µ -µ j ∥ 2t + ∥µ -µ ℓ ∥ 2t ≥ A ⟨a j , w⟩ t ∥µ -µ j ∥ 2t ≤ Bn⟨a j , w⟩ t -1 ∥µ -µ j ∥ t ⟨a ℓ , w⟩ t ∥µ -µ ℓ ∥ 2t ≤ Bn⟨a ℓ , w⟩ t -1 ∥µ -µ ℓ ∥ t for some scalars A, B ≥ 0. Then Ẽ⟨a j , w⟩⟨a ℓ , w⟩ ≤ 2n 2 B √ A .</formula><p>Now we have the tools to prove Lemma 5.3.</p><p>Proof of Lemma 5.3. We will verify the conditions to apply Lemma 5.7. By Lemma 5.5, when (D1) holds, the pseudoexpectation Ẽ satisfies ⟨a j , w⟩ t ∥µ -µ j ∥ 2t ≤ Bn⟨a j , w⟩ t -1 ∥µ -µ j ∥ t for B = 4(4t) t /2 /k, and similarly with j, ℓ interposed. Similarly, by separation of the empirical means, Ẽ satisfies ∥µ -µ j ∥ 2t + ∥µµ ℓ ∥ 2t ≥ A for A = 2 -2t ∆ 2t , recalling that the empirical means are pairwise separated by at least ∆ -2∆ -t . Finally, clearly A ⊢ O (1) ⟨a j , w⟩ ≤ n and similarly for ⟨a ℓ , w⟩. So applying Lemma 5.7 we get Ẽ⟨a</p><formula xml:id="formula_26">j , w⟩⟨a ℓ , w⟩ ≤ 2n 2 B √ A ≤ n 2 2 2t +2 t t /2 k • 1 ∆ t . □</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Proof of Lemma 5.5</head><p>In this subsection we prove Lemma 5.5. We use the following helpful lemmata. The first bounds error from samples selected from the wrong cluster using the moment inequality.</p><p>Lemma 5.8. Let j, A, X 1 , . . . , X n , µ j , µ j be as in Lemma 5.5. Then</p><formula xml:id="formula_27">A ⊢ O (t ) i ∈S j w i ⟨µ -X i , µ -µ j ⟩ t ≤ 2t t /2 • ⟨a j , w⟩ t -1 ∥µ -µ j ∥ t .</formula><p>Proof. The proof goes by Hölder's inequality followed by the moment inequality in A. Carrying this out, by SoS Hölder's inequality (see full version) and evenness of t,</p><formula xml:id="formula_28">{w 2 i = w i } ⊢ O (t ) i ∈S j w i ⟨µ -X i , µ -µ j ⟩ t ≤ i ∈S j w i t -1 • i ∈[n] w i ⟨µ -X i , µ -µ j ⟩ t .</formula><p>Then, using the main inequality in A,</p><formula xml:id="formula_29">A ⊢ O (t ) i ∈S j w i t -1 • 2t t /2 • ∥µ -µ j ∥ t = 2t t /2 • ⟨a j , w⟩ t -1 ∥µ -µ j ∥ t . □</formula><p>The second lemma bounds error from deviations in the empirical t-th moments of the samples from the j-th cluster. Lemma 5.9. Let µ 1 , . . . , µ k , D 1 , . . . , D k be as in Theorem 5.1. Suppose condition (D1) holds for samples X 1 , . . . , X n . Let w 1 , . . . , w n be indeterminates. Let u = u 1 , . . . , u d be an indeterminate. Then for every</p><formula xml:id="formula_30">j ∈ [k], {w 2 i = w i } ⊢ O (t ) i ∈S j w i ⟨X i -µ j , u⟩ t ≤ ⟨a j , w⟩ t -1 • 2 • n k • ∥u ∥ t .</formula><p>Proof. The first step is Hölder's inequality again:</p><formula xml:id="formula_31">{w 2 i =w i } ⊢ O (t ) i ∈S j w i ⟨X i -µ j , u⟩ t ≤ ⟨a j , w⟩ t -1 • i ∈S j ⟨X i -µ j , u⟩ t .</formula><p>Finally, condition (D1) yields</p><formula xml:id="formula_32">{w 2 i =w i } ⊢ O (t ) i ∈S j w i ⟨X i -µ j , u⟩ t ≤ ⟨a j , w⟩ t -1 • 2 • n k • ∥u ∥ t . □</formula><p>We can prove Lemma 5.5 by putting together Lemma 5.8 and Lemma 5.9.</p><p>Proof of Lemma 5.5. Let j ∈ [k] be a cluster and recall a j ∈ {0, 1} n is the 0/1 indicator for the samples in cluster j. Let S j be the samples in the j-th cluster, with empirical mean µ j . We begin by writing ⟨a j , w⟩∥µ -µ j ∥ 2 in terms of samples X 1 , . . . , X n .</p><formula xml:id="formula_33">⟨a j , w⟩∥µ -µ j ∥ 2 = i ∈[n] w i ⟨µ -µ j , µ -µ j ⟩ = i ∈S j w i ⟨µ -X i , µ -µ j ⟩ + i ∈[n] w i ⟨X i -µ j , µ -µ j ⟩ . Hence, using (a + b) t ≤ 2 t (a t + b t ), we obtain ⊢ O (t ) ⟨a j , w⟩ t ∥µ -µ j ∥ 2t ≤2 t • i ∈S j w i ⟨µ -X i , µ -µ j ⟩ t + 2 t • i ∈S j w i ⟨X i -µ j , µ -µ j ⟩ t .</formula><p>Now using Lemma 5.8 and Lemma 5.9,</p><formula xml:id="formula_34">A ⊢ O (t ) ⟨a j , w⟩ t ∥µ -µ j ∥ 2t ≤ 2 t +2 t t /2 • n k • ⟨a j , w⟩ t -1 • ∥µ -µ j ∥ t</formula><p>as desired. □</p><p>5.5 Proof of Lemma 5.7</p><p>We prove Lemma 5.7. The proof only uses standard SoS and pseudodistribution tools. The main inequality we will use is the following version of Hölder's inequality.</p><p>Fact 5.10 (Pseudoexpectation Hölder's, see Lemma A.4 in <ref type="bibr" target="#b6">[7]</ref>). Let p be a degree-ℓ polynomial. Let t ∈ N and let Ẽ be a degree-O(tℓ) pseudoexpectation on indeterminates x. Then</p><formula xml:id="formula_35">Ẽ p(x) t -2 ≤ Ẽ p(x) t t -2 t .</formula><p>Now we can prove Lemma 5.7.</p><p>Proof of Lemma 5.7. We first establish the following inequality. Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t ∥µ -µ j ∥ 2t ≤ B 2 n 2 • Ẽ⟨a j , w⟩ t -2 ⟨a ℓ , w⟩ t . (4) (The inequality will also hold by symmetry with j and ℓ exchanged.) This we do as follows:</p><p>Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t ∥µ -µ j ∥ 2t ≤ Bn Ẽ⟨a j , w⟩ t -1 ⟨a ℓ , w⟩ t ∥µ -µ j ∥ t ≤ Bn Ẽ⟨a j , w⟩ t -2 ⟨a ℓ , w⟩ t 1/2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Rounding</head><p>In this section we state and analyze our second-moment round algorithm. As have discussed already, our SoS proofs in the mixture model setting are quite strong, meaning that the rounding algorithm is relatively naive. The setting in this section is as follows. Let n, m ∈ N with m ≪ n. There is a ground-truth partition of [n] into m parts S 1 , . . . , S m such that |S i | = (1 ± δ ) n m . Let A ∈ R n×n be the 0/1 indicator matrix for this partition, so A i j = 1 if i, j ∈ S ℓ for some ℓ and is 0 otherwise. Let M ∈ R n×n be a matrix such that ∥M -A∥ ≤ εn, where ∥ • ∥ is the Frobenious norm. The algorithm takes M and outputs a partition  To get started analyzing the algorithm, we need a definition. Definition 5.1. For cluster S j , let a j ∈ R n be its 0/1 indicator vector. If i ∈ S j , we say it is E-good if ∥v i -a j ∥ 2 ≤ n/E, and otherwise E-bad, where v i is the i-th row of M. Let I д ⊆ [n] denote the set of E-good indices and I b denote the set of E-bad indices. (We will choose E later.) For any j = 1, . . . , k, let I д, j = I д ∩ S j denote the set of good indices from cluster j.</p><p>We have: Lemma 5.12. Suppose E as in RoundSecondMoments satisfies E ≥ m/8. Suppose that in iterations 1, . . . , m, RoundSecondMoments has chosen only good vectors. Then, there exists a permutation π : [m] → [m] so that C ℓ = I д, π (ℓ) ∪ B ℓ , where B ℓ ⊆ I b for all ℓ.</p><p>Proof. We proceed inductively. We first prove the base case. WLOG assume that the algorithm picks v 1 , and that v 1 is good, and is from component j. Then, for all i ∈ I д, j , by the triangle inequality we have ∥v i -v 1 ∥ 2 ≤ 2 n 1/2 B , and so I д, j ⊆ C 1 . Moreover, if i ∈ I д, j ′ for some j ′ j, we have</p><formula xml:id="formula_36">∥v i -v 1 ∥ 2 ≥ ∥a ′ j -a j ∥ 2 -2 n 1/2 E 1/2 ≥ n 1/2 √ m -2 n 1/2 E 1/2 &gt; 2 n 1/2 E 1/2 ,</formula><p>and so in this case i C 1 . Hence C 1 = I д, j ∪ B 1 for some B 1 ⊆ I b . Inductively, suppose that if the algorithm chooses good indices in iterations 1, . . . , a -1, then there exist distinct j 1 , . . . , j a-1 so that C ℓ = I д, j ℓ ∪ B ℓ for B ℓ ⊆ I b . We seek to prove that if the algorithm chooses a good index in iteration a, then C a = I д, j a ∪ B a for some j a {j 1 , . . . , j a-1 } and B a ⊆ I b . Clearly by induction this proves the Lemma. WLOG assume that the algorithm chooses v 1 in iteration a. Since by assumption 1 is good, and we have removed I д ℓ for ℓ = 1, . . . , a -1, then 1 ∈ I д, j a for some j a {j 1 , . . . , j a-1 }.</p><p>Then, the conclusion follows from the same calculation as in the base case. □ Proof. By Lemma 5.13, in the first iteration the probability that a bad vector is chosen is at most ε 2 E. Conditioned on the event that in iterations 1, . . . , a the algorithm has chosen good vectors, then by Lemma 5.12, there is at least one j a so that no points in I д, j a have been removed. Thus at least (1 -δ )n/m vectors remain, and in total there are at most ε 2 En bad vectors, by Lemma 5.13. So, the probability of choosing a bad vector is at most ε 2 Em. Therefore, by the chain rule of conditional expectation and our assumption , the probability we never choose a bad vector is at least </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>for large-enough C ′ and t, can be made 1/poly(k). Thus for ∆ = C ′ log k and any O(log k)-explicitly bounded distribuion we obtain error 1/poly(k) with d O (log k) samples and d O (log k ) 2 time. In this section we describe and analyze our algorithm. To avoid some technical work we analyze the uniform mixtures setting, with λ i = 1/m. In the full version of this work we describe how to adapt the algorithm to the nonuniform mixture setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma 5 . 2 (Algorithm 1</head><label>521</label><figDesc>Second moment rounding, follows from Theorem 5.11). Let n, m ∈ N with m ≪ n. There is a polynomial time algorithm RoundSecondMoments with the following guarantees. Suppose S 1 , . . . , S m partition [n] into m pieces, each of sizen 2m ≤ |S i | ≤ 2nm . Let A ∈ R n×n be the 0/1 indicator matrix for the partition S; that is, A i j = 1 if i, j ∈ S ℓ for some ℓ and is 0 otherwise. Let M ∈ R n×n be a matrix with ∥A -M ∥ F ≤ εn. Given M, with probability at least 1 -ε 2 m 3 the algorithm returns a partition C 1 , . . . , C m of [n] such that up to a global permutation of [m], C i = T i ∪B i , where T i ⊆ S i and |T i | ≥ |S i | -ε 2 m 2 n and |B i | ≤ ε 2 m 2 n. Mixture Model Learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>function LearnMixtureMeans(t, X 1 , . . . , X n , δ, τ ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 : 4 :</head><label>34</label><figDesc>Let M ← m • Ẽ ww ⊤ .Run the algorithm RoundSecondMoments on M to obtain a partition C 1 , . . . , C m of [n].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>6 :</head><label>6</label><figDesc>Output μ1 , . . . , μm . 7: end function Remark 5.1 (On the use of EstimateMean). As described, Learn-MixtureMeans has two phases: a clustering phase and a meanestimation phase. The clustering phase is the heart of the algorithm; we will show that after running RoundSecondMoments the algorithm has obtained clusters C 1 , . . . , C k which err from the groundtruth clustering on only aThe second lemma is an SoS triangle inequality, capturing the consequences of separation of the means. The proof is standard; see full version. Lemma 5.6. Let a, b ∈ R k and t ∈ N be a power of 2. Let ∆ = ∥a -b ∥. Let u = (u 1 , . . . , u k ) be indeterminates. Then ⊢</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>C 1 , . . . , C m of [m] which makes few errors compared to S 1 , . . . , S m . Algorithm 2 Rounding the second moment of Ẽ[ww ⊤ ]1: function RoundSecondMoments(M ∈ R n×n , E ∈ R) 2: Let S = [n]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 : 4 : 5 : 1 return</head><label>3451</label><figDesc>Let v 1 , . . . , v n be the rows of M for ℓ = 1, . . . , m do Choose i ∈ S uniformly at random 6:LetC ℓ = i ′ ∈ S : ∥v i -v i ′ ∥ 2 ≤2 n The clusters C 1 , . . . , C m . 10: end function We will prove the following theorem. Theorem 5.11. With notation as before Algorithm 2 with E = m, with probability at least 1 -ε 2 m 3 Algorithm 2 returns a partition C 1 , . . . , C m of [n] such that (up to a permutation of [m]), C ℓ = T ℓ ∪B ℓ , where T ℓ ⊆ S ℓ has size |T ℓ | ≥ |S ℓ | -ε 2 mn and |B ℓ | ≤ ε 2 mn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Lemma 5 . 13 . 2 FLemma 5 . 14 .</head><label>5132514</label><figDesc>There are at most ε 2 En indices which are E-bad; i.e. |I b | ≤ ε 2 En.Proof. We haveε 2 n 2 ≥ Mi ≤m a i a ⊤ i ≥ j i ∈S j bad ∥v i -a j ∥ 2 2 ≥ n E |I b | ,from which the claim follows by simplifying. □This in turns implies: With probability at least 1 -ε 2 m 3 , the algorithm RoundSecondMoments chooses good indices in all k iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 -ε 2</head><label>2</label><figDesc>Em m Choosing E = m this is (1 -ε 2 m 2 ) m ≥ 1 -ε 2 m 3 . as claimed.□ Now Theorem 5.11 follows from putting together the lemmas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t ∥µ -µ j ∥ 2t 1/2 where the first line is by assumption on Ẽ and the second is by pseudoexpectation Cauchy-Schwarz. Rearranging gives the inequality (4). Now we use this to bound Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t . By hypothesis, Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t (∥µ -µ j ∥ 2t + ∥µ -µ ℓ ∥ 2t ) ,which, followed by (4) gives Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t≤ 1 A • B 2 n 2 • Ẽ ⟨a j , w⟩ t -2 ⟨a ℓ , w⟩ t + ⟨a ℓ , w⟩ t -2 ⟨a j , w⟩ t .Using ⟨a j , w⟩, ⟨a ℓ , w⟩ ≤ n, we obtainẼ⟨a j , w⟩ t ⟨a ℓ , w⟩ t ≤ 2 A • B 2 n 4 • Ẽ⟨a j , w⟩ t -2 ⟨a ℓ , w⟩ t -2 .Raising both sides to the t/2 power gives Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t ≤ 2 t /2 B t n 2t A t /2 , and finally using Cauchy-Schwarz, Ẽ⟨a j , w⟩⟨a ℓ , w⟩ ≤ Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t 1/t</figDesc><table><row><cell>≤ Finally, using Fact 5.10, the right side is at most 1 A 2B 2 n ≤</cell><cell>2n 2 B √ A</cell><cell>. □</cell></row></table><note><p><p>4 </p>/A • Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t (t -2)/t , so cancelling terms we get Ẽ⟨a j , w⟩ t ⟨a ℓ , w⟩ t 2/t ≤ 2B 2 n 4 A .</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A mixture model consists of probability distributions D 1 , . . . , D k on R d and mixing weights λ 1 , . . . , λ k ≥ 0 with i ≤k λ i = 1. The distribution D i has mean µ i . Each sample x j is generated by first sampling a component i ∈ [k ] according to the weights λ, then sampling x j ∼ D i .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Recent and sophisticated arguments show that the means are identifiable (albeit inefficiently) with error depending only on the number of samples and not on the separation ∆ even when ∆ = O ( log k )<ref type="bibr" target="#b58">[58]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Hüber's contamination model essentially only allows the adversary to add corrupted points, but not remove uncorrupted points.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>We remark that this was the state of affairs even for the Hüber contamination model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>Our definition of SoS proofs differs slightly from O'Donnell and Zhou's in that we allow proofs to use products of axioms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_5"><p>O (t )  t t /2 poly(k ) ∆ t -fraction of points. To obtain estimates μi of the underlying means from such a clustering, one simple option is to output the empirical mean of the clusters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>STOC'18, June 25-29, 2018, Los Angeles, CA, USA Samuel B. Hopkins and Jerry Li</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank David Steurer, Daniel Freund, Gautam Kamath, Pablo Parillo, and especially Aravindan Vijayaraghavan for some helpful conversations, and anonymous reviewers of this manuscript for their helpful remarks.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>If the polynomials r α (x) • i ∈α p i (x) and s i (x)q i (x) have degree at most d, we say the proof has degree at most d, and we write A ⊢ d p(x) ≥ 0 .</p><p>SoS proofs compose well, and we frequently use the following without comment. Fact 3.3. If A ⊢ d p(x) ≥ 0 and A ⊢ d ′ q(x) ≥ 0, then A ∪ B ⊢ max(d,d ′ ) p(x) + q(x) ≥ 0 and A ∪ B ⊢ dd ′ p(x)q(x) ≥ 0.</p><p>We turn to the dual objects to SoS proofs. A degree-d pseudoexpectation (for variety we sometimes say "pseudodistribution") is a linear operator Ẽ : R[x] ≤d → R, where R[x] ≤d are the polynomials in indeterminates x with real coefficients, which satisfies the following (1) Normalization: Ẽ[1] = 1 (2) Positivity: Ẽ[p(x) 2 ] ≥ 0 for every p of degree at most d/2. We say that a degree-d pseudoexpectation Ẽ satisfies inequalities and equalities {p 1 (x) ≥ 0, . . . , p m (x) ≥ 0, q 1 (x) = 0, . . . , q m (x) = 0} at degree r ≤ d if <ref type="bibr" target="#b0">(1)</ref> for every multiset α ⊆ [m] and SoS polynomial s(x) such that the degree of s(x) i ∈α p i (x) is at most r , one has Ẽ s(x) i ∈α p i (x) ≥ 0, and (2) for every q i (x) and every polynomial s(x) such that the degree of q i (x)s(x) ≤ r , one has Ẽ s(x)q i (x) = 0. The main fact relating pseudoexpectations and SoS proofs is: Fact 3.4 (Soundness of SoS proofs, informal). If A is a set of equations and inequalities and A ⊢ ℓ p(x) ≥ 0, and Ẽ is a degree d &gt; ℓ pseudodistribution satisfying A at degree d, then Ẽ satisfies A ∪ {p ≥ 0} at degree dℓ. 8   In the full version of this work we state and prove many basic SoS inequalities that we will require throughout the paper.</p><p>Gaussian distributions are explicitly bounded. In the full version of this work we show that product distributions (and rotations thereof) with bounded t-th moments are explicitly bounded. Lemma 3.5. Let D be a distribution over R d so that D is a rotation of a product distribution D ′ where each coordinate X with mean µ of D satisfies</p><p>Then D is t-explicitly bounded (with variance proxy 1).</p><p>(The factors of 1 2 can be removed for many distributions, including Gaussians.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CAPTURING EMPIRICAL MOMENTS WITH POLYNOMIALS</head><p>To describe our algorithms we need to describe a system of polynomial equations and inequalities which capture the following problem: among X 1 , . . . , X n ∈ R d , find a subset of S ⊆ [n] of size αn such that the empirical t-th moments obey a moment bound:</p><p>Let k, n ∈ N and let w = (w 1 , . . . , w n ), µ = (µ 1 , . . . , µ k ) be indeterminates. Let 8 See <ref type="bibr" target="#b10">[11]</ref> for a full account of completeness and soundness of SoS.</p><p>(1) X 1 , . . . , X n ∈ R d (2) α ∈ [0, 1] be a number (the intention is |S | = αn).</p><p>(3) t ∈ N be a power of 2, the order of moments to control (4) µ 1 , . . . , µ k ∈ R d , which will eventually be the means of a k-component mixture model, or when k = 1, the true mean of the distribution whose mean we robustly estimate. ( <ref type="formula">5</ref>) τ &gt; 0 be some error magnitude accounting for fluctuations in the sizes of clusters (which may be safely ignored at first reading).</p><p>Definition 4.1. Let A be the following system of equations and inequalities, depending on all the parameters above.</p><p>(1) w 2 i = w i for all i ∈ [n] (enforcing that w is a 0/1 vector, which we interpret as the indicator vector of the set S).</p><p>(</p><p>i ∈[n] w i ∥µ -µ j ∥ t for every µ j among µ 1 , . . . , µ m . This enforces that the t-th empirical moment of the samples in S is bounded in the direction µµ j .</p><p>Notice that since we will eventually take µ j 's to be unknown parameters we are trying to estimate, the algorithm cannot make use of A directly, since the last family of inequalities involve the µ j 's. Later in this paper we exhibit a system of inequalities which requires the empirical t-th moments to obey a sub-Gaussian type bound in every direction, hence implying the inequalities here without requiring knowledge of the µ j 's to write down. Formally, we will show:</p><p>There is a family A of polynomial equations and inequalities of degree O(t) on variables w = (w 1 , . . . , w n ), µ = (µ 1 , . . . , µ k ) and at most n O (t ) other variables, whose coefficients depend on α, t, τ , X 1 , . . . , X n , such that (1) (Satisfiability) If there S ⊆ [n] of size at least (α -τ )n so that {X i } i ∈S is an iid set of samples from D, and (1-τ )αn ≥ d 100t , then for d large enough, with probability at least 1 -d -8 , the system A has a solution over R which takes w to be the 0/1 indicator vector of S. (2) (Solvability) For every C ∈ N there is an n O (Ct ) -time algorithm which, when A is satisfiable, returns a degree-Ct pseudodistribution which satisfies A (up to additive error 2 -n ).</p><p>(3) (Moment bounds for polynomials of µ) Let f (µ) be a lengthd vector of degree-ℓ polynomials in indeterminates µ = (µ 1 , . . . , µ k ). A implies the following inequality and the implication has a degree tℓ SoS proof.</p><p>9 The condition t ≥ 4 is merely for technical convenience.</p><p>However, without additional pruning this risks introducing error in the mean estimates which grows with the ambient dimension d. By using the robust mean estimation algorithm instead to obtain mean estimates from the clusters we obtain errors in the mean estimates which depend only on the number of clusters k, the between-cluster separation ∆, and the number t of bounded moments.</p><p>Remark 5.2 (Running time). We observe that LearnMixtureMeans can be implemented in time n O (t ) . The main theorem requires n ≥ k O (1) d O (t ) , which means that the final running time of the algorithm is (kd t ) O (t ) . 10   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Proof of Main Theorem</head><p>In this section we prove our main theorem using the key lemmata; in the following sections we prove the lemmata.</p><p>Deterministic Conditions. We recall the setup. There are k mean vectors µ 1 , . . . , µ k ∈ R d , and corresponding distributions D 1 , . . . , D k where D j has mean µ j . The distributions D j are 10texplicitly bounded for a choice of t which is a power of 2. Vectors X 1 , . . . , X n ∈ R d are samples from a uniform mixture of D 1 , . . . , D k . We will prove that our algorithm succeeds under the following condition on the samples X 1 , . . . , X n .</p><p>(D1) (Empirical moments) For every cluster S j = {X i :</p><p>X i is from D j }, the system A from Lemma 4.1 with α = 1/m and τ = ∆ -t has a solution which takes w ∈ {0, 1} n to be the 0/1 indicator vector of S j . (D2) (Empirical means) Let µ j be the empirical mean of cluster S j .</p><p>The µ j 's satisfy ∥µ i -µ i ∥ ≤ ∆ -t .</p><p>We note a few useful consequences of these conditions, especially (D1). First of all, it implies all clusters have almost the same size:</p><p>Second, it implies that all clusters have explicitly bounded moments: for every S j ,</p><p>Lemmas. The following key lemma captures our SoS identifiability proof for mixture models. Lemma 5.3. Let µ 1 , . . . , µ k , D 1 , . . . , D k be as in Theorem 5.1, with mean separation ∆. Suppose (D1), (D2) occur for samples X 1 , . . . , X n . Let t ∈ N be a power of two. Let Ẽ be a degree-O(t) pseudoexpectation which satisfies A from Lemma 4.1 with α = 1/k and τ ≤ ∆ -t . Then for every j, ℓ ∈ [k],</p><p>10 As discussed in Section 4, correctness of our algorithm at the level of numerical accuracy requires that the coefficients of every polynomial in the SoS program A (and every polynomial in the SoS proofs we use to analyze A) are polynomially bounded. This may not be the case if some vectors µ 1 , . . . , µ m have norms ∥µ i ∥ ≥ d ω (1) . This can be fixed by naively clustering the samples X 1 , . . . , X n via singlelinkage clustering, then running LearnMixtureMeans on each cluster. It is routine to show that the diameter of each cluster output by a naive clustering algorithm is at most poly(d, k ) under our assumptions, and that with high probability singlelinkage clustering produces a clustering respecting the distributions D i . Hence, by centering each cluster before running LearnMixtureMeans we can assume that ∥µ i ∥ ≤ poly(d, k ) for every i ≤ d .</p><p>The other main lemma shows that conditions (D1) and (D2) occur with high probability.</p><p>Lemma 5.4 (Concentration for mixture models). With notation as above, conditions (D1) and (D2) simultaneously occur with probability at least 1 -1/d 15 over samples X 1 , . . . , X n , so long as n ≥ d O (t ) k O (1) , for ∆ ≥ 1. Lemma 5.4 follows from Lemma 4.1, for (D1), and standard concentration arguments for (D2). Now we can prove the main theorem.</p><p>Proof of Theorem 5.1 (uniform mixtures case). Suppose conditions (D1) and (D2) hold. Our goal will be to bound</p><p>, where A is the 0/1 indicator matrix for the ground truth partition S 1 , . . . , S k of X 1 , . . . , X n according to D 1 , . . . , D k . Then by Lemma 5.2, the rounding algorithm will return a partition C 1 , . . . , C k of [n] such that C ℓ and S ℓ differ by at most n 2 O (t ) t t /2 k 10 ∆ t points, with probability at least 1 -2 O (t ) t t /2 k 30 ∆ t . By the guarantees of regarding the algorithm EstimateMean (see full version of this work), with high probability the resulting error in the mean estimates μi will satisfy</p><p>We turn to the bound on ∥M -A∥ 2 . First we bound ⟨ Ẽ ww ⊤ , A⟩. Getting started,</p><p>By Lemma 5.3, choosing t later,</p><p>Together, these imply</p><p>At the same time, ∥ Ẽ ww T ∥ F ≤ 1 k ∥A∥ F by minimality (since the uniform distribution over cluster indicators satisfies A), and by routine calculation and assumption (D1), ∥A∥ F ≤ n √ k</p><p>(1 + O(∆ -t )).</p><p>Together, we have obtained</p><p>which can be rearranged to give ∥M -A∥ 2 ≤ n • 2 O (t ) t t /2 k 4 ∆ t . □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Identifiability</head><p>In this section we prove Lemma 5.3. We use the following helpful lemmas. The first is in spirit an SoS version of Lemma 2.1.</p><p>Lemma 5.5. Let µ 1 , . . . , µ k , D 1 , . . . , D k , t be as in Theorem 5.1. Let µ i be as in (D1). Suppose (D1) occurs for samples X 1 , . . . , X n . Let A be the system from Lemma 4.1, with α = 1/k and any τ . Then A ⊢ O (t ) ⟨a j , w⟩ t ∥µ -µ j ∥ 2t ≤ 2 t +2 t t /2 • n k • ⟨a j , w⟩ t -1 • ∥µ -µ j ∥ t .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On spectral learning of mixtures of distributions</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="458" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Rademacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT (JMLR Workshop and Conference Proceedings)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1135" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning mixtures of separated nonspherical Gaussians</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kannan</surname></persName>
		</author>
		<idno type="DOI">10.1214/105051604000000512</idno>
		<ptr target="https://doi.org/10.1214/105051604000000512" />
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Probab</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1A</biblScope>
			<biblScope unit="page" from="69" to="92" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The power of localization for efficiently learning linear separators with noise</title>
		<author>
			<persName><forename type="first">P</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved spectral-norm bounds for clustering</title>
		<author>
			<persName><forename type="first">Pranjal</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Sheffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Statistical guarantees for the EM algorithm: From population to sample-based analysis</title>
		<author>
			<persName><forename type="first">Sivaraman</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR abs/1408.2156</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rounding sum-ofsquares relaxations</title>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Kelner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</title>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Kelner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="143" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noisy Tensor Completion via the Sumof-Squares Hierarchy</title>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT (JMLR Workshop and Conference Proceedings)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="417" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sum-of-squares proofs and the quest toward optimal algorithms</title>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steurer</surname></persName>
		</author>
		<idno>CoRR abs/1404.5236</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The sos algorithm over general domains</title>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steurer</surname></persName>
		</author>
		<ptr target="http://www.sumofsquares.org/public/lec-definitions-general.html" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>Online; accessed 11-1-2017</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Polynomial Learning of Distribution Families</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robust Estimators are Hard to Compute</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bernholt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Dortmund</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smoothed analysis of tensor decompositions</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravindan</forename><surname>Vijayaraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="594" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust PCA and clustering in noisy mixtures</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1078" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning from Untrusted Data</title>
		<author>
			<persName><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<idno>CoRR abs/1611.02315</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Thresholding based Efficient Outlier Robust PCA</title>
		<author>
			<persName><forename type="first">Yeshwanth</forename><surname>Cherapanamjeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning mixtures of Gaussians</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of computer science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="634" to="644" />
		</imprint>
	</monogr>
	<note>40th annual symposium on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A probabilistic analysis of EM for mixtures of separated, spherical Gaussians</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="203" to="226" />
			<date type="published" when="2007-02">2007. Feb (2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster and sample nearoptimal algorithms for proper learning mixtures of gaussians</title>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ten steps of EM suffice for mixtures of two Gaussians</title>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Tzamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Zampetakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust Estimators in High Dimensions without the Computational Intractability</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Being robust (in high dimensions) can be practical</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robustly Learning a Gaussian: Getting Optimal Error, Efficiently</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Stewart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">personal communication</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Daniel M Kane</surname></persName>
		</author>
		<author>
			<persName><surname>Stewart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03473</idno>
		<title level="m">Statistical Query Lower Bounds for Robust Estimation of High-dimensional Gaussians and Gaussian Mixtures</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Daniel M Kane</surname></persName>
		</author>
		<author>
			<persName><surname>Stewart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01242</idno>
		<title level="m">Learning Geometric Concepts with Nasty Noise</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PAC Learning Axis-Aligned Mixtures of Gaussians with No Separation Assumption</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocco</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan O'</forename><surname>Donnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4005</biblScope>
			<biblScope unit="page" from="20" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<title level="m">Learning mixtures of Gaussians in high dimensions</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>extended abstract</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">STOC&apos;15-Proceedings of the 2015 ACM Symposium on Theory of Computing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="761" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Decomposing overcomplete 3rd order tensors using sum-of-squares algorithms</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05287</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Robust statistics. The approach based on influence functions</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Hampel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Stahel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tight Bounds for Learning a Mixture of Two Gaussians</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The power of sum-of-squares for detecting hidden structures</title>
		<author>
			<persName><forename type="first">Pravesh</forename><surname>Samuel B Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Potechin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tselil</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schramm</surname></persName>
		</author>
		<author>
			<persName><surname>Steurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tselil</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schramm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Steurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="178" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tensor principal component analysis via sum-of-square proofs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Steurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT (JMLR Workshop and Conference Proceedings)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="956" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning mixtures of spherical Gaussians: moment methods and spectral decompositions</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITCS&apos;13-Proceedings of the 2013 ACM Conference on Innovations in Theoretical Computer Science</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964">1964. 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The densest hemisphere problem</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Preparata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="1978">1978. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficiently learning mixtures of two Gaussians</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="553" to="562" />
		</imprint>
	</monogr>
	<note>Ankur Moitra, and Gregory Valiant</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning in the Presence of Malicious Errors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="807" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Klivans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Servedio</surname></persName>
		</author>
		<title level="m">Learning Halfspaces with Malicious Noise</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Better Clustering via Relaxed Tensor Norms</title>
		<author>
			<persName><forename type="first">Pravesh</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>personal communication</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Outlier-robust moment estimation via sum-of-squares</title>
		<author>
			<persName><forename type="first">Pravesh</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steurer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>personal communication</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Clustering with Spectral Norm and the k-Means Algorithm</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravindran</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Agnostic Estimation of Mean and Covariance</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Robust computation of linear models, or How to find a needle in a haystack</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/1202.4044</idno>
		<ptr target="http://arxiv.org/abs/1202.4044" />
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust and Proper Learning for Mixtures of Gaussians via Systems of Polynomial Inequalities</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Polynomial-Time Tensor Decompositions with Sum-of-Squares</title>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="438" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Peel</surname></persName>
		</author>
		<title level="m">Finite mixture models</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Clustering subgaussian mixtures by semidefinite programming. Information and Inference: A</title>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Dustin G Mixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the IMA</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>iax001</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Settling the Polynomial Learnability of Mixtures of Gaussians</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">O'</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><surname>Donnell</surname></persName>
		</author>
		<title level="m">SOS is not obviously automatizable, even approximately</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Approximability and proof complexity</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Donnell</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA. SIAM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1537" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Contributions to the mathematical theory of evolution</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London. A</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="71" to="110" />
			<date type="published" when="1894">1894. 1894</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Exact tensor completion with sum-ofsquares</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Potechin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steurer</surname></persName>
		</author>
		<idno>CoRR abs/1702.06237</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">On the Bit Complexity of Sumof-Squares Proofs</title>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Weitz</surname></persName>
		</author>
		<idno>CoRR abs/1702.05139</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On learning mixtures of wellseparated Gaussians</title>
		<author>
			<persName><forename type="first">Oded</forename><surname>Regev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravindan</forename><surname>Vijayraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fast and robust tensor decomposition with applications to dictionary learning</title>
		<author>
			<persName><forename type="first">Tselil</forename><surname>Schramm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Smooth boosting and learning with malicious noise</title>
		<author>
			<persName><forename type="first">R</forename><surname>Servedio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="633" to="648" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Resilience: A criterion for learning in the presence of arbitrary outliers</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ITCS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Near-optimal-sample estimators for spherical gaussian mixtures</title>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Orlitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayadev</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Jafarpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Statistical analysis of finite mixture distributions</title>
		<author>
			<persName><forename type="first">Adrian Fm</forename><surname>Michael Titterington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udi</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Makov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mathematics and picturing of data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICM</title>
		<meeting>ICM</meeting>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mathematics and the picturing of data</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international congress of mathematicians</title>
		<meeting>the international congress of mathematicians</meeting>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Learning Disjunction of Conjunctions</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
		<editor>IJCAI. Morgan Kaufmann</editor>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="560" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A Spectral Algorithm for Learning Mixtures of Distributions</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">113</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On the convergence properties of the EM algorithm</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Cf</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Global analysis of expectation maximization for mixtures of two gaussians</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arian</forename><surname>Maleki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2676" to="2684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A Novel M-estimator for Robust PCA</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="749" to="808" />
			<date type="published" when="2014-01">2014. Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
