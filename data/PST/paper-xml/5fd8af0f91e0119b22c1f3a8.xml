<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARAMETER-EFFICIENT TRANSFER LEARNING WITH DIFF PRUNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-14">14 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
							<email>dguo@college.harvard.edu</email>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mit-Ibm</forename><surname>Watson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Mit Csail</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University Hugging Face</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PARAMETER-EFFICIENT TRANSFER LEARNING WITH DIFF PRUNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-14">14 Dec 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2012.07463v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While task-specific finetuning of pretrained networks has led to significant empirical advances in NLP, the large size of networks makes finetuning difficult to deploy in multi-task, memory-constrained settings. We propose diff pruning as a simple approach to enable parameter-efficient transfer learning within the pretrain-finetune framework. This approach views finetuning as learning a taskspecific "diff" vector that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks. The diff vector is adaptively pruned during training with a differentiable approximation to the L 0 -norm penalty to encourage sparsity. Diff pruning becomes parameter-efficient as the number of tasks increases, as it requires storing only the nonzero positions and weights of the diff vector for each task, while the cost of storing the shared pretrained model remains constant. It further does not require access to all tasks during training, which makes it attractive in settings where tasks arrive in stream or the set of tasks is unknown. We find that models finetuned with diff pruning can match the performance of fully finetuned baselines on the GLUE benchmark while only modifying 0.5% of the pretrained model's parameters per task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Task-specific finetuning of pretrained deep networks has become the dominant paradigm in contemporary NLP, achieving state-of-the-art results across a suite of natural language understanding tasks <ref type="bibr" target="#b8">(Devlin et al., 2019;</ref><ref type="bibr" target="#b27">Liu et al., 2019c;</ref><ref type="bibr" target="#b70">Yang et al., 2019;</ref><ref type="bibr" target="#b21">Lan et al., 2020)</ref>. While straightforward and empirically effective, this approach is difficult to scale to multi-task, memory-constrained settings (e.g. for on-device applications), as it requires shipping and storing a full set of model parameters for each task. Inasmuch as these models are learning generalizable, task-agnostic language representations through self-supervised pretraining, finetuning the entire model for each task seems especially profligate.</p><p>A popular approach to parameter-efficiency with pretrained models is to learn sparse models for each task where a subset of the final model parameters are exactly zero <ref type="bibr" target="#b11">(Gordon et al., 2020;</ref><ref type="bibr" target="#b49">Sajjad et al., 2020;</ref><ref type="bibr" target="#b74">Zhao et al., 2020;</ref><ref type="bibr" target="#b51">Sanh et al., 2020)</ref>. Such approaches often face a steep sparsity/performance tradeoff, and a substantial portion of nonzero parameters (e.g. 10%-30%) are still typically required to match the performance of the dense counterparts. An alternative is to use multi-task learning or feature-based transfer for more parameter-efficient transfer learning with pretrained models <ref type="bibr" target="#b26">(Liu et al., 2019b;</ref><ref type="bibr" target="#b5">Clark et al., 2019;</ref><ref type="bibr" target="#b56">Stickland &amp; Murray, 2019;</ref><ref type="bibr" target="#b47">Reimers &amp; Gurevych, 2019;</ref><ref type="bibr" target="#b9">Feng et al., 2020)</ref>. These methods learn only a small number of additional parameters (e.g. a linear layer) on top of a shared model. However, multi-task learning generally requires access to all tasks during training to prevent catastrophic forgetting <ref type="bibr" target="#b10">(French, 1999)</ref>, while feature-based transfer learning (e.g. based on task-agnostic sentence representations) is typically outperformed by full finetuning <ref type="bibr" target="#b16">(Howard &amp; Ruder, 2018)</ref>.</p><p>Adapters <ref type="bibr" target="#b46">(Rebuffi et al., 2018)</ref> have recently emerged as a promising approach to parameterefficient transfer learning within the pretrain-finetune paradigm <ref type="bibr" target="#b15">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b37">Pfeiffer et al., 2020a;</ref><ref type="bibr" target="#b1">b;</ref><ref type="bibr" target="#b48">c)</ref>. Adapter layers are smaller, task-specific modules that are inserted between layers of a pretrained model, which remains fixed and is shared across tasks. These approaches do not require access to all tasks during training, making them attractive in settings where one hopes to obtain and Preprint share performant models as new tasks arrive in stream. <ref type="bibr" target="#b15">Houlsby et al. (2019)</ref> find that adapter layers trained on BERT can match the performance of fully finetuned BERT on the GLUE benchmark <ref type="bibr" target="#b63">(Wang et al., 2019a)</ref> while only requiring 3.6% additional parameters (on average) per task.</p><p>In this work, we consider a similar setting as adapters but propose a new diff pruning approach with the goal of even more parameter-efficient transfer learning. Diff pruning views finetuning as learning a task-specific difference vector that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks. In order to learn this vector, we reparameterize the task-specific model parameters as θ task = θ pretrained +δ task , where the pretrained parameter vector θ pretrained is fixed and the task-specific diff vector δ task is finetuned. The diff vector is regularized with a differentiable approximation to the L 0 -norm penalty <ref type="bibr" target="#b29">(Louizos et al., 2018)</ref> to encourage sparsity. This approach can become parameter-efficient as the number of tasks increases as it only requires storing the nonzero positions and weights of the diff vector for each task. The cost of storing the shared pretrained model remains constant and is amortized across multiple tasks. On the GLUE benchmark <ref type="bibr" target="#b63">(Wang et al., 2019a)</ref>, diff pruning can match the performance of the fully finetuned BERT baselines while finetuning only 0.5% of the pretrained parameters per task, making it a potential alternative to adapters for parameter-efficient transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND: TRANSFER LEARNING FOR NLP</head><p>The field of NLP has recently seen remarkable progress through transfer learning with a pretrainand-finetune paradigm, which initializes a subset of the model parameters for all tasks from a pretrained model and then finetunes on a task specific objective. Pretraining objectives include context prediction <ref type="bibr" target="#b34">(Mikolov et al., 2013)</ref>, autoencoding <ref type="bibr" target="#b7">(Dai &amp; Le, 2015)</ref>, machine translation <ref type="bibr" target="#b32">(McCann et al., 2017)</ref>, and more recently, variants of language modeling <ref type="bibr" target="#b36">(Peters et al., 2018;</ref><ref type="bibr" target="#b40">Radford et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref> objectives.</p><p>Here we consider applying transfer learning to multiple tasks. We consider a setting with a potentially unknown set of tasks, where each τ ∈ T has an associated training set {x</p><formula xml:id="formula_0">(n) τ , y (n) τ } N n=1</formula><p>. For all tasks, the goal is to produce (possibly tied) model parameters θ τ to minimize the empirical risk,</p><formula xml:id="formula_1">min θτ 1 N N n=1 L f (x (n) τ ; θ τ ), y (n) τ + λR(θ τ )</formula><p>where f (•; θ) is a parameterized function over the input (e.g. a neural network), L(•, •) is a loss function (e.g. cross-entropy), and R(•) is an optional regularizer with hyperparameter λ.</p><p>This multi-task setting can use the pretrain-then-finetune approach by simply learning independent parameters for each task; however the large size of pretrained models makes this approach exceedingly parameter inefficient. For example, widely-adopted models such as BERT BASE and BERT LARGE have 110M and 340M parameters respectively, while their contemporaries such as T5 <ref type="bibr" target="#b43">(Raffel et al., 2020)</ref>, Megatron-LM <ref type="bibr" target="#b55">(Shoeybi et al., 2019)</ref>, and Turing-NLG <ref type="bibr" target="#b44">(Rajbhandari et al., 2019)</ref> have parameter counts in the billions. Storing the fully finetuned models becomes difficult even for a moderate number of tasks. 1 A classic approach to tackling this parameterinefficiency <ref type="bibr" target="#b2">(Caruana, 1997)</ref> is to train a single shared model (along with a task-specific output layer) against multiple tasks through joint training. However, the usual formulation of multi-task learning requires the set of tasks T to be known in advance in order to prevent catastrophic forgetting <ref type="bibr" target="#b10">(French, 1999</ref>), 2 making it unsuitable for applications in which the set of tasks is unknown (e.g. when tasks arrive in stream).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DIFF PRUNING</head><p>Diff pruning formulates task-specific finetuning as learning a diff vector δ τ that is added to the pretrained model parameters θ pretrained . We first reparameterize the task-specific model parameters,</p><formula xml:id="formula_2">θ τ = θ pretrained + δ τ ,</formula><p>1 An intriguing line of work suggests that large-scale language models can be used without finetuning for a variety of tasks if given the appropriate context <ref type="bibr" target="#b41">(Radford et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al., 2020)</ref>. While interesting, these models generally underperform task-specific models and require billions of parameters, though recent work suggests that they can be made substantially smaller <ref type="bibr" target="#b52">(Schick &amp; Schutze, 2020)</ref>.</p><p>2 However, work on continual learning mitigates these issues to an extent <ref type="bibr" target="#b54">(Shin et al., 2017;</ref><ref type="bibr" target="#b28">Lopez-Paz &amp; Ranzato, 2017;</ref><ref type="bibr" target="#b24">Lee et al., 2017;</ref><ref type="bibr" target="#b19">Kirkpatrick et al., 2017;</ref><ref type="bibr" target="#b35">Parisi et al., 2018)</ref>. Preprint which results in the following empirical risk minimization problem,</p><formula xml:id="formula_3">min δτ 1 N N n=1 L f (x (n) τ ; θ pretrained + δ τ ), y (n) τ + λR(θ pretrained + δ τ ).</formula><p>This trivial reparameterization is equivalent to the original formulation. Its benefit comes in the multi-task setting where the cost of storing the pretrained parameters θ pretrained is amortized across tasks, and the only marginal cost for new tasks is the diff vector. If we can regularize δ τ to be sparse such that δ τ 0 θ pretrained 0 , then this approach can become more parameter-efficient as the number of tasks increases. We can specify this goal with an L 0 -norm penalty on the diff vector,</p><formula xml:id="formula_4">R(θ pretrained + δ τ ) = δ τ 0 = d i=1 1{δ τ,i = 0}.</formula><p>3.1 DIFFERENTIABLE APPROXIMATION TO THE L 0 -NORM This regularizer is difficult to directly optimize as it is non-differentiable. In order to approximate this L 0 objective, we follow the standard approach for gradient-based learning with L 0 sparsity using a relaxed mask vector <ref type="bibr" target="#b29">(Louizos et al., 2018)</ref>. This approach involves relaxing a binary vector into continuous space, and then multiplying it with a dense weight vector to determine how much of the weight vector is applied during training. After training, the mask is deterministic and a large portion of the diff vector is true zero.</p><p>To apply this method we first decompose δ τ into a binary mask vector multiplied with a dense vector,</p><formula xml:id="formula_5">δ τ = z τ w τ , z τ ∈ {0, 1} d , w τ ∈ R d We can now instead optimize an expectation with respect to z τ , whose distribution p(z τ ; α τ ) is initially Bernoulli with parameters α τ , min ατ ,wτ E zτ ∼p(zτ ;ατ ) 1 N N n=1 L f (x (n) τ ; θ pretrained + z τ w τ , ), y (n) τ + λ δ τ 0 .</formula><p>This objective is still difficult in practice due to z τ 's being discrete (which requires the score function gradient estimator), but the expectation provides some guidance for empirically effective relaxations. We follow prior work <ref type="bibr" target="#b29">(Louizos et al., 2018;</ref><ref type="bibr" target="#b65">Wang et al., 2019b)</ref> and relax z τ into continuous space [0, 1] d with a stretched Hard-Concrete distribution <ref type="bibr" target="#b17">(Jang et al., 2017;</ref><ref type="bibr" target="#b30">Maddison et al., 2017)</ref>, which allows for the use of pathwise gradient estimators. Specifically, z τ is now defined to be a deterministic and (sub)differentiable function of a sample u from a uniform distribution, u ∼ U (0, 1),</p><formula xml:id="formula_6">s τ = σ (log u − log(1 − u) + α τ ) , sτ = s τ × (r − l) + l, z τ = min(1, max(0, sτ )).</formula><p>Here l &lt; 0 and r &gt; 1 are two constants used to stretch s τ into the interval (l, r) d before it is clamped to [0, 1] d with the min(1, max(0, •)) operation. In this case we have a differentiable closed-form expression for the expected L 0 -norm,</p><formula xml:id="formula_7">E [ δ τ 0 ] = d i=1 E [1{z τ,i &gt; 0}] = d i=1 σ α τ,i − log −l r .</formula><p>Thus the final optimization problem is given by,</p><formula xml:id="formula_8">min ατ ,wτ E u∼U [0,1] 1 N N n=1 L f (x (n) τ ; θ pretrained + z τ w τ , ), y (n) τ + λ d i=1 σ α τ,i − log −l r ,</formula><p>and we can now utilize pathwise gradient estimators to optimize the first term with respect to α τ since the expectation no longer depends on it. <ref type="foot" target="#foot_0">3</ref> After training we obtain the final diff vector δ τ by sampling u once to obtain z τ (which is not necessarily a binary vector but has a significant number of dimensions equal to exactly zero due to the clamping function), then setting δ τ = z τ w τ . <ref type="foot" target="#foot_1">4</ref>Preprint 3.2 L 0 -BALL PROJECTION WITH MAGNITUDE PRUNING FOR SPARSITY CONTROL Differentiable L 0 regularization provides a strong way to achieve high sparsity rate. However, it would be ideal to have more fine-grained control into the exact sparsity rate in the diff vector, especially considering applications which require specific parameter budgets. As λ is just the Lagrangian multiplier for the constraint E [ δ τ 0 ] &lt; η for some η, this could be achieved in principle by searching over different values of λ. However we found it more efficient and empirically effective to achieve an exact sparsity rate by simply projecting onto the L 0 -ball after training.</p><p>Specifically we use magnitude pruning on the diff vector δ τ and target a sparsity rate t% by only keeping the top t% × d values in δ τ .<ref type="foot" target="#foot_2">5</ref> Note that unlike standard magnitude pruning, this is based on the magnitude of the diff vector values and not the model parameters. As is usual in magnitude pruning, we found it important to further finetune δ τ with the nonzero masks fixed to maintain good performance <ref type="bibr" target="#b13">(Han et al., 2016)</ref>. Since this type of parameter-efficiency through projection onto the L 0 -ball can be applied without adaptive diff pruning,<ref type="foot" target="#foot_3">6</ref> such an approach will serve as one of our baselines in the empirical study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">STRUCTURED DIFF PRUNING</head><p>Diff pruning, as presented above, is architecture-agnostic and does not exploit the underlying model structure-each dimension of z τ is independent from one another. While this makes the approach potentially more flexible, we might expect to achieve better sparsity/performance tradeoff through a structured formulation which encourages active parameters to group together and other areas to be fully sparse. Motivated by this intuition, we first partition the parameter indices into G groups {g(1), . . . , g(G)} where g(j) is a subset of parameter indices governed by group g(j). <ref type="foot" target="#foot_4">7</ref> We then introduce a scalar z j τ (with the associated parameter α j τ ) for each group g(j), and decompose the task-specific parameter for index i ∈ g(j) as δ j τ,i = z τ,i × z j τ × w τ,i . The expected L 0 -norm is then given by,</p><formula xml:id="formula_9">E [ δ τ 0 ] = G j=1 i∈g(j) E [1{z τ,i • z g τ &gt; 0}] = G j=1 i∈g(j) σ α τ,i − log −l r × σ α j τ − log −l r ,</formula><p>and we can train with gradient-based optimization as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MODEL AND DATASETS</head><p>For evaluation we mainly use the GLUE benchmark <ref type="bibr" target="#b65">(Wang et al., 2019b)</ref>, a popular finetuning dataset. Following adapters <ref type="bibr" target="#b15">(Houlsby et al., 2019)</ref>, we test our approach on the following subset of the GLUE tasks: Multi-Genre Natural Language Inference (MNLI), where the goal is two predict whether the relationship between two sentences is entailment, contradiction, or neutral (we test on both MNLI m and MNLI mm which respectively tests on matched/mismatched domains); Quora Question Pairs (QQP), a classification task to predict whether two question are semantically equivalent; Question Natural Language Inference (QNLI), which must predict whether a sentence is a correct answer to the question; Stanford Sentiment Treebank (SST-2), a sentence classification task to predict the sentiment of movie reviews; Corpus of Linguistic Acceptability (CoLA), where the goal is predict whether a sentence is linguistically acceptable or not; Semantic Textual Similarity Benchmark (STS-B), which must predict a similarity rating between two sentences; Microsoft Research Paraphrase Corpus (MRPC), where the goal is to predict whether two sentences are semantically equivalent; Recognizing Textual Entailment (RTE), which must predict whether a second sentence is entailed by the first. The benchmark uses Matthew's correlation for CoLA, Spearman for STS-B, F 1 score for MRPC/QQC, and accuracy for MNLI/QNLI/SST-2/RTE. Finally, to test for generalization beyond the GLUE tasks, we also test our approach on the SQuAD extractive question answering dataset <ref type="bibr" target="#b45">(Rajpurkar et al., 2016)</ref>.</p><p>For all experiments, we use the BERT LARGE model from <ref type="bibr" target="#b8">Devlin et al. (2019)</ref>, which has 24 layers, 1024 hidden size, 16 attention heads, and 340M parameters. We use the Huggingface Transformer library <ref type="bibr" target="#b68">(Wolf et al., 2019)</ref> to conduct our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BASELINES</head><p>We compare both structured and non-structured variants of diff pruning against the following baselines: Full finetuning, which fully finetunes BERT LARGE as usual; Last layer finetuning, which only finetunes the penultimate layer (along with the final output layer)<ref type="foot" target="#foot_5">8</ref> ; Adapters from Houlsby et al. ( <ref type="formula">2019</ref>), which train task-specific bottleneck layers between between each layer of a pretrained model, where parameter-efficiency can be controlled by varying the size of the bottleneck layers; and Non-adaptive diff pruning, which performs diff pruning just based on magnitude pruning (i.e., we obtain θ τ through usual finetuning, set δ τ = θ τ − θ pretrained , and then apply magnitude pruning followed by additional finetuning on δ τ ). For diff pruning we set our target sparsity rate to 0.5% and investigate the effect of different target sparsity rates in section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">IMPLEMENTATION DETAILS AND HYPERPARAMETERS</head><p>Diff pruning introduces additional hyperparameters l, r (for stretching the Hard-Concrete distribution) and λ (for weighting the approximate L 0 -norm penalty). We found l = −1.5, r = 1.5, λ = 1.25 × 10 −7 to work well across all tasks. We also initialize the weight vector w τ to 0, and α τ to a positive vector (we use 5) to encourage z τ to be close to 1 at the start of training.<ref type="foot" target="#foot_6">9</ref> While we mainly experiment with BERT LARGE to compare against prior work with adapters <ref type="bibr" target="#b15">(Houlsby et al., 2019)</ref>, in preliminary experiments we found these hyperparameters to work for finetuning RoBERTa <ref type="bibr" target="#b27">(Liu et al., 2019c)</ref> and XLNet <ref type="bibr" target="#b70">(Yang et al., 2019</ref>) models as well.</p><p>For all tasks we initially train for 3 epochs and perform a hyperparameter search over batch size ∈ {5, 8, 12, 16} and learning rate ∈ {1 × 10 −5 , 2 × 10 −5 , 5 × 10 −5 } . However we found the default settings used for regular finetuning as suggested in the original BERT paper to work well for most tasks. Finetuning with the fixed mask after projecting onto the L 0 -ball with magnitude pruning is done for 3 epochs with a learning rate of 5 × 10 −5 for all datasets except for MRPC/STS-B/RTE/SST-2 dataset, where we finetune for 5 epochs. The exact hyperparameters for each task are given in section A.2 of the appendix. Grouping for the structured version of diff pruning is based on the matrix/bias vectors (i.e. parameters that belong to the same matrix or bias vector are assumed to be in the same group), which results in 393 groups.<ref type="foot" target="#foot_7">10</ref> 5 RESULTS AND ANALYSIS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RESULTS ON GLUE BENCHMARK</head><p>Our main results on the GLUE benchmark are shown in Table <ref type="table" target="#tab_0">1</ref>. Structured diff pruning can match the performance of a fully finetuned BERT LARGE model while only requiring 0.5% additional parameters per task. Diff pruning without structured sparsity also performs well, though slightly worse than the structured approach. Non-adaptive diff pruning, which magnitude prunes the diff vector without learning the binary mask z τ , performs significantly worse, indicating the importance of learning the masking vector. Compared to adapters, diff pruning obtains similar performance while requiring fewer parameters per task, making it a potential alternative for parameter-efficient transfer learning. <ref type="foot" target="#foot_8">11</ref>Preprint  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">VARYING THE TARGET SPARSITY</head><p>In Figure <ref type="figure" target="#fig_0">1</ref> (left), we plot results on the GLUE validation set averaged across all tasks at target sparsity rates of 0.1%, 0.25%, 0.5%, 1.0% for the different baselines. Structured diff pruning consistently outperforms non-structured and and non-adaptive variants across different sparsity rates.</p><p>The advantage of adaptive methods becomes more pronounced at extreme sparsity rates. In Table <ref type="table" target="#tab_1">2</ref>, we report the breakdown of accuracy of structured diff pruning across different tasks and sparsity rates, where we observe that different tasks have different sensitivity to target sparsity rates. This suggests that we can obtain even greater parameter-efficiency through targeting task-specific sparsity rates in the diff vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RESULTS ON SQUAD EXTRACTIVE QUESTION ANSWERING</head><p>To demonstrate the effectiveness of our approach beyond the GLUE tasks, we additionally experiment on SQuAD <ref type="bibr" target="#b45">(Rajpurkar et al., 2016)</ref>, an extractive question answering dataset where the model has to select the answer span to a question given a Wikipedia paragraph. To make direct comparisons with <ref type="bibr" target="#b15">Houlsby et al. (2019)</ref>, we run all experiments on SQuAD v1.1. For diff pruning, we use the same general hyperparameters as our full finetuning baseline (see section A.2). As shown in Figure <ref type="figure" target="#fig_0">1</ref> (right), diff pruning is able achieve comparable or better performance with only 1.0% additional parameters. Interestingly, diff pruning measurably improves the upon the full finetuning baseline while modifying fewer parameters, which indicates that diff pruning can have a useful regularization effect on top of parameter-efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">STRUCTURED VS. NON-STRUCTURED DIFF PRUNING</head><p>Structured diff pruning introduces an additional mask per group, which encourages pruning of entire groups. This is less restrictive than traditional group sparsity techniques that have been used with L 0 -norm relaxations which force all parameters in a group to share the same mask <ref type="bibr" target="#b29">(Louizos et al., 2018;</ref><ref type="bibr" target="#b65">Wang et al., 2019b)</ref>. However we still expect entire groups to be pruned out more often in the structured case, which might bias the learning process towards either eliminating completely or clustering together nonzero diffs. In Table <ref type="table" target="#tab_2">3</ref>, we indeed find that structured diff pruning leads to  finetuned models that are much more likely to leave entire groups unchanged from their pretrained values (zero diffs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">TASK-SPECIFIC SPARSITY</head><p>Different layers of pretrained models have been argued to encode different information <ref type="bibr" target="#b25">(Liu et al., 2019a;</ref><ref type="bibr" target="#b61">Tenney et al., 2019)</ref>. Given that each task will likely recruit different kinds of language phenomena embedded in the hidden layers, we hypothesize that diff pruning will modify different parts of the pretrained model through task-specific finetuning. Figure <ref type="figure" target="#fig_1">2</ref> shows the percentage of nonzero diff parameters attributable to the different layers for each task. We find that different tasks indeed modify different parts of the network, although there are some qualitative similarities between some tasks, for example between QNLI &amp; QQP (both must encode questions), and MRPC &amp; STS-B (both must predict similarity between sentences). The embedding layer is very sparsely modified for all tasks. While some of the variations in the sparsity distributions is due to simple randomness, we do observe some level of consistency over multiple runs of the same task, as shown in section A.1 of the appendix.</p><p>The ability to modify different parts of the pretrained model for each task could explain the improved parameter-efficiency of our approach compared to Houlsby et al. ( <ref type="formula">2019</ref>)'s adapter layers, which can only read/write to the pretrained model at certain points of the computational graph.<ref type="foot" target="#foot_9">12</ref> This potentially suggests that adapter layers with more fine-grained access into model internals (e.g. adapters for key/value/query transformations) might result in even greater parameter-efficiency. While left as future work, we also note that diff pruning can be applied in conjunction with adapters, which might further improve results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">EFFECT OF L 0 -BALL PROJECTION VIA MAGNITUDE PRUNING</head><p>Applying magnitude pruning to project onto the L 0 -ball was crucial in achieving exact sparsity targets. As shown in Table <ref type="table" target="#tab_3">4</ref>, we observed little loss in performance through magnitude pruning.</p><p>We also found it important to finetune with a fixed mask after magnitude pruning, even for the approach that does not apply magnitude pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">LIMITATIONS</head><p>For training, our approach requires more memory than usual finetuning due to additionally optimizing α τ and w τ . Since the majority of GPU memory is typically utilized by a minibatch's intermediate layers, this did not present a significant challenge for pretrained models that we experimented with in this study. However, this could present an issue as model sizes get larger and larger. After training, storing the task-specific diff vector requires storing a compressed version with both the Preprint  nonzero positions and weights, which incurs additional storage requirements. Finally, while training efficiency was not a primary concern of this work, diff pruning was also approximately 1.5× to 2× slower to train per minibatch than regular finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">INFORMATION-EFFICIENT TRANSFER LEARNING</head><p>Efficiently representing pretrained models adapted to new tasks is becoming an increasingly important problem in contemporary NLP. This paper focuses on a rather narrow definition of efficiencyparameter-efficiency. An interesting direction might be to target generalizations of parameterefficiency, for example, information-efficiency, which aims to minimize the number of bits required to represent the task-specific model when given the pretrained model for free. This view can suggest other avenues for achieving information-efficient transfer learning: for example, "what is the minimum number of (potentially synthetic) datapoints that we can finetune BERT on to obtain a good task-specific model?", <ref type="foot" target="#foot_10">13</ref> or "what is the shortest prefix string that we can condition GPT3 on for it to become a good task-specific model?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Multi-task learning Multi-task learning <ref type="bibr" target="#b2">(Caruana, 1997)</ref>, broadly construed, aims to learn models and representations that can be utilized across a diverse range of tasks, and offers a natural approach to training parameter-efficient deep models. Several works have shown that a single BERT model can obtain good performance across multiple tasks when jointly trained <ref type="bibr" target="#b26">(Liu et al., 2019b;</ref><ref type="bibr" target="#b5">Clark et al., 2019;</ref><ref type="bibr" target="#b56">Stickland &amp; Murray, 2019</ref>). An alternative approach to multi-task learning that does not require access to all tasks during training involve training smaller task-specific layers that interact with a fixed pretrained model <ref type="bibr" target="#b46">(Rebuffi et al., 2018;</ref><ref type="bibr" target="#b71">Zhang et al., 2020a)</ref>. In particular, adapter layers <ref type="bibr" target="#b46">(Rebuffi et al., 2018)</ref>, which learn to read and write to layers of a shared model, have been applied to obtain parameter-efficient BERT models <ref type="bibr" target="#b15">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b37">Pfeiffer et al., 2020a;</ref><ref type="bibr" target="#b1">b;</ref><ref type="bibr" target="#b48">c)</ref>. A related line of work targets extreme parameter-efficiency through task-agnostic sentence representations that can be used without finetuning for downstream tasks <ref type="bibr" target="#b22">(Le &amp; Mikolov, 2014;</ref><ref type="bibr" target="#b20">Kiros et al., 2015;</ref><ref type="bibr" target="#b67">Wieting et al., 2016;</ref><ref type="bibr" target="#b14">Hill et al., 2016;</ref><ref type="bibr" target="#b0">Arora et al., 2017;</ref><ref type="bibr" target="#b6">Conneau et al., 2017;</ref><ref type="bibr" target="#b3">Cer et al., 2018;</ref><ref type="bibr" target="#b72">Zhang et al., 2018;</ref><ref type="bibr" target="#b57">Subramanian et al., 2018;</ref><ref type="bibr" target="#b47">Reimers &amp; Gurevych, 2019;</ref><ref type="bibr" target="#b73">Zhang et al., 2020b)</ref>. These feature-based transfer learning methods are however generally outperformed by fully finetuned models <ref type="bibr" target="#b16">(Howard &amp; Ruder, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>Model compression There has been much recent work on compressing pretrained trained with self-supervision (see <ref type="bibr" target="#b10">Ganesh et al. (2020)</ref> for a recent survey). A particularly promising line of work focuses on obtaining smaller pretrained models (for subsequent finetuning) through weight pruning <ref type="bibr" target="#b11">(Gordon et al., 2020;</ref><ref type="bibr" target="#b49">Sajjad et al., 2020;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref> and/or knowledge distillation <ref type="bibr" target="#b50">(Sanh et al., 2019;</ref><ref type="bibr" target="#b58">Sun et al., 2019;</ref><ref type="bibr" target="#b62">Turc et al., 2019;</ref><ref type="bibr" target="#b18">Jiao et al., 2019;</ref><ref type="bibr" target="#b60">Sun et al., 2020b)</ref>. It would be interesting to see whether our approach can be applied on top of these smaller pretrained models to for even greater parameter-efficiency.</p><p>Learning to mask Our work is closely related to the line of work on learning mask parts of deep networks with differentiable relaxations of binary masks for model pruning and parameter sharing <ref type="bibr" target="#b65">(Wang et al., 2019b;</ref><ref type="bibr" target="#b74">Zhao et al., 2020;</ref><ref type="bibr" target="#b51">Sanh et al., 2020;</ref><ref type="bibr" target="#b42">Radiya-Dixit &amp; Wang, 2020;</ref><ref type="bibr" target="#b31">Mallya et al., 2018;</ref><ref type="bibr" target="#b12">Guo et al., 2019;</ref><ref type="bibr" target="#b59">Sun et al., 2020a)</ref>. While these works also enable parameter-efficient transfer learning, they generally apply the masks directly on the pretrained parameters instead of on the difference vector as in the present work.</p><p>Regularization towards pretrained models Finally, diff pruning is also related to works which regularize the learning process towards pretrained/shared models for continual learning <ref type="bibr" target="#b48">(Rusu et al., 2016;</ref><ref type="bibr" target="#b19">Kirkpatrick et al., 2017;</ref><ref type="bibr" target="#b53">Schwarz et al., 2018)</ref>, domain adaptation <ref type="bibr" target="#b66">(Wiese et al., 2017;</ref><ref type="bibr" target="#b33">Miceli Barone et al., 2017)</ref>, and stable finetuning <ref type="bibr">(Lee et al., 2020)</ref>. These works typically do not utilize sparse regularizers and target a different goal than parameter-efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We propose diff pruning as a simple approach for parameter-efficient transfer learning with pretrained models. Experiments on standard NLP benchmarks and models show that diff pruning can match the performance of fully finetuned baselines while requiring only a few additional parameters per task. We also propose a structured variant of diff pruning which provides further improvements. Avenues for future work include (i) applying this approach to other architectures (e.g. ConvNets for vision applications), (ii) injecting parameter-efficiency objectives directly into the pretraining process (to pretrain models that are better suited towards sparse transfer learning), and (iii) combining diff pruning with other techniques (e.g. adapters) to achieve even greater parameter-efficiency. Table <ref type="table" target="#tab_4">5</ref>: Best hyperparameters for the GLUE tasks based on the respective validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 CONSISTENCY OF NONZERO PARAMETERS Figure <ref type="figure" target="#fig_2">3</ref> shows the percentage of modified parameters attributable to each layer across 5 runs of SST-2. We find that there is nonotrivial variation in sparsity across runs, but also a degree of consistency. For example, the first layer is modified considerably more than other layers across all runs.</p><p>A.2 HYPERPARAMETERS </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (Left) Average performance on the GLUE validation set across different target sparsity rates for the different methods. (Right) Results with BERTLARGE on the SQuAD v1.1 validation set.</figDesc><graphic url="image-1.png" coords="6,127.80,219.30,158.40,126.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of modified parameters attributable to each layer for different tasks at 0.5% target sparsity. The layers are ordered from earlier to later (i.e. the embedding layer is shown at the top). The x-axis for each plot goes from 0% to 20%.</figDesc><graphic url="image-2.png" coords="8,108.00,81.86,396.01,118.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Percentage of modified parameters attributable to each layer for 5 different runs of SST-2 at 0.5% target sparsity. The layers are ordered from earlier to later (i.e. the embedding layer is shown at the top). The x-axis for each plot goes from 0% to 20%.</figDesc><graphic url="image-3.png" coords="14,116.66,81.86,376.21,161.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Total New params QNLI * SST-2 MNLI m MNLI mm CoLA MRPC STS-B RTE QQP GLUE benchmark test server results with BERTLARGE models. (Top) Results with adapter bottleneck layers (brackets indicate the size of bottlenecks), taken from from Houlsby et al. (2019). (Bottom) Results from this work. * QNLI results are not directly comparable across the two works as the GLUE benchmark has updated the test set since then. To make our results comparable the average column is calculated without QNLI.</figDesc><table><row><cell>Avg</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Structured diff pruning results on the validation set with different target sparsity rates. Average performance includes all 9 tasks.</figDesc><table><row><cell>Preprint</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Diff vector target sparsity</cell><cell cols="10">QNLI SST-2 MNLIm MNLImm CoLA MRPC STS-B RTE QQP</cell><cell>Avg</cell></row><row><cell>0.10%</cell><cell>92.7</cell><cell>93.3</cell><cell>85.6</cell><cell></cell><cell>85.9</cell><cell>58.0</cell><cell>87.4</cell><cell>86.3</cell><cell cols="2">68.6 85.2</cell><cell>82.5</cell></row><row><cell>0.25%</cell><cell>93.2</cell><cell>94.2</cell><cell>86.2</cell><cell></cell><cell>86.5</cell><cell>63.3</cell><cell>90.9</cell><cell>88.4</cell><cell cols="2">71.5 86.1</cell><cell>84.5</cell></row><row><cell>0.50%</cell><cell>93.4</cell><cell>94.2</cell><cell>86.4</cell><cell></cell><cell>86.9</cell><cell>63.5</cell><cell>91.3</cell><cell>89.5</cell><cell cols="2">71.5 86.6</cell><cell>84.8</cell></row><row><cell>1.00%</cell><cell>93.3</cell><cell>94.2</cell><cell>86.4</cell><cell></cell><cell>87.0</cell><cell>66.3</cell><cell>91.4</cell><cell>89.9</cell><cell cols="2">71.1 86.6</cell><cell>85.1</cell></row><row><cell>100%</cell><cell>93.5</cell><cell>94.1</cell><cell>86.5</cell><cell></cell><cell>87.1</cell><cell>62.8</cell><cell>91.9</cell><cell>89.8</cell><cell cols="2">71.8 87.6</cell><cell>85.0</cell></row><row><cell></cell><cell></cell><cell cols="6">QNLI SST-2 MNLI CoLA MRPC STS-B</cell><cell>RTE</cell><cell>QQP</cell><cell>Avg</cell></row><row><cell>Non-structured</cell><cell></cell><cell>6.2%</cell><cell>6.1%</cell><cell>6.0%</cell><cell>6.4%</cell><cell>6.1%</cell><cell>6.4%</cell><cell>7.1%</cell><cell>6.1%</cell><cell>6.3%</cell></row><row><cell>Structured</cell><cell></cell><cell cols="5">37.7% 64.6% 28.8% 20.8% 13.2%</cell><cell cols="3">12.2% 12.7% 34.9%</cell><cell>28.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Percentage of groups where all of the parameters in the group are fully zero for structured vs. nonstructured diff pruning at 0.5% target sparsity. We group based on each matrix/bias vector, resulting in 393 groups in total.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>QNLI SST-2 MNLI m MNLI mm CoLA MRPC STS-B RTE QQP Avg (Top) Sparsity and performance without magnitude pruning on the validation set with structured diff pruning. We show results before and after finetuning with a fixed mask. (Bottom) Performance with 0.5% target sparsity and fixed mask finetuning.</figDesc><table><row><cell>Sparsity w/o Mag. Pruning</cell><cell>1.5%</cell><cell>0.6%</cell><cell>0.8%</cell><cell>0.8%</cell><cell>1.6%</cell><cell>2.4%</cell><cell>3.3%</cell><cell cols="2">0.7% 0.6%</cell><cell>1.4%</cell></row><row><cell>No Finetuning</cell><cell>92.1</cell><cell>93.8</cell><cell>84.3</cell><cell>84.8</cell><cell>59.8</cell><cell>87.7</cell><cell>87.1</cell><cell>58.9</cell><cell>84.4</cell><cell>81.4</cell></row><row><cell>Finetuning</cell><cell>93.8</cell><cell>94.0</cell><cell>86.2</cell><cell>86.8</cell><cell>63.1</cell><cell>91.9</cell><cell>89.7</cell><cell>71.8</cell><cell>86.5</cell><cell>84.9</cell></row><row><cell>Mag. Pruning + Finetuning</cell><cell>93.4</cell><cell>94.2</cell><cell>86.4</cell><cell>86.9</cell><cell>63.5</cell><cell>91.3</cell><cell>89.5</cell><cell>71.5</cell><cell>86.6</cell><cell>84.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>shows hyperparameters we used for training GLUE tasks. For SQuAD v1.1 experiments, we ran distributed training across 8 GPUs, and used per gpu batch size 3, maximum sequence length 384, document stride 128, learning rate 3 × 10 −5 , number of initial training epochs 2 and number of finetuning epochs 2.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">To reduce notation clutter we subsume the parameters of the task-specific output layer, which is not pretrained, into θpretrained. We do not apply the L0-norm penalty on these parameters during training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">  4  We found sampling once to work as well as other alternatives (e.g. based on multiple samples).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><ref type="bibr" target="#b65">Wang et al. (2019b)</ref> show that it also is possible to inject such a constraint softly into the training objective by regularizing the expected model size towards a certain rate. However, since the constraint is soft this approach also makes it difficult to target an exact sparsity rate.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">  6  Concretely, one can obtain θτ through usual finetuning, set δτ = θτ − θpretrained, and then apply magnitude pruning followed by additional finetuning on δτ</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">.7  While groups can be defined in various ways, we found that defining groups based on each matrix/bias vector of the pretrained model was simple and worked well enough.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><ref type="bibr" target="#b69">Wu et al. (2020)</ref> observe that finetuning later layers generally performs better than finetuning earlier layers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6">These values were found via by a light hyperparameter search on the SST-2 validation set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7">This definition of groups is implementation-specific since it depends on how one concatenates the input vector before each affine layer. Our grouping is based on Huggingface's BERT implementation at commit 656e1386a296d696327a9db37de2ccccc79e2cc7 (available at https://github.com/ huggingface/transformers/blob/656e1386a296d696327a9db37de2ccccc79e2cc7/ src/transformers/modeling_bert.py). In preliminary experiments we found this simple definition to work well compared to alternative group definitions (e.g. based on individual neurons).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8">However diff pruning incurs additional storage cost due to storing the nonzero positions of the diff vector.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9">To simulate this restricted setting, we tried applying diff pruning only on the dense transformations just before the output of each layer (i.e. after self-attention layers), and observed much worse performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10">Dataset distillation<ref type="bibr" target="#b64">(Wang et al., 2018)</ref> tackles this question in the context of vision models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank the anonymous reviewers for their valuable feedback on the initial draft. AMR was supported by NSF 1704834 and NSF Career 2037519.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Simple but Tough-to-Beat Baseline for Sentence Embeddings</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<title level="m">Multitask Learning. Machine Learning</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Universal sentence encoder for English</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><surname>Kurzweil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP: System Demonstrations</title>
				<meeting>EMNLP: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Lottery Ticket Hypothesis for Pre-trained BERT Networks</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12223</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Born-Again Multi-Task Networks for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Bam!</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-Supervised Sequence Learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01852</idno>
		<title level="m">Languageagnostic BERT Sentence Embedding</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Prakhar</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ali Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Winslett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11985</idno>
	</analytic>
	<monogr>
		<title level="m">Compressing Large-Scale Transformer-Based Models: A Case Study on BERT</title>
				<imprint>
			<date type="published" when="1999">1999. 2020</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Rep4NLP 2020 Workshop at ACL 2020</title>
				<meeting>Rep4NLP 2020 Workshop at ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SpotTune: Transfer Learning Through Adaptive Fine-Tuning</title>
		<author>
			<persName><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tajana</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quentin de Laroussilhe, Andrea Gesmundo, and Mona Attariyanand Sylvain Gelly. Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<title level="m">TinyBERT: Distilling BERT for Natural Language Understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. Skip-Thought Vectors</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
				<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Piyush Sharma, and Radu Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models</title>
		<author>
			<persName><forename type="first">Cheolhyoung</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanmo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic Knowledge and Transferability of Contextual Representations</title>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Task Deep Neural Networks for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
				<imprint>
			<date type="published" when="2019">2019c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient Episodic Memory for Continual Learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
				<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Sparse Neural Networks through L 0 Regularization</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kingma</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
				<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularization techniques for fine-tuning in neural machine translation</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Valerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miceli</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient Estimation of Word Representations in Vector Space</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">German</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">L</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07569</idno>
		<title level="m">Continual Lifelong Learning with Neural Networks: A Review</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Ruckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho Amd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00247</idno>
		<title level="m">AdapterFusion: Non-Destructive Task Composition for Transfer Learning</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Ruckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifton</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07779</idno>
		<title level="m">AdapterHub: A Framework for Adapting Transformers</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00052</idno>
		<title level="m">MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer</title>
				<imprint>
			<date type="published" when="2020">2020c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How fine can fine-tuning be? Learning efficient language models</title>
		<author>
			<persName><forename type="first">Evani</forename><surname>Radiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Dixit</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
				<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02054</idno>
		<title level="m">Memory Optimizations Toward Training Trillion Parameter Models</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient Parametrization of Multi-domain Deep Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03844</idno>
		<title level="m">Poor Man&apos;s BERT: Smaller and Faster Transformer Models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing</title>
				<meeting>5th Workshop on Energy Efficient Machine Learning and Cognitive Computing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07683</idno>
		<title level="m">Movement Pruning: Adaptive Sparsity by Fine-Tuning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">It&apos;s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Progress &amp; Compress: A scalable framework for continual learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Continual Learning with Deep Generative Replay</title>
		<author>
			<persName><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
				<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning</title>
		<author>
			<persName><forename type="first">Asa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Patient Knowledge Distillation for BERT Model Compression</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning</title>
		<author>
			<persName><forename type="first">Ximeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
				<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Mobile-BERT: a compact task-agnostic BERT for resource-limited devices</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">BERT Rediscovers the Classical NLP Pipeline</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Iulia Turc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<title level="m">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Dataset Distillation</title>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10959</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Wohlwend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04732</idno>
		<title level="m">Structured Pruning of Large Language Models</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Neural domain adaptation for biomedical question answering</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
				<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Towards Universal Paraphrastic Sentence Embeddings</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Gugger</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.03771</idno>
		<editor>Mariama Drame, Quentin Lhoest, and Alexander M. Rush</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Similarity Analysis of Contextual Word Representation Models</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
				<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Jeffrey O Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning universal sentence representations with mean-max attention autoencoder</title>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">An Unsupervised Sentence Embedding Method byMutual Information Maximization</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwan</forename><surname>Hui Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Masking as an Efficient Alternative to Finetuning for Pretrained Language Models</title>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12406</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
