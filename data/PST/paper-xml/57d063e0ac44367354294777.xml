<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture</title>
				<funder ref="#_uWcx28T #_zPMWhga #_M4fJgJY #_FmXzXxg">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_sbHBZV6">
					<orgName type="full">Dept. of Energy</orgName>
				</funder>
				<funder ref="#_638T3Yt">
					<orgName type="full">AFOSR</orgName>
				</funder>
				<funder ref="#_R2GhPHq #_Eh224Hf">
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_pGx8YAT">
					<orgName type="full">U.S. Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
							<email>yanqiz@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<settlement>Princeton</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Henry</forename><surname>Hoffmann</surname></persName>
							<email>hankhoffmann@cs.uchicago.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
							<email>wentzlaf@princeton.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<settlement>Princeton</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Infrastructure as a Service (IaaS) Clouds have grown increasingly important. Recent architecture designs support IaaS providers through fine-grain configurability, allowing providers to orchestrate low-level resource usage. Little work, however, has been devoted to supporting IaaS customers who must determine how to use such fine-grain configurable resources to meet quality-of-service (QoS) requirements while minimizing cost. This is a difficult problem because the multiplicity of configurations creates a non-convex optimization space. In addition, this optimization space may change as customer applications enter and exit distinct processing phases. In this paper, we overcome these issues by proposing CASH: a fine-grain configurable architecture co-designed with a costoptimizing runtime system. The hardware architecture enables configurability at the granularity of individual ALUs and L2 cache banks and provides unique interfaces to support lowoverhead, dynamic configuration and monitoring. The runtime uses a combination of control theory and machine learning to configure the architecture such that QoS requirements are met and cost is minimized. Our results demonstrate that the combination of fine-grain configurability and non-convex optimization provides tremendous cost savings (70% savings) compared to coarse-grain heterogeneity and heuristic optimization. In addition, the system is able to customize configurations to particular applications, respond to application phases, and provide near optimal cost for QoS targets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Currently, cloud customers have little to no control. CASH gives these customers control with a combination of a configurable architecture -which can be reconfigured on an extremely fine timescale -and an optimizing runtime system -which can meet quality-of-service (QoS) guarantees while minimizing cost. Such configurability, in combination with automated runtime management, gives customers fine-grain control. We believe that deployment of such a system would then also benefit cloud providers by attracting more customers.</p><p>The move to IaaS systems has spawned new architectures optimized for the data center and the Cloud. Microservers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b59">59]</ref>, data center optimized accelerators <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>, and fine-grain reconfigurable processors <ref type="bibr" target="#b64">[64]</ref> are all examples.</p><p>Highly configurable processors have unique benefits for the IaaS Cloud as resources can be moved between customers. For instance, configurable cache hierarchies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref> and configurable pipeline architectures (modifiable issue width, instruction window size, number of physical registers, etc.) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b64">64]</ref> allow fine grain control over resource scheduling. As fine-grain configurability is adopted in data centers, IaaS customers are left with the challenge of configuring and purchasing fine-grain resources to meet their QoS needs while minimizing their cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Challenges</head><p>On the one hand, fine-grain configurability has the potential to reduce costs by tailoring resource usage to the customers' demands. One the other hand, it greatly increases complexity, which could alienate customers. Indeed, finding the optimal configuration for an application and customer's needs is often an integer programming problem. Furthermore, as configurability becomes increasingly fine-grained, the optimization space becomes non-convex, further increasing the difficulty of optimization. To realize the potential of fine-grain configuration in IaaS, we must find simple interfaces that hide users from the complexity of fine-grain configuration while ensuring that configuration is guided to meet their needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The CASH Architecture and Runtime</head><p>This paper addresses the above challenges with CASH: Cost-aware Adaptive Software and Hardware. CASH codesigns both a hardware architecture and a runtime management system to provide QoS guarantees while minimizing cost in support of IaaS customers. CASH allows customers to express their performance needs through a simple interface. The CASH runtime then configures the CASH architecture to meet those goals with minimal cost.</p><p>The CASH architecture consists of a homogeneous fabric of ALUs, FPUs, fetch units, and caches which can be dynamically composed to create cores optimized for a particular application. In an IaaS setting, this configurability enables the runtime to match the in-core resources to user performance goals, providing many of the benefits of heterogeneous multicores while maintaining a homogeneous fabric. The CASH architecture is inspired by the Sharing Architecture <ref type="bibr" target="#b64">[64]</ref>, but improves on it with fast reconfiguration, a well defined software-hardware interface which is needed for the CASH runtime, and the use of an on-chip network to monitor remote cores' performance. Monitoring a remote core's performance is critical for the CASH runtime to be able to assess the effectiveness of its control, but is challenging as the CASH architecture does not contain fixed cores.</p><p>The CASH runtime overcomes the software challenges listed above. It uses a combination of control theory and reinforcement learning to provide QoS guarantees while adapting to application phases and non-convex optimization spaces. The control system ensures QoS guarantees are met while reinforcement learning adapts control to application phases and prevents the system from getting trapped in local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Summary of Results</head><p>We evaluate CASH using a cycle-accurate simulator and 13 applications including the apache webserver, a mail server, and the x264 video encoder. For each application, we set QoS goals and use CASH to meet those goals while minimizing cost. Our results demonstrate the following:</p><p>? Fine-grain configurability enables tremendous potential cost savings, but that savings must be achieved by navigating non-convex optimization spaces that vary with application phase (Sec. II).</p><p>? The architecture and runtime support fast reconfiguration with low overhead (Sec. VI-A). ? The CASH runtime produces near-optimal results with rare QoS violations. In contrast, convex optimization techniques result in more QoS violations and increased cost (Sec. VI-C). ? CASH quickly adapts to application phases to minimize cost (Sec. VI-D). ? CASH produces over 3? cost savings compared to coarse-grain heterogeneous architectures (Sec. VI-E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contributions</head><p>In summary, this paper makes the following contributions: showing that the combination of fine-grain configurability and adaptive management provides near optimal cost for just a small number of QoS violations (see Sec. VI). While other architectures have employed fine-grain configurability to support IaaS providers, we believe this is the first approach designed to support IaaS customers through its combination of cost-optimizing architecture and runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATIONAL EXAMPLE</head><p>There is ample evidence in the literature that fine-grain configurable architectures have the potential to produce greater energy efficiency and cost savings than static architectures or even coarse-grain configurable architectures <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b64">64]</ref>. The one drawback of these approaches is that fine-grain configurability creates complicated, non-convex optimization spaces characterized by the presence of local extrema that can vary as the application moves from phase to phase.</p><p>This section motivates the need to co-design both reconfigurable architecture and runtime management system. We first demonstrate the complexity of fine-grain resource management and then show its potential benefits for IaaS customers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Allocating Resources within the CASH Architecture</head><p>We demonstrate the potential of fine-grain configuration by running the x264 video encoder <ref type="bibr" target="#b4">[5]</ref> on the CASH Architecture (which extends the Sharing Architecture <ref type="bibr" target="#b64">[64]</ref>). The video encoder is an excellent example of our target application as it has a clear QoS requirement: to maintain a particular frame rate. We would like to meet this frame rate while minimizing cost and adapting to phases in the source video.</p><p>The CASH Architecture supports this goal by allowing allocation of Slices and L2 Cache banks. Slices are simple out-of-order cores with 1 ALU, 1 Load-Store Unit, and a small L1 cache. Multiple Slices and L2 Cache banks can be combined to create a powerful virtual core. Users rent these resources and pay a cost based on their area. We would like to minimize resource usage (and thus cost) while ensuring QoS.</p><p>To demonstrate the optimization space, we run x264 on the CASH architecture with every possible virtual core constructed from 1 to 8 Slices and 64KB to 8MB L2 Caches in power of two steps. For our input video, we have identified 10 distinct phases of computation. For each phase and each virtual core configuration we measure the performance in instructions per clock. The results are shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Figs. 1a-1j show contour plots that map virtual core configuration into performance. The x-axes show the L2 cache size (on a logarithmic scale), while y-axes show the number of Slices. Intensity represents performance, brighter shading indicates higher performance, with white being the highest for a given phase. Fig. <ref type="figure" target="#fig_0">1k</ref> shows a summary of the information in the contour plots. This data clearly demonstrates the difficulty of allocating fine-grain resources to an application with distinct phases. Six of ten phases have local optima distinct from the true optimal performance. In addition, the location of the true optimal changes from phase to phase. In fact, no two consecutive phases have the same optimal configuration. In summary, any scheme that dynamically allocates Slices and L2 Cache banks to construct virtual cores must be capable of both avoiding local optima (i.e., handling nonconvex spaces) and adapting to phase changes. Avoiding local extrema is important as the true optimum is often far from local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benefits of Intelligent Resource Allocation</head><p>We have demonstrated the complexity of fine-grain resource management. We now show its potential benefits for IaaS customers. We again consider x264, this time with a QoS requirement that every phase meets the desired throughput. Optimal resource management must find the smallest virtual core configuration that ensures each phase runs at this speed. We compare optimal resource allocation to two other schemes: convex optimization and race-to-idle.</p><p>The convex optimization scheme uses a feedback control system to meet the QoS requirement <ref type="bibr" target="#b20">[21]</ref>. It has a model of cost and performance tradeoffs and allocates resources to keep the performance at the desired QoS, but its convex model cannot account for local optima. We assume the raceto-idle scheme has some prior knowledge of the application and knows the lowest cost configuration that meets the QoS requirement in the worst-case. The race-to-idle approach allocates this worst-case virtual core for every phase. If a phase finishes early, this approach simply idles until the next phase begins. We (optimistically) assume that idling happens instantaneously and incurs no cost.</p><p>The results of this comparison are shown in Fig. <ref type="figure" target="#fig_2">2</ref>. The plot on the top shows the cost. The plot on the bottom shows performance normalized to the QoS goal. The x-axes of both plots show cycle count (in millions of cycles), while the y-    Two observations jump out immediately. First, convex optimization techniques cannot handle resource allocation for this application on this architecture as the incurred cost is much higher than optimal and the QoS requirement is repeatedly violated. Second, the race-to-idle approach does not violate QoS (given our optimistic assumptions), but incurs a much higher cost than necessary. In fact, for x264 both race-to-idle and convex optimization produce over 4.5? optimal cost. This is a significant additional cost for the customer to bear. Clearly, there is a need to help IaaS customers by combining fine-grain configurability with automated resource management. CASH's goal is to achieve near-optimal fine-grain resource management with minimal QoS violations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE CASH HARDWARE ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Overview</head><p>The CASH architecture extends the Sharing Architecture <ref type="bibr" target="#b64">[64]</ref> -a prior configurable core architecture -by <ref type="bibr" target="#b0">(1)</ref> innovating in fast reconfiguration, (2) providing a well designed hardware-software interface, and (3) allowing remote cores' performance to be monitored over an on-chip network. In this section, as background, we begin by describing the overall ideas of the CASH Architecture and then in Section III-B we describe how we co-designed the CASH Architecture to have support for fast reconfiguration.  CASH is a fine-grain configurable architecture which provides the flexibility to dynamically "synthesize" a virtual core with the correct amount of ALUs and cache, based on an application's demand and the need to optimize cost. Unlike multicore and manycore architectures where the processor core is statically fixed at design and fabrication time, the CASH Architecture enables the core to be reconfigured and use resources across a single chip. This fine-grain reconfigurability is especially useful for IaaS applications which have fixed performance goals (in terms of acceptable latency or throughput) and need to meet those goals while minimizing cost.</p><p>At the top level, like existing multicore chips used for IaaS applications, CASH can group multicore cores into Virtual Machines (VMs). Unlike fixed architecture multicore processors, the VMs in the CASH Architecture are composed of cores which themselves are composed of a variable number of ALUs, and cache. We call this flexible core a virtual core. A virtual core is composed out of one or more Slices and one or more L2 Cache Banks. Figure <ref type="figure" target="#fig_3">3</ref> shows an example array of Slices and Cache Banks. A full chip contains 100's of Slices and Cache Banks. The key insight is to construct highly scalable architectures on a homogeneous fabric.</p><p>The basic unit of computation in the CASH Architecture is a Slice as shown in Figure <ref type="figure" target="#fig_4">4</ref>. A Slice is effectively a simple out-of-order processor with one ALU, one Load-Store Unit, the ability to fetch two instructions per cycle, and a small Level 1 Cache. Multiple Slices can be grouped together to increase the performance of sequential programs thereby empowering users to make decisions about trading off ILP vs. TLP vs. Process level parallelism vs. VM level parallelism while all utilizing the same resources.</p><p>The CASH Architecture is designed to enable very flexible sharing of resources and does not impose hierarchical grouping of Slices or Cache Banks. This feature enables IaaS Clouds to spatially allocate computation resources. In order to achieve this, we use general purpose switched interconnects wherever possible. Neither Slices or Cache Banks need to be contiguous in a virtual core for functionality. But for the purpose of performance, we group adjacent Slices into a single virtual core to reduce operand communication cost and Cache access latency. Cache configuration is decoupled from the Slices, and the Cache access latency is modeled in proportion to the distance from the Cache to the Slices and Cache size. We do not see the need for contiguous Slices to be a limitation as all Slices are interchangeable and equally connected. Therefore, fixing fragmentation problems is as simple as rescheduling Slices to virtual cores. As larger Cache size leads to higher communication cost, virtual core configuration search space can be non-convex.</p><p>The CASH Architecture minimizes changes to the software stack by utilizing multiple switched interconnect whenever possible. Distributed local register files enabled by the two register renaming stages improve virtual core scalability. Applications do not need to be recompiled to use different Slice and Cache configurations. This property is key, as it allows the CASH runtime to dynamically change architectural configurations without requiring any application-level changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reconfiguration 1) Hardware Reconfiguration:</head><p>To reconfigure virtual cores, we rely on the CASH runtime to control the connections of the Slice and Cache interconnect. The runtime is time multiplexed with the client virtual machines. But, unlike client VMs which run on reconfigurable cores, we propose having the runtime execute only on single-Slice virtual cores. By having the runtime execute on single Slices only, it can then locally reconfigure protection registers and interconnection state to setup and tear-down client virtual cores. The runtime bypasses use of the reconfigurable cache system to prevent having to flush the L2 Caches on every time Slice.</p><p>When the number of Slices in a virtual core is reduced, there is potentially register state which needs to be transmitted to the surviving Slices within the virtual core. Because we use a distributed local register file scheme, one architectural register can have multiple copies in different Slices. In this case, only one copy (the primary one that originally wrote the value to the register) for each architectural register is conserved. In order to carry this out, we use a Register Flush instruction which pushes all of the dirty architectural register state to other Slices in the same virtual core using operand forwarding messages. This can be done quickly because there are only a limited number of global logical registers, only one copy of data needs to be saved, and the Scalar Operand Network is fast for transferring register data. Architectural registers are mapped onto global logical registers with global logical registers serving as a register name space which is mapped across all of the Slices, while local registers are the actual storage registers in individual Slices. When virtual core's L2 Cache configuration changes, all dirty state in L2 Cache Banks that are being removed must be flushed to main memory before reconfiguration. Figure <ref type="figure" target="#fig_5">5</ref> shows an example of flushing registers when shrinking a two-Slice virtual core to a one-Slice virtual core. Originally global registers gr0, gr1, and gr2 are primarily written and renamed to lr0 by Slice1, lr0 by Slice2 and lr2 by Slice2. Slice1 has an additional copy of gr1 due to an instruction read, and gr1 is renamed to lr1 by Slice1 locally. Similarly, Slice2 has a copy of gr0 renamed to lr1 locally. When the virtual core shrinks from two-Slices to one-Slice, the survivor, Slice1, needs to get all the updated register values from Slice2. As Slice2 is the primary writer of gr1 and gr2, it flushes both values to Slice1. Upon receiving the values, Slice1 determines to only save gr2 to lr3 as it already has a copy of gr1 due to a previous register read. Because only the primary writers need to flush register values, the total number of flushes is bounded by the total number of global registers.</p><p>2) Interface to Software: The software adaptation engine requires information such as performance and cost from the configurable hardware. The hardware communicates the performance and estimated cost to the software adaptation engine. The software engine uses this information to make more informed reconfiguration decisions after a pre-determined reconfiguration interval. We use instruction commit rate as a measure of performance. One challenge is how best to interface between the software adaptation engine and the hardware. This is made even more difficult because performance counter registers are typically read at the core level and the CASH Architecture has no fixed core.</p><p>To solve the challenge of performance counting without fixed cores, the CASH Architecture connects performance counters to a dedicated on-chip network, as shown in Figure <ref type="figure" target="#fig_4">4</ref>. The CASH Runtime Interface Network allows a virtual core with sufficiently high privilege (e.g., executing the CASH runtime) to measure performance counters on other virtual cores. The CASH runtime can read instruction commit rate, cache miss rate, branch miss-predict rate or other common performance counter information. Performance counters are queried with a simple request and reply protocol targeting a particular Slice. Each performance counter sample is timestamped which enables the CASH runtime to synthesize overall performance for a virtual core from performance counter readings from individual Slices.</p><p>The CASH Runtime Interface Network is also used to send reconfiguration commands such as EXPAND and SHRINK which target a particular Slice or L2 cache bank. Removing a Slice from a virtual core requires that the primary written registers be flushed to the surviving Slices through the Operand Network. L2 Cache banks which are being removed flush their dirty data to main memory across the L2 memory network. The data networks already exist in the Sharing Architecture, but the CASH Runtime Interface Network is newly added to support fast reconfiguration and performance gathering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RUNTIME RESOURCE ALLOCATION</head><p>The CASH runtime configures Slices and L2 Cache to meet an application's QoS demands while minimizing cost. The runtime addresses the software challenges listed in the introduction: (1) guaranteeing QoS guarantees, (2) adapting to phases in application workload, and (3) avoiding local optima. Additionally, the runtime is designed to be very low overhead.</p><p>The CASH runtime is illustrated in Fig. <ref type="figure" target="#fig_6">6</ref>. It takes a QoS goal q and measures the current QoS q(t) at time t. It computes the error e(t) between the goal and the delivered QoS. This error is passed to a Controller which computes a speedup s(t) to apply to the application. The controller is modified online by an Estimator that adapts to application phases. The speedup signal is passed to a LearningOptimizer which determines the minimal cost configuration k(t) of Slices and L2 to achieve the desired speedup. The Controller enforces QoS requirements, the Estimator recognizes application phases, and the LearningOptimizer selects the lowest cost configuration. We discuss each in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Control System for QoS</head><p>CASH is influenced by prior research that has effectively deployed control systems to meet QoS demands <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b63">63]</ref>. These control approaches measure runtime QoS feedback and compute a control signal indicating a resource allocation that maintains the desired QoS. The CASH runtime first computes the error e(t) between the QoS requirement q 0 and the current QoS level q(t): e(t) = q 0q(t)</p><p>(1)</p><p>The controller eliminates this error. There are tradeoffs between how fast the controller drives the error to zero and its sensitivity to noise. The CASH runtime employs a deadbeat controller to eliminate the error as fast as possible <ref type="bibr" target="#b33">[34]</ref> and corrects noise in the Estimator. Therefore, CASH computes speedup s(t) given an error signal:</p><formula xml:id="formula_0">s(t) = s(t -1) + e(t) b (<label>2</label></formula><formula xml:id="formula_1">)</formula><p>where b represents the base QoS of the application; i.e., its QoS when running on one Slice with a 64KB L2.</p><p>Eqns. 1 and 2 together comprise a standard control system similar to prior approaches, but with some limitations. It will react to phases, but not as quickly as we might like. Furthermore, it has no ability to adapt to the types of nonconvex optimization spaces illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. In the next two section, we extend this type of standard control design to provide these two features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Estimating Application Phase Changes</head><p>The CASH runtime must adapt to phases like those discussed in Sec. II. The key parameter for handling phases is the constant value b in Eqn. 2 representing base QoS. Analytically, a change in phase represents a fundamental shift in the value of this parameter. It is not feasible, however, to directly measure base speed as doing so would likely violate QoS. Instead, the CASH runtime continually estimates this value, b(t). When the application changes phase, this estimate will change, and the speedup values produced by Eqn. 2 will reflect the phase shift. For example, if the base speed increases by 2? then we would like the speedup produced by the controller to drop by this same factor. Clearly, an accurate estimate of base speed substituted into Eqn. 2 will produce the desired effect.</p><p>CASH learns b using a Kalman filter <ref type="bibr" target="#b58">[58]</ref> based on the time-varying model:</p><formula xml:id="formula_2">b(t) = b(t -1) + ? b(t) q(t) = s(t -1) b(t -1) + ? q(t)<label>(3)</label></formula><p>which describes b and q subject to both disturbance, e.g., a page fault, and noise, i.e., natural variation in application's QoS signal, (respectively: ? b and ? q). Denoting the system QoS variance and measurement variance as v(t) and r, the Kalman filter formulation is b-</p><formula xml:id="formula_3">(t) = b(t -1) E -(t) = E(t -1) + v(t) Kal(t) = E -(t) s(t) [s(t)] 2 E -(t) + r b(t) = b-(t) + Kal(t) [q(t) -s(t) b-(t)] E(t) = [1 -Kal(t) s(t -1)] E -(t)<label>(4)</label></formula><p>where Kal(t) is the Kalman gain for the QoS, b-(t) is the a priori estimate of b(t) and b(t) is the a posteriori one, and E -(t) is the a priori estimate of the error variance while E(t) is the a posteriori one. CASH uses a Kalman filter because it produces a statistically optimal estimate of the system's parameters, and is provably exponentially convergent <ref type="bibr" target="#b9">[10]</ref>. Practically, this means that the number of time steps required to detect phase changes (i.e., changes in the estimate of base speed) will be -in the worst case -logarithmic in the difference between the base speeds for two consecutive phases; i.e., convergence time ? log(|b phase ib phase i+1 |). Note that the only required (i.e., not directly measured from the hardware) parameter is r, the measurement noise, which we take as a constant property of the hardware architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning to Minimize Cost</head><p>The combination of the Controller and Estimator ensures QoS and reacts quickly to phases. It does not, however, have a notion of optimality. The LearningOptimizer from Fig. <ref type="figure" target="#fig_6">6</ref> determines the architectural configuration that meets the speedup signal s(t) with minimal cost; i.e., it maps the speedup signal into the minimal cost Slice and L2 configuration while avoiding local optima.</p><p>The CASH runtime models the architecture as a set of K configurations where each configuration k ? K is a specific number of Slices and L2 size. Each k has a speedup s k and cost c k . CASH schedules configurations over a quantum of ? time units, such that ? k time is spent in each configuration. CASH formulates the cost minimization problem as:</p><formula xml:id="formula_4">minimize(? idle c idle + 1 ? ? k?K (? k c k )) s.t. 1 ? ? k?K ? k s k = s(t) ? idle + ? k?K ? k = ? ? k , ? idle ? 0, ?k<label>(5)</label></formula><p>The first line in Eqn. 5 minimizes the total cost of the schedule. The second line ensures that the average speedup of the schedule is equal to the speedup produced by the Controller (Eqn. 2). The third and fourth lines ensure that the work is completed by the deadline (for QoS).</p><p>While this system has many variables, we know from the theory of linear programming that, because it has only two constraints, there is a solution where only two of ? k are non-zero and all others are zero <ref type="bibr" target="#b7">[8]</ref>. Specifically, the two configurations that will have non-zero times are over and under where:</p><formula xml:id="formula_5">over = argmin k {c k |s k &gt; s(t)} under = argmax k {s k /c k |s k &lt; s(t)} t over = ? ?</formula><p>s(t)s under s overs under t under = ?t over <ref type="bibr" target="#b5">(6)</ref> Thus, if we know s k and c k for all k then the optimization problem is easy to solve <ref type="bibr" target="#b29">[30]</ref>. c k is the cost per configuration per unit time and set by a systems administrator (e.g., it could be $ per chip area). The difficulty is finding s k , which can vary tremendously in different phases of an application. Therefore, the CASH runtime treats the speedup as a timevarying parameter, observes the achieved QoS q(t) and uses a Q-learning approach to learn the true speedup online <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref>. This approach has the advantage that it is computationally cheap, but treats all configurations as independent.</p><p>More sophisticated learning methods that capture correlation between configurations and applications (e.g., <ref type="bibr" target="#b39">[40]</ref>) will be the subject of future work. Using Q-learning, the learned speedup for configuration k at time t is represented as ?k (t):</p><formula xml:id="formula_6">qk (t) = (1 -?) ? qk (t -1) + ? ? q(t) ?k (t) = qk (t) q0 (t)<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Runtime Summary</head><p>The CASH runtime puts the Controller, Estimator, and LearningOptimizer together in Algorithm 1. The runtime executes an infinite loop. At each iteration, it updates its estimate of base speed, then uses that base speed to compute a control signal. The control signal is passed to the optimizer, which computes the optimal configurations for achieving the control signal. The runtime puts the architecture into that configuration and then updates its estimates of the speedups that configuration will produce in the future. The runtime computational complexity is O(1), making it low overhead. As discussed above, it is exponentially convergent to the QoS goal despite changes in application base speed. The LearningOptimizer adapts speedup models online to maintain optimality despite application changes and nonconvex optimization spaces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Infrastructure</head><p>We have created a cycle-accurate simulator, SSim, to model the CASH Architecture and measure its effectiveness. SSim models each subsystem of the CASH Architecture (fetch, rename, issue, execution, memory, commit, and onchip network) along with accurately modeling the Out-of-Order execution and inter-Slice and Slice-to-memory latency. SSim is driven by the full system version of the the Alpha ISA GEM5 <ref type="bibr" target="#b5">[6]</ref> simulator. Table <ref type="table" target="#tab_3">I</ref> shows the base Slice configuration and Table <ref type="table" target="#tab_5">II</ref> shows the base Cache configuration for our simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmark Applications</head><p>We use the complete SPEC CINT2006 benchmark suite <ref type="bibr" target="#b49">[50]</ref>, a subset of PARSEC benchmarks <ref type="bibr" target="#b4">[5]</ref>, the apache web server <ref type="bibr" target="#b2">[3]</ref>, and the postal mail server <ref type="bibr" target="#b11">[12]</ref> to explore the CASH architecture. They are representative benchmarks that provide a measure of performance across a wide practical range of workloads. Another reason we select these benchmarks is because there are abundant phases in the programs <ref type="bibr" target="#b14">[15]</ref>. Programs with phases can benefit most from the reconfigurable hardware. Each PARSEC benchmark denotes a region of interest (ROI) indicating the performance critical part of the benchmark. We consider only the ROI for PARSEC. We evaluate apache serving webpage requests with a concurrency of 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Methodology</head><p>To quantify CASH's ability to minimize cost, we construct an oracle that returns the lowest possible cost configuration for any performance goal. We construct this oracle by running all applications in every possible configuration of the CASH architecture. We manually determine (by examining the simulation output) any distinct processing phases in the application and we then (through brute force) determine the combination of resources that is lowest cost for any performance goal. This brute force exploration allows us to construct the oracle representing true minimal cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>This section evaluates CASH's ability to aid IaaS customers by delivering QoS for minimal cost. We first evaluate both architectural and runtime overhead. We then demonstrate CASH providing near optimal cost for a variety of benchmarks with QoS requirements. We next show timeseries data demonstrating how CASH quickly converges to the required performance. Finally, we compare CASH to a heterogeneous multicore architecture, which provides statically heterogeneous core types; i.e., determined at design time <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref>, rather than CASH's fine-grain configurable cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overhead of Reconfiguration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One of the challenges to exploiting fine-grain reconfigurability is minimizing the architectural and runtime overheads.</head><p>Architectural Overhead: There are four sources of microarchitectural overhead: Slice expansion, Slice contraction, L2 expansion, and L2 contraction. Slice expansion is fast -requiring only a pipeline flush -approximately 15 cycles. Slice contraction takes at most 64 cycles more than expansion to flush local register values to the remaining Slices. Both L2 expansion and contraction require flushing dirty cache lines. Assuming all lines are dirty, a flush takes (BankSize)/(NetworkWidth) cycles; e.g., with L2 bank size of 64KB and network width of 64 bits, flushing the L2 takes 64KB/8B = 8000 cycles. Note this is the worst case cost, in practice we expect that we will not flush the whole cache as only a small number of lines will be dirty. We use a hash table to map physical address to cache banks, the reconfiguration of which is overlapped with dirty line flushing.</p><p>Runtime Overhead: We measure the average cycle count to execute Algorithm 1's 'C' implementation. We note that the execution time of the algorithm is not applicationdependent. We therefore time 1000 iterations of the runtime for the x264 benchmark and report the average cycle count. With one Slice, the runtime takes just under 2000 cycles per iteration. Two and three Slices produce times of 1100 and 977 cycles respectively. This low overhead means a runtime executing on even a single Slice could easily service many applications. As we expect CASH architectures to scale to 100s of Slices, the area overhead of dedicating one slice to the runtime is extremely small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cost Model</head><p>CASH supports IaaS customers by making computer resources rentable at a fine-granularity. To demonstrate this, we assign a cost per unit area. Following Amazon's EC2 pricing (which uses a linear model for additional capacity within a resource class), we assume a linear increase in price per unit area for CASH <ref type="bibr" target="#b1">[2]</ref>. Specifically, we assume $.013 per hour for the minimal architectural configuration of 1 Slice and a 64KB L2 Cache which is the same price Amazon charges for on-demand usage of their t2.micro configuration. Based on silicon area fed by a Verilog implementation of the CASH hardware, this price structure is equivalent to $.0098 per hour for a Slice and $.0032 per hour for 64KB of Cache. We stress that the absolute value of the price does not affect our conclusions. Our comparisons rely on the ratios of the cost incurred by various architectures and resource managers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cost Savings for QoS Requirement</head><p>Our first case study shows that CASH handles a variety of applications, providing near minimal cost with rare QoS violations. For each application, we first determine the QoS requirement as the highest worst case IPC seen for any application; i.e., we tell the runtime system to always be at or above worst case performance. We then deploy the application on CASH, which dynamically allocates the minimal number of Slices and L2 Cache banks to maintain the QoS goal. We compare CASH to three other approaches. First, we compare to our oracle which determined the optimal resource allocation for each application and QoS target. Second, we compare to a convex optimization scheme that uses control theory (similar to what is described in Sec. IV-A), but does not incorporate learning and must rely on a single convex model that captures average case behavior for the entire application. Third, we compare to the race-to-idle approach, which uses the configuration that meets QoS in the worst case and idles (incurring no additional cost) when completing early. For each application we sample performance 1000 times, recording both the total cost and QoS violations. Fig. <ref type="figure">7</ref> presents the data comparing these four resource allocation schemes. In the figure, the top chart shows the total cost for each application while the bottom chart shows the percentage of QoS violations.</p><p>The geometric mean of costs for all applications is listed  in Table <ref type="table" target="#tab_7">III</ref>. This table shows that convex optimization and race-to-idle incur significant costs compared to optimal. Convex optimization is clearly not a good choice for finegrain resource management as it has high costs and high QoS violations. In contrast, race-to-idle produces no QoS violations (under the assumption that the worst case resource allocation is known a priori). These strong QoS guarantees for race-to-idle, however, incur cost over 1.7? the optimal. CASH operates at a reasonable middle ground, as it incurs only 4% more cost than optimal and less than 2% QoS violations. For all applications in this study, CASH delivers the specified QoS at least 95% of the time. We note one application, omnetpp, where convex optimization produces lower cost than CASH, but this comes with the penalty of 20% QoS violations. In this case, convex optimization has reduced cost, but at the penalty of large-scale QoS violations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adapting to Application and Workload Phases</head><p>We present time-series data for individual applications to show how CASH can aid IaaS customers by adapting to  application and workload fluctuations. We show results for the x264 video encoder and the apache webserver.</p><p>We use the same setup as the previous section, but now we report the measured cost rate ($/hour) and performance (normalized to the QoS target) as a function of time. The results are shown in Figs. <ref type="figure">8</ref><ref type="figure" target="#fig_8">9</ref>. We compare convex optimization, race-to-idle, and CASH.</p><p>1) x264: These results show that adaptive, fine-grain resource allocation provides tremendous cost savings for x264 compared to both race-to-idle and convex optimization. If we relate the x264 data from Fig. <ref type="figure">8</ref> to that in Fig. <ref type="figure" target="#fig_0">1</ref>, we see that between 36 and 54 Mcycles (phase 3 from Fig. <ref type="figure" target="#fig_0">1</ref>) has several local extrema and a very expensive true optimal. The convex optimization approach reaches the true optimal at this time, but then stays in the expensive configuration until 144 Mcycles. The CASH runtime, in contrast, detects the application's behavior change and reallocates resources to reduce cost.</p><p>2) apache: The results for the apache webserver are shown in Fig. <ref type="figure" target="#fig_8">9</ref>. For this experiment, we construct an oscillating stream of requests, typical for web servers (e.g., Wikipedia <ref type="bibr" target="#b56">[56]</ref>). In general these oscillations occur over the course of the day. Simulation is too expensive to handle days worth of requests, so we condense the stream into much faster oscillations. The request rate as a function of time is shown in the top chart of Fig. <ref type="figure" target="#fig_8">9</ref>. We set a QoS requirement of 110K cycles per request, which corresponds to the smallest possible worst-case latency on CASH. When request rates are low, it will be easy to hit the target latency. When request rates are high it will require more resources.</p><p>The cost and delivered QoS are shown in the middle and bottom charts of Fig. <ref type="figure" target="#fig_8">9</ref>. All methods adapt to changing request rates and keep the actual latency close to the target. The race-to-idle approach is the most expensive, however, as it always reserves resources to meet the latency in the worstcase. The worst case is only realized around 800 Mcycles, though. Convex optimization saves compared to race-to-idle, but the CASH runtime is the cheapest, providing about an 18% cost reduction compared to convex optimization.</p><p>These results demonstrate CASH's advantage for IaaS customers. Fine-grain configurability allows the customers to provision just the right amount of resources. The adaptive runtime allocates for current case, providing considerable savings over the naive approach of reserving for the worst case and giving up resources when they are not needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison to Heterogeneous Architectures</head><p>To demonstrate the advantages of fine-grain configurability for IaaS customers, we compare the CASH approach to a heterogeneous architecture, similar to the ARM big.LITTLE. The big.LITTLE's coarse-grain approach combines fast, expensive cores with slower, cheaper cores. To compare against this coarse-grain approach, we simulate the CASH architecture with one big core and one small core. The big core is the largest configuration needed to meet the QoS demands of all target applications: 8 Slices with a 4MB L2. The little core is the most cost efficient configuration, on average, across all our benchmarks: 1 Slice with a 128KB L2.</p><p>We want to isolate the cost savings of both the CASH architecture and runtime. Therefore, we perform resource allocation on both the heterogeneous architecture and the CASH architecture using both race-to-idle and the CASH runtime. This gives four points of comparison: CoarseGrain, race; CoarseGrain, adaptive; FineGrain, race; and CASH, which is fine-grain and adaptive. To be clear, the race-to-idle approach cannot change core configuration, but the CoarseGrain, adaptive approach uses the CASH runtime to dynamically shift between big and little cores. We run each benchmark using these combinations of architectures and management systems. The results are shown in Fig. <ref type="figure" target="#fig_9">10</ref>.</p><p>The figure shows cost in the top chart, and the average number of QoS violations in the bottom. These results show the combined power of CASH's fine-grain configuration and an adaptive runtime. The geometric mean of cost for CoarseGrain, race is 0.062$. Replacing race-to-idle with adaptive resource allocation creates the CoarseGrain, adaptive data point, with geometric mean cost $0.048. FineGrain, race produces geometric mean cost $0.029, while CASH's geometric mean cost is $0.017. Adaptation reduces geometric mean cost by about 25%. Fine-grain reconfigurability, by itself, reduces costs by more than 50%. CASH's combination of fine-grain configurability and adaptive resource management reduces the geometric mean of cost by over 70% compared to racingto-idle on a heterogeneous architecture.</p><p>These results summarize the case for the CASH approach. The finer-granularity the architecture, the better it is able to match the needs of the application. Combining such a finegrain architecture with an intelligent management system allows even greater cost savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SMT</head><p>Simultaneous multi-threading (SMT) improves out-oforder core efficiency by time multiplexing the processor pipeline <ref type="bibr" target="#b15">[16]</ref>. CASH also allows resources that are not used by one core to be repurposed, but CASH is a fundamentally different design point from SMT, as CASH focuses on spatial partitioning. For instance, CASH supports cores with many ALUs and a small cache, few ALUs with a large cache, or many other combinations that are not possible in SMTs. Additionally, in multicore SMTs, core size is chosen at fabrication time, while CASH can be configured at runtime. Therefore CASH does not have to face the classic problems that SMT architectures face around resource thrashing; e.g., destructive cache interference when two threads share the same SMT core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Configurable Architectures Without Policies</head><p>The CASH architecture leverages many ideas from Distributed ILP work and fused-core architectures. Distributed ILP removes large centralized structures by replacing them with distributed structures and functional units. This idea has been explored in Raw <ref type="bibr" target="#b55">[55]</ref>, Tilera <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60]</ref>, TRIPS <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, and Wavescalar <ref type="bibr" target="#b54">[54]</ref>. Distributed tiled architectures distribute portions of the register file, cache, and functional units across a switched interconnect. The CASH architecture adopts a similar network topology for communicating instruction operands between function units in different Slices. Unlike Raw and Tilera, we do not expose all of the hardware to software, do not require recompilation, and use dynamic assignment of resources, dynamic transport of operands, and dynamic instruction ordering. TRIPS <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref> has an array of ALUs connected by a Scalar Operand Network and uses dynamic operand transport similar to the CASH architecture. Unlike CASH, TRIPS requires compiler support, and a new EDGE ISA which encodes data dependencies directly. In contrast, CASH uses a dynamic request-and-reply message based distributed local register file to exploit ILP across Slices without the need of a compiler.</p><p>The CASH architecture leverages many ideas from Core Fusion <ref type="bibr" target="#b26">[27]</ref> to distribute resources across cores, but unlike Core Fusion, CASH is designed to scale to larger numbers of Slices (cores). CASH distributes the structures that are centralized in Core Fusion, such as structures for coordinating fetch, steering, and commit across fused cores. Also, we take a more Distributed ILP approach to operand communication and other inter-Slice communication by using a switched interconnect between Slices instead of Core Fusion's operand crossbar. Also, the CASH is designed as a 2D fabric which is created across a large manycore with 100's of cores which can avoid some of the fragmentation challenges that smaller configurations like Core Fusion can have. The interconnect of Slices to Cache is also designed to be flexible across the entire machine unlike Core Fusion which has a shared cache. This can lead to better efficiency and provides a way to partition cache traffic and working sets. The CASH architecture is explicitly co-designed with the CASH runtime. Thus, it supports software driven reconfiguration and monitoring, which is not supported by previous designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Resource management</head><p>PEGASUS <ref type="bibr" target="#b35">[36]</ref>, a feedback-based controller has been proposed to improve energy proportionality of warehouse scale computer systems. Hardware/Software co-design for datacenter power management <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39]</ref> improves datacenter power proportionality while maintaining QoS. MITTS <ref type="bibr" target="#b65">[65]</ref> is a distributed hardware mechanism which shapes memory traffic based on memory request inter-arrival time, enabling fine-grain bandwidth allocation. CASH manages fine-grain computational resources, which is complementary to those techniques. Runtime systems for datacenter resource sharing <ref type="bibr" target="#b61">[61]</ref> improve server utilization while guaranteeing QoS. Bubble-Flux measures instantaneous pressure on the shared hardware resources and controls the execution of batch jobs. All above mentioned techniques are policies for system operators/architects, CASH on the other hand, focuses on policies for Cloud users. It helps Cloud customers to make rational decisions on virtual core configurations of sub-core configurable architecture.</p><p>Scheduling workloads in a IaaS Cloud requires understanding both the software stack and the hardware architecture. Scheduling algorithms for heterogeneous processors have been proposed for MS Bing's index search <ref type="bibr" target="#b44">[45]</ref>. While heterogeneous processors improve interactive data center workloads performance by supporting heterogeneous query lengths, they do not support fine-grain workload heterogeneity. This algorithm is not applicable to fine-grain configurable architecture.</p><p>Offline architectural optimization approaches have been proposed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">62]</ref>, but they have limited applicability to online adaptation. Predictive models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b48">49]</ref> have been proposed to perform online adaptation using offline training. Flicker <ref type="bibr" target="#b41">[42]</ref> also assembles "virtual cores" using dynamic management, but is designed to maximize system throughput for a power budget. CASH tackles the complementary problem of ensuring performance for minimal cost. While similar, these optimization problems have different geometries and require different techniques to solve efficiently. Machine learning based techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref> have been proposed for adaptation of shared caches and off-chip memory bandwidth. These approaches maximize throughput, but do not consider constrained optimization problems; i.e., meeting a performance goal for minimal cost. In addition, the neural networks in these approaches requires significant offline training and the delivered performance is sensitive to the training set. In contrast, CASH requires no a priori knowledge or training.</p><p>Resource-as-a-Service <ref type="bibr" target="#b0">[1]</ref> has been introduced as a model to sell individual resources (such as CPU, memory, and I/O resources) in accordance with market-driven supplyand-demand conditions. CASH enables sub-core fine-grain resource selling by proposing a run-time system supporting sub-core configurable architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Control-based Resource Management</head><p>Like many prior approaches (e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b63">63]</ref>), CASH uses control theory to manage resources and meet performance goals. Control theory is a general technique that allows formal reasoning about the dynamic behavior of the controlled system <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. While the technique is general, implementations are often highly specific to individual applications (e.g., an embedded video encoder <ref type="bibr" target="#b37">[38]</ref>). Some prior work has provided more general implementations by implementing control systems at the middleware layer <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b63">63]</ref>. The middleware handles the complicated construction of the control system, but the application writer must be aware of the components on the system that can be controlled and is responsible for modeling those components. Other approaches have provided automated modeling <ref type="bibr" target="#b16">[17]</ref> or split the modeling process into a piece that is provided by the application developer and a piece that is provided by the system developer <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. CASH differs as it does not require application programmer input at all except for the specification of the performance target. The cost of this generality is that the CASH runtime adapts models on the fly and can sometimes miss performance goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>We have presented CASH, an architecture and runtime codesigned to support IaaS customers. The CASH architecture makes it possible to rent computational resources at an extremely fine granularity. The CASH runtime allocates these resources to applications to meet QoS requirements and minimize cost. Our results show that CASH successfully addresses the challenges listed in the introduction: supporting fast reconfiguration, meeting QoS goals, adapting to application phases, and handling non-convex optimization spaces. Furthermore, our comparison with coarse-grain heterogeneous architectures shows that CASH's fine-grain configurable approach provides significant cost savingson average over 70%. We conclude it is both possible and profitable to (1) expose fine-grain architectural tradeoffs to software and (2) dynamically navigate these tradeoff spaces to save IaaS customers' money.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Phases of x264 executing on the CASH Architecture.</figDesc><graphic url="image-9.png" coords="3,86.70,319.22,141.42,106.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of fine-grain resource allocators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Slices, Cache Banks and Interconnection Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Slice Overview and Inter-Slice Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Register Flush When Shrinking a Virtual Core (VCore)</figDesc><graphic url="image-129.png" coords="5,368.09,135.23,58.27,68.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The CASH runtime.</figDesc><graphic url="image-146.png" coords="6,72.22,72.00,206.55,89.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Cost and QoS violations for different fine-grain resource allocators. (lower is better)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Time series data for apache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Cost and QoS for combinations of coarse and fine grain reconfigurable architectures and resource allocators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 L2 Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>CASH Run-me Interface Register Flush L2 Flush Performance Counter</head><label></label><figDesc></figDesc><table><row><cell>L1 Icache</cell><cell cols="2">Fetch Decode</cell><cell>Global Rename</cell><cell>Local Rename</cell><cell cols="2">Instruc&lt;on Window</cell><cell>ALU</cell><cell cols="2">LS Hash</cell><cell>LD/ ST</cell><cell>L1 Dcache</cell></row><row><cell cols="2">Br_pred &amp; btb</cell><cell></cell><cell cols="2">ROB Scoreboard</cell><cell></cell><cell>Scalar Operand Network</cell><cell cols="2">Local RF</cell><cell>LS Sor&lt;ng Network</cell></row><row><cell></cell><cell>Switched</cell><cell cols="2">Switched</cell><cell>Switched</cell><cell></cell><cell>Switched</cell><cell></cell><cell>Switched</cell><cell>Switched</cell></row><row><cell cols="3">Fetch&amp;BTB Sync</cell><cell cols="3">Global Rename</cell><cell>Operand</cell><cell cols="2">LS bank</cell><cell>L1/L2 Switched</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">&amp;Scoreboard Sync</cell><cell>Network</cell><cell cols="2">sor&lt;ng</cell><cell>Crossbar</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table I :</head><label>I</label><figDesc>Base Slice Configuration</figDesc><table><row><cell cols="2">Number of Functional Units/Slice 2 ROB size</cell><cell>64</cell></row><row><cell>Number of Physical Registers</cell><cell>128 Store Buffer Size</cell><cell>8</cell></row><row><cell cols="3">Number of Local Registers/Slice 64 Maximum In-flight Loads 8</cell></row><row><cell>Issue Window Size</cell><cell>32 Memory Delay</cell><cell>100</cell></row><row><cell>Load/Store Queue Size</cell><cell>32</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Algorithm 1 The CASH Runtime. Solve for t over &amp; t under using Eqn. 6, but substitute speedup estimates. Put the architecture in configuration over for t over time. Sleep for t over time. Put the architecture in configuration under for t under time.</figDesc><table><row><cell>Sleep for t under .</cell></row><row><cell>Compute ?over (t) and ?under (t), according to Eqn. 7.</cell></row><row><cell>end loop</cell></row></table><note><p><p>loop</p>Read current QoS q(t). Compute b(t), the estimate of the current base speed, using Eqn. 3. Compute s(t) according to Eqn. 2, but substitute b(t) for b.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table II :</head><label>II</label><figDesc>Base Cache Configurations: L2 cache hit delays depend on the distance to the cache bank</figDesc><table><row><cell cols="5">Level Size(KB) Block Size(Byte) Associativity Hit Delay</cell></row><row><cell>L1D</cell><cell>16</cell><cell>64</cell><cell>2</cell><cell>3</cell></row><row><cell>L1I</cell><cell>16</cell><cell>64</cell><cell>2</cell><cell>3</cell></row><row><cell>L2</cell><cell>64*2</cell><cell>64</cell><cell>4</cell><cell>distance*2+4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table III :</head><label>III</label><figDesc>Cost comparison for different resource allocators.</figDesc><table><row><cell></cell><cell>Geometric Mean</cell><cell>Ratio to Optimal</cell></row><row><cell>Optimal</cell><cell>$0.0162</cell><cell>1.00</cell></row><row><cell>Convex Optimization</cell><cell>$0.0199</cell><cell>1.23</cell></row><row><cell>Race to Idle</cell><cell>$0.0289</cell><cell>1.78</cell></row><row><cell>CASH</cell><cell>$0.0168</cell><cell>1.03</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was partially supported by the <rs type="funder">NSF</rs> under Grants No. <rs type="grantNumber">CCF-1217553</rs>, <rs type="grantNumber">CCF-1453112</rs>, and <rs type="grantNumber">CCF-1438980</rs>, <rs type="funder">AFOSR</rs> under Grant No. <rs type="grantNumber">FA9550-14-1-0148</rs>, and <rs type="funder">DARPA</rs> under Grants No. <rs type="grantNumber">N66001-14-1-4040</rs> and <rs type="grantNumber">HR0011-13-2-0005</rs>. <rs type="person">Henry Hoffmann</rs>'s effort on this project is funded by the <rs type="funder">U.S. Government</rs> under the <rs type="programName">DARPA BRASS program</rs>, by the <rs type="funder">Dept. of Energy</rs> under <rs type="grantNumber">DOE DE-AC02-06CH11357</rs>, by the <rs type="funder">NSF</rs> under <rs type="grantNumber">CCF 1439156</rs>, and by a <rs type="grantName">DOE Early Career Award</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uWcx28T">
					<idno type="grant-number">CCF-1217553</idno>
				</org>
				<org type="funding" xml:id="_zPMWhga">
					<idno type="grant-number">CCF-1453112</idno>
				</org>
				<org type="funding" xml:id="_M4fJgJY">
					<idno type="grant-number">CCF-1438980</idno>
				</org>
				<org type="funding" xml:id="_638T3Yt">
					<idno type="grant-number">FA9550-14-1-0148</idno>
				</org>
				<org type="funding" xml:id="_R2GhPHq">
					<idno type="grant-number">N66001-14-1-4040</idno>
				</org>
				<org type="funding" xml:id="_Eh224Hf">
					<idno type="grant-number">HR0011-13-2-0005</idno>
				</org>
				<org type="funding" xml:id="_pGx8YAT">
					<orgName type="program" subtype="full">DARPA BRASS program</orgName>
				</org>
				<org type="funding" xml:id="_sbHBZV6">
					<idno type="grant-number">DOE DE-AC02-06CH11357</idno>
				</org>
				<org type="funding" xml:id="_FmXzXxg">
					<idno type="grant-number">CCF 1439156</idno>
					<orgName type="grant-name">DOE Early Career Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Resource-as-a-service (RaaS) Cloud</title>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Agmon</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<editor>HotCloud&apos;12. Boston, MA</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="12" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Amazon EC2 Pricing</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Apache:HTTP server project</title>
		<ptr target="httpd.apache.org" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">TILE64 Processor: A 64-Core SoC with Mesh Interconnect</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<idno>ISSCC. 2008</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Benchmarking Modern Multiprocessors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The GEM5 Simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Coordinated Management of Multiple Interacting Resources in Chip Multiprocessors: A Machine Learning Approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bitirgen</surname></persName>
		</author>
		<idno>MICRO 41</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Applied mathematical programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bradley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Addison-Wesley Pub. Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scaling to the end of silicon with EDGE architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of the Kalman filter based estimation algorithm: an orthogonal decomposition approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="5" to="19" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling Program Resource Demand Using Inherent Program Characteristics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS &apos;11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Coker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MemScale: Active Low-power Modes for Main Memory</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS XVI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="225" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">CoScale: Coordinating CPU and Memory System DVFS in Server Systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
		<idno>MICRO-45. 2012</idno>
		<imprint>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Predictive Model for Dynamic Microarchitectural Adaptivity Control</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dubach</surname></persName>
		</author>
		<idno>MICRO &apos;43. 2010</idno>
		<imprint>
			<biblScope unit="page" from="485" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simultaneous multithreading: a platform for nextgeneration processors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="12" to="19" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated design of self-adaptive software with control-theoretical formal guarantees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Filieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICSE</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SWiFT: A Feedback Control and Dynamic Reconfiguration Toolkit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nd USENIX Windows NT Symposium</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Application-driven Energy-efficient Architecture Explorations for Big Data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Architectures and Systems for Big Data. ASBD &apos;11</title>
		<meeting>the 1st Workshop on Architectures and Systems for Big Data. ASBD &apos;11<address><addrLine>Galveston Island, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="34" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A Framework for Providing Quality of Service in Chip Multi-Processors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<idno>MICRO 40</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="343" to="355" />
			<pubPlace>Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">JouleGuard: energy guarantees for approximate applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SOSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Remote Store Programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HiPEAC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A Generalized Software Framework for Accurate and Efficient Managment of Performance Goals</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>EMSOFT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">POET: A portable approach to minimizing energy under soft real-time constraints</title>
		<author>
			<persName><forename type="first">C</forename><surname>Imes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time and Embedded Technology and Applications Symposium (RTAS)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-Optimizing Memory Controllers: A Reinforcement Learning Approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<idno>ISCA &apos;08</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Core Fusion: accommodating software diversity in chip multiprocessors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>San</publisher>
			<biblScope unit="page" from="186" to="197" />
			<pubPlace>Diego, California, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">QoS policies and architecture for cache/memory in CMP platforms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS &apos;07</title>
		<meeting><address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Big.LITTLE system architecture from ARM: saving power through heterogeneous multiprocessing and task context migration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jeff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>DAC</publisher>
			<biblScope unit="page" from="1143" to="1146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Racing and Pacing to Idle: Theoretical and Empirical Analysis of Energy Optimization Heuristics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CPSNA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Meet the Walkers: Accelerating Index Traversals for In-memory Databases</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<idno>MICRO-46</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="468" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Processor Power Reduction Via Single-ISA Heterogeneous Multi-Core Architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate and Efficient Regression Modeling for Microarchitectural Performance and Power Prediction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="185" to="194" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The control handbook</title>
		<author>
			<persName><forename type="first">W</forename><surname>Levine</surname></persName>
		</author>
		<editor>W. Levine</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A control-based middleware framework for quality-of-service adaptations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nahrstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="1999-09">Sept. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards Energy Proportionality for Large-scale Latency-critical Workloads</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;14</title>
		<meeting><address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Scale-out processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<idno>ISCA &apos;12. 2012</idno>
		<imprint>
			<biblScope unit="page" from="500" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Power Optimization in Embedded Systems via Feedback Control of Resource Allocation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Contr. Sys. Techn</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="239" to="246" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PowerNap: Eliminating Server Idle Power</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS XIV</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A Probabilistic Graphical Model-based Approach for Minimizing Energy Under Performance Constraints</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ASP-LOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A Design Space Evaluation of Grid Processor Architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
			<publisher>MICRO</publisher>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Flicker: A Dynamically Adaptive Architecture for Power Limited Multicore Systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Petrica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;13</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<idno>ISCA 41</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism to Partition Shared Caches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<idno>MICRO 39</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploiting Processor Heterogeneity in Interactive Services</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAC 13. USENIX</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="45" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploiting ILP, TLP, and DLP with the polymorphous TRIPS architecture</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<biblScope unit="page" from="422" to="433" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Distributed Microarchitectural Protocols in the TRIPS Prototype Processor</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<idno>MICRO-39</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="480" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">METE: Meeting End-to-end QoS in Multicores Through System-wide Resource Management</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharifi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS &apos;11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Koala: A Platform for OS-level Power Management</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Snowdon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>EuroSys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">CINT2006 (Integer Component of SPEC CPU</title>
		<author>
			<persName><surname>Spec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Feedback-driven Proportion Allocator for Real-rate Scheduling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Steere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="145" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A new memory monitoring scheme for memory-aware scheduling and partitioning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HPCA</title>
		<imprint>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Introduction to Reinforcement Learning</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Cambridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">WaveScalar</title>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
		<idno>MICRO 36</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">291</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scalar operand networks: on-chip interconnect for ILP in partitioned architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HPCA</title>
		<imprint>
			<biblScope unit="page" from="341" to="353" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wikipedia workload analysis for decentralized hosting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Urdaneta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1830" to="1845" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">WiDGET: Wisconsin decoupled grid execution tiles</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">An Introduction to the Kalman Filter</title>
		<author>
			<persName><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<publisher>UNC Chapel Hill</publisher>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep. TR 95-041</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On-Chip Interconnection Architecture of the Tile Processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="15" to="31" />
			<date type="published" when="2007-09">Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Configurable fine-grain protection for multicore processor virtualization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="464" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bubble-flux: Precise Online QoS Management for Increased Utilization in Warehouse Scale Computers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;13</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="607" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A Statistically Rigorous Approach for Improving Simulation Methodology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA &apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">281</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">ControlWare: A Middleware Architecture for Feedback Control of Software Performance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The Sharing Architecture: Sub-core Configurability for IaaS Clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;14</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="559" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">MITTS: Memory Inter-Arrival Time Traffic Shaping</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
