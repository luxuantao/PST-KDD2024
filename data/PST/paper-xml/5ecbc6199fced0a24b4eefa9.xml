<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEVERAGING UNPAIRED TEXT DATA FOR TRAINING END-TO-END SPEECH-TO-INTENT SYSTEMS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinghui</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hong-Kwang</forename><surname>Kuo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zvi</forename><surname>Kons</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ron</forename><surname>Hoory</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Picheny</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEVERAGING UNPAIRED TEXT DATA FOR TRAINING END-TO-END SPEECH-TO-INTENT SYSTEMS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech-to-intent</term>
					<term>spoken language understanding</term>
					<term>end-to-end systems</term>
					<term>pre-trained text embedding</term>
					<term>synthetic speech augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training an end-to-end (E2E) neural network speech-to-intent (S2I) system that directly extracts intents from speech requires large amounts of intent-labeled speech data, which is time consuming and expensive to collect. Initializing the S2I model with an ASR model trained on copious speech data can alleviate data sparsity. In this paper, we attempt to leverage NLU text resources. We implemented a CTC-based S2I system that matches the performance of a state-ofthe-art, traditional cascaded SLU system. We performed controlled experiments with varying amounts of speech and text training data. When only a tenth of the original data is available, intent classification accuracy degrades by 7.6% absolute. Assuming we have additional text-to-intent data (without speech) available, we investigated two techniques to improve the S2I system: (1) transfer learning, in which acoustic embeddings for intent classification are tied to fine-tuned BERT text embeddings; and (2) data augmentation, in which the textto-intent data is converted into speech-to-intent data using a multispeaker text-to-speech system. The proposed approaches recover 80% of performance lost due to using limited intent-labeled speech.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Spoken language understanding (SLU) systems have traditionally been a cascade of an automatic speech recognition (ASR) system converting speech into text followed by a natural language understanding (NLU) system that interprets the meaning, or intent, of the text. In contrast, an end-to-end (E2E) SLU system <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> processes speech input directly into intent without going through an intermediate text transcript. There are many advantages of end-to-end SLU systems <ref type="bibr" target="#b4">[5]</ref>, the most significant of which is that E2E systems can directly optimize the end goal of intent recognition, without having to perform intermediate tasks like ASR.</p><p>Compared to end-to-end SLU systems, cascaded systems are modular and each component can be optimized separately or jointly (also with end-to-end criteria <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>). One key advantage of modular components is that each component can be trained on data that may be more abundant. For example, there is a lot of transcribed speech data that can be used to train an ASR model. In comparison, there is a paucity of speech data with intent labels, and intent labels, unlike words, are not standardized and may be inconsistent from task to task. Another advantage of modularity is that components can be * The author performed the work while at IBM re-used and adapted for other purposes, e.g. an ASR service used as a component for call center analytics, closed captioning, spoken foreign language translation, etc.</p><p>While end-to-end SLU is an active area of research, currently the most promising results under-perform or just barely outperform traditional cascaded systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. One reason is that deep learning models require a large amount of appropriate training data. To train an end-to-end speech-to-intent model, we need intent-labeled speech data, and such data is usually scarce. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> address this problem using a curriculum and transfer learning approach whereby the model is gradually trained on increasingly relevant data until it is fine-tuned on the actual domain data. Similarly, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref> advocate pre-training an ASR model on a large amount of transcribed speech data to initialize a speech-to-intent model that is then trained on a much smaller training set with both transcripts and intent labels.</p><p>Training data for end-to-end SLU is much scarcer than training data for ASR (speech and transcripts) or NLU (text and semantic annotations). In fact, there are many relevant NLU text resources and models (e.g. named entity extraction) and information in the world is mostly organized in text format, without corresponding speech. As SLU becomes more sophisticated, it is important to be able to leverage such text resources in end-to-end SLU models. Pre-training on ASR resources is straightforward, but it is less clear how to use NLU resources. This problem has not been adequately addressed in the literature that we are aware of.</p><p>In this paper, we pose an interesting question: for an end-to-end S2I model, how can we take advantage of text-to-intent training data without speech? There are many possible approaches, but we focus on two methods in this paper. In the first, we jointly train the speechto-intent model and a text-to-intent model, encouraging the acoustic embedding from the speech model to be close to a fine-tuned BERTbased text embedding, and using a shared intent classification layer. The second method involves data augmentation, where we convert the additional text-to-intent data into synthetic speech-to-intent data using a multi-speaker text-to-speech (TTS) system.</p><p>To evaluate our methods, we performed carefully controlled experiments. First we built strong baselines with conventional cascaded systems, where the acoustic, language, and intent classification models are adapted on in-domain data. We also built a strong speechto-intent model using pre-trained acoustic models for initialization and multi-task training to optimize ASR and intent classification objectives, producing competitive results compared with the cascaded system. We evaluated these models on varying amounts of speech and text training data. Through these experiments, we seek to answer two questions. First, can we improve S2I models with additional text-to-intent data? Second, how does that compare to having actual speech-to-intent data? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TRAINING SPEECH-TO-INTENT SYSTEMS</head><p>End-to-end speech-to-intent systems directly extract the intent label associated with a spoken utterance without explicitly transcribing the utterance. However, it is still useful to derive an intermediate ASR embedding that summarizes the message component of the signal for intent classification. An effective approach to achieve this goal is to train the S2I classifier starting from a pre-trained ASR system. ASR pre-training is also beneficial since intent labels are not required in this step; hence, we can use ASR speech data instead of specific in-domain intent data, which is usually limited.</p><p>In our work, we use a phone-based connectionist temporal classification (CTC) <ref type="bibr" target="#b12">[13]</ref> acoustic model (AM) trained on general speech data as the base ASR system. First, we initialize the S2I model with this model and adapted it to the in-domain data. Once the adapted ASR system is trained, it is modified for intent classification using speech that was transcribed and also annotated with intent labels. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, to construct the intent recognition system we modify the model by adding a classification layer that predicts intent targets. Unlike phone targets which are predicted by the ASR system at the frame level, intent targets span larger contexts. In our case, we assume that each training utterance corresponds to a single intent, although it might be composed of several words. To better capture intents at the utterance level, we derive an acoustic embedding (AE) corresponding to each training utterance. This embedding is computed by time averaging all the hidden states of the final LSTM layer to summarize the utterance into one compact vector that is used to predict the final intent. The final fully connected layer introduced in this step to process the acoustic embeddings can be viewed as an intent classifier. While training the network to predict intent, given that transcripts for the utterances are also available, we continue to refine the network to predict ASR targets as well. With this multi-task objective, the network adapts its layers to the channel and speakers of the in-domain data. During test time, only the outputs of the intent classification layer are used, while the ASR branch is discarded. To improve robustness of the underlying speech model, we also employ data augmentation techniques for end-to-end acoustic model training, namely speed and tempo perturbation, in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMPROVEMENTS USING TEXT-TO-INTENT DATA</head><p>In practice, we expect that end-to-end S2I classifiers will be trained in conditions where there is a limited amount of transcribed S2I data and significantly more text-to-intent (T2I) data. To investigate this setting, we develop two approaches to use additional text data for building S2I systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Leveraging pre-trained text embedding</head><p>Leveraging text embedding (TE) from models pre-trained on large amounts of data, such as BERT <ref type="bibr" target="#b13">[14]</ref> and GPT-2 <ref type="bibr" target="#b14">[15]</ref>, has recently improved performance in a number of NLU tasks. In this paper, we use BERT-based text embeddings to transfer knowledge from text data into a speech-to-intent system. The text embeddings are used to "guide" acoustic embeddings which are trained with a limited amount of S2I data, in the same spirit as learning a shared representation between modalities <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. We employ the following steps to train the final model. (A) T2I model pre-training: As in the standard process outlined in the original BERT paper <ref type="bibr" target="#b13">[14]</ref>, we first fine-tune BERT on the available text-to-intent data using a masked language model (LM) task as the intermediate task. The model is further fine-tuned with intent labels as the target classification task before the representation of the special token [CLS] is used as the text embedding of an utterance. (B) ASR pre-training for S2I model: As described in Section 2, the base ASR model is trained on non-parallel ASR data and subsequently adapted using an augmented S2I data set. This step adapts the model to the acoustic conditions of the intent data to extract better acoustic embeddings. Next, multi-task training further fine-tunes the acoustic model to generate embeddings that are better for intent classification. We now have an initial S2I system that has seen only limited novel intent content. Note that we add a fully connected layer before the classifier to make sure the dimensions of the acoustic embedding and text embedding match for joint-training in the next step.</p><p>(C) Full S2I model training: The final S2I classifier is then assembled by combining the fine-tuned T2I classifier (step A) and the pre-trained S2I system (step B). We jointly train the fine-tuned BERT model with the pre-trained ASR acoustic model in an attempt to leverage the knowledge extracted from larger amounts of text data to improve the quality of the acoustic embedding for intent classification. The training framework is shown in Figure <ref type="figure" target="#fig_1">2</ref>. We extract text embeddings from the fine tuned BERT model with the reference text as input. Acoustic embeddings are also extracted in parallel from a corresponding acoustic signal. These two embeddings are used to train a shared intent classification layer which has been initialized from the text-only classification task described above. Given that the text embedding comes from a well trained extractor, the acoustic embeddings are explicitly forced to match the better text embeddings. We hypothesize that this matching will also allow the shared classifier layer to train better. During test time, we only use the acoustic branch for intent inference.</p><p>To achieve these goals, a training procedure that optimizes two separate loss terms is employed. The first loss term corresponds to a composite cross-entropy intent classification loss derived by using the text embeddings, LCE(T E), and the acoustic embeddings, LCE(AE), separately to predict intent labels using the shared classifier layer. In the combined classification loss, the text-embedding classification loss is scaled by a weight parameter α. The second loss is the mean squared error (MSE) loss between the text embedding and acoustic embedding LMSE(AE, T E). It is important to note that while the gradients from the combined classification loss are propagated back to both the text and acoustic embedding networks, the MSE loss is only back-propagated to the acoustic side because we presume that the acoustic embeddings should correspond closely to the BERT embeddings, which have been trained on massive quantities of text and perform better on intent classification. On the speech branch the minimized loss is LMSE(AE, T E) + LCE(AE) + αLCE(T E), while the loss on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Using text data for speech data augmentation</head><p>Instead of using available text data for pre-training the T2I system, we also try converting the text data to speech using a multi-speaker TTS system. Like in Section 2, the S2I classifier in this case is trained in two steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Experiments were performed on a corpus consisting of call center recordings of open-ended first utterances by customers describing the reasons for their calls, which is described in <ref type="bibr" target="#b7">[8]</ref>. The 8kHz telephony speech data was manually transcribed and labeled with correct intents. The corpus contains real customer spontaneous utterances, not crowdsourced data of people reading from a script, and includes a variety of ways customers naturally described their intent. For example, the intent "BILLING" includes short sentences such as "billing" and longer ones such as "i need to ask some questions on uh to get credit on an invoice adjustment."</p><p>The training data consists of 19.5 hours of speech that was first divided into a training set of 17.5 hours and a held-out set of 2 hours. The held-out set was used during training to track the objective function and tune certain parameters like the initial learning rate. In addition to the 17.5-hour training set (which we call 20hTrainset, containing 21849 sentences, 145K words), we also extracted a 10% subset (1.7h) for low-resource experiments (which we call 2hTrainset, containing 2184 sentences, 14K words). We augmented the training data via speed and tempo perturbation (0.9x and 1.1x), so 2hTrainset finally contains about 8.7 hours and 20hTrainset about 88 hours of speech. The devset consists of 3182 sentences (2.8 hours) and was used for hyperparameter tuning, e.g., tuning the acoustic weight to optimize the word error rate (WER). A separate data set containing 5592 sentences (5h, 40K words) was used as the final testset.</p><p>In the training set, each sentence had a single intent, and there were 29 intent classes. The testset contains additional unseen intent classes and multiple intents per sentence, as naturally happens in real life. For simplicity, in this paper we do not address these cases and always count such sentences as errors when calculating intent accuracy; they account for about 1% of the utterances. The testset has an average of 7 words per utterance, with the longest sentence being over 100 words long. 70% of the sentences are unique (not repetitions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pre-trained models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">ASR CTC model</head><p>We pre-train an ASR system on the 300-hour Switchboard English conversational speech corpus. The AM is a 6-layer bidirectional LSTM network with every layer containing 640 LSTM units per direction. The AM is trained using CTC loss over 44 phones and the blank symbol. Our AM training recipe follows our prior work. We first perform speed and tempo perturbation (0.9x and 1.1x) resulting in a 1500-hour audio data set. We train the AM for 20 epochs using CTC loss, followed by 20 epochs of soft forgetting training <ref type="bibr" target="#b19">[20]</ref>, followed by 20 epochs of guided training <ref type="bibr" target="#b20">[21]</ref>. We use sequence noise injection <ref type="bibr" target="#b21">[22]</ref> and SpecAugment <ref type="bibr" target="#b22">[23]</ref> throughout the training to provide on-the-fly data augmentation, and we also use a dropout probability of 0.5 on the LSTM output at each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">BERT based T2I model</head><p>We start with pre-trained BERTbase model of <ref type="bibr" target="#b13">[14]</ref>. Using the implementation introduced in <ref type="bibr" target="#b23">[24]</ref>, we first pre-train using a masked LM target with learning rate 3e-5 for 10 epochs, followed by 3 epochs of fine-tuning on the intent classification task with learning rate 2e-5. This text-to-intent BERT based classifier trained on 20hTrainset gives 92.0% accuracy on human-generated reference transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">TTS system</head><p>The TTS system architecture is similar to the single speaker system described in <ref type="bibr" target="#b24">[25]</ref>. It is a modular system based on three neural-net models: one to infer prosody, one to infer acoustic features, and an LPCNet <ref type="bibr" target="#b25">[26]</ref> vocoder. The main difference between the single and multi-speaker systems is that both the prosody and the acoustic networks are converted to multi-speaker models by conditioning them on a speaker embedding vector. Each of the three models was independently trained on 124 hours of 16KHz speech from 163 English speakers. The speaker set is composed of 4 high quality proprietary voices with more than 10 hours of speech, 21 VCTK <ref type="bibr" target="#b26">[27]</ref> voices, and 138 LibriTTS <ref type="bibr" target="#b27">[28]</ref> voices. During synthesis, for each sentence we select a random speaker out of the known speakers set. We then synthesize the sentence with this voice. Finally, the samples are downsampled to 8KHz to match the S2I audio sampling rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>We first establish the strongest possible baseline results for the conventional cascaded system where the ASR and T2I models are trained separately. Using the ASR system described above and a BERT T2I  <ref type="table">1</ref>. The same T2I model has 92.0% intent accuracy on human reference transcripts; thus, there is a significant degradation in accuracy with speech input due to ASR errors. Both the AM and LM of this system are then adapted using the domain data 20hTrainset.</p><p>Table <ref type="table">1</ref> shows that such adaptation dramatically improved both WER and intent accuracy, which increased from 73.4% to 89.7%. There is now only about a 2% accuracy gap between using human transcripts (92.0%) and using ASR outputs (89.7%). In the low-resource scenario, adapting the AM and LM on 2hTrainset and also training T2I on only 2hTrainset results in intent accuracy of 82.8%. As shown in Table <ref type="table" target="#tab_0">2</ref>, when paired speech-to-intent data is used our proposed E2E S2I approach gives comparable accuracy as the cascaded approach, both with full training data (20hTrainset) or in the low-resource setting (2hTrainset).</p><p>In the low-resource scenario where only a limited amount of speech is available (2hTrainset), frequently one may have extra text data with intent labels but no corresponding speech data. For the cascaded system, it is straightforward to train different components (AM, LM, T2I) with whatever appropriate data is available. Table <ref type="table" target="#tab_2">3</ref> shows how the intent accuracy varies when the full 20hTrainset (but not the speech) is available as text-to-intent data. By training the LM and T2I on this data, the intent accuracy increased to 89.6%, basically matching the best accuracy of 89.7%, where the AM is also adapted on 20hTrainset. The third row in Table <ref type="table" target="#tab_2">3</ref> shows that if only the T2I model is trained on 20hTrainset, the accuracy is still quite high: 88.9%. Comparing results from the first and third rows, we observe that the accuracy difference between training the T2I model on 2hTrainset versus 20hTrainset is about 6%, accounting for most of the gap between the full resource and limited resource performance. In other words, the T2I model is the weakest link, the component that is most starved for additional data. For the AM and LM, because they were pre-trained on plenty of general data, even just 2h of adaptation is almost enough, but the intent classifier needs much more data. This makes sense because the intent classification task can be quite domain specific.</p><p>For the end-to-end speech-to-intent system, leveraging the text-tointent data is not straightforward. If it were unable to take advantage of such data, it would be at a significant 6-7% accuracy disadvantage compared to the cascaded system in this scenario.</p><p>In the next set of experiments, in In our second approach, we took the extra text-to-intent data (20hTrainset) and generated synthetic speech with multi-speaker TTS to create new speech-to-intent data. Adding this synthetic data to 2hTrainset to train the E2E model resulted in an intent accuracy of 87.8%, a significant improvement from 82.2%. If we generated the synthetic data using single speaker TTS, the accuracy was roughly the same: 87.3%. These results were quite surprising. We hypothesize that this improvement is due to the S2I model learning new semantic information ("embedding"-to-intent) from the new synthetic data rather than adapting to the acoustics. Therefore it was not necessary to generate a lot of variability in the speech (e.g. speakers, etc.) with the TTS data. Running ASR on the TTS speech, the WER was very low, around 4%, so there was little mismatch between the TTS speech and the underlying ASR model. One can imagine that the speech encoder portion of the model removes speaker variability, etc. to produce an embedding that depends largely on just the word sequence; hence, any representative TTS speech would be sufficient because the weakest link was the intent classifier. Finally, if we combine the two methods, joint training text and speech embeddings with synthetic TTS speech data, we obtain modest gains, resulting in an intent classification accuracy of 88.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>End-to-end spoken language understanding systems require paired speech and semantic annotation data, which is typically quite scarce compared to NLU resources (semantically annotated text without speech). We have made progress on this important but neglected issue by showing that an end-to-end speech-to-intent model can learn from annotated text data without paired speech. By leveraging pre-trained text embeddings and data augmentation using speech synthesis, we are able to improve the intent classification error rate by over 60% and achieve over 80% of the improvement from paired speech-to-intent data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A S2I system with pre-trained ASR</figDesc><graphic url="image-1.png" coords="2,91.28,72.00,170.08,125.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Joint-training of the S2I system with text embeddings the text branch is LCE(AE) + αLCE(T E).</figDesc><graphic url="image-2.png" coords="3,70.02,72.00,212.60,183.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(A) ASR pre-training: The base ASR model (trained on non-parallel ASR data) is first adapted using the speech portion of the S2I data set. (B) Training with synthesized data: The TTS-synthesized data is used along with the limited amount of S2I data, without augmentation, for training. Section 4.2.3 describes the generation of the TTS data set in detail. Compared with tying text and acoustic embeddings, S2I data augmentation might be more effective at training the "embedding" to intent classifier layers of the neural network, because novel (utterance, intent) pairs are used in the final training phase rather than just in pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>End-to-end speech-to-intent classification accuracy. model trained on the domain data 20hTrainset, we obtained 21.9% WER and 73.4% intent classification accuracy (IntAcc) for testset, as shown in Table</figDesc><table><row><cell>AM</cell><cell>LM</cell><cell>T2I</cell><cell>WER IntAcc</cell></row><row><cell>unadapted</cell><cell>unadapted</cell><cell cols="2">20hTrainset 21.9% 73.4%</cell></row><row><cell cols="4">20hTrainset 20hTrainset 20hTrainset 10.5% 89.7%</cell></row><row><cell>2hTrainset</cell><cell>2hTrainset</cell><cell>2hTrainset</cell><cell>11.6% 82.8%</cell></row><row><cell cols="4">Table 1. WER and IntAcc (intent accuracy) of baseline cascaded</cell></row><row><cell cols="4">systems (ASR followed by T2I) for speech-to-intent recognition</cell></row><row><cell></cell><cell></cell><cell cols="2">20hTrainset 2hTrainset</cell></row><row><cell cols="2">Cascaded(ASR+T2I)</cell><cell>89.7%</cell><cell>82.8%</cell></row><row><cell>E2E CTC</cell><cell></cell><cell>89.8%</cell><cell>82.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 ,</head><label>4</label><figDesc>we show results from leveraging extra text data to reduce the gap caused by having less S2I training data. Our first approach described in Section 3.1 and illustrated in Figure2ties the speech and text embeddings and trains</figDesc><table><row><cell>AM</cell><cell>LM</cell><cell>T2I</cell><cell>WER IntAcc</cell></row><row><cell cols="2">2hTrainset 2hTrainset</cell><cell>2hTrainset</cell><cell>11.6% 82.8%</cell></row><row><cell cols="4">2hTrainset 20hTrainset 20hTrainset 10.3% 89.6%</cell></row><row><cell cols="2">2hTrainset 2hTrainset</cell><cell cols="2">20hTrainset 11.6% 88.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Limited speech resources with extra text-to-intent data.</figDesc><table><row><cell>Method</cell><cell>IntAcc</cell></row><row><cell>E2E S2I system trained on 2hTrainset</cell><cell>82.2%</cell></row><row><cell>Joint training tying speech/text embeddings</cell><cell>84.7%</cell></row><row><cell>Adding synthetic multi-speaker TTS speech</cell><cell>87.8%</cell></row><row><cell>Joint training + adding synthetic speech</cell><cell>88.3%</cell></row><row><cell>E2E S2I system trained on 20hTrainset</cell><cell>89.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>End-to-End models using extra text-to-intent data to recover accuracy lost by switching from 20hTrainset to 2hTrainset.the intent classifier on speech and text embeddings. By joint training end-to-end S2I CTC model with BERT fine tuned on full text-to-intent data, we observe accuracy improvement from 82.2% to 84.7%.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We thank Ellen Kislal for her initial studies on E2E S2I.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards end-to-end spoken language understanding</title>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="5754" to="5758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring ASR-free end-to-end modeling to improve spoken language understanding in a cloud-based dialog system</title>
		<author>
			<persName><forename type="first">Rutuja</forename><surname>Yao Qian</surname></persName>
		</author>
		<author>
			<persName><surname>Ubale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Vikram Ramanaryanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keelan</forename><surname>Suendermann-Oeft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName><surname>Tsuprun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spoken language understanding without speech recognition</title>
		<author>
			<persName><forename type="first">Yuan-Ping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="6189" to="6193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end named entity and semantic concept extraction from speech</title>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Caubrière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Camelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Simonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="692" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech model pre-training for endto-end spoken language understanding</title>
		<author>
			<persName><forename type="first">Loren</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikrant</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019. ISCA</title>
				<meeting>Interspeech 2019. ISCA</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="814" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From audio to semantics: Approaches to end-to-end spoken language understanding</title>
		<author>
			<persName><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="720" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curriculum-based transfer learning for an effective end-to-end spoken language understanding and domain portability</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Caubrière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Camelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019. ISCA</title>
				<meeting>Interspeech 2019. ISCA</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1198" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language model estimation for optimizing end-toend performance of a natural language call routing system</title>
		<author>
			<persName><forename type="first">Vaibhava</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong-Kwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Deligne</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP 2005. IEEE</title>
				<meeting>ICASSP 2005. IEEE</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="565" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An integrative and discriminative technique for spoken utterance classification</title>
		<author>
			<persName><forename type="first">Sibel</forename><surname>Yaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1207" to="1214" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From speech signals to semantics-tagging performance at acoustic, phonetic and word levels</title>
		<author>
			<persName><forename type="first">Rutuja</forename><surname>Yao Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ubale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keelan</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 11th International Symposium on Chinese Spoken Language Processing (ISCSLP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="280" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigating adaptation and transfer learning for end-to-end spoken language understanding from speech</title>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Caubrière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019. ISCA</title>
				<meeting>Interspeech 2019. ISCA</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="824" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end spoken language understanding: Bootstrapping in low resource scenarios</title>
		<author>
			<persName><forename type="first">Swapnil</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sri</forename><surname>Harsha Dumpala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kopparapu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019. ISCA</title>
				<meeting>Interspeech 2019. ISCA</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1188" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11</title>
				<meeting>the 28th international conference on machine learning (ICML-11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On deep multi-view representation learning</title>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jointly discovering visual objects and spoken words from raw sensory input</title>
		<author>
			<persName><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dídac</forename><surname>Surís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Forget a Bit to Learn Better: Soft Forgetting for CTC-Based Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltán</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2618" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and Knowledge Distillation</title>
		<author>
			<persName><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08311</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence Noise Injected Training for End-to-end Speech Recognition</title>
		<author>
			<persName><forename type="first">George</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltán</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="6261" to="6265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Daniel S Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High Quality, Lightweight and Adaptable TTS Using LPCNet</title>
		<author>
			<persName><forename type="first">Zvi</forename><surname>Kons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slava</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Sorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmel</forename><surname>Rabinovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Hoory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="176" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LPCNET: Improving Neural Speech Synthesis through Linear Prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP 2019</title>
				<meeting>ICASSP 2019</meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="5891" to="5895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirsten</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. The Centre for Speech Technology Research (CSTR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech</title>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viet</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1904">1904.02882, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
