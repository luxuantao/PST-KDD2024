<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Alternative Infinite Mixture Of Gaussian Process Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Edward</forename><surname>Meeds</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<postCode>M5S 3G4</postCode>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
							<email>osindero@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<postCode>M5S 3G4</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Alternative Infinite Mixture Of Gaussian Process Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">30332A3DCED058D42A5E7DD4C6B0877B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an infinite mixture model in which each component comprises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multimodality and overlapping output signals. The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. This allows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian specification of the effective 'gating network' for the different experts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gaussian process (GP) models are powerful tools for regression, function approximation, and predictive density estimation. However, despite their power and flexibility, they suffer from several limitations. The computational requirements scale cubically with the number of data points, thereby necessitating a range of approximations for large datasets. Another problem is that it can be difficult to specify priors and perform learning in GP models if we require non-stationary covariance functions, multi-modal output, or discontinuities.</p><p>There have been several attempts to circumvent some of these lacunae, for example <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. In particular the Infinite Mixture of Gaussian Process Experts (IMoGPE) model proposed by Rasmussen and Ghahramani <ref type="bibr" target="#b0">[1]</ref> neatly addresses the aforementioned key issues. In a single GP model, an n by n matrix must be inverted during inference. However, if we use a model composed of multiple GP's, each responsible only for a subset of the data, then the computational complexity of inverting an n by n matrix is replaced by several inversions of smaller matrices -for large datasets this can result in a substantial speed-up and may allow one to consider large-scale problems that would otherwise be unwieldy. Furthermore, by combining multiple stationary GP experts, we can easily accommodate non-stationary covariance and noise levels, as well as distinctly multi-modal outputs. Finally, by placing a Dirichlet process prior over the experts we can allow the data and our prior beliefs (which may be rather vague) to automatically determine the number of components to use.</p><p>In this work we present an alternative infinite model that is strongly inspired by the work in <ref type="bibr" target="#b0">[1]</ref>, but which uses a different formulation for the mixture of experts that is in the style presented in, for example <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. This alternative approach effectively uses posterior re-PSfrag replacements</p><formula xml:id="formula_0">y i x i z i N PSfrag replacements y i x i z i N</formula><p>Figure <ref type="figure">1</ref>: Left: Graphical model for the standard MoE model <ref type="bibr" target="#b5">[6]</ref>. The expert indicators {z (i) } are specified by a gating network applied to the inputs {x (i) }. Right: An alternative view of MoE model using a full generative model <ref type="bibr" target="#b3">[4]</ref>. The distribution of input locations is now given by a mixture model, with components for each expert. Conditioned on the input locations, the posterior responsibilities for each mixture component behave like a gating network.</p><p>sponsibilities from a mixture distribution as the gating network. Even if the task at hand is simply output density estimation or regression, we suggest a full generative model over inputs and outputs might be preferable to a purely conditional model. The generative approach retains all the strengths of <ref type="bibr" target="#b0">[1]</ref> and also has a number of potential advantages, such as being able to deal with partially specified data (e.g. missing input co-ordinates) and being able to infer inverse functional mappings (i.e. the input space given an output value).</p><p>The generative approach also affords us a richer and more consistent way of specifying our prior beliefs about how the covariance structure of the outputs might vary as we move within input space.</p><p>An example of the type of generative model which we propose is shown in figure <ref type="figure">2</ref>. We use a Dirichlet process prior over a countably infinite number of experts and each expert comprises two parts: a density over input space describing the distribution of input points associated with that expert, and a Gaussian Process model over the outputs associated with that expert. In this preliminary exposition, we restrict our attention to experts whose input space densities are given a single full covariance Gaussian. Even this simple approach demonstrates interesting performance and capabilities. However, in a more elaborate setup the input density associated with each expert might itself be an infinite mixture of simpler distributions (for instance, an infinite mixture of Gaussians <ref type="bibr" target="#b4">[5]</ref>) to allow for the most flexible partitioning of input space amongst the experts.</p><p>The structure of the paper is as follows. We begin in section 2 with a brief overview of two ways of thinking about Mixtures of Experts. Then, in section 3, we give the complete specification and graphical depiction of our generative model, and in section 4 we outline the steps required to perform Monte Carlo inference and prediction. In section 5 we present the results of several simple simulations that highlight some of the salient features of our proposal, and finally in section 6, we discuss our work and place it in relation to similar techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mixtures of Experts</head><p>In the standard mixture of experts (MoE) model <ref type="bibr" target="#b5">[6]</ref>, a gating network probabilistically mixes regression components. One subtlety in using GP's in a mixture of experts model is that IID assumptions on the data no longer hold and we must specify joint distributions for each possible assignment of experts to data. Let {x (i) } be the set of d-dimensional input vectors, {y (i) } be the set of scalar outputs, and {z (i) } be the set of expert indicators which assign data points to experts.</p><p>The likelihood of the outputs, given the inputs, is specified in equation 1, where θ GP r represents the GP parameters of the rth expert, θ g represents the parameters of the gating network, and the summation is over all possible configurations of indicator variables.</p><formula xml:id="formula_1">Σ x ν S f S a νc b νc ν 0 f 0 µ x S ν c Σ 0 Σ r µ r a α0 b α0 α 0 z r i x r (i) Y r {z (i) } v 0r v 1r w jr j = 1 : D r = 1 : K a 0 b 0 a 1 b 1 a w b w µ 0 i = 1 : Nr Figure 2:</formula><p>The graphical model representation of the alternative infinite mixture of GP experts (AiMoGPE) model proposed in this paper. We have used x r (i) to represent the ith data point in the set of input data whose expert label is r, and Y r to represent the set of all output data whose expert label is r. In other words, input data are IID given their expert label, whereas the sets of output data are IID given their corresponding sets of input data. The lightly shaded boxes with rounded corners represent hyper-hyper parameters that are fixed (Ω in the text). The DP concentration parameter α 0 , the expert indicators variables, {z (i) }, the gate hyperparameters, φ x = {µ 0 , Σ 0 , ν c , S}, the gate component parameters, ψ x r = {µ r , Σ r }, and the GP expert parameters, θ GP r = {v 0r , v 1r , w jr }, are all updated for all r and j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P ({y</head><formula xml:id="formula_2">(i) }|{x (i) }, θ) = Z P ({z (i) }|{x (i) }, θ g ) r P ({y (i) : z (i) = r}|{x (i) : z (i) = r}, θ GP r )</formula><p>(1) There is an alternative view of the MoE model in which the experts also generate the inputs, rather than simply being conditioned on them <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> (see figure <ref type="figure">1</ref>). This alternative view employs a joint mixture model over input and output space, even though the objective is still primarily that of estimating conditional densities i.e. outputs given inputs. The gating network effectively gets specified by the posterior responsibilities of each of the different components in the mixture. An advantage of this perspective is that it can easily accommodate partially observed inputs and it also allows 'reverse-conditioning', should we wish to estimate where in input space a given output value is likely to have originated. For a mixture model using Gaussian Processes experts, the likelihood is given by</p><formula xml:id="formula_3">P ({x (i) },{y (i) }|θ) = Z P ({z (i) }|θ g )× r P ({y (i) : z (i) = r}|{x (i) : z (i) = r}, θ GP r )P ({x (i) : z (i) = r}|θ g ) (2)</formula><p>where the description of the density over input space is encapsulated in θ g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Infinite Mixture of Gaussian Processes: A Joint Generative Model</head><p>The graphical structure for our full generative model is shown in figure <ref type="figure">2</ref>. Our generative process does not produce IID data points and is therefore most simply formulated either as a joint distribution over a dataset of a given size, or as a set of conditionals in which we incrementally add data points.To construct a complete set of N sample points from the prior (specified by top-level hyper-parameters Ω) we would perform the following operations:</p><p>1. Sample Dirichlet process concentration variable α 0 given the top-level hyperparameters. 2. Construct a partition of N objects into at most N groups using a Dirichlet process. This assignment of objects is denoted by using a set the indicator variables {z (i) } N i=1 . 3. Sample the gate hyperparameters φ x given the top-level hyperparameters. 4. For each grouping of indicators {z (i) : z (i) = r}, sample the input space parameters ψ x r conditioned on φ x . ψ x r defines the density in input space, in our case a full-covariance Gaussian. 5. Given the parameters ψ x r for each group, sample the locations of the input points X r ≡ {x (i) : z (i) = r}. 6. For each group, sample the hyper-parameters for the GP expert associated with that group, θ GP r . 7. Using the input locations X r and hyper-parameters θ GP r for the individual groups, formulate the GP output covariance matrix and sample the set of output values, Y r ≡ {y (i) : z (i) = r} from this joint Gaussian distribution.</p><p>We write the full joint distribution of our model as follows.</p><formula xml:id="formula_4">P ({x (i) , y (i) } N i=1 , {z (i) } N i=1 , {ψ x r } N r=1 , {θ GP r } N r=1 , α 0 , φ x |N, Ω) = N r=1 H N r P (ψ x r |φ x )P (X r |ψ x r )P (θ GP r |Ω)P (Y r |X r , θ GP r ) + (1 -H N r )D 0 (ψ x r , θ GP r ) × P ({z (i) } N i=1 |N, α 0 )P (α 0 |Ω)P (φ x |Ω)<label>(3)</label></formula><p>Where we have used the supplementary notation:</p><formula xml:id="formula_5">H N r = 0 if {{z (i) } : z (i)</formula><p>= r} is the empty set and H N r = 1 otherwise; and D 0 (ψ x r , θ GP r ) is a delta function on an (irrelevant) dummy set of parameters to ensure proper normalisation.</p><p>For the GP components, we use a standard, stationary covariance function of the form</p><formula xml:id="formula_6">Q(x (i) , x (h) ) = v 0 exp - 1 2 D j=1 x (i)j -x (h)j 2 /w 2 j + δ(i, h)v 1<label>(4)</label></formula><p>The individual distributions in equation 3 are defined as follows <ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_7">P (α 0 |Ω) = G(α 0 ; a α0 , b α0 ) (5) P ({z (i) } N i=1 |N, Ω) = PU(α 0 , N )<label>(6)</label></formula><formula xml:id="formula_8">P (φ x |Ω) = N (µ 0 ; µ x , Σ x /f 0 )W(Σ -1 0 ; ν 0 , f 0 Σ -1 x /ν 0 ) G(ν c ; a νc , b νc )W(S -1 ; ν S , f S Σ x /ν S ) (7) P (ψ x r |Ω) = N (µ r ; µ 0 , Σ 0 )W(Σ -1 r ; ν c , S/ν c ) (8) P (X r |ψ x r ) = N (X r ; µ r , Σ r )<label>(9)</label></formula><formula xml:id="formula_9">P (θ GP r |Ω) = G(v 0r ; a 0 , b 0 )G(v 1r ; a 1 , b 1 ) D j=1</formula><p>LN (w jr ; a w , b w ) (10)</p><formula xml:id="formula_10">P (Y r |X r , θ GP r ) = N (Y r ; µ Qr , σ 2 Qr )<label>(11)</label></formula><p>In an approach similar to Rasmussen <ref type="bibr" target="#b4">[5]</ref>, we use the input data mean µ x and covariance Σ x to provide an automatic normalisation of our dataset. We also incorporate additional hyperparameters f 0 and f S , which allow prior beliefs about the variation in location of µ r and size of Σ r , relative to the data covariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Monte Carlo Updates</head><p>Almost all the integrals and summations required for inference and learning operations within our model are analytically intractable, and therefore necessitate Monte Carlo approximations. Fortunately, all the necessary updates are relatively straightforward to carry out using a Markov Chain Monte Carlo (MCMC) scheme employing Gibbs sampling and Hybrid Monte Carlo. We also note that in our model the predictive density depends on the entire set of test locations (in input space). This transductive behaviour follows from the non-IID nature of the model and the influence that test locations have on the posterior distribution over mixture parameters. Consequently, the marginal predictive distribution at a given location can depend on the other locations for which we are making simultaneous predictions. This may or may not be desired. In some situations the ability to incorporate the additional information about the input density at test time may be beneficial. However, it is also straightforward to effectively 'ignore' this new information and simply compute a set of independent single location predictions.</p><p>Given a set of test locations {x * (t) }, along with training data pairs {x (i) , y (i) } and top-level hyper-parameters Ω, we iterate through the following conditional updates to produce our predictive distribution for unknown outputs {y * (t) }. The parameter updates are all conjugate with the prior distributions, except where noted:</p><p>1. Update indicators {z (i) } by cycling through the data and sampling one indicator variable at a time. We use algorithm 8 from <ref type="bibr" target="#b8">[9]</ref> with m = 1 to explore new experts. 2. Update input space parameters. 3. Update GP hyper-params using Hybrid Monte Carlo <ref type="bibr" target="#b9">[10]</ref>. 4. Update gate hyperparameters. Note that ν c is updated using slice sampling <ref type="bibr" target="#b10">[11]</ref>. 5. Update DP hyperparameter α 0 using the data augmentation technique of Escobar and West <ref type="bibr" target="#b11">[12]</ref>. 6. Resample missing output values by cycling through the experts, and jointly sampling the missing outputs associated with that GP.</p><p>We perform some preliminary runs to estimate the longest auto-covariance time, τ max for our posterior estimates, and then use a burn-in period that is about 10 times this timescale before taking samples every τ max iterations. <ref type="foot" target="#foot_1">2</ref> For our simulations the auto-covariance time was typically 40 complete update cycles, so we use a burn-in period of 500 iterations and collect samples every 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Samples From The Prior</head><p>In figure <ref type="figure" target="#fig_0">3</ref> (A) we give an example of data drawn from our model which is multi-modal and non-stationary. We also use this artificial dataset to confirm that our MCMC algorithm performs well and is able recover sensible posterior distributions. Posterior histograms for some of the inferred parameters are shown in figure <ref type="figure" target="#fig_0">3</ref> (B) and we see that they are well clustered around the 'true' values. We note that the posterior mass is located in the vicinity of the true values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inference On Toy Data</head><p>To illustrate some of the features of our model we constructed a toy dataset consisting of 4 continuous functions, to which we added different levels of noise. The functions used were:</p><formula xml:id="formula_11">f 1 (a 1 ) = 0.25a 2 1 -40 a 1 ∈ (0 . . . 15) Noise SD: 7 (12) f 2 (a 2 ) = -0.0625(a 2 -18) 2 + .5a 2 + 20 a 2 ∈ (35 . . . 60) Noise SD: 7 (13) f 3 (a 3 ) = 0.008(a 3 -60) 3 -70 a 3 ∈ (45 . . . 80) Noise SD: 4 (14) f 4 (a 4 ) = -sin(0.25a 4 ) -6 a 4 ∈ (80 . . . 100) Noise SD: 2<label>(15)</label></formula><p>The resulting data has non-stationary noise levels, non-stationary covariance, discontinuities and significant multi-modality. Figure <ref type="figure" target="#fig_1">4</ref> shows our results on this dataset along with those from a single GP for comparison.</p><p>We see that in order to account for the entire data set with a single GP, we are forced to infer an unnecessarily high level of noise in the function. Also, a single GP is unable to capture the multi-modality or non-stationarity of the data distribution. In contrast, our model seems much more able to deal with these challenges.</p><p>Since we have a full generative model over both input and output space, we are also able to use our model to infer likely input locations given a particular output value. There are a number of applications for which this might be relevant, for example if one wanted to sample candidate locations at which to evaluate a function we are trying to optimise. We provide a simple illustration of this in figure <ref type="figure" target="#fig_1">4</ref> (B). We choose three output levels and conditioned on the output having these values, we sample for the input location. The inference seems plausible and our model is able to suggest locations in input space for a maximal output value (+40) that was not seen in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Regression on a simple "real-world" dataset</head><p>We also apply our model and algorithm to the motorcycle dataset of <ref type="bibr" target="#b12">[13]</ref>. This is a commonly used dataset in the GP community and therefore serves as a useful basis for comparison. In particular, it also makes it easy to see how our model compares with standard GP's and with the work of <ref type="bibr" target="#b0">[1]</ref>. Figure <ref type="figure" target="#fig_2">5</ref> compares the performance of our model with that of a single GP. In particular, we note that although the median of our model closely resembles the mean of the single GP, our model is able to more accurately model the low noise level on the left side of the dataset. For the remainder of the dataset, the noise level modeled by our model and a single GP are very similar, although our model is better able to capture the behaviour of the data at around 30 ms. It is difficult to make an exact comparison to <ref type="bibr" target="#b0">[1]</ref>, however we can speculate that our model is more realistically modeling the noise at the beginning of the dataset by not inferring an overly "flat" GP expert at that location. We can also report that our expert adjacency matrix closely resembles that of <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have presented an alternative framework for an infinite mixture of GP experts. We feel that our proposed model carries over the strengths of <ref type="bibr" target="#b0">[1]</ref> and augments these with the several desirable additional features. The pseudo-likelihood objective function used to adapt the gating network defined in <ref type="bibr" target="#b0">[1]</ref> is not guaranteed to lead to a self-consistent distribution and therefore the results may depend on the order in which the updates are performed; our model incorporates a consistent Bayesian density formulation for both input and output spaces by definition. Furthermore, in our most general framework we are more naturally able to specify priors over the partitioning of space between different expert components. Also, since we have a full joint model we can infer inverse functional mappings.</p><p>There should be considerable gains to be made by allowing the input density models be more powerful. This would make it easier for arbitrary regions of space to share the same covariance structures; at present the areas 'controlled' by a particular expert tend to be local. Consequently, a potentially undesirable aspect of the current model is that strong clustering in input space can lead us to infer several expert components even if a single GP would do a good job of modelling the data. An elegant way of extending the model in this way might be to use a separate infinite mixture distribution for the input density of each expert, perhaps incorporating a hierarchical DP prior across the infinite set of experts to allow information to be shared.</p><p>With regard to applications, it might be interesting to further explore our model's capability to infer inverse functional mappings; perhaps this could be useful in an optimisation or active learning context. Finally, we note that although we have focused on rather small examples so far, it seems that the inference techniques should scale well to larger problems The small dots are samples from our model (160 samples per location) evaluated at 80 equally spaced locations across the range (but plotted with a small amount of jitter to aid visualisation). The solid lines show the ± 2 SD interval from a regular GP.</p><p>and more practical tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (A) A set of samples from our model prior. The different marker styles are used to indicate the sets of points from different experts. (B) The posterior distribution of log α 0 with its true value indicated by the dashed line (top) and the distribution of occupied experts (bottom). We note that the posterior mass is located in the vicinity of the true values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results on a toy dataset. (A) The training data is shown along with the predictive mean of a stationary covariance GP and the median of the predictive distribution of our model. (B)The small dots are samples from the model (160 samples per location) evaluated at 80 equally spaced locations across the range (but plotted with a small amount of jitter to aid visualisation). These illustrate the predictive density from our model. The solid the lines show the ± 2 SD interval from a regular GP. The circular markers at ordinates of 40, 10 and -100 show samples from 'reverse-conditioning' where we sample likely abscissa locations given the test ordinate and the set of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (A) Motorcycle impact data together with the median of our model's point-wise predictive distribution and the predictive mean of a stationary covariance GP model. (B)The small dots are samples from our model (160 samples per location) evaluated at 80 equally spaced locations across the range (but plotted with a small amount of jitter to aid visualisation). The solid lines show the ± 2 SD interval from a regular GP.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use the notation N , W, G, and LN to represent the normal, the Wishart, the gamma, and the log-normal distributions, respectively; we use the parameterizations found in<ref type="bibr" target="#b6">[7]</ref> (Appendix A). The notation PU refers to the Polya urn distribution<ref type="bibr" target="#b7">[8]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This is primarily for convenience. It would also be valid to use all the samples after the burn-in period, and although they could not be considered independent, they could be used to obtain a more accurate estimator.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Ben Marlin for sharing slice sampling code and to Carl Rasmussen for making minimize.m available.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Infinite mixtures of Gaussian process experts</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixture of Gaussian processes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised learning from incomplete data via an EM approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An alternative model for mixtures of experts</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 7</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The infinite Gaussian mixture model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="554" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive mixture of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian Data Analysis</title>
		<imprint>
			<publisher>Chapman and Hall</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ferguson distributions via Polya urn schemes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="355" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for Dirichlet process mixture models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Probabilistic inference using Markov chain Monte Carlo methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<idno>CRG-TR-93-1</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slice sampling (with discussion)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="705" to="767" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computing Bayesian nonparametric hierarchical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Practical Nonparametric and Semiparametric Bayesian Statistics, number 133 in Lecture Notes in Statistics</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Some aspects of the spline smoothing approach to non-parametric regression curve fitting</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Stayt Society. Ser. B</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="52" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
