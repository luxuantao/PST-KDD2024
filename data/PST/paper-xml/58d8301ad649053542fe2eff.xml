<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Depression using Vocal, Facial and Semantic Communication Cues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Lincoln Laboratory</orgName>
								<address>
									<addrLine>244 Wood Street Lexington</addrLine>
									<postCode>02420</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrianne</forename><surname>Schwarzentruber</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MIT Lincoln Laboratory</orgName>
								<address>
									<addrLine>244 Wood Street Lexington</addrLine>
									<postCode>02420</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Dept</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elizabeth</forename><surname>Godoy</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">MIT Lincoln Laboratory</orgName>
								<address>
									<addrLine>244 Wood Street Lexington</addrLine>
									<postCode>02420</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Beckman Institute University of Illinois</orgName>
								<address>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charlie</forename><surname>Dagli</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">MIT Lincoln Laboratory</orgName>
								<address>
									<addrLine>244 Wood Street</addrLine>
									<settlement>Lexington</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miriam</forename><surname>Cha</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Computer Science Dept</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youngjune</forename><surname>Gwon</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">MIT Lincoln Laboratory</orgName>
								<address>
									<addrLine>244 Wood Street</addrLine>
									<settlement>Lexington</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
							<affiliation key="aff8">
								<orgName type="department">MIT Lincoln Laboratory</orgName>
								<address>
									<addrLine>244 Wood Street Lexington</addrLine>
									<postCode>02420</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Depression using Vocal, Facial and Semantic Communication Cues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">00DCC8FE5DF01801D5E5E9B9317643F3</idno>
					<idno type="DOI">10.1145/2988257.2988263</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Affective Computing</term>
					<term>Major Depressive Disorder</term>
					<term>Text Mining</term>
					<term>Speech</term>
					<term>Facial Expression</term>
					<term>Semantic Analysis</term>
					<term>Challenge</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Major depressive disorder (MDD) is known to result in neurophysiological and neurocognitive changes that affect control of motor, linguistic, and cognitive functions. MDD's impact on these processes is reflected in an individual's communication via coupled mechanisms: vocal articulation, facial gesturing and choice of content to convey in a dialogue. In particular, MDDinduced neurophysiological changes are associated with a decline in dynamics and coordination of speech and facial motor control, while neurocognitive changes influence dialogue semantics. In this paper, biomarkers are derived from all of these modalities, drawing first from previously developed neurophysiologicallymotivated speech and facial coordination and timing features. In addition, a novel indicator of lower vocal tract constriction in articulation is incorporated that relates to vocal projection. Semantic features are analyzed for subject/avatar dialogue content using a sparse coded lexical embedding space, and for contextual clues related to the subject's present or past depression status. The features and depression classification system were developed for the 6th International Audio/Video Emotion Challenge (AVEC), which provides data consisting of audio, video-based facial action units, and transcribed text of individuals communicating with the human-controlled avatar. A clinical Patient Health Questionnaire (PHQ) score and binary depression decision are provided for each</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Major depressive disorder (MDD) is the most prevalent mood disorder, with a lifetime risk of 10-20% for women and 5-12% for men <ref type="bibr">[7]</ref>. As the number of people suffering from MDD steadily increases, so too does the burden of accurate diagnosis. The growing global burden of MDD suggests that a convenient and automated method to evaluate depression severity would both simplify and standardize the task of diagnosing and monitoring depression, allowing for greater availability and uniformity in assessment. One advance in this direction presented by AVEC 2016 is use of a virtual avatar to interact with patients, albeit with human control due to current limitations in natural dialogue technologies. An automated approach for depression evaluation may reduce multiple in-office clinical visits, facilitate accurate measurement and identification, and quicken the evaluation of treatment. Toward these objectives, potential depression biomarkers of growing interest are vocal-and facial expressionbased features, two categories of easily-acquired measures that have been shown to change with a patient's mental condition and emotional state <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref>. Other higher-level aspects of speech such as language and cognitive expression are also known to be affected by depressed state <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b20">20]</ref>. In this paper, we bring together biomarkers derived from these modalities through voice, face and semantic analysis.</p><p>Neurophysiological changes in depression impact the motor control functions that drive vocal and facial gesturing, while neurocognitive changes influence the quality and manner of response in dialogue. In neurophysiological change, many individuals suffering from MDD suffer from psychomotor retardation, which affects mechanisms controlling speech production and facial expression. Previously developed neurophysiologically motivated vocal and facial coordination and timing features have been shown to be powerful indicators of depression <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34]</ref>. In particular, for the audio modality, formant correlations and phone-dependent duration features proved very effective in classifying depression. Here, we have extended this basis to also include features based on novel indicators of lower vocal tract constriction and measures of variability vocal quality. For the video modality, we exploited the facial action coding system (FACS), which quantifies localized changes in facial expression representing facial action units (FAUs) that correspond to distinct muscle movements of the face <ref type="bibr" target="#b18">[18]</ref>. Combined, the consistently high depression classification performance of vocal and facial coordination and timing features indicate that these biomarkers are effectively capturing the influence of psychomotor retardation attributed to MDD <ref type="bibr" target="#b26">[26]</ref>.</p><p>In addition to drawing from and extending findings motivated by neurophysiological changes in MDD, our work further seeks to link analysis of motor function to neurocognitive indicators in communication. Specifically, text-based semantic content features are used to exploit word representations using GloVe model embedding in combination with a high-level feature learning method <ref type="bibr" target="#b25">[25]</ref>. In highlighting semantic content, this methodology shows that embedding of text allows for indicators of neurocognitive state, with the interviewer (avatar) content representing the most powerful indicator, presumably because her words avoid sources of intersubject variability that are not related to depression. This observation highlights a limitation of previous research on text-based sentiment analysis that focuses solely on a subject's text (e.g. sentiment classification using Twitter messages <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b16">16]</ref> and movie rating classification based on review texts <ref type="bibr" target="#b15">[15]</ref>). In this paper, we introduce text-based methods that infer semantic content from the interviewer's questions as well as from the interviewee's answers, leading to a better estimation of neurocognitive state (i.e., depression). In addition, text-based semantic context features are used that focus on specific points in the interview dialogue, accumulating indicators of potential depression reoccurrence along with commitment to therapy and the individual's self-described assessment of his or her current state.</p><p>Data for feature and system development is provided by the 6th International Audio/Video Emotion Challenge (AVEC) consisting of audio and corresponding transcribed text elicited from participant interviews with the USC Creative Technology Center avatar, "Ellie." For each participant in the training and development sets, a Patient Health Questionnaire (PHQ) score and binary depression decision are provided. The challenge evaluation criterion is mean F1 score, which is a combined measure of precision and recall in detecting both depressed and non-depressed subjects <ref type="bibr" target="#b31">[31]</ref>. Using a Gaussian staircase regressor with fusion of the multi-modal features designed in-house, our system achieves a mean F1=0.81, RMSE=5.31, and MAE=3.34 on the development set, compared to the challenge baseline mean F1=0.73, RMSE=6.62, and MAE=5.52. Additionally, our best PHQ correlation results were obtained by combining predictions from audio, video and text modalities. This motivates continued study into joint multi-modal feature analysis to capture interplay of motor, linguistic, and cognitive components of communication.</p><p>In Section 2 we discuss our set of audio speech features and Section 3 describes the video facial features for estimating depressive state and Section 4 outlines the text-based semantic features. Section 5 describes the classification approaches and provides results for audio, video, semantic and ensemble systems. Discussion and concluding remarks are in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">AUDIO FEATURES 2.1 Audio Data Preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Audio Data Considerations</head><p>Careful examination of the AVEC 2016 Depression dataset revealed some issues related to the audio acquisition and segmentation. First, the segmentation of participant speech, taken from time stamps on provided transcripts, contained substantial errors for some subjects, resulting in audio-transcript missalignments. Subsequently, forced alignment of the participant speech to acquire phone and word boundaries was sensitive to these errors and could not be used reliably. Some segmentation inconsistencies also result in Ellie the avatar's speech being included in audio intended to be from the Participant. This could distort audio feature statistics according to the percentage of Ellie's speech included in the Participant segments. There also appeared to be a change in Ellie's behavior after approximately one third of the participants audio had been collected. This was observed in the transcript text of Ellie's statements and questions as well as in turn duration statistics based on the segmentation. As in many applications, separation of Ellie and the Participant's speech might benefit from tailored diarization for this dataset.</p><p>Additionally, audio quality varied across subjects, with what seemed like initial (low numbered) collects suffering from higher noise levels and consequently lower SNR. This might be due to a common and understandable real-world problem of adjusting microphone, subject and room set-ups accordingly as a collect gets underway. Audio levels across subjects were also sensitive to microphone taps, coughs and other impulse-like sounds. Finally, for a few of the subjects, the avatar Ellie's speech had higher energy levels and thus could not be considered low-level crosstalk. Ultimately, as in most real-world contexts, treatment of the audio channel requires certain algorithmic adaptations to the data. The following audio pre-processing and feature design sought to limit the impact of any data acquisition and segmentation issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Audio Preprocessing</head><p>For each file, the Participant's speech was extracted using the segmentation provided in the transcript, despite the presence of some errors described in section 2.1.1. The final waveform was amplitude normalized to adjust for level differences in the recordings. Specifically, to mitigate observed disturbances in the audio due to coughs, sniffles, and what seem to be microphone taps, the SNR output level for each file was first limited. With informal listening, a moderate cutoff value of -15dB was selected. Finally, the Participant's audio file was normalized to have a maximum absolute value of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spectral Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Correlation structure of formant tracks</head><p>Properties of vocal tract resonances over time contain information about speech dynamics related to articulatory properties of the depressed voice. A formant tracking algorithm based on Kalman filtering was used to obtain smooth estimates of the first three resonant frequencies over time <ref type="bibr" target="#b21">[21]</ref>. Formant frequencies were extracted at every 10 ms from the audio signal, which was unprocessed other than being segmented based on the transcripts to include subject speech. Embedded in the formant tracking algorithm is a voice-activity detector that allows a Kalman smoother to smoothly coast through non-speech regions. Estimates of the third formant that went above a threshold of 4.5k Hz were truncated.</p><p>Next, speaker turn segments above one second in duration were used for further feature processing. For each of these segments, formant correlation structure features were computed as follows. A channel-delay correlation matrix was computed from the formant tracks using time-delay embedding. The correlation matrix, with dimensionality (45 x 45), based on three formant channels and 15 time delays per channel, with 3-frame (30 ms) delay spacing. From this matrix the 45-dimensional rank-ordered eigenspectrum was computed, characterizing the within-channel and cross-channel distributional properties of the multivariate formant time series. These articulatory coordination measures have previously been used for estimating depression severity, Parkinson's disease, age-related cognitive decline, mild TBI, and cognitive load <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref>.</p><p>Dimensionality reduction of the 45-dimensional correlation structure feature vectors was done as follows. Each feature element was z-scored across the training set so that features at each eigenvalue index are afforded equal weight, and then principal component analysis (PCA) was used to produce a fourdimensional feature vector. The z-scoring and PCA transformations obtained from the training set were then applied to the test set feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Correlation structure of delta MFCCs</head><p>To introduce vocal tract spectral magnitude information, a standard set of 16 MFCCs was generated by Opensmile from segmented but otherwise unprocessed audio files <ref type="bibr" target="#b6">[6]</ref>. Delta MFCCs (dMFCCs) were then computed, which reflect dynamic velocities of the MFCCs over time. Delta coefficients were computed using a delay parameter of 2 (regression over two frames before and after a given frame).</p><p>From each speaker turn over one second in duration, a channeldelay correlation matrix was computed from the dMFCCs using time-delay embedding, with dimensionality (240 x 240), based on 16 dMFCC channels and 15 delays per channel with 1-frame (10 ms) delay spacing. From this matrix the 240-dimensional rankordered eigenspectrum was computed, which characterizes the within-channel and cross-channel distributional properties of the multivariate dMFCC time series. These spectral coordination measures have previously been used to estimate the severity of depression, Parkinson's disease, and cognitive load from speech <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref>. Dimensionality reduction of the 240-dimensional correlation structure feature vectors to four principal components was done using the same procedure used for the formant correlation structure features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lower Vocal Tract Physiology Features</head><p>Though often overlooked in speech processing applications, the lower Vocal Tract (VT) plays a key role in speech production <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>. Situated between the glottis and pharynx, the lower VT cavities regulate flow throughout the speech production system, thus representing a region of high source-filter coupling <ref type="bibr" target="#b30">[30]</ref>. Most notably in the voice community is the lower VT's critical role in the production of a singer's/speaker's formant or a form of production sometimes referred to as "resonant" voice. In this type of production mode, the individual makes a concerted effort to project his or her voice by narrowing the epilarynx cavity and strengthening constrictions at the epilarynx and piriform openings. Spectrally, this results in an enhancement of the distinct lower VT resonance pattern <ref type="bibr" target="#b13">[13]</ref>, specifically amplifying the epilarynx resonance (typically around 3kHz) and deepening the piriform null (typically around 5kHz). On the contrary, for patients with vocal issues (e.g. hoarseness), the opposite trend has been observed <ref type="bibr" target="#b23">[23]</ref>. In the context of MDD, the hypothesis follows similarly that the opposite of a loud and deliberate production mode trend will be observed in depressed subjects, corresponding to less vocal effort manifested as a relaxation of control of the lower VT cavities by muscles at and adjacent to the larynx.</p><p>In order to quantify the relative degree of speakers formant phenomena, the following metric was used. First, the mean (in dB) spectral envelope (True Envelope <ref type="bibr" target="#b28">[28]</ref> order 40 over 3 pitch periods <ref type="bibr" target="#b10">[10]</ref>) was calculated for voiced frames in two frequency regions, 3-4kz and 4-5kHz, respectively approximating epilarynx and piriform bands. Then, to quantify contrast in the relative degree of enhancement between the epilarynx resonance and piriform null, the ratio (difference in dB) of energy between the lower 3-4kHz and upper 4-5kHz band was calculated. As expected, for depressed participants, this localized spectral energy difference was less than for the non-depressed subjects. This represents a novel finding, both in objectively quantifying the degree of enhancement of the lower VT resonance patterns and in application for depression classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loudness Variation Features</head><p>As a gross indicator of loudness linked to waveform shapes, a peak-to-rms measure was calculated on a segmental level, reflecting a local loudness metric related to waveform shape across a few pitch periods (with a standard analysis window of 30ms). In order to capture indications that might be linked to prosodic variation across the session, the global standard deviation of local (mean, std, range) peak-to-rms statistics were the features used for classification. The local mean, standard deviation and range (difference between top and bottom 5% values) statistics were calculated for voiced frames in 2 second time intervals sliding with 50% overlap. For depressed speech, the variations in peak2rms levels across the session were higher, potentially indicating mixed regions of both modal and nonmodal phonation. While the peak-to-rms statistic variations were statistically significant when correlated with the PHQ score, a complementary loudness feature was not statistically significant even though it provided indications of a trend towards overall softer speaking levels for depressed subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VIDEO FEATURES</head><p>To capture the joint dynamical properties across multiple facial action units (FAUs) during speech, the correlation structure of FAUs was computed. From each speaker turn over one second in duration, a channel-delay correlation matrix was computed from the FAUs using time-delay embedding, with dimensionality (300 x 300), based on 20 FAU channels and 15 delays per channel with 1-frame (33 ms) delay spacing. From this matrix the 300dimensional rank-ordered eigenspectrum was computed, which characterizes the within-channel and cross-channel distributional properties of the multivariate FAU time series. These facial coordination measures have previously been used to estimate the severity of depression and cognitive load from speech <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b34">34]</ref>. Dimensionality reduction of the 300-dimensional correlation structure feature vectors to four principal components was done using the same procedure used for the formant and delta MFCC correlation structure features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SEMANTIC FEATURES</head><p>Two approaches for text-based semantic processing were done. The first approach is semantic content analysis, based on summarizing spoken content by projecting transcribed sentences into high dimensional word spaces and learning statistical regression models that relate the word embeddings to PHQ scores. The second approach is semantic context analysis, which obtains a coarse characterization of contextual evidence related to depression, based on factors such as previous depression diagnoses, indications of ongoing therapy, and indicators of negative emotional state and feelings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Content Features</head><p>The semantic content analysis approach uses the provided transcripts to discover, using mappings to a word embedding space followed by sparse coding, correlations between the semantic content of both interviewer and interviewee with the PHQ scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Preprocessing</head><p>The following preprocessing of raw transcripts was first done: Stopword elimination. The most frequently occurring words often do not carry much information. Common words such as 'a', 'on', and 'through', were automatically removed based on the hypothesis that these words contain little discriminative information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">GloVe embedding of questions and answers</head><p>Global vectors for word representation, or GloVe, is a distributed text representation method in which words are embedded in a high dimensional space, with distances between the words arising from their co-occurrence statistics within documents <ref type="bibr" target="#b25">[25]</ref>. The embedding can be trained using word2vec or obtained from a preexisting model trained on a large text corpus <ref type="bibr" target="#b22">[22]</ref>. A preexisting GloVe model with a 50-dimensional embedding space, which was trained on the Wikipedia 2014+Gigaword 5 data set, was used as an embedding space. A question or answer sentence was then represented by the average of its embedded word vectors.</p><p>The embedded vector space from representing the set of questions and answers were next transformed into two alternative representations, using 1) principal components analysis (PCA) and 2) the whitening (ZCA) transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">High-level feature learning</head><p>Next, sparse coded feature representations were formed from the PCA-and ZCA-transformed embedded spaces. With sparse coding, more complex topic-related features can be automatically learned <ref type="bibr" target="#b24">[24]</ref>, often leading to better generalization performance in classification and regression problems than is obtained from handengineered algorithms.</p><p>Given an input feature patch x  R N , sparse coding solves for a representation y  R K in the following optimization problem:</p><formula xml:id="formula_0">min {D,y} (||x -Dy|| 2 ) 2 + ||y|| 1 s.t ||d i ||  1, I,<label>(4)</label></formula><p>where d i is the i th dictionary atom in an overcomplete dictionary DR NK and where &gt;0 is a regularization parameter. In this paper least angle regression (LARS) was used in the dictionary learning algorithm from the SPAMS toolbox <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b19">19]</ref>. Parameters used were K=200 and =0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Multivariate regression</head><p>A linear support vector regressor (SVR) was used to learn a mapping from the text-based feature vectors to subjects' PHQ scores. Separate evaluations were done for question-and answerbased feature vectors, and based on using raw GloVe features, GloVe+PCA, and GloVe+ZCA features prior to sparse coding.</p><p>The SVR was learned from the training set and validated on the development set. Because the SVR produces a separate PHQ prediction for each question or answer, a method is needed for fusing these predictions across a subject's session. The adopted method is to use the average of the top n predicted PHQ scores for each subject, based on the hypothesis that the questions or answers that provide the strongest predictions of depression level are also the most indicative of depression. Validation testing on the development set resulted in the use of n=1 for the PCA-based feature vectors and n=6 for the ZCA-based feature vectors.</p><p>To evaluate the relative usefulness of the different feature approaches, the development set PHQ predictions were converted to depression predictions using thresholds, resulting in mean F1 scores as shown in Table <ref type="table" target="#tab_0">1</ref>. Notice that the highest mean F1 scores were obtained for question-based features, with PCA and ZCA producing better results than using the raw GloVe features (as input to sparse coding). Based on these results, the questionbased GloVe+PCA and GloVe+ZCA PHQ predictions were selected as the text content analysis features used by the depression classification system described in Section 5. Dimensionality reduction of this two-dimensional feature vector to a single principal component was done using the same procedure used for the correlation structure feature vectors in Sections 2 and 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Context Features</head><p>The semantic context indicators use the provided transcripts to infer a subject's status with respect to four conceptual classes. The first conceptual class seeks priors on depression based on prior diagnosis. The second class seeks assessment of current or prior involvement in therapy. For both of the above context classes, the indicator relies on Ellie's questions and her responses to participant answers to provide consistent measures across all participants. The third conceptual class seeks indicators of the participant's present state and feelings, focusing on the PHQ 2 and 8 questionnaire categories. Ellie's questions and responses are considered as well as keywords in the participant's answer. The fourth class seeks indications of past or present suicidal ideation. Figure <ref type="figure" target="#fig_2">2</ref> summarizes the four semantic context indicators. The semantic context feature is the sum of points accrued from all four indicators. If none of the indicators' conditions are satisfied, then the participant receives a feature value of zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DEPRESSION CLASSIFICATION SYSTEM 5.1 Dataset and Evaluation</head><p>The experiments in this section use the Distress Analysis Interview Corpus -Wizard of Oz (DAIC-WOZ) dataset <ref type="bibr" target="#b32">[32]</ref> for train and test provided for the AVEC 2016 depression challenge. The depressed state of subjects is based on the PHQ-8 metric <ref type="bibr" target="#b17">[17]</ref>.</p><p>Our approach to depression classification is to learn a statistical regression model for predicting the PHQ scores from the training set, and to classify as depressed subjects that have suprathreshold PHQ predictions. The training, development, and test sets contain 107, 35, and 47 subjects, respectively, containing 21, 7, and 9 depressed subjects in each set. Development set </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Gaussian Staircase Model</head><p>The Gaussian staircase is a statistical modeling approach that generalizes the use of Gaussian distributions for binary classification into the domain of multivariate regression <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref>. This is done by partitioning the outcome variable into multiple nested ranges with binary class labels for "lower" and "higher" being associated with complementary ranges at each nested partition. A multivariate normal distribution is used to model the class-conditioned features in each partition, and the class-conditioned likelihoods are computed by summing the likelihoods across all the partitions.</p><p>In the current work PHQ is the outcome variable. PHQ scores are partitioned into five staircase levels for each class C k . The PHQ ranges for these levels are [0-5, 0-7, 0-10, 0-12, 0-15] for C 1 (nondepressed) and <ref type="bibr">[6-23, 8-23, 11-23, 13-23, 16-23]</ref> for C 2 (depressed). The likelihood model for each class at each partition is defined using a multivariate normal distribution obtained from the training data.</p><p>The final class-conditioned likelihoods, p(x i | C k ), are obtained by summing over staircase likelihoods. The resulting two-class loglikelihood ratio,</p><formula xml:id="formula_1">y i = log(p(x i | C 2 ))  log(p(x i | C 1 )),<label>(3)</label></formula><p>is used as a basis for PHQ prediction. For the correlation structure features there is a separate feature vector per speaker turn within each session, so further processing is needed to obtain a single log-likelihood ratio from the entire session. Better discrimination performance is obtained by using the median log-likelihood ratio from only the highest n log-likelihood ratios per speaker. This is done based on the hypothesis that the speaker turns with the highest log-likelihood ratios will more reliably capture depression-related speech differences. The median of the same number of log-likelihood ratios, n=25, is computed for all three correlation structure feature sets. Next, a linear regression model is constructed from the training set log-likelihood ratios and applied to the test set log-likelihood ratios, and depression predictions made when the regression outputs, z i , exceed the following empirically derived threshold, z thresh = mean i (z i ) + 0.9 std i (z i ).</p><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Feature Set Results</head><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the results on the development data of the seven individual feature sets used in our depression classification system. The mean F1 scores are shown based on assigning depression predictions using equation ( <ref type="formula" target="#formula_0">4</ref>). An alternative measure of detection accuracy, the area under the receiver operating characteristic (ROC) curve, or AUC, is also shown. Finally, the Pearson correlations, r, of the PHQ predictions to the actual PHQ scores are shown.</p><p>Examining Table <ref type="table" target="#tab_0">1</ref>, we see that the Correlation Structure (CS) features for audio and video perform well, as is consistent with previous results for depression classification. However, the CS features' performance is lower than might be expected here, likely due to the fact that the analyzed speech segments are, on average, of very short duration. Also, with regard to the organizer-provided transcript-based segmentations, the lower VT features appear to be affected by changes in turn segmentation that occurred starting with subject 363 in the data set. Though analysis on voiced frames might have limited some of these sensitivities, hindsight would suggest isolating a subset of segments as in the approaches for the CS-based features. Finally, semantic features prove to be highly informative for depression classification, indicating a powerful modality to be exploited in future work on depression classification. Overall, the audio, video and semantic feature sets in Table <ref type="table" target="#tab_0">1</ref> provide complementary information relating to motor function timing and dynamics as well as dialogue semantics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Fused Results</head><p>As described in Section 5.3, separate PHQ predictions are made from each feature set. Fusion of these predictions is done using a weighted average of PHQ predictions. Using the method from <ref type="bibr" target="#b34">[34]</ref>, fusion weights are determined based on Pearson correlations on the development set: w=1/(1-r 2 ). This weighting system allows substantial weighting of all feature sets (with a minimum weight of one), but also provides stronger weighting of the more useful feature sets. Notice that, while it would be easy to design weights that give better fused performance on the development set, such an approach is avoided due to concerns of overfitting.  Table <ref type="table" target="#tab_4">3</ref> lists the organizer baseline results on the development set within the audio and video modalities, as well as baseline ensemble results. From audio, our system obtains comparable performance to the baseline system, albeit with a small advantage in mean F1 and RMSE. From video, our system performs worse in terms of mean F1 but better in terms of RMSE and MAE. Finally, our ensemble system far outperforms the baseline system due to its inclusion of a semantic analysis component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Test Set Results</head><p>Our ensemble system was also used to predict depressed subjects on the held out test set. Based on the threshold rule in equation (4), our system predicted 13 subjects as being depressed, of which six were actually labeled depressed, resulting in a mean F1 score of 0.70. Thus, we see a moderate performance degradation from the development mean F1 score of 0.81. Access to the true PHQ scores would allow a better understanding of how much of this performance degradation is due to inability to generalize to the test set versus variability in the distribution of depression labels relative to PHQ scores. The mean F1 score that we obtained on the test set is the same as the baseline results from the video and ensemble systems. However, any direct comparisons between these results must be tentative, as the recall values reported in the baseline paper <ref type="bibr" target="#b32">[32]</ref> indicate that the baseline system was evaluated on a data set with a different number of subjects labeled as depressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>In this work, fusion of audio, video and semantic features which were motivated by neurophysiological and neurocognitive effects of MDD allowed for a high performing depression detection system. Previously developed audio and video motor control features based on correlation structure performed moderately well on this AVEC 2016 dataset and were complemented by novel physiologically-based features linked to vocal projection through control of the lower vocal tract. Semantic analyses of dialogue transcripts provided the highest performing features, suggesting that future work on depression classification should exploit semantic features. Also, interestingly enough, the most informative indicators of the dialogue content and context were obtained by analyzing the avatar's text, which avoids sources of inter-subject variability unrelated to depression. This observation could play an important role in the design of automatic screenings for depression diagnostics.</p><p>Future work calls for joint analyses across speech, facial gestures and semantic content and context in a dialogue. For example, degrees of motor coordination might vary as a function of communicative intent in a conversation. Specifically, the question arise of how a person's articulation and facial gesturing are impacted by affectively stressing or neurocognitively challenging dialogue turns (e.g. recalling a difficult experience in life). That is, audio and visual modalities conditioned on semantics might reveal differences based on the individual's affective and/or neurological state. Ultimately, with grounding in neurophysiological and cognitive analyses, we seek to exploit the interplay between what and how a person communicates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Question/</head><label></label><figDesc>Answer pair extraction. Given a raw transcript, question-answer pairs were formed each time Ellie asked a new question. Examples are shown in Figure 1. Filled pause extraction. A substantial portion of human communication is non-verbal. Non-verbal cues (e.g., postures, facial expression, eye gaze, gestures) provide useful information in determining a participant's psychological distress. For example, a participant's laughter can indicate a positive affective response in a conversation. Along with verbal features, commonly occurring filled pauses were extracted, inclusive of [laughter], [sniffle], [cough], [sigh], [deep breath].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example of automated extraction of questions and answers.</figDesc><graphic coords="4,317.76,57.12,240.96,204.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Semantic context indicators. predictions were obtained by training the model on the training set. Test set predictions were obtained by training the model on the combined training/development set.</figDesc><graphic coords="5,319.86,56.88,236.70,211.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Results on development set for our ensemble PHQ prediction system, with predicted PHQ plotted as a function of true PHQ. Depressed subjects are plotted in red and predicted depressed subjects plotted with surrounding green circles.</figDesc><graphic coords="6,332.70,243.42,210.42,157.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Semantic content performance on Development set.</head><label>1</label><figDesc></figDesc><table><row><cell>Text Type</cell><cell>Embedded</cell><cell>Mean F1</cell></row><row><cell></cell><cell>Space</cell><cell></cell></row><row><cell>Question</cell><cell>GloVe</cell><cell>0.24</cell></row><row><cell></cell><cell>Glove+PCA</cell><cell>0.62</cell></row><row><cell></cell><cell>Glove+ZCA</cell><cell>0.75</cell></row><row><cell>Answer</cell><cell>GloVe</cell><cell>0.46</cell></row><row><cell></cell><cell>Glove+PCA</cell><cell>0.62</cell></row><row><cell></cell><cell>Glove+ZCA</cell><cell>0.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 . Performance of individual feature sets on development data.</head><label>1</label><figDesc></figDesc><table><row><cell>Modality</cell><cell>Feature Set</cell><cell>Mean</cell><cell cols="2">AUC r</cell></row><row><cell></cell><cell></cell><cell>F1</cell><cell></cell><cell></cell></row><row><cell>Audio</cell><cell>1. CS-Formant</cell><cell>0.55</cell><cell>0.71</cell><cell>0.41</cell></row><row><cell></cell><cell>2. CS-dMFCC</cell><cell>0.45</cell><cell>0.56</cell><cell>0.33</cell></row><row><cell></cell><cell>3. Lower VT</cell><cell>0.51</cell><cell>0.58</cell><cell>0.07</cell></row><row><cell></cell><cell>4. Loudness</cell><cell>0.76</cell><cell>0.69</cell><cell>0.32</cell></row><row><cell></cell><cell>Var.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Video</cell><cell>5. CS-FAU</cell><cell>0.53</cell><cell>0.63</cell><cell>0.44</cell></row><row><cell>Semantic</cell><cell>6. Content</cell><cell>0.81</cell><cell>0.93</cell><cell>0.81</cell></row><row><cell></cell><cell>7. Context</cell><cell>0.76</cell><cell>0.91</cell><cell>0.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>summarizes the fused results within each sensor modality and across all modalities. In addition to the statistics shown in Table1, the RMSE and MAE values for the PHQ predictions are also shown, which facilitates comparison with the baseline benchmark results. In Figure3the predicted PHQ scores for our ensemble system are plotted as a function of true PHQ score. Non-depressed subjects are plotted in blue and depressed in red. Green circles are plotted around the eight subjects predicted to have depression. Notice that the false positive classifications are subjects with reasonably high PHQ scores of 9, 10 and 15.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . Performance on development set of fused feature sets both within and across sensor modalities.</head><label>2</label><figDesc></figDesc><table><row><cell>Modality</cell><cell>Mean</cell><cell>AUC</cell><cell>r</cell><cell cols="2">RMSE MAE</cell></row><row><cell></cell><cell>F1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio</cell><cell>0.57</cell><cell>0.72</cell><cell>0.44</cell><cell>6.38</cell><cell>5.32</cell></row><row><cell>Video</cell><cell>0.53</cell><cell>0.63</cell><cell>0.44</cell><cell>6.45</cell><cell>5.33</cell></row><row><cell>Semantic</cell><cell>0.84</cell><cell>0.94</cell><cell>0.83</cell><cell>4.46</cell><cell>3.34</cell></row><row><cell>Ensemble</cell><cell>0.81</cell><cell>0.92</cell><cell>0.84</cell><cell>5.31</cell><cell>4.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 . AVEC 2016 Depression baseline performance on development set [32].</head><label>3</label><figDesc></figDesc><table><row><cell>Modality</cell><cell cols="3">Mean F1 RMSE MAE</cell></row><row><cell>Audio</cell><cell>0.50</cell><cell>6.74</cell><cell>5.36</cell></row><row><cell>Video</cell><cell>0.72</cell><cell>7.13</cell><cell>5.88</cell></row><row><cell>Ensemble</cell><cell>0.72</cell><cell>6.62</cell><cell>5.52</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentiment analysis of twitter data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vovsha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL 2011 Workshop on Languages in Social Media</title>
		<meeting>ACL 2011 Workshop on Languages in Social Media</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Voice acoustical measurement of the severity of major depression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cannizzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and cognition</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="35" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech and voice parameters of depression: A pilot study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Darby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Communication Disorders</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="75" to="85" />
			<date type="published" when="1984">1984. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Predicting depression via social media, Association for the Advancement of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Counts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vocal indicators of mood change in depression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ellgring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="110" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-10">2010, October</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Major depressive disorder</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kendler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="335" to="341" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acoustical properties of speech as indicators of depression and suicidal risk</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>France</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Shiavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="829" to="837" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Facial expression and emotional face recognition in schizophrenia and depression. European archives of psychiatry and clinical neuroscience</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gaebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wölwer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992. 1992</date>
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="page" from="46" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Estimating lower vocal tract features with closed-open phase spectral analyses</title>
		<author>
			<persName><forename type="first">E</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malyska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="771" to="775" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Durational variability in speech and the rhythm class hypothesis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Papers in laboratory phonology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="515" to="546" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Articulatory dynamics and coordination in classifying cognitive change with preclinical mTBI</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Helfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Keyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lacrignola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Talavage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heaton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="485" to="489" />
		</imprint>
	</monogr>
	<note>In INTERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualisation of hypopharyngeal cavities and vocal-tract acoustic modelling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hirata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Masaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Methods in Biomech. &amp; Biomed. Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><surname>Inria</surname></persName>
		</author>
		<ptr target="http://spams-devel.gforge.inria.fr/" />
		<title level="m">Sparse modeling software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Movie reviews and revenues: An experiment in text regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-06">2010. June</date>
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Twitter sentiment analysis: The good the bad and the omg</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kouloumpis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ICWSM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The PHQ-8 as a Measure of Current Depression in the General Population</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kroenke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Affective Disorders</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The computer expression recognition toolbox (CERT)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition and Workshops (FG 2011)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-03">2011. March. 2011</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online Dictionary Learning for Sparse Coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Limbic-cortical dysregulation: a proposed model of depression</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Mayberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J, Neuropsychiatry Clin Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="471" to="481" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kalmanbased autoregressive moving average modeling and inference for formant and antiformant tracking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rudoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1732" to="1746" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The speaker&apos;s formant in male voices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nawka</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">C</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cebulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zurakowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Voice</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="422" to="428" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by V1?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014-10">2014, October</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malyska</surname></persName>
		</author>
		<title level="m">Source Biomarkers for Depression: A Link to Psychomotor Activity</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vocal biomarkers to discriminate cognitive load in a working memory task</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Smalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perricone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient spectral envelope estimation and its application to pitch shifting and envelope preservation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Röbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Rodet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Audio Effects</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Articulatory interpretation of the singing formant</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="838" to="844" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Acoustic interactions of the voice source with the lower vocal tract</title>
		<author>
			<persName><forename type="first">I</forename><surname>Titze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Story</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="2234" to="2243" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Phonologically-based biomarkers for major depressive disorder</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Trevino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malyska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01600</idno>
		<title level="m">Depression, Mood, and Emotion Recognition Workshop and Challenge</title>
		<imprint>
			<date type="published" when="2016-05">2016. May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vocal biomarkers of depression based on motor incoordination</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Helfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge</title>
		<meeting>the 3rd ACM international workshop on Audio/visual emotion challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-10">2013. October</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vocal and facial biomarkers of depression based on motor incoordination and timing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Helfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciccarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-11">2014, November</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Segment-dependent dynamics in predicting Parkinson&apos;s disease</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Helfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perricone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciccarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015-09">2015, September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Prediction of cognitive performance in an animal fluency task based on rate and articulatory markers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mundt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1038" to="1042" />
		</imprint>
	</monogr>
	<note>In INTERSPEECH</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
