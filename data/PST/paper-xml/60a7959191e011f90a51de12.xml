<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-20">20 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
							<email>bolin.ding@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
							<email>wlam@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-20">20 May 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3404835.3462913</idno>
					<idno type="arXiv">arXiv:2105.09710v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conversational Recommendation</term>
					<term>Reinforcement Learning</term>
					<term>Graph Representation Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conversational recommender systems (CRS) enable the traditional recommender systems to explicitly acquire user preferences towards items and attributes through interactive conversations. Reinforcement learning (RL) is widely adopted to learn conversational recommendation policies to decide what attributes to ask, which items to recommend, and when to ask or recommend, at each conversation turn. However, existing methods mainly target at solving one or two of these three decision-making problems in CRS with separated conversation and recommendation components, which restrict the scalability and generality of CRS and fall short of preserving a stable training procedure. In the light of these challenges, we propose to formulate these three decision-making problems in CRS as a unified policy learning task. In order to systematically integrate conversation and recommendation components, we develop a dynamic weighted graph based RL method to learn a policy to select the action at each conversation turn, either asking an attribute or recommending items. Further, to deal with the sample efficiency issue, we propose two action selection strategies for reducing the candidate action space according to the preference and entropy information. Experimental results on two benchmark CRS datasets and a real-world E-Commerce application show that the proposed method not only significantly outperforms state-of-the-art methods but also enhances the scalability and stability of CRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Information systems â†’ Users and interactive retrieval; Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Conversational recommender systems (CRS) aim to learn user's preferences and make recommendations through interactive conversations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref>. Since it has the natural advantage of explicitly acquiring user's preferences and revealing the reasons behind recommendation, CRS has become one of the trending research topics for recommender systems and is gaining increasing attention. Unlike traditional recommender systems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref> or interactive recommender systems (IRS) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b50">51]</ref>, which mainly focus on solving the problem of which items to recommend, there exists generally the other two core research questions for CRS <ref type="bibr" target="#b10">[11]</ref>, namely what questions to ask and when to ask or recommend. Recent works have demonstrated the importance of interactivity of asking clarifying questions in CRS <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref>. More importantly, deciding when to ask or recommend is the key to coordinating conversation and recommendation for developing an effective CRS <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Different problem settings of CRS have been proposed, either from the perspective of dialog systems, being a variation of taskoriented dialog <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b47">48]</ref>, or from the perspective of recommender systems, being an enhanced interactive recommender system <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26]</ref>. In this work, we study the multi-round conversational recommendation (MCR) setting <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>, where the system asks questions about users' preferences on certain attributes or recommends items multiple times, and the goal is to make successful recommendation with the minimum number of interactions.</p><p>In MCR scenario, the CRS is typically formulated as a multi-step decision-making process and solved by reinforcement learning (RL) methods for policy learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. As shown in Figure <ref type="figure" target="#fig_1">1</ref>(a), RL-based IRS is only required to learn the policy to decide which items to recommend. However, the situation is more complicated in CRS, since there are two components that need to be coherently considered <ref type="bibr" target="#b18">[19]</ref>, i.e., conversation and recommendation components. Existing methods CRM <ref type="bibr" target="#b25">[26]</ref> and EAR <ref type="bibr" target="#b11">[12]</ref> employ policy gradient <ref type="bibr" target="#b26">[27]</ref> to improve the strategies of when and what attributes to ask, while the recommendation decision is made by an external recommendation model. In order to reduce the action space in policy learning, another state-of-the-art method SCPR <ref type="bibr" target="#b13">[14]</ref> only considers learning the policy of when to ask or recommend, while two isolated components are responsible for the decision of what to ask and which to recommend. The policy learning frameworks of these two kinds of CRS are presented in Figure <ref type="figure" target="#fig_1">1</ref>(b) and 1(c).</p><p>Despite the effectiveness of these methods, there are some issues remained to be tackled for real-world applications: (i) The models trained with existing CRS methods lack generality to different Work done when Yang Deng was an intern at Alibaba. This work was supported by Alibaba Group through Alibaba Research Intern Program, and a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14200719).   <ref type="bibr" target="#b25">[26]</ref>, EAR <ref type="bibr" target="#b11">[12]</ref>, and SCPR <ref type="bibr" target="#b13">[14]</ref>.</p><p>domains or applications, since there are three different decisionmaking processes to be considered in CRS, including what attributes to ask, which items to recommend, and when to ask or recommend. It requires extra efforts to train an offline recommendation model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> or pretrain the policy network with synthetic dialogue history <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. (ii) The policy learning is hard to converge, since the conversation and recommendation components are isolated and lack of mutual influence during the training procedure.</p><p>To address these issues, in this work, we formulate the aforementioned three separated decision-making processes in CRS as a unified policy learning problem to harness the ultimate goal of CRS as well as fill the gap between the recommendation and conversation components during the training procedure. Such unified conversational recommendation policy learning (UCRPL) aims at learning a unified policy to decide the action, either ask an attribute or recommend items, at each conversation turn to maximize the cumulative utility over the whole MCR process. The overview of the proposed unified policy learning for CRS is depicted in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>However, the UCRPL problem comes along with two challenges: (i) How to systematically combine conversation and recommendation components for unified policy learning? (ii) How to deal with sample efficiency issues? As the action space becomes overwhelmingly large in UCRPL, including all available attributes and items, it requires tremendous amount of interaction data to learn the optimal policy. Fortunately, the graph structure, capturing the rich correlated information among different types of nodes (i.e., users, items, and attributes), enables us to discover collaborative user preferences towards attributes and items. Thus, we can leverage the graph structure to integrate the recommendation and conversation components as an organic whole, where the conversation session can be regarded as a sequence of nodes maintained in the graph to dynamically exploit the conversation history for predicting the action at the next turn. On the other hand, although the connectivity of the graph can also be used to eliminate invalid actions by path reasoning <ref type="bibr" target="#b13">[14]</ref>, there are still a large number of candidates left for action searching. Since users are not likely to be interested in all items and attributes, we can focus on the potentially important candidates to improve the sample efficiency of UCRPL.</p><p>To this end, we propose a novel and adaptive graph-based reinforcement learning framework, namely UNIfied COnversational RecommeNder (UNICORN). Specifically, due to the evolving nature of CRS, we leverage a dynamic weighted graph to model the changing interrelationships among users, items and attributes during the conversation, and consider a graph-based Markov Decision Process (MDP) environment for simultaneously handling the  In summary, the contributions of this paper are as follows:</p><p>â€¢ We formulate three separated decision-making processes in conversational recommender systems, namely when to ask or recommend, what to ask, and which to recommend, as a unified conversational recommendation policy learning problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Conversational Recommendation. Based on the problem settings, current CRS studies can be categorized into four directions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>: (1) Exploration-Exploitation Trade-offs for Cold-start Users <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40]</ref>. These approaches leverage bandit approaches to balance the exploration and exploitation trade-offs for cold-start users in conversational recommendation scenarios. (2) Question-driven Approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref> aim at asking questions to users to get more information about their preferences, which is often addressed as "asking clarification/clarifying question". (3) Dialogue Understanding and Generation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. These studies focus on how to understand users' preferences and intentions from their utterances and generate fluent responses so as to deliver natural and effective dialogue actions. (4) Multi-round Conversational Recommendation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. Under this problem setting, the system asks questions about the user's preferences or makes recommendations multiple times, with the goal of achieving engaging and successful recommendations with fewer turns of conversations. Among these problem settings, we focus on the MCR problem. RL in Recommendation. Reinforcement learning (RL) has been widely introduced into recommender systems due to its advantage of considering users' long-term feedbacks <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. RL-based recommendation formulates the recommendation procedure as an MDP of the interactions between the user and a recommendation agent, and employs RL algorithms to learn the optimal recommendation strategies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. Recent works on sequential recommendation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref> and interactive recommendation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref> adopt RL to capture users' dynamic preferences for generating accurate recommendations over time. The goal of these approaches typically is to learn an effective policy for determining which items to recommend. As for CRS, RL-based methods are adopted to improve the strategies of the other two decision processes, including (i) what attributes to ask <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref> and (ii) when to ask or recommend <ref type="bibr" target="#b13">[14]</ref>.</p><p>In order to simplify the overall framework of MCR with better scalability and generality, we formulate these three core decision processes in CRS as a unified policy learning problem.</p><p>Graph-based Recommendation. Graph-based recommendation studies mainly leverage the graph structure for two purposes. The first one is to enhance the recommendation performance by graphbased representation learning, including exploiting the structure information for collaborative filtering <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46]</ref>, and adopting knowledge graph embeddings as rich context information <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>The other group of studies models recommendation as a path reasoning problem for building explainable recommender systems <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Recent years have witnessed many successful applications of graphbased RL methods on different scenarios of recommender systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49]</ref>. For example, Xian et al. <ref type="bibr" target="#b35">[36]</ref> employ a policy-guided graph search method to sample reasoning paths for recommendation, which is enhanced with adversarial actor-critic for demonstration-guided path reasoning <ref type="bibr" target="#b41">[42]</ref>. Lei et al. <ref type="bibr" target="#b14">[15]</ref> and Zhou et al. <ref type="bibr" target="#b48">[49]</ref> employ graph convolutional network (GCN) <ref type="bibr" target="#b9">[10]</ref> for state representation learning to enhance the performance of traditional RL methods on recommendation policy learning. In this work, we study graph-based RL method for the conversational recommendation scenario based on a dynamic weighted graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>Multi-round Conversational Recommendation. In this work, we focus on the multi-round conversational recommendation (MCR) scenario <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, the most realistic conversational recommendation setting proposed so far, in which the CRS is able to ask questions about attributes or make recommendations multiple times. Specifically, on the system side, the CRS maintains a large set of items V to be recommended, and each item ğ‘£ is associated with a set of attributes P ğ‘£ . In each episode, a conversation session is initialized by a user ğ‘¢ specifying an attribute ğ‘ 0 . Then, the CRS is free to ask the user's preference on an attribute selected from the candidate attribute set P cand or recommend a certain number of items (e.g., top-ğ¾) from the candidate item set V cand . Following the assumptions from Lei et al. <ref type="bibr" target="#b11">[12]</ref>, the user preserves clear preferences towards all the attributes and items. Thus, the user will respond accordingly, either accepting or rejecting the asked attributes or the recommended items. The CRS updates the candidate attribute and item sets, and decides the next action based on the user response. The system-ask and user-respond process repeats until the CRS hits the target item or reaches the maximum number of turn ğ‘‡ . Unified Conversational Recommendation Policy Learning. Multi-round conversational recommendation aims to make successful recommendations within a multi-round conversation session with the user. At each timestep ğ‘¡, according to the observation on past interactions, the CRS selects an action ğ‘ ğ‘¡ , either asking an attribute or recommending items. In return, the user expresses his/her feedback (accept or reject). This process repeats until the CRS hits the user-preferred items or reaches the maximum number of turn ğ‘‡ . Such MCR task can be formulated as a Markov Decision Process (MDP). The goal of the CRS is to learn a policy ğœ‹ maximizing the expected cumulative rewards over the observed MCR episodes as</p><formula xml:id="formula_0">ğœ‹ * = arg max ğœ‹ âˆˆÎ  E âˆ‘ï¸ ğ‘‡ ğ‘¡ =0 ğ‘Ÿ (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) ,<label>(1)</label></formula><p>where ğ‘  ğ‘¡ is the state representation learned from the system state and the conversation history, ğ‘ ğ‘¡ is the action that the agent takes at timestep ğ‘¡, and ğ‘Ÿ (â€¢) is the intermediate reward, abbreviated as ğ‘Ÿ ğ‘¡ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>The overview of the proposed method, UNICORN, is depicted in Figure <ref type="figure" target="#fig_3">3</ref>, which consists of four main components: Graph-based MDP Environment, Graph-enhanced State Representation Learning, Action Selection Strategy, and Deep Q-Learning Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph-based MDP Environment</head><p>The MDP environment is responsible for informing the agent about the current state and possible actions to take, and then rewards the agent based on how the current policy fits the observed user interactions. Formally, the MDP environment can be defined by a tuple (S, A, T , R), where S denotes the state space, A denotes the action space, T : S Ã— A â†’ S refers to the state transition function, and R : S Ã— A â†’ R is the reward function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">State.</head><p>As for the graph-based MDP environment, the state ğ‘  ğ‘¡ âˆˆ S at timestep ğ‘¡ is supposed to contain all the given information for conversational recommendation, including the previous conversation history and the full graph G that includes all the users, items, and attributes. Given a user ğ‘¢, we consider two major elements:</p><formula xml:id="formula_1">ğ‘  ğ‘¡ = [H (ğ‘¡ ) ğ‘¢ , G (ğ‘¡ ) ğ‘¢ ],<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">H (ğ‘¡ ) ğ‘¢ = [P (ğ‘¡ ) ğ‘¢ , P (ğ‘¡ ) rej , V (ğ‘¡ )</formula><p>rej ] denotes the conversation history until timestep ğ‘¡, and G (ğ‘¡ ) ğ‘¢ denotes the dynamic subgraph of G for the user ğ‘¢ at timestep ğ‘¡ (The graph construction will be introduced in Section 4.2). P ğ‘¢ denotes the user-preferred attribute. P rej and V rej are the attributes and items rejected by the user, respectively. The initial state ğ‘  0 is initialized by the user-specified attribute ğ‘ 0 , i.e., cand to ask attributes. Following the path reasoning approach <ref type="bibr" target="#b13">[14]</ref>, we have</p><formula xml:id="formula_3">ğ‘  0 = [[{ğ‘ 0 }, {}, {}], G (0) ğ‘¢ ].</formula><formula xml:id="formula_4">V (ğ‘¡ ) cand = V P (ğ‘¡ ) ğ‘¢ \ V (ğ‘¡ ) rej , P (ğ‘¡ ) cand = P V (ğ‘¡ ) cand \ (P (ğ‘¡ ) ğ‘¢ âˆª P (ğ‘¡ ) rej ),<label>(3)</label></formula><p>where V P (ğ‘¡ ) ğ‘¢ is the set of item vertices directly connecting all P (ğ‘¡ ) ğ‘¢ (i.e., items satisfying all the preferred attributes), and P V (ğ‘¡ ) cand is the set of attribute vertices directly connecting to one of V (ğ‘¡ ) cand (i.e., attributes belonging to at least one of the candidate items).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.3</head><p>Transition. We consider that the current state ğ‘  ğ‘¡ will transition to the next state ğ‘  ğ‘¡ +1 when the user responds to the action ğ‘ ğ‘¡ . In specific, if CRS asks an attribute ğ‘ ğ‘¡ and the user accepts it, the next state ğ‘  ğ‘¡ +1 will be updated by P 4.1.4 Reward. Following previous MCR studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, our environment contains five kinds of rewards, namely, (1) ğ‘Ÿ rec_suc , a strongly positive reward when the user accepts the recommended items, (2) ğ‘Ÿ rec_fail , a negative reward when the user rejects the recommended items, (3) ğ‘Ÿ ask_suc , a slightly positive reward when the user accepts the asked attribute, (4) ğ‘Ÿ ask_fail , a negative reward when the user rejects the asked attribute, and (5) ğ‘Ÿ quit , a strongly negative reward when reaching the maximum number of turns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph-enhanced State Representation</head><p>As we formulate conversational recommendation as a unified policy learning problem over a graph-based MDP environment, it is required to encode both the conversational and graph structural information into the latent distributed representations. In order to make use of the interrelationships among users, items, and attributes, we first adopt graph-based pre-training methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref> to obtained node embeddings for all the nodes in the full graph G. </p><formula xml:id="formula_5">ğ‘¢ âˆª P (ğ‘¡ ) cand âˆª V (ğ‘¡ ) cand (4) ğ‘¨ (ğ‘¡ ) ğ‘–,ğ‘— = ï£± ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£³ ğ‘¤ (ğ‘¡ ) ğ‘£ , if ğ‘› ğ‘– = ğ‘¢, ğ‘› ğ‘— âˆˆ V 1, if ğ‘› ğ‘– âˆˆ V, ğ‘› ğ‘— âˆˆ P 0, otherwise<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">ğ‘¤ (ğ‘¡ )</formula><p>ğ‘£ is a scalar indicating the recommendation score of the item ğ‘£ in the current state. In order to incorporate the user preference as well as the correlation between the asked attributes and the items, such weight ğ‘¤ (ğ‘¡ ) ğ‘£ is calculated as</p><formula xml:id="formula_7">ğ‘¤ (ğ‘¡ ) ğ‘£ = ğœ ğ‘’ âŠ¤ ğ‘¢ ğ‘’ ğ‘£ + âˆ‘ï¸ ğ‘ âˆˆ P (ğ‘¡ ) ğ‘¢ ğ‘’ âŠ¤ ğ‘£ ğ‘’ ğ‘ âˆ’ âˆ‘ï¸ ğ‘ âˆˆ P (ğ‘¡ ) rej âˆ©P (ğ‘¡ ) ğ‘£ ğ‘’ âŠ¤ ğ‘£ ğ‘’ ğ‘ ,<label>(6)</label></formula><p>where ğœ (â€¢) denotes the sigmoid function, ğ‘’ ğ‘¢ , ğ‘’ ğ‘£ , and ğ‘’ ğ‘ are the embeddings of the user, item, and attribute, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.2</head><p>Graph-based Representation Learning. In order to comprehensively take advantage of the correlation information among the involved user, items, and attributes from the connectivity of the graph, we employ a graph convolutional network (GCN) <ref type="bibr" target="#b9">[10]</ref> to refine the node representations with structural and relational knowledge. The representations of the node ğ‘› ğ‘– in the (ğ‘™ + 1)-th layer can be computed by:</p><formula xml:id="formula_8">ğ‘’ (ğ‘™+1) ğ‘– = ReLU âˆ‘ï¸ ğ‘— âˆˆN ğ‘– ğš² ğ‘–,ğ‘— ğ‘¾ ğ‘™ ğ‘’ (ğ‘™) ğ‘— + ğ‘© ğ‘™ ğ‘’ (ğ‘™) ğ‘– ,<label>(7)</label></formula><p>where N ğ‘– denotes the neighboring indices of the node ğ‘› ğ‘– , ğ‘¾ ğ‘™ and ğ‘© ğ‘™ are trainable parameters representing the transformation from neighboring nodes and the node ğ‘› ğ‘– itself, and ğš² is a normalization adjacent matrix as ğš² = ğ‘« âˆ’ 1 2 ğ‘¨ğ‘« âˆ’ 1 2 with ğ‘« ğ‘–ğ‘– = ğ‘— ğ‘¨ ğ‘–,ğ‘— .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Sequential Representation</head><p>Learning. Apart from the interrelationships among the involved user, items, and attributes, the CRS is also expected to model the conversation history in the current state. Unlike previous studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> that adopt heuristic features for conversation history modeling, we employ Transformer encoder <ref type="bibr" target="#b28">[29]</ref> for capturing the sequential information of the conversation history as well as attending the important information for deciding the next action. As described in <ref type="bibr" target="#b28">[29]</ref>, each Transformer layer consists of three components: (i) The layer normalization is defined as LayerNorm(â€¢). (ii) The multi-head attention is defined as MultiHead(ğ‘¸, ğ‘², ğ‘½ ), where ğ‘¸, ğ‘², ğ‘½ are query, key, and value, respectively. (iii) The feed-forward network with ReLU activation is defined as FFN(â€¢). Take the ğ‘™-th layer for example:</p><formula xml:id="formula_9">ğ‘¿ * = MultiHead(ğ‘¿ (ğ‘™) , ğ‘¿ (ğ‘™) , ğ‘¿ (ğ‘™) ),<label>(8)</label></formula><formula xml:id="formula_10">ğ‘¿ (ğ‘™+1) = LayerNorm(FFN(ğ‘¿ * ) + ğ‘¿ (ğ‘™) ),<label>(9)</label></formula><p>where ğ‘¿ âˆˆ R ğ¿Ã—ğ‘‘ denotes the embeddings, and ğ¿ is the sequence length. In our case, the input sequence ğ‘¿ (0) is the accepted attributes</p><formula xml:id="formula_11">P (ğ‘¡ )</formula><p>ğ‘¢ in the current conversation history with the learned graph-based representation {ğ‘’ (ğ¿ ğ‘” ) ğ‘ : ğ‘ âˆˆ P (ğ‘¡ ) ğ‘¢ }, where ğ¿ ğ‘” is the number of layers in GCN. After the sequential learning with ğ¿ ğ‘  Transformer layers, we can aggregate the information learned from both the graph and the conversation history by a mean pooling layer to obtain the state representation of ğ‘  ğ‘¡ :</p><formula xml:id="formula_12">ğ‘“ ğœƒ ğ‘† (ğ‘  ğ‘¡ ) = MeanPool(ğ‘¿ ğ¿ ğ‘  ). (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>For simplicity, we denote the learned state representation of ğ‘  ğ‘¡ as ğ‘“ ğœƒ ğ‘† (ğ‘  ğ‘¡ ), where ğœƒ ğ‘† is the set of all network parameters for state representation learning, including GCN and Transformer layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Action Selection Strategy</head><p>A large action search space will harm the performance of the policy learning to a great extent <ref type="bibr" target="#b13">[14]</ref>. Thus, it attaches great importance to handle the overwhelmingly large action space in UCRPL. To this end, we propose two simple strategies to improve the sample efficiency for candidate action selection.</p><p>Preference-based Item Selection. In general, for candidate items to be recommended, we can consider only the action of making recommendations from a small number of candidate items that fit the user preference the most, since users are not likely to be interested in all items. To achieve this, we select top-ğ¾ ğ‘£ candidate items from V (ğ‘¡ ) cand into the candidate action space A ğ‘¡ at each timestep ğ‘¡, which is ranked by the recommendation score ğ‘¤ (ğ‘¡ ) ğ‘£ in Eq.( <ref type="formula" target="#formula_7">6</ref>). Weighted Entropy-based Attribute Selection. Whereas for candidate attributes to be asked, the expected one is supposed to be able to not only better eliminate the uncertainty of candidate items, but also encode the user preference. Inspired by <ref type="bibr" target="#b13">[14]</ref>, we adopt weighted entropy as the criteria to prune candidate attributes:</p><formula xml:id="formula_14">ğ‘¤ (ğ‘¡ ) ğ‘ = âˆ’prob(ğ‘ (ğ‘¡ ) ) â€¢ log(prob(ğ‘ (ğ‘¡ ) )),<label>(11)</label></formula><p>prob(ğ‘</p><formula xml:id="formula_15">(ğ‘¡ ) ) = âˆ‘ï¸ ğ‘£ âˆˆV (ğ‘¡ ) cand âˆ©V (ğ‘¡ ) ğ‘ ğ‘¤ (ğ‘¡ ) ğ‘£ âˆ‘ï¸ ğ‘£ âˆˆV (ğ‘¡ ) cand ğ‘¤ (ğ‘¡ ) ğ‘£ ,<label>(12)</label></formula><p>where V ğ‘ denotes the items that have the attribute ğ‘.  (DQN) <ref type="bibr" target="#b20">[21]</ref> to conduct the unified conversational recommendation policy learning. We further implement some techniques to enhance and stabilize the training of DQN. The training procedure of the Unified Conversational Recommender is presented in Algorithm 1.</p><formula xml:id="formula_16">ğ‘¢ = {ğ‘ 0 }, P (0) rej = { }, V (0) rej = { }, H (0) ğ‘¢ = [ P (0) ğ‘¢ , P (0) rej , V (0) rej ], ğ‘  0 = [H (0) ğ‘¢ , G<label>(0)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.1</head><p>Dueling Q-Network. Following the standard assumption that delayed rewards are discounted by a factor of ğ›¾ per timestep, we define the Q-value ğ‘„ (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) as the expected reward based on the state ğ‘  ğ‘¡ and the action ğ‘ ğ‘¡ . As shown in the rightmost part of Figure <ref type="figure" target="#fig_3">3</ref>, the dueling Q-network <ref type="bibr" target="#b33">[34]</ref> employs two deep neural networks to compute the value function ğ‘“ ğœƒ ğ‘‰ (â€¢) and advantage function ğ‘“ ğœƒ ğ´ (â€¢), respectively. Then the Q-function can be calculated by:</p><formula xml:id="formula_17">ğ‘„ (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) = ğ‘“ ğœƒ ğ‘‰ (ğ‘ ğ‘¡ ) + ğ‘“ ğœƒ ğ´ (ğ‘“ ğœƒ ğ‘† (ğ‘  ğ‘¡ ), ğ‘ ğ‘¡ ),<label>(13)</label></formula><p>where ğ‘“ ğœƒ ğ‘‰ (â€¢) and ğ‘“ ğœƒ ğ´ (â€¢) are two separate multi-layer perceptions with parameters ğœƒ ğ‘‰ and ğœƒ ğ´ , respectively, and let ğœƒ ğ‘„ = {ğœƒ ğ‘‰ , ğœƒ ğ´ }.</p><p>The optimal Q-function ğ‘„ * (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ), which has the maximum expected reward achievable by the optimal policy ğœ‹ * , follows the Bellman equation <ref type="bibr" target="#b0">[1]</ref> as: Then, the agent will receive the reward ğ‘Ÿ ğ‘¡ from the user's feedback. According to the feedback, the current state ğ‘  ğ‘¡ transitions to the next state ğ‘  ğ‘¡ +1 , and the candidate action space A ğ‘¡ +1 is updated accordingly. The experience (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ , ğ‘Ÿ ğ‘¡ , ğ‘  ğ‘¡ +1 , A ğ‘¡ +1 ) is then stored into the replay buffer D. To train DQN, we sample mini-batch of experiences from D, and minimize the following loss function:</p><formula xml:id="formula_18">ğ‘„ * (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) = E ğ‘  ğ‘¡ +1 ğ‘Ÿ ğ‘¡ + ğ›¾ max ğ‘ ğ‘¡ +1 âˆˆA ğ‘¡ +1 ğ‘„ * (ğ‘  ğ‘¡ +1 , ğ‘ ğ‘¡ +1 )|ğ‘  ğ‘¡ , ğ‘ ğ‘¡ . (<label>14</label></formula><formula xml:id="formula_19">L (ğœƒ ğ‘„ , ğœƒ ğ‘† ) = E (ğ‘  ğ‘¡ ,ğ‘ ğ‘¡ ,ğ‘Ÿ ğ‘¡ ,ğ‘  ğ‘¡ +1 ,A ğ‘¡ +1 )âˆ¼D (ğ‘¦ ğ‘¡ âˆ’ğ‘„ (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ; ğœƒ ğ‘„ , ğœƒ ğ‘† )) 2 , (<label>15</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">ğ‘¦ ğ‘¡ = ğ‘Ÿ ğ‘¡ + ğ›¾ max ğ‘ ğ‘¡ +1 âˆˆA ğ‘¡ +1 ğ‘„ (ğ‘  ğ‘¡ +1 , ğ‘ ğ‘¡ +1 ; ğœƒ ğ‘„ , ğœƒ ğ‘† ),<label>(16)</label></formula><p>where ğ‘¦ ğ‘¡ is the target value based on the currently optimal ğ‘„ * .</p><p>To alleviate the overestimation bias problem in conventional DQN, we adopt Double Q-learning <ref type="bibr" target="#b27">[28]</ref>, which employs a target network ğ‘„ â€² as a periodic copy from the online network. The target value of the online network is then changed to:</p><formula xml:id="formula_22">ğ‘¦ ğ‘¡ = ğ‘Ÿ ğ‘¡ + ğ›¾ğ‘„ â€² ğ‘  ğ‘¡ +1 , arg max ğ‘ ğ‘¡ +1 âˆˆA ğ‘¡ +1 ğ‘„ (ğ‘  ğ‘¡ +1 , ğ‘ ğ‘¡ +1 ; ğœƒ ğ‘„ , ğœƒ ğ‘† ); ğœƒ ğ‘„ â€² , ğœƒ ğ‘† , (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>where ğœƒ ğ‘„ â€² denotes the parameter of the target network, which is updated by the soft assignment as:</p><formula xml:id="formula_24">ğœƒ ğ‘„ â€² = ğœğœƒ ğ‘„ + (1 âˆ’ ğœ)ğœƒ ğ‘„ â€² , (<label>18</label></formula><formula xml:id="formula_25">)</formula><p>where ğœ is the update frequency.</p><p>In addition, the conventional DQN samples uniformly from the replay buffer. In order to sample more frequently those important transitions from which there is much to learn, we employ prioritized replay <ref type="bibr" target="#b23">[24]</ref> as a proxy for learning potential, which samples transitions with probability ğ›¿ relative to the absolute TD error:</p><formula xml:id="formula_26">ğ›¿ âˆ ğ‘Ÿ ğ‘¡ +1 + ğ›¾ğ‘„ â€² ğ‘  ğ‘¡ +1 , arg max ğ‘ ğ‘¡ +1 âˆˆA ğ‘¡ +1 ğ‘„ (ğ‘  ğ‘¡ +1 , ğ‘ ğ‘¡ +1 ) âˆ’ ğ‘„ (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) . (19)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Model Inference.</head><p>With the learned UNICORN model, given a user and his/her conversation history, we follow the same process to obtain the candidate action space and the current state representation, and then decide the next action according to max Q-value in Eq.( <ref type="formula" target="#formula_17">13</ref>). If the selected action points to an attribute, the system will ask the user's preference on the attribute. Otherwise, the system will recommend top-ğ¾ items with the highest Q-value to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT 5.1 Dataset</head><p>We evaluate the proposed method, UNICORN, on four existing multi-round conversational recommendation benchmark datasets, and further conduct evaluation on a real-world E-Commerce platform. The statistics of these datasets are presented in Table <ref type="table" target="#tab_5">1</ref>.</p><p>â€¢ LastFM and Yelp. The LastFM dataset is used for evaluation on music artist recommendation, while the Yelp dataset is for business recommendation. Lei et al. <ref type="bibr" target="#b11">[12]</ref> manually categorize the original attributes in LastFM into 33 coarse-grained groups, and build a 2-layer taxonomy with 29 first-layer categories for Yelp. â€¢ LastFM* and Yelp*. Lei et al. <ref type="bibr" target="#b13">[14]</ref> consider that it is not realistic to manually merge attributes for practical applications, so they adopt original attributes to reconstruct these two datasets. For a fair comparison, we adopt both versions for experiments. â€¢ E-Commerce. In order to evaluate the proposed method in realworld E-Commerce scenario, we collect the dataset by sampling logs from Taobao, the largest E-Commerce platform in China, in the same way as LastFM and Yelp. We randomly sample 30,000 items from the Women_Dress category and a certain number of users who have interacted with these items. And we treat the concatenation of the product property and the property value as the attribute to be asked, e.g., "Color=Red", where "Color" is a product property and "Red" is a property value. Following the MCR data construction described in Lei et al. <ref type="bibr" target="#b11">[12]</ref>, we obtain the E-Commerce dataset with the statistics shown in Table <ref type="table" target="#tab_5">1</ref>. As for graph construction, we consider two kinds of relations, including "purchased_by (itemâ†’ user)" and "belong_to (attributeâ†’ item)". Compared with the imbalanced ratio of #items:#attributes at LastFM (0.88:1) and Yelp (119:1), the E-Commerce dataset preserves a large number of items as well as attributes, which is more in line with real-world applications in E-Commerce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">User Simulator.</head><p>Due to the interactive nature of MCR, it needs to be trained and evaluated by interacting with users. Following the user simulator adopted in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>, we simulate a conversation session for each observed user-item interaction pair (ğ‘¢, ğ‘£).</p><p>We regard the item ğ‘£ as the ground-truth target item and treat its attribute set P ğ‘£ as the oracle set of attributes preferred by the user ğ‘¢ in this conversation session. The session is initialized by the simulated user who specifies a certain attribute randomly chosen from P ğ‘£ . Then the session follows the process of "System Ask, User</p><p>Respond" <ref type="bibr" target="#b40">[41]</ref> as described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.2</head><p>Baselines. We compare the proposed method with several state-of-the-art methods on MCR as follows:</p><p>â€¢ Max Entropy. This is a rule-based strategy to decide the next action, where the CRS selects an attribute to ask based on the maximum entropy within the current state, or chooses to recommend the top-ranked items with a certain probability <ref type="bibr" target="#b11">[12]</ref>. â€¢ Abs Greedy <ref type="bibr" target="#b3">[4]</ref>. This method only performs recommendation actions and updates itself until the CRS makes a successful recommendation or exceeds the maximum turns of conversation. â€¢ CRM <ref type="bibr" target="#b25">[26]</ref>. This method is originally designed for single-round conversational recommendation, which employs policy gradient <ref type="bibr" target="#b26">[27]</ref> to learn the policy deciding when and which attributes to ask. We follow <ref type="bibr" target="#b11">[12]</ref> to adapt this method into MCR scenario. â€¢ EAR <ref type="bibr" target="#b11">[12]</ref>. A three-stage method is proposed to enhance the interaction between the conversation and recommendation components with a similar RL framework as CRM.</p><p>â€¢ SCPR <ref type="bibr" target="#b13">[14]</ref>. This is the state-of-the-art method on MCR setting, which leverages interactive path reasoning on the graph to prune off candidate attributes and adopts the DQN <ref type="bibr" target="#b20">[21]</ref> framework to determine when to ask or recommend. Following previous studies on multiround conversational recommendation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, we adopt success rate at the turn ğ‘¡ (SR@ğ‘¡) <ref type="bibr" target="#b25">[26]</ref> to measure the cumulative ratio of successful conversational recommendation by the turn ğ‘¡. Besides, average turn (AT) is adopted to evaluate the average number of turns for all sessions (if the conversation session reaches the maximum turn ğ‘‡ , the turn for such session is also counted as ğ‘‡ ). The higher SR@ğ‘¡ indicates a better performance of the CRS at a turn ğ‘¡, while the lower AT means an overall higher efficiency.</p><p>Although SR@ğ‘¡ and AT are widely adopted for the evaluation of CRS, both of them are not sensitive to the rank order in the recommendation results and only consider a certain aspect of the performance of CRS. In order to conduct a comprehensive evaluation of CRS, we propose to extend the normalized discounted cumulative gain (NDCG@ğ¾) to be a two-level hierarchical version, namely hNDCG@(ğ‘», ğ‘² ), with the definition as follows:</p><formula xml:id="formula_27">â„ğ·ğ¶ğº@(ğ‘‡ , ğ¾) = âˆ‘ï¸ ğ‘‡ ğ‘¡ âˆ‘ï¸ ğ¾ ğ‘˜ ğ‘Ÿ (ğ‘¡, ğ‘˜) 1 log 2 (ğ‘¡ + 2) + 1 log 2 (ğ‘¡+1) âˆ’ 1 log 2 (ğ‘¡+2) 1 log 2 (ğ‘˜+1) ,<label>(20)</label></formula><p>where ğ‘‡ and ğ¾ represent the number of conversation turns and recommended items in each turn, ğ‘Ÿ (ğ‘¡, ğ‘˜) denotes the relevance of the result at the turn ğ‘¡ and position ğ‘˜. Then hNDCG@(ğ‘‡ , ğ¾) can be drawn from hDCG@(ğ‘‡ , ğ¾) with the same way as the original NDCG@ğ¾. Since there is only one target item for each session in MCR, we simply use hDCG@(ğ‘‡ , ğ¾) for evaluation. The intuition behind hNDCG@(ğ‘‡ , ğ¾) is that the less number of turns of a successful session is favorable for the CRS, while the target item is expected to be ranked higher in the recommendation list at the success turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Implementation Details.</head><p>Following <ref type="bibr" target="#b13">[14]</ref>, we split the E-Commerce dataset by 7:1.5:1.5 for training, validation, and testing, and set the size ğ¾ of the recommendation list as 10, the maximum turn ğ‘‡ as 15. We adopt TransE <ref type="bibr" target="#b1">[2]</ref> from OpenKE <ref type="bibr" target="#b5">[6]</ref> to pretrain the node embeddings in the constructed graph with the training set. We adopt the user simulator described in Section 5.2.1 to interact with the CRS for online training the model using the validation set. For all implemented methods, we conduct the online training for 10,000 episodes. To maintain a fair comparison with other baselines <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, we adopt the same reward settings to train the proposed model:  GCN layers ğ¿ ğ‘” and Transformer layers ğ¿ ğ‘  are set to be 2 and 1, respectively. The numbers of selected candidate attributes ğ¾ ğ‘ and items ğ¾ ğ‘£ are set to be 10. During the training procedure of DQN, the size of experience replay buffer is 50,000, and the size of mini-batch is 128. The learning rate and the ğ¿ 2 norm regularization are set to be 1e-4 and 1e-6, with Adam optimizer. The discount factor ğ›¾ and the update frequency ğœ are set to be 0.999 and 0.01.</p><formula xml:id="formula_28">ğ‘Ÿ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Overall</head><p>Table <ref type="table" target="#tab_6">2</ref> shows the performance comparison between the proposed method, UNICORN, and all baselines across five datasets. In general, UNICORN outperforms all the baselines by achieving a significantly higher success rate and less average turn, which is also comprehensively validated by the improvements on hDCG. As for the real-world E-Commerce dataset, SCPR outperforms EAR and CRM, whose performance is largely affected by the large action space in E-Commerce dataset. Despite the effectiveness of SCPR to handle the issue of large action space in CRS, its performance in such a real-world application is still restricted by the separation of its conversation and recommendation components. Our UNICORN not only enables the conversation and recommendation to be mutually enhanced during the training process, but also attains an effective sample efficiency with the proposed action selection strategies. This leads to a substantial margin  from these baselines, about 18% for SR@15, 2 turns for AT, and 30% for hDCG. Detailed analyses can be found as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Training Efficiency.</head><p>Figure <ref type="figure" target="#fig_7">4</ref> shows the test performance curves of different methods. Since Max Entropy and Abs Greedy are unsupervised methods, their curves are presented as two lines for comparisons. It can be clearly observed that UNICORN is trained more stably and requires fewer training episodes (i.e., interaction data) to achieve a better performance than other strong baselines. Among these baselines, the curve of SCPR is the most vibrant, since only considers the policy of when to ask or recommend, while the decisions of question-asking and recommendation are made by two separated components. As for EAR and CRM, due to the large action space in the last three datasets, there is no much performance increase during the online training process, even getting worse. These results demonstrate the efficiency and effectiveness of the proposed unified policy learning for CRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Comparison at Different Conversation Turns.</head><p>Besides SR@15, we also present the performance comparison of success rate at each turn (SR@ğ‘¡) in Figure <ref type="figure" target="#fig_8">5</ref>. In order to better observe the differences among different methods, we report the relative success rate compared with the state-of-the-art baseline SCPR. For example, the line of ğ‘¦ = 0 represents the curve of Success Rate* for SCPR against itself. There are several notable observations as follows:</p><p>(i) The proposed UNICORN substantially and consistently outperforms these baselines across all the datasets and almost each turn in the conversation session.</p><p>(ii) Due to the nature of greedy recommendation approach (Abs Greedy), it may successfully hit target items at the early stage of the conversation, leading to a relatively strong performance at the first few turns, but the performance falls quickly as the turn increases.</p><p>(iii) UNICORN achieves an outstanding performance in the middle stage of the conversation, where there are still a large number of candidate items and attributes to be pruned. This shows the strong scalability of UNICORN to effectively handle large candidate action space in different situations.</p><p>(iv) The performance of SCPR is getting closer to UNICORN at the latter stage of the conversation, as the candidate item and attribute set is getting smaller and the task becomes easier.</p><p>(v) EAR and CRM share similar performance as Abs Greedy in those datasets with a large candidate attribute set, i.e., Yelp* and E-Commerce, indicating their policy learning is merely working when encountering a large action space.  <ref type="table" target="#tab_7">3</ref> (row (c-d)) presents the results that we replace pretrained TransE embeddings with randomly initialized embeddings (row (c)) and pretrained embeddings from FM model described in Lei et al. <ref type="bibr" target="#b11">[12]</ref> (row (d)). We observe a substantial performance gap between UNI-CORN with randomly initialized embeddings and that with pretrained embeddings, either TransE or FM. This result shows that How about these skirts?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>No! Success@Turn 5 !  with limited interaction data. In addition, using TransE embeddings further improves the performance from FM embeddings to a great extent, which also validates the advantages of graph-enhanced RL.</p><formula xml:id="formula_29">| V cand | = 946 | V cand | = 647 | V cand | = 311 | V cand | = 647 | V cand | = 232 | V cand | = 1355 | V cand | = 1355 | V cand | = 1052 | V cand | = 1052 | V cand | = 1345 | V cand | =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Action Selection Strategies.</head><p>The last part in Table <ref type="table" target="#tab_7">3</ref> (row (e-h)) presents the results that we replace or discard the proposed action selection strategies. One alternative attribute selection strategy is to adopt the same strategy as the preference-based item selection by changing the object from items to attributes. Another one is to use the original maximum entropy function <ref type="bibr" target="#b11">[12]</ref>. The results (row (e,f)) show that the performance of UCRPL suffers a noticeable decrease when adopting the preference-based or entropy-based strategy, indicating that it is required to consider both the user preference and the capability of reducing candidate uncertainty when deciding the asked attribute. Without attribute selection, we observe that the impact on applications with small action space (e.g., LastFM and Yelp) is less than those with large action space (e.g., LastFM*, Yelp*, and E-Commerce). UNICORN is merely working without item selection, since there are no pretrained recommendation components in the framework and the preference-based item selection serves as an auxiliary item recall process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Parameter Sensitivity Analysis</head><p>The upper part in Table <ref type="table" target="#tab_8">4</ref> summarizes the experimental results (hDCG) by varying the number of selected candidate actions. Since similar conclusions can also be drawn on other datasets, we only report the results on LastFM* and E-Commerce datasets due to space limit. As for the number of selected attributes, it is likely to discard the important attributes when only selected the attribute with the highest weighted entropy (e.g., ğ¾ ğ‘ =1). However, within a certain training interaction period (10,000 episodes in our case), UNICORN generally achieves the best performance when only selecting a small number of candidate items for policy learning.</p><p>The results also demonstrate the necessity of pruning the available actions when there is a large action search space in UCRPL. The lower part shows the experimental results by varying the number of network layers. As for GCN, 1-hop aggregation (ğ¿ ğ‘” =1) is not enough for capturing all the useful information for graph representational learning. However, there is no much difference between 2-hop (ğ¿ ğ‘” =2) and 3-hop (ğ¿ ğ‘” =3). Besides, for Transformer, we can see that 1-layer (ğ¿ ğ‘  =1) is sufficient for a good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Qualitative Analysis</head><p>In order to intuitively study the difference between the proposed UNICORN and other state-of-the-art CRS methods, we randomly sample a real-world interaction from the E-Commerce dataset. The generated conversations by UNICORN, SCPR, EAR, and CRM with the user simulator are presented in Figure <ref type="figure" target="#fig_10">6</ref>. Facing the large candidate action space, CRM tends to only trigger the recommendation component to make recommendations, and EAR continuously asks the questions that are not preferred by the user. Despite the success of SCPR in predicting user-preferred attributes, the policy learning in SCPR only decides when to ask or recommend, based on the number of candidate items, which leads to some unnecessary or redundant question-asking turns. UNICORN systematically addresses these issues by making a comprehensive decision of the next action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this work, we formulate three separated decision-making processes in CRS, including when to ask or recommend, what to ask and which to recommend, as a unified policy learning problem. To tackle the unified conversational recommendation policy learning problem, we propose a novel and adaptive RL framework, which is based on a dynamic weighted graph. In addition, we further design two simple yet effective action selection strategies to handle the sample efficiency issue. Experimental results show that the proposed method significantly outperforms state-of-the-art CRS methods across four benchmark datasets and the real-world E-Commerce application with remarkable scalability and stability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustration of policy learning frameworks for IRS and CRS, including CRM<ref type="bibr" target="#b25">[26]</ref>, EAR<ref type="bibr" target="#b11">[12]</ref>, and SCPR<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of unified policy learning for CRS. decision-making of recommendation and conversation. Then we integrate graph-enhanced representation learning and sequential conversation modeling to capture dynamic user preferences towards items and attributes. In addition, two simple yet effective action selection strategies are designed to handle the sample efficiency issue. Instead of enumerating the whole candidate item and attribute set, we adopt preference-based item selection and weighted entropy-based attribute selection strategies to only consider potentially important actions.In summary, the contributions of this paper are as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of the proposed method, UNICORN, for unified conversational recommendation policy learning.4.1.2 Action. According to the state ğ‘  ğ‘¡ , the agent takes an action ğ‘ ğ‘¡ âˆˆ A, where ğ‘ ğ‘¡ can be selected from the candidate item set V (ğ‘¡ ) cand</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>ğ‘¢ âˆª ğ‘ ğ‘¡ . Conversely, if the user rejects the action ğ‘ ğ‘¡ , ğ‘  ğ‘¡ +1 will be updated by P (ğ‘¡ +1) rej = P (ğ‘¡ ) rej âˆªğ‘ ğ‘¡ or V (ğ‘¡ +1) rej = V (ğ‘¡ ) rej âˆª ğ‘ ğ‘¡ for ğ‘ ğ‘¡ âˆˆ P or ğ‘ ğ‘¡ âˆˆ V, respectively. As a result, the next state ğ‘  ğ‘¡ +1 will be [H (ğ‘¡ +1) ğ‘¢ , G (ğ‘¡ +1) ğ‘¢ ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 2 . 1</head><label>21</label><figDesc>Dynamic Weighted Graph Construction. As shown in Figure 3, we represent the current state of the graph-based MDP environment as a dynamic weighted graph. Formally, we denote an undirected weighted graph as G = (N, ğ‘¨), with the node ğ‘› ğ‘– âˆˆ N , the adjacency matrix element ğ‘¨ ğ‘–,ğ‘— denoting the weighted edges between nodes ğ‘› ğ‘– and ğ‘› ğ‘— . In our case, given the user ğ‘¢, we denote the dynamic graph at timestep ğ‘¡ as G (ğ‘¡ ) ğ‘¢ = (N (ğ‘¡ ) , ğ‘¨ (ğ‘¡ ) ): N (ğ‘¡ ) = {ğ‘¢} âˆª P (ğ‘¡ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>rec_suc =1, ğ‘Ÿ rec_fail =-0.1, ğ‘Ÿ ask_suc =0.01, ğ‘Ÿ ask_fail =-0.1, ğ‘Ÿ quit =-0.3. The hyper-parameters are empirically set as follows: The embedding size and the hidden size are set to be 64 and 100. The numbers of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Test Performance at Different Training Episodes.</figDesc><graphic url="image-21.png" coords="7,318.78,323.49,120.91,90.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparisons at Different Conversation Turns.</figDesc><graphic url="image-25.png" coords="8,55.10,334.38,120.91,90.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>5. 4 . 1</head><label>41</label><figDesc>Model Components. We first evaluate the model components for the state representation learning, including the graphbased representation learning and the sequential representation learning. The ablation studies are presented in the first part in Table 3 (row (a-b)). We observe that the performance on 4 out of 5 datasets suffers a larger decrease by discarding sequential learning (row (a)) than discarding graph-based learning (row (b)). We attribute this to two reasons: (i) The pretrained TransE embeddings have already encoded certain graph-based knowledge into the node representations. (ii) Since there are relatively larger number of attributes in these four datasets than LastFM, it is more important to model the interrelationships among diverse user-accepted attributes for capturing user preferences. 5.4.2 Pre-training Embeddings. The second part in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Sample conversations generated by UNICORN, SCPR, EAR, and CRM. Due to the space limit, we only show the conversation at the first five turns. |ğ‘‰ cand | denotes the number of candidate items at the current turn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Similar to item selection, we also select top-ğ¾ ğ‘ candidate attributes from P Initialize all parameters: ğœƒ ğ‘† , ğœƒ ğ‘„ , ğœƒ â€² ğ‘„ â† ğœƒ ğ‘„ ; 2 for episode = 1, 2, . . . , ğ‘ do User ğ‘¢ starts the conversation by specifying an attribute ğ‘ 0 ;</figDesc><table><row><cell></cell><cell></cell><cell>4</cell><cell>Update: P</cell><cell>(0)</cell></row><row><cell></cell><cell></cell><cell>(ğ‘¡ )</cell></row><row><cell>into A ğ‘¡ based on the weighted entropy score ğ‘¤</cell><cell>(ğ‘¡ ) ğ‘ .</cell><cell>cand</cell></row></table><note>4.4 Deep Q-Learning NetworkAfter obtaining the graph-enhanced state representation and the candidate action space, we introduce the deep Q-learning network Algorithm 1: Unified Conversational Recommender Input: {e ğ‘– } ğ‘–âˆˆN ; D; ğœ; ğœ–; ğ›¾ ; ğ¾; ğ‘‡ ; ğ¾ ğ‘£ ; ğ¾ ğ‘ ; Output: ğœƒ ğ‘† ; ğœƒ ğ‘„ ; 1 3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Get state representation ğ‘“ ğœƒ ğ‘† (ğ‘  ğ‘¡ ) via Eq.(10); Select an action ğ‘ ğ‘¡ by ğœ–-greedy w.r.t Eq.(13); Update the next state ğ‘  ğ‘¡ +1 = T (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ); Store (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ , ğ‘Ÿ ğ‘¡ , ğ‘  ğ‘¡ +1 , A ğ‘¡ +1 ) to buffer D; Sample mini-batch of (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ , ğ‘Ÿ ğ‘¡ , ğ‘  ğ‘¡ +1 , A ğ‘¡ +1 ) w.r.t Eq.(19); Compute the target value ğ‘¦ ğ‘¡ via Eq. (17); Update ğœƒ ğ‘† , ğœƒ ğ‘„ via SGD w.r.t the loss function Eq.(15);</figDesc><table><row><cell></cell><cell>ğ‘¢ ];</cell></row><row><cell>5</cell><cell>Get candidate action space A 0 via Action Selection;</cell></row><row><cell>6</cell><cell>for turn ğ‘¡ = 0, 1, . . . ,ğ‘‡ âˆ’ 1 do</cell></row><row><cell>7</cell><cell></cell></row><row><cell>8</cell><cell></cell></row><row><cell>9</cell><cell>Receive reward ğ‘Ÿ ğ‘¡ ;</cell></row><row><cell>11</cell><cell>Get A ğ‘¡ +1 via Action Selection;</cell></row><row><cell>13</cell><cell></cell></row><row><cell>14</cell><cell></cell></row><row><cell>15</cell><cell></cell></row><row><cell>16</cell><cell>Update ğœƒ â€² ğ‘„ via Eq.(18) ;</cell></row><row><cell>17</cell><cell>end</cell></row><row><cell>18 end</cell><cell></cell></row></table><note>10 12</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>) 4.4.2 Double Q-Learning with Prioritized Experience Replay. During each episode in the MCR process, at each timestep ğ‘¡, the CRS agent obtains the current state representation ğ‘“ ğœƒ ğ‘† (ğ‘  ğ‘¡ ) via the graph-enhanced state representational learning described in Section 4.2. Then the agent selects an action ğ‘ ğ‘¡ from the candidate action space A ğ‘¡ , which is obtained via the action selection strategies described in Section 4.3. Here we incorporate ğœ–-greedy method to balance the exploration and exploitation in action sampling (i.e.,</figDesc><table /><note>select a greedy action based on the max Q-value with probability 1 âˆ’ ğœ–, and a random action with probability ğœ–).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Summary statistics of datasets.</figDesc><table><row><cell></cell><cell cols="2">LastFM LastFM*</cell><cell>Yelp</cell><cell cols="2">Yelp* E-Com.</cell></row><row><cell>#Users</cell><cell>1,801</cell><cell>1,801</cell><cell>27,675</cell><cell>27,675</cell><cell>26,430</cell></row><row><cell>#Items</cell><cell>7,432</cell><cell>7,432</cell><cell>70,311</cell><cell>70,311</cell><cell>29,428</cell></row><row><cell cols="6">#Interactions 76,693 76,693 1,368,606 1,368,606 748,533</cell></row><row><cell>#Attributes</cell><cell>33</cell><cell>8,438</cell><cell>29</cell><cell>590</cell><cell>1,413</cell></row><row><cell>#Entities</cell><cell cols="2">9,266 17,671</cell><cell>98,605</cell><cell>98,576</cell><cell>57,271</cell></row><row><cell>#Relations</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>3</cell><cell>2</cell></row><row><cell>#Triplets</cell><cell cols="5">138,215 228,217 2,884,567 2,533,827 2,024,962</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Experimental Results. â€  indicates statistically significant improvement (ğ‘ &lt; 0.01) over all baselines. hDCG stands for hDCG@<ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b9">10)</ref>. SR and hDCG are the higher the better, while AT is the lower the better. AT hDCG SR@15 AT hDCG SR@15 AT hDCG SR@15 AT hDCG SR@15 AT hDCG Abs Greedy 0.222 13.48 0.073 0.635 8.66 0.267 0.264 12.57 0.145 0.189 13.43 0.089 0.273 12.19 0.138 Max Entropy 0.283 13.91 0.083 0.669 9.33 0.269 0.921 6.59 0.338 0.398 13.42 0.121 0.328 12.98 0.112 CRM [26] 0.325 13.75 0.092 0.580 10.79 0.224 0.923 6.25 0.353 0.177 13.69 0.070 0.294 12.11 0.146 EAR [12] 0.429 12.88 0.136 0.595 10.51 0.230 0.967 5.74 0.378 0.182 13.63 0.079 0.381 11.48 0.161 SCPR [14] 0.465 12.86 0.139 0.709 8.43 0.317 0.973 5.67 0.382 0.489 12.62 0.159 0.518 12.32 0.168 UNICORN 0.535 â€  11.82 â€  0.175 â€  0.788 â€  7.58 â€  0.349 â€  0.985 â€  5.33 â€  0.397 â€  0.520 â€  11.31 â€  0.203 â€  0.602 â€  10.45 â€  0.217 â€  5.2.3 Evaluation Metrics.</figDesc><table><row><cell>LastFM</cell><cell>LastFM*</cell><cell>Yelp</cell><cell>Yelp*</cell><cell>E-Commerce</cell></row><row><cell>SR@15</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study. SR and hDCG are the higher the better, while AT is the lower the better. Preference-based Attr. Sel. 0.467 12.33 0.129 0.712 8.24 0.303 0.978 5.74 0.385 0.489 11.60 0.171 0.569 11.06 0.206 (f) -Entropy-based Attr. Sel. 0.507 11.96 0.165 0.756 7.63 0.324 0.978 5.71 0.388 0.502 11.33 0.192 0.585 10.69 0.214 (g) -w/o Attribute Selection 0.487 12.12 0.151 0.604 9.88 0.258 0.942 6.80 0.357 0.220 12.97 0.067 0.434 12.01 0.127 (h) -w/o Item Selection 0.150 13.83 0.055 0.638 8.85 0.294 0.780 9.16 0.276 0.158 13.60 0.054 0.144 13.85 0.056</figDesc><table><row><cell></cell><cell>LastFM</cell><cell>LastFM*</cell><cell>Yelp</cell><cell>Yelp*</cell><cell>E-Commerce</cell></row><row><cell></cell><cell cols="5">SR@15 AT hDCG SR@15 AT hDCG SR@15 AT hDCG SR@15 AT hDCG SR@15 AT hDCG</cell></row><row><cell>UNICORN</cell><cell cols="5">0.535 11.82 0.175 0.788 7.58 0.349 0.985 5.33 0.397 0.520 11.31 0.203 0.602 10.45 0.217</cell></row><row><cell>(a) -w/o Transformer</cell><cell cols="5">0.478 12.17 0.147 0.733 7.78 0.320 0.958 6.18 0.370 0.470 11.62 0.167 0.545 11.23 0.178</cell></row><row><cell>(b) -w/o GCN</cell><cell cols="5">0.440 12.74 0.139 0.744 7.62 0.327 0.978 5.48 0.394 0.481 11.45 0.171 0.554 11.07 0.183</cell></row><row><cell>(c) -Random Embeddings</cell><cell cols="5">0.310 13.31 0.096 0.665 9.24 0.291 0.905 7.66 0.318 0.196 13.95 0.049 0.220 13.55 0.069</cell></row><row><cell>(d) -FM Embeddings</cell><cell cols="5">0.465 12.26 0.144 0.748 7.68 0.328 0.982 5.50 0.394 0.496 12.25 0.161 0.579 10.89 0.202</cell></row><row><cell>(e) -</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The effect of hyper-parameters.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>LastFM*</cell><cell></cell><cell></cell><cell>E-Commerce</cell><cell></cell></row><row><cell>ğ¾ ğ‘£</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>10</cell><cell>20</cell><cell>50</cell></row><row><cell>ğ¾ ğ‘ =1</cell><cell cols="6">0.333 0.321 0.306 0.202 0.194 0.186</cell></row><row><cell cols="7">ğ¾ ğ‘ =10 0.349 0.305 0.297 0.217 0.205 0.189</cell></row><row><cell cols="7">ğ¾ ğ‘ =20 0.298 0.286 0.254 0.202 0.199 0.178</cell></row><row><cell>ğ¿ ğ‘”</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>ğ¿ ğ‘  =1</cell><cell cols="6">0.320 0.349 0.341 0.202 0.217 0.212</cell></row><row><cell>ğ¿ ğ‘  =2</cell><cell cols="6">0.324 0.346 0.332 0.182 0.221 0.216</cell></row><row><cell>ğ¿ ğ‘  =3</cell><cell cols="6">0.315 0.340 0.339 0.189 0.208 0.196</cell></row><row><cell cols="7">pretraining is necessary and simply online training with interactive</cell></row><row><cell cols="7">conversations is far from efficient and sufficient for training a CRS</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the role of dynamic programming in statistical communication theory</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kalaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="197" to="203" />
			<date type="published" when="1957">1957. 1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>GarcÃ­a-DurÃ¡n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Q&amp;R: A Two-Stage Approach toward Interactive Recommendation</title>
		<author>
			<persName><forename type="first">Konstantina</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards Conversational Recommender Systems</title>
		<author>
			<persName><forename type="first">Konstantina</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Xiangnan He, Maarten de Rijke, and Tat-Seng Chua. 2021. Advances and Challenges in Conversational Recommender Systems: A Survey</title>
		<author>
			<persName><forename type="first">Chongming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<idno>CoRR abs/2101.09459</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">OpenKE: An Open Toolkit for Knowledge Embedding</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="139" to="144" />
		</imprint>
	</monogr>
	<note>EMNLP 2018: System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR 2018</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conversational Recommendation: Formulation, Methods, and Evaluation</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2425" to="2428" />
		</imprint>
	</monogr>
	<note>Maarten de Rijke, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimation-Action-Reflection: Towards Deep Interaction Between Conversational and Recommender Systems</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;20: The Thirteenth ACM International Conference on Web Search and Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="304" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1437" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive Path Reasoning on Graph for Conversational Recommendation</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reinforcement Learning based Recommendation with Graph Convolutional Q-network</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1757" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards Deep Conversational Recommendations</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="page" from="9748" to="9758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Shijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>arXiv:cs.IR/2005.12979</idno>
		<title level="m">Seamlessly Unifying Attributes and Items: Conversational Recommendation for Cold-Start Users</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards Conversational Recommendation over Multi-Type Dialogs</title>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1036" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bridging the Gap between Conversational Reasoning and Interactive Recommendation. CoRR abs</title>
		<author>
			<persName><forename type="first">Wenchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2020. 2010. 2020</date>
			<biblScope unit="page">10333</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jointly Learning Explainable Rules for Recommendation with Knowledge Graph</title>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woojeong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<meeting><address><addrLine>WWW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1210" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>and Demis Hassabis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Value-aware Recommendation based on Reinforcement Profit Maximization</title>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinru</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<meeting><address><addrLine>WWW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3123" to="3129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization Machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM 2010, The 10th IEEE International Conference on Data Mining</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prioritized Experience Replay</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno>ICLR 2016</idno>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An MDP-Based Recommender System</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1265" to="1295" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conversational Recommender System</title>
		<author>
			<persName><forename type="first">Yueming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR 2018</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Policy Gradient Methods for Reinforcement Learning with Function Approximation</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning with Double Q-Learning</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2094" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">KERL: A Knowledge-Guided Reinforcement Learning Model for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozhang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Explainable Reasoning over Knowledge Graphs for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5329" to="5336" />
		</imprint>
	</monogr>
	<note>Xiangnan He, Yixin Cao, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reinforced Negative Sampling over Knowledge Graph for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaokun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="99" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dueling Network Architectures for Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
				<meeting>the 33nd International Conference on Machine Learning, ICML 2016</meeting>
		<imprint>
			<date type="published" when="1995">2016. 1995-2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reinforcement Knowledge Graph Reasoning for Explainable Recommendation</title>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-Supervised Reinforcement Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="931" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Collaborative Knowledge Base Embedding for Recommender Systems</title>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019</date>
			<biblScope unit="page" from="15188" to="15198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conversational Contextual Bandit: Algorithm and Application</title>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C S</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="662" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards Conversational Search and Recommendation: System Ask, User Respond</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Kangzhi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuren</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">RecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms</title>
		<author>
			<persName><forename type="first">Shanlei</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wen</surname></persName>
		</author>
		<idno>CoRR abs/2011.01731</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1040" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DRN: A Deep Reinforcement Learning Framework for News Recommendation</title>
		<author>
			<persName><forename type="first">Guanjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW 2018</title>
				<meeting>the 2018 World Wide Web Conference on World Wide Web, WWW 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spectral collaborative filtering</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems</title>
				<meeting>the 12th ACM Conference on Recommender Systems<address><addrLine>RecSys</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1006" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards Topic-Guided Conversational Recommender System</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020</title>
				<meeting>the 28th International Conference on Computational Linguistics, COLING 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="4128" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Interactive Recommender System via Knowledge Graph-enhanced Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Sijin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards Question-based Recommender Systems</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pseudo Dyna-Q: A Reinforcement Learning Framework for Interactive Recommendation</title>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;20: The Thirteenth ACM International Conference on Web Search and Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="816" to="824" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
