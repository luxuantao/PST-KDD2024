<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-based semi-supervised learning with multiple labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-12-24">24 December 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MOE-Microsoft Key Laboratory of Multimedia Computing and Communication</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Internet Media Group</orgName>
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Internet Media Group</orgName>
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zengfu</forename><surname>Wang</surname></persName>
							<email>zfwang@ustc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
							<email>xshua@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Internet Media Group</orgName>
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-based semi-supervised learning with multiple labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-12-24">24 December 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">67B671DDB4A51CD2F131BD6F42D7F9C5</idno>
					<idno type="DOI">10.1016/j.jvcir.2008.11.009</idno>
					<note type="submission">Received 2 June 2008 Accepted 12 November 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Graph-based learning Multiple labels Semi-supervised learning Video concept detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional graph-based semi-supervised learning methods predominantly focus on single label problem. However, it is more popular in real-world applications that an example is associated with multiple labels simultaneously. In this paper, we propose a novel graph-based learning framework in the setting of semi-supervised learning with multiple labels. This framework is characterized by simultaneously exploiting the inherent correlations among multiple labels and the label consistency over the graph. Based on the proposed framework, we further develop two novel graph-based algorithms. We apply the proposed methods to video concept detection over TRECVID 2006 corpus and report superior performance compared to the state-of-the-art graph-based approaches and the representative semi-supervised multi-label learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In real-world applications of machine learning, a large amount of unlabeled data are available, while it is costly to obtain labeled data since human labeling is a labor-intensive and time-consuming process. For example, in video concept detection, one may have an easy access to a large database of videos, but a small part of them is manually annotated.</p><p>Consequently, semi-supervised learning, which attempts to leverage the unlabeled data in addition to labeled data, has attracted much attention. Many different semi-supervised learning techniques have been proposed. These methods include, among others, EM with generative mixture models <ref type="bibr" target="#b21">[22]</ref>, co-training <ref type="bibr" target="#b18">[19]</ref>, self-training <ref type="bibr" target="#b20">[21]</ref>, and transductive support vector machines <ref type="bibr" target="#b19">[20]</ref>. Extensive review can be found in <ref type="bibr" target="#b1">[2]</ref>.</p><p>In recent years, the most active area of research in semi-supervised learning has been in graph-based semi-supervised learning methods <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, which model the whole data set as a graph such that the nodes correspond to labeled and unlabeled data points, and the edges reflect the similarities between data points. Almost all the graph-based methods essentially estimate a function on the graph such that it has two properties: <ref type="bibr" target="#b0">(1)</ref> it should be close to the given labels on the labeled examples, and (2) it should be smooth on the whole graph. This can be expressed in a regularization framework where the first term is a loss function to penalize the deviation from the given labels, and the second term is a regularizer to prefer the label smoothness. The typical graph-based methods are similar to each other, and differ slightly in the loss function and the regularizer.</p><p>Although the graph-based approaches have been proved effective, they mainly limit in dealing with single label problems. However, many real-world tasks are naturally posed as multi-label problems, where an example can be associated with multiple labels simultaneously. For example, in video concept detection, a video clip can be annotated with multiple labels at the same time, such as ''sky," ''mountain," and ''water" (see Fig. <ref type="figure" target="#fig_0">1</ref>). Similarly, in text categorization, a document can be classified into multiple categories, e.g., a document can belong to ''novel," ''Jules Verne's writing," and ''books on travelling." A direct way to tackle the typical graph-based learning under multi-label setting is to translate it into a set of independent single label problems <ref type="bibr" target="#b5">[6]</ref>. That is to say, the multi-label problems need to be implemented label-by-label independently. The drawback with this strategy is that it does not take into account the inherent correlations among multiple labels. However, the labels are usually interacting with each other naturally. For example, ''mountain" and ''sky" tend to appear simultaneously, while ''sky" typically does not appear with ''indoor." The value of label correlations has been proven in various application fields <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">26]</ref>.</p><p>To address the above issue, in this paper, we propose a novel graph-based semi-supervised learning framework, which can simultaneously explore the correlations among multiple labels and the label consistency over the graph. The vector-valued function estimated on the graph has three properties: <ref type="bibr" target="#b0">(1)</ref> it should be close to the given labels, (2) it should be smooth on the whole graph, and (3) it should be consistent with the label correlations. Specifically, the framework employs two types of regularizers. One is used to prefer the label smoothness on the graph, the other is adopted to address that the multi-label assignments for each example should be consistent with the inherent label correlations.</p><p>Based on this framework, we develop two novel graph-based learning algorithms, multi-label Gaussian random field (ML-GRF) and multi-label local and global consistency (ML-LGC) algorithm, which are the extensions of two existing graph-based methods: the Gaussian random field and Harmonic function (GRF) <ref type="bibr" target="#b3">[4]</ref> method and the local and global consistency (LGC) method <ref type="bibr" target="#b2">[3]</ref>, respectively. The proposed algorithms are shown to be able to make use of both labeled and unlabeled data, and the correlations among labels. We apply these two algorithms to video concept detection and conduct experiments over TRECVID 2006 development set <ref type="bibr" target="#b14">[15]</ref>. We report the superior performance compared to key existing graph-based learning approaches and semi-supervised multi-label learning methods.</p><p>To summarize, the main contributions of this work include:</p><p>It provides a novel graph-based learning framework to address the multi-label problems, which is an extension of the existing graph-based framework. It develops two graph-based algorithms (i.e., ML-GRF and ML-LGC) under the proposed framework. Compared to the typical graph-based methods, ML-GRF and ML-LGC can simultaneously exploit both the labeled and unlabeled data, and the correlations among multiple labels.</p><p>We have performed extensive experimental comparisons of the proposed algorithms, the representative graph-based approaches, and existing semi-supervised multi-label algorithms.</p><p>Compared to our preliminary work <ref type="bibr" target="#b12">[13]</ref>, in this paper, we comprehensively analyze the multiple label phenomena and provide deeper evaluation. The rest of this paper is organized as follows. We firstly give a brief summary of related work on semi-supervised learning and multi-label learning in Section 2. Section 3 mathematically introduces the existing graph-based learning framework. Section 4 provides the detailed description of the proposed framework and algorithms. Experimental results on TREC-VID 2006 data set are reported in Section 5 followed by concluding remarks in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-supervised learning</head><p>In this section, we only review the intimately related work, i.e., graph-based semi-supervised learning methods. More discussion of semi-supervised learning can be found in <ref type="bibr" target="#b1">[2]</ref>.</p><p>The graph-based learning strategy models the whole data set as a graph G ¼ ðV; EÞ. V is the vertex set, which is equivalent to the data set, and E is the set of edges. A nonnegative weight w ij is associated with each edge e ij and w ij reflects how similar datum i is to datum j. Many graph-based methods are based on the common assumption that the labels are smooth on the graph. They aim at estimating a function f over the graph such that it simultaneously satisfies two properties: (1) it should be close to the given labels on the labeled nodes, and (2) it should be smooth on the whole graph. This can be formulated in a regularization framework where the first term is a loss function, and the second term is a regularizer. The graph-based methods are similar to each other, and differ slightly in the definition of the loss function and the regularizer. For example, Zhu et al. <ref type="bibr" target="#b3">[4]</ref> proposed a Gaussian random field and Harmonic function (GRF) method. They defined a quadratic loss function with infinity weight to clamp the labeled examples, and formulated the regularizer based on the graph combinatorial Laplacian. Belkin et al. <ref type="bibr" target="#b4">[5]</ref> mentioned that p-Laplacian can be used as a regularizer. In <ref type="bibr" target="#b2">[3]</ref>, Zhou et al. presented the Local and global consistency (LGC) method. They also defined a quadratic loss function and used the normalized combinatorial Laplacian in the regularizer. Since the graph is at the heart of graph-based methods, there also exist some works related to the issue of graph construction. For example, the widely used graph is K-nearest-neighbor (K-NN) graph, where each node is connected to its K nearest neighbors under some distance measure and the edges can be weighted by a Gaussian function w ij ¼ expðÀkx i À x j k 2 =r 2 Þ, or unweighted ðw ij ¼ 1Þ. Recently, Tang et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and Wang et al. <ref type="bibr" target="#b11">[12]</ref> proposed structure-sensitive graph-based methods. They addressed that the similarities among the samples are not merely related to distances but also related to the structures around the samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-label learning</head><p>Multi-label learning problems widely exist in real-world applications, such as text classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b27">29]</ref>, functional genomic <ref type="bibr" target="#b23">[25]</ref>, image classification <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b24">26]</ref>, and video concept detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">31]</ref>.</p><p>The typical solution of multi-label learning is to translate the multi-label learning task into a set of single-label problems. For example, Boutell et al. <ref type="bibr" target="#b26">[28]</ref> solved the multi-label scene classification problem by building individual classifier for each label. The labels of a new sample are determined by the outputs of these individual classifiers. Such solution treats the labels in isolation and ignores the correlations among the labels. To exploit label correlations, some researchers have proposed fusion-based methods <ref type="bibr" target="#b25">[27]</ref>. Godbole and Sarawagi <ref type="bibr" target="#b25">[27]</ref> proposed to leverage the correlations by adding a contextual fusion step based on the outputs of the individual classifiers. More sophisticated multi-label learning approaches model labels and correlations between labels simultaneously <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b8">9]</ref>. Liu et al. <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b13">14]</ref> proposed a multi-label learning method based on constrained nonnegative matrix factorization. Kang et al. <ref type="bibr" target="#b24">[26]</ref> developed a Correlation Label Propagation (CLP) approach to explicitly capture the interactions between labels. Rather than treating labels independently, CLP simultaneously co-propagates multiple labels from training examples to testing examples. More recently, Qi et al. <ref type="bibr" target="#b8">[9]</ref> proposed a unified Correlative Multi-Label (CML) Support Vector Machine (SVM) to simultaneously classify labels and model their correlations in a new feature space which encodes both the label models and their interactions together. Chen and Hauptmann <ref type="bibr" target="#b29">[31]</ref> proposed multiconcept discriminative random field (MDRF) to automatically identify concept correlations and learn concept classifiers simultaneously. These two methods are similar in spirit as they modify the regularization term of support vector machines (in <ref type="bibr" target="#b8">[9]</ref>) or logistic regression (in <ref type="bibr" target="#b29">[31]</ref>) to accommodate the correlations between different concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph-based semi-supervised learning with single label</head><p>As aforementioned, most graph-based methods are based on the common assumption that the labels are smooth on the graph. Then, they essentially estimate a function over the graph such that it satisfies two conditions: (1) it should be close to the given labels, and (2) it should be smooth on the whole graph. Generally, these two conditions are presented in a regularization form.</p><p>Let X ¼ fx 1 ; x 2 ; . . . ; x N g be a set of N data points in R d . The first L points are labeled as y l ¼ ½y 1 ; y 2 ; . . . ; y L T with y i 2 f0; 1g ð1 6 i 6 LÞ, and the task is to label the remaining points fx Lþ1 ; . . . ; x N g. Denote the graph by G ¼ ðV; EÞ, where the node set V ¼ L [ U with L corresponding to fx 1 ; . . . ; x L g and U corresponding to fx Lþ1 ; . . . ; x N g. The edges E are weighted by the N Â N affinity matrix W with w ij indicating the similarity measure between x i and x j , and w ii is set to 0. Let f ¼ ½f 1 ; f 2 ; . . . ; f L ; f Lþ1 ; . . . ; f N T denote the predicted labels of X.</p><p>Mathematically, the graph-based methods aim to find an optimal f Ã essentially by minimizing the following objective function</p><formula xml:id="formula_0">EðfÞ ¼ E l ðfÞ þ E s ðfÞ;<label>ð1Þ</label></formula><p>where E l ðfÞ is a loss function to penalize the deviation from the given labels, and E s ðfÞ is a regularizer to prefer the label smoothness. For example, the Gaussian random field method <ref type="bibr" target="#b3">[4]</ref> formulates E l ðfÞ and E s ðfÞ as</p><formula xml:id="formula_1">E l ðfÞ ¼ 1 X i2L ðf i À y i Þ 2 ¼ ðf À yÞ T Kðf À yÞ; E s ðfÞ ¼ 1 2 X i;j2L[U w ij ðf i À f j Þ 2 ¼ f T Df;</formula><p>where K is a diagonal matrix with</p><formula xml:id="formula_2">K ii ¼ 1; i 6 L and K ii ¼ 0; i &gt; L:D ¼ D À W</formula><p>is the combinatorial graph Laplacian, D is a diagonal matrix with its ði; iÞ-element equal to the sum of the ith row of W.</p><p>In Local and Global Consistency method <ref type="bibr" target="#b2">[3]</ref>, a similar graphbased method is developed. Specifically, E l ðfÞ and E s ðfÞ are defined as</p><formula xml:id="formula_3">E l ðfÞ ¼ l X i2L[U ðf i À y i Þ 2 ¼ ðf À yÞ T ðf À yÞ; E s ðfÞ ¼ 1 2 X i;j2L[U w ij f i ffiffiffiffi d i p À f j ffiffiffiffi d j q 2 ¼ f T D À1=2 DD À1=2 f;</formula><p>where D À1=2 DD À1=2 is the normalized combinatorial Laplacian. The existing graph-based methods mainly address the semisupervised problem for single label scenario, and they are suboptimal for multi-label scenario, which is more challenging but much closer to real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Graph-based semi-supervised learning with multiple labels</head><p>In this section, we address the semi-supervised K-label problem. Define an N Â K label matrix Y ¼ ½y 1 ; y 2 ; . . . ; y N T , where y ik is 1 if x i is a labeled sample with its label k, and 0 otherwise. Define an N Â K matrix F ¼ ½f 1 ; f 2 ; . . . ; f N T , where f i ¼ ½f i1 ; f i2 ; . . . ; f iK T is a K-dimensional label vector. f ik is the confidence that x i is associated with label k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motivation</head><p>In multi-label scenario, it is observed that the labels assigned to a sample (e.g., a video clip in video concept detection task) are usually consistent with the inherent label correlations. For example, ''car," and ''road" are usually co-assigned to a certain video clip since they often appear simultaneously, while ''explosion fire" and ''waterscape" are generally not assigned to a sample at the same time since they usually do not co-occur.</p><p>Motivated by this observation, we propose a novel graph-based learning framework, such that the vector-valued function over the graph has three properties: (1) it should be close to the given labels, (2) it should be smooth on the whole graph, and (3) it should be consistent with the label correlations. The former two properties have been addressed in existing graph-based methods. The third one is novel and the key contribution of this paper. Recently, a similar semi-supervised learning framework was developed in a parallel work <ref type="bibr" target="#b0">[1]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, Chen et al. proposed to address the multilabel problems by extending the existing graph-based methods. Specifically, a category level graph was incorporated into the existing methods. The category level graph was defined on all the categories, where each node represents each category and the edge weight reflects the similarity between two categories. They defined a regularization term to address the smoothness of the labels of categories. It can be found that Chen's and our methods are similar in spirit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Framework</head><p>This unified regularization framework consists of three components: a loss function E l ðFÞ, and two types of regularizers E s ðFÞ and E c ðFÞ. Specifically, E l ðFÞ corresponds to the first property to penalize the deviation from the given multi-label assignments, E s ðFÞ is a regularizer to address the label smoothness, and E c ðFÞ is a regularizer to prefer the third property. Then, the proposed framework can be formulated to minimize</p><formula xml:id="formula_4">EðFÞ ¼ E l ðFÞ þ E s ðFÞ þ E c ðFÞ;<label>ð2Þ</label></formula><p>where E l ðFÞ and E s ðFÞ can be specified in a way similar to that adopted in existing graph-based methods. We make use of the correlations among multiple labels to define E c ðFÞ. To capture the label correlations, we introduce a K Â K symmetric matrix C with c ij represents the correlation between label i and label j. Then, given a label matrix F c ¼ ½f 1 ; f 2 ; . . . ; f n T on n data points with certain labeling, C 0 is calculated as</p><formula xml:id="formula_5">c 0 ij ¼ expðÀkf 0 i À f 0 j k 2 =2r 2 c Þ, where f 0 i is the ith column of F c , and</formula><formula xml:id="formula_6">r c ¼ Eðkf 0 i À f 0 j kÞ is the average distance. Then C is defined as C ¼ C 0 À D c</formula><p>, where D c is a diagonal matrix with the ði; iÞ-element equal to the sum of the ith row of C 0 . Let us define e i as f T i Cf i . Intuitively, e i reflects the coherence between the inherent correlation and the label vector f i assigned to x i . That is to say, the larger e i is, f i is more coherent with the label correlations. Consequently, we specify E c ðFÞ ¼ ÀtrðFCF T Þ to make the predicted multiple labels for each sample satisfy the correlations, where tr(M) is the trace of matrix M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Algorithm instances: ML-GRF and ML-LGC</head><p>The proposed framework provides us a powerful platform to design novel algorithms. Here we describe two algorithm instances developed under this framework.</p><p>Firstly, we define E l ðFÞ and E s ðFÞ as the same as those in GRF <ref type="bibr" target="#b3">[4]</ref>, i.e., E l ðFÞ ¼ tr ðF À YÞ T KðF À YÞ ;</p><p>E s ðFÞ ¼ trðF T DFÞ:</p><p>Eq. ( <ref type="formula" target="#formula_4">2</ref>) is then specifically formulated as the following:</p><formula xml:id="formula_7">EðFÞ ¼ trððF À YÞ T KðF À YÞÞ þ atrðF T DFÞ À btrðFCF T Þ;<label>ð3Þ</label></formula><p>where a and b are nonnegative constants which balance E s ðFÞ and E c ðFÞ. For the sake of simplicity, we name this algorithm instance as ML-GRF. We can also specify E l ðFÞ and E s ðFÞ as those adopted in LGC [3] method. Another graph-based algorithm can be obtained as min trððF À YÞ T ðF À YÞÞ þ ltrðF T LFÞ À mtrðFCF T Þ; ð4Þ where L is the normalized combination Laplacian D À1=2 DD À1=2 :l and m are nonnegative trade-off parameters. By minimizing Eq. ( <ref type="formula" target="#formula_7">3</ref>) or solving Eq. ( <ref type="formula">4</ref>), we can obtain the soft labels for unlabeled data. Next we will give the solution of ML-GRF and ML-LGC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Solution</head><p>We firstly describe the solution of ML-GRF. Differentiating EðFÞ with respect to F, we have</p><formula xml:id="formula_8">KðF À YÞ þ aDF À bFC ¼ 0:<label>ð5Þ</label></formula><formula xml:id="formula_9">Let F ¼ ½F T l F T u T</formula><p>, and represent the matrix W (and similarly D) in the form of block matrices:</p><formula xml:id="formula_10">W ¼ W ll W lu W ul W uu :<label>ð6Þ</label></formula><p>Then Eq. ( <ref type="formula" target="#formula_8">5</ref>) can be written as</p><formula xml:id="formula_11">P uu F u þ F u C ¼ S;<label>ð7Þ</label></formula><p>where P uu ¼ Àa=bðD uu À W uu Þ and S ¼ Àa=bW ul Y l . Eq. ( <ref type="formula" target="#formula_11">7</ref>) is essentially a Sylvester equation <ref type="bibr" target="#b15">[16]</ref>, which is widely used in control theory. It is well known that Eq. ( <ref type="formula" target="#formula_11">7</ref>) has a unique solution if and only if the eigenvalues g 1 ; g 2 ; . . . ; g NÀL of P uu and c 1 ; c 2 ; . . . ; c k of C satisfy g i þ g j -0 ði ¼ 1; 2; . . . ; N À LÞ; j ¼ ð1; 2; . . . ; KÞ <ref type="bibr" target="#b15">[16]</ref>. This condition can be easily satisfied in the real-world multi-label learning scenario.</p><p>Vectorizing the unknown matrix F u , Eq. ( <ref type="formula" target="#formula_11">7</ref>) can be transformed to a linear equation:</p><formula xml:id="formula_12">ðI p P uu þ C T I c ÞvecðF u Þ ¼ vecðSÞ;<label>ð8Þ</label></formula><p>where is the Kronecker product, I p and I c are K Â K and ðN À LÞÂ ðN À LÞ identity matrices, representatively. vec ðMÞ is the vectorization of the matrix M. We can then obtain F u from vecðF u Þ, which is equal to</p><formula xml:id="formula_13">ðI p P uu þ C T I c Þ þ vecðSÞ.</formula><p>Similarly, for ML-LGC, we can obtain Eq. ( <ref type="formula" target="#formula_14">9</ref>) by differentiating EðFÞ with respect to F.</p><formula xml:id="formula_14">ðlL þ IÞF À mFC ¼ Y:<label>ð9Þ</label></formula><p>This equation is also a Sylvester equation and the solution is similar to that of ML-GRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the proposed framework on a widely used benchmark video data set and compared it against two state-of-the-art graph-based methods and one existing semisupervised multi-label learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation issues</head><p>In many real-world applications, the labeled data are usually insufficient. Thus the correlation matrix C obtained from that limited data are unreliable. To tackle this difficulty, we propose an iterative solution. In each iteration t, the labels are predicted by solving Eq. ( <ref type="formula" target="#formula_11">7</ref>) (or Eq. ( <ref type="formula" target="#formula_14">9</ref>)) with C tÀ1 estimated at last iteration ðt À 1Þ. Specifically, we resort to an iterative Krylov-subspace approach <ref type="bibr" target="#b28">[30]</ref> to solve the Sylvester equation. Then, the data points labeled with high certainty are incorporated to re-estimated C t .</p><p>Suppose there are M data points with the predicted label matrix F p ¼ ½f 1 ; f 2 ; . . . ; f M T . For the jth label, we calculate its average confidence score over all the M samples as m j ¼ 1 M P M i¼1 f ij , which can be token as the pseudo-classification boundary of label j. The data points, which are far away from the pseudo-boundary, are regarded as the samples labeled with high certainty. Specifically, for the label vector f i associated with x i , we calculate the certainty as</p><formula xml:id="formula_15">gðf i Þ ¼ 1 K P K j¼1 expfÀf ij ð2m j À f ij Þg.</formula><p>The data points with large g are incorporated to re-estimate C t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Data set description</head><p>To evaluate the proposed graph-based framework, we conduct the experiments for subshot-level concept detection on the benchmark TRECVID 2006 development corpus <ref type="bibr" target="#b14">[15]</ref>. This data set contains about 170 h international broadcast news videos from 13 international TV programs in Arabic, English and Chinese. These news videos are automatically segmented into 61,901 subshots. For each subshot, 39 concepts are multi-labeled according to LSCOM-Lite annotations <ref type="bibr" target="#b16">[17]</ref>. These concepts were carefully chose such that they cover a variety of target types, including program category, setting/scene/site, people, object, activity, event, and graphics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance metric</head><p>For performance evaluation, we use the TRECVID performance metric: average precision (AP) <ref type="bibr" target="#b14">[15]</ref> to evaluate and compare the approaches on each concept. Through averaging the AP over all 39 concepts, we obtain the mean average precision (MAP), which is the overall evaluation result. Specifically, AP is proportional to the area under a recall-precision curve. To calculate AP for a certain concept, the test data are firstly ranked according to the prediction of each sample. Let N denote the sample number of the test set and P be the number of positive sample. At the index j, let P j denote the number of positive subshots in the top j subshots, and I j the indicator which indicates whether the jth subshot is positive. The AP of returned k subshots is then calculated as, AP at k ¼ 1 minðk; PÞ X minðk;NÞ j¼1 P j j Â I j :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experimental setup</head><p>We conduct experiments to compare the proposed approaches (i.e., ML-GRF and ML-LGC) against other two representative graph-based methods: GRF <ref type="bibr" target="#b3">[4]</ref> and LGC <ref type="bibr" target="#b2">[3]</ref>, and the semi-supervised multi-label learning method based on Constrained Non-negative Matrix Factorization (CNMF) proposed by Liu et al. <ref type="bibr" target="#b13">[14]</ref>.</p><p>The TRECVID 2006 development corpus is separated into four partitions, which are the same as those in <ref type="bibr" target="#b17">[18]</ref>. This strategy of data set separation is widely used for concept detection over TREC-VID 2006 corpus. Specifically, 90 videos (i.e., about 70% of the whole data set) are selected as training set, 16 videos (i.e., about 10%) as validation set, 16 videos (i.e., around 10%) as fusion set, and 15 videos (i.e., around 10%) are for performance evaluation <ref type="bibr" target="#b17">[18]</ref>. The low-level feature we use here is 225-dimensional block-wise color moment in Lab color space, which are extracted over 5 Â 5 fixed grids, each block is described by nine-dimensional features. We use the Gaussian kernel function to calculate the weighted matrix W for all four methods. For the sake of fair comparison, all the algorithmic parameters in all five approaches, such as the kernel bandwidth r and the trade-off parameters are determined through a validation process according to their performances on the validation set. The reported performance are from the best set of parameters in all the algorithms.</p><p>When implementing the graph-based methods, we need to calculate the inversion or the multiplication of the matrix W. Since the TRECVID 2006 development corpus contains 61,901 sub-shots, we need about 15 GB memory to represent the full similarity matrix. It is difficult to store this large-scale matrix and calculate its inversion. To address this issue, we simplify the graph by only connecting neighboring samples, and thus the matrix W is sparse. Here we adopt K-nearest-neighbor (K-NN) method, which finds the K (K = 200) nearest neighbors for each sample, to construct the sparse representation of W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Experimental result</head><p>Table <ref type="table">1</ref> shows the MAP of our two approaches, the LGC and GRF, and Table <ref type="table">2</ref> provides the detailed AP of these four approaches. The following observation can be obtained: By exploiting the label correlations, the proposed methods outperform the approaches which treat the semantic labels Table <ref type="table">1</ref> Comparison of MAP for the four approaches: LGC, ML-LGC, GRF, and ML-GRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>MAP Improvement (%)</p><p>LGC <ref type="bibr" target="#b2">[3]</ref> 0.307 -ML-LGC 0.329</p><formula xml:id="formula_16">+7.2 GRF [4] 0.325 - ML-GRF 0.346 +6.5</formula><p>Table <ref type="table">2</ref> Performance comparison between the proposed methods (i.e., ML-LGC and ML-GRF) and two respective graph-based approaches (i.e., LGC <ref type="bibr" target="#b2">[3]</ref> and GRF <ref type="bibr" target="#b3">[4]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP2000</head><p>LGC <ref type="bibr">[</ref>  separately and neglect their integration. In details, ML-LGC achieves around 7.2% improvements on MAP compared to LGC, while ML-GRF obtains about 6.5% improvements compared to GRF. ML-LGC outperforms LGC over 35 of all the 39 concepts. Some of the improvements are significant, such as the 52%, 49%, and 31% improvements on ''office," ''airplane," and ''waterscape," respectively. Compared to GRF, ML-GRF obtains improvements on 32 concepts, such as ''waterscape" (105%), ''office" (55%), and ''airplane" (40%).</p><p>The proposed methods degrade slightly on a few concepts. The main reason is that each of these concepts has weak interactions with other concepts. As a result, the presence/absence of these concepts cannot benefit from those of the others.</p><p>In summary, compared to the existing graph-based methods, the proposed approaches consistently outperform the performance on the diverse 39 concepts. We also compare the proposed methods against the existing semi-supervised multi-label learning method (CNMF) <ref type="bibr" target="#b13">[14]</ref>, which has been reported to outperform some other semi-supervised multi-label learning approaches <ref type="bibr" target="#b13">[14]</ref>. In CNMF, Liu et al. assumed that two data points tend to have large overlap in their assigned category memberships if they share high similarity in their input patterns. They firstly calculated two sets of similarities. The one was obtained based on the input patterns of data points, and the other was from the class memberships of the data points. Then, by minimizing the difference between these two sets of similarities, CNMF can determine the assignment of class memberships to the unlabeled data. We apply CNMF over TRECVID 2006 development set and it gets a MAP of 0.322 over the evaluation sub-set. Compared to CNMF, the proposed ML-GRF and ML-LGC methods obtain improvement of about 2.2% and 7.5%, respectively.</p><p>Here, we further analyze the effectiveness of tradeoff parameters in the proposed methods. In ML-GRF, there exist one parameter (i.e., b/a) should be tuned. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the MAP of ML-GRF when varying the value of b/a. The larger b/a is, the predicted labels on unlabeled samples are more consistent with the inherent label correlations, while the smaller it is, the predicted labels are more smooth over the sample graph. If we set b ¼ 0 and a ¼ 1, the ML-GRF method reduces to the GRF method. Fig. <ref type="figure" target="#fig_2">3</ref> shows the MAP of ML-LGC with respect to the tradeoff parameters l and m. We evaluate the influence of parameter l by fixing m, while l is fixed when evaluating m. It can be found that LGC is a special case of ML-LGC method, when m = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have proposed a novel graph-based semi-supervised learning framework to address the multi-label problems, which simul-taneously takes into account both the correlations among multiple labels and the label consistency over the graph. In addition to the label smoothness, the proposed framework also addresses that the multi-label assignment to each sample should be consistent with the inherent label correlations. Furthermore, two new graph-based algorithms, which are the extension of the existing graph-based methods, were developed under the proposed framework. Experiments on the benchmark TRECVID data set demonstrated that the novel framework is superior to key existing graph-based methods and semi-supervised multi-label learning approaches, in both overall performance and the consistency of performance on diverse concepts.</p><p>We will conduct some future works from the following perspectives. First, we will apply our approaches on different data sets. Next, we will generalize it for incremental learning. Furthermore, we will combine other techniques for the multi-labeling consistency into the proposed framework. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample video clips associated with multiple labels.</figDesc><graphic coords="2,106.41,67.92,370.81,104.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Performance of ML-GRF when varying the value of the tradeoff parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance of ML-LGC when varying the value of the tradeoff parameters.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Z.-J. Zha et al. / J. Vis. Commun. Image R. 20 (2009) 97-103</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-label learning by solving a Sylvester equation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining (SDM)</title>
		<meeting><address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Computer Sciences, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 1530</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning (ICML)</title>
		<meeting>the Twentieth International Conference on Machine Learning (ICML)<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Regularization and semi-supervised learning on large graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Computational Learning Theory (COLT)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3120</biblScope>
			<biblScope unit="page" from="624" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Msra-ustc-sjtu at trecvid 2007: high-level feature extraction and search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>TREC Video Retrieval Evaluation Online Proceedings</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collective multi-label classification in</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ghamrawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the ACM International Conference on Information and Knowledge Management (CIKM)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building a comprehensive ontology to refine video concept detection</title>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Multimedia Information Retrieval</title>
		<meeting>the International Workshop on Multimedia Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Correlative multi-label video annotation</title>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (MM)</title>
		<meeting>the ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure-sensitive manifold ranking for video concept detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (MM)</title>
		<meeting>the ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="852" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video Annotation Based on Kernel Linear Neighborhood Propagation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="620" to="628" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video annotation by graph-based learning with neighborhood similarity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (MM)</title>
		<meeting>the ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="325" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph-based semi-supervised learning with multi-label</title>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia and Expo (ICME)</title>
		<meeting>International Conference on Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1321" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-label learning by constrained nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence Conference (AAAI)</title>
		<meeting>National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence Conference (AAAI)<address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="666" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Trecvid</surname></persName>
		</author>
		<ptr target="&lt;http://www-nlpir.nist.gov/projects/trecvid/&gt;" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The theory of matrices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lancaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tismenetsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Social Sciences</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">LSCOM-lite: a light scale concept ontology for multimedia understanding for TRECVID 2005</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>RC23612, IBM T.J. Watson Research Center</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Columbia university&apos;s baseline detectors for 374 LSCOM semantic visual concepts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yanagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<idno>#222-2006-8</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University ADVENT</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 Conference on Computational Learning Theory (COLT)</title>
		<meeting>the 1998 Conference on Computational Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="1998-07">July 1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<title level="m">Transductive inference for text classification using support vector machines in Proceedings of the 16th International Conference on Machine Learning</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE Workshop on Applications of Computer Vision</title>
		<meeting>the Seventh IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005-01">January 2005</date>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using em</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="103" to="134" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-label text classification with a mixture model trained by em</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the AAAI&apos;99 Workshop on Text Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A kernel method for multi-labelled classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Correlated label propagation with application to multi-label learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1719" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative methods for multi-labeled classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting>Pacific-Asia Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>PAKDD</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="22" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning multi-label scene classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maximal margin labeling for multitopic text categorization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Izumitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Krylov-subspace methods for the Sylvester equation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reichel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and Its Applications</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="283" to="313" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminative fields for modeling semantic concepts in video</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Conference on Large-Scale Semantic Access to Content (RIAO)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
