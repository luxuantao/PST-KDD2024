<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIME: Low-light Image Enhancement via Illumination Map Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
						</author>
						<title level="a" type="main">LIME: Low-light Image Enhancement via Illumination Map Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9B0B1F2452FDAA06A67EE5B1740EA71F</idno>
					<idno type="DOI">10.1109/TIP.2016.2639450</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2639450, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2639450, IEEE Transactions on Image Processing 2 IEEE TRANSACTIONS ON IMAGE PROCESSING This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2639450, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Illumination Estimation</term>
					<term>Illumination (Light) Transmission</term>
					<term>Low-light Image Enhancement CVC: 14.11s LDR: 0.06s LDR: 0.11s CVC: 15.11s LDR: 0.08s CVC: 10.35s LDR: 0.07s LDR: 0.07s CVC: 13.00s CVC: 31.57s HE: 0.18s DeHz: 0.97s DeHz: 1.49s DeHz: 0.89s DeHz: 0.85s DeHz: 0.91s MF: 0.71s MF: 1.16s MF: 0.70s MF: 0.57s MF: 0.64s SRIE: 14.56s SRIE: 24.78s SRIE: 13.42s SRIE: 15.42s SRIE: 8.15s LIME: 0.78s LIME: 1.56s LIME: 0.85s LIME: 0.60s LIME: 0.73s NPE: 17.4s NPE: 34.5s NPE: 17.7s NPE: 12.4s NPE: 16.2s</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When one captures images in low-light conditions, the images often suffer from low visibility. Besides degrading the visual aesthetics of images, this poor quality may also significantly degenerate the performance of many computer vision and multimedia algorithms that are primarily designed for highquality inputs. In this paper, we propose a simple yet effective low-light image enhancement (LIME) method. More concretely, the illumination of each pixel is first estimated individually by finding the maximum value in R, G and B channels. Further, we refine the initial illumination map by imposing a structure prior on it, as the final illumination map. Having the wellconstructed illumination map, the enhancement can be achieved accordingly. Experiments on a number of challenging low-light images are present to reveal the efficacy of our LIME and show its superiority over several state-of-the-arts in terms of enhancement quality and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>U NDOUBTEDLY, high-visibility images reflect clear de- tails of target scenes, which are critical to many visionbased techniques, such as object detection <ref type="bibr" target="#b0">[1]</ref> and tracking <ref type="bibr" target="#b1">[2]</ref>. But, images captured in low-light conditions are often of low visibility. The visual quality of images captured under lowlight conditions, for one thing, is barely satisfactory. For another thing, it very likely hurts the performance of algorithms that are primarily designed for high-visibility inputs. Figure <ref type="figure" target="#fig_0">1</ref> provides several such examples, from which, we can see that many details, such as the paintings on the wall in the first case, the distant field on the bottom-left corner in the third case and the reflection on the floor in the last one, have almost been "buried" in the dark. To make the buried information visible, low-light image enhancement is definitely demanded.</p><p>Directly amplifying the low-light image is probably the most intuitive and simplest way to recall the visibility of dark regions. But, this operation gives birth to another problem, say relatively bright regions might be saturated and thus loss corresponding details. Histogram equalization (HE) strategies Manuscript received June 15, 2016; revised October 04, 2016 and November 10, 2016; accepted December 03, 2016. This work was supported in part by the National Natural Science Foundation of China under grant 61402467, the US National Science Foundation Under grants CNS-1618398 and IIS-1350521. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Lei Zhang.</p><p>X. Guo is with State Key Laboratory Of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China (e-mail: xj.max.guo@gmail.com).</p><p>Y. Li is with the Advanced Digital Sciences Center, Singapore (e-mail: li.yu@adsc.com.sg).</p><p>H. Ling is with Department of Computer and Information Sciences, Temple University, Philadelphia, PA 19122, USA (e-mail: hbling@temple.edu). <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> can avoid the above problem by somehow forcing the output image to fall in the range [0, 1]. Further, variational methods aim to improve the HE performance by imposing different regularization terms on the histogram. For instance, contextual and variational contrast enhancement (CVC) <ref type="bibr" target="#b5">[6]</ref> tries to find a histogram mapping that pays attention on large gray-level differences, while the work <ref type="bibr" target="#b6">[7]</ref> achieves improvement by seeking a layered difference representation of 2D histograms (LDR). However, in nature, they focus on contrast enhancement instead of exploiting real illumination causes, having the risk of over-and under-enhancement. Another solution is Gamma correction that is a nonlinear operation on images. The main drawback is that the nonlinear operation of Gamma correction is carried out on each pixel individually without considering the relationship of a certain pixel with its neighbors, and thus may make enhanced results vulnerable and visually inconsistent with real scenes.</p><p>In Retinex theory <ref type="bibr" target="#b7">[8]</ref>, the dominant assumption is that the (color) image can be decomposed into two factors, say reflectance and illumination. Early attempts based on Retinex, such as single-scale Retinex (SSR) <ref type="bibr" target="#b8">[9]</ref> and multi-scale Retinex (MSR) <ref type="bibr" target="#b9">[10]</ref>, treat the reflectance as the final enhanced result, which often looks unnatural and frequently appears to be overenhanced. The method proposed in <ref type="bibr" target="#b10">[11]</ref> tries to enhance contrast while preserving naturalness of illumination. Although it prevents results from over-enhancement, in our experiments, it performs less impressive than our method in terms of both efficiency and visual quality. Fu et al. proposed a method to adjust the illumination by fusing multiple derivations of the initially estimated illumination map (MF) <ref type="bibr" target="#b11">[12]</ref>. The performance of MF is mostly promising. But, due to the blindness of illumination structure, MF may lose the realism of regions with rich textures. The most recent work of <ref type="bibr" target="#b12">[13]</ref> proposed a weighted variational model for simultaneous reflectance and illumination estimation (SRIE). With the estimated reflectance and illumination, the target image can be enhanced by manipulating the illumination. As noticed in <ref type="bibr" target="#b13">[14]</ref>, inverted lowlight images look like haze images, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Based on this observation, the authors of <ref type="bibr" target="#b13">[14]</ref> alternatively resorted to dehaze the inverted low-light images. After dehazing, the obtained unrealistic images are inverted again as the final enhanced results. Recently, Li et al. followed this technical line and further improved the visual quality by first oversegmenting the input image and then adaptively denoising different segments <ref type="bibr" target="#b14">[15]</ref>. Even though the above dehazing-like methods can provide reasonable results, the basic model they rely on is lacking in physical explanation. By contrast, our method has clear physical intuition.  Contribution Our method belongs to the Retinex-based category, which intends to enhance a low-light image by estimating its illumination map. It is worth noting that, different from the traditional Retinex-based methods like <ref type="bibr" target="#b12">[13]</ref> that decompose an image into the reflectance and the illumination components, our method only estimates one factor, say the illumination, which shrinks the solution space and reduces the computational cost to reach the desired result. The illumination map is first constructed by finding the maximum intensity of each pixel in R, G and B channels. Then, we exploit the structure of the illumination to refine the illumination map. An Augmented Lagrangian Multiplier (ALM) based algorithm is given to exactly solve the refinement problem, while another sped-up solver is designed to intensively reduce the computational load. Experiments on a number of challenging images are conducted to reveal the advantages of our method in comparison with other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>Our method is built upon the following (Retinex) model, which explains the formation of a low-light image:</p><formula xml:id="formula_0">L = R • T,<label>(1)</label></formula><p>where L and R are the captured image and the desired recovery, respectively. Furthermore, T represents the illumination map, and the operator • means element-wise multiplication.</p><p>In this paper, we assume that, for color images, three channels share the same illumination map. With slight abuse of notations, we use T ( T) to represent one-channel and threechannel illumination maps interchangeably. The model ( <ref type="formula" target="#formula_0">1</ref>) is with clear physical meaning, say the observed image can be decomposed into the product of the desired light-enhanced scene and the illumination map. The model of our problem is similar with that of the intrinsic image decomposition <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which attempts   <ref type="formula" target="#formula_5">6</ref>) and ( <ref type="formula" target="#formula_2">3</ref>) with the same illumination map. The atmospheric light a estimated by <ref type="bibr" target="#b19">[20]</ref> is larger than 0.95. Even though, the difference is still noticeable.</p><p>to decompose the input into two components<ref type="foot" target="#foot_0">1</ref> . However, the goal of the intrinsic image decomposition is to recover the reflectance component and the shading one from the given image. As shown in Fig. <ref type="figure" target="#fig_2">3</ref> (b), the reflectance loses the shape of the box (the ground truth reflectance is from <ref type="bibr" target="#b15">[16]</ref>), which does not satisfy the purpose of low-light image enhancement. The expectation of our work is to recall the visual content of dark regions as well as keep the visual realism, as shown in Fig. <ref type="figure" target="#fig_2">3 (c</ref>). Some researchers noticed the unrealism of using the reflectance as the enhanced result, for example <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and tried to project the modified illumination back to the reflectance <ref type="bibr" target="#b12">[13]</ref> by R•f ( T), where R and T are the recovered reflectance and illumination respectively, and f (•) stands for a manipulation operator such as Gamma correction. We can see that the desired result of enhancement is obtained by somehow combining the decomposed components again. Further, due to the ill-posedness of the decomposition problem, more priors are required to help constrain the space of solution. But if the task is just to lighten low-light images, which is this paper concentrates on, it is not necessary to decompose the input image into two components. Because, by slightly transforming (1), we have R = L/T, where the division is element-wise.</p><p>It is apparent that the estimation of T is key to the recovery of R. In this way, the problem is simplified, only demanding the estimation of T. Please notice that L/ T can directly act as the light-enhanced result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Illumination Map Estimation</head><p>As one of the first color constancy methods, Max-RGB <ref type="bibr" target="#b7">[8]</ref> tries to estimate the illumination by seeking the maximum value of three color channels, say R, G and B. But this estimation can only boost the global illumination. In this paper, to handle non-uniform illuminations, we alternatively adopt the following initial estimation:</p><formula xml:id="formula_1">T(x) ← max c∈{R,G,B} L c (x),<label>(2)</label></formula><p>for each individual pixel x. The principle behind the above operation is that the illumination is at least the maximal value of three channels at a certain location. The obtained T(x) guarantees that the recovery will not be saturated, because of</p><formula xml:id="formula_2">R(x) = L(x)/(max c L c (x) + ),<label>(3)</label></formula><p>where is a very small constant to avoid the zero denominator. We point out that the goal of this work is to non-uniformly enhance the illumination of low-light images, instead of eliminating the color shift caused by light sources.</p><p>As mentioned, another widely used model is based on the observation that inverted low-light images 1 -L look similar to haze images, which is thus expressed as <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_3">1 -L = (1 -R) • T + a(1 -T),<label>(4)</label></formula><p>where a represents the global atmospheric light. Although the visual effect of inverted low-light images 1 -L is intuitively similar to haze images, compared to the model ( <ref type="formula" target="#formula_0">1</ref>), the physical meaning of the above remains vague. Below we intend to show the relation between ( <ref type="formula" target="#formula_3">4</ref>) and ( <ref type="formula" target="#formula_0">1</ref>).</p><p>Let us here recall the dark channel prior, a commonly used prior to estimate the transmission map for dehazing <ref type="bibr" target="#b19">[20]</ref>, on 1 -L as follows:</p><formula xml:id="formula_4">T(x) ← 1 -min c 1 -L c (x) a = 1 - 1 a + max c L c (x) a .<label>(5)</label></formula><p>Accordingly, substituting (5) into (4) yields:</p><formula xml:id="formula_5">R(x) = L(x) -1 + a (1 -1 a + max c L c (x) a + ) + (1 -a).<label>(6)</label></formula><p>We can see that when a = 1, both (3) and ( <ref type="formula" target="#formula_5">6</ref>) reach the same result. But, if a gets away from 1, the equivalence between the model ( <ref type="formula" target="#formula_5">6</ref>) <ref type="bibr" target="#b13">[14]</ref> and (3) breaks. As can be seen from Fig. <ref type="figure" target="#fig_3">4</ref>, even though the atmospheric light is greater than 0.95, the visual difference between using (6) and using ( <ref type="formula" target="#formula_2">3</ref>) is still conspicuous. The dark regions in Fig. <ref type="figure" target="#fig_3">4</ref> (b) are less enhanced than those in Fig. <ref type="figure" target="#fig_3">4</ref> (c), please see the zoomed-in patches for details.</p><p>In this work, we rely on the model (3) without involving the atmospheric light a.</p><p>In this work, we employ (2) to initially estimate illumination map T, due to its simplicity, although various approaches (e.g. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>) have been developed to improve the accuracy in past decades. Most of these improvements essentially consider the local consistency of illumination by taking into account neighboring pixels within a small region around the target pixel. Two representative ways are:</p><formula xml:id="formula_6">T(x) ← max y∈Ω(x) max c∈{R,G,B} L c (y); T(x) ← mean y∈Ω(x) max c∈{R,G,B} L c (y),<label>(7)</label></formula><p>where Ω(x) is a region centered at pixel x, and y is the location index within the region. These schemes can somewhat enhance the local consistency, but they are structure-blind. In the following, we provide a more powerful scheme to better achieve this goal.</p><p>A "good" solution should simultaneously preserve the overall structure and smooth the textural details. To address this issue, based on the initial illumination map T, we propose to solve the following optimization problem:</p><formula xml:id="formula_7">min T T -T 2 F + α W • ∇T 1 , (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where α is the coefficient to balance the involved two terms and, • F and • 1 designate the Frobenious and 1 norms, respectively. Further, W is the weight matrix, and ∇T is the first order derivative filter. In this work, it only contains ∇ h T (horizontal) and ∇ v T (vertical). In the objective ( <ref type="formula" target="#formula_7">8</ref>), the first term takes care of the fidelity between the initial map T and the refined one T, while the second term considers the (structure-aware) smoothness. Prior to discussing possible strategies of constructing W, we give two solvers in the next two sub-sections to resolve problem <ref type="bibr" target="#b7">(8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Exact Solver to Problem (8)</head><p>Traditionally, the problem ( <ref type="formula" target="#formula_7">8</ref>) can be effectively solved via the alternating direction minimization technique. As can be seen from the objective in ( <ref type="formula" target="#formula_7">8</ref>), both two terms, say 2 and 1 terms, involve T. An auxiliary variable G is introduced to replace ∇T for making the problem separable and thus easy to solve. Accordingly, ∇T = G is added as a constraint. As a result, we have the following equivalent optimization problem:</p><formula xml:id="formula_9">min T,G T -T 2 F + α W • G 1 s. t. ∇T = G.<label>(9)</label></formula><p>The augmented Lagrangian function of ( <ref type="formula" target="#formula_9">9</ref>) can be naturally written in the following shape:</p><formula xml:id="formula_10">L = T -T 2 F + α W • G 1 + Φ(Z, ∇T -G),<label>(10)</label></formula><p>with the definition</p><formula xml:id="formula_11">Φ(Z, ∇T -G) = µ 2 ∇T -G 2 F + Z, ∇T -G ,</formula><p>where •, • represents matrix inner product, µ is a positive penalty scalar, and Z is the Lagrangian multiplier. There are three variables, including T, G and Z to solve. The ALM technique is a common choice to solve the problem <ref type="bibr" target="#b7">(8)</ref>. The solver iteratively updates one variable at a time by fixing the others, and each step has a simple closed-form solution. For conveniently analyzing and comparing the exact solver and the sped-up one (proposed later), we provide the solutions of the sub-problems below: T sub-problem: Collecting the T involved terms from Eq. <ref type="bibr" target="#b9">(10)</ref> gives the problem as follows:</p><formula xml:id="formula_12">T (t+1) ← argmin T T -T 2 F + Φ(Z (t) , ∇T -G (t) ). (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>As can be seen from Eq. ( <ref type="formula" target="#formula_12">11</ref>), it is a classic least squares problem. Thus, the solution can be computed through differentiating <ref type="bibr" target="#b10">(11)</ref> with respect to T and setting it to 0:</p><formula xml:id="formula_14">2(T -T) + µ (t) D T (DT -G (t) ) + D T Z (t) = 0 ⇒(2I + µ (t) D T D)T = 2 T + µ (t) D T (G (t) - Z (t) µ (t) ), (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>where I is the identity matrix with proper size. And D contains D h and D v , which are the Toeplitz matrices from the discrete gradient operators with forward difference. We note that, for convenience, the operations DX and D T X represent reshape(Dx) and reshape(D T x), respectively, where x is the vectorized X, and reshape(•) stands for the operation of reshaping vectors back to their matrix forms. Directly calculating the inverse of 2I + µ (t) D T D is an intuitive way to accomplish this job. However, the matrix inverse is computationally expensive, especially for large matrices like D T D. Fortunately, by assuming circular boundary conditions, we can apply 2D FFT techniques on the above problem, which enables us to compute the solution fast. Consequently, we have</p><formula xml:id="formula_16">T t+1 ← F -1 F 2 T + µ (t) D T (G (t) -Z (t) µ (t) ) 2 + µ (t) d∈{h,v} F(D d ) • F(D d )<label>(13)</label></formula><p>where F(•) is the 2D FFT operator, while F -1 (•) and F(•) stand for the 2D inverse FFT and the complex conjugate of F(•), respectively. The division performs element-wise. In addition, 2 is the matrix with proper size, all the entries of which are 2.</p><p>G sub-problem: Dropping the terms unrelated to G leads to the following optimization problem:</p><formula xml:id="formula_17">G (t+1) ← argmin G α W • G 1 + Φ(Z (t) , ∇T (t+1) -G).<label>(14)</label></formula><p>The closed form solution of ( <ref type="formula" target="#formula_17">14</ref>) can be easily obtained by performing the shrinkage operation like:</p><formula xml:id="formula_18">G (t+1) = S αW µ (t) ∇T (t+1) + Z (t) µ (t) .<label>(15)</label></formula><p>S ε&gt;0 [•] represents the shrinkage operator, the definition of which on scalars is: S ε [x] = sgn(x) max(|x| -ε, 0). The extension of the shrinkage operator to vectors and matrices is to simply process data element-wise, say S A [X] performs the shrinkage on the elements of X with thresholds given by corresponding entries of A. Z and µ: The updating of Z and µ can be done via:</p><formula xml:id="formula_19">Z (t+1) ← Z (t) + µ (t) (∇T (t+1) -G (t+1) ); µ (t+1) ← µ (t) ρ, ρ &gt; 1. (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>For clarity, the entire procedure of exact solver to problem (8) is summarized in Algorithm 1. The iteration is stopped when ∇T (t+1) -G (t+1)</p><p>F ≤ δ T F with δ = 10 -5 or the maximal number of iterations is reached. Please refer to Algorithm 1 for other details that we can not cover in the text.</p><p>Remark 1 (Convergence and Optimality) As aforementioned, the problem ( <ref type="formula" target="#formula_9">9</ref>) is equivalent to <ref type="bibr" target="#b7">(8)</ref>. We can observe that every term appears in the objective function of ( <ref type="formula" target="#formula_9">9</ref>) is convex, and the constraint is affine. The proposed Algorithm 1 follows the framework of Augmented Lagrangian Multiplier with Alternating Direction Minimizing (ALM-ADM), the theoretical guarantee of which has been well established for twoblock convex cases <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. In other words, our proposed exact solver converges to a global optimal solution to the problem <ref type="bibr" target="#b7">(8)</ref>, and thus to the original <ref type="bibr" target="#b8">(9)</ref>.</p><p>Algorithm 1: Exact Solver to Problem <ref type="bibr" target="#b7">(8)</ref> Input: The positive coefficient α, and the initial illumination map T ∈ R m×n . Initialization:</p><formula xml:id="formula_21">T (0) = 0 ∈ R m×n , G (0) = Z (0) = 0 ∈ R 2m×n , t = 0, µ (0) &gt; 0, ρ &gt; 1.</formula><p>while not converged do Update T (t+1) via Eq. ( <ref type="formula" target="#formula_16">13</ref>); Update G (t+1) via Eq. ( <ref type="formula" target="#formula_18">15</ref>)); Update Z (t+1) and µ (t+1) via Eq. ( <ref type="formula" target="#formula_19">16</ref>); t = t + 1; end Output: Optimal solution T * = T (t) .</p><p>Remark 2 (Computational Complexity) Each iteration of Algorithm 1 involves three sub-problems. Regarding the T sub-problem, it requires O(N log N ) to finish the computation, where N is the total amount of pixels. Its dominant cost comes from 2D FFT and inverse FFT operations. As for the G and Z sub-problems, they both are linear with respect to N , say O(N ). Hence, each iteration takes O(N log N ). Based on the above, it is ready to conclude that the complexity of Algorithm 1 is O(tN log N ), where t is the number of iterations required to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sped-up Solver to Problem (8)</head><p>Although the complexity of Algorithm 1 is reasonably low, we want to further reduce it. Let us take a closer look at the problem <ref type="bibr" target="#b7">(8)</ref>. The origin bringing the iterative procedure is the sparse weighted gradient term, i.e. W • ∇T 1 . The 1 norm together with the gradient operation on T makes it somewhat complex. Clearly, the relationship below holds true:</p><formula xml:id="formula_22">lim →0 +</formula><p>x d∈{h,v}</p><formula xml:id="formula_23">W d (x)(∇ d T(x)) 2 |∇ d T(x)| + = W • ∇T 1 . (<label>17</label></formula><formula xml:id="formula_24">)</formula><p>Based on the above, we use</p><formula xml:id="formula_25">x d∈{h,v} W d (x)(∇ d T(x)) 2 |∇ d T(x)|+</formula><p>to approximate W • ∇T 1 . As a result, the approximate problem to (8) can be written as follows:</p><formula xml:id="formula_26">min T T -T 2 F + α x d∈{h,v} W d (x)(∇ d T(x)) 2 |∇ d T(x)| + .<label>(18)</label></formula><p>Although the objective function changes, compared to the original, the goal of extracting the structure of illumination from the initial illumination estimate T is consistent with the original. More specifically, when |+ . In other words, the target T is constrained to avoid creating gradients where the initially estimated illumination map has small magnitudes of gradient. In contrary, if |∇ d T(x)| is strong, the above suppression alleviates, because this location is more likely on structure boundary than on regular texture.</p><formula xml:id="formula_27">|∇ d T(x)| is small, |∇ d T(x)| is about to be suppressed, so is the value (∇ d T(x)) 2 |∇ d T(x)</formula><p>As can be observed, the problem (18) only involves quadratic terms. Thus, the problem can be directly obtained by solving the following: where wd is the vectorized version of Wd with Wd (x) ←</p><formula xml:id="formula_28">(I + d∈{u,v} D T d Diag( wd )D d )t = t,<label>(19)</label></formula><formula xml:id="formula_29">W d (x)</formula><p>|∇ d T(x)|+ . Further, the operator Diag(x) is to construct a diagonal matrix using vector x. Since (I + d∈{u,v} D T d Diag( wd )D d ) is a symmetric positive definite Laplacian matrix, there are many techniques available for solving it, for example, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Remark 3 (Computational Complexity) Solvers such as the multi-resolution preconditioned conjugate gradient can reach O(N ) complexity. Compared to the complexity of Algorithm 1, the sped-up solver eliminates the iteration requirement t as well as reduces N log N to N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Possible Weighting Strategies</head><p>For the structure-aware refinement on the initial illumination map, the key is the design of W. In this part, we discuss three possible weighing strategies as follows.</p><p>Strategy I: It can be seen that setting the weight matrix as</p><formula xml:id="formula_30">W h (x) ← 1; W v (x) ← 1,<label>(20)</label></formula><p>leads (8) to a classic 2 loss total variation minimization problem <ref type="bibr" target="#b32">[33]</ref>.</p><p>Strategy II: As discussed in Sec. II-C, it is reasonable to use the gradient of the initial illumination map as the weight. In the sequel, we have:</p><formula xml:id="formula_31">W h (x) ← 1 |∇ h T(x)| + ; W v (x) ← 1 |∇ v T(x)| + .<label>(21)</label></formula><p>Strategy III: Inspired by Relative Total Variation (RTV) <ref type="bibr" target="#b33">[34]</ref>, for each location, the weight is set via:</p><formula xml:id="formula_32">W h (x) ← y∈Ω(x) G σ (x, y) | y∈Ω(x) G σ (x, y)∇ h T(y)| + ; W v (x) ← y∈Ω(x) G σ (x, y) | y∈Ω(x) G σ (x, y)∇ v T(y)| + ,<label>(22)</label></formula><p>where G σ (x, y) is produced by the Gaussian kernel with the standard deviation σ. Formally, G σ (x, y) is expressed as:</p><formula xml:id="formula_33">G σ (x, y) ∝ exp - dist(x, y) 2σ 2 , (<label>23</label></formula><formula xml:id="formula_34">)</formula><p>where the function dist(x, y) is to measure the spatial Euclidean distance between locations x and y. In fact, the second weighting strategy is an instance of this one. When → 0 + , the two strategies obtain the same weight matrix. We note that, different from RTV, our weight matrix is constructed based on the given T instead of being iteratively updated according to T. That means W only needs to be calculated once. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Other Operations</head><p>Having the refined illumination map T, we can recover R by following <ref type="bibr" target="#b2">(3)</ref>. One can also manipulate the illumination map through gamma transformation, say T ← T γ . From the upper row of Fig. <ref type="figure" target="#fig_5">7</ref>, we can see the difference between the results by setting γ to 0.5, 0.8 and 1. For the rest experiments, we adopt γ = 0.8. Moreover, possible noises previously hiding in the dark are also accordingly amplified, especially for the very low-light inputs (regions), as shown in Fig. <ref type="figure" target="#fig_5">7</ref>. Denoising techniques are required to further improve the visual quality. Many off-the-shelf denosing tools, such as <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, can be employed to do the job. Considering the comprehensive performance, BM3D <ref type="bibr" target="#b34">[35]</ref> is the choice of this work. In our implementation, for further cutting the computational load, we only execute BM3D on the Y channel by converting R from the RGB colorspace into the YUV one. In addition, the magnitude of noises is not the same for different regions of the input, as the amplification is different. And BM3D treats different patches equally. Therefore, to avoid the unbalance of processing, e.g. some (dark) places are well-denoised while some (brighter) over-smoothed, we employ the following operation:</p><formula xml:id="formula_35">R f ← R • T + R d • (1 -T),<label>(24)</label></formula><p>where R d and R f are the results after denoising and recomposing, respectively. The merit of this operation can be viewed from Fig. <ref type="figure" target="#fig_5">7</ref> (e), compared with Fig. <ref type="figure" target="#fig_5">7 (d</ref>). We would like to mention that the denoising plus recomposing, as a postprocessing step, can be concatenated to any low-light image enhancing method. The whole procedure of LIME is outlined in Algorithm 2.</p><p>Please note that, sometimes other specific techniques are needed to remedy the complication caused by low-light enhancement. For image compression using like JPEG <ref type="bibr" target="#b37">[38]</ref>, the blocking effect becomes noticeable in the low-light enhanced results. Hence, deblocking techniques <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> may be required. Further, for color distorted images, adopting some color constancy methods <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> as post-processing can alleviate the negative effect. In this paper, we do not consider these issues (and other possible complications caused by low-light enhancement) for avoiding distraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>In this section, we first see the performance difference between different weighting strategies, and the effect of involved parameters. Next, the analysis of our exact and spedup solvers is given. Then, we qualitatively and quantitatively compare our LIME with several state-of-the-art methods, including Histogram Equalization (HE), Adaptive Histogram Equalization (AHE), Gamma Correction (GC), Contextual and Variational Contrast enhancement (CVC) <ref type="bibr" target="#b5">[6]</ref>, Layered Difference Representation (LDR) <ref type="bibr" target="#b6">[7]</ref>, dehazing based method <ref type="bibr" target="#b13">[14]</ref> (DeHz), Multi-deviation Fusion method (MF) <ref type="bibr" target="#b11">[12]</ref>, Naturalness Preserved Enhancement algorithm (NPE) <ref type="bibr" target="#b10">[11]</ref> and Simultaneous Reflection and Illumination Estimation (SRIE) <ref type="bibr" target="#b12">[13]</ref>. All the codes are in Matlab 2 , which ensures the fairness of time comparison. All the experiments are conducted on a PC running Windows 7 OS with 64G RAM and 2.4GHz CPU. 2 HE and AHE uses histeq and adapthisteq functions integrated in the Matlab toolbox. GC is achieved by L γ , while the codes of CVC, LDR, MF, NPE and SRIE are downloaded from the authors' websites. The code of DeHz is not publicly available when this paper is prepared, but it is easy to be implemented based on <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Weighting Strategy and Parameter Effect</head><p>Here, we evaluate the performance difference brought by three different weighting strategies with varying parameters α and σ our model <ref type="bibr" target="#b7">(8)</ref>. The relationship between the weighting strategy II and III, as aforementioned, is that the former one is a special case of the latter one. Their equivalence is reached by setting σ → 0 + in <ref type="bibr" target="#b21">(22)</ref>. To clearly see the effect of each parameter, we first compare the weighting strategy I and II by varying the parameter α without σ involved. The effect of σ in the weighting strategy III is then tested by keeping α fixed and varying σ. Please notice that, for easier and better viewing the performance difference, we employ a color image with good visibility instead of an illumination map in this test.</p><p>Figure <ref type="figure">5</ref> provides the comparison of the weighting strategy I and II. It is worth mentioning that the value scale of the term W •∇T 1 may significantly change when applying different weighting strategies. Thus, for comparison fairness, we need to eliminate the scale issue. To this end, we alternatively control the difference norms (DN) of the two cases, defined as T -T F , to be sufficiently close. For the upper row (Strategy I), we use α ∈ {0.2, 0.5, 1.0, 3.0} corresponding to the four results, respectively. While for the lower row (Strategy II), we use α ∈ 0.15 × {0.2, 0.5, 1.0, 3.0} to accomplish the test. It is certain that as α grows, the fidelity between T and T decreases, as well as the smoothness of the desired T increases. From the picture pairs shown in Fig. <ref type="figure">5</ref>, we observe that the results by the weighting strategy II retain the overall structure as well as smooth the texture better than those by the strategy I. This corroborates that the 2 loss TV model is short of ability to distinguish between strong structural edges and texture <ref type="bibr" target="#b33">[34]</ref>. Further, Figure <ref type="figure">6</ref> shows a set of results obtained by fixing α = 0.15 × 0.2 and varying σ in the weighting strategy III. The result using σ = 10 -5 , as shown in Fig. <ref type="figure">6</ref> (b), confirms the relationship between the weighting strategy II and III. From Fig. <ref type="figure">6</ref> (a) to (e), we can see the smoothing effect becomes heavier. This is because increasing σ involves more neighboring locations taken into account. Even though, the overall structure is well preserved. For the rest experiments, we simply adopt the weighting strategy III with σ = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Exact Solver vs. Sped-up Solver</head><p>Although the theoretical complexity of the exact solver is given as O(tN log N ), it would be more intuitive to see how many iterations Algorithm 1 needs to converge empirically. The left picture in Fig. <ref type="figure" target="#fig_10">13</ref> plots four curves. From the plots, we find that the trends of the four plots are very close. And the exact algorithm converges within 60 iterations for all the four tests. In the rest experiments, we set the maximum number of iterations of Algorithm 1 to 60. Please notice that, to eliminate the scale difference in the stop criterion, we have normalized the stop criterion in [0, 1] for different cases.</p><p>The second picture shown in Fig. <ref type="figure" target="#fig_10">13</ref> gives the comparison between the exact and sped-up solvers in terms of time cost. From the curves, we see that when the image size is smaller than 400, both the two solvers are sufficiently efficient. But, after that, the processing time demanded by the exact solver rapidly grows up while that of the sped-up solver varies gently and keeps reasonably low. In other words, the benefit of the sped-up solver compared with the exact one becomes more conspicuous, as the image size increases.</p><p>Figure <ref type="figure" target="#fig_7">8</ref> shows a comparison of the exact solver and spedup solver on illumination maps, from which, we can see that the illumination maps by the exact solver are sharper than those by the sped-up solver. We adopt α = 0.15 for both the exact and sped-up solvers in this subsection and for all the rest experiments. Although there is a gap between the illumination maps obtained by these two solvers, the visual difference between their enhanced results is acceptable. Sometimes, relatively soft illumination maps may provide visually more comfortable results (see the second case). Considering the efficiency, the sped-up solver is more attractive for practical use. Further, compared to the other illumination maps, the advance of our results is obvious.   From the pictures shown in Fig. <ref type="figure">9</ref> and 10, the lightness is still somewhat dim for CVC, LDR, AHE, NPE, MF and NPE, which can be further enhanced by gamma correction intuitively. We note that SRIE itself has a gamma correction step on the estimated illumination. Figure <ref type="figure" target="#fig_0">11</ref> depicts the results  <ref type="figure" target="#fig_6">12</ref> gives another test. The very low-light input hides intensive noises in the dark. After performing LIME, the details of the scene get enhanced, but the noises also come out, as shown in the middle of Fig. <ref type="figure" target="#fig_6">12</ref>. This is an inevitable problem encountered by almost all of existing lowlight enhancement algorithms. As we have discussed in Sec. II-E, denoising is required. The right picture in Fig. <ref type="figure" target="#fig_6">12</ref> is the denoised result by executing BM3D on the middle of Fig. <ref type="figure" target="#fig_6">12</ref>, from which we can see the improvement in terms of visual quality. Fig. <ref type="figure" target="#fig_13">17</ref> provides more qualitative results by LIME.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison</head><p>As pointed out in <ref type="bibr" target="#b10">[11]</ref>, the relative order of lightness represents the light source directions and the lightness variation, the naturalness of an enhanced image is related to the relative order of lightness in different local areas. Therefore, we employ the lightness order error (LOE) as our objective metric to measure the performance. The definition of LOE is as follows:</p><formula xml:id="formula_36">LOE = 1 m m x=1 m y=1 U (Q(x), Q(y)) ⊕ U (Q r (x), Q r (y)) ,<label>(25)</label></formula><p>where m is the pixel number. The function U (p, q) returns 1 if p &gt;= q, 0 otherwise. ⊕ stands for the exclusive-or operator. In addition, Q(x) and Q r (x) are the maximum values among R, G and B channels at location x of the enhanced and reference images, respectively. The lower the LOE is, the better the enhancement preserves the naturalness of lightness. Due to the heavy load of computing LOE, as suggested in <ref type="bibr" target="#b10">[11]</ref>, downsampling is used to reduce the complexity. Similarly, in this part, we set the resize factor r to 100/ min(h, w), where h and w are the height and width of the image, respectively.</p><p>From the definition of LOE, we notice that Q r plays an important role in quantitatively measuring the quality of enhancement. Using the low-light input as the reference is problematic. Because, take an extreme case for example, the LOE is 0 when no enhancement is performed. While few datasets with ground truth are publicly available and it is not easy to construct such datasets, we have to choose reliable data to do the job for the sake of objectiveness. Different from <ref type="bibr" target="#b10">[11]</ref>, we adopt the HDR <ref type="bibr" target="#b44">[45]</ref> result as the reference instead of the input low-light image. The HDR reconstruction results from a set of bracketed exposures are more proper to act as the reference. The HDR dataset contains eight groups, several samples from this dataset are shown in Fig. <ref type="figure" target="#fig_0">15</ref>. Table I contains the LOE numbers of all the competitors on the HDR dataset. From the numbers, we observe that our LIME significantly outperform the others. In addition, we give the visual comparison on two cases in Fig. <ref type="figure" target="#fig_12">16</ref>, from which, we can find that the results obtained by LIME are more visually pleasant and closer to the references than the others. To allow more experimental verification and comparisons, we release our code at http://cs.tju.edu.cn/orgs/vision/ ∼ xguo/LIME.htm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we have proposed an efficient and effective method to enhance low-light images. The key to the low-light enhancement is how well the illumination map is estimated. The structure-aware smoothing model has been developed to improve the illumination consistency. We have designed two algorithms: one can obtain the exact optimal solution to the target problem, while the other alternatively solves the approximate problem with significant saving of time. Moreover, our model is general to different (structure) weighting strategies. The experimental results have revealed the advance of our method compared with several state-of-the-art alternatives. It is positive that our low-light image enhancement technique can feed many vision-based applications, such as edge detection, feature matching, object recognition and tracking, with high visibility inputs, and thus improve their performance.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Top Row: Natural low-light images. Middle Row: The illumination maps estimated by our method. Bottom Row:The results enhanced by our method.</figDesc><graphic coords="2,100.99,371.00,80.14,88.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The inverted versions (unrealistic images) of images shown in the top row of Fig. 1.</figDesc><graphic coords="2,183.62,372.88,80.14,88.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Different purposes of intrinsic image decomposition and low-light image enhancement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Comparison of (6) and (3) with the same illumination map. The atmospheric light a estimated by<ref type="bibr" target="#b19">[20]</ref> is larger than 0.95. Even though, the difference is still noticeable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig. 5: Difference between the weighting strategy I and II with varying parameter α. For the first row (Strategy I), we use α ∈ {0.2, 0.5, 1.0, 3.0} corresponding to the four results, respectively. While for the second row (Strategy II), we use α ∈ 0.15 × {0.2, 0.5, 1.0, 3.0} to do the test. The choice of the coefficient 0.15 is based on the observation that the difference norms (DN), defined as T -T F , of the two cases are close, so that the comparison is fair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Gamma correction for illumination maps in the range of [0, 1], denosing and recomposition. (a)-(c) are the recovered images using T γ with γ = 0.5, γ = 0.8 and γ = 1, respectively. The corresponding illumination map is given on the top of each recovery. Noises appear in the light enhanced images. (d) is the denoised version of (b), while (e) is the recomposed result of (b) and (d). It can be seen from the zoomed-in patches that the recomposition adaptively keeps the fine details of the bright region and suppresses the noises of the dark region.</figDesc><graphic coords="6,208.82,212.48,65.09,72.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 2 :</head><label>2</label><figDesc>LIME Input: Low-light Input L, positive coefficient α, Gamma transformation parameter γ. Initialization: Constructing weight matrix by Eq. (20), Eq. (21) or Eq. (22) Do the job 1. Estimate initial illumination map T on L via Eq. (2); 2. Refine illumination map T based on T via exact solver Alg. 1 or sped-up solver Eq. (19); 3. Gamma correction on T via T ← T γ ; 4. Enhance L using T according to L = R • T; 5. If denoising and recomposing needed, then denoise R by BM3D (R d ) and recompose via Eq. (24). Output: Final enhanced result</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Comparison of different illumination maps and corresponding enhanced results. From (a) to (f): Illumination map estimated individually on each pixel (Initial), refined by local max (7) (Max), bilateral filtering (BF), our results by exact solver and sped-up solver, respectively.</figDesc><graphic coords="7,270.89,298.84,70.20,78.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Result comparison between MF, DeHz, NPE, SRIE and LIME. Please see also Fig. 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figures 9 Fig. 11 :Fig. 12 :</head><label>91112</label><figDesc>Figures 9 and 10 provide several comparisons. The inputs are from the top row of Fig. 1. The operations of HE, AHE and GC are executed on the V channel of images by first converting it from the RGB colorspace to the HSV one and then converting the processed HSV back to the RGB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 :</head><label>13</label><figDesc>Fig.13: Left: convergence speed of the proposed exact solver (Algorithm 1). The images used in this experiment are, without loss of generality, the second and third pictures in Fig.1(Image 1 and 2), and the one in Fig.4(Image 3), respectively. Right: time comparison between the exact solver and the spedup one with varying image sizes.</figDesc><graphic coords="9,146.10,578.67,56.78,64.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 :Fig. 15 :</head><label>1415</label><figDesc>Fig. 14: From Left to Right: results by HE, GC and AHE directly on the RGB colorspace, respectively.</figDesc><graphic coords="9,203.78,578.67,57.68,63.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 :</head><label>16</label><figDesc>Fig. 16: Visual comparison among the competitors on the HDR dataset.</figDesc><graphic coords="11,370.50,340.39,62.27,93.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 :</head><label>17</label><figDesc>Fig. 17: More results by our proposed LIME (sped-up). All the results are obtained by adopting weighting strategy III and fixing α = 0.15, σ = 2 and γ = 0.8. The results are better viewed in color.</figDesc><graphic coords="12,313.74,262.55,68.47,90.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Quantitative performance comparison on the HDR dataset in terms of LOE. LOE has a factor 10 3 . For the pictures obtained by different methods, we tune the parameter γ to achieve their possible best visual quality. The lightness is indeed increased, but similar to using only GC, the visual artifact appears for all the further enhanced results of AHE, CVC, LDR, MF, DeHz and NPE. This is mainly because the nonlinear operation of GC is carried out on each pixel individually without considering the relationship of a certain pixel with its neighbors. Although our LIME also employs the GC as described in Sec. II-E, the estimated illumination map itself is structure-aware and thus LIME survives from such artifacts. It is worth noticing that the parameter γ used in LIME is fixed to 0.8 for all the experiments in Sec. III instead of being fine-tuned image by image. Figure</figDesc><table><row><cell>Method</cell><cell>BabyAtWin</cell><cell>BabyOnGrass</cell><cell cols="3">ChrisRider FeedingTime HighChair</cell><cell cols="2">LadyEating PianoMan</cell><cell>SantaHelper</cell><cell>Ave. LOE</cell></row><row><cell>HE</cell><cell>4.536</cell><cell>4.492</cell><cell>2.433</cell><cell>3.117</cell><cell>3.127</cell><cell>3.395</cell><cell>3.759</cell><cell>3.652</cell><cell>3.564</cell></row><row><cell>AHE</cell><cell>3.481</cell><cell>2.471</cell><cell>2.127</cell><cell>2.098</cell><cell>1.919</cell><cell>2.648</cell><cell>2.591</cell><cell>2.907</cell><cell>2.530</cell></row><row><cell>GC</cell><cell>4.518</cell><cell>4.496</cell><cell>2.430</cell><cell>3.101</cell><cell>3.141</cell><cell>3.401</cell><cell>3.755</cell><cell>3.645</cell><cell>3.561</cell></row><row><cell>CVC</cell><cell>4.549</cell><cell>4.488</cell><cell>2.557</cell><cell>3.132</cell><cell>3.148</cell><cell>3.402</cell><cell>3.823</cell><cell>3.695</cell><cell>3.599</cell></row><row><cell>LDR</cell><cell>4.501</cell><cell>4.500</cell><cell>2.509</cell><cell>3.120</cell><cell>3.134</cell><cell>3.401</cell><cell>3.775</cell><cell>3.670</cell><cell>3.572</cell></row><row><cell>MF</cell><cell>3.626</cell><cell>2.838</cell><cell>2.124</cell><cell>2.005</cell><cell>2.291</cell><cell>2.749</cell><cell>3.113</cell><cell>3.145</cell><cell>2.736</cell></row><row><cell>NPE</cell><cell>3.811</cell><cell>4.489</cell><cell>3.191</cell><cell>3.183</cell><cell>3.401</cell><cell>3.250</cell><cell>3.872</cell><cell>3.773</cell><cell>3.621</cell></row><row><cell>DeHz</cell><cell>4.591</cell><cell>4.527</cell><cell>2.854</cell><cell>3.114</cell><cell>3.227</cell><cell>3.408</cell><cell>3.837</cell><cell>3.732</cell><cell>3.661</cell></row><row><cell>SRIE</cell><cell>4.133</cell><cell>4.224</cell><cell>2.770</cell><cell>3.047</cell><cell>3.201</cell><cell>3.196</cell><cell>3.233</cell><cell>3.497</cell><cell>3.413</cell></row><row><cell>LIME</cell><cell>3.263</cell><cell>2.356</cell><cell>1.945</cell><cell>2.091</cell><cell>2.330</cell><cell>2.305</cell><cell>2.513</cell><cell>2.352</cell><cell>2.394</cell></row><row><cell cols="3">after executing gamma correction.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The intrinsic image decomposition originally involves three factors including Lambertian shading (T), reflectance (R) and specularities (C), formally expressed as L = R • T + C. As pointed out in<ref type="bibr" target="#b15">[16]</ref>, the simplified model with the component C discarded can also work well. And many works, such as<ref type="bibr" target="#b16">[17]</ref>,<ref type="bibr" target="#b17">[18]</ref> and<ref type="bibr" target="#b18">[19]</ref>, are based on this simplified model.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatio-temporal object detection proposals</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="737" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time compressive tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="866" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contrast limited adaptive histogram equalization image processing to improve the detection of simulated spiculations in dense mammograms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hemminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deluce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Braeuning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="200" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple and effective histogram equalization approach to image enhancement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="170" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A dynamic histograme equalization for image contrast enhancement</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdullah-Al-Wadud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="593" to="600" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contextual and variational contrast enhancement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tjahjadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3431" to="3441" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5372" to="5384" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The retinex theory of color vision</title>
		<author>
			<persName><forename type="first">E</forename><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="108" to="128" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Properties and performance of a center/surround retinex</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1996</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multi-scale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3538" to="3578" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fusion-based enhancing method for weakly illuminated images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2782" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast efficient algorithm for enhancement of low lighting video</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A low-light image enhancement method for both denoising and contrast enlarging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ground-truth dataset and baseline evaluations for intrinsic image algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2335" to="2342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recovering intrinsic images with a global sparsity prior on reflectance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="765" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Color constancy, intrinsic images, and shape estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="57" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single image layer separation using relative smoothness</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A novel approach to color constancy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="9" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The rehabilitation of maxrgb</title>
		<author>
			<persName><forename type="first">B</forename><surname>Funt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Color Imaging Conference</title>
		<meeting>Color Imaging Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The role of bright pixels in illumination estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Colour in Graphics, Imaging and Vision</title>
		<meeting>European Conference on Colour in Graphics, Imaging and Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linearized alternating direction method with adaptive penalty for low rank representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="612" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On the linear convergence of the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1208.3922</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Edge-preserving decomposition for multi-scale tone and detail manipulation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multigrid and multilevel preconditioners for computational photography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="689" to="694" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interactive local adjustment of tonal values</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="646" to="653" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Locally adapted hierarchical basis preconditioning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1135" to="1143" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An augmented lagrangian method for total variation video restoration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khoshabeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3097" to="3111" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Strucutre extraction from texture via relative total variation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3d transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with bm3d</title>
		<author>
			<persName><forename type="first">H</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with applications to image denoising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Information technology digital compression and coding of continuoustone still images requirements and guidelines</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>International Telecommunication Union</publisher>
		</imprint>
	</monogr>
	<note>tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A contrast enhancement framework with jpeg artifacts suppression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="174" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual data deblocking using structural layer priors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICME</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A learning-based approach to reduce JPEG artifacts in image matting</title>
		<author>
			<persName><forename type="first">I</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2880" to="2887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointwise shape-adaptive dct for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A spatial processor model for object colour perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Buchsbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Frank. Inst</title>
		<imprint>
			<biblScope unit="volume">310</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Color constancy by category correlation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vazquz-Corral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vanrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baldrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1997" to="2007" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Xiaojie Guo (M&apos;13) received the B.E. degree in software engineering from the School of</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yaesoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">respectively. He is currently an Associate Professor with the Institute of Information Engineering, Chinese Academy of Sciences. He was a recipient of the Piero Zamperoni Best Student Paper Award in the International Conference on Pattern Recognition (International Association on Pattern Recognition), in 2010. Yu Li (M&apos;16) received his Ph.D. degree in National University of Singapore. He is now with Advanced Digital Sciences Center, a research center found</title>
		<meeting><address><addrLine>Wuhan, China; Tianjin, China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2012. 2010 and 2013</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="203" />
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology, Wuhan University of Technology ; University of Illinois at Urbana-Champaign (UIUC) and the Agency for Science, Technology and Research (A*STAR</orgName>
		</respStmt>
	</monogr>
	<note>2008, and the M.S. and Ph.D. degrees in computer science from the School of Computer Science and Technology, Tianjin University. His research interests include computer vision, computational photography, and computer graphics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
