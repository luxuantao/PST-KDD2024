<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Congxuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhongkai</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Chen</surname></persName>
							<email>dr_chenzhen@163.com</email>
						</author>
						<author>
							<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
							<email>wmhu@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
							<email>liming@nchu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shaofeng</forename><surname>Jiang</surname></persName>
							<email>jiangshaofeng@nchu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Nondestructive Testing</orgName>
								<orgName type="institution">Nanchang Hangkong University</orgName>
								<address>
									<postCode>330063</postCode>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Internet of Things Engineering</orgName>
								<orgName type="institution">Hohai University</orgName>
								<address>
									<postCode>213022</postCode>
									<settlement>Changzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Nondestructive Testing</orgName>
								<orgName type="institution">Nanchang Hangkong University</orgName>
								<address>
									<postCode>330063</postCode>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Key Laboratory of Jiangxi Province for Image Processing and Pattern Recognition</orgName>
								<orgName type="institution">Nanchang Hangkong University</orgName>
								<address>
									<postCode>330063</postCode>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">Key Laboratory of Nondestructive Testing</orgName>
								<orgName type="institution" key="instit1">Min-istry of Education</orgName>
								<orgName type="institution" key="instit2">Nanchang Hangkong University</orgName>
								<address>
									<postCode>330063</postCode>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TMM.2021.3096083</idno>
					<note type="submission">received January 13, 2020; revised May 18, 2021; accepted July 6, 2021. Date of publication July 13, 2021; date of current version July 12, 2022.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Learning optical flow</term>
					<term>self-attention</term>
					<term>multiscale feature</term>
					<term>large displacements</term>
					<term>occlusions</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-Attention-Based Multiscale Feature Learning Optical Flow With Occlusion Feature Map Prediction Congxuan Zhang , Member, IEEE, Zhongkai Zhou, Zhen Chen , Weiming Hu , Senior Member, IEEE, Ming Li , and Shaofeng Jiang Abstract-Even though optical flow approaches based on convolutional neural networks have achieved remarkable performance with respect to both accuracy and efficiency, large displacements and motion occlusions remain challenges for most existing learning-based models. To address the abovementioned issues, we propose in this paper a self-attention-based multiscale feature learning optical flow computation method with occlusion feature map prediction. First, we exploit a self-attention mechanism-based multiscale feature learning module to compensate for large displacement optical flows, and the presented module is able to capture long-range dependencies from the input frames. Second, we design a simple but effective self-learning module to acquire an occlusion feature map, in which the predicted occlusion map is utilized to correct the optical flow estimation in occluded areas. Third, we explore a hybrid loss function that integrates the photometric and smoothness losses into the classical endpoint error (EPE)-based loss to ensure the accuracy and robustness of the presented network. Finally, we compare the proposed method with some state-of-the-art approaches using the MPI-Sintel and KITTI test databases. The experimental results demonstrate that the proposed method achieved competitive performance with respect to both accuracy and robustness, and it produced the better results compared to other methods under large displacements and motion occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E STIMATING the optical flow from consecutive frames is a core research topic in image processing and computer vision, and it has been applied to many high-level vision tasks: robot navigation and obstacle avoidance <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b1">[2]</ref>, unmanned direction of aerial vehicles <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b3">[4]</ref>, autonomous driving <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b5">[6]</ref>, posture estimation and recognition <ref type="bibr" target="#b6">[7]</ref>-8], multimedia coding and transmission <ref type="bibr" target="#b9">[9]</ref>- <ref type="bibr" target="#b10">[10]</ref>, medical image analysis <ref type="bibr" target="#b11">[11]</ref>, and many other areas <ref type="bibr" target="#b12">[12]</ref>- <ref type="bibr" target="#b13">[13]</ref>.</p><p>Regarding past studies, the variational optical flow method derived from Horn and Schunck <ref type="bibr" target="#b14">[14]</ref> was dominant because it can produce a dense flow field and can easily be improved by applying various optimizing strategies to the basic energy function <ref type="bibr" target="#b15">[15]</ref>- <ref type="bibr" target="#b19">[18]</ref>. However, minimizing a complex energy function usually causes expensive computational costs, which may prevent optical flow estimation from being used in real-time applications.</p><p>Recently, inspired by the successes of convolutional neural networks (CNNs) in many vision-related tasks <ref type="bibr" target="#b20">[19]</ref>- <ref type="bibr" target="#b21">[20]</ref>, the deep learning approach has become increasingly popular in optical flow computations, and CNN-based methods have outperformed the variational models in many public optical flow benchmarks, e.g., MPI-Sintel and KITTI. Although CNN-based methods have achieved remarkable performance with respect to both accuracy and efficiency, large displacements and occlusions remain challenges for most of the existing learning-based models because either large displacements or occlusions may cause networks to produce obvious errors in the predicted flow fields.</p><p>To improve the accuracy and robustness of optical flow estimation under large displacements and occlusions, we propose in this paper a self-attention-based multiscale feature learning optical flow computation method with occlusion map detection. The experimental results demonstrated that our method has high accuracy for optical flow computations, and it achieved robust outcomes under large displacements and occlusions. Our main contributions are highlighted as follows.</p><p>r We adopt the self-attention mechanism to construct a mul- tiscale feature learning network for optical flow estimation, and the proposed model is able to capture the long-range dependencies to compensate for the large displacement optical flow.</p><p>r We exploit a self-learning-based occlusion prediction mod- ule to produce the normalized occlusion feature map, and we utilize the yielded occlusion feature map to correct the optical flow in occluded areas.</p><p>r To ensure good performance of the presented method, we explore a hybrid loss function by combining the photometric and smoothness losses with classic EPE loss. The presented hybrid loss promotes the proposed optical flow network to be accurate and robust under large displacements, occlusions, and image noise. The remainder of this paper is organized as follows. Section II briefly reviews the previous work. In Section III, we describe the proposed self-attention-based multiscale feature learning optical flow computation method with occlusion feature map prediction. The experimental results and discussions are presented in Section IV. Finally, we conclude the project in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>As a research focus of image processing and computer vision, numerous studies on optical flow estimation have been reported in recent decades. However, it is beyond the scope of this article to summarize all studies. To provide a direct overview, we only review and discuss the most related studies that focus on dealing with large displacements and occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Variational Optical Flow Estimation</head><p>After the prominent work of Horn and Schunck <ref type="bibr" target="#b14">[14]</ref>, many studies were devoted to improving the computational accuracy of variational optical flows by applying various image constraint assumptions to the data term <ref type="bibr" target="#b22">[21]</ref>- <ref type="bibr" target="#b23">[22]</ref> or modifying the flow diffusion strategy of the regularization term <ref type="bibr" target="#b24">[23]</ref>- <ref type="bibr" target="#b26">[25]</ref>. However, most of these improved variational models are incapable of dealing with large displacements and occlusions.</p><p>To overcome the challenge of large displacements, Brox et al. <ref type="bibr" target="#b27">[26]</ref> exploited an image pyramid-based optical flow method by using a coarse-to-fine warping strategy, and they found that the proposed method resulted in a significant improvement in large displacement optical flow estimation compared with the previous variational approaches. Afterward, Sun et al. <ref type="bibr" target="#b28">[27]</ref> studied a non-local total variation optical flow model with the L 1 norm. By replacing the non-local and coupling terms with weighted median filtering to minimize the classical objective function, optical flow can be estimated using the coarse-to-fine warping program. Additionally, some publications <ref type="bibr" target="#b29">[28]</ref>- <ref type="bibr" target="#b30">[29]</ref> recommended using a patch matching strategy to address the issues related to large displacement motion. For instance, Brox and Malik <ref type="bibr" target="#b31">[30]</ref> incorporated a descriptor matching term into the classic objective function to improve the optical flow estimation performance for large displacement areas. However, the local descriptor is not reliable in all image regions. In addition, Chen et al. <ref type="bibr" target="#b32">[31]</ref> presented a fusion flow field framework by using a post-processing scheme to merge the variational optical flow with an approximate nearest neighbor field. Their method achieved excellent results for large and small displacements. Moreover, Bailer et al. <ref type="bibr" target="#b33">[32]</ref> constructed a dense correspondence-based flow field estimation approach using regular grid optimization, and they found that the proposed model could remove outliers and improve the accuracy of the flow field under large displacements.</p><p>It is clear that occlusions are major challenges for variational optical flow estimation since image data are based on constant assumptions, such as the brightness constant and gradient constant, which may be invalid in occluded areas. To overcome the influence of motion occlusions, Xiao et al. <ref type="bibr" target="#b34">[33]</ref> modeled a multi-cue driven adaptive bilateral filter to improve the variational optical flow computation, and the presented method utilized the detected motion discontinuities to optimize the flow diffusion. To utilize the occlusion information to improve the flow field estimation performance, several studies <ref type="bibr" target="#b35">[34]</ref>- <ref type="bibr" target="#b36">[35]</ref> incorporated the occlusion terms into the objective function of the optical flow, which can simultaneously estimate the optical flow and occlusions. For example, Hur et al. <ref type="bibr" target="#b37">[36]</ref> utilized forward-backward consistency and occlusion-disocclusion symmetry to optimize the optical flow estimation and found that the proposed method achieved superior performance on some public evaluation databases. In a different way, several publications <ref type="bibr" target="#b38">[37]</ref> recommended using an interactional scheme to refine the optical flow and occlusions. For instance, Kennedy and Taylor <ref type="bibr" target="#b39">[38]</ref> used regular triangulation to describe occlusion and constructed a multi-frame-based optical flow estimation model using the extracted occlusion map. In addition, Zhang et al. <ref type="bibr" target="#b40">[39]</ref> adopted dynamic regular triangulations to extract the occlusion regions from two consecutive frames, and then they utilized the occlusion information to optimize the variational optical flow with a coarse-to-fine computational program.</p><p>Even though the accuracy and robustness of variational optical flow estimation have been greatly improved in recent years, most variational approaches usually require a large number of iterations to optimize the energy function, which may prevent the use of variational optical flow in real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CNN Based Optical Flow Estimation</head><p>In recent years, with the significant success of convolutional neural networks in many image and vision tasks, CNN-based approaches have become increasingly popular in optical flow estimation. By using the generic U-Net network <ref type="bibr" target="#b41">[40]</ref>, Dosovitskiy et al. <ref type="bibr" target="#b42">[41]</ref> adopted a fully convolutional neural network to construct the first deep learning-based optical flow model named FlowNet, which includes two independent optical flow estimators: FlowNetS and FlowNetC. Using a pre-training process on Flying Chairs datasets, the FlowNet method achieved the real-time computation of the optical flow. However, its performance was still inferior to the state-of-the-art variational approaches.</p><p>Following the remarkable study of Dosovitskiy et al. <ref type="bibr" target="#b42">[41]</ref>, Ilg et al. <ref type="bibr" target="#b43">[42]</ref> presented a FlowNet2 method by stacking several FlowNetS and FlowNetC networks into a larger model. With a careful training procedure, the FlowNet2 model achieved competitive performance that was compared with the state-of-the-art approaches. However, the large-scale network may be more prone to the issue of overfitting. To form a small network that can handle large displacements, Ranjan and Black <ref type="bibr" target="#b44">[43]</ref> exploited a spatial pyramid network by combining a classical spatial pyramid formulation with a convolutional network. Even though their SpyNet method notably decreases the network size, the accuracy of SpyNet is worse than that of the FlowNet2 model. To balance the prediction accuracy and network size, Hui et al. <ref type="bibr" target="#b45">[44]</ref> constructed a lightweight cascaded network using the image pyramid with warping, which was called LiteFlowNet. Their framework performed on par with the FlowNet2 method, but it had a much smaller model size and a faster running speed.</p><p>To gain superior performance on large displacements and occlusions, some works <ref type="bibr" target="#b46">[45]</ref>- <ref type="bibr" target="#b48">[47]</ref> recommended using a convolutional network as an embedded module in their optical flow frameworks. For example, Bailer et al. <ref type="bibr" target="#b49">[48]</ref> adopted a Siamese network to learn the CNN features for patch matching with a tailored loss function, and then they estimated the flow field by using the FlowFields approach <ref type="bibr" target="#b33">[32]</ref> to optimize the obtained CNN features. As a result, their FlowFieldCNN method produced results that compared competitively with the state-of-the-art approaches, and it achieved better performance for large displacement motion. Moreover, Xu et al. <ref type="bibr" target="#b50">[49]</ref> used CNN features to learn a full cost volume and applied post-processing to regularize the flow field. Afterwards, Bao et al. <ref type="bibr" target="#b51">[50]</ref> utilized the DCFlow model as a component of a Kalman filtering-based refining system. By using long video sequences to couple the system, the presented refining framework was able to improve the robustness of the flow field. Even though these hybrid optical flow models gained superior performance on large displacements, as evidenced by the MPI-Sintel and KITTI benchmarks, they did not function in real-time and required complex computation processes.</p><p>Recently, Sun et al. <ref type="bibr" target="#b52">[51]</ref> explored a compact but effective network for optical flow, which was named PWC-Net. By combining well-established principles, including feature pyramid program, warping and cost volume, the PWC-Net model outperformed the state-of-the-art approaches on both MPI-Sintel final pass and KITTI 2015 benchmarks, and it especially achieved excellent results in large displacement regions. Due to the remarkable benefits associated with its smaller size and high accuracy, many subsequent studies <ref type="bibr" target="#b53">[52]</ref>- <ref type="bibr" target="#b55">[54]</ref> utilized PWC-Net as a foundation to construct a larger network for optical flow prediction. For instance, Neoral et al. <ref type="bibr" target="#b56">[55]</ref> integrated occlusion estimation into the PWC-Net model to regulate the optical flow in occluded regions. Although their ContinualFlow network achieved the top performance on the MPI-Sintel final datasets, it needed multiple frames to refine the flow field. Later, Hur and Roth <ref type="bibr" target="#b57">[56]</ref> constructed an iterative residual refinement scheme to optimize the PWC-Net model by combining occlusion prediction and optical flow estimation. The proposed IRR-PWC network achieved excellent performance on large displacements and occlusions, but it required a truth occlusion map in model training. Although some studies <ref type="bibr" target="#b58">[57]</ref>- <ref type="bibr" target="#b59">[58]</ref> recommended using an unsupervised learning-based network to estimate the optical flow and occlusion map, the accuracies of these methods are still below those of supervised learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Overview</head><p>After the remarkable work of Sun et al. <ref type="bibr" target="#b52">[51]</ref>, many of the subsequent approaches utilized the pyramid, warping, and cost volume model to construct new convolutional networks for the improvement of optical flow estimation performance. However, most of the existing methods tend to be poor when large displacements and occlusions exist. To tackle the challenges of large displacements and occlusions in this article, we exploit a self-attention-based multiscale feature learning optical flow estimation approach with occlusion feature map prediction. The overall architecture of the proposed method is displayed in Fig. <ref type="figure" target="#fig_1">1</ref>.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, we utilize a normal PWC-Net model <ref type="bibr" target="#b52">[51]</ref> as the foundation of the proposed method, where the original PWC-Net network mainly includes a feature pyramid with a warping operation, a cost volume, an optical flow estimator and a context network. Among the components of the PWC-Net model, the feature pyramid with the warping operation is employed to overcome the issues related to lighting changes and large motion, and the cost volume is used to represent the optical flow. The optical flow estimator is constructed by a multilayer Dense CNN <ref type="bibr" target="#b60">[59]</ref>, which uses the cost volume, the features of the reference frame and the upsampled flow field as its inputs, and it outputs the estimated flow field at the current pyramid layer. Finally, the context network is utilized to extract the contextual information to refine the output flow field.</p><p>To improve the optical flow estimation performance when large displacements exist, we design a self-attention-based multiscale feature learning module called SAMFL to capture the residual information from the optical flows and the pyramid features of each pyramid layer, where the captured long-range dependencies are able to compensate for the large displacement optical flow. Moreover, we construct an occlusion feature map detection operator based on a self-learning scheme, and the predicted occlusion feature map is concatenated with the cost volume to correct the optical flow estimation in occlusion regions. Furthermore, we exploit a hybrid loss function by combining the smoothness and photometric loss with the classic EPE loss to improve the accuracy and robustness of the proposed system.</p><p>The detailed ideas of our contributions, including the selfattention-based multiscale feature learning module, the occlusion feature map detection operator and the novel loss function, are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-Attention-Based Multiscale Feature Learning Module</head><p>Even though the PWC-Net <ref type="bibr" target="#b52">[51]</ref> method adopted a feature pyramid with warping and a dilated-convolution-based context network to improve the optical flow estimation performance in regions with large displacements, it ignores some important long-range dependencies of the different-scale features, which may cause the PWC-Net method to fall into a suboptimal result in regions with large displacements, especially near image and motion boundaries.</p><p>Because the attention mechanism has been proven to be an effective way to capture long-range dependencies from various scale features in many previous studies <ref type="bibr" target="#b61">[60]</ref>- <ref type="bibr" target="#b63">[61]</ref>, we exploit a self-attention-based multiscale feature learning module called SAMFL to capture the long-range dependencies of multiscale features. Here, the obtained long-range dependencies are used to compensate for the optical flow, especially in the regions with   First, we fuse the estimated flow field and the corresponding features of the second last layer from the optical flow estimator of the second level pyramid by using a 3 × 3 convolutional operation. Then, we use average pooling to make the size of the fused features match the size of the features of the second last layer from the optical flow estimator of the different upper pyramid layers. Afterwards, we concatenate the resized fusion features with the corresponding features of the second last layer from the optical flow estimator of the upper pyramid layers to produce the concatenated features of the various pyramid layers.</p><p>Let X → R C×HW denote the concatenated feature of a pyramid layer, where C, H and W respectively indicate the number of channels, the width, and the height of the concatenated feature. To transform the concatenated feature X into an attention map Y → R C×HW , we first transform the concatenated feature X into three embeddings α → R C×HW , β → R C×HW and η → R C×HW by using 1 × 1 convolutions. Then, we calculate the attention energy ω → R HW ×HW using the following equation:</p><formula xml:id="formula_0">ω = α T ⊗ β (1)</formula><p>where ⊗ indicates the matrix multiplication operation. Afterwards, we normalize the computed attention energy ω through a softmax operation, as shown in the following:</p><formula xml:id="formula_1">ωi,j = exp(ω i,j ) HW i=1,j=1 exp(ω i,j )<label>(2)</label></formula><p>Given the normalized attention energy ω, the component of each row in ω represents the energy value of one pixel associated with other pixels. Thus, we use the normalized attention energy ω to produce the feature map Y by multiplying it by the embedding η, as shown in the following:</p><formula xml:id="formula_2">Y = η ⊗ wT<label>(3)</label></formula><p>To encourage the attention feature map Y to learn the cues from a local region to the global area for the input concatenated feature X, we warp the attention feature map Y by using a residual connection <ref type="bibr" target="#b64">[62]</ref> between the input feature map X and the attention feature map Y ; the output self-attention feature map Ỹ can be expressed as shown below:</p><formula xml:id="formula_3">Ỹ = λY + X (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where Ỹ is the output self-attention feature map, which has the same shape as the input concatenated feature map X. The symbol λ represents a learning factor in the training process that is used for controlling the self-attention energy of the output self-attention feature map. With the self-attention feature maps of the different pyramid layers, we upsample and concatenate the multiscale selfattention feature maps to produce a residual flow field that includes the long-range dependencies of the different-scale features. Finally, we fuse the residual flow field with the original flow field of the bottom pyramid layer to compensate for the optical flow in areas with large displacements.</p><p>To show the benefits of the proposed self-attention-based multiscale feature learning module for improving optical flow, we utilize the temple_3 dataset of the MPI-Sintel database to test the optical flow estimation performance of the proposed SAMFL module. Fig. <ref type="figure" target="#fig_3">3</ref> displays the original flow field of the PWC-Net method and the developed flow field using the proposed SAMFL module. The comparison of the original and developed flow fields demonstrates that the proposed SAMFL module is able to improve the optical flow accuracy, especially in regions with large displacements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Occlusion Feature Map Prediction</head><p>It is clear that motion occlusion is a significant challenge for most of the current CNN-based optical flow methods because occluded pixels will be invalid in consecutive frames. To predict robust optical flows in occluded regions, some studies <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b56">[55]</ref> applied supervised networks to detect occlusions, and they used post-processing with the obtained occlusion information to refine the optical flows. However, these methods require a large number of ground truths to train the occlusion detection networks. In contrast, other methods <ref type="bibr" target="#b53">[52]</ref>, <ref type="bibr" target="#b59">[58]</ref> used unsupervised networks to extract the occluded pixels from the input frames. Nonetheless, these unsupervised approaches usually need multiple frames to predict the occlusions.</p><p>To achieve accurate and convenient occlusion detection, we exploit a self-learning module to predict the occlusion feature map by using the warping errors between the input and the warped features in the feature pyramid. The proposed unsupervised occlusion detection module only needs two frames to generate a normalization occlusion feature map. Fig. <ref type="figure" target="#fig_4">4</ref>(a) illustrates the architecture of the proposed occlusion feature map detection module. Given the input and warped features, we first subtract warping feature 2 from feature 1 to determine the warping errors. Then, we use two convolutions and a leaky ReLU operation to  extract the occlusion features of pixels from the feature warping errors. These multiple convolution operations allow the occlusion detection procedure to reach a larger receptive field. Finally, we utilize a sigmoid activation function to produce the occlusion feature map by normalizing the outputs in the range of [0, 1].</p><p>Fig. <ref type="figure" target="#fig_5">5</ref>(a), (b) and (c) display the input frames, the occlusion truth and the estimated occlusion feature map of the temple_3 dataset, respectively. Unlike the binary occlusion map used by the previous occlusion detection methods, the proposed occlusion detection module produces a normalized occlusion map in which the occlusion indicator of each pixel is normalized in the range of [0, 1]. Specifically, a gray value of zero in the estimated occlusion feature map denotes that a pixel appeared in the reference frame and disappeared in the next frame. In contrast, a gray value of one denotes that a pixel disappeared in the reference frame and appeared in the next frame. Thus, the gray value of a pixel close to zero or one indicates that the pixel is prone to be occluded in consecutive frames. To directly illustrate the occlusion detection results of the proposed method, we use a threshold to segment the estimated occlusion feature map, and the corresponding binary occlusion map is shown in Fig. <ref type="figure" target="#fig_5">5(d)</ref>.</p><p>As seen in Fig. <ref type="figure" target="#fig_5">5(d</ref>), the proposed method can detect most of the occlusion regions from the input frames, and it demonstrates that the presented self-learning module of the occlusion feature map prediction achieves good performance on occlusion detection.</p><p>To improve the accuracy and robustness of an optical flow under motion occlusion, we apply the proposed occlusion detection module in the PWC-Net model. As shown in Fig. <ref type="figure" target="#fig_4">4(b)</ref>, we predict the occlusion feature map by using the warping errors at each feature pyramid level, and then we use the estimated occlusion feature map to regularize the cost volume computation. Finally, the optical flow of each pyramid level is estimated using the optimized cost volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hybrid Training Loss</head><p>In the current studies of supervised learning optical flow, most of the existing methods use the endpoint error (EPE) metric to measure the L 2 distance between the ground truth and the predicted optical flow during the network training process <ref type="bibr" target="#b42">[41]</ref>- <ref type="bibr" target="#b43">[42]</ref>, which is calculated as follows:</p><formula xml:id="formula_5">L EP E = x∈Ω |w E (x) − w G (x)| 2<label>(5)</label></formula><p>where L EP E indicates the EPE loss and | | 2 represents the L 2 norm of a vector. The symbols w E (x) and w G (x) respectively denote the estimated optical flow and ground truth at pixel x, and Ω represents the image domain.</p><p>Even though the EPE-based loss function enables optical flow networks to achieve high learning rates, it may cause the optical flow network to fall into a suboptimal result since the L 2 distance is difficult to minimize in regions with large displacements or motion occlusions <ref type="bibr" target="#b52">[51]</ref>. Inspired by how the traditional variational optical flow methods deal with illumination changes, large displacements and occlusions, some unsupervised learning optical flow studies <ref type="bibr" target="#b65">[63]</ref>- <ref type="bibr" target="#b66">[64]</ref> adopted a variational energy function that consists of a photometric and a smoothness loss to train the networks. To ensure a high learning rate and achieve accurate optical flow prediction under large displacements and motion occlusion, we exploit a hybrid training loss by combining the EPE, photometric and smoothness loss functions to train the proposed network.</p><p>The photometric loss L photometric of the warped second frames using the predicted flow field and ground truth can be expressed as follows:</p><formula xml:id="formula_6">L photometric = x∈Ω f (I(x + w G (x), t + 1), I(x + w E (x), t + 1)) (6)</formula><p>where I(x + w G , t + 1) and I(x + w E , t + 1) represent the brightness values of pixel x in the second frame warped by the ground truth w G and the predicted flow field w E , respectively. The symbol f (x, y) indicates a robust penalty function, which can be expressed as follows:</p><formula xml:id="formula_7">f (x, y) = τ • (x − y) 2 , if x − y &lt; θ θ • x − y − τ, otherwise<label>(7)</label></formula><p>where τ and θ are the adjustment parameters, and we set τ = 0.5 and θ = 1 according to the literature <ref type="bibr" target="#b67">[65]</ref>. With the robust penalty function, the loss function with the L 2 norm can carefully tune the learning rates when the ground truth and predicted flow field are similar. In contrast, the loss function with the L 1 norm results in a high learning rate when the ground truth and predicted flow field diverge.</p><p>As a supplemental term for regularizing the photometric loss, the smoothness loss L Smoothness is defined as follows:</p><formula xml:id="formula_8">L Smoothness = x∈Ω f (H x (w E (x)) , H x (w G (x))) +f (H y (w E (x)) , H y (w G (x))) (8)</formula><p>where H x (.) and H y (.) denote the horizontal and vertical gradient operators, respectively. The notation f (x, y) is a robust penalty function, as shown in Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><p>Next, we combine the EPE, photometric and smoothness loss functions to construct a hybrid training loss, as shown in the following:</p><formula xml:id="formula_9">L hybrid = L EP E + βL photometric + λL Smoothness (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>where β and λ indicate the weights of the photometric and smoothness losses, respectively, which are employed to balance the effects of the three components in Eq. ( <ref type="formula" target="#formula_9">9</ref>).</p><p>To achieve a high learning rate, we use a multiscale training loss during the training process by referring to PWC-Net <ref type="bibr" target="#b52">[51]</ref>, as shown in the following:</p><formula xml:id="formula_11">L hybrid = L l=l 0 α l L l hybrid + η |Θ| 2 2<label>(10)</label></formula><p>where L l hybrid represents the loss with a weight α l at the l th pyramid level. The notation Θ denotes the set of all the learnable parameters in our network. In Eq. ( <ref type="formula" target="#formula_11">10</ref>), the second term with the adjustment factor η regularizes the parameters of the proposed network.</p><p>In the fine-tuning program, we replace the robust penalty function in Eq. ( <ref type="formula" target="#formula_7">7</ref>) with a generalized Charbonnier penalty function ρ(x, y) = ((x − y) 2 + ε 2 ) κ to suppress the influence of the outliers in the estimated flow field, in which ε = 0.001, and κ = 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Databases and Error Metrics</head><p>We respectively test our network on the MPI-Sintel and KITTI datasets and conduct a comprehensive comparison with several state-of-the-art approaches.</p><p>The MPI-Sintel database <ref type="bibr" target="#b68">[66]</ref> is a challenging benchmark that includes large displacements, motion occlusions and complex scenes, and it offers both training and test datasets to evaluate the various optical flow approaches. For the quantitative evaluation of the MPI-Sintel datasets, we use the AEPE all, AEPE matched and AEPE unmatched metrics from the MPI-Sintel online benchmark to compare the optical flow estimation results. Specifically, the AEPE all, AEPE matched and AEPE unmatched metrics represent the results of the AEPE over complete frames, the regions that remain visible in adjacent frames and the regions that are visible only in one of two adjacent frames, respectively. Moreover, we use the "d0-10", "d10-60", and "d60+" metrics to indicate the optical flow performance for motion occlusion, and we employ the "s0-10", "s10-40", and "s40+" metrics to make specific comparisons for large displacements. The "d0-10", "d10-60" and "d60+" metrics represent the endpoint errors over regions close to occlusion boundaries at different distances, and the "s0-10", "s10-40" and "s40+" metrics denote the endpoint errors for the regions with different velocities per frame.</p><p>The KITTI database <ref type="bibr" target="#b69">[67]</ref> consists of a 2012 set and 2015 set, and it was produced by using a moving camera to capture outdoor scenes in streets, which commonly contain large displacements, illumination changes and occlusions. For the KITTI 2012 datasets, we utilize the AEPE and flow error (Fl) metrics of all pixels and the non-occluded pixels to assess the optical flow estimation performance, where the flow error is measured by calculating the statistics of the pixels with an endpoint error greater than 3 pixels. For the KITTI 2015 datasets, we use the "Fl-all", "Fl-fg", and "Fl-bg" metrics of all pixels and the non-occluded pixels to compare the performances of various optical flow methods, where the "Fl-all", "Fl-fg", and "Fl-bg" metrics denote the flow error (Fl) for all pixels (all), foreground pixels (fg) and background pixels (bg), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We adopt a six-level feature pyramid to conduct an accurate optical flow estimation; the weights α l in Eq. ( <ref type="formula" target="#formula_11">10</ref>) are set to α 6 = 0.32, α 5 = 0.08, α 4 = 0.002, α 3 = 0.01, and α 2 = 0.005 at the various pyramid levels based on PWC-Net <ref type="bibr" target="#b52">[51]</ref>. In addition, the adjustment factor η in Eq. ( <ref type="formula" target="#formula_11">10</ref>) is fixed as η = 0.0004 to prevent the proposed network from overfitting. To balance the effects of the EPE, photometric and smoothness loss functions when training our network, we set the weights β and λ of the photometric and smoothness losses to be λ = 0.5 and β = 0.8 <ref type="bibr" target="#b65">[63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Training:</head><p>We first train our network on the FlyingChairs dataset with a learning rate of 1e-4 for 400 k iterations, and then we divide it by two every 200 k iterations after the first 400 k iterations. Afterwards, we train the proposed network on the FlyingThings3D dataset with a learning rate of 1e-5 for 100 k iterations, and then we divide it by two every 200 k iterations after the first 100 k iterations. In the training process, we use the same data augmentation schemes, including translation, rotation, scaling, Gaussian noise, brightness changes, contrast, gamma and color, as those in PWC-Net <ref type="bibr" target="#b52">[51]</ref>. We crop 448×384 patches with a batch size of 8 and 768×384 patches with a batch size of 4 on the FlyingChairs and FlyingThings3D datasets, respectively.</p><p>Fine-tuning: To fine-tune our network on the MPI-Sintel dataset, we crop 768×384 image patches with a batch size of 4, and we add horizontal flips and remove the additive noise during the data augmentation. When fine-tuning on the KITTI dataset, we crop 896×320 image patches with a batch size of 4 since the large patches can better capture the large motion in the KITTI dataset. The learning rate and data augmentation are set to be the same as those for the MPI-Sintel dataset. Because the ground truth of the KITTI dataset is semidense, we upsample the estimated flow field at the quarter resolution to compare it with the scaled ground truth at the full resolution, and we remove the invalid pixels before computing the loss function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison Results of the Occlusion Prediction</head><p>To evaluate the performance of our self-learning occlusion detection scheme, we run our method on the MPI-Sintel training datasets to compare the occlusion prediction results of our method with those of the FlowNet3 <ref type="bibr" target="#b48">[47]</ref> and UnFlow <ref type="bibr" target="#b66">[64]</ref> methods. Specifically, the FlowNet3 method integrates a supervised learning-based occlusion estimation scheme with an optical flow network to estimate the occlusions and optical flows simultaneously. The UnFlow method utilizes a forward/backward consistency checking framework to detect occlusions and then designs an unsupervised learning network to estimate optical flows.</p><p>Table <ref type="table" target="#tab_0">I</ref> summarizes the F-Measure results of our method and the other comparison methods evaluated on the MPI-Sintel datasets, in which the F-Measure metric indicates the percentage of the correctly estimated occlusion pixels over the occlusion mask. As shown in Table <ref type="table" target="#tab_0">I</ref>, the FlowNet3 method produces the best results on both the MPI-Sintel clean and final passes. However, it requires a large amount of labeled data to supervise the network to achieve a good result. This may prevent the FlowNet3 method from a broad application to real-world data where the ground truth of occlusions is not easily accessible. Although our method results in a lower performance compared with the FlowNet3 method, it outperforms the classical forward/backward consistency checking scheme used in the Un-Flow method. This indicates that the proposed occlusion prediction method has a good capacity for occlusion detection.</p><p>To make a visual comparison, Fig. <ref type="figure" target="#fig_7">6</ref> shows the occlusion detection results of the various comparison methods evaluated on some MPI-Sintel datasets. As seen in Fig. <ref type="figure" target="#fig_7">6</ref>, the FlowNet3 method yields good results in both the large occlusion areas and the detailed occlusion regions. Although the UnFlow method extracts the main occlusion regions, its results include many errors. Although the proposed method may miss some details, it is able to capture most of the large occlusion areas. In addition, our occlusion detection method does not require the labeled occlusion maps to train the model parameters, which encourages the proposed occlusion detection module to be conveniently incorporated into the presented optical flow estimation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison Results Using the MPI-Sintel Benchmark</head><p>We first run our method on the MPI-Sintel test dataset to conduct a detailed comparison with the following state-of-theart approaches: FlowNet2 <ref type="bibr" target="#b43">[42]</ref>, ProFlow <ref type="bibr" target="#b38">[37]</ref>, FlowFieldsCNN <ref type="bibr" target="#b49">[48]</ref>, PWC-Net <ref type="bibr" target="#b52">[51]</ref>, ConFlow_ROB <ref type="bibr" target="#b56">[55]</ref>, LiteFlowNet <ref type="bibr" target="#b45">[44]</ref>, and FlowNet3 <ref type="bibr" target="#b48">[47]</ref>. FlowNet2 is constructed by stacking several FlowNetC and FlowNetS models, and it achieves superior results on the MPI-Sintel database. ProFlow combines the forward   and backward motion from multiple frames to predict the refined optical flow, which improves the optical flow performance for occlusions. FlowFieldsCNN estimates the flow field by using patch matching to optimize the CNN features, which can achieve good results for large displacements. PWC-Net adopts the feature pyramid, warping and cost volume to deal with large displacements, and it produces the best performance on several databases for the first time of submission. ConFlow_ROB integrates occlusion estimation into the PWC-Net model, which can improve the optical flow performance in regions of occlusions. LiteFlowNet constructs a lightweight cascaded network by using the image pyramid with warping, which is smaller and faster than the other approaches. Finally, FlowNet3 method joins a supervised learning-based occlusion estimation module with an optical flow network to predict optical flows and occlusions simultaneously.</p><p>Table <ref type="table" target="#tab_1">II</ref> shows comparisons of AEPE all, AEPE matched, and AEPE unmatched of our method and those of the other evaluated approaches on the MPI-Sintel test datasets. As seen in Table <ref type="table" target="#tab_1">II</ref>, the proposed method achieves competitive results compared with the other state-of-the-art approaches. Although the ProFlow, PWC-Net and FlowNet3 methods achieve better performance on the clean pass compared with our method, the proposed method performs significantly better than these methods on the final pass. Because the final pass of the MPI-Sintel database includes more image noise, specular reflections, motion blur, defocus blur and atmospheric effects, a good performance on the final pass indicates a robust optical flow estimation result. Therefore, the comparison results demonstrate that the proposed method is more robust to image noise than the other state-of-the art methods.</p><p>To make a straightforward quantitative comparison of the optical flows in the areas of large displacements and motion occlusions, Table <ref type="table" target="#tab_2">III</ref> summarizes the comparison results of some specific metrics between our method and the other state-of-the-art approaches on the MPI-Sintel test datasets. As seen in Table <ref type="table" target="#tab_2">III</ref>, the proposed method performs the best results for the "d10-60" and "s10-40" metrics, and it achieves the second best performance on the "d0-10", "d60-140", and "s40+" metrics. Because the metrics of "d0-10", "d10-60", and "d60-140" and the metrics of "s0-10", "s10-40", and "s40+" reflects the performance of optical flows in regions of motion occlusions and large displacements, respectively. The specific comparison results in Table <ref type="table" target="#tab_2">III</ref> demonstrate that the proposed optical flow estimation method has significant benefits in dealing with motion occlusions and large displacements. Although the FlowNet3 method  achieves a better result on occlusion detection than our method, the proposed method performs better on the flow field estimation, especially in the regions of motion occlusions and large displacements.</p><p>For a qualitative comparison, Fig. <ref type="figure" target="#fig_8">7</ref> displays the reference frames, ground truths, and estimated flow field results of the comparison approaches evaluated on several MPI-Sintel test datasets, where the red squares indicate several regions that include motion occlusions and large displacements. To perform a direct visual comparison of the optical flow performance in areas with large displacements and motion occlusions, we further illustrate the close-up views within the red squares in Fig. <ref type="figure" target="#fig_9">8</ref>. As seen from the amplified pictures in Fig. <ref type="figure" target="#fig_9">8</ref>, the presented method produces the best performance among all evaluated approaches because the optical flow results of the presented method well preserve the image edges and motion boundaries in regions with motion occlusions and large displacements. The visual comparison results from Fig. <ref type="figure" target="#fig_8">7</ref> and Fig. <ref type="figure" target="#fig_9">8</ref> demonstrate that the proposed method is able to easily deal with large displacements and motion occlusions in comparison to the other state-of-the-art approaches.</p><p>To make a clearer visual comparison between the proposed method and the other state-of-the-art methods, we show the flow error maps of the various comparison methods evaluated on the MPI-Sintel test datasets in Fig. <ref type="figure" target="#fig_10">9</ref>, where the red squares indicate the areas that include motion occlusions and large displacements. Moreover, Fig. <ref type="figure" target="#fig_11">10</ref> displays the amplified views of the motion occlusion and large displacement areas. As illustrated in Fig. <ref type="figure" target="#fig_10">9</ref> and Fig. <ref type="figure" target="#fig_11">10</ref>, our method performs better than the other methods, especially in the regions of motion occlusions and large displacements. The comparison results of flow error maps further demonstrate that the proposed method is capable of coping with motion occlusions and large displacements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison Results Using the KITTI Benchmark</head><p>To assess optical flow estimation performance in real-world scenes, we respectively run our method on the KITTI 2012 and KITTI 2015 test datasets to conduct a further comparison with some state-of-the-art approaches. Because the performance of the ConFlow_ROB method was not evaluated on the KITTI 2012 test datasets, we only compare our method with the ConFlow_ROB approach on the KITTI 2015 test datasets and compare the proposed method with the other state-of-the-art approaches on both the KITTI 2012 and KITTI 2015 test datasets.</p><p>Table IV lists the comparison results of the proposed method and the other evaluated approaches tested on the KITTI 2012 datasets, and it indicates that the proposed method performs the best among all the evaluated approaches. Because the KITTI 2012 datasets only include static backgrounds and contain few large displacements and motion occlusions, the comparison results of the various approaches based on the KITTI 2012 test   datasets are not sufficient to show the benefits of the presented method in dealing with large displacements and motion occlusions.</p><p>To make a more convincing comparison, Table <ref type="table" target="#tab_4">V</ref> summarizes the quantitative comparison results of our method and the other state-of-the-art approaches evaluated on the KITTI 2015 test datasets. As seen in Table <ref type="table" target="#tab_4">V</ref>, the presented method outperforms the current state-of-the-art approaches in all the metrics, and it especially achieves significant improvements on the three measurements for all pixels. Because the Fl-bg, Fl-fg, and Fl-all metrics for all pixels measure the endpoint errors of both the occluded and non-occluded pixels, the better results for the metrics on all pixels indicate the better optical flow performance for large displacements and motion occlusions. The quantitative comparison results in  method is accurate and robust for real-world scenes, and it is dramatically better at dealing with large displacements and motion occlusions.</p><p>To make a qualitative comparison between our method and the other state-of-the-art approaches on the KITTI benchmark, Fig. <ref type="figure" target="#fig_12">11</ref> displays the reference frames and the flow error maps of the evaluated methods on several KITTI 2015 datasets, where the white squares indicate regions that include large displacements and motion occlusions. For a carefully visual examination, Fig. <ref type="figure" target="#fig_13">12</ref> illustrates the amplified pictures of the motion occlusion and large displacement regions. As shown in Fig. <ref type="figure" target="#fig_12">11</ref> and Fig. <ref type="figure" target="#fig_13">12</ref>, our method produces competitive results compared with the other state-of-the-art approaches, and it especially achieves the best performance in regions with large displacements and motion occlusions because its flow errors in these regions are significantly smaller than those of the other methods. The qualitative comparison indicates that the presented method achieves the best results compared with the current state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation Experiment</head><p>To reveal the benefits of each component in our method for improving the optical flow performance, we employ the PWC-Net method <ref type="bibr" target="#b52">[51]</ref> as the baseline model and then respectively incorporate the self-attention-based multiscale feature learning module, the occlusion detection scheme and the hybrid training loss into the baseline model to conduct an ablation experiment.</p><p>Table <ref type="table" target="#tab_6">VI</ref> shows the comparison results of the AEPE of the various ablation models, where Base+SAMFL represents the   As shown in Table <ref type="table" target="#tab_6">VI</ref>, by incorporating the self-attentionbased multiscale feature learning module and the occlusion detection scheme into the baseline model, the flow field errors of the Base+SAMFL, Base+Occ and Base+SAMFL+ Occ models are increased compared with that of the baseline model. However, the other ablation models and the full model perform better than the baseline method. This comparison result indicates that the proposed hybrid training loss is a key component in our method because the traditional EPE loss is incapable of guiding the proposed self-attention-based multiscale feature learning module and the occlusion detection module to be optimized.</p><p>It is noticeable that although the Base+SAMFL+Loss model performs slightly better than the baseline model on the Fly-ingChairs and Sintel-clean datasets, it achieves significantly better performance on the Sintel-final, KITTI 2012 and KITTI 2015 datasets compared with the baseline method. Because the Sintel-final and KITTI 2015 datasets include more image noise and large displacements, this comparison result demonstrates that the combination of the self-attention-based multiscale feature learning module and the hybrid training loss is helpful for improving the robustness of optical flow estimation under image noise and large displacements. Moreover, the Base+Occ+Loss model achieves better performance on all of the evaluation datasets compared with the baseline model, which indicates that the occlusion detection scheme combined with the hybrid training loss is able to significantly improve the accuracy of optical flows. Finally, the full model achieves the best performance among all of the evaluation ablation models. The quantitative comparison results of the various ablation models prove the benefits of each component in our method for improving the accuracy and robustness of optical flow estimation.</p><p>For a visual comparison between the different evaluated models, we display the flow field results of the various models evaluated on the market_5 sequence of the MPI-Sintel final datasets in Fig. <ref type="figure" target="#fig_14">13</ref>. As shown in Fig. <ref type="figure" target="#fig_14">13</ref>, the full model performs significantly better than the other ablation models, especially in regions of motion boundaries. The visual comparison further demonstrates the remarkable benefit of our method in dealing with large displacements and motion occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a self-attention-based multiscale feature learning optical flow estimation method with occlusion feature map prediction to cope with the issues of large displacements and motion occlusions. First, we proposed a self-attention-based multiscale feature learning module to capture the long-range dependencies to compensate for a large displacement optical flow. Second, we implemented a self-learning module for occlusion feature map prediction and utilized the predicted occlusion feature map to regularize the cost volume at each pyramid level. Third, we integrated a photometric loss and a smoothness loss into the classical EPE loss function to form a hybrid loss for network training. Finally, we compared our method with several state-of-the-art approaches using the MPI-Sintel and KITTI test datasets. The experimental results indicate that the proposed method performed better with respect to both accuracy and robustness and that it can deal with the issues of large displacements and motion occlusions in particular.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:10:51 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall architecture of the proposed method.</figDesc><graphic url="image-1.png" coords="4,79.85,66.25,430.17,227.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the proposed SAMFL module. large displacements. Fig. 2 summarizes the architecture of the proposed SAMFL module.First, we fuse the estimated flow field and the corresponding features of the second last layer from the optical flow estimator of the second level pyramid by using a 3 × 3 convolutional operation. Then, we use average pooling to make the size of the fused features match the size of the features of the second last layer from the optical flow estimator of the different upper pyramid layers. Afterwards, we concatenate the resized fusion features with the corresponding features of the second last layer from the optical flow estimator of the upper pyramid layers to produce the concatenated features of the various pyramid layers.Let X → R C×HW denote the concatenated feature of a pyramid layer, where C, H and W respectively indicate the number of channels, the width, and the height of the concatenated feature. To transform the concatenated feature X into an attention map Y → R C×HW , we first transform the concatenated feature X into three embeddings α → R C×HW , β → R C×HW and η → R C×HW by using 1 × 1 convolutions. Then, we calculate the attention energy ω → R HW ×HW using the following equation:</figDesc><graphic url="image-2.png" coords="4,41.44,329.65,243.96,111.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the benefits of the proposed self-attention-based multiscale feature learning module for optical flow. (a) Overlapping frames of the Temple_3 dataset, (b) ground truth, (c) flow field computed by PWC-Net, (d) attention map, (e) residual flow field produced by the proposed SAMFL module, and (f) flow field computed by combining the original and residual flow fields.</figDesc><graphic url="image-3.png" coords="5,47.15,66.80,241.44,156.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the proposed occlusion feature map prediction. (a) The structure of the occlusion feature map prediction method. (b) The architecture of the optical flow network with occlusion feature map detection.</figDesc><graphic url="image-4.png" coords="5,310.17,66.60,240.96,130.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results of the occlusion map prediction. (a) Overlapping frames of the temple_3 dataset, (b) ground truth, (c) predicted occlusion feature map, and (d) binary occlusion map corresponding to the occlusion feature map.</figDesc><graphic url="image-5.png" coords="5,309.66,249.50,242.28,103.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:10:51 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Occlusion detection results of the various comparison methods evaluated on some MPI-Sintel datasets.</figDesc><graphic url="image-6.png" coords="8,86.45,67.00,417.12,209.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Flow field results of the comparison methods evaluated on several MPI-Sintel test datasets.</figDesc><graphic url="image-8.png" coords="9,53.16,496.50,492.00,126.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Visual comparison of the close-up views within the red squares in Fig. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Flow error maps of the various comparison methods evaluated on the MPI-Sintel test datasets, where the red squares indicate the areas including motion occlusions and large displacements. (The black color indicates the small flow error, and the white color indicates the large flow error.).</figDesc><graphic url="image-9.png" coords="10,75.94,66.96,437.76,409.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The close-up views within the red squares in Fig. 9.</figDesc><graphic url="image-10.png" coords="11,53.16,66.08,492.00,139.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Flow error maps of the various comparison methods evaluated on the KITTI2015 test datasets, where the white squares indicate the areas including motion occlusions and large displacements. (The blue color indicates the small flow error, and the red color indicates the large flow error.).</figDesc><graphic url="image-11.png" coords="12,52.44,66.08,484.56,442.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The close-up views within the white squares in Fig. 11.</figDesc><graphic url="image-12.png" coords="12,48.95,576.04,492.00,121.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. A visual comparison of the flow field results of different ablation models on the MPI-Sintel market_5 dataset. baseline model with the self-attention-based multiscale feature learning module, Base+Occ indicates the baseline model with the occlusion detection scheme, Base+Loss denotes the baseline model with the hybrid training loss, Base+SAMFL+Occ represents the baseline model with the self-attention-based multiscale feature learning module and the occlusion detection scheme, Base+Occ+Loss represents the baseline model with the occlusion detection scheme and the hybrid training loss, Base+SAMFL+Loss denotes the baseline model with the selfattention-based multiscale feature learning module and the hybrid training loss, and the full model denotes the baseline model with all three components. Specifically, the various evaluation models are trained on the training datasets of the FlyingChairs and FlyingThings3D databases and are tested on the test datasets of the FlyingChairs database and the training datasets of the MPI-Sintel and KITTI databases, respectively.As shown in TableVI, by incorporating the self-attentionbased multiscale feature learning module and the occlusion detection scheme into the baseline model, the flow field errors of the Base+SAMFL, Base+Occ and Base+SAMFL+ Occ models are increased compared with that of the baseline model. However, the other ablation models and the full model perform better than the baseline method. This comparison result indicates that the proposed hybrid training loss is a key component in our method because the traditional EPE loss is incapable of guiding the proposed self-attention-based multiscale feature learning module and the occlusion detection module to be optimized.It is noticeable that although the Base+SAMFL+Loss model performs slightly better than the baseline model on the Fly-ingChairs and Sintel-clean datasets, it achieves significantly better performance on the Sintel-final, KITTI 2012 and KITTI 2015 datasets compared with the baseline method. Because the Sintel-final and KITTI 2015 datasets include more image noise and large displacements, this comparison result demonstrates that the combination of the self-attention-based multiscale feature learning module and the hybrid training loss is helpful for improving the robustness of optical flow estimation under image noise and large displacements. Moreover, the Base+Occ+Loss model achieves better performance on all of the evaluation datasets compared with the baseline model, which indicates that the occlusion detection scheme combined with the hybrid training loss is able to significantly improve the accuracy of optical flows. Finally, the full model achieves the best performance among all of the evaluation ablation models. The quantitative comparison results of the various ablation models prove the benefits of each component in our method for improving the accuracy and robustness of optical flow estimation.</figDesc><graphic url="image-13.png" coords="13,53.16,66.16,492.00,87.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>F-MEASURE RESULTS OF VARIOUS METHODS EVALUATED ON THE MPI-SINTEL DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>RESULTS OF THE AEPE ALL, AEPE MATCHED, AND AEPE UNMATCHED OF OUR METHOD AND THE OTHER EVALUATED APPROACHES ON THE MPI-SINTEL TEST DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>RESULTS OF THE SPECIFIC METRICS OF OUR METHOD AND THE OTHER EVALUATED APPROACHES ON THE MPI-SINTEL TEST DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>RESULTS OF OUR METHOD AND OTHER APPROACHES EVALUATED ON THE KITTI 2012 TEST DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISON</head><label>V</label><figDesc>RESULTS OF OUR METHOD AND OTHER APPROACHES EVALUATED ON THE KITTI 2015 TEST DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table V demonstrate that the proposed</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>RESULTS OF THE PROPOSED METHOD WITH DIFFERENT MODELING CHOICES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:10:51 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Key Research and Development Program of China under Grant 2020YFC2003800, in part by the National Natural Science Foundation of China under Grants 61866026, 61772255, and 61866025, in part by the Advantage Subject Team Project of Jiangxi Province under Grant 20165BCB19007, in part by the Outstanding Young Talents Program of Jiangxi Province under Grant 20192BCB23011, in part by the National Natural Science Foundation of Jiangxi Province under Grant 20202ACB214007, in part by the Aeronautical Science Foundation of China under Grant 2018ZC56008, and in part by the China Postdoctoral Science Foundation (2019 M650894).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object detection and tracking under occlusion for Objectlevel RGB-D video segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="580" to="592" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient optical flow and stereo vision for velocity estimation and obstacle avoidance on an autonomous pocket drone</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Croon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Wagter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rob. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1070" to="1076" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time traffic flow parameter estimation from UAV video based on ensemble classifier and optical flow</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="64" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Obstacle detection and collision avoidance for a UAV with complementary low-cost sensors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gageik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Benz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Montenegro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="599" to="609" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Illumination robust video foreground prediction based on color recovering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="637" to="652" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video object segmentation via dense trajectories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2225" to="2234" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision-Based fingertip tracking utilizing curvature points clustering and hash model representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1730" to="1741" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Authorized licensed use limited to: Tsinghua University</title>
		<imprint/>
	</monogr>
	<note>Downloaded on December 31,2022 at 08:10:51 UTC from IEEE Xplore. Restrictions apply</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1913" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online video stream abstraction and stylization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1286" to="1294" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An optical flow-based full reference video quality assessment algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Manasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2480" to="2492" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An unsupervised learning model for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9252" to="9260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video aesthetic quality assessment by temporal integration of photo-and motion-based features</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast HEVC inter CU selection method based on pyramid motion divergence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="559" to="564" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Illumination invariant optical flow using neighborhood descriptors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Galbrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understanding</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optical flow with semantic segmentation and localized layers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Weighted local intensity fusion method for variational optical flow estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="223" to="232" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularization strategies for discontinuity-preserving optical flow methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Monzón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1580" to="1591" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6129" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning dual convolutional neural networks for lowlevel vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3070" to="3079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Velocity estimation from image sequences with second order differential operators</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P O</forename><surname>Tretiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognit</title>
				<meeting>Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning optical flow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understanding</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="104" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Anisotropic Huber-L 1 optical flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Br. Mach. Vis. Conf</title>
				<meeting>Br. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="108" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High accuracy optic flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="137" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast edge-preserving patchmatch for large displacement optical flow</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3534" to="3541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: Descriptor matching in variational motion estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large displacement optical flow from nearest neighbor fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2443" to="2450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4015" to="4023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bilateral filtering-based optical flow estimation with occlusion detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isnardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Occlusion-aware optical flow estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1443" to="1451" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">EpicFlow: Edgepreserving interpolation of correspondences for optical flow</title>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MirrorFlow: Exploiting symmetries in joint optical flow and occlusion estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ProbFlow: Joint optical flow and uncertainty estimation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Wannenwetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust nonlocal TV-L 1 optical flow estimation with occlusion detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4055" to="4067" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optical flow with geometric occlusion estimation and fusion of multiple frames</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Energy Minimization Methods Comput. Vis. Pattern Recognit</title>
				<meeting>Int. Conf. Energy Minimization Methods Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="364" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med</title>
				<meeting>Int. Conf. Med</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">FlowNet2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">LiteFlownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Change</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4671" to="4680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PatchBatch: A batch augmented loss for optical flow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gadot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4236" to="4245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CNN-based patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3250" to="3259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1289" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">KalmanFlow 2.0: Efficient video optical flow estimation via context-aware kalman filtering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4233" to="4246" />
			<date type="published" when="2019-09">Sep. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SelFlow: Self-supervised learning of optical flow</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4571" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Continual occlusion and optical flow estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neoral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Šochman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comput. Vis</title>
				<meeting>Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="159" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31th AAAI Conf. Artif. Intell</title>
				<meeting>31th AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1495" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Authorized licensed use limited to: Tsinghua University</title>
		<imprint/>
	</monogr>
	<note>Downloaded on December 31,2022 at 08:10:51 UTC from IEEE Xplore. Restrictions apply</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32th AAAI Conf</title>
				<meeting>32th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7251" to="7259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
