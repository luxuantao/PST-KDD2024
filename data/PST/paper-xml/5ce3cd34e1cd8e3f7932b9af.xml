<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Low-Precision Learning in GPU-Accelerated Spiking Neural Network</title>
				<funder>
					<orgName type="full">Office of Naval Research Young Investigator Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xueyuan</forename><surname>She</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Long</surname></persName>
							<email>yunlong@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
							<email>saibal.mukhopadhyay@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Low-Precision Learning in GPU-Accelerated Spiking Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spiking neural network (SNN) uses biologically inspired neuron model coupled with Spike-timing-dependentplasticity (STDP) to enable unsupervised continuous learning in artificial intelligence (AI) platform. However, current SNN algorithms shows low accuracy in complex problems and are hard to operate at reduced precision. This paper demonstrates a GPU-accelerated SNN architecture that uses stochasticity in the STDP coupled with higher frequency input spike trains. The simulation results demonstrate 2 to 3 times faster learning compared to deterministic SNN architectures while maintaining high accuracy for MNIST (simple) and fashion MNIST (complex) data sets. Further, we show stochastic STDP enables learning even with 2 bits of operation, while deterministic STDP fails.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Spiking neural network (SNN) are artificial neural networks (ANN) with biological plausible neuron and synapse models. The SNN has drawn significant attention in the field of artificial intelligence. In particular, SNN demonstrates the ability of unsupervised learning using Spike-Time-Dependent-Plasticity (STDP) in the synapse models. The STDP is a phenomenon observed in biology experiments <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, which can be used as the synapse model of SNN. With STDP, synapse changes its conductance based on the time difference between pre-synaptic and post-synaptic spikes, enabling the learning ability of SNN. Several research efforts have explored STDP algorithms to enable learning in SNN <ref type="bibr" target="#b2">[3]</ref> [4] <ref type="bibr" target="#b4">[5]</ref>. Simulations of SNN has received significant attention in recent years to facilitate both understanding brain and develop AI algorithms. Early parallel SNN simulation tools like pGENESIS <ref type="bibr" target="#b5">[6]</ref> required cluster computers to run. The recent developments focused on achieving SNN simulation with higher accuracy and better simulation speed <ref type="bibr" target="#b6">[7]</ref>. In particular, advancements of Graphics Processing Units (GPUs) has led to feasibility of orders of magnitude improvement in computing speed of SNNs. Several SNN simulators with integrated STDP learning have been presented such as Brain <ref type="bibr" target="#b7">[8]</ref>, NEST <ref type="bibr" target="#b8">[9]</ref> and CARLsim <ref type="bibr" target="#b9">[10]</ref>. In a recent work, Long et. al. has presented a region-of-interest (ROI) based approach that vary complexity of neuron models based on spiking activity in a region, but they did not discuss STDP-based learning using SNN <ref type="bibr" target="#b10">[11]</ref>.</p><p>The existing SNN simulators such as NEST and CARLSim use deterministic STDP learning which suffers from several drawbacks, as shown in Section IV for details. First, while networks demonstrate good accuracy for simple tasks such as MNIST-based digit recognition <ref type="bibr" target="#b11">[12]</ref>, the learning accuracy for difficult tasks such as Fashion-MNIST <ref type="bibr" target="#b12">[13]</ref> (images of apparel items that contains complex features) is much lower. Second, deterministic STDP provides limited opportunity for fast and low-precision simulation of unsupervised learning in SNN. For example, deterministic STDP for MNIST performed in 8bit fixed-point (28%) shows significantly lower accuracy than floating-point (92%). This paper presents a GPU-accelerated SNN simulator, ParallelSpikeSim for high-accuracy, fast, and low-precision unsupervised learning. The key innovation of this paper is to demonstrate stochastic STDP for unsupervised learning in SNN, instead of well-explored deterministic STDP algorithms <ref type="bibr">[3] [4]</ref> used in prior simulators. Moreover, we provide controllability to precision (down to 2-bit) with different rounding options, and frequency of input spike trains during unsupervised learning (and inference) in SNN to effectively exploit stochastic STDP for fast and low-precision learning. This paper makses following key contributions:</p><p>? We present the ParallelSpikeSim as a GPU-acclerated SNN simulator supporting unsupervised learning. The simulator is designed for parallel computing, programmed in C++ using CUDA libraries, and support different neuron/synaptic models. ? We demonstrate that the stochastic STDP allows good learning accuracy for both simple (MNIST <ref type="bibr" target="#b11">[12]</ref>, 96.1% accuracy) and complex (feature-rich) (Fashion-MNIST <ref type="bibr" target="#b12">[13]</ref>, 77.2% accuracy) data sets. ? We show that the stochastic STDP allows SNN to operate under input frequency ranges much higher than that of deterministic STDP design, and hence, enables up to 3x lower learning time with graceful quality degradation. ? We show that stochastic STDP enables robust learning from 32-bit floating-point (accuracy 96.1%) down to 2bit fixed-point (accuracy 64.6%) learning, whereas deterministic STDP based SNN fails to provide meaningful results (accuracy drops from 92.2% to 9.6%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SPIKING NEURAL NETWORK MODELS A. Spiking Neuron Model</head><p>The spiking neuron model used in this work is leaky integrate-and-fire (LIF). For LIF model, membrane potential of a neuron is described by: The current I received by neuron a is described by:</p><formula xml:id="formula_0">dv/dt = a + bv + cI (1) v = v reset , if v &gt; v threshold<label>(2)</label></formula><formula xml:id="formula_1">I a = N n=0 g n,a v pren (3)</formula><p>N is the total number of pre-neurons connected to neuron a. g n,a is the conductance of the synapse connecting neuron a and its pre-neuron n. v pren is the voltage spike of pre-neuron n. Fig. <ref type="figure" target="#fig_0">1</ref> (a) shows change of spiking frequency of LIF model with parameters used in this work with respect to different input current.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Synapse Model</head><p>In an SNN, a synapse connects two neurons, which are referred to as the pre-neuron and post-neuron. The synapse transmits signals when its pre-neuron spikes and sends current signal to its post-neuron. The conductance of synapses is the weight of the connection and learning is achieved through modulating the conductance of synapses. Conductance modulation algorithm used in this unsupervised learning architecture is STDP. With STDP, magnitude and direction of each synaptic operation is determined by the temporal relationship between the spiking activities of the pre-neuron and post-neuron. When post-neuron spikes closely after a pre-neuron spike, the synapse experience long-term potentiation (LTP); when post-neuron spikes before a pre-neuron spike, the synapse experience long-term depression (LTD). As a result, the level of causal relationship between two neurons can be encoded as conductance of the synapse. Using model introduced in <ref type="bibr" target="#b3">[4]</ref>, conductance is modulated by equations ?G p = ? p e -?p(G-Gmin)/(Gmax-Gmin) (4)</p><formula xml:id="formula_2">?G d = ? d e -? d (Gmax-G)/(Gmax-Gmin) (<label>5</label></formula><formula xml:id="formula_3">)</formula><p>?G p is the increase of conductance for potentiation operations, and ?G d is the decrease of conductance for depression operations. ? p , ? d , ? p , ? d , G max and G min are parameters that are tuned based on input spiking frequency and voltage, as well as bit-width under which synapses are operating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stochastic behavior of synapses</head><p>For synapses with stochastic STDP behavior, potentiation or depression of synapses is not deterministic, but has a probability that depends on the time difference of the two spike events that initiates the modulation of conductance. For instance, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> (b), ?t is below zero when spiking neuron spikes before a spike from input train arrives at the synapse. Stochastic STDP is achieved with an algorithm inspired by the work of Srinivasan <ref type="bibr" target="#b13">[14]</ref>. The probabilities for potentiation and depression are defined by:</p><formula xml:id="formula_4">P pot = ? pot e (-?t/(?pot))<label>(6</label></formula><p>)</p><formula xml:id="formula_5">P dep = ? dep e (?t/(? dep ))<label>(7)</label></formula><p>The probabilities are exponentially related to time difference, with maximum value controlled by ? pot and ? dep , as shown in Fig. <ref type="figure" target="#fig_0">1 (c</ref>). In the event of potentiation, the probability is higher when ?t is smaller, indicating a stronger causal relationship. As for depression, the probability is higher when ?t is larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL PLATFORM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Design of the Simulator</head><p>Fig. <ref type="figure">2</ref> shows the flowchart of the unsupervised learning architecture with SNN achieved with ParallelSpikeSim. The SNN simulator has two major components. First, a spiking neuron simulator to simulate the differential equations governing the neuron dynamics (i.e. equations (i.e. ( <ref type="formula">1</ref>),( <ref type="formula" target="#formula_0">2</ref>),( <ref type="formula">3</ref>)) for a given synapse conductance. The second component is the learning module that implement the synaptic models and allow synapse conductance to be updated based on the spiking activity using STDP rules (i.e. equations ( <ref type="formula">4</ref>),( <ref type="formula" target="#formula_2">5</ref>)). Many past SNN simulators (e.g. CARLSim, BRAIN, NEST, etc.) includes the STDP based learning modules. The key innovation in ParallelSpikeSim is to augment the learning modules to include stochastic STDP (i.e. equations ( <ref type="formula" target="#formula_4">6</ref>),( <ref type="formula" target="#formula_5">7</ref>)) and various precision control and rounding options (see Section III-C). Moreover, we introduce an additional module between input images and spiking neuron simulator that allows controlling the frequency of the input spike train as shown in Fig. <ref type="figure">2</ref>.</p><p>CPU serves as data I/O and controls data flow of GPU. It constructs the simulation environment with configuration and input data file, allocate memory and transfer data in unified data structures to GPU memory when simulation starts. The unified data structures of ParallelSpikeSim encapsulate all network information into the network object and all input into the data object, to facilitate swift addition of functionality and customization of network hierarchy, layer connectivity and behavior of each synapse and neuron. After initialization, simulation of spiking neurons runs in parallel on GPU threads. Stochastic STDP module uses spike timers to track the temporal relationship between pre-synaptic and post-synaptic spikes, and performs stochastic process on-board the GPU to leverage the fast CUDA random number generator. Low precision learning module operate with reduced bit-width down to 2 bits, and has three available rounding options: bit truncation, rounding to nearest and stochastic rounding. Frequency control module works in two phases: frequency boost and learning time reduction. More details about the impact of the three modules are discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture and Configuration</head><p>We implement the SNN architecture shown in Fig. <ref type="figure">3</ref> to demonstrate the proposed simulator. Input image is converted to a spike train array (one spike train per pixel) connected to the first layer of neurons. The frequency of the spike trains can be controlled. The first layer consists of 1000 spiking (LIF) neurons. Each neuron receives signal from the input spike trains and when it spike, it sends excitatory signal to one neuron in the corresponding position on the second layer. The excitatory signal activates the second layer neuron, which then sends inhibitory signal to all other neurons on the first layer for t inh amount of time. This inhibition behavior achieves a winner-take-all (WTA) principle, preventing more than one neuron to learn one specific pattern. Input spike trains and first layer are connected by synapses in an all-to-all fashion. Conductance of each synapse connecting input to first layer neurons collectively forms a conductance array that learns to recognize a specific pattern.</p><p>The causal relationship between pre-synapse and postsynapse neurons explored by STDP algorithm makes it pos-sible for the network to achieve unsupervised learning. The MNIST and Fashion-MNIST data sets contain 60,000 training images and 10,000 testing images. In this work, the SNN learns the full set of training images. Pixel intensity of input images, which is an 8-bit value, is encoded into specific spiking frequency of one spike train. For darker pixels, the spiking frequency is higher, as shown in Fig. <ref type="figure" target="#fig_0">1 (d)</ref>. Frequency is in a range between f input max and f input min , and proportional to the pixel intensity. Each image is presented to the network for t learn ms. After learning is complete, the first 1000 images in the test set are used to label all the neurons in the first layer. The rest of the test set, which contains 9000 images, are used for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Low precision learning and rounding options</head><p>For low precision learning, conductance of synapses is represented in numbers with precision no greater than 32 bits. Quantization for low precision learning is performed before the LTP/LTD phase of synapse conductance. For 16-bit and 32-bit learning, after floating point calculation of change in conductance ?G, the result is rounded to a value that can be represented in the current bit width. For 8-bit and lower precision learning, ?G is set to 1/2 n , with n being the bit width. Low precision learning in other neural networks such as recurrent neural network (RNN) is shown to have different performance with different rounding options <ref type="bibr" target="#b14">[15]</ref>. For low precision learning in SNN, we study the impact of rounding options to determine if there exists a similar influence. Three rounding options including rounding to nearest, bit truncation and stochastic rounding are tested in this work. For stochastic rounding, the probability of rounding up is related to the position between the two quantized value, and is defined as:</p><formula xml:id="formula_6">P round up = (?G -?G truncated ) ? 2 n (8)</formula><p>When a value does not round up in the stochastic operation, it rounds down automatically, i.e. probability of round down is (1 -P round up ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameters and other details</head><p>For the LIF model used in this work, V th is -60.2, V reset is -74.7, a is -6.77, b is -0.0989 and c is 0.314. Parameters for STDP algorithm and stochastic behavior of synapses in different precision learning are shown in TABLE I. Initial membrane potential of all neurons on the first layer is -70.0 and conductance of each synapse is initialized with  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Accuracy Comparison with Existing Simulators</head><p>We first evaluate the accuracy of the spiking neuron simulation (no learning) considering an SNN of 10 3 LIF neurons and 10 4 synapses. Fig. <ref type="figure" target="#fig_2">4</ref> shows that our platform is able to produce spiking activities similar to CARLsim <ref type="bibr" target="#b9">[10]</ref>. However, we observe an increased simulation time in ParallelSpikeSim due to the use of more complex unified data structures. The impact of this increased spike simulation time is overshadowed by the higher learning rate achieved using stochastic STDP. We next verify that deterministic STDP (defined as baseline) in ParallelSpikeSim shows comparable accuracy with the stateof-the-art SNN design with deterministic STDP from Diehl <ref type="bibr" target="#b2">[3]</ref>. In Diehl's work, the network yields an accuracy of 91.9% for the MNIST data set while our baseline test achieves 92.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Improved Learning Accuracy with Stochastic STDP</head><p>We observe that both baseline and stochastic STDP are able to produce good accuracy in learning the MNIST data set. Each is able to provide conductance array with good contrast for each class of image as can be seen in Fig. <ref type="figure" target="#fig_3">5 (a)</ref>. Inference result shows that Stochastic STDP is able to provide better result with around 4% higher accuracy. However, for the Fashion MNIST data set, baseline test fails to gain accuracy even after learning all 60,000 images. Visualization in Fig. <ref type="figure" target="#fig_3">5(a)</ref> shows that all synapses learns the overlapping features of all classes. On the other hand, stochastic STDP is able to learn the more complex data set. Comparing visualization of synapse conductance from stochastic STDP learning on the right of Fig. <ref type="figure" target="#fig_3">5</ref> (a), baseline design struggles to learn any unique features from input images. This result shows that the level of causal relationship implied by stochastic STDP provides SNN with additional learning ability, and this effect is more prominent in more complex learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fast Learning with Higher Input Frequency</head><p>ParallelSpikeSim allows controlling the frequency of the input spike train to enable trade-off between learning rate and accuracy. One of the bottleneck of SNN learning rate is the time it takes to learn features in each individual image. Due to the influence of inhibition period of the WTA principle, and inherent nature of spiking neurons such as the reset of membrane potential after spikes, learning requires each image to be presented to the network for an extended period of time, so that a sufficient amount of spikes are generated. Since information in this spiking neural network architecture is transmitted in form of spikes, the more frequently spikes can be sent, the faster information can be delivered. Therefore it is desirable to make the spike train operate with higher frequency.</p><p>On the other hand, a higher input spike frequency can degrade the learning accuracy. Fig. <ref type="figure" target="#fig_5">7</ref> (a) shows learning accuracy loss for different input spike train frequency. We observe that using a value of f input max above certain value will cause the network to drop sharply in accuracy. This is because at higher input frequencies, the rapid arriving current signal drives multiple spiking neurons to spiking state disregard of their previous learned features, making the inhibition layer less useful and the network gradually shifts to chaotic states. This effect can be observed in the conductance visualization of four frequency ranges shown in Fig. <ref type="figure" target="#fig_3">5 (b</ref>). As a result, for SNN using deterministic STDP, the optimal f input max is limited to a relatively low value. In baseline test, the optimal spiking frequency range of neurons in the input layer is 1-22 Hz. At such frequency range, 500 ms learning time for each image is used in order to generate sufficient spikes. For a total simulation time of 542 minutes the baseline architecture is able to learn the 60,000 MNIST images.</p><p>In this work, we find that using stochastic STDP with shortterm behavior, working frequency range of f input max can be expanded, as can be seen in Fig. <ref type="figure" target="#fig_3">5</ref> (b). More specifically, higher ? pot and lower ? dep values for ( <ref type="formula" target="#formula_4">6</ref>) and ( <ref type="formula" target="#formula_5">7</ref>) are used to create a short-term stochastic STDP behavior, which enhances its ability to adapt to the fast switching input feature. We find the frequency range with maximum error rate of 20%, and the result is f input min -f input max at 5-78 Hz. Comparing Fig. <ref type="figure" target="#fig_4">6</ref> (a) left and Fig. <ref type="figure" target="#fig_4">6</ref> (a) right, which show spike train behavior of baseline and high frequency learning for the MNIST data set, it can be observed that the pattern of darker region, where the written digit is located, is more distinct in high frequency learning. Learning efficiency is therefore significantly increased as time for the network to learn features in each image is reduced. In this high frequency learning mode, frequency range of input spike train can be expanded up to 5-78 Hz before significant decrease of accuracy occurs. At this frequency range, learning time for each image is reduced to 100 ms, leading to a total simulation time of 131 minutes to learn the entire MNIST data set. In baseline test, as shown </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Low precision learning</head><p>In this work, we performed learning in 2, 4, 8 and 16 fixed point numbers. Baseline test shows poor accuracy result for low precision learning as shown in Table <ref type="table" target="#tab_1">II</ref>. This is due to the fact that quantization of conductance in low precision learning increases gap between adjacent conductance values. This leads to rapid changes in conductance during LTP/LTD process and the network quickly lose memory of learned features. This effect is shown in Fig. <ref type="figure" target="#fig_4">6</ref> (b), which is the distribution of conductance of all 784,000 synapses connecting input and first layer, for Q1.7 precision learning of the MNIST data set. Distribution of stochastic STDP is on the top of Fig. <ref type="figure" target="#fig_3">5 (b</ref>) and deterministic STDP on the bottom. Deterministic STDP results in less ideal distribution, in which a large portion of synapses drops to the minimal conductance value. Stochastic STDP in synapse model greatly improves accuracy in low  precision learning as it prevents rapid changes from loosely correlated spiking events to help retain memory and at the same time guard the network from fast convergence. The improvement is present in all precision tested, as shown in Table <ref type="table" target="#tab_1">II</ref>. Low precision learning, especially the ones with Q1.7 and lower, exhibit larger gain in accuracy from the application of stochastic STDP. Such robustness observed in this SNN design is important as there are many well-known benefits for digital systems to operate in lower bit-width, including less memory usage and less power consumption.</p><p>As shown in Table <ref type="table" target="#tab_1">II</ref>, for learning in different precision, accuracy drops significantly from Q1.15 to lower precision fixed point. Three rounding options for low precision learning tested exhibit different learning performance. Bit truncation shows the lowest accuracy in all precision tested, while stochastic rounding performs the best in most cases. This is because in low precision learning, stochastic rounding helps to maintain information about numeric position between two quantization points on a statistical point of view. It is also worth noting that stochastic rounding and round to nearest shows similar results for network using stochastic STDP, and the benefit of using stochastic rounding diminishes as bit width increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Summary of Results</head><p>Fig. <ref type="figure" target="#fig_6">8</ref> summarizes the comparison of comparison of baseline and stochastic STDP learning results. Stochastic STDP shows higher accuracy in challenging learning tasks and lower precision while using similar simulation time as baseline. The high frequency learning (with stochastic STDP) greatly reduces learning time (moving error rate reduces quickly as shown in Fig. <ref type="figure" target="#fig_6">8 (c</ref>)) but with a graceful degradation of final accuracy.</p><p>V. CONCLUSIONS This paper has presented ParallelSpikeSim, a SNN simulator with unsupervised learning using stochastic STDP. We show that ParallelSpikeSim enables accurate learning for complex tasks, enables fast learning, and allows low-precision operation even down to 2 bits. The ParallelSpikeSim will be released as an open-source, GPU-based SNN simulation platform to help researchers explore applications of SNN to various AI problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Neuron models: (a) Spiking frequency vs. input current of LIF neurons, (b) spiking behavior (c) synaptic behavior under stochastic STDP, (d) Conversion of input image to spike trains;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: ParallelSpikeSim: a GPU accelerated SNN simulator with stochastic STDP, low precision learning and frequency control module</figDesc><graphic url="image-12.png" coords="3,63.35,49.61,156.98,85.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Simulation of spiking activity and performance</figDesc><graphic url="image-116.png" coords="4,78.57,159.70,99.43,81.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Visualization of synapse conductance (a) Baseline and stochastic STDP for MNIST and Fashion MNIST, and (b) effect of input spike train frequency on stochastic STDP.</figDesc><graphic url="image-118.png" coords="4,384.89,219.11,60.51,60.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: High-frequency and low-precision operations (a) Input spike trains at low (left) and high (right) frequencies (each dot represents one spike). (b) Conductance distribution of Q1.7 precision (MNIST) with stochastic (top) and deterministic STDP (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: High-frequency learning (a) Accuracy loss vs. max input frequency, and (b) Accuracy vs. run-time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Comparison of learning configurations: (a) conductance map, (b) Accuracy and run-time, and (c) accuracy loss vs. simulation time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Parameters for different learning options ?P ?P ?D ?D Gmax Gmin ?pot ?pot ? dep ? dep finput max finput min</figDesc><table><row><cell>2 bit</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.2</cell><cell>20</cell><cell>0.2</cell><cell>10</cell><cell>22</cell><cell>1</cell></row><row><cell>4 bit</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.3</cell><cell>30</cell><cell>0.3</cell><cell>10</cell><cell>22</cell><cell>1</cell></row><row><cell>8 bit</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.5</cell><cell>30</cell><cell>0.5</cell><cell>10</cell><cell>22</cell><cell>1</cell></row><row><cell>16 bit</cell><cell>0.01</cell><cell>3</cell><cell>0.005</cell><cell>3</cell><cell>1.0</cell><cell>0</cell><cell>0.9</cell><cell>30</cell><cell>0.9</cell><cell>10</cell><cell>22</cell><cell>1</cell></row><row><cell cols="2">high frequency 0.01</cell><cell>3</cell><cell>0.005</cell><cell>3</cell><cell>1.0</cell><cell>0</cell><cell>0.3</cell><cell>80</cell><cell>0.2</cell><cell>5</cell><cell>78</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Accuracy results (%) for rounding options</figDesc><table><row><cell></cell><cell cols="3">Truncation Rounding to nearest Stochastic</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q0.2</cell><cell>9.6</cell><cell>11.3</cell><cell>16.8</cell></row><row><cell>Q0.4</cell><cell>13.1</cell><cell>16.3</cell><cell>21.3</cell></row><row><cell>Q1.7</cell><cell>28.2</cell><cell>30.8</cell><cell>33.7</cell></row><row><cell>Q1.15</cell><cell>52.6</cell><cell>52.8</cell><cell>55.2</cell></row><row><cell>Stochastic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q0.2</cell><cell>62.3</cell><cell>66.7</cell><cell>64.6</cell></row><row><cell>Q0.4</cell><cell>72.4</cell><cell>77.6</cell><cell>79.0</cell></row><row><cell>Q1.7</cell><cell>88.5</cell><cell>91.1</cell><cell>90.1</cell></row><row><cell>Q1.15</cell><cell>93.2</cell><cell>94.2</cell><cell>94.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Design, Automation And Test in Europe(DATE 2019)  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Design, Automation And Test in Europe (DATE 2019)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACKNOWLEDGMENT This work was supported in part by the <rs type="funder">Office of Naval Research Young Investigator Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal contiguity requirements for longterm associative potentiation/depression in the hippocampus</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Steward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="791" to="797" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long-term potentiation in the hippocampus using depolarizing current pulses as the conditioning stimulus to single volley synaptic potentials</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gustafsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W C</forename><surname>Wigstr?m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y Y</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="774" to="780" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of digit recognition using spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2015-08">August. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Immunity to device variations in a spiking neural network with memristive nanodevices</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Querlioz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="288" to="295" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Categorization and decision-making in a neurobiologically plausible spiking network using a STDP-like learning rule</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Beyeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="109" to="124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The GENESIS simulation system. The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName><forename type="first">J M</forename><surname>Bower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hucka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-08">August 2000. 2003</date>
			<biblScope unit="page" from="475" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simulation of networks of spiking neurons: A review of tools and strategies</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Brette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Carnevale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><surname>Brette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The brian simulator</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><surname>Morrison</surname></persName>
		</author>
		<idno>12.0. doi.org</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CARLsim 4: An Open Source Library for Large Scale, Biologically Detailed Spiking Neural Network Simulation using Heterogeneous Clusters</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Shuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chou</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCNN</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating biophysical neural network simulation with region of interest based approximation</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyuan</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automation Test in Europe Conference Exhibition (DATE)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2323" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>CoRR, abs/1708.0</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Magnetic Tunnel Junction Based Long-Term Short-Term Stochastic Synapse for a Spiking Neural Network with On-Chip STDP Learning</title>
		<author>
			<persName><forename type="first">Gopalakrishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhronil</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On-chip training of recurrent neural networks with limited numerical precision</title>
		<author>
			<persName><forename type="first">Taesik</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Hwan</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeha</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2017-05">May. 2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="3716" to="3723" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
