<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chaos: Scale-out Graph Processing from Secondary Storage</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Amitabha</forename><surname>Roy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Bindschaedler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jasmina</forename><surname>Malicevic</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
						</author>
						<author>
							<persName><surname>Switzerland</surname></persName>
						</author>
						<title level="a" type="main">Chaos: Scale-out Graph Processing from Secondary Storage</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B9FCC7104562A382888766D5624FFB2C</idno>
					<idno type="DOI">10.1145/2815400.2815408</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chaos scales graph processing from secondary storage to multiple machines in a cluster. Earlier systems that process graphs from secondary storage are restricted to a single machine, and therefore limited by the bandwidth and capacity of the storage system on a single machine. Chaos is limited only by the aggregate bandwidth and capacity of all storage devices in the entire cluster.</p><p>Chaos builds on the streaming partitions introduced by X-Stream in order to achieve sequential access to storage, but parallelizes the execution of streaming partitions. Chaos is novel in three ways. First, Chaos partitions for sequential storage access, rather than for locality and load balance, resulting in much lower pre-processing times. Second, Chaos distributes graph data uniformly randomly across the cluster and does not attempt to achieve locality, based on the observation that in a small cluster network bandwidth far outstrips storage bandwidth. Third, Chaos uses work stealing to allow multiple machines to work on a single partition, thereby achieving load balance at runtime.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In terms of performance scaling, on 32 machines Chaos takes on average only 1.61 times longer to process a graph 32 times larger than on a single machine. In terms of capacity scaling, Chaos is capable of handling a graph with 1 trillion edges representing 16 TB of input data, a new milestone for graph processing capacity on a small commodity cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Processing large graphs is an application area that has attracted significant interest in the research community <ref type="bibr">[10, 13, 14, 16-18, 21, 25, 26, 28, 30-32, 34]</ref>. Triggered by the availability of graph-structured data in domains ranging from social networks to national security, researchers are exploring ways to mine useful information from such graphs. A serious impediment to this effort is the fact that many graph algorithms exhibit irregular access patterns <ref type="bibr" target="#b13">[20]</ref>. As a consequence, most graph processing systems require that the graphs fit entirely in memory, necessitating either a supercomputer or a very large cluster <ref type="bibr" target="#b6">[13,</ref><ref type="bibr" target="#b7">14,</ref><ref type="bibr" target="#b18">25,</ref><ref type="bibr" target="#b19">26]</ref>.</p><p>Systems such as GraphChi <ref type="bibr" target="#b11">[18]</ref>, GridGraph <ref type="bibr" target="#b27">[34]</ref> and X-Stream <ref type="bibr" target="#b24">[31]</ref> have demonstrated that it is possible to process graphs with edges in the order of billions on a single machine, by relying on secondary storage. This approach considerably reduces the entry barrier to processing large graphs. Such problems no longer require the resources of very large clusters or supercomputers. Unfortunately, the amount of storage that can be attached to a single machine is limited, while graphs of interest continue to grow <ref type="bibr" target="#b23">[30]</ref>. Furthermore, the performance of a graph processing system based on secondary storage attached to a single machine is limited by its bandwidth to secondary storage <ref type="bibr" target="#b15">[22]</ref>.</p><p>In this paper we investigate how to scale out graph processing systems based on secondary storage to multiple machines, with the dual goals of increasing the size of graphs they can handle, to the order of a trillion edges, and improving their performance, by accessing secondary storage on different machines in parallel.</p><p>The common approach for scaling graph processing to multiple machines is to first statically partition the graph, and then to place each partition on a separate machine, where the graph computation for that partition takes place. Partitioning aims to achieve load balance to maximize parallelism and locality to minimize network communication. Achieving highquality partitions that achieve these two goals can be timeconsuming, especially for out-of-core graphs. Optimal partitioning is NP-hard <ref type="bibr" target="#b5">[12]</ref>, and even approximate algorithms may take considerable running time. Also, static partitioning cannot cope with later changes to the graph structure or variations in access patterns over the course of the computation.</p><p>Chaos takes a fundamentally different approach to scaling out graph processing on secondary storage. First, rather than performing an elaborate partitioning step to achieve load balance and locality, Chaos performs a very simple initial partitioning to achieve sequential storage access. It does this by using a variant of the streaming partitions introduced by X-Stream <ref type="bibr" target="#b24">[31]</ref>. Second, rather than locating the data for each partition on a single machine, Chaos spreads all graph data (vertices, edges and intermediate data, known as updates) uniformly randomly over all secondary storage devices. Data is stored in large enough chunks to maintain sequential storage access. Third, since different streaming partitions can have very different numbers of edges and updates, and therefore require very different amounts of work, Chaos allows more than one machine to work on the same streaming partition, using a form of work stealing <ref type="bibr" target="#b1">[8]</ref> for balancing the load between machines.</p><p>These three components together make for an efficient implementation, achieving sequentiality, I/O load balance and computational load balance, while avoiding lengthy preprocessing due to elaborate partitioning. It is important to point out what Chaos does not do: it does not attempt to achieve locality. A fundamental assumption underlying the design of Chaos is that machine-to-machine network bandwidth exceeds the bandwidth of a storage device and that network switch bandwidth exceeds the aggregate bandwidth of all storage devices in the cluster. Under this assumption the network is never the bottleneck. Locality is then no longer a primary concern, since data can be streamed from a remote device at the same rate as from a local device. This assumption holds true for clusters of modest size, in which machines, even with state-of-the art SSDs, are connected by a commodity high-speed network, which is the environment targeted by Chaos. Recent work on datacenter networks suggests that this assumption also holds on a larger scale <ref type="bibr" target="#b8">[15,</ref><ref type="bibr" target="#b20">27,</ref><ref type="bibr" target="#b22">29]</ref>.</p><p>We evaluate Chaos on a cluster of 32 machines with ample secondary storage and connected by a high-speed network. We are able to scale up the problem size by a factor of 32, going from 1 to 32 machines, with on average only a 1.61X increase in runtime. Similarly, for a given graph size, we achieve speedups of 10 to 22 on 32 machines. The aggregated storage also lets us handle a graph with a trillion edges. This result represents a new milestone for graph processing systems on small commodity clusters. In terms of capacity it rivals those from the high performance computing community [1] and very large organizations [2] that place the graph on supercomputers or in main memory on large clusters. Chaos therefore enables the processing of very large graphs on rather modest hardware.</p><p>We also examine the conditions under which good scaling occurs. We find that sufficient network bandwidth is critical, as it underlies the assumption that locality has little effect. Once sufficient network bandwidth is available, performance improves more or less linearly with available storage bandwidth. The number of cores per processor has little or no effect, as long as enough cores are available to sustain high network bandwidth.</p><p>The contributions of this paper are:</p><p>• We build the first efficient scale-out graph processing system from secondary storage.</p><p>• Rather than expensive partitioning for locality and load balance, we use a very cheap partitioning scheme to achieve sequential access to secondary storage, leading to a short pre-processing time.</p><p>• We do not aim for locality in storage access, and we achieve I/O load balance by randomizing<ref type="foot" target="#foot_0">1</ref> data location and access.</p><p>• We allow multiple machines to work on the same partition in order to achieve computational load balance through randomized work stealing.</p><p>• We demonstrate that the system achieves its goals in terms of capacity and performance on a cluster of 32 machines.</p><p>The outline of the rest of this paper is as follows. In Section 2 we describe the GAS programming model used by Chaos. Partitioning and pre-processing are covered in Section 3. In Section 4 we give an overview of the Chaos design, followed by a detailed discussion of the computation engine in Section 5, the storage engine in Section 6, and some implementation details in Section 7. We describe the environment used for the evaluation in Section 8, present scaling results for Chaos in Section 9, and explore the impact of some of the design decisions in Section 10. Finally, we present related work in Section 11, before concluding in Section 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Programming Model</head><p>Chaos adopts an edge-centric and somewhat simplified GAS (Gather-Apply-Scatter) model <ref type="bibr" target="#b3">[10,</ref><ref type="bibr" target="#b6">13,</ref><ref type="bibr" target="#b24">31]</ref>.  Figure <ref type="figure">1</ref> provides pseudo-code for the overall computation.</p><p>During the scatter phase, for each edge, the Scatter function is called, taking as argument the vertex value of the source vertex of the edge, and returning the value of an update sent to the destination vertex of the edge. During the gather phase, for each update, the Gather function is called, updating the accumulator value of the destination vertex of the update using the value supplied with the update. Finally, during the apply phase, for each vertex, the Apply function is called, applying the value of the accumulator to compute the new vertex value. Figure <ref type="figure" target="#fig_0">2</ref> shows, for example, how Pagerank is implemented in the GAS model.</p><p>When executing on multiple machines, vertices may be replicated to achieve parallelism. For each replicated vertex there is a master. Edges or updates are never replicated.</p><p>During the scatter phase parallelism is achieved by processing edges (and producing updates) on different machines. The update phase is distributed by each replica of a vertex gathering a subset of the updates for that vertex in its local accumulator. The apply phase then consists of applying all these accumulators to the vertex value (see Figure <ref type="figure" target="#fig_1">3</ref>).</p><p>The edge-centric nature of the programming model is evidenced by the iteration over edges and updates in the scatter and gather phases, unlike the vertex-centric model <ref type="bibr" target="#b14">[21]</ref>, in which the scatter and gather loops iterate over vertices. This model is inherited from X-Stream, and has been demonstrated to provide superior performance for graph processing from secondary storage <ref type="bibr" target="#b24">[31]</ref>. The GAS model was introduced by PowerGraph, and naturally expresses distributed graph processing, in which vertices may be replicated <ref type="bibr" target="#b6">[13]</ref>.</p><p>Finally, Chaos follows the simplifications of the GAS model introduced by PowerLyra [10], scattering updates only over outgoing edges and gathering updates only for incoming edges.</p><p>As in other uses of the GAS model, Chaos expects the final result of multiple applications of any of the user-supplied functions Scatter, Gather and Apply to be independent of the order in which they are applied in the scatter, gather and ap-  ply loops respectively. Chaos takes advantage of this orderindependence to achieve an efficient solution. In practice, all our algorithms satisfy this requirement, and so we do not find it to be a limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Streaming Partitions</head><p>Chaos uses a variation of X-Stream's <ref type="bibr" target="#b24">[31]</ref> streaming partitions to achieve efficient sequential secondary storage access. A streaming partition of a graph consists of a set of vertices that fits in memory, all of their outgoing edges and all of their incoming updates. Executing the scatter and gather phases one streaming partition at a time allows sequential access to the edges and updates, while keeping all (random) accesses to the vertices in memory.</p><p>In X-Stream the size of the vertex set of a streaming partition is -allowing for various auxiliary data structures -equal to the size of main memory. This choice optimizes sequential access to edges and updates, while keeping all accesses to the vertex set in memory. In a distributed setting other considerations play a role into the proper choice for the size of the vertex set. Main memory size remains an upper bound in order to guarantee in-memory access to the vertex set, and large sizes facilitate sequential access to edges and updates, but smaller sizes are desirable, as they lead to easier load balancing.</p><p>Therefore, we choose the number of partitions to be the smallest multiple of the number of machines such that the vertex set of each partition fits into memory. We simply partition the vertex set in ranges of consecutive vertex identifiers. Edges are partitioned such that an edge belongs to the partition of its source vertex.</p><p>This partitioning is the only pre-processing done in Chaos. It requires one pass over the edge set, and a negligible amount of computation per edge. Furthermore, it can easily be parallelized by splitting the input edge list evenly across ma-chines. This low-cost pre-processing stands in stark contrast to the elaborate partitioning algorithms that are typically used in distributed graph processing systems. These complex partitioning strategies aim for static load balance and locality <ref type="bibr" target="#b6">[13]</ref>. Chaos dispenses with locality entirely, and achieves load balance at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Design overview</head><p>Chaos consists of a computation sub-system and a storage sub-system.</p><p>The storage sub-system consists of a storage engine on each machine. It supplies vertices, edges and updates of different partitions to the computation sub-system. The vertices, edges and updates of a partition are uniformly randomly spread over the different storage engines.</p><p>The computation sub-system consists of a number of computation engines, one per machine. The computation engines collectively implement the GAS model. Unlike the conceptual model described in Section 2, the actual implementation of the model in Chaos has only two phases per iteration, a scatter and a gather phase. The apply phase is incorporated into the gather phase, for reasons of efficiency. There is a barrier after each scatter phase and after each gather phase. The Apply function is executed as needed during the gather phase, and does not imply any global synchronization.</p><p>The Chaos design allows multiple computation engines to work on a single partition at the same time, in order to achieve computational load balance. When this is the case, it is essential that each engine read a disjoint set of edges (during the scatter phase) or updates (during the gather phase). This responsibility rests with the storage sub-system. This division of labor allows multiple computation engines to work on the same partition without synchronization between them.</p><p>The protocol between the computation and storage engines is designed such that all storage devices are kept busy all the time, thereby achieving maximum utilization of the bottleneck resource, namely the bandwidth of the storage devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Computation sub-system</head><p>The number of streaming partitions is a multiple k of the number of computation engines. Therefore, each computation engine is initially assigned k partitions. This engine is the master for all vertices of those partitions, or, for short, the master of those partitions.</p><p>We start by describing the computation in the absence of work stealing. This aspect of Chaos is similar to X-Stream, but is repeated here for completeness. Later, we show how </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Scatter phase</head><p>Each computation engine works on its assigned partitions, one at a time, moving from one of its assigned partition to the next (lines <ref type="bibr" target="#b16">[23]</ref><ref type="bibr" target="#b17">[24]</ref><ref type="bibr" target="#b18">[25]</ref><ref type="bibr" target="#b19">[26]</ref><ref type="bibr" target="#b20">[27]</ref><ref type="bibr" target="#b21">[28]</ref><ref type="bibr" target="#b22">[29]</ref><ref type="bibr" target="#b23">[30]</ref><ref type="bibr" target="#b24">[31]</ref><ref type="bibr" target="#b25">[32]</ref><ref type="bibr" target="#b26">[33]</ref> without any global synchronization between machines. The vertex set of the partition is read into memory, and then the edge set is streamed into a large main memory buffer. As edges are processed, updates may be produced. These updates are binned according to the partition of their target vertex, and buffered in memory. When a buffer is full, it is written to storage. Multiple buffers are used, both for reading edges and writing updates, in order to overlap computation and I/O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The gather phase</head><p>Each computation engine works on its assigned partitions, one at a time, moving from one of its assigned partition to the next (lines 35-45) without any global synchronization between machines. The vertex set of the partition is read into memory, and then the update set is streamed into a large main memory buffer. As updates are processed, the accumulator of the destination vertex is updated. Multiple buffers are used for reading updates in order to overlap computation and I/O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Work stealing</head><p>The number of edges or updates to be processed may differ greatly between partitions, and therefore between computation engines. Chaos uses work stealing to even out the load, as described next.</p><p>When computation engine i completes the work for its assigned partitions (lines 23-26 for scatter and lines 35-38 for gather), it goes through every partition p (for which it is not the master) and sends a proposal to help out with p to its master j (line 30 for scatter and line 49 for gather). Depending on how far along j is with that partition, it accepts or rejects the proposal, and sends a response to i accordingly.</p><p>In the case of a negative answer, engine i continues to iterate through the other partitions, each time proposing to help. It does so until it receives a positive response or until it has determined that no help is needed for any of the partitions. In the latter case its work for the current scatter or gather phase is finished, and it waits at a barrier (line 33 for scatter and line 53 for gather).</p><p>When engine i receives a positive response to help out with partition p, it reads the vertex set of that partition from storage into its memory, and starts working on it. When two or more engines work on the same partition, it is essential that they work on a disjoint set of edges (during scatter) or updates (during gather). Chaos puts this responsibility with the storage system: it makes sure that in a particular iteration an edge or an update is processed only once, independent of how many computation engines work on the partition to which that edge or update belongs. This is easy to do in the storage system, and avoids the need for synchronization between the computation engines involved in stealing.</p><p>For stealing during scatter, a computation engine proceeds exactly as it does for its own partitions. Using the usersupplied Scatter function, the stealer produces updates into in-memory buffers and streams them to storage when the buffers become full.</p><p>Stealing during gather is more involved. As before, a computation engine reads updates from storage, and uses the user-supplied Gather function to update the accumulator of the destination vertex of the update. There are now, however, multiple instances of the accumulator for this vertex, and their values need to be combined before completing the gather phase. To this end the master of the partition keeps track of which other computation engines have stolen work from it for this partition. When the master completes its part of the gather for this partition, it sends a request to all those computation engines, and waits for an answer. When a stealer completes its part of the gather, it waits to receive a request for its accumulator from the master, and eventually sends it to the master (line 52). The master then uses the user-supplied Apply function to compute the new vertex values from these different accumulators, and writes the vertex set back to storage.</p><p>The order in which the master and the stealers complete their work is unpredictable. When a stealer completes its work before the master, it waits until the master requests its accumulator values before it does anything else (line 52). When the master completes its work before one or more of the stealers, it waits until those stealers return their accumulator (line 42).</p><p>On the plus side, this approach guarantees that all accumulators are in memory at the time the master performs the apply. On the minus side, there may be some amount of time during which a computation engine remains idle. An alternative would have been for an engine that has completed its work on a partition to write its accumulators to storage, from where the master could later retrieve them. This strategy would allow an engine to immediately start work on another partition. The idle time in our approach is, however, very short, because all computation engines that work on the same partition read from the same set of updates, and therefore all finish within a very short time of one another. We therefore prefer this simple and efficient in-memory approach over more complicated ones, such as writing the accumulators to storage, or interrupting the master to incorporate the accumulators from stealers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">To steal or not to steal</head><p>Stealing is helpful if the cost, the time for the stealer to read in the vertex set, is smaller than the benefit, the reduction in processing time for the edges or updates still to be processed at the time the stealer joins in the work. Since Chaos is I/Obound, this decrease in processing time can be estimated by the decrease in I/O time caused by the stealer.</p><p>This estimate is made by considering the following quantities: B is the bandwidth to storage seen by each computation engine, D is the amount of edge or update data remaining to be read for processing the partition, H is the number of computation engines currently working on the partition (including the master), and V is the size of the vertex state of the partition.</p><p>If the master declines the stealing proposal, the remaining time to process this partition is D BH . If the master accepts the proposal, then V B time is required to read the vertex set of size V . Since we assume that bandwidth is limited by the storage engines and not by the network, an additional helper increases the bandwidth from BH to B(H + 1), and decreases the remaining processing time from D BH to D B(H+1) . The master therefore accepts the proposal if and only if:</p><formula xml:id="formula_0">V B + D B(H + 1) &lt; D BH<label>(1)</label></formula><formula xml:id="formula_1">=⇒ V + D (H + 1) &lt; D H<label>(2)</label></formula><p>The master knows the size of the vertex set V , and keeps track of the number of stealers H. It estimates the value of D by multiplying the amount of edge or update data still to be processed on the local storage engine by the number of machines. Since the data is evenly spread across storage engines, this estimate is accurate and makes the decision process local to the master. This stealing criterion is incorporated in need help() on lines 30 and 49 of the pseudocode in Figure <ref type="figure" target="#fig_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Storage sub-system</head><p>For an out-of-core graph processing system such as Chaos, computation is only one half of the system. The other half consists of the storage sub-system that supplies the I/O bandwidth necessary to move graph data between storage and main memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Stored data structures and their access patterns</head><p>For each partition, Chaos records three data structures on storage: the vertex set, the edge set and the update set. The accumulators are temporary structures, and are never written to storage.</p><p>The access patterns of the three data structures are quite different. Edge sets are created during pre-processing and are read during scatter.<ref type="foot" target="#foot_1">2</ref> Update sets are created and written to storage during scatter, and read during gather. After the end of a gather phase, they are deleted. Vertex sets are initialized during pre-processing, and always read in their entirety, both during scatter and gather. Read and write operations to edges and updates may be performed by the master or by any stealers. In contrast, read operations to the vertex state may be performed by the master or any stealers, but only the master updates the vertex values during apply and writes them back to storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Chunks</head><p>One of the key design decisions in Chaos is to spread all data structures across the storage engines in a random uniform manner, without worrying about locality.</p><p>All data structures are maintained and accessed in units called chunks. The size of a chunk is chosen large enough so that access to storage appears sequential, but small enough so that they can serve as units of distribution to achieve random uniform distribution across storage engines. Chunks are also the "unit of stealing", the smallest amount of work that can be stolen. Therefore, to achieve good load balance, they need to be relatively small.</p><p>A storage engine always serves a request for a chunk in its entirety before serving the next request, in order to maintain sequential access to the chunk, despite concurrent requests to the storage engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Edge and update sets</head><p>Edges and updates are always stored and retrieved one chunk at a time.</p><p>Edges are stored during pre-processing, and updates during the scatter phase, but Chaos uses the same mechanism to choose a storage engine on which to store a chunk. It simply picks a random number uniformly distributed between 1 and the number of storage engines, and stores the chunk there.</p><p>To retrieve a chunk of edges or updates, a computation engine similarly picks a random number between 1 and the number of machines, and sends the request to that storage engine. In its request it specifies a partition identifier, but it does not specify a particular chunk.</p><p>When a storage engine receives a request for a chunk for a given partition, it checks if it still has unprocessed chunks for this partition, and, if so, it is free to return any unprocessed chunk. This approach is in keeping with the observation that the order of edges or updates does not matter, and therefore it does not matter in which order chunks of edges or updates are processed. For both edges and updates, it is essential, however, that they are read only once during an iteration. To this end, a storage engine keeps track of which chunks have already been consumed during the current iteration.</p><p>If all chunks for this partition on this storage engine have already been processed, the storage engine indicates so in the reply to the computation engine. A computation engine knows that the input for the current streaming partition is empty when all storage engines fail to supply a chunk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Vertex sets</head><p>Vertex sets are always accessed in their entirety, but they are also stored as chunks. For vertices, the chunks are mapped to storage engines using the equivalent of hashing on the partition identifier and the chunk number. During pre-processing the chunks of the vertex set of a partition are stored at storage engines in this manner. During the execution of the main computation loop the computation engines wishing to read or write the vertex set use the same approach to find the storage engines storing these chunks, and request the next chunk from this storage engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Keeping all storage engines busy</head><p>As described, a computation engine only issues one request at a time to the storage system. Although randomization, on average, spreads such requests evenly across storage engines, the lack of coordination can cause significant inefficiencies due to some storage engines becoming instantaneously idle. With an equal number of computation and storage engines, and with storage and not computation being the bottleneck, there are always as many requests as there are storage engines. Without coordination between computation engines, several of them may address their request to the same storage engine, leaving other storage engines idle.</p><p>To keep all storage engines busy with high probability, each computation engine keeps, at all times, multiple requests to different storage engines outstanding. The number of such outstanding requests, called the batch factor k, is chosen to be the smallest number that with high probability keeps all storage engines busy all the time. The proper batch factor is derived as follows.</p><p>If a computation engine has k requests outstanding, then only some fraction are being processed by the storage sub-system. The other requests are in transit. To ensure there are k outstanding requests at the storage engines, the computation engines use a larger request window φ k. This amplification factor φ can easily be computed by repeated application of Little's law <ref type="bibr" target="#b12">[19]</ref>:</p><formula xml:id="formula_2">k = λ R storage φ k = λ (R storage + R network )</formula><p>where λ is the throughput of the storage engine in terms of requests per unit time, R storage is the latency for the storage engine to service the request and R network is the latency of a round trip on the network. Solving we have the required amplification φ :</p><formula xml:id="formula_3">φ = 1 + R network R storage<label>(3)</label></formula><p>With this choice of φ , we end up with k outstanding requests from each of m computation engines distributed at random across the m storage engines. We can then derive the utilization of a particular storage engine as follows. The probability that a storage engine is un-utilized is equal to the probability that no computation engine picks it for any of its k requests:</p><formula xml:id="formula_4">C m-1 k C m k m = (1 - k m ) m</formula><p>The utilization of the storage engine is therefore the probability that at least one computation engine picks it, a function of the number of machines m and the batch-factor k:</p><formula xml:id="formula_5">ρ(m, k) = 1 -(1 - k m ) m<label>(4)</label></formula><p>Figure <ref type="figure" target="#fig_3">5</ref> shows the utilization as a function of the number of machines and for various values of k. For a fixed value of k, the utilization reduces with an increasing number of machines due to a greater probability of machines being left idle but is asymptotic to a lower bound. The lower bound is simply:</p><formula xml:id="formula_6">lim m→∞ ρ(m, k) = 1 - 1 e k<label>(5)</label></formula><p>It therefore suffices to pick a value for k large enough to approach 100% utilization regardless of the number of machines. For example, using k = 5 means that the utilization cannot drop below 99.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Fault tolerance</head><p>The fact that all graph computation state is stored in the vertex values, combined with the synchronous nature of the computation, allows Chaos to tolerate transient machine failures in a simple and efficient manner. At every barrier at the end of a scatter or gather phase, the vertex values are checkpointed by means of a 2-phase protocol <ref type="bibr" target="#b4">[11]</ref> that makes sure that the new values are completely stored before the old values are removed.  In its current implementation Chaos does not support recovery from storage failures, although such support could easily be added by replicating the vertex sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Implementation</head><p>Chaos is written in C++ and amounts to approximately 15'000 lines of code. Figure <ref type="figure" target="#fig_5">6</ref> shows the high-level architecture and typical deployment of Chaos.</p><p>We run the computation engine and the storage engine on each machine in separate threads within the same process. We use / 0MQ [3] on top of TCP sockets for message-oriented communication between computation and storage engines, assuming a full bisection bandwidth network between the machines. We tune the number of / 0MQ threads for optimal performance.</p><p>The storage engines provide a simple interface to the local ext4 <ref type="bibr" target="#b16">[23]</ref> file system. Unlike X-Stream, which uses direct I/O, Chaos uses pagecache-mediated access to the storage devices. On each machine, for each streaming partition, the vertex, edge and update set correspond to a separate file. A read or write causes the file pointer to be advanced. The file pointer is reset to the beginning of the file at the end of each iteration. This provides a very simple implementation of the requirement that edges or updates are read only once during an iteration. The chunk corresponds to a 4MB block in the file, leading to good sequentiality and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experimental environment and benchmarks</head><p>We evaluate Chaos on a rack with 32 16-core machines, each equipped with 32 GB of main memory, a 480GB SSD and 2 6TB magnetic disks (arranged in RAID 0 range of 400MB/s and 200MB/s, respectively, well within the capacity of the 40 GigE interface on the machine.</p><p>We use the same set of algorithms as used by X-Stream <ref type="bibr" target="#b24">[31]</ref> to demonstrate that all the single machine algorithms used in the evaluation of X-Stream can be scaled to our cluster. Table <ref type="table" target="#tab_2">1</ref> presents the complete set of algorithms, as well as the X-Stream runtime and the single machine Chaos runtime for an RMAT-27 graph. As can be seen, the singlemachine runtimes are similar but not exactly the same. In principle, Chaos running on a single machine is equivalent to X-Stream. The two systems have, however, different code bases and, in places, different implementation strategies. In particular, Chaos uses a client-server model for I/O, to facilitate distribution, and pagecache-mediated storage access, to simplify I/O for variable-sized objects.</p><p>We use a combination of synthetic RMAT graphs <ref type="bibr" target="#b2">[9]</ref> and the real-world Data Commons dataset <ref type="bibr">[4]</ref>. RMAT graphs can be scaled in size easily: a scale-n RMAT graph has 2 n vertices and 2 n+4 edges. In other words, the size of the vertex and edge sets doubles with each increment in the scale factor. We use the newer 2014 version of the Data Commons graph that encompasses 1.7 billion webpages and 64 billion hyperlinks between them.</p><p>Input to the computation consists of an unsorted edge list, with each edge represented by its source and target vertex and an optional weight. If necessary, we convert directed to undirected graphs by adding a reverse edge. Graphs with fewer than 2 32 vertices are represented in compact format, with 4 bytes for each vertex and for the weight, if any. Graphs with more vertices are represented in non-compact format, using 8 bytes instead. A scale-32 graph with weights on the edges thus results in 768 GB of input data. The input of the unweighted Data Commons graph is 1 TB.</p><p>All results report the wall-clock time to go from the unsorted edge list, randomly distributed over all storage devices, to the final vertex state, recorded on storage. All results therefore include pre-processing time.  Weak scaling, RMAT-27 to RMAT-32, SSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Scaling results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Weak scaling</head><p>In the weak scaling experiment we run RMAT-27 on one machine, and then double the size for each doubling of the number of machines, ending up with RMAT-32 on 32 machines.</p><p>Figure <ref type="figure" target="#fig_7">7</ref> shows the runtime results for these experiments, normalized to the runtime of a single machine. In this experiment, Chaos takes on average 1.61X the time taken by a single machine to solve a problem 32X the size on a single machine. The fastest algorithm (Cond) takes 0.97X, while the slowest (MCST) takes 2.29X.</p><p>The differences in scaling between algorithms result from a combination of characteristics of the algorithms, including the fact that the algorithm itself may not scale perfectly, the degree of load imbalance in the absence of stealing, and the size of the vertex sets. One interesting special case is Conductance, where the scaling factor is slightly smaller than 1. This somewhat surprising behavior is the result of the fact that with a larger number of machines the updates fit in the buffer cache and do not require storage accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Strong scaling</head><p>In this experiment we run all algorithms on 1 to 32 machines on the RMAT-27 graph. Figure <ref type="figure" target="#fig_8">8</ref> shows the runtime, again normalized to the runtime on one machine. For this RMAT graph, 32 machines provide on average a speedup of about 13X over a single machine. The fastest algorithm (Cond) runs 23X faster and the slowest (MCST) 8X. The results are somewhat inferior to the weak scaling results, because of the small size of the graph.</p><p>To illustrate this, we perform a strong scaling experiment on the much larger Data Commons graph. This graph does  not fit on a single SSD, so we use HDDs. Furthermore, given the long running times, we only present results for two representative algorithms, BFS and Pagerank. Figure <ref type="figure" target="#fig_9">9</ref> shows the runtimes on 1 to 32 machines, normalized to the single-machine runtime. Using 32 machines Chaos provides a speedup of 20 for BFS and 18.5 for Pagerank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Capacity scaling</head><p>We use RMAT-36 with 250 billion vertices and 1 trillion edges to demonstrate that we can scale to very large graphs. This graph requires 16TB of input data, stored on HDDs. Chaos finds a breadth-first order of the vertices of the graph in a little over 9 hours. Similarly, Chaos runs 5 iterations of PR in 19 hours. These experiments require I/O in the range of 214 TB for BFS and 395 TB for PR, and the Chaos store is able to provide an aggregate of 7 GB/s from the 64 magnetic disks running in orchestration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Scaling limitations</head><p>We evaluate the limitations to scaling Chaos with respect to the specific processor, storage bandwidth, and network links available.</p><p>Figure <ref type="figure">10</ref> presents the results of running BFS and PR as we vary the number of CPU cores available to Chaos. As can be seen, the system performs adequately even with half the CPU cores available. It is nevertheless worth pointing out that Chaos requires a minimum number of cores to maintain good network throughput.  stead of the faster 40GigE. The throughput achieved by the 1GigE interface is approximately 1/4th of the disk bandwidth. In other words, the network is the performance bottleneck. We conclude from these results that Chaos does not scale as well in such a situation, highlighting the need for network links which are faster (or at least as fast) as the storage bandwidth per machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Checkpointing</head><p>For large graph analytics problems, Chaos provides the ability to checkpoint state. Figure <ref type="figure" target="#fig_1">13</ref> shows the runtime overhead for checkpoints on a scale 36 graph for BFS and PR. As can be seen, the overhead is under 6% even though the executions write hundreds of terabytes of data to the Chaos store. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Evaluation of design decisions</head><p>Chaos is based on three synergistic principles: no attempt to achieve locality, dynamic instead of static load balancing, and simple partitioning. In this section we evaluate the effect of these design decisions. All discussion in this section is based on the weak scaling experiments. The effect of the design decisions for other experiments is similar and not repeated here. For some experiments we only show the results of BFS and Pagerank as representative algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">No locality</head><p>Instead of seeking locality, Chaos spreads all graph data uniformly randomly across all storage devices and uses a randomized procedure to choose from which machine to read or to which machine to write data.</p><p>Figure <ref type="figure" target="#fig_13">14</ref> shows the aggregate bandwidth obtained as seen by all computation engines during the weak scaling experiment. The figure also shows the maximum bandwidth of the storage devices, measured by fio <ref type="bibr">[5]</ref>.</p><p>Two conclusions can be drawn from these results. First, the aggregate bandwidth achieved by Chaos scales linearly with the number of machines. Second, the bandwidth achieved by Chaos is within 3 percent of the available storage bandwidth, the bottleneck resource in the system.</p><p>We also evaluate a couple of more detailed design choices in terms of storage access, namely the randomized selection of storage device and the batching designed to keep all storage devices busy.</p><p>Figure <ref type="figure" target="#fig_3">15</ref> compares the runtime for Pagerank on 1 to 32 machines for Chaos to a design where a centralized entity selects the storage device for reading and writing a chunk. In short, all read and writes go through the centralized entity, which maintains a directory of where each chunk of each vertex, edge or update set is located. As can be seen, the running time with Chaos increases more slowly as a function of the number of machines than with the centralized entity, which increasingly becomes a bottleneck.</p><p>Next we evaluate the efficacy of batching in our disk selection strategy. Figure <ref type="figure" target="#fig_5">16</ref> shows the effect of increasing the window size of outstanding requests on performance. We measured the latency to the SSD to be approximately equal to that on the 40 GigE network. This means φ = 2 (Equation 3). The graph shows a clear sweet spot at φ k = 10, which corresponds to k = 5. This means an utilization of 99.56% with 32 machines (Equation <ref type="formula" target="#formula_5">4</ref>), indicating that the devices are near saturation. The experiment therefore agrees with theory. Further, Equation 5 tells us that even if we increase the number of machines in the deployment, this choice of settings means that we cannot drop below 99.3% given a fixed latency on the network. The increased runtime past this choice of settings can be attributed to increased queuing delays and incast congestion. Figure <ref type="figure" target="#fig_15">17</ref> shows a breakdown of the runtime of the weak scaling experiments at 32 machines in three categories: graph processing time, idle time, and time spent copying and merging. The first category represents useful work, broken down further into processing time for the partitions for which the machine is the master and processing time for partitions initially assigned to other machines. The idle time reflects load imbalance, and the copying and merging time represents the overhead of achieving load balance.</p><p>The results are somewhat different for different algorithms. The processing time ranges from 74 to 87 percent with an average of 83 percent. The idle time is very low for all algorithms, below 4 percent. The cost of copying and merging varies considerably, from 0 to 22 percent with an average of 14 percent. Most of the idle time occurs at barriers between phases. Overall, we conclude from Figure <ref type="figure" target="#fig_15">17</ref> that load balancing is very good, but comes at a certain cost for some of the algorithms.</p><p>Next, we evaluate the quality of the decisions made by the stealing criterion we describe in Section 5.3. To do this, we introduce a factor α in Equation 2 as follows:</p><formula xml:id="formula_7">V B + D B(H + 1) &lt; α D BH</formula><p>Varying the factor α allows us to explore a range of strategies.</p><p>• No stealing: α = 0</p><p>• Less aggressive stealing: α = 0.8  As additional evidence for the need for dynamic load balancing, we compare the performance of Chaos to that of Giraph <ref type="bibr">[6]</ref>, an open-source implementation of Pregel, recently augmented with support for out-of-core graphs. Giraph uses a random partitioning of the vertices to distribute the graph across machines, without any attempt to perform any dynamic load balancing (similar to the experiment reported in Figure <ref type="figure" target="#fig_16">18</ref>, with α equal to zero).</p><formula xml:id="formula_8">• Chaos default: α = 1 0 1 2 3 B F S α = 0 B F S α = 0 . 8 B F S α = 1 . 0 B F S α = 1 . 2 B F S α = ∞ P R α = 0 P R α = 0 . 8 P R α = 1 . 0 P R α = 1 . 2 P R α = ∞ Fraction of</formula><p>Out-of-core Giraph is an order of magnitude slower than Chaos in runtime, apparently largely due to engineering issues (in particular, JVM overheads in Giraph). To eliminate these differences and to focus on scalability, Figure <ref type="figure" target="#fig_9">19</ref> shows the runtime of both Chaos and Giraph on Pagerank on RMAT-27, normalized to the single-machine runtime for each system. The results clearly confirm that the static partitions in Giraph severely affect scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Partitioning for sequentiality rather than for locality and load balance</head><p>An important question to ask is whether it would have been better to expend pre-processing time to generate high-quality partitions to avoid load imbalance in the first place instead of paying the cost of dynamic load balancing. To answer this question, we compare, for each algorithm and for 32 machines, the worst-case dynamic load balancing cost across all machines to the time required to initially partition the graph. We use Powergraph's <ref type="bibr" target="#b6">[13]</ref> grid partitioning algorithm, which requires the graph to be in memory. We lack the necessary main memory in our cluster to fit the RMAT scale-32 graph, that Chaos uses on 32 machines. We therefore run the Powergraph grid partitioning algorithm on a smaller graph (RMAT scale-27), and assume that the partitioning time for Powergraph scales perfectly with graph size. As Figure <ref type="figure" target="#fig_0">20</ref> shows, Chaos dynamic load balancing out-of-core takes only a tenth of the time required by Powergraph to partition the graph in memory. From this comparison, carried out in circumstances highly favorable to partitioning, it is clear that dynamic load balancing in Chaos is more efficient than upfront partitioning in Powergraph. Chaos therefore achieves its goal of providing high performance graph processing, while avoiding the need for high-quality partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Related Work</head><p>In recent years a large number of graph processing systems have been proposed <ref type="bibr">[10, 13, 14, 16-18, 21, 25, 26, 28, 30-32, 34]</ref>. We mention here only those most closely related to our work. We also mention some general-purpose distributed systems techniques from which we draw.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Distributed in-memory systems</head><p>Pregel <ref type="bibr" target="#b14">[21]</ref> and its open-source implementation Giraph [6] follow an edge-cut approach. They partition the vertices of a graph and place each partition on a different machine, potentially leading to severe load imbalance. Mizan <ref type="bibr" target="#b10">[17]</ref> addresses this problem by migrating vertices between iterations in the hope of obtaining better load balance in the next iteration. Chaos addresses load imbalance within each iteration, by allowing more than one machine to work on a partition, if needed. Pregel optimizes network traffic by aggregating updates to the same vertex. While this optimization is also possible in Chaos, we find that the cost of merging the updates to the same vertex outweighs the benefits from reduced network traffic.</p><p>Powergraph proposes the GAS model, and PowerLyra <ref type="bibr" target="#b3">[10]</ref> introduces a simpler variant, which we adopt in Chaos. Powergraph <ref type="bibr" target="#b6">[13]</ref> introduces the vertex-cut approach, partitioning the set of edges across machines and replicating vertices on machine that have an attached edge. PowerLyra improves on Powergraph by treating high-and low-degree nodes differently, reducing communication and replication. Both systems require lengthy pre-processing times. Also, in both systems, each partition is assigned to exactly one machine. In contrast, Chaos performs only minimal pre-processing, and allows multiple machines to work on the same partition.</p><p>Finally, a recent graph processing system called GraM <ref type="bibr" target="#b26">[33]</ref> has shown how a graph with a trillion edges can be handled in the main memory of the machines in a cluster. Chaos represents a different approach where the graph is too large to be held in memory. Thus, while Chaos is slower than GrAM it requires only a fraction of the amount of main memory to process a similarly sized graph.</p><p>11.2 Single-machine out-of-core systems GraphChi <ref type="bibr" target="#b11">[18]</ref> was one of the first systems to propose graph processing from secondary storage. It uses the concept of parallel sliding windows to achieve sequential secondary storage access. X-Stream <ref type="bibr" target="#b24">[31]</ref> improves on GraphChi by using streaming partitions to provide better sequentiality. In recent work, GridGraph <ref type="bibr" target="#b27">[34]</ref> further improves on both GraphChi and X-Stream by reducing the amount of I/O necessary. Chaos extends out-of-core graph processing to clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">Distributed systems techniques</head><p>The work on flat datacenter storage (FDS) <ref type="bibr" target="#b20">[27]</ref> shows how one could take our assumption of local storage bandwidth being the same as remote storage bandwidth and scale it out to an entire datacenter. Chaos is the first graph processing system that exploits this property but at the smaller scale of a rack of machines. Also, unlike FDS (and similar systems such as CORFU <ref type="bibr" target="#b0">[7]</ref>), we leverage the order-independence of our workload to remove the central bottleneck of a meta-data server.</p><p>The batching in Chaos is inspired by power-of-two scheduling <ref type="bibr" target="#b17">[24]</ref>, although the goal is quite different. Power-of-two scheduling aims to find the least loaded servers in order to achieve load balance. Chaos aims to prevent storage engines from becoming idle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Conclusion</head><p>Chaos is a system for processing graphs from the aggregate secondary storage of a cluster. It extends the reach of small clusters to graph problems with edges in the order of trillions. With very limited pre-processing, Chaos achieves sequential storage access, computational load balance and I/O load balance through the application of three synergistic techniques: streaming partitions adapted for parallel execution, flat storage without a centralized meta-data server, and work stealing, allowing several machines to work on a single partition.</p><p>We have demonstrated, through strong and weak scaling experiments, that Chaos scales on a cluster of 32 machines, and outperforms Giraph extended to out-of-core graphs by at least an order of magnitude. We have also quantified the dependence of Chaos' performance on various design decisions and environmental parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pagerank using Chaos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Apply in Distributed Computation Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Chaos Computation Engine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Theoretical utilization for different number of machines as a function of the batch factor k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Architecture of Chaos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Runtime normalized to 1-machine runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Runtime normalized to 1-machine runtime. Strong scaling, RMAT-27, SSD .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Runtime normalized to 1-machine runtime. Strong scaling, Data Commons, RAID0-HDD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Runtime for Chaos with different number of CPU cores, normalized to 1-machine runtime with cores=16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11</head><label>11</label><figDesc>Figure 11  compares the performance of BFS and PR when running from SSDs and HDDs. The HDD bandwidth is 2X less than the SSD bandwidth. Chaos scales as expected regardless of the bandwidth, but the application takes time inversely proportional to the available bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 Figure 12 :Figure 13 :</head><label>121213</label><figDesc>Figure 12  looks into the performance impact of a slower network by using a 1GigE interface to connect all machines in-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Aggregate bandwidth normalized to 1-machine bandwidth and maximum theoretical aggregate bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :Figure 16 :</head><label>1516</label><figDesc>Figure 15: Chaos vs. centralized chunk directory (weak scaling, RMAT-27 to -32, SSD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Breakdown of runtime (32 machines, RMAT-32, SSD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 :•</head><label>18</label><figDesc>Figure 18: Breakdown of runtime with work-stealing bias. 32 machines, RMAT-32, normalized to α = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 19 :Figure 20 :</head><label>1920</label><figDesc>Figure 19: Runtime for Chaos and Giraph, normalized to the 1-machine runtime of each system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>// Apply for all v in Vertices for all replicas v' of v Apply(v.value, v'.accum)</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Algorithms, single-machine runtime for X-Stream and Chaos, SSD. The first five algorithms require an undirected graph while the remaining ones run on a directed graph.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">X-Stream Chaos</cell></row><row><cell>Breadth-First Search (BFS)</cell><cell>497s</cell><cell>594s</cell></row><row><cell>Weakly Connected Comp. (WCC)</cell><cell>729s</cell><cell>995s</cell></row><row><cell>Min. Cost Spanning Trees (MCST)</cell><cell>1239s</cell><cell>2129s</cell></row><row><cell>Maximal Independent Sets (MIS)</cell><cell>983s</cell><cell>944s</cell></row><row><cell>Single Source Shortest Paths (SSSP)</cell><cell>2688s</cell><cell>3243s</cell></row><row><cell>Pagerank (PR)</cell><cell>884s</cell><cell>1358s</cell></row><row><cell>Strongly Connected Comp. (SCC)</cell><cell>1689s</cell><cell>1962s</cell></row><row><cell>Conductance (Cond)</cell><cell>123s</cell><cell>273s</cell></row><row><cell>Sparse Matrix Vector Mult. (SpMV)</cell><cell>206s</cell><cell>508s</cell></row><row><cell>Belief Propagation (BP)</cell><cell>601s</cell><cell>610s</cell></row></table><note><p>). Unless otherwise noted, the experiments use the SSDs as storage devices. The machines are connected through 40 GigE links to a top-ofrack switch. The SSDs and disks provide bandwidth in the</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The extensive use of randomization instead of any order is the reason for naming the system Chaos</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In an extended version of the model, edges may also be rewritten during the computation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We would like to thank our anonymous reviewers, shepherd Michael Swift, Rong Chen, Peter Peresini, Diego Didona and Kristina Spirovska for their feedback that improved this work. We would also like to thank Florin Dinu for his feedback, help in setting up the cluster and for motivating us to keep working on graph processing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CORFU: A shared log design for flash clusters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wobber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Networked Systems Design and Implementation</title>
		<meeting>the conference on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduling multithreaded computations by work stealing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Blumofe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="720" to="748" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A recursive model for graph mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><surname>R-Mat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining</title>
		<meeting>the SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Differentiated graph computation and partitioning on skewed graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Powerlyra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Systems</title>
		<meeting>the European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The performance of consistent checkpointing</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Elnozahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Reliable Distributed Systems</title>
		<meeting>the Symposium on Reliable Distributed Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="39" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Some simplified NP-complete graph problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stockmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical computer science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="237" to="267" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Powergraph: distributed graph-parallel computation on natural graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Operating Systems Design and Implementation</title>
		<meeting>the Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GraphX: Graph processing in a distributed dataflow framework</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Operating Systems Design and Implementation</title>
		<meeting>the Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="599" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VL2: A scalable and flexible data center network. SIGCOMM Comput</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Turbograph: a fast parallel graph engine handling billion-scale graphs in a single PC</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A system for dynamic load balancing in large-scale graph processing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Khayyat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Awara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alonazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jamjoom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kalnis</surname></persName>
		</author>
		<author>
			<persName><surname>Mizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Systems</title>
		<meeting>the European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GraphChi: Large-scale graph computation on just a PC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Operating Systems Design and Implementation</title>
		<meeting>the Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A proof for the queuing formula: L = λW</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="383" to="387" />
			<date type="published" when="1961-05">May 1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Challenges in parallel graph processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Processing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pregel: a system for large-scale graph processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Management of Data</title>
		<meeting>the International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaleup graph processing in the cloud: Challenges and solutions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Cloud Data and Platforms</title>
		<meeting>the International Workshop on Cloud Data and Platforms</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The new ext4 filesystem: current status and future plans</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dilger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linux Symposium</title>
		<meeting>the Linux Symposium</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="21" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The power of two choices in randomized load balancing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latency-tolerant software distributed shared memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Usenix Annual Technical Conference</title>
		<meeting>the Usenix Annual Technical Conference</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="291" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A lightweight infrastructure for graph analytics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lenharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Operating Systems Principles</title>
		<meeting>the Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="456" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Flat datacenter storage</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Operating Systems Design and Implementation</title>
		<meeting>the Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SSD prefetcher for large-scale graph traversal</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nilakant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yoneki</surname></persName>
		</author>
		<author>
			<persName><surname>Prefedge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Systems and Storage</title>
		<meeting>the International Conference on Systems and Storage</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A scalable faulttolerant layer 2 data center network fabric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Niranjan Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pamboris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subra-Manya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><surname>Portland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication</title>
		<meeting>the ACM SIGCOMM 2009 Conference on Data Communication</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multithreaded asynchronous graph traversal for in-memory and semi-external memory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xstream: Edge-centric graph processing using streaming partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mihailovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM symposium on Operating Systems Principles</title>
		<meeting>the ACM symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph query processing with abstraction refinement: Scalable and programmable analytics over very large graphs on a single PC</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Graphq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Usenix Annual Technical Conference</title>
		<meeting>the Usenix Annual Technical Conference</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="387" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GraM: Scaling graph computation to the trillions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Cloud Computing</title>
		<meeting>the Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GridGraph: Large-scale graph processing on a single machine using 2-level hierarchical partitioning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Usenix Annual Technical Conference</title>
		<meeting>the Usenix Annual Technical Conference</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
