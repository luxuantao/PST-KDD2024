<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haixu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiehui</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<email>mingsheng@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformerbased models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields stateof-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: https://github.com/thuml/Autoformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Time series forecasting has been widely used in energy consumption, traffic and economics planning, weather and disease propagation forecasting. In these real-world applications, one pressing demand is to extend the forecast time into the far future, which is quite meaningful for the long-term planning and early warning. Thus, in this paper, we study the long-term forecasting problem of time series, characterizing itself by the large length of predicted time series. Recent deep forecasting models <ref type="bibr" target="#b45">[41,</ref><ref type="bibr" target="#b21">17,</ref><ref type="bibr" target="#b24">20,</ref><ref type="bibr" target="#b32">28,</ref><ref type="bibr" target="#b27">23,</ref><ref type="bibr" target="#b33">29,</ref><ref type="bibr" target="#b23">19,</ref><ref type="bibr" target="#b39">35]</ref> have achieved great progress, especially the Transformer-based models. Benefiting from the self-attention mechanism, Transformers obtain great advantage in modeling long-term dependencies for sequential data, which enables more powerful big models <ref type="bibr" target="#b11">[7,</ref><ref type="bibr" target="#b15">11]</ref>.</p><p>However, the forecasting task is extremely challenging under the long-term setting. First, it is unreliable to discover the temporal dependencies directly from the long-term time series because the dependencies can be obscured by entangled temporal patterns. Second, canonical Transformers with self-attention mechanisms are computationally prohibitive for long-term forecasting because of the quadratic complexity of sequence length. Previous Transformer-based forecasting models <ref type="bibr" target="#b45">[41,</ref><ref type="bibr" target="#b21">17,</ref><ref type="bibr" target="#b24">20]</ref> mainly focus on improving self-attention to a sparse version. While performance is significantly improved, these models still utilize the point-wise representation aggregation. Thus, in the process of efficiency improvement, they will sacrifice the information utilization because of the sparse point-wise connections, resulting in a bottleneck for long-term forecasting of time series.</p><p>To reason about the intricate temporal patterns, we try to take the idea of decomposition, which is a standard method in time series analysis <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b31">27]</ref>. It can be used to process the complex time series and extract more predictable components. However, under the forecasting context, it can only be used as the pre-processing of past series because the future is unknown <ref type="bibr" target="#b19">[15]</ref>. This common usage limits the capabilities of decomposition and overlooks the potential future interactions among decomposed components. Thus, we attempt to go beyond pre-processing usage of decomposition and propose a generic architecture to empower the deep forecasting models with immanent capacity of progressive decomposition. Further, decomposition can ravel out the entangled temporal patterns and highlight the inherent properties of time series <ref type="bibr" target="#b19">[15]</ref>. Benefiting from this, we try to take advantage of the series periodicity to renovate the point-wise connection in self-attention. We observe that the sub-series at the same phase position among periods often present similar temporal processes. Thus, we try to construct a series-level connection based on the process similarity derived by series periodicity.</p><p>Based on the above motivations, we propose an original Autoformer in place of the Transformers for long-term time series forecasting. Autoformer still follows residual and encoder-decoder structure but renovates Transformer into a decomposition forecasting architecture. By embedding our proposed decomposition blocks as the inner operators, Autoformer can progressively separate the long-term trend information from predicted hidden variables. This design allows our model to alternately decompose and refine the intermediate results during the forecasting procedure. Inspired by the stochastic process theory <ref type="bibr" target="#b12">[8,</ref><ref type="bibr" target="#b28">24]</ref>, Autoformer introduces an Auto-Correlation mechanism in place of self-attention, which discovers the sub-series similarity based on the series periodicity and aggregates similar sub-series from underlying periods. This series-wise mechanism achieves O(L log L) complexity for length-L series and breaks the information utilization bottleneck by expanding the point-wise representation aggregation to sub-series level. Autoformer achieves the state-of-the-art accuracy on six benchmarks. The contributions are summarized as follows:</p><p>• To tackle the intricate temporal patterns of the long-term future, we present Autoformer as a decomposition architecture and design the inner decomposition block to empower the deep forecasting model with immanent progressive decomposition capacity. • We propose an Auto-Correlation mechanism with dependencies discovery and information aggregation at the series level. Our mechanism is beyond previous self-attention family and can simultaneously benefit the computation efficiency and information utilization. • Autoformer achieves a 38% relative improvement under the long-term setting on six benchmarks, covering five real-world applications: energy, traffic, economics, weather and disease.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Models for Time Series Forecasting</head><p>Due to the immense importance of time series forecasting, various models have been well developed. Many time series forecasting methods start from the classic tools <ref type="bibr" target="#b36">[32,</ref><ref type="bibr" target="#b13">9]</ref>. ARIMA <ref type="bibr" target="#b10">[6,</ref><ref type="bibr" target="#b9">5]</ref> tackles the forecasting problem by transforming the non-stationary process to stationary through differencing. The filtering method is also introduced for series forecasting <ref type="bibr" target="#b22">[18,</ref><ref type="bibr" target="#b14">10]</ref>. Besides, recurrent neural networks (RNNs) models are used to model the temporal dependencies for time series <ref type="bibr" target="#b40">[36,</ref><ref type="bibr" target="#b30">26,</ref><ref type="bibr" target="#b44">40,</ref><ref type="bibr" target="#b26">22]</ref>. DeepAR <ref type="bibr" target="#b32">[28]</ref> combines autoregressive methods and RNNs to model the probabilistic distribution of future series. LSTNet <ref type="bibr" target="#b23">[19]</ref> introduces convolutional neural networks (CNNs) with recurrent-skip connections to capture the short-term and long-term temporal patterns. Attention-based RNNs <ref type="bibr" target="#b43">[39,</ref><ref type="bibr" target="#b34">30,</ref><ref type="bibr" target="#b35">31]</ref> introduce the temporal attention to explore the long-range dependencies for prediction. Also, many works based on temporal convolution networks (TCN) <ref type="bibr" target="#b38">[34,</ref><ref type="bibr" target="#b8">4,</ref><ref type="bibr" target="#b7">3,</ref><ref type="bibr" target="#b33">29]</ref> attempt to model the temporal causality with the causal convolution. These deep forecasting models mainly focus on the temporal relation modeling by recurrent connections, temporal attention or causal convolution.</p><p>Recently, Transformers <ref type="bibr" target="#b39">[35,</ref><ref type="bibr" target="#b42">38]</ref> based on the self-attention mechanism shows great power in sequential data, such as natural language processing <ref type="bibr" target="#b15">[11,</ref><ref type="bibr" target="#b11">7]</ref>, audio processing <ref type="bibr" target="#b18">[14]</ref> and even computer vision <ref type="bibr" target="#b16">[12,</ref><ref type="bibr" target="#b25">21]</ref>. However, applying self-attention to long-term time series forecasting is computationally prohibitive because of the quadratic complexity of sequence length L in both memory and time.</p><p>LogTrans <ref type="bibr" target="#b24">[20]</ref> introduces the local convolution to Transformer and proposes the LogSparse attention to select time steps following the exponentially increasing intervals, which reduces the complexity to O(L(log L) 2 ). Reformer <ref type="bibr" target="#b21">[17]</ref> presents the local-sensitive hashing (LSH) attention and reduces the complexity to O(L log L). Informer <ref type="bibr" target="#b45">[41]</ref> extends Transformer with KL-divergence based ProbSparse attention and also achieves O(L log L) complexity. Note that these methods are based on the vanilla Transformer and try to improve the self-attention mechanism to a sparse version, which still follows the point-wise dependency and aggregation. In this paper, our proposed Auto-Correlation mechanism is based on the inherent periodicity of time series and can provide series-wise connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decomposition of Time Series</head><p>As a standard method in time series analysis, time series decomposition <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b31">27]</ref> deconstructs a time series into several components, each representing one of the underlying categories of patterns that are more predictable. It is primarily useful for exploring historical changes over time. For the forecasting tasks, decomposition is always used as the pre-processing of historical series before predicting future series <ref type="bibr" target="#b19">[15,</ref><ref type="bibr" target="#b6">2]</ref>, such as Prophet <ref type="bibr" target="#b37">[33]</ref> with trend-seasonality decomposition and N-BEATS <ref type="bibr" target="#b27">[23]</ref> with basis expansion and DeepGLO <ref type="bibr" target="#b33">[29]</ref> with matrix decomposition. However, such pre-processing is limited by the plain decomposition effect of historical series and overlooks the hierarchical interaction between the underlying patterns of series in the long-term future. This paper takes the decomposition idea from a new progressive dimension. Our Autoformer harnesses the decomposition as an inner block of deep models, which can progressively decompose the hidden series throughout the whole forecasting process, including both the past series and the predicted intermediate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Autoformer</head><p>The time series forecasting problem is to predict the most probable length-O series in the future given the past length-I series, denoting as input-I-predict-O. The long-term forecasting setting is to predict the long-term future, i.e. larger O. As aforementioned, we have highlighted the difficulties of long-term series forecasting: handling intricate temporal patterns and breaking the bottleneck of computation efficiency and information utilization. To tackle these two challenges, we introduce the decomposition as a builtin block to the deep forecasting model and propose Autoformer as a decomposition architecture. Besides, we design the Auto-Correlation mechanism to discover the period-based dependencies and aggregate similar sub-series from underlying periods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decomposition Architecture</head><p>We renovate Transformer <ref type="bibr" target="#b39">[35]</ref> to a deep decomposition architecture (Figure <ref type="figure" target="#fig_0">1</ref>), including the inner series decomposition block, Auto-Correlation mechanism, and corresponding Encoder and Decoder.</p><p>Series decomposition block To learn with the complex temporal patterns in long-term forecasting context, we take the idea of decomposition <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b31">27]</ref>, which can separate the series into trend-cyclical and seasonal parts. These two parts reflect the long-term progression and the seasonality of the series respectively. However, directly decomposing is unrealizable for future series because the future is just unknown. To tackle this dilemma, we present a series decomposition block as an inner operation of Autoformer (Figure <ref type="figure" target="#fig_0">1</ref>), which can extract the long-term stationary trend from predicted intermediate hidden variables progressively. Concretely, we adapt the moving average to smooth out periodic fluctuations and highlight the long-term trends. For length-L input series X ∈ R L×d , the process is:</p><formula xml:id="formula_0">X t = AvgPool(Padding(X )) X s = X − X t ,<label>(1)</label></formula><p>where X s , X t ∈ R L×d denote the seasonal and the extracted trend-cyclical part respectively. We adopt the AvgPool(•) for moving average with the padding operation to keep the series length unchanged. We use X s , X t = SeriesDecomp(X ) to summarize above equations, which is a model inner block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model inputs</head><p>The inputs of encoder part are the past I time steps X en ∈ R I×d . As a decomposition architecture (Figure <ref type="figure" target="#fig_0">1</ref>), the input of Autoformer decoder contains both the seasonal part X des ∈ R ( I 2 +O)×d and trend-cyclical part X det ∈ R ( I 2 +O)×d to be refined. Each initialization consists of two parts: the component decomposed from the latter half of encoder's input X en with length I 2 to provide recent information, placeholders with length O filled by scalars. It's formulized as follows: where 1 Introduction</p><formula xml:id="formula_1">X ens , X ent = SeriesDecomp(X en I 2 :I ) X des = Concat(X ens , X 0 ) X det = Concat(X ent , X Mean ),<label>(2)</label></formula><formula xml:id="formula_2">X ens , X ent ∈ R I … L Q K V FFT x FFT</formula><formula xml:id="formula_3">⌧ 1 ⌧ 2 ⌧ k R(⌧ 1 ) R(⌧ 2 ) R(⌧ k ) 1 test haixu wu April 2021 1 Introduction ⌧ 1 ⌧ 2 ⌧ k R(⌧ 1 ) R(⌧ 2 ) R(⌧ k ) 1 test haixu wu April 2021 1 Introduction ⌧ 1 ⌧ 2 ⌧ k R(⌧ 1 ) R(⌧ 2 ) R(⌧ k ) 1 test haixu wu April 2021 1 Introduction ⌧ 1 ⌧ 2 ⌧ k R(⌧ 1 ) R(⌧ 2 ) R(⌧ k ) 1 test haixu wu April 2021 1 Introduction ⌧1 ⌧2 ⌧k R(⌧1) R(⌧2) R(⌧k) 1 test haixu wu April 2021 1 Introduction ⌧1 ⌧2 ⌧k R(⌧1) R(⌧2) R(⌧k)</formula><p>1 The final prediction is the sum of the two refined decomposed components, as W S * X M de + T M de , where W S is to project the deep transformed seasonal component X M de to the target dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Auto-Correlation Mechanism</head><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we propose the Auto-Correlation mechanism with series-wise connections to expand the information utilization. Auto-Correlation discovers the period-based dependencies by calculating the series autocorrelation and aggregates similar sub-series by time delay aggregation.</p><p>Period-based dependencies It is observed that the same phase position among periods naturally provides similar sub-processes.</p><p>Inspired by the stochastic process theory <ref type="bibr" target="#b12">[8,</ref><ref type="bibr" target="#b28">24]</ref>, for a real discrete-time process {X t }, we can obtain the autocorrelation R X X (τ ) by the following equations:</p><formula xml:id="formula_4">R X X (τ ) = lim L→∞ 1 L L t=1 X t X t−τ .<label>(5)</label></formula><p>R X X (τ ) reflects the time-delay similarity between {X t } and its τ lag series {X t−τ }. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we use the autocorrelation R(τ ) as the unnormalized confidence of estimated period length τ . Then, we choose the most possible k period lengths τ 1 , • • • , τ k . The period-based dependencies are derived by the above estimated periods and can be weighted by the corresponding autocorrelation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time delay aggregation</head><p>The period-based dependencies connect the sub-series among estimated periods. Thus, we present the time delay aggregation block (Figure <ref type="figure" target="#fig_1">2</ref>), which can roll the series based on selected time delay τ 1 , • • • , τ k . This operation can align similar sub-series that are at the same phase position of estimated periods, which is different from the point-wise dot-product aggregation in self-attention family. Finally, we aggregate the sub-series by softmax normalized confidences.</p><p>For the single head situation and time series X with length-L, after the projector, we get query Q, key K and value V. Thus, it can replace self-attention seamlessly. The Auto-Correlation mechanism is:</p><formula xml:id="formula_5">τ 1 , • • • , τ k = arg Topk τ ∈{1,••• ,L} (R Q,K (τ )) R Q,K (τ 1 ), • • • , R Q,K (τ k ) = SoftMax (R Q,K (τ 1 ), • • • , R Q,K (τ k )) Auto-Correlation(Q, K, V) = k i=1 Roll(V, τ k ) R Q,K (τ k ),<label>(6)</label></formula><p>where arg Topk(•) is to get the arguments of the Topk autocorrelations and let k = c × log L , c is a hyper-parameter. R Q,K is autocorrelation between series Q and K. Roll(X , τ ) represents the operation to X with time delay τ , during which elements that are shifted beyond the first position are re-introduced at the last position. For the encoder-decoder Auto-Correlation (Figure <ref type="figure" target="#fig_0">1</ref>), K, V are from the encoder X N en and will be resized to length-O, Q is from the previous block of the decoder. For the multi-head version used in Autoformer, with hidden variables of d model channels, h heads, the query, key and value for i-th head are</p><formula xml:id="formula_6">Q i , K i , V i ∈ R L× d model h , i ∈ {1, • • • , h}.</formula><p>The process is:</p><formula xml:id="formula_7">MultiHead(Q, K, V) = W output * Concat(head 1 , • • • , head h ) where head i = Auto-Correlation(Q i , K i , V i ).<label>(7)</label></formula><p>Efficient computation For period-based dependencies, these dependencies point to sub-processes at the same phase position of underlying periods and are inherently sparse. Here, we select the most possible delays to avoid picking the opposite phases. Because we aggregate O(log L) series whose length is L, the complexity of Equations 6 and 7 is O(L log L). For the autocorrelation computation (Equation <ref type="formula" target="#formula_4">5</ref>), given time series {X t }, R X X (τ ) can be calculated by Fast Fourier Transforms (FFT) based on the Wiener-Khinchin theorem <ref type="bibr" target="#b41">[37]</ref>:</p><formula xml:id="formula_8">S X X (f ) = F (X t ) F * (X t ) = ∞ −∞ X t e −i2πtf dt ∞ −∞ X t e −i2πtf dt R X X (τ ) = F −1 (S X X (f )) = ∞ −∞ S X X (f )e i2πf τ df,<label>(8)</label></formula><p>where τ ∈ {1, • • • , L}, F denotes the FFT and F −1 is its inverse. * denotes the conjugate operation and S X X (f ) is in the frequency domain. Note that the series autocorrelation of all lags in {1, • • • , L} can be calculated at once by FFT. Thus, Auto-Correlation achieves the O(L log L) complexity.</p><p>Auto-Correlation vs. self-attention family Different from the point-wise self-attention family, Auto-Correlation presents the series-wise connections (Figure <ref type="figure" target="#fig_2">3</ref>). Concretely, for the temporal dependencies, we find the dependencies among sub-series based on the periodicity. In contrast, the self-attention family only calculates the relation between scattered points. Though some selfattentions <ref type="bibr" target="#b24">[20,</ref><ref type="bibr" target="#b45">41]</ref> consider the local information, they only utilize this to help point-wise dependencies discovery. For the information aggregation, we adopt the time delay block to aggregate the similar sub-series from underlying periods. In contrast, self-attentions aggregate the selected points by dot-product. Benefiting from the inherent sparsity and sub-series-level representation aggregation, Auto-Correlation can simultaneously benefit the computation efficiency and information utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We extensively evaluate the proposed Autoformer on six real-world benchmarks, covering five mainstream time series forecasting applications: energy, traffic, economics, weather and disease.</p><p>Datasets Here is a description of the six experiment datasets: (1) ETT <ref type="bibr" target="#b45">[41]</ref> dataset contains the data collected from electricity transformers, including load and oil temperature that are recorded every </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>To compare performances under different future horizons, we fix the input length and evaluate models with a wide range of prediction lengths: 96, 192, 336, 720. This setting precisely meets the definition of long-term forecasting. Here are results on both the multivariate and univariate settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multivariate results</head><p>As for the multivariate setting, Autoformer achieves the consistent state-ofthe-art performance in all benchmarks and all prediction length settings (Table <ref type="table" target="#tab_1">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Univariate results</head><p>We list the univariate results of two typical datasets in Table <ref type="table">2</ref>. Under the comparison with extensive baselines, our Autoformer still achieves state-of-the-art performance for the long-term forecasting tasks. In particular, for the input-96-predict-336 setting, our model achieves 14% (0.180→0.145) MSE reduction on the ETT dataset with obvious periodicity. For the Exchange dataset without obvious periodicity, Autoformer surpasses other baselines by 17% (0.611→0.508) and shows greater long-term forecasting capacity. Also, we find that ARIMA <ref type="bibr" target="#b5">[1]</ref> performs best in the input-96-predict-96 setting of the Exchange dataset but fails in the long-term setting. This situation of ARIMA can be benefited from its inherent capacity for non-stationary economic data but is limited by the intricate temporal patterns of real-world series. Decomposition architecture With our proposed progressive decomposition architecture, other models can gain consistent promotion, especially as the prediction length O increases (Table <ref type="table" target="#tab_3">3</ref>). This verifies that our method can generalize to other models and release the capacity of other dependencies learning mechanisms, alleviate the distraction caused by intricate patterns. Besides, our architecture outperforms the pre-processing, although the latter employs a bigger model and more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation studies</head><p>Especially, pre-decomposing may even bring negative effect because it neglects the interaction of components during long-term future, such as Transformer <ref type="bibr" target="#b39">[35]</ref> predict-720, Informer <ref type="bibr" target="#b45">[41]</ref> predict-336.</p><p>Auto-Correlation vs. self-attention family As shown in Table <ref type="table" target="#tab_4">4</ref>, our proposed Auto-Correlation achieves the best performance under various input-I-predict-O settings, which verifies the effectiveness of series-wise connections comparing to point-wise self-attentions (Figure <ref type="figure" target="#fig_2">3</ref>). Furthermore, we can also observe that Auto-Correlation is memory efficiency from the last column of Table <ref type="table" target="#tab_4">4</ref>, which can be used in long sequence forecasting, such as input-336-predict-1440. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Analysis</head><p>Time series decomposition As shown in Figure <ref type="figure" target="#fig_4">4</ref>, without our series decomposition block, the forecasting model cannot capture the increasing trend and peaks of the seasonal part. By adding the series decomposition blocks, Autoformer can aggregate and refine the trend-cyclical part from series progressively. This design also facilitates the learning of the seasonal part, especially the peaks and troughs. This verifies the necessity of our proposed progressive decomposition architecture.  de and trend-cyclical T M de of the last decoder layer. We gradually add the decomposition blocks in decoder from left to right. This case is from ETT dataset under input-96-predict-720 setting. For clearness, we add the linear growth to raw data additionally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependencies learning</head><p>The marked time delay sizes in Figure <ref type="figure" target="#fig_5">5</ref>(a) indicate the most likely periods. Our learned periodicity can guide the model to aggregate the sub-series from the same or neighbor phase of periods by Roll(X , τ i ), i ∈ {1, • • • , 6}. For the last time step (declining stage), Auto-Correlation fully utilizes all similar sub-series without omissions or errors compared to self-attentions. This verifies that Autoformer can discover the relevant information more sufficiently and precisely.</p><p>Complex seasonality modeling As shown in Figure <ref type="figure" target="#fig_6">6</ref>, the lags that Autoformer learns from deep representations can indicate the real seasonality of raw series. For example, the learned lags of the      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper studies the long-term forecasting problem of time series, which is a pressing demand for real-world applications. However, the intricate temporal patterns prevent the model from learning reliable dependencies. We propose the Autoformer as a decomposition architecture by embedding the series decomposition block as an inner operator, which can progressively aggregate the longterm trend part from intermediate prediction. Besides, we design an efficient Auto-Correlation mechanism to conduct dependencies discovery and information aggregation at the series level, which contrasts clearly from the previous self-attention family. Autoformer can naturally achieve O(L log L) complexity and yield consistent state-of-the-art performance in extensive real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Autoformer architecture. The encoder eliminates the long-term trend-cyclical part by series decomposition blocks (blue blocks) and focuses on seasonal patterns modeling. The decoder accumulates the trend part extracted from hidden variables progressively. The past seasonal information from encoder is utilized by the encoder-decoder Auto-Correlation (center green block in decoder).</figDesc><graphic url="image-1.png" coords="4,170.99,144.61,309.95,84.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Auto-Correlation (left) and Time Delay Aggregation (right). We utilize the Fast Fourier Transform to calculate the autocorrelation R(τ ), which reflects the time-delay similarities. Then the similar sub-processes are rolled to the same index based on selected delay τ and aggregated by R(τ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Auto-Correlation vs. self-attention family. Full Attention [35] (a) adapts the fully connection among all time points. Sparse Attention [17, 41] (b) selects points based on the proposed similarity metrics. LogSparse Attention [20] (c) chooses points following the exponentially increasing intervals. Auto-Correlation (d) focuses on the connections of sub-series among underlying periods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of learned seasonal X Mde and trend-cyclical T M de of the last decoder layer. We gradually add the decomposition blocks in decoder from left to right. This case is from ETT dataset under input-96-predict-720 setting. For clearness, we add the linear growth to raw data additionally.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of learned dependencies. For clearness, we select the top-6 time delay sizes τ 1 , • • • , τ 6 of Auto-Correlation and mark them in raw series (red lines). For self-attentions, top-6 similar points with respect to the last time step (red stars) are also marked by orange points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Statistics of learned lags. For each time series in the test set, we count the top 10 lags learned by decoder for the input-96-predict-336 task. Figure (a)-(d) are the density histograms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Efficiency Analysis. For memory, we replace Auto-Correlation with self-attention family in Autoformer and record the memory with input 96. For running time, we run the Auto-Correlation or self-attentions 10 3 times to get the execution time per step. The output length increases exponentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Multivariate results with different prediction lengths O ∈ {96, 192, 336, 720}. We set the input length I as 36 for ILI and 96 for the others. A lower MSE or MAE indicates a better prediction.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation of decomposition in multivariate ETT with MSE metric. Ours adopts our progressive architecture into other models. Sep employs two models to forecast pre-decomposed seasonal and trend-cyclical components separately. Promotion is the MSE reduction compared to Origin.</figDesc><table><row><cell cols="2">Input-96 Transformer[35]</cell><cell>Informer[41]</cell><cell>LogTrans[17]</cell><cell>Reformer[20]</cell><cell>Promotion</cell></row><row><cell cols="6">Predict-O Origin Sep Ours Origin Sep Ours Origin Sep Ours Origin Sep Ours Sep Ours</cell></row><row><cell>96</cell><cell cols="5">0.604 0.311 0.204 0.365 0.490 0.354 0.768 0.862 0.231 0.658 0.445 0.218 0.069 0.347</cell></row><row><cell>192</cell><cell cols="5">1.060 0.760 0.266 0.533 0.658 0.432 0.989 0.533 0.378 1.078 0.510 0.336 0.300 0.562</cell></row><row><cell>336</cell><cell cols="5">1.413 0.665 0.375 1.363 1.469 0.481 1.334 0.762 0.362 1.549 1.028 0.366 0.434 1.019</cell></row><row><cell>720</cell><cell cols="5">2.672 3.200 0.537 3.379 2.766 0.822 3.048 2.601 0.539 2.631 2.845 0.502 0.079 2.332</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of Auto-Correlation and self-attention in the multivariate ETT. We replace the Auto-Correlation in Autoformer with different self-attentions. The "-" indicates the out-of-memory.</figDesc><table><row><cell cols="2">Input Length I</cell><cell></cell><cell>96</cell><cell></cell><cell></cell><cell>192</cell><cell></cell><cell></cell><cell>336</cell></row><row><cell cols="2">Prediction Length O</cell><cell>336</cell><cell>720</cell><cell>1440</cell><cell>336</cell><cell>720</cell><cell>1440</cell><cell>336</cell><cell>720</cell><cell>1440</cell></row><row><cell>Auto-</cell><cell>MSE</cell><cell cols="9">0.339 0.422 0.555 0.355 0.429 0.503 0.361 0.425 0.574</cell></row><row><cell>Correlation</cell><cell cols="10">MAE 0.372 0.419 0.496 0.392 0.430 0.484 0.406 0.440 0.534</cell></row><row><cell>Full</cell><cell>MSE</cell><cell cols="5">0.375 0.537 0.667 0.450 0.554</cell><cell>-</cell><cell cols="2">0.501 0.647</cell><cell>-</cell></row><row><cell cols="7">Attention[35] MAE 0.425 0.502 0.589 0.470 0.533</cell><cell>-</cell><cell cols="2">0.485 0.491</cell><cell>-</cell></row><row><cell>LogSparse</cell><cell>MSE</cell><cell cols="8">0.362 0.539 0.582 0.420 0.552 0.958 0.474 0.601</cell><cell>-</cell></row><row><cell cols="10">Attention[20] MAE 0.413 0.522 0.529 0.450 0.513 0.736 0.474 0.524</cell><cell>-</cell></row><row><cell>LSH</cell><cell>MSE</cell><cell cols="8">0.366 0.502 0.663 0.407 0.636 1.069 0.442 0.615</cell><cell>-</cell></row><row><cell cols="10">Attention[17] MAE 0.404 0.475 0.567 0.421 0.571 0.756 0.476 0.532</cell><cell>-</cell></row><row><cell>ProbSparse</cell><cell>MSE</cell><cell cols="9">0.481 0.822 0.715 0.404 1.148 0.732 0.417 0.631 1.133</cell></row><row><cell cols="11">Attention[41] MAE 0.472 0.559 0.586 0.425 0.654 0.602 0.434 0.528 0.691</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="35" xml:id="foot_0">35th Conference on Neural Information Processing Systems (NeurIPS 2021).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">×d denote the seasonal and trend-cyclical parts of X en respectively, and X 0 , X Mean ∈ R O×d denote the placeholders filled with zero and the mean of X en respectively.Encoder As shown in Figure1, the encoder focuses on the seasonal part modeling. The output of the encoder contains the past seasonal information and will be used as the cross information to help the decoder refine prediction results. Suppose we have N encoder layers. The overall equations for l-th encoder layer are summarized as X l en = Encoder(X l−1 en ). Details are shown as follows:S l,1 en , = SeriesDecomp Auto-Correlation(X l−1 en ) + X l−1 en S l,2 en , = SeriesDecomp FeedForward(S l,1 en ) + S l,1 en ,(3)where " " is the eliminated trend part. X l en = S l,2 en , l ∈ {1, • • • , N } denotes the output of l-th encoder layer and X 0 en is the embedded X en . S l,i en , i ∈ {1, 2} represents the seasonal component after the i-th series decomposition block in the l-th layer respectively. We will give detailed description of Auto-Correlation(•) in the next section, which can seamlessly replace the self-attention.Decoder The decoder contains two parts: the accumulation structure for trend-cyclical components and the stacked Auto-Correlation mechanism for seasonal components (Figure1). Each decoder layer contains the inner Auto-Correlation and encoder-decoder Auto-Correlation, which can refine the prediction and utilize the past seasonal information respectively. Note that the model extracts the potential trend from the intermediate hidden variables during the decoder, allowing Autoformer to progressively refine the trend prediction and eliminate interference information for period-based dependencies discovery in Auto-Correlation. Suppose there are M decoder layers. With the latent variable X N en from the encoder, the equations of l-th decoder layer can be summarized as X l de = Decoder(X l−1 de , X N en ). The decoder can be formalized as follows:S l,1 de , T l,1 de = SeriesDecomp Auto-Correlation(X l−1 de ) + X l−1 de S l,2 de , T l,2 de = SeriesDecomp Auto-Correlation(S l,1 de , X N en ) + S l,1 de S l,3 de , T l,3 de = SeriesDecomp FeedForward(S l,2 de ) + S l,2 deT l de = T l−1 de + W l,1 * T l,1 de + W l,2 * T l,2 de + W l,3 * T l,3 de ,(4)where X l de = S l,3 de , l ∈ {1, • • • , M } denotes the output of l-th decoder layer. X 0 de is embedded from X des for deep transform and T 0 de = X det is for accumulation. S l,i de , T l,i de , i ∈ {1, 2,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">3} represent the seasonal component and trend-cyclical component after the i-th series decomposition block in the l-th layer respectively. W l,i , i ∈ {1, 2, 3} represents the projector for the i-th extracted trend T l,i de .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported by the National Natural Science Foundation of China under Grants 62022050 and 62021002, Beijing Nova Program under Grant Z201100006820041, China's Ministry of Industry and Information Technology, the MOE Innovation Plan and the BNRist Innovation Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Models Autoformer</title>
		<author>
			<persName><forename type="first">Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae Ett *</forename><surname>Metric</surname></persName>
		</author>
		<idno>96 0.255 0.339 0.365 0.453 0.768 0.642 0.658 0.619 3.142 1.365 2.041 1.073 3.041 1.330 192 0.281 0.340 0.533 0.563 0.989 0.757 1.078 0.827 3.154 1.369 2.249 1.112 3.072 1.339 336 0.339 0.372 1.363 0.887 1.334 0.872 1.549 0.972 3.160 1.369 2.568 1.238 3.105 1.348 720 0.422 0.419 3.379 1.388 3.048 1.328 2.631 1.242 3.171 1.368 2.720 1.287 3.135 1.354 Electricity 96 0.201 0.317 0.274 0.368 0.258 0.357 0.312 0.402 0.680 0.645 0.375 0.437 0.985 0.813 192 0.222 0.334 0.296 0.386 0.266 0.368 0.348 0.433 0.725 0.676 0.442 0.473 0.996 0.821 336 0.231 0.338 0.300 0.394 0.280 0.380 0.350 0.433 0.828 0.727 0.439 0.473 1.000 0.824 720 0.254 0.361 0.373 0.439 0.283 0.376 0.340 0.420 0.957 0.811 0.980 0.814 1.438 0.784</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exchange</title>
		<idno>96 0.197 0.323 0.847 0.752 0.968 0.812 1.065 0.829 1.551 1.058 1.453 1.049 3.004 1.432 192 0.300 0.369 1.204 0.895 1.040 0.851 1.188 0.906 1.477 1.028 1.846 1.179 3.048 1.444 336 0.509 0.524 1.672 1.036 1.659 1.081 1.357 0.976 1.507 1.031 2.136 1.231 3.113 1.459 720 1.447 0.941 2.478 1.310 1.941 1.127 1.510 1.016 2.285 1.243 2.984 1.427 3.150 1.458</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<idno>96 0.613 0.388 0.719 0.391 0.684 0.384 0.732 0.423 1.107 0.685 0.843 0.453 1.438 0.784 192 0.616 0.382 0.696 0.379 0.685 0.390 0.733 0.420 1.157 0.706 0.847 0.453 1.463 0.794 336 0.622 0.337 0.777 0.420 0.733 0.408 0.742 0.420 1.216 0.730 0.853 0.455 1.479 0.799 720 0.660 0.408 0.864 0.472 0.717 0.396 0.755 0.423 1.481 0.805 1.500 0.805 1.499 0.804</idno>
	</analytic>
	<monogr>
		<title level="j">Traffic</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weather 3 is recorded every 10 minutes for 2020 whole year, which contains 21 meteorological indicators, such as air temperature, humidity, etc. (6) ILI 4 includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients. We follow standard protocol and split all datasets into training, validation and test set in chronological order by the ratio of 6:2:2 for the ETT dataset and 7:1:2 for the other datasets</title>
		<author>
			<persName><forename type="first">Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae Mse</forename><surname>Metric</surname></persName>
		</author>
		<author>
			<persName><surname>Mae</surname></persName>
		</author>
		<idno>ETT 96 0.065 0.189 0.082 0.219 0.088 0.225 0.082 0.217 0.131 0.288 0.099 0.237 0.287 0.456 0.211 0.362 192 0.118 0.256 0.120 0.268 0.132 0.283 0.133 0.284 0.186 0.354 0.154 0.310 0.312 0.483 0.261 0.406 336 0.154 0.305 0.226 0.370 0.180 0.336 0.201 0.361 0.220 0.381 0.277 0.428 0.331 0.474 0.317 0.448 720 0.182 0.335 0.188 0.338 0.300 0.435 0.268 0.407 0.267 0.430 0.332 0.468 0.534 0.593 0.366 0.487</idno>
		<ptr target="https://www.bgc-jena.mpg.de/wetter/4https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html" />
	</analytic>
	<monogr>
		<title level="m">* ETT means the ETTm2. See supplementary materials for the full benchmark of ETTh1, ETTh2, ETTm1. 15 minutes between</title>
				<imprint>
			<date type="published" when="1990">July 2016 and July 2018. 2012 to 2014. 1990 to 2016</date>
		</imprint>
	</monogr>
	<note>Table 2: Univariate results with different prediction lengths O ∈ {96, 192, 336, 720} on typical datasets. We set the input length I as 96. A lower MSE or MAE indicates a better prediction</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exchange</title>
		<idno>96 0.241 0.387 0.156 0.299 0.591 0.615 0.279 0.441 1.327 0.944 0.417 0.515 0.828 0.762 0.112 0.245 192 0.273 0.403 0.669 0.665 1.183 0.912 1.950 1.048 1.258 0.924 0.813 0.735 0.909 0.974 0.304 0.404 336 0.508 0.539 0.611 0.605 1.367 0.984 2.438 1.262 2.179 1.296 1.331 0.962 1.304 0.988 0.736 0.598 720 0.991 0.768 1.111 0.860 1.872 1.072 2.010 1.247 1.280 0.953 1.894 1.181 3.238 1.566 1.871 0.935</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Time-series</title>
		<author>
			<persName><forename type="first">O</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. (Series D)</title>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
	<note>2nd edn</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A spatio-temporal decomposition based deep neural network for time series forecasting</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><forename type="middle">C</forename><surname>Regan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Borovykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Bohte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelis</forename><forename type="middle">W</forename><surname>Oosterlee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04691</idno>
		<title level="m">Conditional time series forecasting with convolutional neural networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Time series analysis, forecasting and control</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwilym</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Some recent advances in forecasting and control</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwilym</forename><forename type="middle">M</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. (Series-C</title>
		<imprint>
			<biblScope unit="page">1968</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The analysis of time series: an introduction</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Chatfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data-driven prediction of general hamiltonian dynamics via learning exactlysymplectic maps</title>
		<author>
			<persName><forename type="first">Renyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Molei</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Normalizing kalman filters for multivariate time series analysis</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>De Bézenac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syama</forename><surname>Sundar Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Benidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bohlke-Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Kurle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hilaf</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Januschowski</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Music transformer</title>
		<author>
			<persName><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Forecasting: principles and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><surname>Athanasopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep rao-blackwellised particle filters for time series forecasting</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Kurle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syama</forename><surname>Sundar Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>De Bézenac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyou</forename><surname>Yao Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep factors with gaussian processes for forecasting</title>
		<author>
			<persName><forename type="first">Danielle</forename><forename type="middle">C</forename><surname>Maddix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00098</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">N-BEATS: Neural basis expansion analysis for interpretable time series forecasting</title>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Boris N Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probability, random variables and stochastic processes</title>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Papoulis</surname></persName>
		</author>
		<author>
			<persName><surname>Saunders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adam Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep state space models for time series forecasting</title>
		<author>
			<persName><forename type="first">Syama</forename><surname>Sundar Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><forename type="middle">W</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Januschowski</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">STL: A seasonal-trend decomposition procedure based on loess</title>
		<author>
			<persName><forename type="first">Cleveland</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terpenning</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><surname>Irma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Off. Stat</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepAR: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Forecast</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting</title>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal pattern attention for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">Shun-Yao</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan-Keng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attend and diagnose: Clinical time series analysis using attention models</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepta</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayaraman</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Spanias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Methodology for long-term prediction of time series</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Sorjamaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Reyhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongnan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amaury</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Forecasting at scale</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Stat</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A multi-horizon quantile recurrent forecaster</title>
		<author>
			<persName><forename type="first">Ruofeng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kari</forename><surname>Torkkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balakrishnan</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Madeka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generalized harmonic analysis</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Math</title>
		<imprint>
			<date type="published" when="1930">1930</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adversarial sparse transformer for time series forecasting</title>
		<author>
			<persName><forename type="first">Sifan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianggang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A dual-stage attention-based recurrent neural network for time series prediction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00073</idno>
		<title level="m">Long-term forecasting using tensor-train rnns</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
