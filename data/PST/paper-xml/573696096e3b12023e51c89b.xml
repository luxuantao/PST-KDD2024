<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Space Model for Road Networks to Predict Time-Varying Traffic</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dingxiong</forename><surname>Deng</surname></persName>
							<email>dingxiod@usc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
							<email>shahabi@usc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ugur</forename><surname>Demiryurek</surname></persName>
							<email>demiryur@usc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linhong</forename><surname>Zhu</surname></persName>
							<email>linhong@isi.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Space Model for Road Networks to Predict Time-Varying Traffic</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E3C7F5C1B3F80FE6092AB0A8054CBAB6</idno>
					<idno type="DOI">10.1145/2939672.2939860</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Latent space model</term>
					<term>real-time traffic forecasting</term>
					<term>road network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor datasets is an important problem for intelligent transportation systems and sustainability. However, it is challenging due to the complex topological dependencies and high dynamism associated with changing road conditions. In this paper, we propose a Latent Space Model for Road Networks (LSM-RN) to address these challenges holistically. In particular, given a series of road network snapshots, we learn the attributes of vertices in latent spaces which capture both topological and temporal properties. As these latent attributes are time-dependent, they can estimate how traffic patterns form and evolve. In addition, we present an incremental online algorithm which sequentially and adaptively learns the latent attributes from the temporal graph changes. Our framework enables real-time traffic prediction by 1) exploiting real-time sensor readings to adjust/update the existing latent spaces, and 2) training as data arrives and making predictions on-the-fly. By conducting extensive experiments with a large volume of real-world traffic sensor data, we demonstrate the superiority of our framework for real-time traffic prediction on large road networks over competitors as well as baseline graph-based LSM's.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recent advances in traffic sensing technology have enabled the acquisition of high-fidelity spatiotemporal traffic datasets. For example, at our research center, for the past five years, we have been collecting data from 15000 loop detectors installed on the highways and arterial streets of Los Angeles County, covering 3420 miles cumulatively (see the case study in <ref type="bibr" target="#b12">[13]</ref>). The collected data include several main traffic parameters such as occupancy, volume, and speed at the rate of 1 reading/sensor/min. These large scale data streams enable accurate traffic prediction, which in turn improves route navigation, traffic regulation, urban planning, etc.</p><p>The traffic prediction problem aims to predict the future travel speed of each and every edge of a road network, given the historical speed readings from the sensors on these edges. To solve Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. the traffic prediction problem, the majority of existing techniques utilize the historical information of an edge to predict its future travel-speed using regression techniques such as Auto-regressive Integrated Moving Average (ARIMA) <ref type="bibr" target="#b17">[18]</ref>, Support Vector Regression (SVR) <ref type="bibr" target="#b19">[20]</ref> and Gaussian Process (GP) <ref type="bibr" target="#b30">[31]</ref>. There are also studies that leverage spatial/topological similarities to predict the readings of an edge based on its neighbors in either the Euclidean space <ref type="bibr" target="#b10">[10]</ref> or the network space <ref type="bibr" target="#b13">[14]</ref>. Even though there are few notable exceptions such as Hidden Markov Model (HMM) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> that predict traffic of edges by collectively inferring temporal information, these approaches simply combine the local information of neighbors with temporal information. Furthermore, existing approaches such as GP and HMM are computationally expensive and require repeated offline trainings. Therefore, it is very difficult to adapt the models to real-time traffic forecasting.</p><p>Motivated by these challenges, we propose Latent Space Modeling for Road Networks (LSM-RN), which enables more accurate and scalable traffic prediction by utilizing both topology similarity and temporal correlations. Specifically, with LSM-RN, vertices of dynamic road network are embedded into a latent space, where two vertices that are similar in terms of both time-series traffic behavior and the road network topology are close to each other in the latent space. Recently, Latent Space Modeling has been successfully applied to several real-world problems such as community detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>, link prediction <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref> and sentiment analysis <ref type="bibr" target="#b31">[32]</ref>. Among them, the work on social networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref> (hereafter called LSM-SN) is most related to ours because in both scenarios data are represented as graphs and each vertex of these graphs has different attributes. However, none of the approaches to LSM-SN are suitable for both identifying the edge and/or sensor latent attributes in road networks and exploiting them for real-time traffic prediction due to the following reasons.</p><p>First, road networks show significant topological (e.g., travelspeeds between two sensors on the same road segment are similar), and temporal (e.g., travel-speeds measured every 1 minute on a particular sensor are similar) correlations. These correlations can be exploited to alleviate the missing data problem, which is unique to road networks, due to the fact that some road segments may contain no sensors and any sensor may occasionally fail to report data. Second, unlike social networks, LSM-RN is fast evolving due to the time-varying traffic conditions. On the contrary, social networks evolve smoothly and frequent changes are very unlikely (e.g., one user changes its political preferences twice a day). Instead, in road networks, traffic conditions on a particular road segment can change rapidly in a short time (i.e., time-dependent) because of rush/non-rush hours and traffic incidents. Third, LSM-RN is highly dynamic where fresh data come in a streaming fashion, whereas the connections (weights) between nodes in social net-works are mostly static. The dynamic nature requires frequent model updates (e.g., per minute), which necessitates partial updates of the model as opposed to the time-consuming full updates in LSM-SN. Finally, with LSM-RN, the ground truth can be observed shortly after making the prediction (by measuring the actual speed later in future), which also provides an opportunity to improve/adjust the model incrementally (i.e., online learning).</p><p>With our proposed LSM-RN, each dimension of the embedded latent space represents a latent attribute. Thus the attribute distribution of vertices and how the attributes interact with each other jointly determine the underlying traffic pattern. To enforce the topology of road network, LSM-RN adds a graph Laplacian constraint which not only enables global graph similarity, but also completes the missing data by a set of similar edges with non-zero readings. Subsequently, we incorporate the temporal properties into our LSM-RN model by considering time-dependent latent attributes and a global transition process. With these time-dependent latent attributes and the transition matrix, we are able to better model how traffic patterns form and evolve.</p><p>To infer the time-dependent latent attributes of our LSM-RN model, a typical method is to utilize multiplicative algorithms <ref type="bibr" target="#b15">[16]</ref> based on Non-negative Matrix Factorization, where we jointly infer the whole latent attributes via iterative updates until they become stable, termed as global learning. However, global learning is not only slow but also not practical for real-time traffic prediction. This is because, traffic data are of high-fidelity (i.e., updates are frequent in every one minute) and the actual ground-truth of traffic speed becomes available shortly afterwards (e.g., after making a prediction for the next five minutes, the ground truth data will be available instantly after five minutes). We thus propose an incremental online learning with which we sequentially and adaptively learn the latent attributes from the temporal traffic changes. In particular, each time when our algorithm makes a prediction with the latent attributes learned from the previous snapshot, it receives feedback from the next snapshot (i.e., the ground truth speed reading we already obtained) and subsequently modifies the latent attributes for more accurate predictions. Unlike traditional online learning which only performs one single update (e.g., update one vertex per prediction) per round, our goal is to make predictions for the entire road network, and thus the proposed online algorithm allows updating latent attributes of many correlated vertices simultaneously.</p><p>Leveraging global and incremental learning algorithms our LSM-RN model can strike a balance between accuracy and efficiency for real-time forecasting. Specifically, we consider a setting with a predefined time window where at each time window (e.g., 5 minutes), we learn our traffic model with the proposed incremental inference approach on-the-fly, and make predictions for the next time span. Meanwhile, we batch the re-computation of our traffic model at the end of one large time window (e.g., one hour). Under this setting, our LSM-RN model enables the following two properties: (1) realtime feedback information can be seamlessly incorporated into our framework to adjust for existing latent spaces, thus allowing for more accurate predictions, and (2) our algorithms perform training and predictions on-the-fly with small amount of data rather than requiring large training datasets.</p><p>We conducted extensive experiments on a large scale of realworld traffic sensor dataset. We demonstrated that the LSM-RN framework achieves better accuracy than that of both existing time series methods (e.g. ARIMA and SVR) and the LSM-SN approaches. Moreover, we show that our algorithm scales to large road networks. For example, it only takes 4 seconds to make a prediction for a network with 19,986 edges. Finally, we show that our batch window setting works perfectly for streaming data, alternating the executions of our global and incremental algorithms, which strikes a compromise between prediction accuracy and efficiency. For instance, incremental learning is one order of magnitude faster than global learning, and it requires less than 1 seconds to incorporate real-time feedback information.</p><p>The remainder of this paper is organized as follows. We discuss the related work in Section 2 and define our problem in Section 3. and explain LSM-RN in Section 4. We present the global learning and increment learning algorithms, and discuss how to adapt our algorithms for real-time traffic forecasting in Section 5. In Section 6, we report the experiment results and conclude the paper afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND AND RELATED WORKS 2.1 Traffic analysis</head><p>Many studies have been conducted to address the traffic prediction problem, but no single study so far has tackled all the challenges in a holistic manner. Some focused on missing values <ref type="bibr" target="#b18">[19]</ref> or missing sensors <ref type="bibr" target="#b29">[30]</ref>, but not both. Some studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref> utilize temporal data which models each sensor (or edge) independently and makes predictions using time series approaches (e.g., ARIMA <ref type="bibr" target="#b17">[18]</ref>, SVR <ref type="bibr" target="#b19">[20]</ref> and GP <ref type="bibr" target="#b30">[31]</ref>). For instance, Pan et. al. <ref type="bibr" target="#b17">[18]</ref> learns an enhanced ARIMA model for each edge in advance, and then performs traffic prediction on top of these models. Very few studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> utilize spatiotemporal model with correlated time series based on Hidden Markov Model, but only for small number of time series and not always using the network space as the spatial dimension (e.g., using Euclidean space <ref type="bibr" target="#b10">[10]</ref>). In <ref type="bibr" target="#b26">[27]</ref>, Xu et. al. consider using the newly arrived data as feedback to reward one classifier vs. the other but not for dynamically updating the model. Note that many existing studies <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref> on traffic prediction are based on GPS dataset, which is different with the sensor dataset, where we have fine-grained and steady readings from roadequipped sensors. We are not aware of any study that applies latent space modeling (considering both time and network topology) to real-time traffic prediction from incomplete (i.e., missing sensors and values) sensor datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Latent space model and NMF</head><p>Recently, many real data analytic problems such as community detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>, recommendation system <ref type="bibr" target="#b6">[6]</ref>, topic modeling <ref type="bibr" target="#b21">[22]</ref>, image clustering <ref type="bibr" target="#b4">[4]</ref>, and sentiment analysis <ref type="bibr" target="#b31">[32]</ref>, have been formulated as the problem of latent space learning. These studies assume that, given a graph, each vertex resides in a latent space with attributes, and vertices which are close to each other are more likely to be in the same cluster (e.g., community or topic) and form a link. In particular, the objective is to infer the latent matrix by minimizing the difference (e.g., squared loss <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref> or KL-divergence <ref type="bibr" target="#b4">[4]</ref>) between observed and estimated links. However, existing methods are not designed for the highly correlated (topologically and temporally) and dynamic road networks. Few studies <ref type="bibr" target="#b20">[21]</ref> have considered the temporal relationships in SN with the assumption that networks evolve over time. The temporal graph snapshots in <ref type="bibr" target="#b20">[21]</ref> are treated separately and thus newly observed data are not incorporated to improve the model. Compared with existing works, we explore the feasibility of modeling road networks with time-varying latent space. The traffic speed of a road segment is determined by their latent attributes and the interaction between corresponding attributes. To tackle the sparsity of road network, we utilize the graph topology by adding a graph Laplacian constraint to impute the missing values. In addition, the latent position of each vertex, varies over time and allows for sudden movement from one timestamp to the next timestamp via a transition matrix.</p><p>Different techniques have been proposed to learn the latent properties, where Non-negative Matrix Factorization (NMF) is one of the most popular methods thanks to ease of interpretability and flexibility. In this work, we explore the feasibility of applying dynamic NMF to traffic prediction domain. We design a global algorithm to infer the latent space based on the traditional multiplicative algorithm <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b15">16]</ref>. We further propose a topology-aware incremental algorithm, which adaptively updates the latent space representation for each node in the road network with topology constraints. The proposed algorithms differ from traditional online NMF algorithms such as <ref type="bibr" target="#b3">[3]</ref>, which independently perform the online update. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM DEFINITION</head><p>We denote a road network as a directed graph N = (V, E), where V is the set of vertices and E ∈ V × V is the set of edges, respectively. A vertex vi ∈ V models a road intersection or an end of road. An edge e(vi, vj), which connects two vertices, represents a directed network segment. Each edge e(vi, vj) is associated with a travel speed c(vi, vj) (e.g., 40 miles/hour). In addition, N has a corresponding adjacency matrix representation, denoted as G, whose (i, j) th entry represents the edge weight between the i th and j th vertices.</p><p>The road network snapshots are constructed from a large-scale, high resolution traffic sensor dataset (see detailed description of sensor data in Section 6). Specifically, a sensor s (i.e., a loop detector) is located at one segment of road network N , which provides a reading (e.g., 40 miles/hour) per sampling rate (e.g., 1 min). We divide one day into different intervals, where span is the length of each time interval. For example, when span = 5 minutes, we have 288 time intervals per day. For each time interval t, we aggregate (i.e., average) the readings of one sensor. Subsequently, for each edge segment of network N , we average all sensor readings located at that edge as its weight. Therefore, at each timestamp t, we have a road network snapshot Gt from traffic sensors.</p><p>Example. Figure <ref type="figure" target="#fig_1">1</ref> (a) shows a simple road network with 7 vertices and 10 edges at one timestamp. Three sensors (i.e., s1, s2, s3) are located in edges (v1, v2), (v3, v4) and (v7, v6) respectively, and each sensor provides an aggregated reading during the time interval. Figure <ref type="figure" target="#fig_1">1(b)</ref> shows the corresponding adjacent matrix after mapping the sensor readings to the road segments. Note that the sensor dataset is incomplete with both missing values (i.e., sensor fails to report data) and missing sensors (i.e., edges without any sensors). Here sensor s3 fails to provide reading, thus the edge weight of c(v3, v4) is ? due to missing value. In addition, the edge weight of c(v3, v2) is marked as × because of missing sensors. With a dynamic road network, we formally define the problem of edge traffic prediction with missing data as follows:</p><formula xml:id="formula_0">v v v v v v v s :40 s :? s :28.6 v v v v v v v v v v v v v v</formula><p>Problem 1 Given a dynamic road network (G1, G2, • • • , GT ) with missing data at each timestamp, we aim to achieve the following two goals:</p><p>• complete the missing data (i.e., both missing value and sensor) of Gi , where 1 ≤ i ≤ T ;</p><p>• predict the future readings of G T +h , where h is the prediction horizon. For example, when h = 1, we predict the traffic condition of GT +1 at the next timestamp.</p><p>For ease of presentation, Table <ref type="table" target="#tab_0">1</ref> lists the notations we use throughout this paper. Note that since each dimension of a latent space represents a latent attribute, we thus use latent attributes and latent positions interchangeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LATENT SPACE MODEL FOR ROAD NET-WORKS (LSM-RN)</head><p>In this section, we describe our LSM-RN model in the context of traffic prediction. We first introduce the basic latent space model (Section 4.1) by considering the graph topology, and then incorporate both temporal and transition patterns (Section 4.2). Finally, we describe the complete LSM-RN model to solve the traffic prediction problem with missing data (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topology in LSM-RN</head><p>Our traffic model is built upon the latent space model of the observed road network. Basically, each vertex of road network have different attributes and each vertex has an overlapping representation of attributes. The attributes of vertices and how each attribute interacts with others jointly determine the underlying traffic patterns. Intuitively, if two highway vertices are connected, their corresponding interaction generates a higher travel speed than that of two vertices located at arterial streets. In particular, given a snapshot of road network G, we aim to learn two matrices U and B, where matrix U ∈ R n×k + denotes the latent attributes of vertices, and matrix B ∈ R k×k + denotes the attribute interaction patterns. The product of UBU T represents the traffic speed between any two vertices, where we use to approximate G. Note that B is an asymmetric matrix since the road network G is directed. Therefore, the basic traffic model which considers the graph topology can be determined by solving the following optimization problem:  Similar Non-negative Tri-factorization frameworks have been utilized in clustering <ref type="bibr" target="#b9">[9]</ref>, community detection <ref type="bibr" target="#b28">[29]</ref> and sentimental analysis <ref type="bibr" target="#b31">[32]</ref>. Figure <ref type="figure" target="#fig_3">2</ref> (a) illustrates the intuition of our static traffic model. As shown in Figure <ref type="figure" target="#fig_3">2</ref> (b), suppose we know that each vertex is associated with two attributes (e.g., highway and business area), and the interaction pattern between two attributes is encoded in matrix B, we can accurately estimate the travel speed between vertex v1 and v2, using their latent attributes and the matrix B.</p><formula xml:id="formula_1">arg min U ≥0,B≥0 J = ||G -UBU T || 2 F (1) U B U G ≈ × n × n × n × k k × k k × n 28.6 c(v , v ) U(v ) U (v ) B = × × 0.6 0.1 0.4</formula><p>Overcome the sparsity of Road Network. In our road network, G is very sparse (i.e., zero entries dominate the items in G) for the following reasons: (1) the average degree of a road network is small <ref type="bibr" target="#b25">[26]</ref>, and thus the edges of road network is far from fully connected, (2) the distribution of sensors is non-uniform, and only a small number of edges are equipped with sensors; and (3) there exists missing values (for those edges equipped with sensors) due to the failure and/or maintenance of sensors.</p><p>Therefore, we define our loss function only on edges with observed readings, that is, the set of edges with travel cost c(vi, vj) &gt; 0. In addition, we also propose an in-filling method to reduce the gap between the input road network and the estimated road network. We consider graph Laplacian dynamics, which is an effective smoothing approach for finding global structure similarity <ref type="bibr" target="#b14">[15]</ref>. Specifically, we construct a graph Laplacian matrix L, defined as L = D -W , where W is a graph proximity matrix that is constructed from the network topology, and D is a diagonal matrix Dii = j (Wij). With these new constraints, our traffic model for one snapshot of road network G is expressed as follows:</p><formula xml:id="formula_2">arg min U,B J = ||Y ⊙ (G -UBU T )|| 2 F + λT r(U T LU ),<label>(2)</label></formula><p>where Y is an indication matrix for all the non-zero entries in G, i.e, Yij = 1 if and only if G(i, j) &gt; 0; ⊙ is the Hadamard product operator, i.e., (X ⊙ Z)ij = Xij × Zij; and λ is the Laplacian regularization parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Time in LSM-RN</head><p>Next, we will incorporate the temporal information, including time-dependent modeling of latent attributes and the temporal transition. With this model, each vertex is represented in a unified latent space, where each dimension either represents a spatial or temporal attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Temporal effect of latent attributes</head><p>The behavior of the vertices of road networks may evolve quickly. For instance, the behavior of a vertex that is similar to that of a highway vertex during normal traffic condition, may become similar to that of an arterial street node during congestion hours. Because the behavior of each vertex can change over time, we must employ a time-dependent modeling for attributes of vertices for real-time traffic prediction. Therefore, we add the time-dependent effect of attributes into our traffic model. Specifically, for each t ≤ T , we aim to learn a corresponding time-dependent latent attribute representation Ut. Although the latent attribute matrix Ut is timedependent, we assume that the attribute interaction matrix B is an inherent property, and thus we opt to fix B for all timestamps. By incorporating this temporal effect, we obtain our model based on the following optimization problem:</p><formula xml:id="formula_3">arg min U t ,B J = T t=1 ||Yt ⊙ (Gt -UtBU T t )|| 2 F + T t=1 λT r(UtLU T t )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Transition matrix</head><p>Due to the dynamics of traffic condition, we aim to learn not only the time-dependent latent attributes, but also a transition model to capture the evolving behavior from one snapshot to the next. The transition should capture both periodic evolving patterns (e.g., morning/afternoon rush hours) and non-recurring patterns caused by traffic incidents (e.g., accidents, road construction, or work zone closures). For example, during the interval of an accident, a vertex transition from the normal state to the congested at the beginning, then become normal again after the accident is cleared.</p><p>We thus assume a global process to capture the state transitions. Specifically, we use a matrix A that approximates the changes of U between time t -1 to time t, i.e., Ut = Ut-1A, where U ∈ R n×k + , A ∈ R k×k + . The transition matrix A represents how likely a vertex is to transit from attribute i to attribute j from timestamp 1 to timestamp T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LSM-RN Model</head><p>Considering all the above discussions, the final objective function for our LSM-RN model is defined as follows:</p><formula xml:id="formula_4">arg min U t ,B,A J = T t=1 ||Yt ⊙ (Gt -UtBU T t )|| 2 F + T t=1 λT r(UtLU T t )+ T t=2 γ||Ut -U t-1 A|| 2 F (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where λ and γ are the regularization parameters. By solving Eq. 4, we obtain the learned matrices of Ut, B and A from our LSM-RN model. Consequently, the task of both missing value and sensor completion can be accomplished by the following:</p><formula xml:id="formula_6">Gt =UtBU T t , when 1 ≤ t ≤ T .<label>(5)</label></formula><p>Subsequently, the edge traffic for snapshot G T +h (where h is the number of future time spans) can be predicted as follows:</p><formula xml:id="formula_7">G T +h =(U T A h )B(U T A h ) T<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">LEARNING&amp;PREDICTION BY LSM-RN</head><p>In this section, we first present a typical global multiplicative algorithm to infer the LSM-RN model, and then discuss a fast incremental algorithm that scales to large road networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Global learning algorithm</head><p>We develop an iterative update algorithm to solve Eq. 4, which belongs to the category of traditional multiplicative update algorithm <ref type="bibr" target="#b15">[16]</ref>. By adopting the methods from <ref type="bibr" target="#b15">[16]</ref>, we can derive the update rule of Ut, B and A. The details of derivation can be found in the technical report <ref type="bibr">[8]</ref>.</p><p>Lemma 1 The Update rule of Ut, B and A can be expressed as follows:</p><formula xml:id="formula_8">(Ut) ←(Ut)⊙ (Yt ⊙ G)(UtB T + UtB) + λW Ut + γ(Ut-1A + Ut+1A T ) (Yt ⊙ UtBU T t )(UtB T + UtB) + λDUt + γ(Ut + UtAA T ) 1 4<label>(7)</label></formula><formula xml:id="formula_9">B ← B ⊙ T t=1 U T t (Yt ⊙ Gt)Ut T t=1 U T t (Yt ⊙ (UtBU T t ))Ut<label>(8)</label></formula><formula xml:id="formula_10">A ← A ⊙ T t=1 U T t-1 Ut T t=1 U T t-1 U t-1 A<label>(9)</label></formula><p>Algorithm 1 outlines the process of updating each matrix using aforementioned multiplicative rules to optimize Eq. 4. The general idea is to jointly infer and cyclically update all the latent attribute matrices Ut, B and A. In particular, we first jointly learn the latent attributes for each time t from all the graph snapshots (Lines 3-4). Based on the sequence of time-dependent latent attributes (i.e., U1, U2, • • • , UT ), we then learn the global attribute interaction pattern B and the transition matrix A (Lines 5-6).</p><p>From Algorithm 1, we now explain how our LSM-RN model jointly learns the spatial and temporal properties. Specifically, when we update the latent attribute of one vertex Ut(i), the spatial property is preserved by (1) considering the latent positions of its adjacent vertices (Yt ⊙ Gt), and (2) incorporating the local graph Laplacian constraint (i.e., matrix W and D). Moreover, the temporal property of one vertex is then captured by leveraging its latent attribute in the previous and next timestamps (i.e., Ut-1(i) and Ut+1(i)), as well as the transition matrix.</p><formula xml:id="formula_11">Algorithm 1 Global-learning(G1, G2, • • • , GT ) Input: graph matrix G 1 , G 2 , • • • , G T . Output: Ut (1 ≤ t ≤ T ), A and B.</formula><p>1: Initialize Ut, B and A 2: while Not Convergent do 3: for t = 1 to T do 4:</p><p>update Ut according to Eq. 7 5: update B according to Eq. 8 6: update A according to Eq. 9</p><p>In the following, we briefly discuss the time complexity and convergence of global learning algorithm. From Lemma 1 In each iteration, the computation is dominated by matrix multiplication operations. Therefore, the worst case time complexity per iteration is dominated by O(T (nk 2 + n 2 k)). In practice , we opt to choose a low-rank latent space representation, where k is a small number (e.g., 20). In terms of convergence, followed the proof shown in previous works <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>, we can prove that Algorithm 1 converges into a local minimal and the objective value is non-increasing in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Incremental learning algorithm</head><p>The intuition behind our incremental algorithm is based on the observation that each time when we make a prediction for the next five minutes, the ground truth reading will be available immediately after five minutes. This motivates us to adjust the latent position of each vertex so that the prediction is closer to the ground truth. On the other hand, it is not necessary to perform the latent position adjustment for each vertex. This is because during a short time interval, the overall traffic condition of the whole network tends to stay steady, and the travel cost of most edges changes at a slow pace, although certain vertices can go through obvious variations. Therefore, instead of recomputing the latent positions of all the vertices from scratch at every time stamp, we perform a "lazy" update. In particular, to learn the latent space Ut, the incremental algorithm utilizes the latent space we have already learned in the previous snapshot (i.e., Ut-1), makes predictions for the next snapshot (i.e., Gt), and then conditionally adjusts latent attributes of a subset of vertices based on the changes of traffic condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Framework of incremental algorithm</head><p>Algorithm 2 presents the pseudo-code of incremental learning algorithm. Initially, we learn the latent space of U1 from our global multiplicative algorithm (Line 1). With the learned latent matrix Ut-1, at each time stamp t between 2 and T , our incremental update consists of the following two components: 1) identify candidate vertices based on feedbacks (Lines 3-8); 2) update their latent attributes and propagate the adjustment from one vertex to its neighbors (Line 9). As outlined in Algorithm 2, given Ut-1 and Gt, we first make an estimation of Gt based on Ut-1 (Line 3). Subsequently, we use Gt as the feedback information, select the set of vertices where we make inaccurate predictions, and insert them into a candidate set cand (Lines 4-8). Consequently, we update Ut based on the learned latent matrix Ut-1, the ground truth observation Gt and candidate set cand (Line 9). After that, we learn the global transition matrix A (Line 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Topology-aware incremental update</head><p>Given Ut-1 and Gt, we now explain how to calculate Ut incrementally from Ut-1 with the candidate set cand, with which we can accurately approximate Gt. The main idea is similar to an online learning process. At each round, the algorithm predicts an outcome for the required task (i.e., predict the speed of edges). Once the algorithm makes a prediction, it receives feedback indicating the correct outcome. Then, the online algorithm can modify its prediction mechanism for better predictions on subsequent timestamps. In our scenario, we first use the latent attribute matrix Ut-1 to predict Gt as if we do not know the observation, subsequently we adjust the model of Ut according to the true observation of Gt we already have in hand.</p><p>However, in our problem, we are making predictions for the entire road network, not for a single edge. When we predict for one edge, we only need to adjust the latent attributes of two vertices, whereas in our scenario we need to update the latent attributes for many correlated vertices. Therefore, the effect of adjusting the latent attribute of one vertex can potentially affect its neighboring vertices, and influence the convergence speed of incremental learning. Hence, the adjustment order of vertices is very important.</p><formula xml:id="formula_12">Algorithm 2 Incremental-Learning(G1, G2, • • • , GT ) Input: graph matrix G 1 , G 2 , • • • , G T . Output: Ut (1 ≤ t ≤ T ), A and B. 1: (U 1 , B) ←Global-learning(G 1 ) 2: for t = 2 to T do 3: Gt ← U t-1 BU T t-1 (prediction)</formula><p>4: cand ← ∅ (a subset of vertices to be updated) 5: for each i ∈ G do 6:</p><formula xml:id="formula_13">for each j ∈ out(i) do 7: if |Gt(i, j) -Gt(i, j)| ≥ δ then 8:</formula><p>cand ← cand ∪ {i, j} 9: Ut ← Incremental-Update(U t-1 , Gt, cand) (See Section 5.2.2) 10: Iteratively learn transition matrix A using Eq. 9 until A converges Algorithm 3 Incremental-Update(Ut-1, Gt, cand)</p><p>Input: the latent matrix U t-1 , observed graph reading Gt, candidate set cand, hyper-parameters δ and τ Output: Updated latent space Ut.</p><p>1: Ut ← U t-1 2: while Not Convergent AND cand / ∈ ∅ do 3: order cand from the reverse topological order 4: for i ∈ cand do 5:</p><p>oldu ← Ut(i)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for each j ∈ out(i) do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>adjust Ut(i) with Eq. 11 8:</p><formula xml:id="formula_14">if ||Ut(i) -oldu|| 2 F ≤ τ then 9:</formula><p>cand ← cand \ {i} 10:</p><p>for each j ∈ out(i) do 11:</p><formula xml:id="formula_15">p ← Ut(i)BUt(j)</formula><p>12:</p><formula xml:id="formula_16">if |p -Gt(i, j)| ≥ δ then 13: cand ← cand ∪ {j}</formula><p>Algorithm 3 presents the details of updating Ut incrementally from Ut-1. For each vertex i of cand, we adjust its latent position so that we could make more accurate predictions (Line 7) and then examine how this adjustment would influence the candidate task set from the following two aspects: (1) if the latent attribute of i does not change much, we remove it from the set of cand (Lines 8-9);</p><p>(2) if the adjustment of i also affects its neighbor j, we add vertex j to cand (Lines 10-13).</p><p>The remaining questions in our Incremental-Update algorithm are how to adjust the latent position of one vertex according to feedbacks, and how to decide the order of update. In the following, we address each of them. </p><formula xml:id="formula_17">t-1 (v 1 ) U t (v 1 ) highway business 1 2 v 1 28.6 35 v 2 v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 3 v 4 v 6 v 7 v 1 v 2 v 5</formula><p>(a) Adjustment method (b) Adjustment order Adjusting latent attribute of one vertex. To achieve high efficiency of adjusting the latent attribute, we propose to make the smallest changes of the latent space (as fast as possible) to predict the correct value. For example, as shown in Figure <ref type="figure" target="#fig_5">3</ref> (a), suppose we already know the new latent position of v1, then fewer step movement (Option 1) is preferable than gradual adjustment (Option 2). Note that in our problem, when we move the latent position of a vertex to a new position, the objective of this movement is to produce a correct prediction for each of its outgoing edges. Specifically, given Ut-1(i), we want to find Ut(i) which could accurately predict the weight of each edge e(vi, vj) that is adjacent to vertex vi. We thus formulate our problem as follows:</p><formula xml:id="formula_18">Ut(i), ξ * = arg min U (i)∈R k + 1 2 ||U (i) -U t-1 (i)|| 2 F + Cξ s.t. |U (i)BU T (j) -Gt(i, j)| ≤ δ + ξ,<label>(10)</label></formula><p>where ξ is a non-negative slack variable, C &gt; 0 is a parameter which controls the trade-off between being conservative (do not change the model too much) and corrective (satisfy the constraint), and δ is a precision parameter. Note that we have non-negativity constraint over the latent space of Ut(i). We thus adopt the approaches from <ref type="bibr" target="#b3">[3]</ref>: When the predicted value yt (i.e., Ut(i)BU T t (j)) is less than the correct value yt (i.e., Gt(i, j)), we use the traditional online passive-aggressive algorithm <ref type="bibr" target="#b7">[7]</ref> because it guarantees the non-negativity of U (i); Otherwise, we update U (i) by solving a quadratic optimization problem. The detailed solution is as follows:</p><formula xml:id="formula_19">Ut(i) = max(U t-1 (i) + (k * -θ * ) • BU t-1 (j) T , 0)<label>(11)</label></formula><p>k * and θ * are computed as follows:</p><formula xml:id="formula_20">⎧ ⎨ ⎩ k * = αt, θ * = 0 if yt &lt; yt k * = 0, θ * = C if yt &gt; yt and f (C) ≥ 0 k * = 0, θ * = f -1 (0) if yt &gt; yt and f (C) &lt; 0<label>(12)</label></formula><p>where</p><formula xml:id="formula_21">αt = min C, max(| yt -yt| -δ, 0) ||BU t-1 (j) T || 2 ft(θ) = max Ut(i) -θBUt(j) T , 0 • BUt(j) T -Gt(i, j) -δ</formula><p>Updating order of cand. As we already discussed, the update order is important because it influences the convergence speed of our incremental algorithm. Take the example of the road network shown in Figure <ref type="figure" target="#fig_1">1</ref>, suppose our initial cand contains three vertices v7, v6 and v2, where we have two edges e(v7, v6) and e(v6, v2). If we randomly choose the update sequence as &lt; v7, v6, v2 &gt;, that is, we first adjust the latent attribute of v7 so that c(v7, v6) has a correct reading; subsequently we adjust the latent attribute of v6 to correct our estimation of c(v6, v2). Unfortunately,the adjustment of v6 could influence the correction we have already made to v7, thus leading to an inaccurate estimation of c(v7, v6) again. A desirable order is to update vertex v6 before updating v7. Therefore, we propose to consider the reverse topology of road network when we update the latent position of each candidate vertex v ∈ cand. The general principle is that: given edge e(vi, vj),  the update of vertex vi should be proceeded after the update of vj, because the position of vi is dependent on vj. This motivates us to derive a reverse topological order in the graph of G. Unfortunately, the road network G is not a Directed Acyclic Graph (DAG), and contains cycles. To address this issue, we first generate a condensed super graph where we contract each Strongly Connected Component (SCC) of the graph G as a super node. We then derive a reverse topological order based on this condensed graph. For the vertex order in each SCC, we generate an ordering of vertices inside each SCC by random algorithms or some heuristics. Figure <ref type="figure" target="#fig_5">3(b)</ref> shows an example of ordering for the road network of Figure <ref type="figure" target="#fig_1">1</ref>, where each rectangle represents a SCC. After generating a reverse topological order based on the contracted graph and randomly ordering the vertices within each SCC, we obtain one final ordering &lt; v2, v6, v7, v1, v5, v4, v3 &gt;. Each time when we update the latent attributes of cand, we follow this ordering of vertices. Time complexity. For each vertex i, the computational complexity of adjusting its latent attributes using Eq. 11 is O(k), where k is number of attributes. Therefore, to compute latent attributes u, the time complexity per iteration is O(kT (∆n + ∆m)), where ∆n is number of candidate vertex in cand, and ∆m is total number of edges incident to vertices in cand. In practice, ∆n ≪ n and ∆m ≪ m ≪ n 2 . In addition, the SCC can be generated in linear time O(m+n) via Tarjan's algorithm <ref type="bibr" target="#b22">[23]</ref>. Therefore, we conclude that the computational cost per iteration is significantly reduced using Algorithm 2 as compared to using the global learning approach.</p><formula xml:id="formula_22">2T G G G U U U U U U U U G G G G Incremental learning time ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global learning Incremental learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Real-time forecasting</head><p>In this section, we discuss how to apply our learning algorithms to real-time traffic prediction, where the sensor reading is received in a streaming fashion. In practice, if we want to make a prediction for the current traffic, we cannot afford to apply our global learning algorithm to all the previous snapshots because it is computationally expensive. Moreover, it is not always true that more snapshots would yield a better prediction performance. The alternative method is to treat each snapshot independently: i.e., each time we only apply our incremental learning algorithm for the most recent snapshot, and then use the learned latent attribute to predict the traffic condition. Obviously, this might yield poor prediction quality as it totally ignores the temporal transitions.</p><p>To achieve a good trade-off between the above two methods, we propose to adapt a sliding window setting for the learning of our LSM-RN model, where we apply incremental algorithm at each timestamp during one time window, and only run our global learning algorithm at the end of one time window. As shown in Figure <ref type="figure" target="#fig_7">4</ref>, we apply our global learning at timestamps T (i.e., the end of one time window), which learns the time-dependent latent attributes for the previous T timestamps. Subsequently, for each timestamp T +i between [T, 2T], we apply our incremental algorithm to adjust the latent attribute and make further predictions: i.e., we use UT +i to predict the traffic of G T +(i+1) . Each time we receive the true observation of G T +(i+1) , we calculate U T +(i+1) via the incremental update from Algorithm 3. The latent attributes U2T will be recomputed at timestamp 2T (the end of one time window), and the U2T would be used for the next time window [2T, 3T ]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENT 6.1 Dataset</head><p>We used a large-scale high resolution (both spatial and temporal) traffic sensor (loop detector) dataset collected from Los Angeles county highways and arterial streets. This dataset includes both inventory and real-time data for 15000 traffic sensors covering approximately 3420 miles. The sampling rate of the data, which provides speed, volume (number of cars passing from sensor locations) and occupancy, is 1 reading/sensor/min. We have been collecting and archiving this sensor dataset continuously since 2010.</p><p>We chose sensor data between March and April in 2014 for our experiments, which include more than 60 million records of readings. As for the road network, we used Los Angeles road network which was obtained from HERE Map dataset <ref type="bibr">[11]</ref>. We constructed two subgraphs of Los Angeles road network, termed as SMALL and LARGE. The SMALL (resp. LARGE) network contains 5984 (resp. 8242) vertices and 12538 (resp. 19986) edges. As described in Section 3, the sensor data are mapped to the road network, where 1642 (resp. 4048) sensors are mapped to SMALL (resp. LARGE). Figure <ref type="figure" target="#fig_8">5</ref> shows sensors locations and road network segments, where the green lines depict the sensors, and blue lines represent the road network segments. After mapping the sensor data, we have two months of network snapshots for both SMALL and LARGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Algorithms</head><p>Our methods are termed as LSM-RN-All (i.e., global learning algorithm) and LSM-RN-Inc (i.e., incremental learning algorithm).</p><p>For edge traffic prediction, we compare with LSM-RN-Naive, where we adapted the formulations from LSM-SN ( <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b20">[21]</ref>) by simply combining the topology and temporal correlations. In addition, LSM-RN-Naive uses a Naive incremental learning strategy in <ref type="bibr" target="#b20">[21]</ref>, which independently learns the latent attributes of each timestamp first, then the transition matrix. We also compare our algorithms with two representative time series prediction methods: a linear model (i.e., ARIMA <ref type="bibr" target="#b17">[18]</ref>) and a non-linear model (i.e., SVR <ref type="bibr" target="#b19">[20]</ref>). We train each model independently for each time series with historical data. In addition, because these methods will be affected negatively due to the missing values during the prediction stages (i.e, some of the input readings for ARIMA and SVR could be zero), for fair comparison we consider ARIMA-Sp and SVR-Sp, which use the completed readings from our global learning algorithm. We also implemented the Tensor method <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>, however, it cannot address the sparsity problem of our dataset and thus produce meaningless results (most of the prediction values are close to 0).</p><p>For missing-value completion, we compare our algorithms with two methods: (1) KNN <ref type="bibr" target="#b10">[10]</ref>, which uses the average values of the nearby edges in Euclidean distance as the imputed value, (2) LSM-RN-Naive, which independently learns the latent attributes of each snapshot, then uses them to approximate the edge readings.</p><p>To evaluate the performance of online prediction, we consider the scenario of a batch-window setting described in Section 5.3. </p><formula xml:id="formula_23">2 -7 , 2 -5 , 2 -3 , 2 -1 , 2 1 , 2 3 , 2 5 γ 2 -7 , 2 -5 , 2 -3 , 2 -1 , 2 1 , 2 3 , 2 5</formula><p>Considering a time window [0, 2T ], we first batch learn the latent attributes of UT and transition matrix A from [0, T ], we then sequentially predict the traffic condition for the timestamps during [T + 1, 2T ]. Each time when we make a prediction, we receive the true observations as the feedback. We compare our Incremental algorithm (Inc), with three baseline algorithms: Old, LSM-RN-Naive and LSM-RN-All. Specifically, to predict GT +i, LSM-RN-Inc utilizes the feedback of G T +(i-1) to adjust the time-dependent latent attributes of U T +(i-1) , whereas Old does not consider the feedback, and always uses latent attributes UT and transition matrix A from the previous time window. On the other hand, LSM-RN-Naive ignores the previous snapshots, and only applies the inference algorithm to the most recent snapshot G T +(i-1) (aka Minibatch). Finally, LSM-RN-All applies the global learning algorithm consistently to all historical snapshots (i.e., G1 to G T +(i-1) ) and then makes a prediction (aka Full-batch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Configurations and measures.</head><p>We selected two different time ranges that represent rush hour (i.e., 7am-8am) and non-rush hour (i.e., 2pm-3pm), respectively. For the task of missing value completion, during each timestamps of one time range (e.g., rush hour), we randomly selected 20% of values as unobserved and manipulated them as missing <ref type="foot" target="#foot_0">1</ref> , with the objective of completing those missing values. For each traffic prediction task at one particular timestamp (e.g., 7:30 am), we randomly selected 20% of the values as unknown and use them as ground-truth values.</p><p>We varied the parameters T and span: where T is the number of snapshots, and span is time gap between two continuous snapshots. We also varied k, λ, and γ, which are parameters of our model. The default settings (shown with bold font) of the experiment parameter are listed in Table <ref type="table" target="#tab_1">2</ref>. Because of space limitations, the results of varying γ are not reported, which are similar to result of varying λ. We use Mean Absolute Percentage Error (MAPE) and Root Mean Square Error (RMSE) to measure the accuracy. In the following we only report the experiment results based on MAPE, the experiment results based on RMSE are reported in the technical report <ref type="bibr">[8]</ref>. Specifically, MAPE is defined as follows:</p><formula xml:id="formula_24">MAP E = ( 1 N N i=1 |y i -ŷi | y i )</formula><p>With ARIMA and SVR, we use the dataset of March to train a model for each edge, and use 5-fold cross-validation to choose the best parameters. All the tasks of missing value completion and edge traffic prediction tasks are conducted on April data. We conducted our experiments with C++ on a Linux PC with i5-2400 CPU @ 3.10G HZ and 24GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with edge traffic prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">One-step ahead prediction</head><p>The experimental results of SMALL are shown in Figures <ref type="figure" target="#fig_9">6 (a</ref> than LSM-RN-Inc. This demonstrates the effectiveness of timedependent latent attributes and the transition matrix. We observe that without imputing of missing values, time series prediction techniques (i.e., ARIMA and SVR) perform much worse than LSM-RN-All and LSM-RN-Inc. Meanwhile, LSM-RN-Naive, which separately learns the latent attributes of each snapshot, cannot achieve good prediction results as compared to LSM-RN-All and LSM-RN-Inc. This indicates that simply combining topology and time is not enough for accurate predictions. We note that even with completed readings, the accuracy of SVR-Sp and ARIMA-Sp is worse than that of LSM-RN-All and LSM-RN-Inc. One reason is that simply combining the spatial and temporal properties does not necessarily yield a better performance. Another reason is that both SVR-Sp and ARIMA-Sp also suffer from missing data during the training stage, which results in less accurate predictions. In the technical report <ref type="bibr">[8]</ref>, we show how the ratio of missing data would influence the prediction performance. Finally, we observe that SVR is more robust than ARIMA when encountering missing values: i.e., ARIMA-Sp performs significantly better than ARIMA, while the improvement of SVR-Sp over SVR is marginal. This is because ARIMA is a linear model which mainly uses the weighted average of the previous readings for prediction, while SVR is a non-linear model that utilizes a kernel function. Figures <ref type="figure" target="#fig_9">6 (c</ref>) and (d) show the experiment results on LARGE, the trend is similar to SMALL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Multi-steps ahead prediction</head><p>We now present the experiment results on long-term predictions, with which we predict the traffic conditions for the next 30 minutes (i.e., h = 6). The prediction accuracy of different methods on SMALL are shown in Figures <ref type="figure" target="#fig_10">7 (a</ref>) and (b). Although LSM-RN-All and LSM-RN-Inc still outperform other methods, the margin between our methods and the baselines is narrower. The reason is that: when we make long-term predictions, we use the predicted values from the past for future prediction. This leads to the problem of error accumulation, i.e., errors incurred in the past are propagated into future predictions. We observe the similar trends on LARGE, the results are reported in Figures <ref type="figure" target="#fig_10">7 (c</ref>) and (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison for missing value completion</head><p>In this set of experiments, we evaluate the completion accuracy of different methods. Due to space limitation, we only report the experiment results on LARGE in Figures <ref type="figure" target="#fig_11">8 (a</ref>  methods. This is because LSM-RN-All and LSM-RN-Inc capture both spatial and temporal relationships, while LSM-RN-Naive and KNN only use spatial property. LSM-RN-All performs better than LSM-RN-Inc by jointly inferring all the latent attributes. On the other hand, we note that LSM-RN-Naive and KNN have similar performances, which is inferior to our methods. This also indicates that utilizing both spatial and temporal properties yields a larger gain than only utilizing the spatial property. As shown in Figure <ref type="figure" target="#fig_11">8</ref>(b), the completion performance during the non-rush hour is better as compared to the rush hour time. This is because during rush hour range, the traffic condition is more dynamic, and the underlying pattern and transition changes frequently. Table <ref type="table" target="#tab_2">3</ref> shows the running time of different methods. Although ARIMA and SVR are fast in each prediction, they require large volume of training data and have much higher training time, which can be a problem for real systems. On the contrary, our methods do not require extra training data, i.e., our methods efficiently train and predict at the same time. Among them, LSM-RN-Inc is the most efficient approach: it only takes less than 500 milliseconds to learn the time-dependent latent attributes and make predictions for all the edges of the road network. This is because our incremental learning algorithm conditionally adjusts the latent attributes of certain vertices, and utilizes the topological order that enables fast convergence. Even for the LARGE dataset, LSM-RN-Inc takes less than five seconds, which is acceptable considering that the span between two snapshots is at least five minutes in practice. This demonstrates that LSM-RN-Inc scales well to large road networks. Regarding LSM-RN-All and LSM-RN-Naive, they both require much longer running time than that of LSM-RN-Inc. In addition, LSM-RN-All is faster than LSM-RN-Naive. This is because LSM-RN-Naive independently runs the global learning algorithm for each snapshot T times, while LSM-RN-All only applies global learning for all the snapshots once.  when the number of iterations is around 20, our algorithm tends to converge in terms of our objective value in Eq. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Scalability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Comparison for real-time forecasting</head><p>In this set of experiments, we evaluate our online setting algorithms. Due to space limitation, we only report the experiment results on LARGE. As shown in Figures <ref type="figure" target="#fig_15">10 (a</ref>) and (b), LSM-RN-Inc achieves comparable accuracy with LSM-RN-All (Full-batch). This is because LSM-RN-Inc effectively leverages the real-time feedback to adjust the latent attributes. We observe that LSM-RN-Inc performs much better than Old and LSM-RN-Naive (Minibatch), which ignore either the feedback information (i.e., Old) or the previous snapshots (i.e., LSM-RN-Naive). One observation is that Old performs better than LSM-RN-Naive for the initial timestamps, whereas Old surpasses Mini-batch at the later timestamps. This indicates that the latent attributes learned in the previous timewindow are more reliable for predicting the near-future traffic conditions, but may not be good for long-term predictions because of the error accumulation problem.</p><p>Figures <ref type="figure" target="#fig_1">11 (a</ref>) and (b) show the running time comparisons of different methods. One important observation from this experiment is that LSM-RN-Inc is the most efficient approach, which is on average two times faster than LSM-RN-Naive and one order of magnitude faster than LSM-RN-All. This is because LSM-RN-Inc performs a conditional latent attribute update for vertices within a small portion of road network, whereas LSM-RN-Naive and LSM-RN-All both recompute the latent attributes from at least one entire road network snapshot. Since in the real-time setting, LSM-RN-All utilizes all the up-to-date snapshots and LSM-RN-Naive only considers the most recent single snapshot, LSM-RN-Naive is faster than LSM-RN-All. We observe that LSM-RN-Inc only takes less than 1 second to incorporate the real-time feedback information, while LSM-RN-Naive and LSM-RN-All take much longer.</p><p>Therefore, we conclude that LSM-RN-Inc achieves a good tradeoff between prediction accuracy and efficiency, which is applicable for real-time traffic prediction applications.  In this section, we evaluate the performance of our methods by varying the parameters of our model. Due to space limitation, we only show the experimental results on SMALL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.1">Effect of varying T</head><p>Figure <ref type="figure" target="#fig_17">12</ref> (a) and Figure <ref type="figure" target="#fig_17">12</ref> (b) show the prediction performance and the running time of varying T , respectively. We observe that with more snapshots, the prediction error decreases. In particular, when we increase T from 2 to 6, the results improve significantly. However, the performance tends to stay stable at T ≥ 6. This indicates that fewer snapshots (i.e., two or less) are not enough to capture the traffic patterns and the evolving changes. On the other hand, more snapshots (i.e., more historical data) do not necessarily yield better gain, considering the running time increases when we have more snapshots. Therefore, to achieve a good trade-off between running time and prediction accuracy, we suggest to use at least 6 snapshots, but no more than 12 snapshots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.2">Effect of varying span</head><p>The results of varying span are shown in Figure <ref type="figure" target="#fig_5">13</ref>. Clearly, as the time gap between two snapshots increases, the performance declines. This is because when span increases, the evolving process of underlying traffic may not evolve smoothly, the transition process learned in the previous snapshot is not applicable for the future. Fortunately our sensor dataset usually have high-resolution, so it is better to use smaller span to learn the latent attributes. In addition, span does not affect the running time of either algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.3">Effect of varying k and λ</head><p>Figure <ref type="figure" target="#fig_7">14</ref> (a) shows the effect of varying k. We observe that:</p><p>(1) we achieve better results with increasing number of latent attributes; (2) the performance is stable when k ≥ 20. This indicates that a low-rank latent space representation can already capture the attributes of the traffic data. In addition, our results show that when the number of latent attributes is small (i.e., k ≤ 30), the running time increases with k but does not change much when we vary k from 5 to 30. Therefore, setting k to 20 achieves a good balance between computational cost and accuracy.</p><p>Figure <ref type="figure" target="#fig_7">14</ref> (b) depicts the effect of varying λ, which is the regularization parameter for our graph Laplacian dynamics. We observe that the graph Laplacian has a larger impact on LSM-RN-All al-gorithm than on LSM-RN-Inc. This is because λ controls how the global structure similarity contributes to latent attributes and LSM-RN-All jointly learns those time-dependent latent attribute, thus λ has larger effect on LSM-RN-All. In contrast, LSM-RN-Inc adaptively updates the latent positions of a small number of changed vertices in limited localized view, and thus is less sensitive to the global structure similarity than LSM-RN-All. In terms of parameters choices, λ = 2 and λ = 8 yields best results for LSM-RN-All and LSM-RN-Inc, respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper, we studied the problem of real-time traffic prediction using real-world sensor data for road networks. We proposed LSM-RN, where each vertex is associated with a set of latent attributes that captures both topological and temporal properties of road networks. We showed that the latent space modeling of road networks with time-dependent weights accurately estimates the traffic patterns and their evolution over time. To efficiently infer these time-dependent latent attributes, we developed an incremental online learning algorithm which enables real-time traffic prediction for large road networks. With extensive experiments we verified the effectiveness, flexibility and scalability of our model in identifying traffic patterns and predicting future traffic conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>KDD ' 16 ,</head><label>16</label><figDesc>August 13-17, 2016, San Francisco, CA, USA c ⃝ 2016 ACM. ISBN 978-1-4503-4232-2/16/08. . . $15.00 DOI: http://dx.doi.org/10.1145/2939672.2939860</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of road network Given a small number of road network snapshots, or a dynamic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Travel time of c(v1, v2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of our traffic model, where G represents a road network, U denotes the attributes of vertices in the road network, n is number of nodes, and k is number of attributes, and B denotes how one type of attributes interacts with others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>U</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Challenges of adjusting the latent attribute with feedbacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A batch window framework for real-time forecasting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sensor distribution and Los Angeles road network.</figDesc><graphic coords="7,92.49,45.19,211.22,97.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: One-step ahead prediction MAPE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Six-steps ahead prediction MAPE LSM-RN-Inc LSM-RN-Naive LSM-RN-All KNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Missing value completion MAPE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Converge rate Convergence analysis. Figures 9 (a) and (b) report the convergence rate of iterative algorithm LSM-RN-All on both SMALL and LARGE. As shown in Figure9, LSM-RN-All converges very fast: when the number of iterations is around 20, our algorithm tends to converge in terms of our objective value in Eq. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Rush hour on LARGE (b) Non-Rush hour on LARGE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Online prediction MAPE LSM-RN-Inc LSM-RN-Naive LSM-RN-All</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Effect of varying T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Figure 13: Effect of varying span</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notations and explanations</figDesc><table><row><cell cols="2">Notations Explanations</cell></row><row><cell>N , n</cell><cell>road network, number of vertices of the road network</cell></row><row><cell>G</cell><cell>the adjacency matrix of a graph</cell></row><row><cell>U</cell><cell>latent space matrix</cell></row><row><cell>B</cell><cell>attribute interaction matrix</cell></row><row><cell>A</cell><cell>the transition matrix</cell></row><row><cell>k</cell><cell>the number of dimensions of latent attributes</cell></row><row><cell>T</cell><cell>the number of snapshots</cell></row><row><cell>span</cell><cell>the gap between two continuous graph snapshots</cell></row><row><cell>h</cell><cell>the prediction horizon</cell></row><row><cell>λ, γ</cell><cell>regularization parameters for graph Laplacian and transition process</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experiment parameters Parameters Value range T 2, 4, 6, 8, 10, 12 span 5, 10, 15, 20, 25, 30 k 5, 10, 15, 20, 25, 30 λ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Running time comparisons. For ARIMA and SVR, the training time cost is the total training time for all the edges for one-step ahead prediction, and the prediction time is the average prediction time per edge per query.</figDesc><table><row><cell>data</cell><cell cols="2">SMALL</cell><cell cols="2">LARGE</cell></row><row><cell></cell><cell cols="2">train (s) pred.(ms)</cell><cell>train (s)</cell><cell>pred. (ms)</cell></row><row><cell>LSM-RN-Naive</cell><cell>-</cell><cell>1353</cell><cell>-</cell><cell>29439</cell></row><row><cell>LSM-RN-All</cell><cell>-</cell><cell>869</cell><cell>-</cell><cell>14247</cell></row><row><cell>LSM-RN-Inc</cell><cell>-</cell><cell>407</cell><cell>-</cell><cell>4145</cell></row><row><cell>ARIMA</cell><cell>484</cell><cell>0.00015</cell><cell>987</cell><cell>0.00024</cell></row><row><cell>SVR</cell><cell>47420</cell><cell>0.00042</cell><cell>86093.99</cell><cell>0.00051</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that missing values are plenty in our dataset, especially for arterials. However, we needed ground-truth for evaluation purposes and that is why we generated missing values artificially.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. Dingxiong Deng, Cyrus Shahabi and Ugur Demiryurek were supported in part by NSF grants IIS-1115153, IIS-1320149, CNS-1461963 and Caltrans-65A0533, the USC Integrated Media Systems Center (IMSC), and unrestricted cash gifts from Google, Northrop Grumman, Microsoft, and Oracle. Linhong Zhu was supported in part by DARPA grant Number W911NF-12-1-0034. Rose Yu and Yan Liu were supported by the U. S. Army Research Office under grant Number W911NF-15-1-0491, NSF IIS-1254206 and the USC Integrated Media System Center (IMSC). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any of the sponsors such as NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable tensor factorizations for incomplete data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dunlavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mørup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemometrics and Intelligent Laboratory Systems</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="56" />
			<date type="published" when="2011-03">March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Matlab tensor toolbox version 2.6. Available online</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-02">February 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online passive-aggressive algorithms for non-negative matrix factorization and completion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kubo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Naonori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with data calibration for long-term time series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-N</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="133" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling temporal adoptions using dynamic matrix factorization</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C T</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online passive-aggressive algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Latent space model for road networks to predict time-varying traffic</title>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Demiryurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1602.04301</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Orthogonal nonnegative matrix t-factorizations for clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-parametric regression for space-time forecasting under missing data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers, Environment and Urban Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="538" to="550" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Trajectory regression on road networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Idé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Big data and its technical challenges</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Labrinidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="86" to="94" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modeling freeway traffic with coupled hmms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Delvenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barahona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0812.1770</idno>
		<title level="m">Laplacian dynamics and multiscale modular structure in networks</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Link prediction via matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Utilizing real-world transportation data for accurate traffic prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Demiryurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDM</title>
		<imprint>
			<biblScope unit="page" from="595" to="604" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A bpca based missing value imputing method for traffic flow volume data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="985" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Time series forecasting using distribution enhanced linear regression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ristanoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling dynamic behavior in large evolving graphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning evolving and emerging topics in social media: a dynamic nmf approach with temporal regularization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="693" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth-first search and linear graph algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="160" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Community discovery using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="521" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Travel time estimation of a path using sparse trajectories</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shortest path and distance queries on road networks: An experimental evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="406" to="417" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mining the situation: Spatiotemporal traffic prediction with big data. Selected Topics in Signal Processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Demiryurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V D</forename><surname>Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="702" to="715" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Travel cost inference from sparse, spatio temporally correlated time series using markov models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="769" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overlapping community detection via bounded nonnegative matrix tri-factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="606" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Time-dependent trajectory regression on road networks via multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Smiler: A semi-lazy time series prediction system for sensors. SIGMOD &apos;15</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Tung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1871" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tripartite graph clustering for dynamic sentiment analysis on social media</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD&apos;14</title>
		<imprint>
			<biblScope unit="page" from="1531" to="1542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scalable link prediction in dynamic networks via non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.3675</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
