<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reversible Sketches for Efficient and Accurate Change Detection over Network Data Streams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><surname>Schweller</surname></persName>
							<email>schwellerr@cs.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60201-3150</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashish</forename><surname>Gupta</surname></persName>
							<email>ashish@cs.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60201-3150</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elliot</forename><surname>Parsons</surname></persName>
							<email>e-parsons@cs.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60201-3150</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Chen</surname></persName>
							<email>ychen@cs.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60201-3150</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reversible Sketches for Efficient and Accurate Change Detection over Network Data Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B6081662E384D84EF761FFAA514F2A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.2.3 [Network Operations]: Network monitoring Measurement</term>
					<term>Algorithms Change detection</term>
					<term>Network anomaly detection</term>
					<term>Data stream computation</term>
					<term>Sketch</term>
					<term>Modular hashing</term>
					<term>IP mangling</term>
					<term>Reverse hashing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traffic anomalies such as failures and attacks are increasing in frequency and severity, and thus identifying them rapidly and accurately is critical for large network operators. The detection typically treats the traffic as a collection of flows and looks for heavy changes in traffic patterns (e.g., volume, number of connections). However, as link speeds and the number of flows increase, keeping per-flow state is not scalable. The recently proposed sketch-based schemes <ref type="bibr" target="#b14">[14]</ref> are among the very few that can detect heavy changes and anomalies over massive data streams at network traffic speeds. However, sketches do not preserve the key (e.g., source IP address) of the flows. Hence, even if anomalies are detected, it is difficult to infer the culprit flows, making it a big practical hurdle for online deployment. Meanwhile, the number of keys is too large to record.</p><p>To address this challenge, we propose efficient reversible hashing algorithms to infer the keys of culprit flows from sketches without storing any explicit key information. No extra memory or memory accesses are needed for recording the streaming data. Meanwhile, the heavy change detection daemon runs in the background with space complexity and computational time sublinear to the key space size. This short paper describes the conceptual framework of the reversible sketches, as well as some initial approaches for implementation. See <ref type="bibr" target="#b23">[23]</ref> for the optimized algorithms in details. Evaluated with netflow traffic traces of a large edge router, we demonstrate that the reverse hashing can quickly infer the keys of culprit flows even for many changes with high accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Real-time network flow monitoring at high packet rates is a challenging but crucial service for network administrators of large ISPs or institutions. Such service is important for accounting, provisioning, traffic engineering, scalable queue management, and anomaly and intrusion detection <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b14">14]</ref>. Take the intrusion detection systems (IDSs) for instance, most existing IDSs reside on single host or low-end routers, examining the application-level <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b15">15]</ref> or system-level <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b25">25]</ref> logs, or the sniffed network packets <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. However, today's fast propagation of viruses/worms (e.g., Sapphire worm) can infect most of the vulnerable machines in the Internet within ten minutes <ref type="bibr" target="#b18">[18]</ref> or even less than 30 seconds with some highly virulent techniques <ref type="bibr" target="#b24">[24]</ref>. Thus it is crucial to identify such outbreaks in their early phases, which can only be possibly achieved by detection at routers instead of at end hosts <ref type="bibr" target="#b26">[26]</ref>.</p><p>Given today's traffic volume and link speeds, the detection method has to be able to handle potentially several millions or more of concurrent network time series. Thus it is either too slow or too expensive to directly apply existing techniques on a per-flow basis <ref type="bibr">[8,</ref><ref type="bibr" target="#b14">14]</ref>. The essential requirements for online flow-level monitoring on high-speed networks are two-fold: 1) small amount of memory usage (to be implemented in SRAM) and 2) small amount of memory accesses per packet <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">8]</ref>. In response to this demand, the field of data streaming computation is emerging, which deals with various computations that can be performed in a space-and time-efficient fashion. Most of the existing work comes from the database and theory communities, as reviewed in a comprehensive survey <ref type="bibr" target="#b17">[17]</ref>. Here we call for bringing techniques from these domains to bear on networking. One particularly powerful technique is the sketch <ref type="bibr" target="#b10">[10]</ref>, a probabilistic summary technique for analyzing large network traffic streams without keeping per-flow states.</p><p>Most existing research on data streaming computation focus on scalable heavy-hitter detection <ref type="bibr">[8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b2">2]</ref>. However, heavy-hitters do not necessarily correspond to flows experiencing anomalies (e.g., significant changes), and thus it is not clear how their techniques can be adapted for anomaly detection. Here, we focus on a more generic and powerful primitive: heavy change detection, which spans from simple absolute or relative changes, to variational changes and linear transformation of these changes for various time-series forecasting models <ref type="bibr" target="#b14">[14]</ref>. Recently, a variant of the sketch data structure, the -ary sketch, was proposed as one of the first schemes for real-time heavy change detection over massive data streams <ref type="bibr" target="#b14">[14]</ref>. As shown in Section 2, the -ary sketch uses a constant, small amount of memory, and has constant per-record update and reconstruction cost <ref type="bibr" target="#b14">[14]</ref>.</p><p>However, one major obstacle for building anomaly/intrusion detection system on -ary sketch is its irreversibility. As modeled in Section 2.1, the streaming data can be viewed as a series of (key, value) pairs where the key can be a source IP address, or the pair of IP addresses, and the value can be the number of bytes or packets, etc. While for any given key, sketch can indicate if it exhibits big change, and if so give an accurate estimation of such change, such process is irreversible. That is, a sketch cannot efficiently report the set of all keys that have large change estimates in the sketch. This means that to compare two streams, we have to know which items (keys) to query to find the streams with big changes <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b5">5]</ref>. This would require either exhaustively testing all possible keys, or recording and testing all data stream keys and corresponding sketches. Unfortunately, neither of these are scalable.</p><p>In this paper we focus on this problem and provide efficient algorithms to reverse sketches, focusing primarily on -ary sketches <ref type="bibr" target="#b14">[14]</ref>. The observation is that only streaming data recording needs to done continuously in real-time, while the change/anomaly detection can run in the background with more memory (DRAM) and at a frequency only in the order of seconds. Then the challenge is: how to keep extremely fast data recording while still being able to detect the heavy change keys with reasonable speed and high accuracy? In this extended abstract, we set up the general framework for the reversible -ary sketch, and discuss some initial approaches for implementation. The fully optimized algorithms and evaluations are presented in <ref type="bibr" target="#b23">[23]</ref>, especially for multiple heavy change detection. With no or negligible extra memory and extra memory accesses for recording streaming data, the heavy change detection daemon runs in the background with space complexity and computational time sublinear to the key space size.</p><p>The rest of the paper is organized as follows. In Section 2, we introduce the data stream model, formulate the heavy change detection problem, and present the architecture of reversible -ary sketch system. The system has two parts: streaming data recording (Section 3) and heavy change detection (Section 4). We show some preliminary evaluation results based on a large edge router traffic data in Section 5. For detecting the keys corresponding to the top 100 changes, we achieve over 95% of true positive rate and less than 2% of false positive rate in 0.42 seconds. The streaming data are recorded with less than 200KB memory. Related work are surveyed in Section 6, and finally the paper concludes in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Stream Model and the -ary Sketch</head><p>Among the multiple data stream models, one of the most general is the Turnstile Model <ref type="bibr" target="#b19">[19]</ref>. Let Á «½ « ¾ be an input stream that arrives sequentially, item by item. Each item «</p><p>´ Ù µ consists of a key ¾ Ò , where Ò ¼ ½ Ò ½ , and an update Ù ¾ Ê. Each key ¾ Ò is associated with a time varying signal Í . Whenever an item ´ Ù µ arrives, the signal Í is incremented by Ù .</p><p>Ã-ary sketch is a powerful data structure to efficiently keep accurate estimates of the signals Í . A -ary sketch consists of À hash tables of size Ã . The hash functions for each table are chosen independently at random from a class of hash functions from Ò to Ã . From here on we will use the variable Ñ Ã interchangeably with Ã. We store the data structure as a À ¢ Ã ´ µ Ë Í Å Ã ½ ½ Ã</p><p>If the hash functions in the sketch are 4-universal, this estimate gives an unbiased estimator of the signal Í with variance inversely proportional to ´Ã ½µ <ref type="bibr" target="#b14">[14]</ref>. See <ref type="bibr" target="#b14">[14]</ref> for details on the appropriate selection of À and Ã to obtain accurate estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Change Detection Problem Formulation</head><p>Ã-ary sketches can be used in conjunction with various forcasting models to perform sophisticated change detection as discussed in <ref type="bibr" target="#b14">[14]</ref>. We focus on the simple model of change detection in which we break up the sequence of data items into two temporally adjacent chunks. We are interested in keys whose signals differ dramatically in size when taken over the first chunk versus the second chunk. In particular, for a given percentage , a key is a heavy change key if the difference in its signal exceeds percent of the total change over all keys. That is, for two inputs sets 1 and 2, if the signal for a key Ü is is Í½ Ü over the first input and Í¾ Ü over the second, then the difference signal for Ü is defined to be Ü Í½ Ü Í¾ Ü . The total difference is È Ü¾ Ò Ü . A key Ü is then defined to be a heavy change key if and only if Ü ¡ .</p><p>In our approach, to detect the set of heavy keys we create twoary sketches, one for each time interval, by updating them for each incoming packet. We then subtract the two sketches. Say Ë½ and Ë¾ are the sketches recorded for the two consecutive time intervals.</p><p>For detecting significant change in these two time periods, we obtain the difference sketch Ë Ë¾ Ë½ . The linearity property of sketches allows us to add or subtract sketches to find the sum or difference of different sketches. Any key whose estimate value in Ë that exceeds the threshold ¡ ËÍÅ ¡ is denoted as a suspect heavy key in sketch Ë and offered as a proposed element of the set of heavy change keys. Instead of focusing directly on finding the set of keys that have heavy change, we instead can attempt to find the set of keys denoted as suspects by a sketch. <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b23">23]</ref> discuss how to choose appropriate values for Ã and À so that the set of suspects is a sufficiently good approximation to the set of actual heavy change keys. For simplicity we focus on the simpler problem of finding the set of keys that hash to heavy buckets in all À hash tables. That is, we can think of our input as a sketch Ì in which certain buckets in each hash table are marked as heavy. Let Ø be the maximum number of distinct heavy buckets in any given hash table, we get the following Reverse Sketch Problem:</p><p>Input: An integer Ø ¼, a sketch Ì with hash functions À ½ ¼ from Ò to Ñ , and for each hash table a set of at most Ø heavy buckets Ê Ñ ;</p><formula xml:id="formula_0">Output: All Ü ¾ Ò such that ´Üµ ¾ Ê for each ¾ À .</formula><p>Solving the reverse sketch problem is a good way to approximate the set of heavy change keys. Consider the case in which there is exactly one heavy bucket in each hash table. The expected number of false positives (number of keys that hash to all heavy buckets by chance) for À hash tables is Ü Ò´½ Ñ µ À where Ñ is the size of the bucket space and Ò is the size of the key space. For À , Ò ¾ ¿¾ and Ñ ¾ ½¾ we get Ü ¿ ¢ ½¼ , which is exceedingly small. Thus solving the reverse sketch problem is an effective way to converge to the set of heavy change keys. To reduce false negatives, in <ref type="bibr" target="#b23">[23]</ref> we consider the more general version of this problem in which we are interested in finding the set of keys that map to heavy buckets in at least À Ö of the À hash tables.</p><p>For simplicity in this paper we focus on the algorithms for the case of Ö ¼ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Architecture</head><p>The conceptual framework of our change detection system has two parts as in Fig. <ref type="figure" target="#fig_0">1</ref>: streaming data recording and heavy change detection. Next, we will introduce each part in this system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RECORDING OF DATA STREAMS</head><p>The first phase of change detection involves receiving key, update pairs one after another from an incoming data stream, and recording them in a summary data structure. As discussed in Section 1, each update should require very few memory accesses and the entire summary structure should be small enough to fit into fast memory. These requirements are fulfilled by the -ary sketch. However, to allow reversibility, we modify the update procedure of the -ary sketch with modular hashing (see Section 3.1). To maintain the accuracy of the sketch with this type of hashing, we also need to perform IP-mangling (see Section 3.2).  The results of each of the hash functions are then concatenated to form the final hash. In our example, the final hash value would consist of 12 bits, deriving each of its 3 bits from the separate hash functions ½ ¾ ¿</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modular Hashing</head><p>. If it requires constant time to hash a value, modular hashing increases our update time from Ç´Àµ to Ç´Õ¡Àµ. In Section 4, we discuss how this modular hashing allows us to efficiently perform change detection.</p><p>However, an important issue with modular hashing is the quality of the hashing scheme. The probabilistic estimate guarantees forary sketch assume 4-universal hash functions, which can map the input keys uniformly over the buckets. Modular hashing does not have this property. Consider network traffic streams, which exhibit strong spatial localities in the IP addresses, i.e., many simultaneous flows only vary in the last few bits of their source/destination IP addresses, and share the same prefixes. With the basic modular hashing, the collision probability of such addresses is significantly increased. For example, consider a set of IP addresses ½¾ ½¼ £ that share the first 3 octets. Our modular hashing always maps the first 3 octets to the same hash values. Thus, assuming our small hash functions are completely random, all distinct IP addresses with these octets will be uniformly mapped to ¾ ¿ buckets, resulting in a lot of collisions. This observation is further confirmed when we apply our modular hashing scheme with the network traces used for evaluation, the distribution of the number of keys per bucket was highly skewed, with most of the IP addresses going to a few buckets as shown in Figure <ref type="figure" target="#fig_2">3</ref>. This significantly disrupts the estimation accuracy of our reversible -ary sketch. To overcome this problem, we introduce the technique of IP mangling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IP Mangling</head><p>In IP mangling we attempt to artificially randomize the input data in an attempt to destroy any correlation or spatial locality in the input data. The objective is to obtain a completely random set of keys, and this process should be still reversible. We refer to this as the OLE (odd linear equation) transformation.</p><p>We are interested in values of Ò equal to ¾ ¿¾ in the case of an IP address. Thus for any odd ¾ Ò such a function on the domain Ò yields a bijection. Our implementation is to choose uniformly at random an odd value for from ½ to Ò ½. The mangled key can easily be reversed by computing ½ <ref type="bibr" target="#b1">[1]</ref> and applying the same function to the mangled key, using ½ instead of . This scheme is good at randomly mapping keys independently as long as their suffixes differ. Ideally, Ò would be a prime and we could choose any from 0 to Ò ½ and thus have a universal hashing scheme.</p><p>The weakness of our method can be seen in that for any two keys that share the last bits, the mangled versions will also share the same last bits. Thus distinct keys that have common suffixes will be more likely to collide than keys with distinct suffixes. However, in the particular case of IP addresses, this is not a problem. Due to the hierarchical nature of IP addresses, it is perfectly reasonable to assume that there is no correlation between the traffic of two IP addresses if they differ in their most significant bits. We thus believe that this mapping will sufficiently alter the original set of keys such that the locality (in terms of hamming distance <ref type="bibr" target="#b12">[12]</ref> or absolute difference) of streaming keys will be destroyed. We find that in practice our intuition holds true and the mangling effectively resolves the highly skewed distribution caused by the modular hash functions. Using the source IP address of each flow as the key, we compare the hashing distribution of the following three hashing methods with the real network flow traces: 1) modular hashing with no IP mangling, 2) modular hashing with OLE transformation for IP mangling, and 3) direct hashing with a completely random hash function. Figure <ref type="figure" target="#fig_2">3</ref> shows the distribution of the number of keys per bucket for each hashing scheme. We observe that the key distribution of modular hashing with OLE transformation is almost the same as that of direct hashing. The distribution for modular hashing without IP mangling is highly skewed. Thus IP mangling is very effective in randomizing the input keys and removing hierarchical correlations among the keys. We note that for non-hierarchical keys, such as source/destintion pairs of IP addresses, an alternate (and slightly less efficient) scheme needs to be used. Such a scheme is described in <ref type="bibr" target="#b23">[23]</ref>.</p><p>Note that no extra memory or memory access is needed for modular hashing or IP mangling. Modular hashing of each word with small number of bits can be performed efficiently without pre-storing the mappings and then executing the table lookup. We can simply ignore any bits higher than ÐÓ Ò for the modular operation of the OLE transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">REVERSE HASHING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single Heavy Bucket</head><p>Once we have updated the sketch for each item in the data stream we want to obtain the set of suspect heavy change keys from the sketch, i.e., the set of keys that hash to heavy buckets in each of the À hash tables. There are two possible approaches to solve this problem. The first is to iterate through all possible keys in the space Ò and output the keys that hash to some heavy buckets for all the hash tables. This brute force approach obviously is not scalable.</p><p>The second basic approach is to perform bucket intersections. Suppose for each hash table there is exactly one heavy bucket. Denote the set of keys that hash to the heavy bucket in table as . We can determine the set of suspect keys by computing the set Ì ¾ À . This approach in general is also not scalable. Each set is expected to be of size Ò Ñ . To perform detection, we need to obtain this set of keys for any given bucket in the sketch. This requires to store the mapping for the whole key space. In addition, it is inefficient to take intersections of such large sets, For example, for 32-bit IP address keys and Ñ ¾ ½¾ , the sets are each of expected size ¾ ¾¼ . However, with modular hashing we can store and intersect these sets more efficiently. To implicity represent the sets , for each bucket we store in memory Õ reverse lookup tables that map the bucket to its modular bucket potential sets</p><formula xml:id="formula_1">½ ¾ Õ .</formula><p>That is, if the index of the bucket corresponding to is Ý½ Ý¾ Ý¿ Ý</p><p>for Õ , then a modular key ÜÛ ¾ Ò ½ Õ is in</p><formula xml:id="formula_2">Û if Û ´ÜÛµ</formula><p>ÝÛ. These modular potential sets give a compact representation of each set of bucket potentials because a key Ü is in if and only if the Û Ø word of Ü is in Û for each Û from 1 to Õ. In addition, we can compute Ì ¾ À by computing Ì ¾ À Û for each Û from 1 to Õ. That is, a key Ü is in Ì ¾ À if and only if the Û Ø word ÜÛ of Ü is in Ì ¾ À Û for each Û from 1 to Õ.</p><p>For example, suppose a heavy bucket has the modular potentials sets ´ ½µ ´ ¾µ ´ ¿µ ´ µ for Õ . In the case of À the intersection involves four separate intersection operations:</p><formula xml:id="formula_3">´½ µ Ì ´¾ µ Ì ´¿ µ Ì ´ µ Ì</formula><p>´ µ for ½ ¾ ¿ , corresponding to four partitions of the IP address. </p><formula xml:id="formula_4">Õ ¡ À´Ò Ñ µ ½ Õ ¡ ¡ ¿¾ ¼ versus À ¡ Ò Ñ ¡¾ ¾¼ ¾ ¾ ¼.</formula><p>Finally, we note that while increasing Õ decreases the the run time of reverse hashing, there is a limit. The size of the space the modular hash functions map to, Ñ ½ Õ , must be greater than 1. There is thus a tradeoff between the size of Ñ, which effectively determines the size of the sketch, and the size of Õ, which determines the efficiency of reverse hashing. In <ref type="bibr" target="#b23">[23]</ref> we discuss in detail this tradeoff and give reasons for choosing Õ ¢´ÐÓ ÐÓ Òµ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multiple Heavy Buckets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: For two heavy changes, various possibilities exist for taking the intersection of each bucket's potential keys</head><p>We now consider how to generalize the method of reverse hashing described above to the case where there are multiple heavy buckets, say at most Ø, in each table. For Ø ¾ the problem is more difficult since it is not clear how to take the bucket intersections described for Ø ½ . For example, for Ø ¾ there are Ø À ¾ À possible ways to take the À-wise intersections discussed above as shown in Figure <ref type="figure">4</ref>. One heuristic solution is to union all of the bucket potential sets in each hash table and intersect these À union sets. But it is easy to see that such a set can contain keys that do not hash to heavy buckets in each of the À hash tables. We thus expect to get a large number of false positives from this method. We could verify each output key of this method by estimating its value through -ary sketch. But in our evaluation section we show that the number of false positives generated by this method grows exponentially in the number of heavy buckets Ø. Thus this verification procedure is applicable only for small values of Ø.</p><p>Our more efficient scheme is as follows. For each Û from 1 to Õ we compute a set ÁÛ which consists of the set of all ÜÛ ¾ Ò ½ Õ such that for each hash table , there is some heavy bucket such that ÜÛ is contained in the modular potential set Û for . In addition, we attach to each element in ÁÛ an À-dimensional bucket vector which denotes which heavy bucket in each hash table the corresponding modular word occurs in. Since a given modular key can potentially occur in up to Ø heavy buckets for a given hash table, each modular key for a word Û can have multiple vectors. The set ÁÛ may thus have multiple occurrences of a given key, once for each of its vectors. We then create a graph whose vertices are the elements of the Õ sets ÁÛ. Edges are drawn between vertices Ü ¾ ÁÛ ½ and Ý ¾ ÁÛ if the bucket vectors for elements Ü and Ý are the same. It then follows that any length Õ path through this graph corresponds to a key that hashes to heavy buckets in all À hash tables.</p><p>For example, in Figure <ref type="figure" target="#fig_4">5</ref>, suppose Ø and each heavy bucket in each hash table is indexed from 1 to 5. From the figure we have that for key ¾ Ò ½ Õ the modular hash functions are such that</p><formula xml:id="formula_5">¼ ½ ´ µ ¾ , ½ ½ ´ µ ½ , ¾ ½ ´ µ , ¿ ½</formula><p>´ µ ½ , and ½ ¿. We also have, by coincidence, that ¾ Ò ½ Õ hash to exactly the same values for each of the À hash functions ¿ . Thus there are two length Õ paths through the graph, and . These are thus the two suspect heavy change keys for the sketch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Work in Progress</head><p>In <ref type="bibr" target="#b23">[23]</ref>, we give the details of our reverse hashing algorithms for multiple heavy changes, as well as various optimizations for reducing false positives and false negatives and for improving efficiency. We highlight those techniques as the following.</p><p>¯We generalize the algorithm so that it detects keys that occur in heavy buckets for at least À Ö of the À hash tables. We can then adjust the parameter Ö to balance the trade-off between false positives and false negatives.</p><p>¯We introduce bucket index matrix algorithm to significantly reduce the size of the produced graph. This allows us to detect larger numbers of heavy changes efficiently.</p><p>¯To further increase the number of heavy changes we can efficiently detect, we propose the iterative approach to reverse hashing. This scheme can also help reduce false positives.</p><p>¯To reduce false positives we use a second verifier sketch that uses 4-universal hash functions. We give analytical bounds on the false positives for this scheme.</p><p>¯We introduce a new IP-mangling scheme with better statistical properties that permits reverse hashing with non-hierarchical keys such as source/destination IP address pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PRELIMINARY EVALUATION</head><p>In this section we show some preliminary evaluation results of our schemes, and refer the readers to <ref type="bibr" target="#b23">[23]</ref> for more comprehensive testing and results. Our evaluation is based on one-day netflow traffic traces collected from a large edge router in Northwestern University. The traces are divided into five-minute intervals with the traffic size for each interval averaging about 7.5GB. In addition to the reversible -ary sketch, we implemented a naive algorithm to record the per-source-IP volumes, and find the top IPs with heavy changes as the ground truth. Then we use the volume change of the top Ü-th (Ü ¾¼ ¼ Ø ) IP as threshold to infer the top Ü IPs with heavy volume changes over that threshold through reversible -ary sketch. Our metrics include speed, real positive and false positive percentages. The real positive percentage refers to the number of true positives reported by the detection algorithm divided by the number of real heavy change keys. The false positive percentage is the number of false positives output by the algorithm divided by the number of keys output by the algorithm.   For single heavy change, we always have 100% real positive and zero false positive. For multiple heavy changes, we also achieve very high accuracy with the algorithms outlined in Sections 4.2 and 4.3. Figure <ref type="figure" target="#fig_6">6</ref> shows some sample results from <ref type="bibr" target="#b23">[23]</ref>, and compare them with those of the simple case Ö ¼ . Here À and ¾ ½¾ , thus only 192KB memory are used for recording the reversible -ary sketches. The results are very accurate for Ö ¾ : over 95% true positive rates for up to 140 heavy changes and negligible false positive rates; while the true positive rate drops significantly with Ö ¼ because it is very sensitive to the collision. In general, higher values of Ö (with Ö À ¾) results in higher true positive percentage with a slight degradation in false positive percentage. We tried several different traces from different time intervals and obtained similar results. Note that for any given key, -ary sketch can only estimate its value with bounded errors. In our experiments, we found that all the heavy changes missed are due to boundary effects caused by estimation error, and all the major changes are detected.</p><p>Next, we compare our bucket-vector algorithm with the naive way of intersecting the unions of buckets in each hash table as described earlier in Section 4.2. Figure <ref type="figure" target="#fig_8">7</ref> shows the number of false positives for the naive method for which the number of false positives rises exponentially even for a small number of true heavy changes. As described in more detail in <ref type="bibr" target="#b23">[23]</ref>, the reverse hashing process is very fast, taking only 0.42 seconds for up to 100 heavy changes with an un-optimized implementation on a Pentium-IV 2.4GHz PC.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>For high-speed network monitoring, most existing high-speed network monitoring systems estimate the flow-level traffic through packet sampling <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref>, but this has two shortcomings. First, sampling is still not scalable, especially after aggregation; there are up to ¾ flows even defined only by source and destination IP addresses. Second, long-lived traffic flows, increasingly prevalent for peer-to-peer applications, will be split up if the time between sampled packets exceeds the flow timeout <ref type="bibr" target="#b6">[6]</ref>.</p><p>Applications of sketches in the data streaming community have been researched quite extensively in the past. This has also been motivated by the emerging popularity of applications for network traffic accounting, anomaly detection and very large databases with massive data streams. Usually the work has focused on extracting certain data aggregation functions with the use of sketches, like quantiles and heavy hitters <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b2">2]</ref>, distinct items <ref type="bibr" target="#b9">[9]</ref> etc.</p><p>Recently, Cormode and Muthukrishnan proposed deltoids approach for heavy change detection <ref type="bibr" target="#b5">[5]</ref>. Though developed independently of -ary sketch, deltoid essentially expands -ary sketch with multiple counters for each bucket in the hash tables. The number of counters is logarithmic to the key space size (e.g., 32 for IP address), so that for every (key, value) entry, instead of adding the value to one counter in each hash table, it is added to multiple counters (32 for IP addresses and 64 for IP address pairs) in each hash table. This significantly increases the necessary amount of fast memory and number of memory accesses per packet, thus violating both of the aforementioned performance constraints. For instance, it requires more than 1MB to detect 100 or more changes, and therefore cannot even fit into the latest FPGAs, which only has up to 600KB of block SRAM that can be efficiently utilized <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>We have proposed novel reverse hashing methods for improving sketch-based change detection in high speed traffic. Our techniques can efficiently and accurately output the set of keys which show heavy change in two different time intervals, without storing the key information explicitly. Being able to reverse a sketch in this fashion is a key step to enhance the power of sketch based change detection to online, single pass settings. Without adding any memory or memory access to record the streaming data, our algorithms use sub-linear time and space in the size of the key space for heavy change detection <ref type="bibr" target="#b23">[23]</ref>, and are scalable to large key spaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Conceptual architecture of the reversible -ary sketch based heavy change detection system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Modular hashing uses Õ hash functions to hash each word of the key, which are then combined for the final hash. Modular hashing is illustrated in Figure 2. Instead of hashing the entire key in Ò directly to a bucket in Ñ , we partition the key into Õ words, each word of size ½ Õ ÐÓ Ò bits. Each word is then hashed separately with different hash functions which map from space Ò ½ Õ to Ñ ½ Õ . For example, in Figure 2, a 32-bit IP address is partitioned into Õ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of number of keys for each bucket under three hashing methods.The general framework for the technique is to use a bijective (one-to-one) function from key space Ò to Ò . For an input data set consisting of a set of distinct keys Ü , we map each Ü to ´Ü µ. We then use our algorithm to compute the set of proposed heavy change keys</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The sets ÁÛ form a graph whose length Õ paths correspond to the heavy change keys.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Heavy change detection accuracy: true positive (top) and false positive (bottom). The top-Ü axis show the corresponding change threshold defined in Section 2.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The number of false positives by intersecting the unions of buckets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The re- sultant intersections from the four partitions can then be combined to form the final set of suspect keys, i.e., any Ü½ Ü¾ Ü¿ Ü such that each Ü ¾ . Since each set being intersected has size ´Ò Ñ µ ½ For the parameter values given above, our method yields</figDesc><table><row><cell>Õ we can determine these Õ different sets of À set intersections in time Ç´Õ ¡ À ¡ ´Ò Ñ µ ½ Õ µ. Without modular hashing, the intersection takes Ç´À¡ Ò Ñ µ.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introduction to Algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rivest</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding hierarchical heavy hitters in data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VLDB</title>
		<meeting>of VLDB</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Holistic UDAFs at streaming speeds</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMOD</title>
		<meeting>of ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved data stream summaries: The count-min sketch and its applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DIMACS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 2003-20</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What&apos;s new: Finding significant differences in network data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Infocom</title>
		<meeting>of IEEE Infocom</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Properties and prediction of flow statistics from sampled packet streams</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thorup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM Internet Measurement Workshop (IMW)</title>
		<meeting>of ACM SIGCOMM Internet Measurement Workshop (IMW)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flow sampling under hard resource constraints</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thorup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMETRICS</title>
		<meeting>of ACM SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">New directions in traffic measurement and accounting</title>
		<author>
			<persName><forename type="first">C</forename><surname>Estan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic counting algorithms for data base applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Flajolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="182" to="209" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">QuickSAND: Quick summary and analysis of network data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DIMACS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 2001-43</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">QuickSAND: Quick summary and analysis of network data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kotidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Strauss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>DIMACS</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 2001-43</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Coding and Information Theory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Hamming</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intrusion detection using sequences of system calls</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hofmeyr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Forrest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sketch-based change detection: Methods, evaluation, and applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM IMC</title>
		<meeting>of ACM SIGCOMM IMC</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building adaptive and agile applications using intrusion detection and response</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Loyall</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NDSS</title>
		<meeting>of NDSS</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Approximate frequency counts over data streams</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE VLDB</title>
		<meeting>of IEEE VLDB</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data streams: algorithms and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mirkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reiher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SODA</title>
		<meeting>of ACM SODA</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The spread of the Sapphire/Slammer worm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Magazine on Security and Privacy</title>
		<imprint>
			<date type="published" when="2003-08">August 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data streams: Algorithms and applications (short)</title>
		<author>
			<persName><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SODA</title>
		<meeting>of ACM SODA</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A system for detecting network intruders in real-time</title>
		<author>
			<persName><forename type="first">V</forename><surname>Paxson</surname></persName>
		</author>
		<author>
			<persName><surname>Bro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2435" to="2463" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Snort: The lightweight network intrusion detection system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roesch</surname></persName>
		</author>
		<ptr target="http://www.snort.org/" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integrated access control and intrusion detection for web servers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ryutov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="841" to="850" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reverse hashing for sketch-based change detection on high-speed networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schweller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Memik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno>NWU-CS-2004-45</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Northwestern University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How to own the Internet in your spare time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Staniford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Paxson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Security Symposium</title>
		<meeting>the 11th USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Intrusion detection via static analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Symposium on Security and Privacy</title>
		<meeting>of the IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Large scale malicious code: A research agenda</title>
		<author>
			<persName><forename type="first">N</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Paxson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staniford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cunningham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. DARPA-sponsored report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Virtex-II Pro and Virtex-II Pro X Platform FPGAs: Complete data sheet</title>
		<author>
			<persName><surname>Xilinx Inc</surname></persName>
		</author>
		<ptr target="www.xilinx.com/bvdocs/publications/ds083.pdf" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
