<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</title>
				<funder ref="#_AqavEgx">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Cox</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Harvard University</orgName>
								<address>
									<country>IBM USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">MAPL &apos;19</orgName>
								<address>
									<addrLine>June 22</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">MAPL &apos;19</orgName>
								<address>
									<addrLine>June 22</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3315508.3329973</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts -usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial.</p><p>We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.</p><p>CCS Concepts ? Computing methodologies ? Parallel computing methodologies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent resurgence of Deep Neural Networks (DNNs) was largely enabled <ref type="bibr" target="#b24">[24]</ref> by the widespread availability of programmable, parallel computing devices. In particular, continuous improvements in the performance of many-core architectures (e.g., GPUs) have played a fundamental role, by enabling researchers and engineers to explore an evergrowing variety of increasingly large models, using more and more data. This effort was supported by a collection of vendor libraries (cuBLAS, cuDNN) aimed at bringing the latest hardware innovations to practitioners as quickly as possible. Unfortunately, these libraries only support a restricted set of tensor operations, leaving the implementation of novel primitives to experts <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b25">25]</ref>.</p><p>This observation has led to the development of various Domain-Specific Languages (DSLs) for DNNs, based on polyhedral machinery (e.g., Tensor Comprehensions <ref type="bibr" target="#b43">[43]</ref>) and/or loop synthesis techniques (e.g., Halide <ref type="bibr" target="#b37">[37]</ref>, TVM <ref type="bibr" target="#b10">[10]</ref> and PlaidML <ref type="bibr" target="#b22">[22]</ref>). But while these systems generally perform well for certain classes of problems such as depthwise-separable convolutions (e.g., MobileNet <ref type="bibr" target="#b20">[20]</ref>), they are often much slower than vendor libraries in practice (see, e.g., Figure <ref type="figure" target="#fig_0">1</ref>), and lack the expressivity necessary to implement structured sparsity patterns <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b47">47]</ref> that cannot be directly specified using affine array indices in nested loops.  <ref type="bibr" target="#b46">[46]</ref> Model (NVIDIA GeForce GTX1070), where A ? R 1760?1760 and B ? R N ?1760 as N modulates arithmetic intensity. These issues have often been addressed by the use of micro-kernels <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b21">21]</ref> -i.e., hand-written tile-level intrinsics -but this solution requires a lot of manual labor and lacks portability. And while several high-level programming abstractions for tiling have recently been proposed <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41]</ref>, underlying compiler backends still lack support for tile-level operations and optimizations. To this end we present Triton (Figure <ref type="figure" target="#fig_1">2</ref>), an open-source 1 intermediate language and compiler for specifying and compiling tile programs into efficient GPU code. The main contributions of this paper are summarized as follows:</p><p>? Triton-C (Section 3): A C-like language for expressing tensor programs in terms of parametric tile variables. The purpose of this language is to provide a stable interface for existing DNN transcompilers (e.g., PlaidML, Tensor Comprehensions) and programmers familiar with CUDA. Listing 1 shows the Triton-C source code associated with a simple matrix multiplication task. any compilation target; (2) a set of tile-level machinedependent passes for generating efficient GPU-ready LLVM-IR; and (3) an auto-tuner that optimize any meta-parameter associated with the above passes.</p><p>? Numerical Experiments (Section 6): A numerical evaluation of Triton that demonstrates its ability to (1) generate matrix multiplication implementations on par with cuBLAS and up to 3? faster than alternatives DSLs on recurrent and transformer neural networks;</p><p>(2) re-implement cuDNN's IMPLICIT_GEMM algorithm for dense convolution without performance loss; and (3) create efficient implementations of novel research ideas such as shift-conv <ref type="bibr" target="#b47">[47]</ref> modules. This paper will be prefaced by a brief analysis of the existing related literature (Section 2) and concluded by a summary and directions of future work (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The existence of frameworks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b36">36]</ref> and libraries for Deep Learning has been critical to the emergence of novel neural network architectures and algorithms. But despite advances in analytical <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b48">48]</ref> and empirical <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">30]</ref> heuristics for linear algebra compilers, these software still invariably rely on hand-optimized sub-routines (e.g., cuBLAS and cuDNN). This has led to development of various DSLs and compilers for DNNs, generally based on one of three distinct approaches:</p><p>? Tensor-level IRs have been used by XLA <ref type="bibr" target="#b16">[16]</ref> and Glow <ref type="bibr" target="#b38">[38]</ref> to transform tensor programs into predefined LLVM-IR and CUDA-C operation templates (e.g., tensor contractions, element-wise operations, etc.) using pattern-matching. ? The polyhedral model <ref type="bibr" target="#b18">[18]</ref> has been used by Tensor Comprehensions (TC) <ref type="bibr" target="#b43">[43]</ref> and Diesel <ref type="bibr" target="#b14">[14]</ref> to parameterize and automate the compilation of one or many DNN layers into LLVM-IR and CUDA-C programs. ? Loop synthesizers have been used by Halide <ref type="bibr" target="#b37">[37]</ref> and TVM <ref type="bibr" target="#b10">[10]</ref> to transform tensor computations into loop nests that can be manually optimized using userdefined (though possibly parametric <ref type="bibr" target="#b11">[11]</ref>) schedules. By contrast, Triton relies on the addition of tile-level operations and optimizations into traditional compilation pipelines. This approach provides (1) more flexibility than XLA and Glow; (2) support for non-affine tensor indices contrary to TC and Diesel; and (3) automatic inference of possible execution schedule that would otherwise have to be specified manually to Halide or TVM. The benefits of Triton come at the cost of increased programming efforts -see Listing 2 for implementations of matrix multiplication in these DSLs.</p><formula xml:id="formula_0">C = tf . matmul (A , tf . transpose ( B ) ) // TF C [i , j: I , J] = +( A [i , k ] * B [j , k ]) ; // PlaidML C (i , j) +=! A(i , k ) * B (j , k ) // TC tvm . sum (A[i , k] * B [j , k ] , axis = k ) // TVM</formula><p>Listing 2. C = A ? B T in TF, PlaidML, TC and TVM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Triton-C Language</head><p>The purpose of Triton-C is to provide a stable frontend for existing (and future) DNN transcompilers, as well as programmers familiar with low-level GPU programming. In this section we describe the CUDA-like syntax of Triton-C (Section 3.1), its Numpy <ref type="bibr" target="#b35">[35]</ref>-like semantics (Section 3.2) and its "Single-Program, Multiple-Data" (SPMD) programming model (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Syntax</head><p>The syntax of Triton-C is based on that of ANSI C (and more specifically CUDA-C), but was modified and extended (see Listing 3) to accomodate the semantics and programming model described in the two next subsections. These changes fall into the following categories:</p><p>Tile declarations: We added special syntax for declaring multi-dimensional arrays (e.g., int tile <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b16">16]</ref>) so as to emphasize their semantical difference with nested arrays found in ANSI C (e.g., int tile <ref type="bibr">[16][16]</ref>). Tile shapes must be constant but can also be made parametric with the tunable keyword. One-dimensional integer tiles may be initialized using ellipses (e.g., int range <ref type="bibr" target="#b7">[8]</ref>  Built-in function: While common C syntax was retained for element-wise array operations (+, -, &amp;&amp;, *, etc.), various built-in functions (dot, trans, get_global_range) were added to support tile semantics (Section 3.2.1) and the SPMD programming model.</p><p>Broadcasting: N-dimensional tiles can be broadcast along any particular axis using the newaxis keyword and usual slicing syntax (e.g., int broadcast <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b7">8]</ref> = range[:, newaxis] for stacking columns). Note that slicing tiles to retrieve scalars or sub-arrays is otherwise forbidden.</p><p>Predication: Basic control-flow within tile operations (Section 4.3) is achieved through the use of predicated statements via the '@' prefix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Tile Semantics</head><p>The existence of built-in tile types and operations (i.e., tile semantics) in Triton-C offers two main benefits. First, it simplifies the structure of tensor programs by hiding important performance details pertaining to intra-tile memory coalescing <ref type="bibr" target="#b12">[12]</ref>, cache management <ref type="bibr" target="#b32">[32]</ref> and specialized hardware utilization <ref type="bibr" target="#b27">[27]</ref>. Second, it opens the door for compilers to perform these optimizations automatically, as discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Broadcasting Semantics</head><p>Tiles in Triton-C are strongly typed in the sense that certain instructions statically require their operands to obey strict shape constraints. For example, a scalar may not be added to an array unless it is first appropriately broadcast. Broadcasting semantics <ref type="bibr" target="#b35">[35]</ref> provide a set of rules to perform these conversions implicitly (see Listing 4 for an example):</p><p>1. Padding: the shape of the shortest operand is leftpadded with ones until both operands have the same dimensionality.</p><p>2. Broadcasting: the content of both operands is replicated as many times as needed until their shape is identical; an error is emitted if this cannot be done.</p><p>int a <ref type="bibr" target="#b16">[16]</ref> , b <ref type="bibr">[32 , 16]</ref> , c <ref type="bibr">[16 , 1]</ref>; // a is first reshaped to <ref type="bibr">[1 , 16]</ref> // and then broadcast to <ref type="bibr">[32 , 16]</ref> int x_1 <ref type="bibr">[32 , 16]</ref> = a [ newaxis , :] + b ; // Same as above but implicitly int x_2 <ref type="bibr">[32 , 16]</ref> = a + b ; // a is first reshaped to <ref type="bibr">[1 , 16]</ref> // a is broadcast to <ref type="bibr">[16 , 16]</ref> // c is broadcast to <ref type="bibr">[16 , 16]</ref> int y <ref type="bibr">[16 , 16]</ref> = a + c ;</p><p>Listing 4. Broadcasting semantics in practice</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Programming Model</head><p>The execution of CUDA <ref type="bibr" target="#b33">[33]</ref> code on GPUs is supported by an SPMD <ref type="bibr" target="#b3">[4]</ref> programming model in which each kernel is associated with an identifiable thread-block in a so-called launch grid. The Triton programming model is similar, but each kernel is single-threaded -though automatically parallelized -and associated with a set of global ranges that varies from instance to instance (see Figure <ref type="figure" target="#fig_3">3</ref>). This approach leads to simpler kernels in which CUDA-like concurrency primitives (shared memory synchronization, inter-thread communication, etc.) are inexistent.</p><p>The global ranges associated with a kernel can be queried using the get_global_range(axis) built-in function in order to create e.g., tiles of pointers as shown in Listing 1.   from Triton-C during parsing, although they could also be generated directly from higher level DSLs in the future.</p><p>Triton-IR and LLVM-IR programs share the same highlevel structure (recalled in Section 4.1), but the former also includes a number of extensions necessary for tile-level dataflow (Section 4.2) and control-flow (Section 4.3) analysis. These novel extensions are crucial for carrying out the optimizations outlined in Section 5, and for safely accessing tensors of arbitrary shapes as shown in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Structure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Modules</head><p>At the highest level, Triton-IR programs consist of one or multiple basic units of compilation known as modules. These modules are compiled independently from one another, and eventually aggregated by a linker whose role is to resolve forward declarations and adequately merge global definitions.</p><p>Each module itself is composed of functions, global variables, constants and other miscellaneous symbols (e.g., metadata, function attributes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Functions</head><p>Triton-IR function definitions consist of a return type, a name and a potentially empty arguments list. Additional visibility, alignment and linkage specifiers can be added if desired. Function attributes (such as inlining hints) and parameter attributes (such as read-only, aliasing hints) can also be specified, allowing compiler backends to perform more aggressive optimizations by, for instance, making better use of read-only memory caches.</p><p>This header is followed by a body composed of a list of basic blocks whose interdependencies form the Control Flow Graph (CFG) of the function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Basic Blocks</head><p>Basic blocks are, by definition, straight-line code sequences that may only contain so-called terminator instructions (i.e., branching, return) at their end.</p><p>Triton-IR uses the Static Single Assignment (SSA) form, meaning that each variable in each basic block must be (1) assigned to only once and (2) defined before being used. In so doing, each basic block implicitly defines a Data-Flow Graph (DFG) whose different paths correspond to use-def chains in the program's SSA representation. This form can be created directly from Abstract Syntax Trees (ASTs) as shown in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Support for Tile-Level Data-Flow Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Types</head><p>Multi-dimensional tiles are at the center of data-flow analysis in Triton-IR and can be declared using syntax similar to vector declarations in LLVM-IR. For example, i32&lt;8, 8&gt; is the type corresponding to 8 ? 8 32-bit integer tiles. Note that there is no tunable keyword in Triton-IR, hence parametric shape values must be resolved before programs are generated. In our case, this is done by Triton-JIT's auto-tuner (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Instructions</head><p>Triton-IR introduces a set of retiling instructions whose purpose is to support broadcasting semantics as described in Section 3.2.2:</p><p>? The reshape instruction creates a tile of the specified shapes using data from its input argument. This is particularly useful to re-interpret variables as higherdimensional arrays by padding their input shapes with ones in preparation of implicit or explicit broadcasting. ? The broadcast instruction creates a tile of the specified shapes by replicating its input argument as many times as necessary along dimensions of size 1 -as shown in Figure <ref type="figure" target="#fig_5">4</ref>. Usual scalar instructions (cmp, getelementptr, add, load...) were preserved and extended to signify element-wise operations on tile operands. Finally, Triton-IR also exposes specialized arithmetic instructions for transpositions (trans) and matrix multiplications (dot).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Support for Tile-Level Control-Flow Analysis</head><p>One problem that arises from the existence of tile-level operations in Triton-IR is the inexpressibility of divergent control flow within tiles. For example, a program may need to partially guard tile-level loads against memory access violations, but this cannot be achieved using branching since tile elements cannot be accessed individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Tile-Level Peephole Optimization</head><p>The presence of tile-level operations in Triton-IR offers new opportunities for peephole <ref type="bibr" target="#b29">[29]</ref> optimizers. For instance, chains of transpositions can be simplified using the identity X = (X T ) T for any tile X. We believe that other algebraic properties related to e.g., diagonal tiles could also be exploited in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Machine-Dependent Passes</head><p>We now present a set of optimization passes for machines that follow the high-level model shown in Figure <ref type="figure" target="#fig_4">5</ref>. Specifically, the optimizations performed by Triton-JIT consist of (1) hierarchical tiling, (2) memory coalescing, (3) shared memory allocation and (4) shared memory synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Hierarchical Tiling</head><p>Nested tiling strategies (see Figure <ref type="figure" target="#fig_4">5</ref>) aim at decomposing tiles into micro-tiles and eventually nano-tiles in order to fit a machine's compute capabilities and memory hierarchy as tightly as possible. While this technique is routinely used in auto-tuning frameworks <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b40">40]</ref>, the structure of Triton-IR makes it possible to automatically enumerate and optimize valid nested tiling configurations for any expressible program (and without the need for polyhedral machinery). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Memory Coalescing</head><p>Memory accesses are said to be coalesced when adjacent threads simultaneously access nearby memory locations. This is important because memory is usually retrieved in large blocks from DRAM.</p><p>Because Triton-IR programs are single-threaded and automatically parallelized, our compiler backend is able to order threads internally within each micro-tile so as to avoid uncoalesced memory accesses when possible. This strategy reduces the number of memory transactions necessary to load a tile column (see Figure <ref type="figure" target="#fig_7">6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Shared Memory Allocation</head><p>Tile-level operations that have high arithmetic intensity (e.g., dot) can benefit from temporarily storing their operands in fast shared memory. The purpose of the Shared Memory Allocation pass is to determine when and where a tile should be stashed to this space. This can be done, as illustrated in Figure <ref type="figure" target="#fig_8">7</ref>, by first calculating the live range of each variable of interest, and then using the linear-time storage allocation algorithm proposed in <ref type="bibr" target="#b15">[15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Shared Memory Synchronization</head><p>Reads from and write to shared memory are asynchronous in our machine model. The goal of the Shared Memory Synchronization pass automatically inserts barriers in the generated GPU source code so as to preserve program correctness. This is done by detecting read-after-writes (RAW) and write-after-read(WAR) hazards using forward data-flow analysis with the following data-flow equations:</p><formula xml:id="formula_1">in (RAW ) s = p ?pred(s) out (RAW ) p in (W AR) s = p ?pred(s) out (W AR) p out (RAW ) s = ? if in (RAW ) s ? read(s) ? (barrier) in (RAW ) s ? write(s) otherwise out (W AR) s = ? if in (W AR) s ? write(s) ? (barrier) in (W AR) s ? read(s) otherwise</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Auto-tuner</head><p>Traditional auto-tuners <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b45">45]</ref> typically rely on handwritten parameterized code templates to achieve good performance on pre-defined workloads. By contrast, Triton-JIT can extract optimization spaces directly from Triton-IR programs by simply concatenating meta-parameters associated with each of the above optimization passes.</p><p>In this work, only the Hierarchical Tiling pass is considered, leading to no more than 3 tiling parameters per dimension per tile. These parameters are then optimized using an exhaustive search over powers of two between (a) 32 and 128 for tile sizes; (b) 8 and 32 for micro-tile sizes; and (c) 1 and 4 for nano-tile sizes. Better auto-tuning methods could be used in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Numerical Experiments</head><p>In this section we evaluate the performance Triton on various workloads from the Deep Learning literature. We used an NVIDIA GeForce GTX1070 and compared our system against the most recent vendor libraries (cuBLAS 10.0, cuDNN 7.0) as well as related compiler technology (Auto-TVM, TC, PlaidML). When applicable, we auto-tuned these DSLs for each individual problem size following official documentation guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Matrix Multiplication</head><p>Matrix multiplication tasks of the form:</p><formula xml:id="formula_2">A = D ? W T (D ? R M ?K , W ? R N ?K )</formula><p>are at the heart of neural network computations. Here we consider a variety of tasks from recurrent (DeepSpeech2 <ref type="bibr" target="#b2">[3]</ref>) and transformer <ref type="bibr" target="#b44">[44]</ref> neural networks; we report their performance in Figure <ref type="figure" target="#fig_9">8</ref>. Triton and cuBLAS are generally on par with each other, and achieve more than 90% of the device's peak performance on certain tasks. CuBLAS, however, remains faster than Triton on shallow transformer neural networks thanks to the use of a 3D algorithm <ref type="bibr" target="#b1">[2]</ref> which splits deep reductions into independent chunks to provide more parallelism when M and N are too small. Otherwise, existing DSLs are 2-3x slower than our solution -except for TVM (&lt; 2x slower) when input shapes are multiples of 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Convolutions</head><p>Convolutional Neural Networks (CNNs) are an important class of machine learning models which should be well supported by DSLs and compilers. They are based around convolutional layers (Figure <ref type="figure" target="#fig_10">9a</ref>) whose implementation as matrix multiplication (Figure <ref type="figure" target="#fig_10">9b</ref>) is necessary to make use of specialized tensor-processing hardware -yet unsupported by existing DSLs. Here we benchmark a Triton re-implementation of cuDNN's "IMPLICIT_GEMM" algorithm (Section 6.2.1) and provide the first fused kernel available for shifted convolutions (Section 6.2.2). We implemented these routines using look-up tables of pointer increments, as shown in Listing 8. The convolutional layers considered in this subsection are from the Deep Learning literature and shown in Table <ref type="table" target="#tab_2">1</ref>.</p><p>As shown in Figure <ref type="figure" target="#fig_11">10</ref>, Triton outperforms cuDNN's implementation of IMPLICIT_GEMM for ResNet. This may be due to the fact that cuDNN also maintains better algorithms for 3 ? 3 convolutions (i.e., Winograd <ref type="bibr" target="#b25">[25]</ref>), thereby leaving  We finally consider an implementation of Task1-5 from Table <ref type="table" target="#tab_2">1</ref> as shifted convolutions -a novel approach to CNNs (see Figure <ref type="figure" target="#fig_10">9a</ref>). We compare our implementation of a fused shift-conv module in Triton (Listing 8) against that of a naive implementation relying on a hand-written shift kernel and a separate call to cuBLAS. We also report the maximum attainable performance when shift is not done (i.e., 1 ? 1 convolution). As we can see in Figure <ref type="figure" target="#fig_13">11</ref>, our Triton implementation is able to almost entirely hide the cost of shifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we presented Triton, an open-source language and compiler for expressing and compiling tiled neural network computations into efficient machine code. We showed that the addition of just a few data-and control-flow extensions to LLVM-IR could enable various tile-level optimization passes which jointly lead to performance on-par with vendor libraries. We also proposed Triton-C, a higherlevel language in which we were able to concisely implement efficient kernels for novel neural network architectures for CNNs.  Directions of future work includes support for tensor cores, implementation of quantized kernels <ref type="bibr" target="#b26">[26]</ref> and integration into higher level DSLs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Performance of various implementations of C = AB T vs. Roofline<ref type="bibr" target="#b46">[46]</ref> Model (NVIDIA GeForce GTX1070), where A ? R 1760?1760 and B ? R N ?1760 as N modulates arithmetic intensity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of Triton</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>Triton-IR (Section 4): An LLVM-based Intermediate Representation (IR) that provides an environment suitable for tile-level program analysis, transformation and optimization. Listing 5 shows the Triton-IR code for a Rectified Linear Unit (ReLU) function. Here Triton-IR programs are constructed directly from Triton-C during parsing, but automatic generation from embedded DSLs or higher-level DNN compilers (e.g., TVM) could also be explored in the future.? Triton-JIT (Section 5): A Just-In-Time (JIT) compiler and code generation backend for compiling Triton-IR programs into efficient LLVM bitcode. This includes (1) a set of tile-level, machine-independent passes aimed at simplifying input compute kernels independently of 1 http://triton-lang.org / / T i l e s h a p e s a r e p a r a m e t r i c and can be o p t i m i z e d / / by c o m p i l a t i o n b a c k e n d s c o n s t t u n a b l e i n t TM = { 1 6 , 3 2 , 6 4 , 1 2 8 } c o n s t t u n a b l e i n t TN = { 1 6 , 3 2 , 6 4 , 1 2 8 } c o n s t t u n a b l e i n t TK = { 8 , 1 6 } / / C = A * B . T k e r n e l v o i d matmul_nt ( f l o a t * a , f l o a t * b , f l o a t * c , i n t M, i n N , i n t K ) { / / 1D t i l e o f i n d i c e s i n t rm [TM] = g e t _ g l o b a l _ r a n g e ( 0 ) ; i n t rn [TN] = g e t _ g l o b a l _ r a n g e ( 1 ) ; i n t r k [ TK ] = 0 . . . TK ; / / 2D t i l e o f a c c u m u l a t o r s f l o a t C[TM, TN] = 0 ; / / 2D t i l e o f p o i n t e r s f l o a t * pa [TM, TK ] = a + rm [ : , n e w a x i s ] + r k * M; f l o a t * pb [TN , TK ] = b + rn [ : , n e w a x i s ] + r k * K ; f o r ( i n t k = K ; k &gt;= 0 ; k -= TK ) { b o o l c h e c k _ k [ TK ] = r k &lt; k ; b o o l c h e c k _ a [TM, TK ] = ( rm &lt; M) [ : , n e w a x i s ] &amp;&amp; c h e c k _ k ; b o o l c h e c k _ b [TN , TK ] = ( rn &lt; N ) [ : , n e w a x i s ] &amp;&amp; c h e c k _ k ; / / l o a d t i l e o p e r a n d s f l o a t A[TM, TK ] = c h e c k _ a ? * pa : 0 ; f l o a t B [TN , TK ] = c h e c k _ b ? * pb : 0 ; / / a c c u m u l a t e C += d o t ( A , t r a n s ( B ) ) ; / / u p d a t e p o i n t e r s pa = pa + TK * M; pb = pb + TK * N ; } / / w r i t e -back a c c u m u l a t o r s f l o a t * pc [TM, TN] = c + rm [ : , n e w a x i s ] + rn * M; b o o l c h e c k _ c [TM, TN] = ( rm &lt; M) [ : , n e w a x i s ] &amp;&amp; ( rn &lt; N ) ; @check_c * pc = C ; } Listing 1. C = A ? B T in Triton-C. Keywords specific to Triton are shown in purple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Difference between the CUDA and the Triton programming model</figDesc><graphic url="image-1.png" coords="4,96.01,417.92,156.03,176.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Listing 5 .</head><label>5</label><figDesc>A = max(A, 0) in Triton-IR. Note that tile shapes are non-parametric here. In this paper their values are instantiated by the Triton-JIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The broadcast &lt;3,3&gt; instruction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Hierarchical Tiling in the Triton-IR Machine Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Uncoalesced (a) and coalesced (b) DRAM accesses. Different threads are shown in different colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Shared Memory Allocation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Performance of matrix multiplication</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Dense and shifted convolutional layers (a) viewed as matrix multiplication (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Performance of implicit matrix multiplication 6.2.2 Shift Convolutions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>const tunable int TM = {16 , 32 , 64 , 128}; const tunable int TN = {16 , 32 , 64 , 128}; const tunable int TK = {8}; __constant__ int * delta = alloc_const int [512]; for ( int c = 0; c &lt; C ; c ++) delta [ c ] = c * H * W + shift_h [ c ]* W + shift_w [ c ] void shift_conv ( restrict read_only float *a , restrict read_only float *b , float *c , int M , int N , int K ) { int rxa [ TM ] = get_global_range [ TM ](0) ; int ryb [ TN ] = get_global_range [ TN ](1) ; int rka [ TK ] = 0 ... TK ; int rkb [ TK ] = 0 ... TK ; float C [ TM , TN ] = 0; float * pxa [ TM , TK ] = a + rxa [: , newaxis ]; float * pb [ TN , TK ] = b + ryb [: , newaxis ] + rkb *N; __constant__ int * pd [ TK ] = delta + rka ; for ( int k = K ; k &gt; 0; k = k -TK ) { int delta [ TK ] = * pd ; float * pa [ TM , TK ] = pxa + delta [ newaxis , :]; float a [ TM , TK ] = * pa ; float b [ TN , TK ] = * pb ; C = dot (a , trans ( b ) , C ) ; pb = pb + TK * N ; pd = pd + TK ; } int rxc [ TM ] = get_global_range [ TM ](0) ; int ryc [ TN ] = get_global_range [ TN ](1) ; float * pc [ TM , TN ] = c + rxc [: , newaxis ] + ryc *M; bool checkc0 [ TM ] = rxc &lt; M ; bool checkc1 [ TN ] = ryc &lt; N ; bool checkc [ TM , TN ] = checkc0 [: , newaxis ] &amp;&amp; checkc1 ; @checkc * pc = C ; } Listing 8. shift-convolutions in Triton-C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Performance of shifted convolutions in Triton</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>= 0 ... 8).</figDesc><table><row><cell cols="2">// Broadcasting semantics</cell></row><row><cell>slice</cell><cell>: ': ' | ' newaxis '</cell></row><row><cell>slice_list</cell><cell>: slice | slice_list ',' slice</cell></row><row><cell>slice_expr</cell><cell>: postfix_expr | expr '[ ' slice_list '] '</cell></row><row><cell cols="2">// Range initialization</cell></row><row><cell cols="2">constant_range : expr ' ... ' expr</cell></row><row><cell>// Intrinsics</cell><cell></cell></row><row><cell>global_range</cell><cell>: ' get_global_range ' '( ' constant ') '</cell></row><row><cell>dot</cell><cell>: ' dot ' '( ' expr ',' expr ') '</cell></row><row><cell>trans</cell><cell>: ' trans ' '( ' expr ', ' expr ') '</cell></row><row><cell cols="2">intrinsic_expr : global_range | dot | trans</cell></row><row><cell cols="2">// Predication</cell></row><row><cell cols="2">predicate_expr : '@ ' expr</cell></row><row><cell cols="2">// Tile extensions for abstract declarators</cell></row><row><cell cols="2">abstract_decl : abstract_decl | '[ ' constant_list '] '</cell></row><row><cell cols="2">// Extensions of C expressions</cell></row><row><cell>expr</cell><cell>: expr | constant_range | slice_expr</cell></row><row><cell></cell><cell>| intrinsic_expr</cell></row><row><cell cols="2">// Extensions of C specifiers</cell></row><row><cell cols="2">storage_spec : storage_spec | ' kernel '</cell></row><row><cell>type_spec</cell><cell>: type_spec | ' tunable '</cell></row><row><cell cols="2">// Extensions of C statements</cell></row><row><cell>statement</cell><cell>: statement | predicate_expr statement</cell></row></table><note><p>Listing 3. Grammar extensions for Triton-C. We assume the existence of certain C constructs shown in blue.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Convolution tasks considered in this paper</figDesc><table><row><cell cols="2">H W</cell><cell cols="2">C B K R S Application</cell></row><row><cell cols="4">Task 1 112 112 64 4 128 3 3</cell><cell>ResNet [19]</cell></row><row><cell cols="4">Task 2 56 56 128 4 256 3 3</cell><cell>ResNet</cell></row><row><cell cols="4">Task 3 28 28 256 4 512 3 3</cell><cell>ResNet</cell></row><row><cell cols="4">Task 4 14 14 512 4 512 3 3</cell><cell>ResNet</cell></row><row><cell>Task 5 7</cell><cell cols="3">7 512 4 512 3 3</cell><cell>ResNet</cell></row><row><cell cols="3">Task 6 161 700 1</cell><cell>8 64 5 5 DeepSpeech2</cell></row><row><cell cols="4">Task 7 79 341 32 8 32 5 10 DeepSpeech2</cell></row><row><cell cols="4">little engineering resources for optimizing kernels of lesser</cell></row><row><cell cols="4">importance. When fast algorithms are not available (e.g.,</cell></row><row><cell cols="4">DeepSpeech2), cuDNN and Triton are on par.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="grantName">NVIDIA Graduate Fellowship</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AqavEgx">
					<orgName type="grant-name">NVIDIA Graduate Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>; pt [i , j ] , pf [i , j ] = ( true , false ) if x [i , j ] &lt; 5 ; pt [i , j ] , pf [i , j ] = ( false , true ) if x [i , j ] &gt;= 5 % pt , % pf = icmpp slt %x , 5 @ % pt % x1 = add %y , 1 @ % pf % x2 = sub %y , 1 ; merge values from different predicates</p><p>Listing 6. Tile-Level Predication in Triton-IR We propose to solve this issue through the use of the Predicated SSA (PSSA) form <ref type="bibr" target="#b7">[8]</ref> and ? -functions <ref type="bibr" target="#b39">[39]</ref>. This requires the addition of two instruction classes (see <ref type="bibr">Listing 6)</ref> to Triton-IR:</p><p>? The cmpp instructions <ref type="bibr" target="#b7">[8]</ref> are similar to usual comparison (cmp) instructions, except for the fact that they return two opposite predicates instead of one. ? The psi instruction merges instructions from different streams of predicated instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Triton-JIT Compiler</head><p>The goal of Triton-JIT is to simplify and compile Triton-IR programs into efficient machine code, via a set of machineindependent (Section 5.1) and machine-dependent (Section 5.2) passes backed by an auto-tuning engine (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Machine-Independent Passes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Pre-Fetching</head><p>Tile-level memory operation inside loops can be problematic, as they may induce severe latency that cannot be hidden in the absence of enough independent instructions. It is however possible to mitigate this problem in Triton-IR directly by detecting loops and adding adequate prefetching code where necessary (See Listing 7). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-scale Machine Learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A three-dimensional approach to parallel matrix multiplication</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Gustavson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Palkar</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.395.0575</idno>
		<ptr target="https://doi.org/10.1147/rd.395.0575" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="575" to="582" />
			<date type="published" when="1995-09">1995. Sep. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linxi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Billy</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libby</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Seetapun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02595</idno>
		<ptr target="http://arxiv.org/abs/1512.02595" />
		<title level="m">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Opsila: an advanced SIMD for numerical analysis and signal processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Auguin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Larbey</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Defensive Loop Tiling for Shared Cache</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO) (CGO &apos;13)</title>
		<meeting>the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO) (CGO &apos;13)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parameterized Tiling Revisited</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Muthu Manikandan Baskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Hartono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Tavarageri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><surname>Sadayappan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772954.1772983</idno>
		<ptr target="https://doi.org/10.1145/1772954.1772983" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization (CGO &apos;10)</title>
		<meeting>the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization (CGO &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple and Efficient Construction of Static Single Assignment Form</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Buchwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Leissa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Mallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Zwinkau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Compiler Construction (CC&apos;13</title>
		<meeting>the 22Nd International Conference on Compiler Construction (CC&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="102" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lori</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beth</forename><forename type="middle">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferrante</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicated Static Single Assignment</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PACT 1999 Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the PACT 1999 Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TVM: End-to-End Optimization Stack for Deep Learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno>CoRR abs/1802.04799</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">TVM: End-to-End Optimization Stack for Deep Learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno>CoRR abs/1802.04799</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Memory Access Coalescing: A Technique for Eliminating Redundant Memory Accesses</title>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Jinturkar</surname></persName>
		</author>
		<idno type="DOI">10.1145/178243.178259</idno>
		<ptr target="https://doi.org/10.1145/178243.178259" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1994 Conference on Programming Language Design and Implementation (PLDI &apos;94)</title>
		<meeting>the ACM SIGPLAN 1994 Conference on Programming Language Design and Implementation (PLDI &apos;94)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Persistent RNNs: Stashing Recurrent Weights On-Chip</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diesel: DSL for Linear Algebra and Neural Net Computations on GPUs</title>
		<author>
			<persName><forename type="first">Venmugil</forename><surname>Elango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norm</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahesh</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hariharan</forename><surname>Sandanagobalane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Grover</surname></persName>
		</author>
		<idno type="DOI">10.1145/3211346.3211354</idno>
		<ptr target="https://doi.org/10.1145/3211346.3211354" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2Nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages (MAPL 2018)</title>
		<meeting>the 2Nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages (MAPL 2018)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithms for Compile-time Memory Optimization</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Gergov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA &apos;99)</title>
		<meeting>the Tenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA &apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<ptr target="https://www.tensorflow.org/performance/xla/" />
	</analytic>
	<monogr>
		<title level="j">Tensorflow XLA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Google Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gpu kernels for block-sparse weights</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Radford Ans Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno>CoRR abs/1711.09224</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Code generation in the polytope model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Griebl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lengauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wetzel</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACT.1998.727179</idno>
		<ptr target="https://doi.org/10.1109/PACT.1998.727179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings. 1998 International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>1998 International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="106" to="111" />
		</imprint>
	</monogr>
	<note>Cat. No.98EX192</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<ptr target="http://arxiv.org/abs/1704.04861" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">BLISlab: A Sandbox for Optimizing GEMM</title>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Van De Geijn</surname></persName>
		</author>
		<idno>TR-16-13</idno>
		<ptr target="http://arxiv.org/pdf/1609.00076v1.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">80</biblScope>
		</imprint>
		<respStmt>
			<orgName>The University of Texas at Austin, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Intel</surname></persName>
		</author>
		<ptr target="https://www.intel.ai/reintroducing-plaidml" />
		<title level="m">PlaidML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">CUTLASS: Fast Linear Algebra in CUDA C++</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duane</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<ptr target="https://devblogs.nvidia.com/cutlass-linear-algebra-cuda/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ima-geNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fast Algorithms for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fixed Point Quantization of Deep Convolutional Networks</title>
		<author>
			<persName><forename type="first">Darryl</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sreekanth Annapureddy</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045390.3045690" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><surname>Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mcintosh-Smith</surname></persName>
		</author>
		<title level="m">Benchmarking the NVIDIA V100 GPU and Tensor Cores</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Embedded Binarized Neural Networks</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surat</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3108009.3108031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks</title>
		<meeting>the 2017 International Conference on Embedded Wireless Systems and Networks<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Junction Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="168" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Peephole Optimization</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Mckeeman</surname></persName>
		</author>
		<idno type="DOI">10.1145/364995.365000</idno>
		<ptr target="https://doi.org/10.1145/364995.365000" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="443" to="444" />
			<date type="published" when="1965-07">1965. July 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TurboTiling: Leveraging Prefetching to Boost Performance of Tiled Codes</title>
		<author>
			<persName><forename type="first">Sanyam</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishad</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pen-Chung</forename><surname>Yew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Supercomputing (ICS &apos;16)</title>
		<meeting>the 2016 International Conference on Supercomputing (ICS &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Article 38, 12 pages</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Undersander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<idno>Networks. CoRR abs/1711.02782</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Block-Sparse Recurrent Neural</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accelerating GPU Kernels for Dense Linear Algebra</title>
		<author>
			<persName><forename type="first">Rajib</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanimire</forename><surname>Tomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on High Performance Computing for Computational Science (VECPAR&apos;10)</title>
		<meeting>the 9th International Conference on High Performance Computing for Computational Science (VECPAR&apos;10)<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scalable Parallel Programming with CUDA</title>
		<author>
			<persName><forename type="first">John</forename><surname>Nickolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
		<idno type="DOI">10.1145/1365490.1365500</idno>
		<ptr target="https://doi.org/10.1145/1365490.1365500" />
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="40" to="53" />
			<date type="published" when="2008-03">2008. March 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">CLBlast: A Tuned OpenCL BLAS Library</title>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Nugteren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05249</idno>
		<ptr target="http://arxiv.org/abs/1705.05249" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Travis</forename><surname>Oliphant</surname></persName>
		</author>
		<ptr target="http://www.numpy.org/" />
		<title level="m">NumPy: A guide to NumPy. USA: Trelgol Publishing</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note>Online; accessed &lt;today&gt;</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch/pytorch" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI &apos;13)</title>
		<meeting>the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Glow: Graph Lowering Compiler Techniques for Neural Networks</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleem</forename><surname>Abdulrasool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Summer</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Dzhabarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hegeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Levenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Olesen</surname></persName>
		</author>
		<idno>CoRR abs/1805.00907</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Jongsoo Park, Artem Rakhov, and Misha Smelyanskiy</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient Static Single Assignment Form for Predication</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Stoutchinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>De Ferriere</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=563998.564022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 34th Annual ACM/IEEE International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="172" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Input-aware Auto-tuning of Compute-bound HPC Kernels</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;17)</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;17)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Didem</forename><surname>Unat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammed</forename><surname>Nufail Farooqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burak</forename><surname>Bastem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Michelogiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Almgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<title level="m">TiDA: High-Level Programming Abstractions for Data Locality Management</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BLIS: A Framework for Rapidly Instantiating BLAS Functionality</title>
		<author>
			<persName><forename type="first">G</forename><surname>Field</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Van Zee</surname></persName>
		</author>
		<author>
			<persName><surname>Van De Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015-06">2015. June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno>CoRR abs/1802.04730</idno>
		<title level="m">Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Clint</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Petitet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Empirical Optimization of Software and the ATLAS Project</title>
		<imprint>
			<date type="published" when="2000">2000. 2000. 2001</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Roofline: An Insightful Visual Performance Model for Multicore Architectures</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1145/1498765.1498785</idno>
		<ptr target="https://doi.org/10.1145/1498765.1498785" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2009-04">2009. April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>CoRR abs/1711.08141</idno>
		<title level="m">Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting Loop Tiling for Datacenters: Live and Let Live</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Supercomputing (ICS &apos;18)</title>
		<meeting>the 2018 International Conference on Supercomputing (ICS &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="328" to="340" />
		</imprint>
	</monogr>
	<note>Jingling Xue, and Xiaobing Feng</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
