<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X</title>
				<funder ref="#_EwEnX9X">
					<orgName type="full">Natural Science Foundation of China</orgName>
					<orgName type="abbreviated">NSFC</orgName>
				</funder>
				<funder>
					<orgName type="full">Tsinghua</orgName>
				</funder>
				<funder ref="#_p5qTBDq">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-30">30 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qinkai</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yufei</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Teng</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Tsinghua</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhipu</forename><surname>Ai ?</surname></persName>
						</author>
						<title level="a" type="main">CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-30">30 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.17568v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Model Properties Dataset Evaluation Open Multilingual # Params Source Languages Size Multilingual Evaluation Translation Benchmark Codex (Chen et al.</term>
					<term>2021) 12B Collected Python Code: 159GB HumanEval</term>
					<term>APPS AlphaCode (Li et al.</term>
					<term>2022) 41B Collected 12 langs Code: 715.1GB HumanEval</term>
					<term>APPS CodeContest PaLM-Coder (Chowdhery et al.</term>
					<term>2022) 8B</term>
					<term>62B</term>
					<term>540B Collected Multiple</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large pre-trained code generation models, such as OpenAI Codex, can generate syntax-and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep.</p><p>2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given the description of a human intent, such as "write a factorial function", can the machine automatically generate an executable program that addresses this need? This is the problem of automatic program writing that has been explored since the early days of computer science in the 1960s <ref type="bibr" target="#b37">(Waldinger and Lee, 1969;</ref><ref type="bibr" target="#b31">Summers, 1977)</ref>. From LISP-based pioneering deductive synthesis approaches <ref type="bibr" target="#b37">(Waldinger and Lee, 1969;</ref><ref type="bibr" target="#b31">Summers, 1977)</ref> to modern program synthesis systems <ref type="bibr" target="#b30">(Solar-Lezama, 2008;</ref><ref type="bibr" target="#b23">Polozov and Gulwani, 2015)</ref>, to end-to-end code generation via deep neural networks <ref type="bibr" target="#b19">(Mou et al., 2015;</ref><ref type="bibr" target="#b33">Svyatkovskiy et al., 2020;</ref><ref type="bibr" target="#b32">Sun et al., 2020)</ref>, tremendous efforts have been made to enable machines to automatically write correct programs as part of the quest to artificial general intelligence.</p><p>By treating programs as language sequences, neural sequential architectures, such as recurrent neural networks and transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>, can be naturally applied to code generation. In fact, transformer-based techniques <ref type="bibr" target="#b33">(Svyatkovskiy et al., 2020;</ref><ref type="bibr" target="#b32">Sun et al., 2020)</ref> have shown the potential of automatic program writing by starting to generate code that is both syntactically correct and consistent in 2020. This progress is significantly furthered when large language models (transformers with billions of parameters) meet the massive open-sourced code data.</p><p>Notably, the OpenAI Codex <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> model (Python only) with 12 billion (12B) parameters pioneered and demonstrated the potential of large code generation models pre-trained on billions lines of public code. By using the generative pre-training (GPT) strategy, Codex can solve introductorylevel programming problems in Python with a high probability. Research studies <ref type="bibr" target="#b48">(Ziegler et al., 2022)</ref> also show that 88% of users of GitHub Copilot-a paid service powered by Codex-feel more productive when coding with it. Since then, large pre-trained code models have been extensively developed, including DeepMind AlphaCode <ref type="bibr" target="#b17">(Li et al., 2022)</ref>, Salesforce CodeGen <ref type="bibr">(Nijkamp et al., 2022)</ref>, Meta InCoder <ref type="bibr" target="#b10">(Fried et al., 2022)</ref>, and Google PaLM-Coder-540B <ref type="bibr" target="#b7">(Chowdhery et al., 2022)</ref>.</p><p>In this work, we present CodeGeeX, a multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of 23 programming languages. It was trained on more than 850 billion tokens on a cluster of 1,536 Ascend 910 AI Processors between April and June 2022, and was publicly released in <ref type="bibr">Sep. 2022 (Cf. the GitHub repo)</ref>. CodeGeeX has the following properties. First, different from Codex in <ref type="bibr" target="#b6">Chen et al. (2021)</ref>, both CodeGeeX-the model itself-and how such scale of code models can be pre-trained are open-sourced, facilitating the understanding and advances in pre-trained code generation models. CodeGeeX also supports cross-platform inference on both Ascend and NVIDIA GPUs. Second, in addition to code generation and code completion as Codex and others, CodeGeeX supports the tasks of code explanation and code translation between language pairs (Cf. Figure <ref type="figure" target="#fig_0">1 (a)</ref>). Third, it offers consistent performance advantages over well-known multilingual code generation models of the similar scale, including CodeGen-16B, GPT-NeoX-20B, InCode-6.7B, and GPT-J-6B (Cf. Figure <ref type="figure" target="#fig_0">1</ref> (b) and (c)).</p><p>We also build the free CodeGeeX extensions in several IDEs, currently including Visual Studio <ref type="bibr">Code, JetBrains, and Tencent Cloud Studio (a Web IDE)</ref>. It supports several different modescode completion, function-level generation, code translation, code explanation, and customizable prompting-to help users' programming tasks in real time. Since its release, there are tens of thousands of daily active users, each of which on average makes 250+ API calls per weekday. As of this writing, the CodeGeeX model generates 4.7 billion tokens per week. Our user survey suggests that 83.4% of users feel the CodeGeeX extensions improve their programming efficiency.</p><p>Finally, we develop the HumanEval-X benchmark for evaluating multilingual code models as 1) HumanEval <ref type="bibr" target="#b6">(Chen et al., 2021)</ref>-developed by OpenAI for evaluating Codex-and other bench-Table <ref type="table">1</ref>: Large pre-trained language models related to programming languages in the literature. marks <ref type="bibr" target="#b1">(Austin et al., 2021;</ref><ref type="bibr" target="#b13">Hendrycks et al., 2021;</ref><ref type="bibr">Nijkamp et al., 2022)</ref> only consist of programming problems in a single language and 2) existing multilingual datasets <ref type="bibr" target="#b28">(Ren et al., 2020;</ref><ref type="bibr" target="#b18">Lu et al., 2021;</ref><ref type="bibr" target="#b47">Zhu et al., 2022)</ref> use string similarity metrics like BLEU <ref type="bibr" target="#b21">(Papineni et al., 2002)</ref> for evaluation rather than really verify the functional correctness of generated code. Specifically, for each problemdefined only for Python-in HumanEval, we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go. In total, HumanEval-X covers 820 hand-written problemsolution pairs (164 problems, each having solutions in 5 languages). Importantly, HumanEval-X support the evaluation of both code generation and code translation between different languages.</p><p>The contributions of this work can be summarized as follows:</p><p>? We develop and release CodeGeeX, a 13B pre-trained 23-language code generation model that demonstrates consistent outperformance on code generation and translation over its multilingual baselines of the same scale.</p><p>? We build the CodeGeeX extensions on VS Code<ref type="foot" target="#foot_0">4</ref> , JebBrains<ref type="foot" target="#foot_1">5</ref> , and Tencent Cloud Studio. Compared to Copilot, it supports more diverse functions, including code completion, generation, translation, and explanation. According to the user survey, CodeGeeX can improve the coding efficiency for 83.4% of its users.</p><p>? We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness, facilitating the understanding and development of pre-trained (multilingual) code models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The CodeGeeX Model</head><p>CodeGeeX is a multilingual code generation model with 13 billion (13B) parameters, pre-trained on a large code corpus of 23 programming languages. As of June 22, 2022, CodeGeeX has been trained on more than 850 billion tokens on a cluster of 1,536 Ascend 910 AI Processors for over two months.</p><p>We introduce the CodeGeeX model and its design choices. The consensus reality is that it is computationally unaffordable to test different architectural designs for large pre-trained models <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr" target="#b7">Chowdhery et al., 2022;</ref><ref type="bibr" target="#b46">Zhang et al., 2022;</ref><ref type="bibr" target="#b44">Zeng et al., 2022)</ref>, though they define the inductive bias of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CodeGeeX's Architecture</head><p>The Transformer Backbone. Similar to recent pre-trained models, such as GPT-3 <ref type="bibr" target="#b5">(Brown et al., 2020)</ref>, PaLM <ref type="bibr" target="#b7">(Chowdhery et al., 2022)</ref>, and Codex <ref type="bibr" target="#b6">(Chen et al., 2021)</ref>, CodeGeeX follows the generative pre-training (GPT) architecture <ref type="bibr" target="#b25">(Radford et al., 2018)</ref> with the decoder-only style for autoregressive (programming) language modeling. The core architecture of CodeGeeX is a 39-layer transformer decoder. In each transformer layer (in Figure <ref type="figure" target="#fig_1">2</ref>), we apply a multi-head self-attention mechanism <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> followed by MLP layers, together with layer normalization <ref type="bibr" target="#b2">(Ba et al., 2016)</ref> and residual connection <ref type="bibr" target="#b12">(He et al., 2016)</ref>. We use an approximation of GELU (Gaussian Linear Units) operation <ref type="bibr" target="#b14">(Hendrycks and Gimpel, 2016)</ref>, namely FastGELU, which is more efficient under the Ascend 910 AI Processor:</p><formula xml:id="formula_0">FastGELU(Xi) = Xi 1 + exp(-1.702 * |Xi|) * exp(0.851 * (Xi -|Xi|))</formula><p>(1) Generative Pre-Training Objective. By adopting the GPT paradigm <ref type="bibr" target="#b26">(Radford et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2021)</ref>, we train the model on a large amount of unlabeled code data. The principle is to iteratively take code tokens as input, predict the next token, and compare it with the ground truth. Specifically, for any input sequence {x 1 , x 2 , ..., x n } of length n, the output of CodeGeeX is a probability distribution of the next token P(x n+1 |x 1 , x 2 , ..., x n , ?) = p n+1 ? [0, 1] 1?v , where ? represents all parameters of the model and v is the vocabulary size. By comparing it with the real distribution, i.e., a one-hot vector y n+1 ? {0, 1} 1?v of the ground-truth token, we can optimize the cumulative cross-entropy loss:</p><formula xml:id="formula_1">L = - N -1 n=1 yn+1 log P(xn+1|x1, x2, ..., xn, ?) (2)</formula><p>The Top Query Layer and Decoding. The original GPT model uses a pooler function to obtain the final output. We use an extra query layer <ref type="bibr">(Zeng et al., 2021)</ref> on top of all other transformer layers to obtain the final embedding through attention. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the input of the top query layer replaces the query input X in by the query embedding of position n + 1. The final output is multiplied by the transpose of word embedding matrix to get the output probability. For decoding strategies, CodeGeeX supports greedy, temperature sampling, top-k sampling, top-p sampling, and beam search. Finally, detokenization will turn the selected token ID into an actual word.</p><p>Figure <ref type="figure">3</ref>: Language distribution and tags of CodeGeeX's data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-Training Setup</head><p>Code Corpus. The training corpus contains two parts. The first part is from open source code datasets, the Pile <ref type="bibr" target="#b11">(Gao et al., 2020)</ref> and CodeParrot<ref type="foot" target="#foot_2">6</ref> . The Pile contains a subset of public repositories with more than 100 stars on GitHub, from which we select files of 23 popular programming languages including C++, Python, Java, JavaScript, C, Go, and so on. We identify the programming language of each file based on its suffix and the major language of the repository it belongs to. CodeParrot is another public Python dataset from BigQuery. The second part is supplementary data of Python, Java, and C++ directly scraped from GitHub public repositories that do not appear in the first part. We choose repositories that have at least one star and a total size within 10MB, then we filter out files that: 1) have more than 100 characters per line on average, 2) are automatically generated, 3) have a ratio of alphabet less than 40%, 4) are bigger than 100KB or smaller than 1KB. We format Python code according to the PEP8 standards.</p><p>Figure <ref type="figure">3</ref> shows the composition of the 158B-token training data, containing 23 programming languages. We divide the training data into segments of equal length. To help the model distinguish between multiple languages, we add a language-specific tag before each segment in the form of [Comment sign]language: [LANG], e.g., # language: Python.</p><p>Tokenization. The first step is to convert code snippets into numerical vectors. Considering that 1) there is a large number of natural language comments in code data, 2) the naming of variables, functions, and classes are often meaningful words, we treat code data the same as text data and apply the GPT-2 tokenizer <ref type="bibr" target="#b26">(Radford et al., 2019)</ref>. It is a BPE (Byte Pair Encoding) <ref type="bibr" target="#b29">(Sennrich et al., 2015)</ref> tokenizer that deals with the open-vocabulary problem using a fixed-size vocabulary with variable-length characters. The initial vocabulary size is 50,000, we encode multiple whitespaces as extra tokens following <ref type="bibr" target="#b6">Chen et al. (2021)</ref> to increase the encoding efficiency. Specifically, L whitespaces are represented by &lt;|extratoken_X|&gt;, where X=8+L. Since the vocabulary contains tokens from various natural languages, it allows CodeGeeX to process tokens in languages other than English, like Chinese, French, Russia, Japanese and more. The final vocabulary size is v = 52, 224.</p><p>After tokenization, any code snippet or text description can be transformed into a vector of integers. More details can be found in Appendix A.2.</p><p>The Input Word and Positional Embeddings. Given the tokens, the next step is to associate each token with a word embedding. By looking up the token ID in a word embedding matrix W word ? R v?h , where h = 5120 is the hidden size, a learnable embedding x word ? R h is obtained for each token. To capture positional information, we also adopt learnable positional embedding that maps the current position ID to a learnable embedding x pos ? R h , from W pos ? R nmax?h , where n max = 2048 is the maximum sequence length. Then, two embeddings are added to obtain the input embeddings x in = x word + x pos for the model. Finally, the entire sequence can be turned into input embeddings X in ? R n?h , where n is the input sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CodeGeeX Training</head><p>Parallel Training on Ascend 910. CodeGeeX was trained on a cluster of the Ascend 910 AI processors (32GB) with Mindspore (v1.7.0). We faced and addressed numerous unknown technical and engineering challenges during pre-training, as Ascend and Mindspore are relatively new compared to NVIDIA GPUs and PyTorch/TensorFlow. The entire pre-training process takes two months on 192 nodes with 1,536 AI processors, during which the model consumes 850B tokens, equivalent to 5+ epochs (213,000 steps). Detailed configurations can be found in Table <ref type="table" target="#tab_0">2</ref>. Table <ref type="table" target="#tab_1">3</ref> shows the comparison of training efficiency before and after our optimization. The overall efficiency is measured by trained tokens per day. We observe that the efficiency per processor was improved 3? compared to the non-optimized implementation and the overall token throughput of 1,536 GPUs was improved by 224%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Fast Inference</head><p>To serve the pre-trained CodeGeeX, we implement a pure PyTorch version of CodeGeeX that supports inference on NVIDIA GPUs. To achieve fast and memory-efficient inference, we apply both quantization and acceleration techniques to the pre-trained CodeGeeX.</p><p>Quantization. We apply post-training quantization techniques to decrease memory consumption of CodeGeeX during inference. We transform weights W in all linear transformations from FP16 to INT8 using the common absolute maximum quantization:</p><formula xml:id="formula_2">W q = Round( W ? ), ? = Max(|W |) 2 b-1 -1 (4)</formula><p>where b is the bitwidth and b = 8. ? is the scaling factor. This quantization transform FP16 values in</p><formula xml:id="formula_3">[-Max(|W |), Max(|W |)] to integers between [-127, 127].</formula><p>As in Table <ref type="table" target="#tab_2">4</ref>, the memory consumption of CodeGeeX decreases from ?26.9GB to ?14.7GB (down by 45.4%), allowing CodeGeeX inference on one RTX 3090 GPU. Importantly, Figure <ref type="figure" target="#fig_2">4</ref> shows that the quantization only slightly affects the performance on the code generation task (Cf Section 3.2 for details about HumanEval-X.).  Acceleration. After quantization, we further implement a faster version of CodeGeeX using the NVIDIA FasterTransformer (FastTrans). It supports highly-optimized operations by using layer fusion, GEMM autotuning, and hardware-accelerated functions. For INT8 quantized version, we also implement a custom kernel that accelerates the mixed precision matrix multiplication between INT8 weights and FP16 activation vectors. According to Table <ref type="table" target="#tab_2">4</ref>, the INT8 quantization plus FastTrans implementation achieves the fastest inference speed and the lowest GPU memory consumption on a single GPU. The inference time per token is within 13ms (1.61 seconds / 128 tokens). We also compare the inference speed with implementations in LLM.int() <ref type="bibr" target="#b8">(Dettmers et al., 2022)</ref> and Oneflow <ref type="bibr" target="#b43">(Yuan et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The HumanEval-X Benchmark</head><p>We develop the HumanEval-X benchmark<ref type="foot" target="#foot_3">7</ref> for evaluating multilingual code models. There are 164 code problems defined for five major languages: C++, Java, JavaScript, Go, and Python, resulting in 164?5=820 problem-solution pairs. For each problem, it supports both code generation and code translation. Examples of the problems can be found in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HumanEval-X: A Multilingual Benchmark</head><p>HumanEval <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> has been developed to evaluate Codex by OpenAI. However, similar to MBPP <ref type="bibr" target="#b1">(Austin et al., 2021)</ref> and APPS <ref type="bibr" target="#b13">(Hendrycks et al., 2021)</ref>, it only consists of handcrafted programming problems in Python, thus cannot be directly applied to systematically evaluate the performance of multilingual code generation. Generation uses declaration and docstring as input to generate the solution. Translation uses declaration in both languages and solution in source language as input, to generate solution in the target language (docstring is not used to prevent models from directly solving the problem).</p><p>To this end, we propose to develop a multilingual variant of HumanEval, referred to as HumanEval-X. This is not trivial. For each problem, defined only for Python, in HumanEval, we manually rewrite its prompt, canonical solution, and test cases in the other four languages-C++, Java, JavaScript, and Go. Altogether, we have 820 problem-solution pairs in total in HumanEval-X, each comprising the following parts:</p><p>? task_id: programming language and numerical problem id, e.g., Java/0 represents the 0-th problem in Java;</p><p>? declaration: function declaration including necessary libraries or packages;</p><p>? docstring: description that specifies the functionality and example input/output;</p><p>? prompt: function declaration plus docstring;</p><p>? canonical_solution: a verified solution to the problem;</p><p>? test: test program including test cases.</p><p>Each problem-solution pair in HumanEval-X supports both code generation code translation. An illustrative example is shown in Figure <ref type="figure" target="#fig_3">5</ref>. We take the following efforts to make sure that the rewritten code conforms to the programming style of the corresponding language. First, we use the customary naming styles, like CamelCase in Java, Go, and JavaScript, and snake_case in C++. Second, we put the docstrings before the function declaration in Java, JavaScript, C++, and Go. Symbols in docstrings are modified, e.g., single quotes are replaced by double quotes in some languages, and keywords like True/False, None are also replaced. Third, we refine test cases according to language-specific behaviors, rather than forcing the programs to return the same result for different languages. For example, when converting an integer to a binary string, Python method bin adds a prefix "0b" before the string while Java method Integer.toBinaryString does not, so we remove such prefix in Java test cases. Last, we also take care of the rounding function. In Python, round converts half to the closest even number, unlike in other languages. Thus, we change the test cases to match the rounding implementations in each language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HumanEval-X: Tasks</head><p>In HumanEval-X, we evaluate two tasks: code generation and code translation.</p><p>Code Generation. The task of code generation takes a problem description (e.g., "write a factorial function") as input and generates the solution in the selected languages (Cf Fig- <ref type="table">ure 1 (a)</ref>). Specifically, the model takes in the prompt including declaration and docstrings, and generates the implementation of the function. Note that HumanEval-X uses the same problem set for all the five languages, thus, for solving each problem, it supports either one single language or multiple languages simultaneously.</p><p>Code Translation. The task of code translation takes the implementation of a problem in the source language and generates its counterpart implementation in the target language. Precisely, its input includes the function declaration and a canonical solution in the source language (e.g., Python).</p><p>The model should translate the solution to the target language. Adding declaration in the target language restricts function names and variable types, making the evaluation easier, especially under the zero-shot setting. To prevent the models from directly solving the problem rather than translating, we do not include the docstrings. HumanEval-X supports the translation between all pairs of 5 languages, that is, in total 20 source-target language pairs. Metric. For both tasks, we use test cases to evaluate the exact functional correctness of the generated code, measuring the performance with pass@k <ref type="bibr" target="#b16">(Kulal et al., 2019)</ref>, making it real-world useful and also completely different from the string similarity metrics like BLEU <ref type="bibr" target="#b21">(Papineni et al., 2002), and</ref><ref type="bibr">CodeBLEU (Ren et al., 2020;</ref><ref type="bibr" target="#b18">Lu et al., 2021;</ref><ref type="bibr" target="#b47">Zhu et al., 2022)</ref>. Specifically, we use the unbiased method to estimate pass@k (Chen et al., 2021):</p><formula xml:id="formula_4">pass@k := E[1 - n-c k n k ], n = 200, k ? {1, 10, 100}<label>(5)</label></formula><p>where n is the total number of generation (n=200 in this work), k is the sampling budget (typically k ? {1, 10, 100}) and c is the number of samples that pass all test cases. We average over the problem set to get the expectation. 1 -</p><formula xml:id="formula_5">n-c k n k</formula><p>is the estimated pass@k for a single problem, and E is the expectation of pass@k over all problems. In practice, we average single-problem pass@k among all test-set problems to get the expectation.</p><p>Multilingual Metric with Budget Allocation. Unlike mono-lingual models, multilingual code models can solve problems by allocating generation budgets to various languages to increase the sampling diversity and improve the solve rate. Given a budget k, we can distribute part of it n i to each language with the assignment ? = (n1, n2, ..., nm),</p><formula xml:id="formula_6">m i=1 ni = k,<label>(6)</label></formula><p>where n i is the generation budget assigned to language i, m is the number of candidate languages. Under an assignment ? = (n 1 , ...n m ), for a problem p, the pass@k ? can be estimated by:</p><formula xml:id="formula_7">pass@k? = E[1 - m i=1 n-c i n i n n i ],<label>(7)</label></formula><p>where n is the total number of generation, n i is the sampling budget and c i is the number of samples that pass all test cases for language i. We show in Section 4.3 that multilingual models can benefit from budget allocation strategies and have higher solve rate than using any single language.</p><p>4 Evaluating CodeGeeX on HumanEval-X</p><p>We evaluate CodeGeeX for the code generation and translation tasks on the multilingual benchmark HumanEval-X. By inheriting from HumanEval, the HumanEval-X results on Python are equivalent to the evaluation on HumanEval. Baselines. We compare CodeGeeX with five competitive open-source baselines: GPT-J-6B <ref type="bibr" target="#b38">(Wang and Komatsuzaki, 2021)</ref>, GPT-NeoX-20B <ref type="bibr" target="#b3">(Black et al., 2022)</ref>, InCoder-6.7B <ref type="bibr" target="#b10">(Fried et al., 2022)</ref>, and CodeGen-Multi-6B/16B <ref type="bibr">(Nijkamp et al., 2022)</ref>. These models are all trained on multilingual code data, but is previously only evaluated in HumanEval (Python). And they are closer to the scale of CodeGeeX or even larger, while smaller models in the literature are ignored. For all baselines, we use the versions available on HuggingFace <ref type="bibr" target="#b41">(Wolf et al., 2019)</ref>. We follow the experimental settings of HumanEval-X in Section 3.2. Further details can be found in Appendix A.3.</p><p>Environment. Experiments are conducted by using the NVIDIA A100-SXM-40GB GPUs with Linux system. We design a distributed framework for generation based on ZeroMQ to balance GPU loads. All generated codes are tested in language-specific environments with necessary packages installed.</p><p>Decoding Strategy. We use temperature sampling (t ? [0, 1]) and nucleus sampling (p ? [0, 1]) for generation. For CodeGeeX in code generation, we use t = 0.2, p = 0.95 for pass@1 and t = 0.8, p = 0.95 for pass@10 and pass@100 (except for Go and JavaScript, where p = 0.9). For CodeGeeX in code translation, we use t = 0.2, p = 0.95 for pass@1 and t = 0.8, p = 0.95 for pass@10 and pass@100 for all language pairs. For the fine-tuned CodeGeeX-13B-FT used for code translation, we use p = 0.95. For all baselines in both tasks, we use t = 0.2, p = 0.95 for pass@1, t = 0.8, p = 0.95 for pass@10 and pass@100. All pass@k, k ? {1, 10, 100} results are estimated with n = 200. The maximum number of generated tokens is set to 1024 for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results of Code Generation and Translation</head><p>Multilingual Code Generation. Table <ref type="table" target="#tab_3">5</ref> and Figure <ref type="figure">6</ref> report the code generation results in terms of the pass@k, k ? {1, 10, 100} for CodeGeeX and five baseline models on five programming languages. CodeGeeX significantly outperforms models trained with mixed corpora (GPT-J-6B and GPT-NeoX-20B), even though GPT-NeoX-20B has much more parameters. For models trained on codes, CodeGeeX outperforms those with smaller scales (InCoder-6.7B, CodeGen-Multi-6B) by a large margin, and is competitive with CodeGen-Multi-16B with a larger scale. CodeGeeX achieves the best average performance among all models, even slightly better than the larger CodeGen-Multi-16B in all three metrics (0.37%?1.67% improvements). When considering individual languages, models have preferences highly related to the training set distribution. For example, the best language  Figure <ref type="figure" target="#fig_4">7</ref> shows the proportions of running results of four models. For all languages, the most common error type is wrong answer, with ratio ranging from 0.44 to 0.75 except for Go, showing that code generation models at the current stage mainly suffer from incorrect code logic rather than semantics. Go samples have a high syntax error rate, which may be due to Go having strict restrictions on syntax and forbidding unused variables and imports, failing to compile many logically correct codes.</p><p>CodeGeeX has less rate to generate code that produces runtime, syntax, or semantic errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Multilingual Pre-Training Helps Problem Solving</head><p>We perform studies to understand whether and how multilingual pre-training can benefit problemsolving of CodeGeeX.</p><p>Exploration vs. Exploitation under Fixed Budgets. Given a fixed budget k, pass@k evaluates the ability of models generating at least 1 correct solution under k generations. Previous works <ref type="bibr" target="#b6">(Chen et al., 2021;</ref><ref type="bibr" target="#b17">Li et al., 2022)</ref> have already discovered that there's a trade-off between exploration and exploitation: When the budget is small, it is better to use a low temperature to ensure accuracy on easy problems. When the budget is large, instead, adjusting a higher temperature is vital, as it makes the model more likely to find at least one solution for difficult problems.</p><p>Pass Rate Distribution vs. Languages. Unlike monolingual models, multilingual models can solve problems using various programming languages. In Figure <ref type="figure" target="#fig_5">8</ref>, we observe that the pass rate distribution of problems against different languages are diverse. This inspires us to use budget allocation methods to help improve the diversity of the generated solutions.</p><p>Budget Allocation Strategies. We compare three basic strategies: Best Single chooses a single language with the best performance; Uniform allocates the budget uniformly; Weighted allocates the budget to multiple languages based on their proportions in the training corpus (detailed weights can be found in Appendix Table <ref type="table" target="#tab_6">9</ref>). Table <ref type="table" target="#tab_4">7</ref> illustrates how budget allocation improves the multilingual generation. Both Uniform and Weighted outperform Best Single by promoting a more diverse generation, which gives a higher chance of solving problems. Weighted is slightly better due to the prior knowledge on the model. For model-wise comparison, CodeGeeX shows up a decent advantage over other baselines in both strategies, which suggests that it might have a more diverse solution set under multiple languages. Programming languages are created with a specific purpose and unique design; in real-world scenarios, multilingual models might take this advantage for certain tasks. Negative Correlations in Pair-Language Translation. When evaluating the translation ability in HumanEval-X, an interesting observation is that the performance of A-to-B and B-to-A are usually negatively-correlated, shown in Figure <ref type="figure" target="#fig_6">9</ref>. Such asymmetry suggests that multilingual code generation models may have imbalanced focus on source and target languages during code translation. We provide two possible explanations. First, language distributions in training corpus differ a lot, resulting in different level of generation ability. For example, the ratio of Python is 26.6% (vs. Go 4.7%) in CodeGeeX training corpus, and average pass@100 of Others-to-Python reaches ~90% (vs. Others-to-Go only ~50%). Second, some languages are themselves harder to automatically write with syntactic and semantic accuracy due to language-dependent features, affecting translation performance as target languages. For instance, Go, which models translate poorly into, has more constraints on syntax level, forbidding unused variables or imports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The CodeGeeX Tools and Users</head><p>Based on CodeGeeX, we build open-source extensions for IDEs including VS Code, JetBrains and Cloud Studio. The extensions support code generation, completion, translation and explanation, aiming at improving the development efficiency of programmers. As of this writing, CodeGeeX has served tens of thousands of users, with an average of 250+ API calls per active user per weekday. It currently generates 4.7+ billion tokens per week, which has been steadily growing since its release.  We perform a survey on CodeGeeX's user experience from 168 users covering front-end developer, backend developer, full stack engineer, algorithm engineer, students, researcher, and other programmers. Figure <ref type="figure" target="#fig_7">10</ref> illustrates users' profession distribution and the satisfaction score. We evaluate the satisfaction considering five dimensions, "Ease of Use", "Reliability", "Feature", "Visual", "Speed", each scored from 0 to 5. Figure <ref type="figure" target="#fig_7">10</ref> shows that the majority of users have positive experiences with CodeGeeX, especially for researchers and students, while there is still room for improvement for professional developers. This can be interpreted by our training code corpus: open-sourced repositories contain many introductory or research projects, while production codes are often close-sourced. To increase the CodeGeeX's capability in professional domain, these codes are needed in the future.</p><p>We further investigate how multilinguality of CodeGeeX help coding. Figure <ref type="figure" target="#fig_8">11</ref> illustrates how users evaluate the helpfulness of CodeGeeX during development. There are on average over 83.4% of users think CodeGeeX can improve or slightly increase their coding efficiency, especially for mainstream programming languages like Go, C++, Python, C, C#, etc. Note that these well-performing programming languages also appear more frequently in the training data (Figure <ref type="figure">3</ref>), which encourages us to train CodeGeeX on more language-specific data to enhance its capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce CodeGeeX, a 13B pre-trained 23-language code generation model, as well as we build HumanEval-X, to fill the gap of multilingual code generation. CodeGeeX consistently outperforms open-sourced multilingual baselines of the same scale on code generation and translation tasks.</p><p>The extensions built on CodeGeeX bring significant benefits in increasing coding efficiency. The multilinguality of CodeGeeX brings the potential of solving problems with an ubiquitous set of formalized languages. We open sourced CodeGeeX aiming to help researchers and developers to widely take benefit of large pre-trained models for code generation.</p><p>The multilingual ability of CodeGeeX shows the potential of solving problems with a ubiquitous set of formalized languages. Here, we share three of our observations as the future directions.</p><p>First, we find that the model capacity is essential for multilingual programming ability. It is not trivial for the model to benefit from learning multiple languages. Human programmers can abstract the high-level concept of programming, thus learning one language can help them master the others. On the contrary, the model seems to require a large capacity to concurrently store the knowledge of each language. How to help the model extract the most essential knowledge of programming remains a research challenge.</p><p>Second, similar to others, CodeGeeX shows the reasoning potential as a model though its lack of strong generality. We demonstrate that CodeGeeX can solve problems in different languages. However, the pass rate distribution varies a lot among languages, i.e., it is not able to solve the same problem using different languages on occasion. We assume that this could probably be related to some language-specific features (e.g., some problems are easier to solve in Python), or it could be simply due to the appearance of a similar language-specific implementation in training data. Either case, there is a long way to go for the model to have a reliable reasoning ability.</p><p>Third, the few-shot ability of CodeGeeX is worth exploration. Instead of using costly fine-tuning approaches, we may do priming using a few examples and make the model achieve comparable performance. Recent works like chain-of-thought (CoT) prompting <ref type="bibr" target="#b40">(Wei et al., 2022)</ref> have shown impressive results using such an approach, inspiring us to examine CoT in code models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Statistics of Code Corpus</head><p>Table <ref type="table" target="#tab_5">8</ref> summarizes the composition of CodeGeeX's code corpus. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Tokenization of CodeGeeX</head><p>Given a code snippet as in Figure <ref type="figure" target="#fig_9">12</ref>, it is first separated into token pieces by the tokenizer. Then, each token is mapped to an integer according to its ID in the pre-defined dictionary. For example, 4 or 8 whitespaces (one or two indents in Python) are concatenated to &lt;|extratoken_12|&gt; or &lt;|extratoken_16|&gt;, respectively. Note that in Figure Figure <ref type="figure" target="#fig_9">12</ref>, tokens are starting with "_", which represents whitespace and is often used to indicate if the token appears in the middle of a sentence. After tokenization, any code snippet or text description can be transformed into a vector of integers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Details of Budget Allocation Strategies</head><p>We compare three strategies: Best Single, choose a single language with the best performance; Uniform, allocate the budget uniformly; Weighted, allocate the budget to multiple languages based their proportions in the training corpus. Detailed weights can be found in Table <ref type="table" target="#tab_6">9</ref>. The allocation of CodeGen-Multi-16B and InCoder-6.7B are extracted from the training corpora description in the original papers. The allocation of GPT-J-6B/GPT-NeoX-20B are from the number of tokens in the GitHub section of the Pile.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Summary of CodeGeeX. (a): In supported IDEs, users can interact with CodeGeeX by providing prompts. Different models are used to support three tasks: code generation, code translation and code explanation. (b) and (c): In HumanEval and our newly-proposed HumanEval-X, CodeGeeX shows promising multilingual abilities and consistently outperforms other multilingual code generation models.</figDesc><graphic url="image-1.png" coords="2,108.00,72.00,395.98,196.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CodeGeeX's model architecture. CodeGeeX is a code generation model with 13B parameters, consisting of 39-layer left-to-right transformer decoders and a top query layer. It takes text/code tokens as input and outputs the probability of the next token autoregressively.</figDesc><graphic url="image-2.png" coords="4,167.40,255.71,277.20,328.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CodeGeeX vs. its quantized version on code generation of HumanEval-X.</figDesc><graphic url="image-4.png" coords="8,207.00,288.81,198.00,110.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An illustration of code generation and translation tasks in HumanEval-X. Declarations, docstrings, solutions, and test cases are marked with red, green, blue, and purple respectively.Generation uses declaration and docstring as input to generate the solution. Translation uses declaration in both languages and solution in source language as input, to generate solution in the target language (docstring is not used to prevent models from directly solving the problem).</figDesc><graphic url="image-5.png" coords="9,108.00,72.00,396.04,280.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Left: the proportions of running results of four models for each language. Right: the average result ratios across four models, with lines representing minimum and maximum values. For each model and each language, we study 200 samples generated under t = 0.8 and p = 0.95.</figDesc><graphic url="image-7.png" coords="13,128.34,71.99,203.16,212.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: In HumanEval-X, each problem's pass rate varies when generating in different programming languages with CodeGeeX. Left: t = 0.2, p = 0.95; Right: t = 0.8, p = 0.95.</figDesc><graphic url="image-9.png" coords="13,114.68,341.25,190.07,147.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The performance of translating A-to-B is negatively correlated with B-to-A. Such asymmetry indicates that multilingual models still lack of high-level understanding between languages.</figDesc><graphic url="image-11.png" coords="14,108.00,372.90,395.95,89.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Profession vs. satisfaction. Left: Profession distribution. Right: Averaged rating score of CodeGeeX extensions.</figDesc><graphic url="image-12.png" coords="15,113.82,72.00,178.19,104.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Survey on "Has CodeGeeX improved your coding efficiency?". Over 83.4% of users have positive answers.</figDesc><graphic url="image-13.png" coords="15,300.18,76.26,198.00,96.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Illustration of tokenization in CodeGeeX. "_" represents a whitespace, and "&lt;|extratoken_X|&gt;" represents concatenated whitespaces of different lengths.</figDesc><graphic url="image-14.png" coords="22,127.80,100.88,356.41,356.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Training loss of CodeGeeX.Figure 14: HumanEval-X pass rate vs. iteration.</figDesc><graphic url="image-16.png" coords="22,287.44,551.46,195.61,121.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: In HumanEval-X, each problem's pass rate varies when generating in different programming languages. Left: t = 0.2, p = 0.95; Right: t = 0.8, p = 0.95. From top to bottom: InCoder-6.7B, CodeGen-Multi-6B, CodeGen-Multi-16B.</figDesc><graphic url="image-21.png" coords="23,108.00,501.52,198.00,153.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Solutions (Problem 95 in HumanEval-X) translated by CodeGeeX. Prompt and generated codes are separated by the 'Translation' line (added after the generation as an indicator).</figDesc><graphic url="image-24.png" coords="27,127.80,381.23,356.40,257.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Solutions (Problem 109 in HumanEval-X) generated by CodeGeeX. Prompt and generated codes are separated by the 'Generation' line (added after the generation as an indicator).</figDesc><graphic url="image-26.png" coords="28,108.00,406.13,396.00,227.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Solutions (Problem 13 in HumanEval-X) generated by CodeGeeX. Prompt and generated codes are separated by the 'Generation' line (added after the generation as an indicator).</figDesc><graphic url="image-27.png" coords="29,108.00,85.92,396.00,241.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Solutions (Problem 142 in HumanEval-X) generated by CodeGeeX. Prompt and generated codes are separated by the 'Generation' line (added after the generation as an indicator).</figDesc><graphic url="image-28.png" coords="29,108.00,377.67,396.00,313.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Solutions (Problem 33 in HumanEval-X) translated by CodeGeeX. Prompt and generated codes are separated by the 'Translation' line (added after the generation as an indicator).</figDesc><graphic url="image-30.png" coords="30,108.00,450.98,396.00,249.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Examples of CodeGeeX generation with prompts in Chinese, French, Russia and Japanese. Prompt and generated codes are separated by multiple '#'s (added after the generation as an indicator).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-3.png" coords="5,121.86,223.89,368.30,190.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Training configuration of CodeGeeX.To increase training efficiency, we adopt an 8-way model parallel training together with 192-way data parallel training, with ZeRO-2<ref type="bibr" target="#b27">(Rajbhandari et al., 2020)</ref> optimizer enabled to further reduce the memory consumption of optimizer states. Finally, the micro-batch size is 16 per node and the global batch size reaches 3,072. Specifically, we use Adam optimizer<ref type="bibr" target="#b15">(Kingma and Ba, 2014)</ref> to optimize the loss in Equation2. The model weights are under FP16 format, except that we use FP32 for layer-norm and softmax for higher precision and stability. The model takes about 27GB of GPU memory. We start from an initial learning rate 1e-4, and apply a cosine learning rate decay by: During the two-month training, the training loss of CodeGeeX continues to decrease as the training goes on. We evaluate the checkpoints on HumanEval-X code generation task and observe that the performance is continuously increasing. See details in Figures 13 and 14 in Appendix A.3.Training Efficiency Optimization. Over the course of the training, we actively attempted to optimize the Mindspore framework to release the power of Ascend 910. Notably, we adopt the following techniques that significantly improve training efficiency:? Kernel fusion: We fuse several element-wise operators to improve calculation efficiency on Ascend 910, including Bias+LayerNorm, BatchMatmul+Add, FastGeLU+Matmul, Softmax, etc. We also optimize LayerNorm operator to support multi-core calculation. ? Auto Tune optimization: When loading models, Mindspore first compiles them to static computational graphs. It uses the Auto Tune tool to optimize the choice of operators (e.</figDesc><table><row><cell>Category</cell><cell>Parameter</cell><cell>Value</cell></row><row><cell></cell><cell>Framework</cell><cell>Mindspore v1.7.0</cell></row><row><cell></cell><cell>Hardwares</cell><cell>1,536x Ascend 910 AI processors</cell></row><row><cell>Environment</cell><cell>Mem per GPU GPUs per node</cell><cell>32GB 8</cell></row><row><cell></cell><cell>CPUs per node</cell><cell>192</cell></row><row><cell></cell><cell>RAM per node</cell><cell>2048GB</cell></row><row><cell></cell><cell>Model parameters</cell><cell>13B</cell></row><row><cell></cell><cell>Vocabulary size</cell><cell>52224</cell></row><row><cell></cell><cell>Position embedding</cell><cell>Learnable</cell></row><row><cell></cell><cell>Maximum sequence length</cell><cell>2048</cell></row><row><cell></cell><cell>Hidden size h</cell><cell>5120</cell></row><row><cell>Model</cell><cell>Feed-forward size 4h Feed-forward activation</cell><cell>20480 FastGELU</cell></row><row><cell></cell><cell>Layernorm epsilon</cell><cell>1e-5</cell></row><row><cell></cell><cell>Layernorm precision</cell><cell>FP32</cell></row><row><cell></cell><cell>Number of attention heads h n</cell><cell>40</cell></row><row><cell></cell><cell>Attention softmax precision</cell><cell>FP32</cell></row><row><cell></cell><cell>Dropout rate</cell><cell>0.1</cell></row><row><cell></cell><cell>Model parallel size</cell><cell>8</cell></row><row><cell>Parallelism</cell><cell>Data parallel size</cell><cell>192</cell></row><row><cell></cell><cell>Global batch size</cell><cell>3072</cell></row><row><cell></cell><cell>Optimizer</cell><cell>Adam</cell></row><row><cell></cell><cell>Optimizer parameters</cell><cell>? 1 = 0.9, ? 2 = 0.999</cell></row><row><cell></cell><cell>Initial/final learning rate</cell><cell>1e-4/1e-6</cell></row><row><cell></cell><cell>Warm-up step</cell><cell>2000</cell></row><row><cell>Optimization</cell><cell>Decay step</cell><cell>200000</cell></row><row><cell></cell><cell>Learning rate scheduler</cell><cell>cosine decay</cell></row><row><cell></cell><cell>Loss function L</cell><cell>Cross entropy</cell></row><row><cell></cell><cell>Loss scaling</cell><cell>Dynamic</cell></row><row><cell></cell><cell>Loss scaling window</cell><cell>1000</cell></row><row><cell></cell><cell>Trained steps</cell><cell>213000</cell></row></table><note><p>g., matrix multiplication in different dimensions). And it applies graph optimization techniques to deal with operator fusion and constant folding.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Training efficiency (before and after optimization).</figDesc><table><row><cell></cell><cell>Before</cell><cell>After</cell></row><row><cell>Device</cell><cell>Ascend 910</cell><cell>Ascend 910</cell></row><row><cell>#GPUs</cell><cell>1536</cell><cell>1536</cell></row><row><cell>Parallelism</cell><cell cols="2">Data parallel + Model parallel Data parallel + Model parallel</cell></row><row><cell>Sequence length</cell><cell>2048</cell><cell>2048</cell></row><row><cell>Global batch size</cell><cell>2048</cell><cell>3072</cell></row><row><cell>Step time(s)</cell><cell>15s</cell><cell>10s</cell></row><row><cell>Overall efficiency</cell><cell>24.2B tokens/day</cell><cell>54.3B tokens/day</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>GPU memory and inference time of CodeGeeX w/ and w/o quantization on different GPUs and frameworks.</figDesc><table><row><cell>Implementation</cell><cell>GPU</cell><cell>Format</cell><cell cols="10">L=128 Mem (G) Time (s) Mem (G) Time (s) Mem (G) Time (s) Mem (G) Time (s) Mem (G) Time (s) L=256 L=512 L=1024 L=2048</cell></row><row><cell>Pytorch</cell><cell>3090</cell><cell>FP16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>OOM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pytorch</cell><cell>A100</cell><cell>FP16</cell><cell>26.9</cell><cell>3.66</cell><cell>27.1</cell><cell>7.16</cell><cell>27.6</cell><cell>14.35</cell><cell>28.9</cell><cell>29.95</cell><cell>34.6</cell><cell>63.20</cell></row><row><cell>Megatron</cell><cell>A100</cell><cell>FP16</cell><cell>26.9</cell><cell>4.55</cell><cell>27.1</cell><cell>9.40</cell><cell>27.6</cell><cell>18.65</cell><cell>28.9</cell><cell>37.63</cell><cell>34.6</cell><cell>75.02</cell></row><row><cell>Megatron</cell><cell>2xA100</cell><cell>FP16</cell><cell>17.9</cell><cell>5.11</cell><cell>22.1</cell><cell>10.17</cell><cell>22.1</cell><cell>20.42</cell><cell>22.1</cell><cell>41.04</cell><cell>22.1</cell><cell>82.93</cell></row><row><cell>Megatron</cell><cell>4xA100</cell><cell>FP16</cell><cell>8.0</cell><cell>5.25</cell><cell>11.1</cell><cell>10.35</cell><cell>11.1</cell><cell>20.89</cell><cell>11.1</cell><cell>41.86</cell><cell>11.1</cell><cell>84.95</cell></row><row><cell>Megatron</cell><cell>8xA100</cell><cell>FP16</cell><cell>4.8</cell><cell>5.47</cell><cell>5.7</cell><cell>11.04</cell><cell>6.3</cell><cell>22.38</cell><cell>6.5</cell><cell>45.50</cell><cell>6.5</cell><cell>90.47</cell></row><row><cell>Pytorch</cell><cell>3090</cell><cell>INT8</cell><cell>14.7</cell><cell>13.82</cell><cell>15.7</cell><cell>27.10</cell><cell>16.1</cell><cell>55.42</cell><cell>17.1</cell><cell>110.83</cell><cell>18.7</cell><cell>228.67</cell></row><row><cell>Pytorch</cell><cell>A100</cell><cell>INT8</cell><cell>14.7</cell><cell>9.40</cell><cell>15.7</cell><cell>18.65</cell><cell>16.1</cell><cell>37.38</cell><cell>17.1</cell><cell>75.60</cell><cell>18.7</cell><cell>155.01</cell></row><row><cell>LLM.int8()</cell><cell>A100</cell><cell>INT8</cell><cell>14.7</cell><cell>20.65</cell><cell>15.1</cell><cell>35.86</cell><cell>15.6</cell><cell>72.76</cell><cell>16.7</cell><cell>147.59</cell><cell>22.3</cell><cell>301.93</cell></row><row><cell>Oneflow</cell><cell>A100</cell><cell>FP16</cell><cell>25.9</cell><cell>2.61</cell><cell>26.2</cell><cell>5.25</cell><cell>27.0</cell><cell>10.89</cell><cell>29.0</cell><cell>22.49</cell><cell>33.6</cell><cell>47.54</cell></row><row><cell>Oneflow</cell><cell>A100</cell><cell>INT8</cell><cell>13.6</cell><cell>1.85</cell><cell>13.9</cell><cell>3.73</cell><cell>14.4</cell><cell>7.83</cell><cell>15.9</cell><cell>16.24</cell><cell>21.1</cell><cell>35.98</cell></row><row><cell>FastTrans</cell><cell>A100</cell><cell>FP16</cell><cell>26.0</cell><cell>2.43</cell><cell>26.1</cell><cell>4.93</cell><cell>26.3</cell><cell>10.21</cell><cell>26.7</cell><cell>22.60</cell><cell>27.5</cell><cell>50.09</cell></row><row><cell>FastTrans</cell><cell>A100</cell><cell>INT8</cell><cell>14.9</cell><cell>1.61</cell><cell>15.0</cell><cell>3.24</cell><cell>15.2</cell><cell>6.35</cell><cell>15.6</cell><cell>14.32</cell><cell>17.4</cell><cell>34.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Results of code generation task in HumanEval-X.</figDesc><table><row><cell>Language</cell><cell>Metric</cell><cell>GPT-J -6B</cell><cell>GPT-NeoX -20B</cell><cell>InCoder -6.7B</cell><cell>CodeGen -Multi-6B</cell><cell>CodeGen -Multi-16B</cell><cell>CodeGeeX -13B (ours)</cell></row><row><cell></cell><cell>pass@1</cell><cell>11.10%</cell><cell>13.83%</cell><cell>16.41%</cell><cell>19.41%</cell><cell>19.22%</cell><cell>22.89%</cell></row><row><cell>Python</cell><cell cols="2">pass@10 18.67%</cell><cell>22.72%</cell><cell>26.55%</cell><cell>30.29%</cell><cell>34.64%</cell><cell>39.57%</cell></row><row><cell></cell><cell cols="2">pass@100 30.98%</cell><cell>39.56%</cell><cell>43.95%</cell><cell>49.63%</cell><cell>55.17%</cell><cell>60.92%</cell></row><row><cell></cell><cell>pass@1</cell><cell>7.54%</cell><cell>9.90%</cell><cell>9.50%</cell><cell>11.44%</cell><cell>18.05%</cell><cell>17.06%</cell></row><row><cell>C++</cell><cell cols="2">pass@10 13.67%</cell><cell>18.99%</cell><cell>19.30%</cell><cell>26.23%</cell><cell>30.84%</cell><cell>32.21%</cell></row><row><cell></cell><cell cols="2">pass@100 30.16%</cell><cell>38.75%</cell><cell>36.10%</cell><cell>42.82%</cell><cell>50.90%</cell><cell>51.00%</cell></row><row><cell></cell><cell>pass@1</cell><cell>7.86%</cell><cell>8.87%</cell><cell>9.05%</cell><cell>15.17%</cell><cell>14.95%</cell><cell>20.04%</cell></row><row><cell>Java</cell><cell cols="2">pass@10 14.37%</cell><cell>19.55%</cell><cell>18.64%</cell><cell>31.74%</cell><cell>36.73%</cell><cell>36.70%</cell></row><row><cell></cell><cell cols="2">pass@100 32.96%</cell><cell>42.23%</cell><cell>40.70%</cell><cell>53.91%</cell><cell>60.62%</cell><cell>58.42%</cell></row><row><cell></cell><cell>pass@1</cell><cell>8.99%</cell><cell>11.28%</cell><cell>12.98%</cell><cell>15.41%</cell><cell>18.40%</cell><cell>17.59%</cell></row><row><cell>JavaScript</cell><cell cols="2">pass@10 16.32%</cell><cell>20.78%</cell><cell>22.98%</cell><cell>27.92%</cell><cell>32.80%</cell><cell>32.28%</cell></row><row><cell></cell><cell cols="2">pass@100 33.77%</cell><cell>42.67%</cell><cell>43.34%</cell><cell>48.81%</cell><cell>56.48%</cell><cell>56.33%</cell></row><row><cell></cell><cell>pass@1</cell><cell>4.01%</cell><cell>5.00%</cell><cell>8.68%</cell><cell>9.98%</cell><cell>13.03%</cell><cell>14.43%</cell></row><row><cell>Go</cell><cell cols="2">pass@10 10.81%</cell><cell>15.70%</cell><cell>13.80%</cell><cell>23.26%</cell><cell>25.46%</cell><cell>25.68%</cell></row><row><cell></cell><cell cols="2">pass@100 23.70%</cell><cell>32.08%</cell><cell>28.31%</cell><cell>41.01%</cell><cell>48.77%</cell><cell>47.14%</cell></row><row><cell></cell><cell>pass@1</cell><cell>7.90%</cell><cell>9.78%</cell><cell>11.33%</cell><cell>14.28%</cell><cell>16.73%</cell><cell>18.40%</cell></row><row><cell>Average</cell><cell cols="2">pass@10 14.77%</cell><cell>19.55%</cell><cell>20.25%</cell><cell>27.89%</cell><cell>32.09%</cell><cell>33.29%</cell></row><row><cell></cell><cell cols="2">pass@100 30.32%</cell><cell>39.06%</cell><cell>38.48%</cell><cell>47.24%</cell><cell>54.39%</cell><cell>54.76%</cell></row><row><cell cols="2">4.1 Evaluation Settings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>Results for fixed-budget multilingual generation on HumanEval-X.</figDesc><table><row><cell>Metric</cell><cell>Method</cell><cell>GPT-J -6B</cell><cell>GPT-NeoX -20B</cell><cell>InCoder -6.7B</cell><cell>CodeGen -Multi-6B</cell><cell>CodeGen -Multi-16B</cell><cell>CodeGeeX -13B</cell></row><row><cell>pass@k ? (k = 100)</cell><cell cols="2">Best Single 33.77% Uniform 36.40% Weighted 36.76%</cell><cell>42.67% 44.75% 44.97%</cell><cell>43.95% 43.89% 45.60%</cell><cell>53.19% 53.47% 53.94%</cell><cell>60.62% 61.01% 61.34%</cell><cell>60.92% 62.41% 62.95%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Composition of our code corpus for pre-training.</figDesc><table><row><cell>Language</cell><cell cols="2"># Tokens (B) % Tokens (%)</cell><cell>Language Tag</cell></row><row><cell>C++</cell><cell>45.2283</cell><cell>28.4963</cell><cell>// language: C++</cell></row><row><cell>Python</cell><cell>42.3250</cell><cell>26.667</cell><cell># language: Python</cell></row><row><cell>Java</cell><cell>25.3667</cell><cell>15.9824</cell><cell>// language: Java</cell></row><row><cell>JavaScript</cell><cell>11.3165</cell><cell>7.13</cell><cell>// language: JavaScript</cell></row><row><cell>C</cell><cell>10.6590</cell><cell>6.7157</cell><cell>// language: C</cell></row><row><cell>Go</cell><cell>7.4774</cell><cell>4.7112</cell><cell>// language: Go</cell></row><row><cell>HTML</cell><cell>4.9355</cell><cell>3.1096</cell><cell>&lt;!-language: HTML-&gt;</cell></row><row><cell>Shell</cell><cell>2.7498</cell><cell>1.7325</cell><cell># language: Shell</cell></row><row><cell>PHP</cell><cell>2.1698</cell><cell>1.3671</cell><cell>// language: PHP</cell></row><row><cell>CSS</cell><cell>1.5674</cell><cell>0.9876</cell><cell>/* language: CSS */</cell></row><row><cell>TypeScript</cell><cell>1.1667</cell><cell>0.7351</cell><cell>// language: TypeScript</cell></row><row><cell>SQL</cell><cell>1.1533</cell><cell>0.7267</cell><cell>-language: SQL</cell></row><row><cell>TeX</cell><cell>0.8257</cell><cell>0.5202</cell><cell>% language: TeX</cell></row><row><cell>Rust</cell><cell>0.5228</cell><cell>0.3294</cell><cell>// language: Rust</cell></row><row><cell>Objective-C</cell><cell>0.4526</cell><cell>0.2851</cell><cell>// language: Objective-C</cell></row><row><cell>Scala</cell><cell>0.3786</cell><cell>0.2385</cell><cell>// language: Scala</cell></row><row><cell>Kotlin</cell><cell>0.1707</cell><cell>0.1075</cell><cell>// language: Kotlin</cell></row><row><cell>Pascal</cell><cell>0.0839</cell><cell>0.0529</cell><cell>// language: Pascal</cell></row><row><cell>Fortran</cell><cell>0.077</cell><cell>0.0485</cell><cell>!language: Fortran</cell></row><row><cell>R</cell><cell>0.0447</cell><cell>0.0281</cell><cell># language: R</cell></row><row><cell>Cuda</cell><cell>0.0223</cell><cell>0.014</cell><cell>// language: Cuda</cell></row><row><cell>C#</cell><cell>0.0218</cell><cell>0.0138</cell><cell>// language: C#</cell></row><row><cell>Objective-C++</cell><cell>0.0014</cell><cell>0.0009</cell><cell>// language: Objective-C++</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>Detailed assignment of budget allocation strategies. Given budget k = 100, Weighted distribute the budgets according to the proportions of language in the training corpus of each model.</figDesc><table><row><cell>Strategy</cell><cell>Model</cell><cell cols="5">Python C++ Java JavaScript Go</cell></row><row><cell>Uniform</cell><cell>All</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell></cell><cell>GPT-J-6B</cell><cell>17</cell><cell>36</cell><cell>11</cell><cell>22</cell><cell>14</cell></row><row><cell></cell><cell>GPT-NeoX-20B</cell><cell>17</cell><cell>36</cell><cell>11</cell><cell>22</cell><cell>14</cell></row><row><cell>Weighted</cell><cell>InCoder-6.7B</cell><cell>45</cell><cell>12</cell><cell>5</cell><cell>34</cell><cell>4</cell></row><row><cell></cell><cell>CodeGen-Multi-6B/16B</cell><cell>17</cell><cell>38</cell><cell>29</cell><cell>8</cell><cell>8</cell></row><row><cell></cell><cell>CodeGeeX-13B (ours)</cell><cell>32</cell><cell>33</cell><cell>20</cell><cell>9</cell><cell>6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>https://marketplace.visualstudio.com/items?itemName=aminer.codegeex</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>https://plugins.jetbrains.com/plugin/20587-codegeex</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://huggingface.co/datasets/transformersbook/codeparrot</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>The HumanEval-X dataset and docker image are at https://hub.docker.com/r/codegeex/codegeex.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research was supported by <rs type="funder">Natural Science Foundation of China (NSFC)</rs> for Distinguished Young Scholars No. <rs type="grantNumber">61825602</rs>, <rs type="funder">NSFC</rs> No. <rs type="grantNumber">62276148</rs> and a research fund from Zhipu.AI. We give our special thanks to <rs type="person">Wenguang Chen</rs> from <rs type="funder">Tsinghua</rs>, the <rs type="institution">Peng Cheng Laboratory</rs>, and <rs type="person">Zhipu.AI</rs> for sponsoring the training and inference GPU resources. We thank all our collaborators and partners from <rs type="person">Tsinghua KEG</rs>, IIIS, <rs type="institution">Peng Cheng Laboratory</rs>, and <rs type="person">Zhipu.AI</rs>, including <rs type="person">Aohan Zeng</rs>, <rs type="person">Wendi Zheng</rs>, <rs type="person">Lilong Xue</rs>, <rs type="person">Yifeng Liu</rs>, <rs type="person">Yanru Chen</rs>, <rs type="person">Yichen Xu</rs>, <rs type="person">Qingyu Chen</rs>, <rs type="person">Zhongqi Li</rs>, <rs type="person">Gaojun Fan</rs>, <rs type="person">Yifan Yao</rs>, <rs type="person">Qihui Deng</rs>, <rs type="person">Bin Zhou</rs>, <rs type="person">Ruijie Cheng</rs>, <rs type="person">Peinan Yu</rs>, <rs type="person">Jingyao Zhang</rs>, <rs type="person">Bowen Huang</rs>, <rs type="person">Zhaoyu Wang</rs>, <rs type="person">Jiecai Shan</rs>, <rs type="person">Xuyang Ding</rs>, <rs type="person">Xuan Xue</rs>, and <rs type="person">Peng Zhang</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EwEnX9X">
					<idno type="grant-number">61825602</idno>
				</org>
				<org type="funding" xml:id="_p5qTBDq">
					<idno type="grant-number">62276148</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We also find that it gains performance when the sampling budgets are properly distributed to multiple languages.</p><p>Table <ref type="table">6</ref>: Results of code translation task in HumanEval-X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Python C++ Java JavaScript Go @1 @10 @100 @1 @10 @100 @1 @10 @100 @1 @10 @100 @1 @10 @100 Py InCoder-6. CodeGeeX-13B-FT is first fine-tuned using the training set of code translation task in XLCoST <ref type="bibr" target="#b47">(Zhu et al., 2022)</ref>, and then continuously fine-tuned by a small amount of Go data (since Go is missing in XLCoST). Among all translation pairs, CodeGeeX-13B-FT performs the best on pass@100 in 11 out of the 20, while CodeGen-Multi-16B is the best on 7 of them. We also observe a clear preference of languages by different models. CodeGeeX performs the best when translating other languages to Python and C++, while CodeGen-Multi-16B performs better when translating to JavaScript and Go. The evaluation setting on HumanEval is the same as HumanEval-X. We show that among multilingual code generation models, CodeGeeX achieves the second highest performance on HumanEval, reaching 60% in pass@100 (surpassed by PaLMCoder-540B). We also notice that monolingual models outperforms multilingual ones with a large margin, indicating that multilingual models require a larger model capacity to master different languages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Evaluation on MBPP</head><p>MBPP dataset is proposed by <ref type="bibr" target="#b1">Austin et al. (2021)</ref>, containing 974 problems in Python. Due to specific input-output format, MBPP need to be evaluated under a few-shot setting. We follow the splitting in the original paper and use problems 11-510 for testing. Under 1-shot setting, we use problem 2 in prompts. Under 3-shot setting, we use problem 2,3,4 in prompts. The metric is pass@k, k ? {1, 10, 80}. For pass@1, the temperature is 0.2 and top-p is 0.95; for pass@10 and pass@ 80, the temperature is 0.8 and top-p is 0.95. For baselines, we consider LaMDA-137B, PaLM-540B, Code-davinci-002 (online API version of OpenAI Codex), PaLMCoder-540B and InCoder-6.7B.</p><p>The results indicate that the model capacity is essential for multilingual code generation model. With significantly more parameters, PaLM and Codex outperform CodeGeeX with a large margin. Meanwhile, we find that more shot in the prompts harm the performance of CodeGeeX, the same phenomenon have also been discovered in InCoder <ref type="bibr" target="#b10">(Fried et al., 2022)</ref>. We assume that it is because smaller models do not have enough reasoning ability to benefit from the few-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 Evaluation on CodeXGLUE</head><p>CodeXGLUE is a benchmark proposed by <ref type="bibr" target="#b18">Lu et al. (2021)</ref>, containing multiple datasets to support evaluation on multiple tasks, using similarity-based metrics like CodeBLEU, BLEU and accuracy Table <ref type="table">11</ref>: The results of CodeGeeX on MBPP dataset <ref type="bibr" target="#b1">(Austin et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Model</head><p>Pass@1 Pass@10 Pass@80</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-shot</head><p>LaMDA-137B <ref type="bibr" target="#b1">(Austin et al., 2021)</ref> 14.80 -62.40 PaLM-540B <ref type="bibr" target="#b7">(Chowdhery et al., 2022)</ref> 36.80 -75.00 Code-davinci-002 <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> 50.40 -84.40 PaLMCoder-540B <ref type="bibr" target="#b7">(Chowdhery et al., 2022)</ref> 47.00 -80. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.4 Evaluation on XLCoST</head><p>XLCoST is a benchmark proposed by <ref type="bibr" target="#b47">Zhu et al. (2022)</ref>, containing parallel multilingual code data, with code snippets aligned among different languages. For generation tasks, XLCoST uses CodeBLEU, BLEU for evaluation. We choose the code translation task of XLCoST for CodeGeeX evaluation. We first fine-tune the parameters of CodeGeeX on the given training set, combining the training data in all 42 languages pairs to obtain one fine-tuned model. Then, we test the performance of the fine-tuned model on each language pair with CodeBLEU score.</p><p>For all language pairs, we set temperature to 0.2 and top-p to 0.95, and generate one translation for each sample in the test set. We report the results in Table <ref type="table">13</ref>. CodeGeeX performs better than all baseline models on all language pairs except for: PHP to Python on program level, C++ to Python on snippet level, and PHP to Python on snippet level. On average, CodeGeeX outperforms the baseline by 4.10 on program level and by 1.99 on snippet level. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unified pretraining for program understanding and generation</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Wasi Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06333</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<title level="m">Program synthesis with large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06745</idno>
		<title level="m">Gpt-neox-20b: An open-source autoregressive language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<title level="m">Palm: Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07339</idno>
	</analytic>
	<monogr>
		<title level="m">-bit matrix multiplication for transformers at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<title level="m">Codebert: A pre-trained model for programming and natural languages</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05999</idno>
		<title level="m">Incoder: A generative model for code infilling and synthesis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Puranik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09938</idno>
		<title level="m">Measuring coding challenge competence with apps</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spoc: Search-based pseudocode to code</title>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><surname>Dal Lago</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07814</idno>
		<title level="m">Competition-level code generation with alphacode</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Codexglue: A machine learning benchmark dataset for code understanding and generation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrosio</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04664</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On end-to-end program generation from user intention by deep neural networks</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07211</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13474</idno>
		<title level="m">Silvio Savarese, and Caiming Xiong. 2022. A conversational paradigm for program synthesis</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Anibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Peltekian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08645</idno>
		<title level="m">Cotext: Multi-task learning with code-text transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flashmeta: A framework for inductive program synthesis</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications</title>
		<meeting>the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="107" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Prophetnet-x: large-scale pre-training models for english, chinese, multi-lingual, dialog, and code generation</title>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartuer</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08006</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Daya</forename><surname>Shuo Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrosio</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10297</idno>
		<title level="m">Codebleu: a method for automatic evaluation of code synthesis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Program synthesis by sketching</title>
		<author>
			<persName><forename type="first">Armando</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A methodology for lisp program construction from examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Phillip</surname></persName>
		</author>
		<author>
			<persName><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Treegen: A tree-based transformer architecture for code generation</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yican</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8984" to="8991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intellicode compose: Code generation using transformer</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Lamda: Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Natural language processing with transformers</title>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prow: A step toward automatic program writing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard Ct</forename><surname>Waldinger</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st international joint conference on Artificial intelligence</title>
		<meeting>the 1st international joint conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1969">1969</date>
			<biblScope unit="page" from="241" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gpt-j-6b: A 6 billion parameter autoregressive language model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weishi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Ch</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00859</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A systematic evaluation of large language models of code</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Frank F Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><surname>Josua Hellendoorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming</title>
		<meeting>the 6th ACM SIGPLAN International Symposium on Machine Programming</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Oneflow: Redesign the distributed deep learning framework from scratch</title>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15032</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengtao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaojun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Yonghong Tian. 2021. Pangu-?: Large-scale autoregressive pretrained chinese language models with auto-parallel computation</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aneesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sindhu</forename><surname>Tipirneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandan K</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08474</idno>
		<title level="m">Xlcost: A benchmark dataset for cross-lingual code intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Productivity assessment of neural code completion</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirini</forename><surname>Kalliamvakou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Simister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Sittampalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Aftandilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming</title>
		<meeting>the 6th ACM SIGPLAN International Symposium on Machine Programming</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
