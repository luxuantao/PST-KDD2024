<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative modeling describes the framework of estimating the underlying probability distribution p(x) used to generate data x. This can facilitate a wide range of applications, from sampling novel datapoints to unsupervised representation learning to estimating the probability of an existing datapoint under the learned distribution. Much recent progress in generative modeling has been expedited by the use of deep neural networks, producing "deep generative models," which leverage the expressive power of deep networks to model complex and high-dimensional distributions. Practical achievements include generating realistic images with Proceedings of the 35 th International Conference on Machine <ref type="bibr">Learning, Stockholm, Sweden, PMLR 80, 2018.</ref> Copyright 2018 by the author(s). The latent codes for the top and bottom sequences are averaged and decoded by our model to produce the middle sequence. The latent-space mean involves a similar repeating pattern to the top sequence, but in a higher register and with intermittent pauses like the bottom sequence. Audio for this example is available in the online supplement. 3 See Figs. 12 and 13 in Appendix E for a baseline comparison.</p><p>millions of pixels <ref type="bibr" target="#b21">(Karras et al., 2017)</ref>, generating synthetic audio with hundreds of thousands of timesteps (van den <ref type="bibr" target="#b35">Oord et al., 2016a)</ref>, and achieving state-of-the-art performance on semi-supervised learning tasks <ref type="bibr" target="#b36">(Wei et al., 2018)</ref>.</p><p>A wide variety of methods have been used in deep generative modeling, including implicit models such as Generative Adversarial Networks (GANs) <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref> and explicit deep autoregressive models such as PixelCNN <ref type="bibr" target="#b35">(van den Oord et al., 2016b)</ref> and <ref type="bibr" target="#b35">WaveNet (van den Oord et al., 2016a)</ref>. In this work, we focus on deep latent variable models such as the Variational Autoencoder (VAE) <ref type="bibr" target="#b23">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b30">Rezende et al., 2014)</ref>. The advantage of these models is that they explicitly model both p(z|x) and p(z), where z is a latent vector which can either be inferred from existing data or sampled from a distribution over the latent space. Ideally, the latent vector captures the pertinent characteristics of a given datapoint and disentangles factors of variation in a dataset. These autoencoders also model the likelihood p(x|z) which provides an efficient way of mapping the latent vector back to the data space.</p><p>Our interest in deep latent variable models primarily comes from their increasing use in creative applications of machine learning <ref type="bibr" target="#b5">(Carter &amp; Nielsen, 2017;</ref><ref type="bibr" target="#b15">Ha &amp; Eck, 2018;</ref><ref type="bibr" target="#b9">Engel et al., 2017a)</ref>. This arises from surprising and convenient characteristics of the latent spaces typically learned by these models. For example, averaging the latent codes for all datapoints which possess a given attribute produces a so-called attribute vector, which can be used to make targeted changes to data examples. Encoding a datapoint with some attribute (say, a photograph of a person with brown hair) to obtain its latent code, subtracting the corresponding attribute vector (the "brown hair" vector), adding another attribute vector ("blond hair"), and decoding the resulting latent vector can produce a realistic manifestation of the initial point with the attributes swapped (the same person with blond hair) <ref type="bibr" target="#b25">(Larsen et al., 2016;</ref><ref type="bibr" target="#b27">Mikolov et al., 2013)</ref>. As another example, interpolating between latent vectors and decoding points on the trajectory can produce realistic intermediate datapoints which morph between the characteristics of the ends in a smooth and semantically meaningful way.</p><p>Most work on deep latent variable models has focused on continuous-valued data with a fixed dimensionality, e.g., images. Modeling sequential data is less common, particularly sequences of discrete tokens such as musical scores, which typically require the use of an autoregressive decoder. This is partially because autoregression is often sufficiently powerful that the autoencoder ignores the latent code <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref>. While they have shown some success on short sequences (e.g., sentences), deep latent variable models have yet to be successfully applied to very long sequences.</p><p>To address this gap, we introduce a novel sequential autoencoder with a hierarchical recurrent decoder, which helps overcome the aforementioned issue of modeling long-term structure with recurrent VAEs. Our model encodes an entire sequence to a single latent vector, which enables many of the creative applications enjoyed by VAEs of images. We show experimentally that our model is capable of effectively autoencoding substantially longer sequences than a baseline model with a "flat" decoder RNN.</p><p>In this paper, we focus on the application of modeling sequences of musical notes. Western popular music exhibits strong long-term structure, such as the repetition and variation between measures and sections of a piece of music. This structure is also hierarchical-songs are divided into sections, which are broken up into measures, and then into beats, and so on. Further, music is fundamentally a multi-stream sig-nal, in the sense that it often involves multiple players with strong inter-player dependencies. These unique properties, in additional to the potential creative applications, make music an ideal testbed for our sequential autoencoder.</p><p>After covering a background of work our approach builds on, we describe our model and its novel architectural enhancements. We then provide an overview of related work on applying latent variable models to sequences. Finally, we demonstrate the ability of our method to model musical data through quantitative and qualitative evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Fundamentally, our model is an autoencoder, i.e., its goal is to accurately reconstruct its inputs. However, we additionally desire the ability to draw novel samples and perform latent-space interpolations and attribute vector arithmetic.</p><p>For these properties, we adopt the framework of the Variational Autoencoder. Successfully using VAEs for sequences benefits from some additional extensions to the VAE objective. In the following subsections, we cover the prior work which forms the backbone for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Variational Autoencoders</head><p>A common constraint applied to autoencoders is that they compress the relevant information about the input into a lower-dimensional latent code. Ideally, this forces the model to produce a compressed representation which captures important factors of variation in the dataset. In pursuit of this goal, the Variational Autoencoder <ref type="bibr" target="#b23">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b30">Rezende et al., 2014)</ref> introduces the constraint that the latent code z is a random variable distributed according to a prior p(z). The data generation model is then z ∼ p(z), x ∼ p(x|z). The VAE consists of an encoder q λ (z|x), which approximates the posterior p(z|x), and a decoder p θ (x|z), which parameterizes the likelihood p(x|z).</p><p>In practice, the approximate posterior and likelihood distributions ("encoder" and "decoder") are parameterized by neural networks with parameters λ and θ respectively. Following the framework of Variational Inference, we do posterior inference by minimizing the KL divergence between our approximate posterior, the encoder, and the true posterior p(z|x) by maximizing the evidence lower bound (ELBO)</p><formula xml:id="formula_0">E[log p θ (x|z)] − KL(q λ (z|x)||p(z)) ≤ log p(x)<label>(1)</label></formula><p>where the expectation is taken with respect to z ∼ q λ (z|x) and KL(•||•) is the KL-divergence. Naively computing the gradient through the ELBO is infeasible due to the sampling operation used to obtain z. In the common case where p(z) is a diagonal-covariance Gaussian, this can be circumvented by replacing z ∼ N (µ, σI) with</p><formula xml:id="formula_1">∼ N (0, I), z = µ + σ (2)</formula><p>2.1.1. β-VAE AND FREE BITS One way of interpreting the ELBO used in the VAE is by considering its two terms, E[log p θ (x|z)] and KL(q λ (z|x)||p(z)), separately. The first can be thought of as requiring that p(x|z) is high for samples of z from q λ (z|x)-ensuring accurate reconstructions. The second encourages q λ (z|x) to be close to the prior-ensuring we can generate realistic data by sampling latent codes from p(z).</p><p>The presence of these terms suggests a trade-off between the quality of samples and reconstructions-or equivalently, between the rate (amount of information encoded in q λ (z|x)) and distortion (data likelihood) <ref type="bibr" target="#b0">(Alemi et al., 2017)</ref>.</p><p>As is, the ELBO has no way of directly controlling this trade-off. A common modification to the ELBO introduces the KL weight hyperparameter β <ref type="bibr" target="#b4">(Bowman et al., 2016;</ref><ref type="bibr" target="#b17">Higgins et al., 2017)</ref> producing</p><formula xml:id="formula_2">E[log p θ (x|z)] − β KL(q λ (z|x)||p(z))<label>(3)</label></formula><p>Setting β &lt; 1 encourages the model to prioritize reconstruction quality over learning a compact representation.</p><p>Another approach for adjusting this trade-off is to only enforce the KL regularization term once it exceeds a threshold <ref type="bibr" target="#b24">(Kingma et al., 2016)</ref>:</p><formula xml:id="formula_3">E[log p θ (x|z)] − max(KL(q λ (z|x)||p(z)) − τ, 0) (4)</formula><p>This stems from the interpretation that KL(q λ (z|x)||p(z)) measures the amount of information required to code samples from p(z) using q λ (z|x). Utilizing this threshold therefore amounts to giving the model a certain budget of "free bits" to use when learning the approximate posterior. Note that these modified objectives no longer optimize a lower bound on the likelihood, but as is custom we still refer to the resulting models as "Variational Autoencoders."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">LATENT SPACE MANIPULATION</head><p>The broad goal of an autoencoder is to learn a compact representation of the data. For creative applications, we have additional uses for the latent space learned by the model <ref type="bibr" target="#b5">(Carter &amp; Nielsen, 2017;</ref><ref type="bibr" target="#b31">Roberts et al., 2018)</ref>. First, given a point in latent space which maps to a realistic datapoint, nearby latent space points should map to datapoints that are semantically similar. By extrapolation, this implies that all points along a continuous curve connecting two points in latent space should be decodable to a series of datapoints which produce a smooth semantic interpolation in data space. Further, this requirement effectively mandates that the latent space is "smooth" and does not contain any "holes," i.e., isolated regions which do not map to realistic datapoints. Second, we desire that the latent space disentangles meaningful semantic groups in the dataset.</p><p>Ideally, these requirements should be satisfied by a VAE if the likelihood and KL divergence terms are both sufficiently small on held-out test data. A more practical test of these properties is to interpolate between points in the latent space and test whether the corresponding points in the data space are interpolated in a semantically meaningful way. Concretely, if z 1 and z 2 are the latent vectors corresponding to datapoints x 1 and x 2 , then we can perform linear interpolation in latent space by computing</p><formula xml:id="formula_4">c α = αz 1 + (1 − α)z 2 (5) for α ∈ [0, 1]. Our goal is satisfied if p θ (x|c α</formula><p>) is a realistic datapoint for all α, p θ (x|c α ) transitions in a semantically meaningful way from p θ (x|c 0 ) to p θ (x|c 1 ) as we vary α from 0 to 1, and that p θ (x|c α ) is perceptually similar to p θ (x|c α+δ ) for small δ. Note that because the prior over the latent space of a VAE is a spherical Gaussian, samples from high-dimensional priors are practically indistinguishable from samples from the uniform distribution on the unit hypersphere <ref type="bibr" target="#b19">(Huszár, 2017)</ref>. In practice we therefore use spherical interpolation <ref type="bibr" target="#b37">(White, 2016)</ref> instead of Eq. ( <ref type="formula">5</ref>).</p><p>An additional test for whether our autoencoder will be useful in creative applications measures whether it produces reliable "attribute vectors." Attribute vectors are computed as the average latent vector for a collection of datapoints which share some particular attribute. Typically, attribute vectors are computed for pairs of attributes, e.g., images of people with and without glasses. The model's ability to discover attributes is then tested by encoding a datapoint with attribute A, subtracting the "attribute A vector" from its latent code, adding the "attribute B vector", and testing whether the decoded result appears like the original datapoint with attribute B instead of A. In our experiments, we use the above latent space manipulation techniques to demonstrate the power of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recurrent VAEs</head><p>While a wide variety of neural network structures have been considered for the encoder and decoder network in a VAE, in the present work we are most interested in models with a recurrent encoder and decoder <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref>. Concretely, the encoder, q λ (z|x), is a recurrent neural network (RNN) which processes the input sequence x = {x 1 , x 2 , . . . , x T } and produces a sequence of hidden states h 1 , h 2 , . . . , h T . The parameters of the distribution over the latent code z is then set as a function of h T . The decoder, p θ (x|z), uses the sampled latent vector z to set the initial state of a decoder RNN, which autoregressively produces the output sequence y = {y 1 , y 2 , . . . , y T }. The model is trained both to reconstruct the input sequence (i.e., y i = x i , i ∈ {1, . . . , T }) and to learn an approximate posterior q λ (z|x) close to the prior p(z), as in a standard VAE.</p><p>There are two main drawbacks of this approach. First, RNNs are themselves typically used on their own as powerful autoregressive models of sequences. As a result, the decoder in a recurrent VAE is itself sufficiently powerful to produce an effective model of the data and may disregard the latent code. With the latent code ignored, the KL divergence term of the ELBO can be trivially set to zero, despite the fact that the model is effectively no longer acting as an autoencoder.</p><p>Second, the model must compress the entire sequence to a single latent vector. While this approach has been shown to work for short sequences <ref type="bibr" target="#b4">(Bowman et al., 2016;</ref><ref type="bibr" target="#b35">Sutskever et al., 2014)</ref>, it begins to fail as the sequence length increases <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>. In the following section, we present a latent variable autoencoder that overcomes these issues by using a hierarchical RNN for the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>At a high level, our model follows the basic structure used in previously-proposed VAEs for sequential data <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref>. However, we introduce a novel hierarchical decoder, which we demonstrate produces substantially better performance on long sequences in Section 5. A schematic of our model, which we dub "MusicVAE," is shown in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bidirectional Encoder</head><p>For the encoder q λ (z|x), we use a two-layer bidirectional LSTM network <ref type="bibr" target="#b18">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b32">Schuster &amp; Paliwal, 1997)</ref>. We process an input sequence x = {x 1 , x 2 , . . . , x T } to obtain the final state vectors − → h T , ← − h T from the second bidirectional LSTM layer. These are then concatenated to produce h T and fed into two fullyconnected layers to produce the latent distribution parameters µ and σ:</p><formula xml:id="formula_5">µ = W hµ h T + b µ (6) σ = log (exp(W hσ h T + b σ ) + 1)<label>(7)</label></formula><p>where W hµ , W hσ and b µ , b σ are weight matrices and bias vectors, respectively. In our experiments, we use an LSTM state size of 2048 for all layers and 512 latent dimensions.</p><p>As is standard in VAEs, µ and σ then parametrize the latent distribution as in Eq. ( <ref type="formula">2</ref>). The use of a bidirectional recurrent encoder ideally gives the parametrization of the latent distribution longer-term context about the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Decoder</head><p>In prior work, the decoder in a recurrent VAE is typically a simple stacked RNN. The decoder RNN uses the latent vector z to set its initial state, and proceeds to generate the output sequence autoregressively. In preliminary experiments (discussed in Section 5), we found that using a simple RNN as the decoder resulted in poor sampling and reconstruction for long sequences. We believe this is caused by the vanishing influence of the latent state as the output sequence is generated.</p><p>To mitigate this issue, we propose a novel hierarchical RNN for the decoder. Assume that the input sequence (and target output sequence) x can be segmented into U nonoverlapping subsequences y u with endpoints i u so that</p><formula xml:id="formula_6">y u = {x iu , x iu+1 , x iu+2 , . . . , x iu+1−1 } (8) → x = {y 1 , y 2 , . . . , y U }<label>(9)</label></formula><p>where we define the special case of i U +1 = T . Then, the latent vector z is passed through a fully-connected layer<ref type="foot" target="#foot_0">4</ref> followed by a tanh activation to get the initial state of a "conductor" RNN. The conductor RNN produces U embedding vectors c = {c 1 , c 2 , . . . , c U }, one for each subsequence. In our experiments, we use a two-layer unidirectional LSTM for the conductor with a hidden state size of 1024 and 512 output dimensions.</p><p>Once the conductor has produced the sequence of embedding vectors c, each one is individually passed through a shared fully-connected layer followed by a tanh activation to produce initial states for a final bottom-layer decoder RNN. The decoder RNN then autoregressively produces a sequence of distributions over output tokens for each subsequence y u via a softmax output layer. At each step of the bottom-level decoder, the current conductor embedding c u is concatenated with the previous output token to be used as the input. In our experiments, we used a 2-layer LSTM with 1024 units per layer for the decoder RNN.</p><p>In principle, our use of an autoregressive RNN decoder still allows for the "posterior collapse" problem where the model effectively learns to ignore the latent state. Simliar to <ref type="bibr" target="#b6">(Chen et al., 2017)</ref>, we find that it is important to limit the scope of the decoder to force it to use the latent code to model long-term structure. For a CNN decoder, this is as simple as reducing the receptive field, but no direct analogy exists for RNNs, which in principle have an unlimited temporal receptive field. To get around this, we reduce the effective scope of the bottom-level RNN in the decoder by only allowing it to propagate state within an output subsequence. As described above, we initialize each subsequence RNN state with the corresponding embedding passed down by the conductor. This implies that the only way for the decoder to get longer-term context is by using the embeddings produced by the conductor, which in turn depend solely on the latent code. We experimented with an autoregressive version of the conductor where the decoder state was passed back to the conductor at the end of each subsequence, but found it exhibited worse performance. We believe that these combined constraints effectively force the model to utilize the conductor embeddings, and by extension the latent vector, in order to correctly autoencode the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Stream Modeling</head><p>Many common sources of sequential data, such as text, consist solely of a single "stream," i.e., there is only one sequence source which is producing tokens. However, music is often a fundamentally multi-stream signal-a given musical sequence may consist of multiple players producing notes in tandem. Modeling music therefore may also involve modeling the complex inter-stream dependencies.</p><p>We explore this possibility by introducing a "trio" model, which is identical to our basic MusicVAE except that it produces 3 separate distributions over output tokens-one for each of three instruments (drum, bass, and melody). In our hierarchical decoder model, we consider these separate streams as an orthogonal "dimension" of hierarchy, and use a separate decoder RNN for each instrument. The embeddings from the conductor RNN initialize the states of each instrument RNN through separate fully-connected layers followed by tanh activations. For our baseline with a "flat" (non-hierarchical) decoder, we use a single RNN and split its output to produce per-instrument softmaxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>A closely related model is the aforementioned recurrent VAE of <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref>. Like ours, their model is effectively a VAE that uses RNNs for both the encoder and decoder. With careful optimization, <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref> demonstrate the ability to generate and interpolate between sentences which have been modeled at the character level.</p><p>A very similar model was also proposed by <ref type="bibr" target="#b11">(Fabius &amp; van Amersfoort, 2015)</ref>, which was applied with limited success to music. This approach was also extended to utilize a convolutional encoder and decoder with dilated convolutions in <ref type="bibr" target="#b38">(Yang et al., 2017)</ref>. The primary difference between these models and ours is the decoder architecture; namely, we use a hierarchical RNN. The flat RNN decoder we use as a baseline in Section 5 exhibits significantly degraded performance when dealing with very long sequences.</p><p>Various additional VAE models with autoregressive decoders have also been proposed. <ref type="bibr" target="#b33">(Semeniuta et al., 2017)</ref> consider extensions of the recurrent VAE where the RNNs are replaced with feed-forward and convolutional networks. The PixelVAE <ref type="bibr" target="#b14">(Gulrajani et al., 2017</ref>) marries a VAE with a PixelCNN (van den <ref type="bibr" target="#b35">Oord et al., 2016b)</ref> and applies the result to the task of natural image modeling. Similarly, the Variational Lossy Autoencoder <ref type="bibr" target="#b6">(Chen et al., 2017</ref>) combines a VAE with a PixelCNN/PixelRNN decoder. The authors also consider limiting the power of the decoder and using a more expressive Inverse Autoregressive Flow <ref type="bibr" target="#b24">(Kingma et al., 2016)</ref> prior on the latent codes. Another example of a VAE with a recurrent encoder and decoder is SketchRNN <ref type="bibr" target="#b15">(Ha &amp; Eck, 2018)</ref>, which successfully models sequences of continuously-valued pen coordinates.</p><p>The hierarchical paragraph autoencoder proposed in <ref type="bibr" target="#b26">(Li et al., 2015)</ref> has several parallels to our work. They also consider an autoencoder with hierarchical RNNs for the encoder and decoder, where each level in the hierarchy corresponds to natural subsequences in text (e.g., sentences and words). However, they do not impose any constraints on the latent code, and as a result are unable to sample or interpolate between sequences. Our model otherwise differs in its use of a flat bidirectional encoder and lack of autoregressive connections in the first level of the hierarchy.</p><p>More broadly, our model can be considered in the sequenceto-sequence framework <ref type="bibr" target="#b35">(Sutskever et al., 2014)</ref>, where an encoder produces a compressed representation of an input sequence which is then used to condition a decoder to generate an output sequence. For example, the NSynth model learns embeddings by compressing audio waveforms with a downsampling convolutional encoder and then reconstructing audio with a WaveNet-style decoder <ref type="bibr" target="#b10">(Engel et al., 2017b)</ref>. Recurrent sequence-to-sequence models are most often applied to sequence transduction tasks where the input and output sequences are different. Nevertheless, sequence-to-sequence autoencoders have been occasionally considered, e.g., as an auxiliary unsupervised training method for semi-supervised learning <ref type="bibr" target="#b8">(Dai &amp; Le, 2015)</ref> or in the paragraph autoencoder described above. Again, our approach differs in that we impose structure on the compressed representation (our latent vector) so that we can perform sampling and interpolation.</p><p>Finally, there have been many recurrent models proposed where the recurrent states are themselves stochastic latent variables with dependencies across time <ref type="bibr" target="#b7">(Chung et al., 2015;</ref><ref type="bibr" target="#b2">Bayer &amp; Osendorfer, 2014;</ref><ref type="bibr" target="#b12">Fraccaro et al., 2016)</ref>. A particularly similar example to our model is that of <ref type="bibr" target="#b34">(Serban et al., 2017)</ref>, which also utilizes a hierarchical encoder and decoder. Their model uses two levels of hierarchy and gener-ates a stochastic latent variable for each subsequence of the input sequence. The crucial difference between this class of models and ours is that we use a single latent variable to represent the entire sequence, which allows for creative applications such as interpolation and attribute manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To demonstrate the power of the MusicVAE, we carried out a series of quantitative and qualitative studies on music data. First, we demonstrate that a simple recurrent VAE like the one described in <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref> can effectively generate and interpolate between short sequences of musical notes. Then, we move to significantly longer note sequences, where our novel hierarchical decoder is necessary in order to effectively model the data. To verify this assertion, we provide quantitative evidence that it is able to reconstruct, interpolate between, and model attributes from data significantly better than the baseline. We conclude with a series of listening studies which demonstrate that our proposed model also produces a dramatic improvement in the perceived quality of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data and Training</head><p>For our data source, we use MIDI files, which are a widelyused digital score format. MIDI files contain instructions for the notes to be played on each individual instrument in a song, as well as meter (timing) information. We collected ≈1.5 million unique files from the web, which provided ample data for training our models. We extracted the following types of training data from these MIDI files: 2-and 16-bar melodies (monophonic note sequences), 2-and 16-bar drum patterns (events corresponding to playing different drums), and 16-bar "trio" sequences consisting of separate streams of a melodic line, a bass line, and a drum pattern. For further details on our dataset creation process, refer to Appendix A. For ease of comparison, we also evaluated reconstruction quality (as in Section 5.3 below) on the publicly available Lakh MIDI Dataset <ref type="bibr" target="#b28">(Raffel, 2016)</ref> in Appendix B.</p><p>We modeled the monophonic melodies and basslines as sequences of 16th note events. This resulted in a 130dimensional output space (categorical distribution over tokens) with 128 "note-on" tokens for the 128 MIDI pitches, plus single tokens for "note-off" and "rest". For drum patterns, we mapped the 61 drum classes defined by the General MIDI standard (International MIDI Association, 1991) to 9 canonical classes and represented all possible combinations of hits with 512 categorical tokens. For timing, in all cases we quantized notes to 16th note intervals, such that each bar consisted of 16 events. As a result, our two-bar data (used as a proof-of-concept with a flat decoder) resulted in sequences with T = 32 and 16-bar data had T = 256. For our hierarchical models, we use U = 16, meaning each subsequence corresponded to a single bar.</p><p>All models were trained using Adam <ref type="bibr" target="#b22">(Kingma &amp; Ba, 2014)</ref> with a learning rate annealed from 10 −3 to 10 −5 with exponential decay rate 0.9999 and a batch size of 512. The 2-and 16-bar models were run for 50k and 100k gradient updates, respectively. We used a cross-entropy loss against the ground-truth output with scheduled sampling <ref type="bibr" target="#b3">(Bengio et al., 2015)</ref> for 2-bar models and teacher forcing for 16-bar models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Short Sequences</head><p>As a proof that modeling musical sequences with a recurrent VAE is possible, we first tried modeling 2-bar (T = 32) monophonic music sequences (melodies and drum patterns) with a flat decoder. The model was given a tolerance of 48 free bits (≈33.3 nats) and had the KL cost weight, β, annealed from 0.0 to 0.2 with exponential rate 0.99999. Scheduled sampling was introduced with an inverse sigmoid rate of 2000.</p><p>We found the model to be highly accurate at reconstructing its input (Table <ref type="table" target="#tab_1">1</ref> discussed below in Section 5.3). It was also able to produce compelling interpolations (Fig. <ref type="figure" target="#fig_0">14</ref>, Appendix E) and samples. In other words, it learned to effectively use its latent code without suffering from posterior collapse or exposure bias, as particularly evidenced by the relatively small difference in teacher-forced and sampled reconstruction accuracy (a few percent).</p><p>Despite this success, the model was unable to reliably reconstruct 16-bar (T = 256) sequences. For example, the discrepancy between teacher-forced and sampled reconstruction accuracy increased by more than 27%. This motivated our design of the hierarchical decoder described in Section 3.2. In the following sections, we provide an extensive comparison of our proposed model to the flat baseline. To begin, we evaluate whether the hierarchical decoder produces better reconstruction accuracy on 16-bar melodies and drum patterns. For 16-bar models, we give a tolerance of 256 free bits (≈177.4 nats) and use β = 0.2. Table <ref type="table" target="#tab_1">1</ref> shows the per-step accuracies for reconstructing the sequences in our test set. As mentioned above, we see signs of posterior collapse with the flat baseline, leading to reductions in accuracy of ≈27 − 32% when teacher forcing is removed for inference. Our hierarchical decoder both increases the next-step prediction accuracy and further reduces the exposure bias by better learning to use its latent code. With the hierarchical model, the decrease in sampling accuracy versus teacher forcing only ranges between ≈5 − 11%. In general, we also find that the reconstruction errors made by our models are reasonable, e.g., notes shortened by a beat or the addition of notes in the appropriate key.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Reconstruction Quality</head><p>We also explored the performance of our models on our multi-stream "trio" dataset, consisting of 16-bar sequences of melody, bass, and drums. As with single-stream data, the hierarchical model was able to achieve higher accuracy than the flat baseline while exhibiting a much smaller gap between teacher-forced and sampled performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Interpolations</head><p>For creative purposes, we desire interpolations that are smoothly varying and semantically meaningful. In Fig. <ref type="figure" target="#fig_1">3</ref>, we compare latent-space interpolations from a flat decoder (yellow circles) and hierarchical decoder (magenta squares) to a baseline of naive blending of the two sequences (green diamonds). We averaged the behavior of interpolating between 1024 16-bar melodies from the evaluation dataset (A) and 1024 other unique evaluation melodies (B), using a softmax temperature of 0.5 to sample the intermediate sequences. We constructed baseline "Data" interpolations by sampling a Bernoulli random variable with parameter α to choose an element from either sequence a or b for each time step, i.e., p(x t = b t ) = α, p(x t = a t ) = 1 − α.</p><p>The top graph of Fig. <ref type="figure" target="#fig_1">3</ref> shows that the (sequence lengthnormalized) Hamming distance, i.e., the proportion of timestep predictions that differ between the interpolation and sequence A, increases monotonically for all methods.</p><p>The data interpolation varies linearly as expected, following the mean of the Bernoulli distribution. The Hamming distances also vary monotonically for latent space interpolations, showing that the decoded sequences morph smoothly to be less like sequence A and more like sequence B. For example, reconstructions don't remain on one mode for several steps and then jump suddenly to another. Samples have a non-zero Hamming distance at the endpoints because of imperfect reconstructions, and the hierarchical decoder has a lower intercept due to its higher reconstruction accuracy.</p><p>For the bottom graph of Fig. <ref type="figure" target="#fig_1">3</ref>, we first trained a 5-gram language model on the melody dataset <ref type="bibr" target="#b16">(Heafield, 2011)</ref>. We show the normalized cost for each interpolated sequence given by C α (αC B +(1−α)C A ), where C α is the language model cost of an interpolated sequence with interpolation amount α, and C A and C B are the costs for the endpoint sequences A and B. The large hump for the data interpolation shows that interpolated sequences in data space are deemed by the language model to be much less probable than the original melodies. The flat model does better, but produces less coherent interpolated sequences than the hierarchical model, which produces interpolations of equal probability to the originals across the entire range of interpolation.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows two example melodies and their corresponding midpoint in MusicVAE space. The interpolation synthesizes semantic elements of the two melodies: a similar repeating pattern to A, in a higher register with intermittent sparsity like B, and in a new shared musical key. On the other hand, the baseline data interpolation just mixes the two resulting in harmonic and rhythmic dissonance (Fig. <ref type="figure" target="#fig_0">12</ref>, Appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Attribute Vector Arithmetic</head><p>We can also exploit the structure of the latent space to use "attribute vectors" to alter the attributes of a given sequence.</p><p>Apart from the score itself, MIDI contains limited annotations <ref type="bibr" target="#b29">(Raffel &amp; Ellis, 2016)</ref>, so we defined five attributes which can be trivially computed directly from the note sequence: C diatonic membership, note density, average interval, and 16th and 8th note syncopation. See Appendix C for their full definitions. Ideally, computing the difference between the average latent vector for sequences which exhibit the two extremes of each attribute and then adding or subtracting it from latent codes of existing sequences will produce the intended semantic manipulation.</p><p>To test this, we first measured these attributes in a set of 370k random training examples. For each attribute, we ordered the set by the amount of attribute exhibited, partitioned it into quartiles, and computed an attribute vector by subtracting the mean latent vector of the bottom quartile from the mean latent vector of the top quartile. We then sampled 256 random vectors from the prior, added and subtracted vectors for each attribute, and measured the average percentage change for all attributes against the sequence decoded from the unaltered latent code. The results are shown in Fig. <ref type="figure">4</ref>.</p><p>In general, we find that applying a given attribute vector consistently produces the intended change to the targeted attribute. We also find cases where increasing one attribute decreases another (e.g. increasing density decreases 8th note syncopation), which we believe is largely because our heuristics capture overlapping characteristics. We are interested in evaluating attribute vector manipulations for labels that are non-trivial to compute (e.g., ornamentation, call/response, etc.) in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Listening Tests</head><p>Capturing whether samples from our model sound realistic is difficult to do with purely quantitative metrics. To compare the perceived sample quality of the different models, we therefore carried out listening studies for melodies, trio compositions, and drum patterns.</p><p>Participants were presented with two 16-bar (≈30s) compositions that were either sampled from one of the models or extracted from our evaluation dataset. They were then asked which they thought was more musical on a Likert scale. For each study, 192 ratings were collected, with each source involved in 128 pair-wise comparisons. All samples were generated using a softmax temperature of 0.5.</p><p>Fig. <ref type="figure" target="#fig_2">5</ref> shows the number of comparisons in which a composition from each model was selected as more musical.</p><p>Our listening test clearly demonstrates the improvement in sample quality gained by using a hierarchical decoder-in all cases the hierarchical model was preferred dramatically more often than the flat model and at the same rate as the evaluation data. In fact, the hierarchical drum model was preferred more often than real data, but the difference is not statistically significant. This was likely due to a listener bias towards variety, as the true drum data, while more realistic, was also more repetitive and perhaps less engaging.</p><p>Further, a Kruskal-Wallis H test of the ratings showed that there was a statistically significant difference between the models: χ 2 (2) = 37.85, p &lt; 0.001 for melodies, χ 2 (2) = 76.62, p &lt; 0.001 for trios, and χ 2 (2) = 44.54, p &lt; 0.001 for drums. A post-hoc analysis using the Wilcoxon signedrank test with Bonferroni correction showed that participants rated samples from the 3 hierarchical models as more musical than samples from their corresponding flat models with p &lt; 0.01/3. Participants also ranked real data as more musical than samples from the flat models with p &lt; 0.01/3. There was no significant difference between samples from the hierarchical models and real data.</p><p>Audio of some of the examples used in the listening tests is available in the online supplement. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed MusicVAE, a recurrent Variational Autencoder which utilizes a hierarchical decoder for improved modeling of sequences with long-term structure. In experiments on music data, we thoroughly demonstrated through quantitative and qualitative experiments that our model achieves substantially better performance than a flat baseline. In future work, we are interested in testing our model on other types of sequential data. To facilitate future research on recurrent latent variable models, we make our code and pre-trained models publicly available. 2</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Demonstration of latent-space averaging using MusicVAE. The latent codes for the top and bottom sequences are averaged and decoded by our model to produce the middle sequence. The latent-space mean involves a similar repeating pattern to the top sequence, but in a higher register and with intermittent pauses like the bottom sequence. Audio for this example is available in the online supplement. 3 See Figs. 12 and 13 in Appendix E for a baseline comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Latent-space interpolation results. All values are averaged over 1024 interpolated sequences. X-axis denotes interpolation between sequence A to B from left to right. Top: Sequencenormalized Hamming distance between sequence A and interpolated points. The distance from B is symmetric to A (decreasing as A increases) and is not shown. Bottom: Relative log probability according to an independently-trained 5-gram language model.</figDesc><graphic url="image-1.png" coords="7,78.84,349.73,187.21,193.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results of our listening tests. Black error bars indicate estimated standard deviation of means. Double asterisks for a pair indicate a statistically significant difference in ranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoder</cell></row><row><cell></cell><cell></cell><cell>Z</cell><cell></cell><cell></cell><cell>Latent Code</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conductor</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decoder</cell></row><row><cell>♪ ♪</cell><cell>♪</cell><cell>♪ ♪</cell><cell>♪</cell><cell>♪ ♪</cell><cell>♪</cell></row><row><cell>16</cell><cell></cell><cell>16</cell><cell></cell><cell>16</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell cols="6">Figure 2. Schematic of our hierarchical recurrent Variational Au-</cell></row><row><cell cols="3">toencoder model, MusicVAE.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Reconstruction</figDesc><table><row><cell></cell><cell cols="2">Teacher-Forcing</cell><cell></cell><cell>Sampling</cell></row><row><cell>Model</cell><cell>Flat</cell><cell>Hierarchical</cell><cell>Flat</cell><cell>Hierarchical</cell></row><row><cell>2-bar Drum</cell><cell>0.979</cell><cell>-</cell><cell>0.917</cell><cell>-</cell></row><row><cell>2-bar Melody</cell><cell>0.986</cell><cell>-</cell><cell>0.951</cell><cell>-</cell></row><row><cell cols="2">16-bar Melody 0.883</cell><cell>0.919</cell><cell>0.620</cell><cell>0.812</cell></row><row><cell>16-bar Drum</cell><cell>0.884</cell><cell>0.928</cell><cell>0.549</cell><cell>0.879</cell></row><row><cell>Trio (Melody)</cell><cell>0.796</cell><cell>0.848</cell><cell>0.579</cell><cell>0.753</cell></row><row><cell>Trio (Bass)</cell><cell>0.829</cell><cell>0.880</cell><cell>0.565</cell><cell>0.773</cell></row><row><cell>Trio (Drums)</cell><cell>0.903</cell><cell>0.912</cell><cell>0.641</cell><cell>0.863</cell></row></table><note>accuracies calculated both with teacherforcing (i.e., next-step prediction) and full sampling. All values are reported on a held-out test set. A softmax temperature of 1.0 was used in all cases, meaning we sampled directly from the logits.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">Throughout, whenever we refer to a "fully-connected layer," we mean a simple affine transformation as in Eq. (6).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors wish to thank David Ha for inspiration and guidance. We thank Claire Kayacik and Cheng-Zhi Anna Huang for their assistance with the user study analysis. We thank Erich Elsen for additional editing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An information-theoretic analysis of deep latent-variable models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00464</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning stochastic recurrent networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Advances in Variational Inference</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Conference on Computational Natural Language Learning</title>
				<meeting>the Twentieth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Using Artificial Intelligence to Augment Human Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Distill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Variational</surname></persName>
		</author>
		<title level="m">Fifth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Latent constraints: Learning to generate conditionally from unconditional generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05772</idno>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Variational recurrent auto-encoders</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fabius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Van Amersfoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A latent variable model for natural images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Pixelvae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Oph a neural representation of sketch drawings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Faster and smaller language model queries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><surname>Kenlm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
				<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">betavae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gaussian distributions are soap bubbles. inFER-ENCe</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">International MIDI Association, T. General MIDI level 1 specification</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<title level="m">Progressive growing of gans for improved quality, stability, and variation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extracting ground-truth information from MIDI files: A MIDIfesto</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Society for Music Information Retrieval Conference</title>
				<meeting>the 17th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning Latent Representations of Music to Generate Interactive Musical Palettes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
		<editor>Eck, D.</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A hybrid convolutional variational autoencoder for text generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02390</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wavenet ; Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014. 2016a. 2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving the improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04468</idno>
		<title level="m">Sampling generative networks: Notes on a few effective techniques</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved variational autoencoders for text modeling using dilated convolutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
