<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongjun</forename><surname>Zhang</surname></persName>
							<email>zhangyj@whu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Huang</surname></persName>
							<email>xhuang@whu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Photogrammetry</orgName>
								<orgName type="department" key="dep2">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Remote Sensing</orgName>
								<orgName type="department" key="dep2">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Radio and Television Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Telecommunication and Information Engineering</orgName>
								<orgName type="laboratory">Nanjing Uni-versity of Posts and Telecommunications</orgName>
								<address>
									<postCode>210003</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Department of Communication Engineering</orgName>
								<orgName type="department" key="dep2">Elec-tronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A9078BF6188004C6E6994B18AD0B3CF</idno>
					<idno type="DOI">10.1109/TGRS.2017.2756911</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large-Scale Remote Sensing Image Retrieval by Deep Hashing Neural Networks</head><p>Yansheng Li , Yongjun Zhang , Xin Huang, Senior Member, IEEE, Hu Zhu, and Jiayi Ma</p><p>Abstract-As one of the most challenging tasks of remote sensing big data mining, large-scale remote sensing image retrieval has attracted increasing attention from researchers. Existing large-scale remote sensing image retrieval approaches are generally implemented by using hashing learning methods, which take handcrafted features as inputs and map the highdimensional feature vector to the low-dimensional binary feature vector to reduce feature-searching complexity levels. As a means of applying the merits of deep learning, this paper proposes a novel large-scale remote sensing image retrieval approach based on deep hashing neural networks (DHNNs). More specifically, DHNNs are composed of deep feature learning neural networks and hashing learning neural networks and can be optimized in an end-to-end manner. Rather than requiring to dedicate expertise and effort to the design of feature descriptors, we can automatically learn good feature extraction operations and feature hashing mapping under the supervision of labeled samples. To broaden the application field, DHNNs are evaluated under two representative remote sensing cases: scarce and sufficient labeled samples. To make up for a lack of labeled samples, DHNNs can be trained via transfer learning for the former case. For the latter case, DHNNs can be trained via supervised learning from scratch with the aid of a vast number of labeled samples. Extensive experiments on one public remote sensing image data set with a limited number of labeled samples and on another public data set with plenty of labeled samples show that the proposed remote sensing image retrieval approach based on DHNNs can remarkably outperform state-of-the-art methods under both of the examined conditions.</p><p>Index Terms-Deep hashing neural networks (DHNNs), large-scale remote sensing image retrieval, remote sensing big data (RSBD) mining, supervised learning from scratch, transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the rapid development of remote sensing obser- vation technologies, we have entered an era of remote sensing big data (RSBD) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. There is no doubt that RSBD contain invaluable information. Due to the large volume of RSBD, manual information extraction from RSBD is time consuming and prohibitive. Hence, useful information must be automatically drawn from RSBD. Driven by the demand from multiple fields (e.g., disaster rescue), automatic knowledge discovery from RSBD has become increasingly urgent. Among emerging RSBD mining efforts <ref type="bibr" target="#b0">[1]</ref>, content-based large-scale remote sensing image retrieval <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b7">[8]</ref> has attracted an increasing amount of research interest due to its broad applications.</p><p>In earlier remote sensing image retrieval systems, remote sensing image retrieval mainly relied on manual tags in terms of sensor types, waveband information, and geographical locations of remote sensing images. As a consequence, the retrieval performance of these systems was highly dependent on the availability and quality of manual tags. However, the manual generation of tags is often time consuming and becomes especially prohibitive when the volume of remote sensing images increases considerably. In fact, recent efforts show that the visual contents of remote sensing images themselves are more relevant than manual tags <ref type="bibr" target="#b8">[9]</ref>. Hence, researchers have begun to exploit ways to search through similar remote sensing images in terms of visual content. Specifically, Wang and Song <ref type="bibr" target="#b9">[10]</ref> used the spatial relationships of classification results to measure similarities between two remote sensing images. With this approach, however, image retrieval performance is highly dependent on classification accuracy levels. To avoid this dependence, numerous feature descriptors have been specifically designed for indexing remote sensing images. More specifically, local invariant <ref type="bibr" target="#b10">[11]</ref>, morphological <ref type="bibr" target="#b11">[12]</ref>, textural <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, and data-driven features <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref> have been evaluated in terms of content-based remote sensing image retrieval tasks. To further improve image retrieval performance levels, we have proposed a multiple feature-based remote sensing image retrieval approach <ref type="bibr" target="#b19">[20]</ref> that not only considers handcrafted features but also utilizes data-driven features via unsupervised feature learning <ref type="bibr" target="#b20">[21]</ref>. In addition, Wang et al. <ref type="bibr" target="#b21">[22]</ref> proposed a multilayered graph model for hierarchically refining retrieval results from coarse to fine. For the aforementioned methods, the visual contents of remote sensing images are often represented by thousands of dimensional feature descriptors. Exhaustively comparing the highdimensional feature descriptor of an inquiry remote sensing image with each image in a data set is computationally expensive and impossible to achieve when the volume of a data set is oversized.</p><p>To address the aforementioned problems with exhaustive high-dimensional feature searching, two strategies may be employed: improving search methods and reducing the dimensions of feature descriptors. The former strategy is implemented by using data partition algorithms that recursively split data spaces into subspaces and record these divisions via a tree structure. In benefiting from this data partitioning strategy, the search speed of tree-based methods <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> is significantly improved, but retrieval performance levels decrease dramatically, especially when the dimension of the original feature descriptor is very high <ref type="bibr" target="#b22">[23]</ref>. Unfortunately, the dimensions of feature descriptors of remote sensing images are often very high. To avoid this issue, several researchers have exploited feature reduction methods for large-scale remote sensing image retrieval. Recently, hashing learning methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> have been introduced into large-scale remote sensing image retrieval tasks. These hashing learning methods take handcrafted feature descriptors with dimensions that are often very high as an input and map high-dimensional feature vectors (HDFVs) to low-dimensional binary feature vectors (LDBFVs). Accordingly, the complexity of exhaustive searches using LDBFV is dramatically reduced relative to that of HDFV. Although existing hashing learning methods can significantly increase search speeds, retrieval accuracy levels still fail to meet the demands of practical applications. In view of the great successes of deep learning methods <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> in recently developed applications, replacing low-level handcrafted features of hashing learning methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> with highlevel semantic features of deep learning can further improve retrieval performance levels. To fully employ the respective merits of deep and hashing learning, deep hashing neural networks (DHNNs) <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref> have been proposed by pioneers of the computer vision community, and exciting results of large-scale natural image retrieval tasks have been retrieved. Generally, remote sensing images differ considerably from natural images in both spectral and spatial domains. Due to this substantial gap, DHNNs trained in a natural image data set cannot be applied directly to large-scale remote sensing image retrieval tasks. Hence, the modeling and learning of DHNNs based on specific remote sensing image retrieval tasks deserve more exploration.</p><p>Based on the aforementioned considerations, this paper proposes a novel large-scale remote sensing image retrieval approach based on DHNNs. More specifically, this paper presents a comprehensive study of DHNNs and introduces DHNNs into large-scale remote sensing image retrieval tasks. To clarify fundamental theories of DHNNs, this paper provides a systematic review of existing DHNNs. Different from existing DHNNs studies <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>, this paper for the first time illustrates the importance of the similarity weight and quantization loss function of DHNNs. To cover as many cases as possible, DHNNs are utilized in two remote sensing situations: remote sensing data sets with limited and sufficient quantities of labeled samples. For the former case, the deep feature learning module of DHNNs can be derived from suitable pretrained neural networks, and the hashing learning module of DHNNs is randomly initialized; then, DHNNs can be incrementally trained using the limited number of labeled samples available. For the latter case, DHNNs can be randomly constructed based on the specific data characteristics of remote sensing images and then trained from scratch using a sufficient number of labeled samples. Compared to existing hashing learning methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> that have been applied to largescale remote sensing image retrieval, some recently presented hashing learning methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and three existing DHNN methods <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>, the DHNNs proposed in this paper can achieve significant performance improvements when applied to two public remote sensing image data sets, where one includes a limited number of labeled samples and the other contains a sufficient number of labeled samples. As a whole, the main contributions of this paper are twofold.</p><p>1) From a methodological perspective, this paper provides a systematic review of DHNNs and illustrates the importance of critical components of DHNNs that are disregarded in existing DHNNs. 2) In terms of applications, for the first time, DHNNs are employed for large-scale remote sensing image retrieval.</p><p>To cover as many remote sensing applications as possible, this paper illustrates ways to design and train DHNNs for large-scale remote sensing image retrieval when labeled samples are scarce and sufficient. This paper is organized as follows. A comprehensive review of DHNNs is given in Section II, where we also list key parameters of DHNNs that can significantly affect performance outcomes. In Section III, we introduce solutions for designing and training DHNNs for large-scale remote sensing image retrieval in cases involving scarce and sufficient numbers of labeled samples. Using two public remote sensing image data sets, the overall performance of the proposed approach based on DHNNs and comparisons with state-of-the-art approaches are reported in Section IV. Finally, Section V presents the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEEP HASHING NEURAL NETWORKS</head><p>In the last decade, deep learning <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> has achieved considerable success when applied to nearly all computer vision tasks due to its superiority in terms of feature representation. In the remote sensing community, deep learning methods have been successively utilized for remote sensing image scene classification <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b34">[35]</ref>, hyper-spectral image classification <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>, SAR image classification <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, remote sensing image object recognition <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, and so forth. Generally, the dimension of the feature vector output generated by these deep learning methods <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b41">[42]</ref> is often very high and may be acceptable for these processing tasks. However, large-scale image retrieval based on HDFVs is impossible, as noted above.</p><p>In tailoring deep learning techniques to large-scale image retrieval, DHNNs have been proposed in <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>. More specifically, DHNNs are composed of deep feature learning neural networks (DFLNNs) for high-level semantic feature representation and of hashing learning neural networks (HLNNs) for compact feature representation, and can be jointly optimized in an end-to-end manner. We note that joint optimization benefits render the feature representation and hashing mapping modules simultaneously optimal for a specific task.</p><p>To clearly describe the features of DHNNs, model formulations and learning paradigms for DHNNs are introduced in Sections II-A and II-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modeling of DHNNs</head><p>Based on existing approaches <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>, DHNNs can be represented by the integration of DFLNNs and HLNNs. More specifically, DFLNNs are composed of multiple convolutional and fully connected layers and pursue the high-level semantic feature representation of an input image scene. In addition, HLNNs can be constructed from one fully connected layer and aim at mapping the high-dimensional feature representation of DFLNNs for compact feature representation (i.e., the LDBFV). Unlike the high-dimensional feature representation of DFLNNs, the feature representation of DHNNs is extremely compact and can be applied to large-scale image retrieval tasks.</p><p>As depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, each image shares the same neural networks (i.e., DHNNs) throughout the compact feature representation process, and DHNNs can be optimized under constraints such as binary quantization loss and pairwise similarity constraints. More specifically, the binary quantization loss can render each element of the final feature representation of the DHNNs approach as -1 or 1, and the pairwise similarity constraint can cause similarities between feature representations of DHNNs to agree with real similarities based on manual labels of image scenes.</p><p>For an image data set {(I i , y i )|i = 1, 2, . . . , N}, where I i denotes the image and y i denotes its label, the similarity matrix ∈ R 2×N×N for the given image data set is specifically defined as</p><formula xml:id="formula_0">1 i, j + 2 i, j = 1, where 1 i, j = 1, if y i = y j and 1 i, j = 0, if y i = y j . Fig. 2.</formula><p>Visual comparison of different sigmoid functions. In the visual comparison, the length of the binary feature is set to 64, and the similarity factor is set to 0.25. In addition, the identical ratio is calculated by dividing the number of identical bits between two binary features by the length of the binary feature.</p><p>Assuming that low-dimensional binary vectors of the image data set</p><formula xml:id="formula_1">I = {I i } N i=1 can be represented by B = {b i } N i=1</formula><p>, where b i = {-1, 1} l and l denotes the length of the binary feature vector, the likelihood function of the pairwise similarity can be defined as</p><formula xml:id="formula_2">P 1 i, j = 1|B = σ ( i, j ) P 2 i, j = 1|B = 1 -σ ( i, j )<label>(1)</label></formula><p>where i, j = b T i b j and σ ( i, j ) = 1/(1+e -i, j ) is the classic sigmoid function that easily leads to a large saturation zone where its gradient is close to 0.</p><p>In the literature, the classic sigmoid function σ ( i, j ) = 1/(1 + e -i, j ) is adopted in <ref type="bibr" target="#b26">[27]</ref>, and the improved sigmoid function σ ( i, j ) = 1/(1 + e -i, j /2 ) is utilized in <ref type="bibr" target="#b28">[29]</ref>. However, both sigmoid functions adopted in <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b28">[29]</ref> would result in the generation of large saturation zone, which hinders the updating of network parameters through backpropagation. To avoid this result, this paper proposes the use of a weighted sigmoid function σ ( i, j ) = 1/(1 + e -i, j /w ), where w = s • l is the similarity weight, s is the similarity factor, and l is the length of the binary feature b. Fig. <ref type="figure" target="#fig_2">2</ref> intuitively shows why the proposed weighted sigmoid function can effectively decrease the saturation zone relative to the classic sigmoid function used in <ref type="bibr" target="#b26">[27]</ref> and the improved sigmoid function used in <ref type="bibr" target="#b28">[29]</ref>. For the case illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>, the classic and improved sigmoid functions should cause the objective optimization function used in (2) to enter the saturation zone when the identical ratio exceeds 0.6 or falls below 0.4. In contrast, the weighted sigmoid function can cause the objective optimization function to pursue a higher identical ratio when two remote sensing images share the same visual content and vice versa.</p><p>The ideal binary feature representations B = {b i } N i=1 are unknown in advance. Under the similarity matrix constraint , we can determine binary representations by minimizing the following cross-entropy function:</p><formula xml:id="formula_3">min B E = i, j ∈ 2 k=1 -k i, j log P k i, j = 1 B = i, j ∈ 1 i, j i, j + log(1 + e i, j ) . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>To draw a link between deep feature learning and hashing learning, we give the parameter formulation of DFLNNs and HLNNs in the following. Let denote all parameters of multilayers of DFLNNs, and let {W, v} denote the weights of HLNNs. For a given input image I i , the high-dimensional semantic feature representation of DFLNNs can be represented by d i = ϕ(I i ; ), where d i ∈ R d , and the continuous low-dimensional feature representation of HLNNs can be represented by</p><formula xml:id="formula_5">f i = W T d i + v = W T ϕ(I i ; ) + v, where f i ∈ R l , W ∈ R d×l , and v ∈ R l .</formula><p>To simultaneously optimize the DFLNNs and HLNNs, the optimization function shown in ( <ref type="formula" target="#formula_3">2</ref>) can be converted into min</p><formula xml:id="formula_6">B, ,W,v E 1 = i, j ∈ 1 i, j ϒ i, j + log(1 + e ϒ i, j ) + η N i=1 f i -b i 1 (3)</formula><p>where ϒ i, j = f T i f j /P, P is the similarity penalty, and η is the regularization coefficient. Using formula derivation, it is not difficult to see that P varies with the selection of sigmoid functions. The similarity penalty P is equal to 1, 2, and w = s • l when the classic sigmoid function in <ref type="bibr" target="#b26">[27]</ref>, the improved sigmoid function in <ref type="bibr" target="#b28">[29]</ref>, and the weighted sigmoid function are, respectively, adopted.</p><p>We note that the optimization function used in (3) takes the pairwise similarity constraint and the binary quantization loss function into consideration. Intuitively, the optimization function shown in ( <ref type="formula">3</ref>) is equivalent to that used in <ref type="bibr" target="#b3">(4)</ref>. As the optimization function used in (3) and ( <ref type="formula">4</ref>) uses the L1 norm to define the quantization loss, the corresponding DHNNs optimized by (3) or (4) are referred to as DHNNs-L1 in the following. In the proposed DHNNs-L1, the weighted sigmoid function is adopted and ϒ i, j in ( <ref type="formula">4</ref>) is equal to f T i f j /w, where w = s • l is the similarity weight. In contrast, the existing deep hashing method used in <ref type="bibr" target="#b26">[27]</ref> employs the classic sigmoid function, which renders ϒ i, j used in (4) equal to f T i f j . The binary quantization loss from the L1 norm is also adopted in <ref type="bibr" target="#b27">[28]</ref> min</p><formula xml:id="formula_7">,W,v E 1 = i, j ∈ 1 i, j + ϒ i, j + log(1 + e ϒ i, j ) + η N i=1 |f i | -1 1 . (4)</formula><p>Unlike the function used in (3) and ( <ref type="formula">4</ref>), the optimization function used in <ref type="bibr" target="#b4">(5)</ref> employs the square of the L2 norm to define the quantization loss. In the following, the DHNNs optimized by ( <ref type="formula">5</ref>) are referred to as DHNNs-L2. Unlike the proposed DHNNs-L2, the existing deep hashing approach used in <ref type="bibr" target="#b28">[29]</ref> adopts the improved sigmoid function, rendering ϒ i, j in <ref type="bibr" target="#b4">(5)</ref> </p><formula xml:id="formula_8">equal to f T i f j /2 min B, ,W,v E 2 = i, j ∈ 1 i, j ϒ i, j + log(1 + e ϒ i, j ) + η N i=1 f i -b i 2 2 . (5)</formula><p>As noted above, we comprehensively review DHNN methods <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref> employed in the literature under the crossentropy optimization framework employed in <ref type="bibr" target="#b1">(2)</ref>. In diverging from prior efforts, the importance of the similarity weight w is revealed for the first time. In addition, we evaluate the final performance of DHNNs when applied under different quantization loss functions.</p><p>In Section II-B, ways to learn DHNNs-L1 and DHNNs-L2 from (3) and ( <ref type="formula">5</ref>) are demonstrated in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DHNN Learning</head><p>Given that the volume of training samples is generally very large, we adopt a batch-based learning strategy widely adopted in deep learning to optimize DHNNs-L1 used in (3) and DHNNs-L2 used in <ref type="bibr" target="#b4">(5)</ref>. More specifically, for each iteration, we sample a batch of data to learn parameters until all data are processed. As B and { , W, v} are dependent on one another in (3) or ( <ref type="formula">5</ref>), we adopt an alternative way to learn them. Therefore, one parameter is updated while other parameters remain fixed.</p><p>Regardless of whether we optimize DHNNs-L1 or DHNNs-L2, binary feature vectors B = {b i } N i=1 should be first estimated based on neural network parameters { , W, v}</p><formula xml:id="formula_9">b i = sign(f i ) = sign(W T ϕ(I i ; ) + v)<label>(6)</label></formula><p>where sign(•) maps each element of the feature vector to -1 or 1 based on the sign of the given element.</p><p>To learn neural network parameters via the backpropagation algorithm, we must compute derivatives of the optimization function. In the following, we, respectively, give the derivatives of optimization functions used in (3) and <ref type="bibr" target="#b4">(5)</ref>.</p><p>To learn the parameters employed in DHNNs-L1, the derivative of the optimization function used in (3) with respect to f i should be computed as illustrated in <ref type="bibr" target="#b6">(7)</ref>. The optimization function used in (3) with respect to f i is nondifferentiable due to its use of the L1 norm. As noted in <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b6">(7)</ref> gives derivatives on multiple intervals that can be written as</p><formula xml:id="formula_10">∂ E 1 ∂f m i = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ j : i, j ∈ σ f T i f i /(s • l) -1 i, j f m j + η, f m i ≥ 1 j : i, j ∈ σ f T i f i /(s • l) -1 i, j f m j + η, -1 ≤ f m i ≤ 0 j : i, j ∈ σ f T i f i /(s • l) -1 i, j f m j -η, otherwise<label>(7)</label></formula><p>where l is the length of f i and m = 1 : l. • Compute the high-dimensional feature from d i = ϕ(I i ; ) by forward propagation;</p><p>• Calculate the low-dimensional binary feature from b i = sign(W T d i + v) using Eq. ( <ref type="formula" target="#formula_9">6</ref>); • Calculate derivatives of the optimization function using</p><p>Eq. ( <ref type="formula" target="#formula_10">7</ref>) -Eq. ( <ref type="formula" target="#formula_11">10</ref>); • Update weights { , W, v} based on the derivatives via back propagation; Continue until all images are processed over a fixed number of iterations Furthermore, we can calculate derivatives of (3) with respect to { , W, v}, which can refer to the following:</p><formula xml:id="formula_11">∂ E 1 ∂ϕ(I i ; ) = W ∂ E 1 ∂f i (8) ∂ E 1 ∂W = ϕ(I i ; ) ∂ E 1 ∂f i T (9) ∂ E 1 ∂v = ∂ E 1 ∂f i . (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>To illustrate, we summarize the optimization process employed for DHNNs-L1 as Algorithm 1.</p><p>In the following, we give the optimization solution for DHNNs-L2. As for the optimization process for DHNNs-L1, we must determine the derivative of the optimization function used in <ref type="bibr" target="#b4">(5)</ref> with respect to f i . In benefiting from the L2 norm, the optimization function used in <ref type="bibr" target="#b4">(5)</ref> with respect to f i is differentiable. More specifically, the closed-form gradient is as follows:</p><formula xml:id="formula_13">∂ E 2 ∂f i = j : i, j ∈ σ f T i f j /(s • l) -1 i, j f m j + 2η(f i -b i ). (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>Based on the gradient result shown in <ref type="bibr" target="#b10">(11)</ref>, derivatives of the optimization function shown in <ref type="bibr" target="#b4">(5)</ref> with respect to { , W, v} can be computed from</p><formula xml:id="formula_15">∂ E 2 ∂ϕ(I i ; ) = W ∂ E 2 ∂f i (<label>12</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">∂ E 2 ∂W = ϕ(I i ; ) ∂ E 2 ∂f i T (<label>13</label></formula><formula xml:id="formula_18">)</formula><formula xml:id="formula_19">∂ E 2 ∂v = ∂ E 2 ∂f i . (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>To avoid confusing this process with the optimization process employed for the DHNNs-L1, we summarize the optimization process of DHNNs-L2 as Algorithm 2. As illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>, the proposed large-scale remote sensing image retrieval approach based on the DHNNs involves two stages: a training stage and a testing stage. In the training stage, the DHNNs should be trained offline using labeled remote sensing images. In the testing stage, based on the DHNNs learned from the training stage, low-dimensional binary features of the given remote sensing images can be computed based on <ref type="bibr" target="#b5">(6)</ref>. As illustrated by the testing stage presented in Fig. <ref type="figure" target="#fig_3">3</ref>, the large-scale remote sensing image retrieval task is transformed into a feature-searching problem. As noted above, the final feature representation of the DHNNs is very compact. In benefiting from this characteristic, the large-scale remote sensing image retrieval task can be easily implemented via exhaustive feature similarity comparisons, where similarities between binary features can be efficiently computed from the hamming distance <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b30">[31]</ref>. As final features of the remote sensing image generated from the DHNNs are very compact, features of remote sensing images in the large-scale remote sensing image data set can be computed in advance and then saved as the feature data set without incurring considerable storage costs. Hence, in the retrieval stage, feature extraction time dedicated to the largescale remote sensing image data set can be saved, and it is only necessary to compute the feature representation of the inquiry image based on the DHNNs.</p><p>It is well known that deep learning-based methods are often dependent on the use of millions of labeled samples to learn complex neural network parameters <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. The DHNNs discussed in this paper also suffer from this problem. Hence, the performance of DHNNs depends heavily on the volume of labeled samples. To broader DHNNs applications, Sections III-A and III-B present ways to design and train DHNNs under two typical cases for which the number of labeled remote sensing samples available is limited or sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Large-Scale Remote Sensing Image Retrieval by Virtue of Limited Number of Labeled Samples</head><p>In the majority of remote sensing applications, large numbers of remote sensing images are available, but labeled images are very rare. In such cases, fully learning convolutional neural networks (CNNs) from scratch is impossible. In the literature, several efforts have been made to transfer CNNs that have been pretrained in a large-scale natural image data set (e.g., ImageNet) <ref type="bibr" target="#b42">[43]</ref> to remote sensing image tasks of scene classification <ref type="bibr" target="#b33">[34]</ref>, object recognition <ref type="bibr" target="#b38">[39]</ref>, and so on.</p><p>Inspired by such successful experiences <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we train DHNNs via transfer learning when the number of labeled remote sensing images available is very limited. More specifically, we expect to transfer CNNs pretrained on the source domain (e.g., the natural image object recognition task) to the target domain (i.e., the remote sensing image retrieval task). To this end, the DFLNNs of DHNNs can inherit from suitable pretrained CNNs (e.g., the one pretrained on ImageNet), and the HLNNs of DHNNs can be randomly initialized based on the size of the adopted DFLNNs. Furthermore, the constructed DHNNs can be incrementally trained by applying Algorithm 1 or Algorithm 2 under the supervision of a limited number of labeled remote sensing images. As the weights of DHNNs mainly concentrate on DFLNNs, a relatively strong DFLNNs initialization can decrease the optimization difficulty of DHNNs. In benefiting from the reuse of CNNs, the advocated DHNNs can be trained to achieve strong levels of generalization performance, even when the number of labeled remote sensing images available is very limited.</p><p>As a precondition to the success of this transfer learning strategy, the remote sensing image in the target domain relatively resembles the image in the source domain in terms of spectral ranges and spatial resolutions. In the training and testing stages, the remote sensing image in the target domain must be projected to the size of the image in the source domain to reuse CNNs trained in the source domain. Although the projection may lose some information on remote sensing images, this approach is still very cost effective when the remote sensing image adopted is similar to natural images. This strategy is verified for a public aerial image data set <ref type="bibr" target="#b43">[44]</ref>, and corresponding results are shown in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Large-Scale Remote Sensing Image Retrieval With the Aid of a Sufficient Number of Labeled Samples</head><p>We note that the aforementioned transfer learning strategy for DHNNs may decline in efficacy when the remote sensing image used is significantly different from the image in the source domain. As is well known, remote sensing images include much more spectral channels than natural images do. Hence, remote sensing images include even more cues that can be used in image analyses than natural ones do. When transferring CNNs pretrained on a natural image data set to construct the DFLNNs of DHNNs, only three RGB spectral channels of remote sensing images are used for feature representation, while the rich spectral information of remote sensing images is disregarded.</p><p>Along with the great successes of deep learning, more and more researchers have realized the importance of labeled samples. Accordingly, the remote sensing image data set with large volumes of labeled samples <ref type="bibr" target="#b44">[45]</ref> has been released. In particular, a large-scale remote sensing image data set with manual labels is available. However, to our knowledge, no report has illustrated the feasibility of joint deep feature and hashing learning for remote sensing image data sets. To allow rich annotation information of remote sensing images to generate good yields, we attempt to specifically design and train DHNNs for remote sensing images from scratch. The solution proposed is verified based on one public satellite image data set <ref type="bibr" target="#b44">[45]</ref>, where each image contains four RGB-near infrared (NIR) spectral channels, and corresponding results are presented in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>Section IV-A introduces widely adopted evaluation criteria used for large-scale remote sensing image retrieval. Section IV-B provides an example that shows how DHNNs are designed and trained when the number of labeled samples available is very limited. In reference to such conditions, the overall performance of DHNNs and its performance relative to other approaches are reported. With the support of plenty of labeled samples, Section IV-C illustrates the means of designing and training DHNNs and reports on the overall performance of DHNNs and compares this performance with those of state-of-the-art approaches. Finally, Section IV-D provides a brief discussion of the experimental results and describes our future work related to DHNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Criteria</head><p>In this paper, large-scale remote sensing image retrieval performance is quantitatively evaluated using the following two widely adopted metrics <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b30">[31]</ref>: the mean average precision (MAP) and the precision-recall curve. More specifically, the MAP score can be computed from where q i ∈ Q is the inquiry image, |Q| denotes the volume of the inquiry image data set, and n i is the number of images relevant to q i in the searching image data set. Assuming that relevant images are ordered as {r 1 , r 2 , . . . r n i } across images in the searching image data set, R j i is the set of ranked results from the 1-st result to the r j -th result.</p><formula xml:id="formula_21">MAP = 1 |Q| |Q| i=1 1 n i n i j =1 precision R j i (<label>15</label></formula><formula xml:id="formula_22">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on the Data Set With a Limited Number of Labeled Samples 1) Evaluation Data Set:</head><p>In this paper, we take the publicly available University of California, Merced remote sensing image data set (UCMD) <ref type="bibr" target="#b43">[44]</ref> to demonstrate how to design and train DHNNs from a limited number of labeled samples. The UCMD is generated by manually labeling aerial image scenes, and it covers 21 land cover categories. More specifically, each land cover category includes 100 images of 256 × 256 pixels, the spatial resolution of each pixel is 30 cm, and each pixel is measured in the RGB spectral space. Four representative images of each category of the UCMD are visually shown in Fig. <ref type="figure" target="#fig_4">4</ref>. We note that the UCMD has been widely used for the performance evaluation of remote sensing image retrieval <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref> and remote sensing image scene classification <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b34">[35]</ref> efforts. Hence, the UCMD is a representative remote sensing image data set that includes a limited number of labeled samples.</p><p>2) Experimental Setup: To slightly augment the volume of the UCMD, each image from the UCMD is rotated by 90°, 180, and 270°. This strategy has been widely adopted to enlarge data sets without any manual labor <ref type="bibr" target="#b33">[34]</ref> and can increase the size of a UCMD by a factor of 4. In the following, we describe experiments conducted on the augmented UCMD containing 8400 images. Furthermore, the inquiry image data set is composed of 1000 images randomly sampled from the augmented UCMD, and the others are taken as searching and training image data sets with a volume of 7400.</p><p>In this experiment, the DFLNNs of DHNNs are constructed by transferring the CNNs pretrained on ImageNet <ref type="bibr" target="#b45">[46]</ref> based on the fact that the aerial image of the UCMD resembles the natural image included in ImageNet in terms of spectral ranges and spatial resolutions, and the HLNNs of DHNNs are randomly initialized based on the output size of the DFLNNs. The specific configuration of the transferred DFLNNs is shown in Table <ref type="table" target="#tab_0">I</ref>, and the DFLNNs can process an input image of 224 × 224 × 3. In Table <ref type="table" target="#tab_0">I</ref>, "filter" specifies the number of filters, the size of a field, and the dimensions of input data, and it can be formulated as num × size × size × dim. "stride1" denotes the sliding step of the convolution operation. "pool" denotes the down sampling factor. "stride2" denotes the sliding step of the local pooling operation. Furthermore, the constructed DHNNs are incrementally optimized by Algorithm 1 or Algorithm 2 from the training aerial image data set. To distinguish between optimization algorithms, DHNNs-L1 denotes the DHNNs optimized by Algorithm 1, and DHNNs-L2 denotes the DHNNs optimized by Algorithm 2. In the incremental optimization process, the DFLNNs and HLNNs of DHNNs can be jointly updated under the supervision of the training aerial image data set.</p><p>3) Overall Performance of the DHNNs: In this section, we explore the performance of DHNNs-L1 and DHNNs-L2 and the sensitivity of key parameters, including the similarity factor and regularization coefficient. In this experiment, the length of the final hashing feature is set to 64. The inquiry aerial image data set contains 1000 images, and the searching aerial image data set includes 7400 images. Based on this experimental setting, Table II reports the image retrieval performance of DHNNs-L1, and the retrieval performance is measured based on the MAP value. In addition, Table <ref type="table" target="#tab_0">II</ref> presents sensitivity analysis results for key parameters, including the similarity factor s and the regularization coefficient η. In addition, Table III illustrates the image retrieval performance of DHNNs-L2 based on two critical parameters.</p><p>As illustrated in Tables <ref type="table" target="#tab_0">II</ref> and<ref type="table" target="#tab_0">III</ref>, DHNN-L2 performs better than DHNNs-L1. More specifically, DHNNs-L2 achieves the  best remote sensing image retrieval outcomes when the similarity factor is set to 0.50 and the regularization coefficient is equal to 5.0e1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Comparisons With State-of-the-Art Approaches:</head><p>With the similarity factor and regularization coefficient in DHNNs-L2 fixed, we report MAP values of our proposed DHNNs-L2 for different hashing feature lengths in Table <ref type="table" target="#tab_2">IV</ref>. To show the superiority of the adopted DHNNs-L2, we compare it with state-of-the-art approaches, including two existing large-scale remote sensing image retrieval approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, two recently developed hashing learning methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and three existing DHNNs methods <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>. More specifically, the large-scale remote sensing image retrieval method based on partial randomness hashing (PRH) <ref type="bibr" target="#b6">[7]</ref>, the large-scale remote sensing image retrieval method based on kernel-based supervised hashing (KSH) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b46">[47]</ref>, the potential method based on supervised discrete hashing (SDH) <ref type="bibr" target="#b29">[30]</ref>, and the candidate method based on column sampling-based discrete supervised hashing (COSDISH) <ref type="bibr" target="#b30">[31]</ref> are reimplemented or provided by the authors. These approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> take the 512-D GIST feature <ref type="bibr" target="#b47">[48]</ref> as an input for hashing learning methods. To illustrate the benefits of the proposed DHNNs-L2, we also compare it with existing DHNNs models, including the deep hashing network (DHN) <ref type="bibr" target="#b26">[27]</ref>, deep supervised hashing (DSH) <ref type="bibr" target="#b27">[28]</ref>, and deep pairwise-supervised hashing (DPSH) <ref type="bibr" target="#b28">[29]</ref>. Experimental parameters are set according to suggestions made in corresponding papers. To illustrate the superiority of the optimization function of the proposed DHNNs-L2, the DHN <ref type="bibr" target="#b26">[27]</ref>, DSH <ref type="bibr" target="#b27">[28]</ref>, and DPSH <ref type="bibr" target="#b28">[29]</ref> are based on the same deep network architecture of the proposed DHNNs-L2. As shown in Table <ref type="table" target="#tab_2">IV</ref>, we can easily conclude that the proposed DHNNs-L2 can clearly outperform other state-of-the-art approaches.</p><p>To further illustrate aerial image retrieval performance outcomes, we present precision-recall curves of DHNNs-L2 and of other approaches. Fig. <ref type="figure" target="#fig_5">5</ref> shows the precision-recall curves of methods based on different hashing feature lengths. As illustrated in Fig. <ref type="figure" target="#fig_5">5</ref>, DHNNs-L2 significantly outperforms the other approaches.</p><p>In addition to the above quantitative comparison with stateof-the-art approaches, we draw intuitive comparisons, as illustrated in Fig. <ref type="figure" target="#fig_6">6</ref>. For this visual comparison, the hashing feature length of all methods is set to 96, and all methods use the same inquiry image and the same search image data set. In Fig. <ref type="figure" target="#fig_6">6</ref>, the aerial scene containing storage tanks is taken as the inquiry image, and retrieval results of different methods are shown. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, DHNNs-L2 clearly outperforms other methods and retrieves true aerial images, even in the midst of considerable appearance variations. Due to space limitations, we only provide one visual retrieval example, though DHNNs-L2 applies to other cases as reflected in the comprehensive results shown in Table <ref type="table" target="#tab_2">IV</ref> and Fig. <ref type="figure" target="#fig_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on the Data Set With Oversized</head><p>Labeled Samples 1) Evaluation Data Set: In this section, we use a public satellite image data set based on four land cover categories (SAT4) <ref type="bibr" target="#b44">[45]</ref> as a case to explore the feasibility of jointly learning deep feature representation and hashing mapping  <ref type="bibr" target="#b6">[7]</ref>. (c) KSH retrieval results presented in <ref type="bibr" target="#b7">[8]</ref>. (d) SDH retrieval results presented in <ref type="bibr" target="#b29">[30]</ref>. (e) COSDISH retrieval results presented in <ref type="bibr" target="#b30">[31]</ref>. (f) DHN retrieval results presented in <ref type="bibr" target="#b26">[27]</ref>. (g) DSH retrieval results presented in <ref type="bibr" target="#b27">[28]</ref>. (h) DPSH retrieval results presented in <ref type="bibr" target="#b28">[29]</ref>. (i) Retrieval results of our DHNNs-L2. The 1st, 5th, 10th, 15th, 20th, 30th, 40th, and 50th retrieval results of each method are shown. In addition, false retrieval results are marked with red rectangles. functions from scratch. Images in the SAT4 were drawn from the National Agriculture Imagery Program. Each image in the SAT4 includes 28 × 28 pixels, the spatial resolution of each pixel is 1 m, and each pixel is measured in the RGB-NIR spectral space. In addition, the SAT4 includes 500 000 images covering four land cover categories (barren land, trees,  grassland, and all land cover types other than the former three classes). Visual samples drawn from the SAT4 are shown in Fig. <ref type="figure">7</ref>.</p><p>2) Experimental Setup: From this experiment, we randomly selected 1000 images from the SAT4 as an inquiry image data set, and others were used as a searching and training image data sets with a volume of 499 000. Hence, it was sufficient to learn a specific deep neural network aiming at given types of satellite images under the supervision of this training satellite image data set. In addition, the inquiry and searching image data sets were further used to evaluate image retrieval performance outcomes.</p><p>As the satellite image was measured in the RGB-NIR spectral space and the size of the image is relatively small, Table <ref type="table" target="#tab_3">V</ref> presents the architecture of the DFLNN specifically designed for such satellite images. As shown in Table <ref type="table" target="#tab_3">V</ref>, the architecture contains three convolutional layers and two fully connected layers and is relatively compact compared to the ImageNet network. We note that the architecture given in Table <ref type="table" target="#tab_3">V</ref> is just one of the many candidates. This paper merely introduces a general solution for designing DFLNNs and for further constructing DHNNs. More DFLNNs architectures can be explored and evaluated in future works. Under the applied experimental setting, both the DFLNNs and HLNNs of the DHNNs were randomly initialized. Furthermore, we can use Algorithm 1 or Algorithm 2 to train it from scratch using the training satellite image data set.</p><p>3) Overall Performance of the DHNNs: In this experiment, we used a training image data set of 499 000 images to train the DHNNs from scratch using different optimization algorithms. In the following, DHNNs-L1 is the constructed DHNNs optimized by Algorithm 1, and DHNNs-L2 is the As shown in Tables VI and VII, DHNNs-L2 performs better than DHNNs-L1. DHNNs-L2 can achieve the best satellite image retrieval performance outcomes when the similarity factor s is set to 0.75 and the regularization coefficient η is equal to 1.0e2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Comparisons With State-of-the-Art Approaches:</head><p>According to the sensitivity analysis of the similarity factor and the regularization coefficient shown in Section IV-C-3, the similarity factor s and regularization coefficient η of the DHNNs-L2 are set as 0.75 and 1.0e2, respectively. Furthermore, Table VIII reports the accuracy of DHNNs-L2 when a different hashing feature length l is adopted. To illustrate the superiority of DHNNs-L2, we also present the accuracy of the following seven state-of-the-art approaches: PRH <ref type="bibr" target="#b6">[7]</ref>, KSH <ref type="bibr" target="#b7">[8]</ref>, SDH <ref type="bibr" target="#b29">[30]</ref>, COSDISH <ref type="bibr" target="#b30">[31]</ref>, DHN <ref type="bibr" target="#b26">[27]</ref>, DSH <ref type="bibr" target="#b27">[28]</ref>, and DPSH <ref type="bibr" target="#b28">[29]</ref>. These shallow hashing methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> used the 512-D GIST feature <ref type="bibr" target="#b47">[48]</ref> as an input. In addition, these deep hashing methods <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref> use the same deep network architecture as that employed for the proposed DHNNs-L2. For the comparisons, all methods employ the same inquiry and searching data sets. As shown in Table <ref type="table" target="#tab_5">VIII</ref>, the proposed DHNNs-L2 achieves significant satellite image retrieval performance improvements relative to other existing methods.</p><p>To clearly show image retrieval performance variations of the different methods, we report the precision-recall curves of DHNNs-L2 and of other approaches. More specifically, Fig. <ref type="figure">8</ref> reports the precision-recall curves of the different methods for different hashing feature lengths. As shown in Fig. <ref type="figure">8</ref>, the proposed DHNNs-L2 significantly outperforms the other approaches.</p><p>For the same hashing feature length l = 96, we report the visual retrieval results of DHNNs-L2 and other approaches in Fig. <ref type="figure">9</ref>. As a whole, the quantitative and qualitative results illustrate the superiority of the proposed DHNNs-L2.</p><p>There is no doubt that the feature-searching module can be efficiently applied through the utilization of hashing Fig. <ref type="figure">7</ref>. RGB channel visualization of the adopted SAT4. More specifically, SAT4 covers four land cover categories, and 24 images of each category, randomly selected from the SAT4, are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE VIII MAP VALUES OF DHNNS-L2 AND OTHER APPROACHES ON SAT4</head><p>features <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In practice, the efficient extraction of hashing features from images is very challenging. Fortunately, the proposed DHNNs can be easily applied with the use of parallel hardware. In this paper, the proposed DHNNs-L2 is implemented via GPU. The proposed DHNNs can extract hashing features of dozens of aerial images of the UCMD per second and can output hashing features of hundreds of satellite images of the SAT4 each second. As a whole, the proposed DHNNs-L2 is accurate and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion and Avenues for Future Research</head><p>In the aforementioned experiments, the two remote sensing image data sets used (i.e., the UCMD and SAT4) represent two typical remote sensing image retrieval task conditions. Under these two different conditions, DHNNs can be designed and learned under a unified framework. Our two representative experiments fully show the generalization of the proposed DHNNs-L2. In addition, the experiments show that the proposed DHNNs-L2 can achieve significant performance improvements relative to the outcomes of two existing largescale remote sensing image retrieval approaches <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, two potential approaches based on recent hashing learning methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and three existing deep hashing methods <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>.</p><p>In future work, we will explore ways to train DHNNs from scratch using large-scale labeled data with noisy, possibly  <ref type="bibr" target="#b6">[7]</ref>. (c) KSH retrieval results presented in <ref type="bibr" target="#b7">[8]</ref>. (d) SDH retrieval results presented in <ref type="bibr" target="#b29">[30]</ref>. (e) COSDISH retrieval results presented in <ref type="bibr" target="#b30">[31]</ref>. (f) DHN retrieval results presented in <ref type="bibr" target="#b26">[27]</ref>. (g) DSH retrieval results presented in <ref type="bibr" target="#b27">[28]</ref>. (h) DPSH retrieval results presented in <ref type="bibr" target="#b28">[29]</ref>. (i) Retrieval results of our DHNNs-L2. The 1st, 5th, 10th, 15th, 20th, 25th, 30th, 35th, 40th, 45th, and 50th retrieval results of each method are shown. In addition, false retrieval results are marked with red rectangles. incorrect labels. These data are often generated at a relatively low cost. For example, remote sensing images can be efficiently labeled through crowd-sourcing <ref type="bibr" target="#b48">[49]</ref>, but labeled data can contain a certain number of incorrect labels <ref type="bibr" target="#b49">[50]</ref>. Guided by the geography information system, remote sensing images can also be labeled automatically with the cost of a certain number of alignment errors <ref type="bibr" target="#b50">[51]</ref>. Hence, DHNNs training from noisy labeled data should be very cost effective.</p><p>As noted above, DHNNs can output the compact semantic feature representation of an input remote sensing image in urgent need of remote sensing image interpretation. Hence, we plan to explore more applications of DHNNs such as hyper-spectral image classification <ref type="bibr" target="#b51">[52]</ref>, image matching and registration <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, information fusion <ref type="bibr" target="#b54">[55]</ref>, built-up area detection <ref type="bibr" target="#b55">[56]</ref>, urban village detection <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, and land cover recognition <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Due to an urgent need for RSBD mining, large-scale remote sensing image retrieval has attracted increasing attention. Although several efforts have been made to address issues of large-scale remote sensing image retrieval, this task remains a very challenging problem. This paper is the first to advocate the use of DHNNs to address this problem.</p><p>We conduct a comprehensive study of DHNN systems. Based on the general cross-entropy theory, we provide a systematic review of existing DHNN methods. This paper is the first to highlight the importance of the similarity weight, which is set to a constant and disregarded in existing works. To broaden the applications of DHNNs, we adapt DHNNs to two representative remote sensing cases where the remote sensing data set includes either a limited number of labeled samples or plenty of labeled samples. For these two conditions, we present the means to design and train DHNNs. Extensive experiments conducted on one public aerial image data set and one public satellite image data set demonstrate that the proposed large-scale remote image retrieval approach based on the adjusted DHNNs can remarkably outperform state-ofthe-art approaches.</p><p>Large-scale remote sensing image retrieval methods and DHNNs should be increasingly adapted to address the requirements of more and more practical applications. To facilitate this, we present potential avenues for future research on DHNNs from method optimization and application perspectives. In future work, we plan to explore ways to train DHNNs using labeled data containing a certain number of errors from scratch, as such data can often be generated at a low cost. In addition, we plan to exploit the feasibility of applying DHNNs to more remote sensing image interpretation applications. Broadly speaking, DHNNs and their future extensions could realize new solutions for a broad range of remote sensing applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Visualization of DHNNs and corresponding learning constraints. Subcomponents of DHNNs, including DFLNNs and HLNNs, are also shown.</figDesc><graphic coords="3,53.99,58.85,240.02,198.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Optimization Process for DHNNs-L1 Input: Training images I = {I i } N i=1 with the pairwise similarity matrix ; Output: Weights for DHNNs-L1 { , W, v} and by-product binary features B; Repeat Randomly sample a batch of images from the training images. For each image I i in the sampled batch, execute the following operations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2</head><label>2</label><figDesc>Optimization Process for DHNNs-L2 Input: Training images I = {I i } N i=1 with the pairwise similarity matrix ; Output: Weights for DHNNs-L2 { , W, v} and by-product binary features B; Repeat Randomly sample a batch of images from the training images. For each image I i in the sampled batch, execute the following operations: • Compute the high-dimensional feature by d i = ϕ(I i ; ) by forward propagation; • Calculate the low-dimensional binary feature b i = sign(W T d i + v) from Eq. (6); • Calculate derivatives of the optimization function from Eq. (11) -Eq. (14); • Update weights { , W, v} • based on the derivatives by back propagation; Continue until all images are processed with a fixed number of iterations III. LARGE-SCALE REMOTE SENSING IMAGE RETRIEVAL VIA DEEP HASHING NEURAL NETWORKS In this section, we propose a novel large-scale remote sensing image retrieval approach based on the aforementioned DHNNs composed of DFLNNs and HLNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Flowchart of the proposed large-scale remote sensing image retrieval approach based on DHNNs. The proposed approach involves training and testing stages. More specifically, the training stage involves learning DHNNs, and the testing stage addresses large-scale remote sensing image retrieval based on the DHNNs learned in the training stage.</figDesc><graphic coords="6,77.99,58.61,456.02,306.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the UCMD. The UCMD covers 21 land cover categories, and four images of each category randomly selected from the UCMD are shown.</figDesc><graphic coords="7,84.47,58.73,442.22,346.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance of DHNNs-L2 and other methods when applied with different hashing feature lengths on UCMD. (a) Performance when l = 32. (b) Performance when l = 64. (c) Performance when l = 96.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visual image retrieval results of different methods examined. (a) Inquiry aerial image of the storage tanks category. (b) PRH retrieval results presented in<ref type="bibr" target="#b6">[7]</ref>. (c) KSH retrieval results presented in<ref type="bibr" target="#b7">[8]</ref>. (d) SDH retrieval results presented in<ref type="bibr" target="#b29">[30]</ref>. (e) COSDISH retrieval results presented in<ref type="bibr" target="#b30">[31]</ref>. (f) DHN retrieval results presented in<ref type="bibr" target="#b26">[27]</ref>. (g) DSH retrieval results presented in<ref type="bibr" target="#b27">[28]</ref>. (h) DPSH retrieval results presented in<ref type="bibr" target="#b28">[29]</ref>. (i) Retrieval results of our DHNNs-L2. The 1st, 5th, 10th, 15th, 20th, 30th, 40th, and 50th retrieval results of each method are shown. In addition, false retrieval results are marked with red rectangles.</figDesc><graphic coords="10,71.99,58.01,467.06,576.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Performance of DHNNs-L2 and other methods when applied with different hashing feature lengths on SAT4. (a) Performance when l = 32. (b) Performance when l = 64. (c) Performance when l = 96.</figDesc><graphic coords="13,89.99,234.41,431.78,427.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,84.47,58.73,442.22,344.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CONFIGURATION</head><label>I</label><figDesc>OF DFLNN ON UCMD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV MAP</head><label>IV</label><figDesc>VALUES OF DHNNS-L2 AND OTHER APPROACHES ON UCMD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V CONFIGURATION</head><label>V</label><figDesc>OF DFLNN ON SAT4</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI MAP</head><label>VI</label><figDesc>VALUES OF DHNNS-L1 UNDER DIFFERENT PARAMETERS ON SAT4</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII MAP</head><label>VII</label><figDesc>VALUES OF DHNNS-L2 UNDER DIFFERENT PARAMETERS ON SAT4 constructed DHNNs optimized by Algorithm 2. With the hashing feature length set to 64, Table VI illustrates the satellite image retrieval accuracy of DHNNs-L1 equipped with two parameters, including the similarity factor s and regularization coefficient η. Table VII reports the satellite image retrieval accuracy of DHNNs-L2 under two key parameters.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 41601352, Grant 41322010, Grant 41522110, and Grant 61503288, in part by the China Postdoctoral Science Foundation under Grant 2016M590716 and Grant 2017T100581, and in part by the Fundamental Research Funds for the Central Universities under Grant 2042016KF0054.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Big data for remote sensing: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="2207" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Remote sensing big data computing: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generat. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimating the statistical characteristics of remote sensing big data in the wavelet transform domain</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Topics Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="324" to="337" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entropybalanced bitmap tree for shape-based object retrieval from large-scale satellite imagery databases</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Klaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-R</forename><surname>Shyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1603" to="1616" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated feature generation in large-scale geospatial libraries for content-based indexing</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Tobin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="531" to="540" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GeoIRIS: Geospatial Information Retrieval and Indexing System-Content mining, semantics modeling, and complex queries</title>
		<author>
			<persName><forename type="first">C.-R</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Barb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Palaniappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="839" to="852" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Partial randomness hashing for large-scale remote sensing image retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="464" to="468" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hashing-based scalable remote sensing image search and retrieval in large archives</title>
		<author>
			<persName><forename type="first">B</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="892" to="904" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel active learning method in relevance feedback for content-based remote sensing image retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2323" to="2334" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Remote sensing image retrieval by scene semantic matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2874" to="2886" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geographic image retrieval using local invariant features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="818" to="832" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Remote sensing image retrieval with global morphological texture descriptors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3023" to="3034" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Remote sensing imagery retrieval based-on Gabor texture feature classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hongyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bicheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Signal Process</title>
		<meeting>7th Int. Conf. Signal ess</meeting>
		<imprint>
			<date type="published" when="2004-09">Aug./Sep. 2004</date>
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using texture to analyze and manage large collections of remote sensed image and video data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhagavathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="217" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Indexing of satellite images with different resolutions by wavelet features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Aujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ladjal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1465" to="1472" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure tensor Riemannian statistical models for CBIR and classification of remote sensing images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Donias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bombrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Regniers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P. Da</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="248" to="260" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-resolution remotesensing imagery retrieval using sparse features by auto-encoder</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="775" to="783" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning low dimensional convolutional neural networks for high-resolution remote sensing image retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="489" to="508" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Local structure learning in high resolution remote sensing image retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="813" to="822" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Content-based high-resolution remote sensing image retrieval via unsupervised feature learning and collaborative affinity metric fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="709" to="723" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised multilayer feature learning for satellite image scene classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="161" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A three-layered graph-based learning approach for remote sensing image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6020" to="6034" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VISAPP Int. Conf. Comput. Vis. Theory Appl</title>
		<meeting>VISAPP Int. Conf. Comput. Vis. Theory Appl</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu</title>
		<title level="s">Conf. Neural Inf. Process. Syst</title>
		<meeting>26th Annu</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep hashing network for efficient similarity retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th AAAI Conf</title>
		<meeting>30th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2415" to="2421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep supervised hashing for fast image retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="2064" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature learning based deep supervised hashing with pairwise labels</title>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Joint Conf. Artif. Intell</title>
		<meeting>25th Int. Joint Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1711" to="1717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Column sampling based discrete supervised hashing</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th AAAI Conf</title>
		<meeting>30th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1230" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep feature fusion for VHR remote sensing scene classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4775" to="4784" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AID: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training deep convolutional neural networks for landcover classification of high-resolution imagery</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Starms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Marcum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="553" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep feature extraction and classification of hyperspectral images based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep fully convolutional network-based spatial distribution prediction for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5585" to="5599" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Polarimetric SAR feature extraction with neighborhood preservation-based deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1456" to="1466" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep supervised and contractive neural network for SAR image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2442" to="2459" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accurate object localization in remote sensing images based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2486" to="2498" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Adv. Geogr</title>
		<meeting>Int. Conf. Adv. Geogr</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DeepSat-A learning framework for satellite imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd SIGSPATIAL Int. Conf. Adv. Geogr</title>
		<meeting>23rd SIGSPATIAL Int. Conf. Adv. Geogr</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The devil is in the details: An evaluation of recent feature encoding methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2074" to="2081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Online crowdsourcing: Rating annotators and obtaining cost-effective labels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops</title>
		<imprint>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Error rate analysis of labeling by crowdsourcing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. Workshop</title>
		<meeting>Int. Conf. Mach. Learn. Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Int. Conf. Mach. Learn</title>
		<meeting>29th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient multiple feature fusion with hashing for hyperspectral imagery classification: A comparative study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4461" to="4478" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robust feature matching for remote sensing image registration via locally linear transforming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6469" to="6481" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Regularized vector field learning with sparse approximation for mismatch removal</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3519" to="3532" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion via gradient transfer and total variation minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="100" to="109" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cauchy graph embedding optimization for built-up areas detection from high-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2078" to="2096" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spatiotemporal detection and analysis of urban villages in mega city regions of China using high-resolution remotely sensed imagery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3639" to="3657" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised deep feature learning for urban village detection from high-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="567" to="579" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for land-use scene recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2250" to="2261" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
