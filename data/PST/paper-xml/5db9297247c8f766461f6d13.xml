<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long and Diverse Text Generation with Planning-based Hierarchical Variational Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangtao</forename><surname>Wen</surname></persName>
							<email>jtwen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenfei</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Baozun, Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long and Diverse Text Generation with Planning-based Hierarchical Variational Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing neural methods for data-to-text generation are still struggling to produce long and diverse texts: they are insufficient to model input data dynamically during generation, to capture inter-sentence coherence, or to generate diversified expressions. To address these issues, we propose a Planning-based Hierarchical Variational Model (PHVM). Our model first plans a sequence of groups (each group is a subset of input items to be covered by a sentence) and then realizes each sentence conditioned on the planning result and the previously generated context, thereby decomposing long text generation into dependent sentence generation sub-tasks. To capture expression diversity, we devise a hierarchical latent structure where a global planning latent variable models the diversity of reasonable planning and a sequence of local latent variables controls sentence realization. Experiments show that our model outperforms state-of-theart baselines in long and diverse text generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data-to-text generation is to generate natural language texts from structured data <ref type="bibr" target="#b10">(Gatt and Krahmer, 2018)</ref>, which has a wide range of applications (for weather forecast, game report, product description, advertising document, etc.). Most neural methods focus on devising encoding scheme and attention mechanism, namely, (1) exploiting input structure to learn better representation of input data <ref type="bibr" target="#b19">(Lebret et al., 2016;</ref><ref type="bibr" target="#b22">Liu et al., 2018)</ref>, and</p><p>(2) devising attention mechanisms to better employ input data <ref type="bibr" target="#b25">(Mei et al., 2016;</ref><ref type="bibr" target="#b22">Liu et al., 2018;</ref><ref type="bibr" target="#b27">Nema et al., 2018)</ref> or to dynamically trace which part of input has been covered in generation <ref type="bibr" target="#b14">(Kiddon et al., 2016)</ref>. These models are able to pro- * *Corresponding author: Minlie Huang.</p><p>duce fluent and coherent short texts in some applications.</p><p>However, to generate long and diverse texts such as product descriptions, existing methods are still unable to capture the complex semantic structures and diversified surface forms of long texts. First, existing methods are not good at modeling input data dynamically during generation. Some neural methods <ref type="bibr" target="#b14">(Kiddon et al., 2016;</ref><ref type="bibr" target="#b9">Feng et al., 2018)</ref> propose to record the accumulated attention devoted to each input item. However, these records may accumulate errors in representing the state of already generated prefix, thus leading to wrong new attention weights. Second, inter-sentence coherence in long text generation is not well captured <ref type="bibr" target="#b42">(Wiseman et al., 2017)</ref> due to the lack of high-level planning. Recent studies propose to model planning but still have much space for improvement. For instance, in <ref type="bibr" target="#b31">(Puduppully et al., 2019)</ref> and <ref type="bibr" target="#b34">(Sha et al., 2018)</ref>, planning is merely designed for ordering input items, which is limited to aligning input data with the text to be generated. Third, most methods fail to generate diversified expressions. Existing data-to-text methods inject variations at the conditional output distribution, which is proved to capture only low-level variations of expressions <ref type="bibr" target="#b33">(Serban et al., 2017)</ref>.</p><p>To address the above issues, we propose a novel Planning-based Hierarchical Variational Model (PHVM). To better model input data and alleviate the inter-sentence incoherence problem, we design a novel planning mechanism and adopt a compatible hierarchical generation process, which mimics the process of human writing. Generally speaking, to write a long text, a human writer first arranges contents and discourse structure (i.e., high-level planning) and then realizes the surface form of each individual part (low-level realization). Motivated by this, our proposed model first performs planning by segmenting input data into a sequence of groups, and then generates a sentence conditioned on the corresponding group and preceding generated sentences. In this way, we decompose long text generation into a sequence of dependent sentence generation sub-tasks where each sub-task depends specifically on an individual group and the previous context. By this means, the input data can be well modeled and inter-sentence coherence can be captured. Figure <ref type="figure">1</ref> depicts the process.</p><p>To deal with expression diversity, this model also enables us to inject variations at both highlevel planning and low-level realization with a hierarchical latent structure. At high level, we introduce a global planning latent variable to model the diversity of reasonable planning. At low level, we introduce local latent variables for sentence realization. Since our model is based on Conditional Variational Auto-Encoder (CVAE) <ref type="bibr" target="#b37">(Sohn et al., 2015)</ref>, expression diversity can be captured by the global and local latent variables.</p><p>We evaluate the model on a new advertising text<ref type="foot" target="#foot_0">1</ref> generation task which requires the system to generate a long and diverse advertising text that covers a given set of attribute-value pairs describing a product (see Figure <ref type="figure">1</ref>). We also evaluate our model on the recipe text generation task from <ref type="bibr" target="#b14">(Kiddon et al., 2016</ref>) which requires the system to correctly use the given ingredients and maintain coherence among cooking steps. Experiments on advertising text generation show that our model outperforms state-of-the-art baselines in automatic and manual evaluation. Our model also generalizes well to long recipe text generation and outperforms the baselines. Our contributions are twofold:</p><p>• We design a novel Planning-based Hierarchical Variational Model (PHVM) which integrates planning into a hierarchical latent structure. Experiments show its effectiveness in coverage, coherence, and diversity.</p><p>• We propose a novel planning mechanism which segments the input data into a sequence of groups, thereby decomposing long text generation into dependent sentence generation sub-tasks. Thus, input data can be better modeled and inter-sentence coherence can be better captured. To capture expression diversity, we devise a hierarchical latent structure which injects variations at both high-level planning and low-level realization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional methods <ref type="bibr" target="#b32">(Reiter and Dale, 1997;</ref><ref type="bibr" target="#b38">Stent et al., 2004)</ref> for data-to-text generation consist of three components: content planning, sentence planning, and surface realization. Content planning and sentence planning are responsible for what to say and how to say respectively; they are typically based on hand-crafted <ref type="bibr" target="#b18">(Kukich, 1983;</ref><ref type="bibr" target="#b5">Dalianis and Hovy, 1993;</ref><ref type="bibr" target="#b13">Hovy, 1993)</ref> or automatically-learnt rules <ref type="bibr" target="#b7">(Duboue and McKeown, 2003)</ref>. Surface realization generates natural language by carrying out the plan, which is template-based <ref type="bibr" target="#b24">(McRoy et al., 2003;</ref><ref type="bibr" target="#b6">van Deemter et al., 2005)</ref> or grammar-based <ref type="bibr" target="#b1">(Bateman, 1997;</ref><ref type="bibr" target="#b8">Espinosa et al., 2008)</ref>. As these models are shallow and the two stages (planning and realization) often function separately, traditional methods are unable to capture rich variations of texts.</p><p>Recently, neural methods have become the mainstream models for data-to-text generation due to their strong ability of representation learning and scalability. These methods perform well in generating weather forecasts <ref type="bibr" target="#b25">(Mei et al., 2016)</ref> or very short biographies <ref type="bibr" target="#b19">(Lebret et al., 2016</ref> As for long text generation, recent studies tackle the incoherence problem from different perspectives. To keep the decoder aware of the crucial information in the already generated prefix, <ref type="bibr" target="#b35">Shao et al. (2017)</ref> appended the generated prefix to the encoder, and <ref type="bibr" target="#b12">Guo et al. (2018)</ref> leaked the extracted features of the generated prefix from the discriminator to the generator in a Generative Adversarial Nets <ref type="bibr" target="#b11">(Goodfellow et al., 2014)</ref>. To model dependencies among sentences, <ref type="bibr" target="#b21">Li et al. (2015)</ref> utilized a hierarchical recurrent neural network (RNN) decoder. <ref type="bibr" target="#b17">Konstas and Lapata (2013)</ref> proposed to plan content organization with grammar rules while <ref type="bibr" target="#b31">Puduppully et al. (2019)</ref> planned by reordering input data. Most recently, <ref type="bibr" target="#b26">Moryossef et al. (2019)</ref> proposed to select plans from all possible ones, which is infeasible for large inputs.</p><p>As for diverse text generation, existing methods can be divided into three categories: enriching conditions <ref type="bibr" target="#b43">(Xing et al., 2017)</ref>, post-processing with beam search and rerank <ref type="bibr" target="#b20">(Li et al., 2016)</ref>, and designing effective models <ref type="bibr" target="#b44">(Xu et al., 2018)</ref>. Some text-to-text generation models <ref type="bibr" target="#b33">(Serban et al., 2017;</ref><ref type="bibr" target="#b45">Zhao et al., 2017)</ref> inject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) <ref type="bibr" target="#b30">(Park et al., 2018)</ref> is a most similar model to ours, which also adopts a hierarchical latent structure. Our method differs from VHCR in two aspects: (1) VHCR has no planning mechanism, and the global latent variable is mainly designed to address the KL collapse problem, while our global latent variable captures the diversity of reasonable planning; (2) VHCR injects distinct local latent variables without direct dependencies, while our method explicitly models the dependencies among local latent variables to better capture inter-sentence connections. <ref type="bibr" target="#b36">Shen et al. (2019)</ref> proposed ml-VAE-D with multi-level latent variables. However, the latent structure of ml-VAE-D consists of two global latent variables: the top-level latent variable is introduced to learn a more flexible prior of the bottom-level latent variable which is then used to decode a whole paragraph. By contrast, our hierarchical latent structure is tailored to our planning mechanism: the top level latent variable controls planning results and a sequence of local latent variables is introduced to obtain fine-grained control of sentence generation sub-tasks.</p><p>We evaluated our model on a new advertising text generation task which is to generate a long and diverse text that covers all given specifications about a product. Different from our task, the advertising text generation task in <ref type="bibr" target="#b3">(Chen et al., 2019)</ref> is to generate personalized product description based on product title, product aspect (e.g., "appearance"), and user category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>Given input data x = {d 1 , d 2 , ..., d N } where each d i can be an attribute-value pair or a keyword, our task is to generate a long and diverse text y = s 1 s 2 ...s T (s t is the t th sentence) that covers x as much as possible. For the advertising text generation task, x consists of specifications about a product where each d i is an attribute-value pair &lt; a i , v i &gt;. For the recipe text generation task, x is an ingredient list where each d i is an ingredient. Since the recipe title r is also used for generation, we abuse the symbol x to represent &lt; {d 1 , d 2 , ..., d N }, r &gt; for simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>Figure <ref type="figure">2</ref> shows the architecture of PHVM. PHVM first samples a global planning latent variable z p based on the encoded input data; z p serves as a condition variable in both planning and hierarchical generation process. The plan decoder takes z p as initial input. At time step t, it decodes a group g t which is a subset of input items (d i ) and specifies the content of the t th sentence s t . When the plan decoder finishes planning, the hierarchical generation process starts, which involves the high-level sentence decoder and the low-level word decoder. The sentence decoder models intersentence coherence in semantic space by computing a sentence representation h s t and sampling a local latent variable z s t for each group. h s t and z s t , along with g t , guide the word decoder to realize the corresponding sentence s t .</p><p>The planning process decomposes the long text generation task into a sequence of dependent sentence generation sub-tasks, thus facilitating the hierarchical generation process. With the hierarchical latent structure, PHVM is able to capture multi-level variations of texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Input Encoding</head><p>We first embed each input item d i into vector e(d i ). The recipe title r is also embedded as e(r). We then encode x<ref type="foot" target="#foot_1">2</ref> with a bidirectional Gated Recurrent Unit (GRU) <ref type="bibr" target="#b4">(Cho et al., 2014)</ref>. For advertising text generation, x is represented as the concatenation of the last hidden states of the forward and backward GRU enc</p><formula xml:id="formula_0">(x) = [ − → h N ; ← − h 1 ]; for recipe text generation, enc(x) = [ − → h N ; ← − h 1 ; e(r)]. h i = [ − → h i ; ← − h i ]</formula><p>is the context-aware representation of d i . Note that input encoder is not necessarily an RNN; other neural encoders or even other encoding schemes are also feasible, such as multi-layer perceptron (MLP) and bag of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Planning Process</head><p>The planning process generates a subset of input items to be covered for each sentence, thus decomposing long text generation into easier dependent sentence generation sub-tasks. Due to the flexibility of language, there may exist more than one reasonable text that covers the same input but in different order. To capture such variety, we model the diversity of reasonable planning with a global planning latent variable z p . Different samples of z p may lead to different planning results which control the order of content. This process can be formulated as follows:</p><formula xml:id="formula_1">g = argmax g P (g|x, z p )<label>(1)</label></formula><p>where g = g 1 g 2 ...g T is a sequence of groups, and each group g t is a subset of input items which is a main condition when realizing the sentence s t .</p><p>The global latent variable z p is assumed to follow the isotropic Gaussian distribution, and is sampled from its prior distribution p θ (z p |x) = N (µ p , σ p2 I) during inference and from its approximate posterior distribution q θ ′ (z p |x, y) = N (µ p ′ , σ p ′ 2 I) during training:</p><formula xml:id="formula_2">[µ p ; log σ p2 ] = M LP θ (x)<label>(2)</label></formula><formula xml:id="formula_3">[µ p ′ ; log σ p ′ 2 ] = M LP θ ′ (x, y)<label>(3)</label></formula><p>We solve Eq. 1 greedily by computing g t = argmax gt P (g t |g &lt;t , x, z p ) with the plan decoder (a GRU). Specifically, at time step t, the plan decoder makes a binary prediction for each input item by estimating P (d i ∈ g t |g &lt;t , x, z p ):</p><formula xml:id="formula_4">P (d i ∈ g t ) = σ(v T p tanh(W p [h i ; h p t ] + b p ))<label>(4</label></formula><p>) where σ denotes the sigmoid function, h i is the vector of input item d i , and h p t is the hidden state of the plan decoder. Each group is therefore formed as</p><formula xml:id="formula_5">g t = {d i |P (d i ∈ g t ) &gt; 0.5} (If this is empty, we set g t as {argmax d i P (d i ∈ g t )}.).</formula><p>We feed bow(g t ) (the average pooling of {h i |d i ∈ g t }) to the plan decoder at the next time step, so that h p t+1 is aware of what data has been selected and what has not. The planning process proceeds until the probability of stopping at the next time step is over 0.5:</p><formula xml:id="formula_6">P stop t = σ(W c h p t + b c )<label>(5)</label></formula><p>The hidden state is initialized with enc(x) and z p . The plan decoder is trained with full supervision, which is applicable to those tasks where reference plans are available or can be approximated. For both tasks we evaluate in this paper, we approximate the reference plans by recognizing the subset of input items covered by each sentence with string match heuristics. The loss function at time step t is given by:</p><formula xml:id="formula_7">− log P (g t = g t | g &lt;t , x, z p ) = − d i ∈ gt log P (d i ∈ g t ) − d i / ∈ gt log(1 − P (d i ∈ g t ))<label>(6)</label></formula><p>where g t is the reference group. As a result, z p is forced to capture features of reasonable planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hierarchical Generation Process</head><p>The generation process produces a long text y = s 1 s 2 ...s T in alignment with the planning result g = g 1 g 2 ...g T , which is formulated as follows:</p><formula xml:id="formula_8">c = {x, z p } (7) y = argmax y P (y|g, c)<label>(8)</label></formula><p>We perform sentence-by-sentence generation and solve Eq. 8 greedily by computing s t = argmax st P (s t |s &lt;t , g, c). s t focuses more on g t than on the entire plan g. The generation process is conducted hierarchically, which consists of sentence-level generation and word-level generation. Sentence-level generation models intersentence dependencies at high level, and interactively controls word-level generation which conducts low-level sentence realization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-level Generation</head><p>The sentence decoder (a GRU) performs sentence-level generation; for each sentence s t to be generated, it produces a sentence representation h s t and introduces a local latent variable z s t to control sentence realization.</p><p>The latent variable z s t is assumed to follow the isotropic Gaussian distribution. At time step t, the sentence decoder samples z s t from the prior distribution p φ (z s t |s &lt;t , g, c) = N (µ s t , σ s2 t I) during inference and from the approximate posterior distribution q φ ′ (z s t |s ≤t , g, c) = N (µ s ′ t , σ s ′ 2 t I) during training. h s t and the distribution of z s t are given by:</p><formula xml:id="formula_9">h s t = GRU s ([z s t−1 ; h w t−1 ], h s t−1 ) (9) [µ s t ; log σ s2 t ] = M LP φ (h s t , bow(g t )) (10) [µ s ′ t ; log σ s ′ 2 t ] =M LP φ ′ (h s t , bow(g t ), s t ) (11)</formula><p>where h w t−1 is the last hidden state of the word decoder after decoding sentence s t−1 , and GRU s denotes the GRU unit of the sentence decoder. By this means, we constrain the distribution of z s t in two aspects. First, to strengthen the connection from the planning result g, we additionally condition z s t on g t to keep z s t focused on g t . Second, to capture the dependencies on s &lt;t , we explicitly model the dependencies among local latent variables by inputting z s t−1 to the sentence decoder, so that z s t is conditioned on z s &lt;t and is expected to model smooth transitions in a long text.</p><p>We initialize the hidden state h s 0 by encoding the input x, the global planning latent variable z p and the planning result g:</p><formula xml:id="formula_10">h g t = GRU g (bow(g t ), h g t−1 ) (12) h s 0 = W s [enc(x); z p ; h g T ] + b s (13)</formula><p>where h g T is the last hidden state of GRU g that encodes the planning result g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word-level Generation</head><p>The word decoder (a GRU) conducts word-level generation; it decodes a sentence s t = argmax st P (s t |s &lt;t , z s t , g, c) conditioned on {h s t , z s t , g t }. Specifically, we sample word w t k of s t as follows:</p><formula xml:id="formula_11">w t k ∼ P (w t k |w t &lt;k , s &lt;t , z s t , g, c)<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Loss Function</head><p>We train our model end-to-end. The loss function has three terms: the negative evidence lower bound (ELBO) of log P (y|x) (L 1 ), the loss of predicting the stop signal (L 2 ) and the bag-of-word loss <ref type="bibr" target="#b45">(Zhao et al., 2017</ref>) (L 3 ). We first derive the ELBO: L 2 is given by:</p><formula xml:id="formula_12">log P (y|x) ≥ E q θ ′ (z p |x,y) [log P (y|x, z p )] − D KL (q θ ′ (z p |x, y)||p θ (z p |x))<label>(</label></formula><formula xml:id="formula_13">L 2 = T −1 t=1 log P stop t + log(1 − P stop T ) (<label>18</label></formula><formula xml:id="formula_14">)</formula><p>L 3 is the sum of bag-of-word loss <ref type="bibr" target="#b45">(Zhao et al., 2017)</ref> applied to each sentence, which is another technique to tackle the KL collapse problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We evaluated PHVM on two generation tasks. The first task is the new advertising text generation task which is to generate a long advertising text that covers all given attribute-value pairs for a piece of clothing. The second task is the recipe generation task from <ref type="bibr" target="#b14">(Kiddon et al., 2016)</ref> which is to generate a correct recipe for the given recipe title and ingredient list.</p><p>Advertising Text Generation We constructed our dataset from a Chinese e-commerce platform. The dataset consists of 119K pairs of advertising text and clothing specification table. Each table is a set of attribute-value pairs describing a piece of clothing. We made some modifications to the original specification tables. Specifically, if some attribute-value pairs from a table do not occur in the corresponding text, the pairs are removed from the table. We also recognized attribute values by string matching with a dictionary of attribute values. If a pair occurs in the text but not in the table, the pair is added to the table.</p><p>The statistics are shown in Table <ref type="table" target="#tab_5">1 and Table 2</ref> categories (e.g., hats and socks) are discarded because these categories have insufficient data for training. The average length of advertising text is about 110 words. To evaluate the expression diversity of our dataset, we computed distinct-4 (see Section 5.3) on 3,000 randomly sampled texts from our dataset. The distinct-4 score is 85.35%, much higher than those of WIKIBIO <ref type="bibr" target="#b19">(Lebret et al., 2016)</ref> and ROTOWIRE <ref type="bibr" target="#b42">(Wiseman et al., 2017)</ref> (two popular data-to-text datasets). Therefore, our dataset is suitable for evaluating long and diverse text generation<ref type="foot" target="#foot_2">3</ref> . We left 1,070 / 3,127 instances for validation / test, and used the remainder for training.</p><p>Recipe Text Generation We used the same train-validation-test split (82,590 / 1,000 / 1,000) and pre-processing from <ref type="bibr" target="#b14">(Kiddon et al., 2016)</ref>. In the training set, the average recipe length is 102 tokens, and the vocabulary size of recipe title / text is 3,793 / 14,103 respectively. The recipe dataset covers a wide variety of recipe types indicated by the vocabulary size of recipe title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compared our model with four strong baselines where the former two do not perform planning and the latter two do: Checklist: This model utilizes an attention-based checklist mechanism to record what input data has been mentioned, and focuses more on what has not during generation <ref type="bibr" target="#b14">(Kiddon et al., 2016)</ref>. CVAE: The CVAE model proposed by <ref type="bibr" target="#b45">Zhao et al. (2017)</ref> uses a latent variable to capture the diversity of responses in dialogue generation. We adapted it to our task by replacing the hierarchical encoder with a one-layer bidirectional GRU. Pointer-S2S: A two-stage method <ref type="bibr" target="#b31">(Puduppully et al., 2019)</ref> that decides the order of input data with Pointer Network <ref type="bibr" target="#b41">(Vinyals et al., 2015)</ref> before generation with Sequence-to-Sequence (Seq2Seq) <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. Link-S2S Link-S2S <ref type="bibr" target="#b34">(Sha et al., 2018)</ref>  where a link matrix parameterizes the probability of describing one type of input item after another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>For both advertising text generation and recipe text generation, the settings of our model have many in common. The dimension of word embedding is 300. All embeddings were randomly initialized. We utilized GRU for all RNNs. All RNNs, except the input encoder, the plan decoder, and the plan encoder, have a hidden size of 300. The global planning latent variable and local latent variables have 200 dimensions. We set batch size to 32 and trained our model using the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 and gradient clipping threshold at 5. We selected the best model in terms of L 1 + L 2 on the validation set.</p><p>As we need to train the plan decoder with full supervision, we extracted plans from the texts by recognizing attribute values (or ingredients) in each sentence with string match heuristics. Some sentences do not mention any input items; we associated these sentences with a special tag, which is treated as a special input item for Pointer-S2S, Link-S2S, and our model. Although our extraction method can introduce errors, the extracted plans are sufficient to train a good plan decoder 4 . Advertising Text Generation We embedded an attribute-value pair by concatenating the embedding of attribute and the embedding of attribute value. Embedding dimensions for attribute and attribute value are 30 and 100 respectively. The input encoder, the plan decoder, and the plan encoder all have a hidden size of 100. Recipe Text Generation We embedded a multiword title (ingredient) by taking average pooling 4 Our corpus and code are available at https://github.com/ZhihongShao/Planning-based-Hierarchical-Variational-Model.</p><p>of the embeddings of its constituent words. Embedding dimensions for title word and ingredient word are 100 and 200 respectively. The input encoder, the plan decoder, and the plan encoder all have a hidden size of 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Automatic Evaluation Metrics</head><p>We adopted the following automatic metrics. (1) Corpus BLEU: BLEU-4 <ref type="bibr" target="#b29">(Papineni et al., 2002)</ref>.</p><p>(2) Coverage: This metric measures the average proportion of input items that are covered by a generated text. We recognized attribute values (ingredients) with string match heuristics. For the advertising text generation task, synonyms were also considered. (3) Length: The average length of the generated texts. (4) Distinct-4: Distinctn <ref type="bibr" target="#b20">(Li et al., 2016</ref>) is a common metric for diversity which measures the ratio of distinct n-grams in generated tokens. We adopted distinct-4. (5) Repetition-4: This metric measures redundancy with the percentage of generated texts that repeat at least one 4-gram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Advertising Text Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Automatic Evaluation</head><p>Table <ref type="table" target="#tab_6">3</ref> shows the experimental results. As our dataset possesses high expression diversity, there are many potential expressions for the same content, which leads to the low BLEU scores of all models. Our model outperforms the baselines in terms of coverage, indicating that it learns to arrange more input items in a long text. With content ordering, Pointer-S2S outperforms both Checklist and CVAE in coverage. By contrast, our planning mechanism is even more effective in controlling generation: each sentence generation subtask is specific and focused, and manages to cover 95.16% of the corresponding group on average. Noticeably, Link-S2S also models planning but To investigate the influence of each component in the hierarchical latent structure, we conducted ablation tests which removed either global latent variable z p or local latent variables z s t . As observed, removing z p leads to significantly lower distinct-4, indicating that z p contributes to expression diversity. The lower coverage is because the percentage of input items covered by a planning result decreases from 98.4% to 94.4% on average, which indicates that z p encodes useful information for planning completeness. When removing z s t , distinct-4 drops substantially, as the model tends to generate shorter and more common sentences. This indicates that z s t contributes more to capturing variations of texts. The significantly higher repetition-4 is because removing z s t weakens the dependencies among sentences so that the word decoder is less aware of the preceding generated context. The lower coverage is because each generated sentence covers less planned items (from 95.16% to 93.07% on average), indicating that z s t keeps sentence s t more focused on its group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Manual Evaluation</head><p>To better evaluate the quality of the generated texts, we conducted pair-wise comparisons manually. Each model generates texts for 200 randomly sampled inputs from the test set. We hired five annotators to give preference (win, lose or tie) to each pair of texts (ours vs. a baseline, 800 pairs in total). Metrics Two metrics were independently eval-uated during annotation: grammaticality which measures whether a text is fluent and grammatical, and coherence which measures whether a text is closely relevant to input, logically coherent, and well-organized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The annotation results in Table <ref type="table" target="#tab_7">4</ref> show that our model significantly outperforms baselines in both metrics. Our model produces more logically coherent and well-organized texts, which indicates the effectiveness of the planning mechanism. It is also worth noting that our model performs better in terms of grammaticality. The reason is that long text generation is decomposed into sentence generation sub-tasks which are easier to control, and our model captures inter-sentence dependencies through modeling the dependencies among local latent variables. To evaluate how well our model can capture the diversity of planning, we conducted another manual evaluation. We randomly sampled 100 test inputs and generated 10 texts for each input by repeatedly sampling latent variables. Five annotators were hired to score (a Likert scale ∈ [1, 5])</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Diversity of Planning</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average number of distinct planning results (left) / average score of generation quality (right) when the number of input pairs varies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>; Liu d 1 d 2 d N … ~zp &lt;SG&gt; bow(g 1 ) bow(g T-1 ) g 1 g 2 g T … ~ … Input Encoder Plan Decoder Plan Encoder Sentence Decoder</head><label></label><figDesc></figDesc><table><row><cell>h 1</cell><cell>h 2</cell><cell>h N</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell>0.8</cell><cell>0.2</cell><cell>…</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell>Word Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell>0.2</cell><cell>0.8</cell><cell>…</cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell>s 1</cell><cell>s 2</cell><cell>s T</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>d 1 d 2 d N d 3 … d 1 d 3 g t group … bow(g T-1 ) g t s t … … … d 3 h 3 Probability of discarding Probability of selecting ~ ~ ~ Concatenation Sampling Figure 2</head><label></label><figDesc>: Architecture of PHVM. The model controls planning with a global latent variable z p . The plan decoder conducts planning by generating a sequence of groups g = g 1 g 2 ...g T where g t is a subset of input items and specifies the content of sentence s t to be generated. The sentence decoder controls the realization of s t with a local latent variable z s t ; dependencies among z s t are explicitly modeled to better capture inter-sentence coherence.</figDesc><table /><note>et al., 2018;<ref type="bibr" target="#b34">Sha et al., 2018;</ref><ref type="bibr" target="#b27">Nema et al., 2018)</ref> using well-designed data encoder and attention mechanisms. However, as demonstrated in Wiseman et al. (2017) (a game report generation task), existing neural methods are still problematic for long text generation: they often generate incoherent texts. In fact, these methods also lack the ability to model diversity of expressions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(s t |s &lt;t , g, x, z p ) ≥ E q φ ′ (z s t |s ≤t ,g,x,z p ) [log P (s t |s &lt;t , z s t , g, x, z p )] − D KL (q φ ′ (z s</figDesc><table><row><cell>alleviate the KL collapse problem(Bowman et al., log P training, we use linear KL annealing technique to 2016).</cell><cell>T t=1 + log P (s t |s &lt;t , g, x, z p ) log P (g t |g &lt;t , x, z p )</cell><cell>(16)</cell></row></table><note>15) log P (y|x, z p ) = log P (g, y|x, z p ) = t |s ≤t , g, x, z p )||p φ (z s t |s &lt;t , g, x, z p ))(17)We can obtain the ELBO by unfolding the right hand side of Eq. 15 with Eq. 16 and 17. During</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>. Detailed statistics of our dataset. # Attr. / # Val.: the total number of attributes / attribute values.Our dataset consists of three categories of clothing: tops, dress / skirt, and pants, which are further divided into 22, 23, and 9 types respectively (E.g., shirt, sweater are two types of tops). Other # Attr. # Val. Vocab Avg. # Input Pairs Avg. # Len.</figDesc><table><row><cell>Category</cell><cell cols="3">Tops Dress / Skirt Pants</cell></row><row><cell># Type</cell><cell>22</cell><cell>23</cell><cell>9</cell></row><row><cell># Attr.</cell><cell>13</cell><cell>16</cell><cell>11</cell></row><row><cell># Val.</cell><cell>264</cell><cell>284</cell><cell>203</cell></row><row><cell>Avg. # Input Pairs</cell><cell>7.7</cell><cell>7.7</cell><cell>6.6</cell></row><row><cell>Avg. Len.</cell><cell>110</cell><cell>111</cell><cell>108</cell></row><row><cell># Instances</cell><cell>48K</cell><cell>47K</cell><cell>24K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>General statistics of our dataset. We counted the size of vocabulary after removing brand names.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Automatic evaluation for advertising text generation. We applied bootstrap resampling<ref type="bibr" target="#b16">(Koehn, 2004)</ref> for significance test. Scores that are significantly worse than the best results (in bold) are marked with ** for p-value &lt; 0.01.</figDesc><table><row><cell>is a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Manual pair-wise evaluation for advertising text generation. We conducted Sign Test for significance test. Scores marked with * mean p-value &lt; 0.05 and ** for p-value &lt; 0.01. κ denotes Fleiss' kappa, all indicating moderate agreement.</figDesc><table><row><cell>Models</cell><cell></cell><cell>Grammaticality</cell><cell></cell><cell>κ</cell><cell></cell><cell>Coherence</cell><cell></cell><cell>κ</cell></row><row><cell></cell><cell cols="3">Win (%) Lose (%) Tie (%)</cell><cell></cell><cell cols="3">Win (%) Lose (%) Tie (%)</cell><cell></cell></row><row><cell>PHVM vs. Checklist</cell><cell>59.0**</cell><cell>23.5</cell><cell>17.5</cell><cell>0.484</cell><cell>54.5*</cell><cell>42.5</cell><cell>3.0</cell><cell>0.425</cell></row><row><cell>PHVM vs. CVAE</cell><cell>69.5**</cell><cell>13.5</cell><cell>17.0</cell><cell>0.534</cell><cell>60.0**</cell><cell>37.0</cell><cell>3.0</cell><cell>0.426</cell></row><row><cell>PHVM vs. Pointer-S2S</cell><cell>76.5**</cell><cell>17.0</cell><cell>6.5</cell><cell>0.544</cell><cell>56.5**</cell><cell>39.0</cell><cell>4.5</cell><cell>0.414</cell></row><row><cell>PHVM vs. Link-S2S</cell><cell>66.0**</cell><cell>28.5</cell><cell>5.5</cell><cell>0.462</cell><cell>62.5**</cell><cell>31.5</cell><cell>6.0</cell><cell>0.415</cell></row><row><cell cols="4">has the lowest coverage, possibly because a static</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">link matrix is unable to model flexible content ar-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">rangement in long text generation. As for diver-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">sity, our model has substantially lower repetition-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4 and higher distinct-4, indicating that our gener-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ated texts are much less redundant and more di-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">versified. Notably, Link-S2S has the longest texts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">but with the highest repetition-4, which produces</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">many redundant expressions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">An advertising text describes a product with attractive wording. The goal of writing such texts is to advertise a product and attract users to buy it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">For advertising text generation, x is first ordered by attributes so that general attributes are ahead of specific ones; for recipe text generation, we retain the order in the dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We presented a detailed comparison with other benchmark corpora in Appendix A.2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Science Foundation of China (Grant No. 61936010/61876096) and the National Key R&amp;D Program of China (Grant No. 2018YFC0830200). We would like to thank THUNUS NExT Joint-Lab for the support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Table <ref type="table">5</ref>: Automatic evaluation for recipe text generation. Checklist was trained with its own source code. We also re-printed results from <ref type="bibr" target="#b14">(Kiddon et al., 2016)</ref> (i.e., Checklist §). We applied bootstrap resampling <ref type="bibr" target="#b16">(Koehn, 2004)</ref> for significance test. Scores that are significantly worse than the best results (in bold) are marked with * for p-value &lt; 0.05 or ** for p-value &lt; 0.01.</p><p>a text about whether it is a qualified advertising text, which requires comprehensive assessment in terms of fluency, redundancy, content organization, and coherence. We computed the average of five ratings as the final score of a generated text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The average score of a generated text is 4.27. Among the 1,000 generated texts, 79.0% of texts have scores above 4. These results demonstrate that our model is able to generate multiple high-quality advertising texts for the same input.</p><p>We further analyzed how our model performs with different numbers of input attribute-value pairs (see Figure <ref type="figure">3</ref>). A larger number of input items indicates more potential reasonable ways of content arrangement. As the number of input items increases, our model produces more distinct planning results while still obtaining high scores (above 4.2). It indicates that our model captures the diversity of reasonable planning. The average score drops slightly when the number of input pairs is more than 12. This is due to insufficient training data for this range of input length (accounting for 6.5% of the entire training set).</p><p>To further verify the planning diversity, we also computed self-BLEU <ref type="bibr" target="#b46">(Zhu et al., 2018)</ref> to evaluate how different planning results (or texts) for the same input overlap (by taking one planning result (or text) as hypothesis and the rest 9 for the same input as reference and then computing BLEU-4). The average self-BLEU of the planning results is 43.37% and that of the texts is 16.87%, which demonstrates the much difference among the 10 results for the same input. Annotation Statistics The Fleiss' kappa is 0.483, indicating moderate agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Recipe Text Generation</head><p>Table <ref type="table">5</ref> shows the experimental results. Our model outperforms baselines in terms of coverage and diversity; it manages to use more given ingredients and generates more diversified cooking steps. We also found that Checklist / Link-S2S produces the general phrase "all ingredients" in 14.9% / 24.5% of the generated recipes, while CVAE / Pointer-S2S / PHVM produce the phrase in 7.8% / 6.3% / 5.0% of recipes respectively. These results demonstrate that our model may generalize well to other data-to-text generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Case Study</head><p>We present examples for both tasks in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We present the Planning-based Hierarchical Variational Model (PHVM) for long and diverse text generation. A novel planning mechanism is proposed to better model input data and address the inter-sentence incoherence problem. PHVM also leverages a hierarchical latent structure to capture the diversity of reasonable planning and sentence realization. Experiments on two data-to-text corpora show that our model is more competitive to generate long and diverse texts than state-of-theart baselines.</p><p>Our planning-based model may be inspiring to other long text generation tasks such as long text machine translation and story generation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enabling technology for multilingual natural language generation: the KPML development environment</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Bateman</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324997001514</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="55" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>CoNLL; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11">2016. 2016. August 11-12, 2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards knowledge-based personalized product description generation in e-commerce</title>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330725</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-04">2019. August 4-8, 2019</date>
			<biblScope unit="page" from="3040" to="3050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
				<meeting>SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25">2014. 25 October 2014</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aggregation in natural language generation</title>
		<author>
			<persName><forename type="first">Hercules</forename><surname>Dalianis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-60800-1_25</idno>
	</analytic>
	<monogr>
		<title level="m">Trends in Natural Language Generation, An Artificial Intelligence Perspective, Fourth European Workshop, EWNLG &apos;93</title>
				<meeting><address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-04-28">1993. April 28-30, 1993</date>
			<biblScope unit="page" from="88" to="105" />
		</imprint>
	</monogr>
	<note>Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real versus template-based natural language generation: A false opposition?</title>
		<author>
			<persName><forename type="first">Mariët</forename><surname>Kees Van Deemter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Theune</surname></persName>
		</author>
		<author>
			<persName><surname>Krahmer</surname></persName>
		</author>
		<idno type="DOI">10.1162/0891201053630291</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical acquisition of content selection rules for natural language generation</title>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 conference on Empirical methods in natural language processing</title>
				<meeting>the 2003 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hypertagging: Supertagging for surface realization with CCG</title>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Mehay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting><address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-15">2008. June 15-20, 2008</date>
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic-to-essay generation with neural networks</title>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/567</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13">2018. 2018. July 13-19. 2018</date>
			<biblScope unit="page" from="4078" to="4084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.5477</idno>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. 2014. December 8-13 2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long text generation via adversarial training with leaked information</title>
		<author>
			<persName><forename type="first">Jiaxian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">McIlraith and Weinberger</title>
		<imprint>
			<biblScope unit="page" from="5141" to="5148" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated discourse generation using discourse structure relations</title>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.1016/0004-3702(93)90021-3</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="341" to="385" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inducing document plans for concept-to-text generation</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-10">2013. 18-21 October 2013</date>
			<biblScope unit="page" from="1503" to="1514" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Design of a knowledge-based report generator</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Annual Meeting of the Association for Computational Linguistics, Massachusetts Institute of Technology, Cambridge, Massachusetts</title>
				<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1983-06-15">1983. June 15-17, 1983</date>
			<biblScope unit="page" from="145" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter</title>
				<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-26">2015. 2015. July 26-31, 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1106" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Table-to-text generation by structure-aware seq2seq learning</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">McIlraith and Weinberger</title>
		<imprint>
			<biblScope unit="page" from="4881" to="4888" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
		</imprint>
	</monogr>
	<note>editors</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An augmented template-based approach to text realization</title>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">W</forename><surname>Mcroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songsak</forename><surname>Channarukul</surname></persName>
		</author>
		<author>
			<persName><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="420" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016, The 2016 Conference of the North American Chapter</title>
				<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Step-by-step: Separating planning from realization in neural data-to-text generation</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating descriptions from structured data using a bifocal attention mechanism and gated orthogonalization</title>
		<author>
			<persName><forename type="first">Preksha</forename><surname>Nema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreyas</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parag</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1539" to="1550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</editor>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09">2017. 2017. September 9-11, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A hierarchical latent structure for variational conversation modeling</title>
		<author>
			<persName><forename type="first">Yookoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1792" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data-to-text generation with content selection and planning</title>
		<author>
			<persName><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324997001502</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA.</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04">2017. February 4-9, 2017</date>
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Orderplanning neural text generation from structured data</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">McIlraith and Weinberger</title>
		<imprint>
			<biblScope unit="page" from="5414" to="5421" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Generating high-quality and informative conversation responses with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Yuanlong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2210" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards generating long and coherent text with multi-level latent variable models</title>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Asli C ¸elikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
				<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy; Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Long</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2079" to="2089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Trainable sentence planning for complex information presentations in spoken dialog systems</title>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><surname>July</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="79" to="86" />
			<pubPlace>Barcelona, Spain.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01">2016. 2016. November 1-4, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. 2015. December 7-12, 2015</date>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Diversity-promoting gan: A crossentropy based generative adversarial network for diversified text generation</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3940" to="3949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskénazi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-30">2017. July 30 -August 4</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Texygen: A benchmarking platform for text generation models</title>
		<author>
			<persName><forename type="first">Yaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210080</idno>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR 2018</title>
				<meeting><address><addrLine>Ann Arbor, MI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-08">2018. July 08-12. 2018</date>
			<biblScope unit="page" from="1097" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
