<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimMIM: A Simple Framework for Masked Image Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-18">18 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>yuecao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
							<email>t-yutonglin@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
							<email>jianmin.bao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Dai</surname></persName>
							<email>qi.dai@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SimMIM: A Simple Framework for Masked Image Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-18">18 Nov 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.09886v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents SimMIM, a simple framework for masked image modeling. We simplify recently proposed related approaches without special designs such as blockwise masking and tokenization via discrete VAE or clustering. To study what let the masked image modeling task learn good representations, we systematically study the major components in our framework, and find that simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a strong pre-text task; 2) predicting raw pixels of RGB values by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6%. When applied on a larger model of about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to facilitate the training of a 3B model (SwinV2-G), that by 40× less data than that in previous practice, we achieve the state-of-the-art on four representative vision benchmarks. The code and models will be publicly available at https: //github.com/microsoft/SimMIM .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>"What I cannot create, I do not understand."</p><p>-Richard Feynman "Masked signal modeling" is such a task learning to create: masking a portion of input signals and trying to predict these masked signals. In NLP, following this philosophy, the self-supervised learning approaches built on the masked language modeling task have largely repainted the field <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30]</ref>, that very large-scale language models are learnt by using huge unlabelled data and are demonstrated generalizable to broad NLP applications.</p><p>In computer vision, while there are pioneers leveraging this philosophy for self-supervised representation learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b56">58]</ref>, in previous years, this line of works is almost buried by the contrastive learning approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">48]</ref>. The different difficulties of applying this task to the language and visual domain can be explained by the differences between two modalities. One of these differences is that images exhibit much stronger locality: pixels near each other tend to be highly correlated <ref type="bibr" target="#b24">[25]</ref>, thus the task may be well accomplished by copying the nearing pixels but not by reasoning of semantics. Another difference is that visual signals are raw and low-level, while text tokens are high-level concepts generated by humans. This raises a question that whether the prediction of low-level signals would be useful for high-level visual recognition tasks. A third difference is that the visual signals are continuous and text tokens are discrete. It is unknown how to adapt the classification-based masked language modeling approach to deal well with continuous visual signals.</p><p>Until very recently, there were a few trials attempting to bridge the modality gap and address the obstacles, by introducing several special designs, e.g., adapting the continuous signals for classification by color clustering <ref type="bibr" target="#b6">[7]</ref> or by tokenization using an additional network <ref type="bibr" target="#b0">[1]</ref>, a block-wise masking strategy to break the short-range connections <ref type="bibr" target="#b0">[1]</ref>, and etc. With these special designs, the learnt representations are demonstrated well transferable to several visual recognition tasks.</p><p>In contrary to requiring special complex designs, in this paper, we present a simple framework which aligns well with the nature of visual signals, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, and is able to learn similar or even better representations than previous more complex approaches: random masking of input image patches, with a linear layer to regress the raw pixel values of masked area under an 1 loss. The key designs and insights behind this simple framework involve:</p><p>• Random masking is applied on image patches, which is simple and convenient for vision Transformers. For masked pixels, either larger patch size or higher masking ratio can result in less chance to find close-by visible pixels to predict itself. For a large masked patch size of 32, the approach could achieve highly competitive performance in a wide range of masking ratios (10%-70%). For a small masked patch size of 8, the masking ratio is required to reach as high as 80% to perform relatively well. Note that the preferred masking ratios are very different from that in the language domain, where a small masking ratio of 0.15 is adopted as default. We hypothesize that different degrees of information redundancy by two modalities may lead to the different behaviors.</p><p>• A raw pixel regression task is used. The regression task aligns well with the continuous nature of visual signals, which possesses ordering property. This simple task performs no worse than the classification approaches with classes specially defined by tokenization, clustering or discretization.</p><p>• An extremely lightweight prediction head (e.g., a linear layer) is adopted, which achieves similarly or slightly better transferring performance than that of heavier prediction heads (e.g., an inverse Swin-B). The use of extremely lightweight prediction head brings a remarkable speedup in pre-training. In addition, we note that a broad range of target resolutions (e.g., 12 2 -96 2 ) perform competitive with the highest 192 2 . While heavier head or higher resolution generally leads to stronger generation capability, such stronger capability does not necessarily benefit the down-stream finetuning tasks.</p><p>Though simple, the proposed SimMIM approach is pretty effective for representation learning. Using ViT-B, it achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach ( <ref type="bibr" target="#b0">[1]</ref>) by +0.6%. SimMIM is also demonstrated to be scalable to larger models: with a SwinV2-H model (658M parameters) <ref type="bibr" target="#b30">[31]</ref>, it achieves 87.1% top-1 accuracy on ImageNet-1K classification, which is the highest number among methods that use ImageNet-1K data only. This result encourages the use of self-supervised learning to address the increasing data-hungry problem caused by quickly rising model capacity. In fact, with the aid of Sim-MIM, we successfully trained a SwinV2-G model with 3 billion parameters <ref type="bibr" target="#b30">[31]</ref> using ∼40× smaller data than that of Google's JFT-3B dataset, and sets new records on several representative benchmarks: 84.0% top-1 accuracy on ImageNet-V2 classification <ref type="bibr" target="#b38">[40]</ref>, 63.1/54.4 box/mask mAP on COCO object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>, 59.9 mIoU on ADE20K semantic segmentation <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b58">60]</ref>, and 86.8% top-1 accuracy on Kinetics-400 action recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>While in recent years we have witnessed increasing overlaps between NLP and computer vision in both basic modeling and learning algorithms, as well as in multi-modal applications, which align well with how human brains work for general intelligence capabilities, we hope our demonstration of "masked signal modeling" in computer vision can drive this trend a bit further, and encourage deeper interaction of different AI fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Masked language modeling (MLM) Masked language modeling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> and its auto-regressive variants <ref type="bibr" target="#b1">[2]</ref> are the dominant self-supervised learning approaches in the field of natural language processing (NLP). Given visible tokens in a sentence or a sentence pair / triplet, the approaches learn representations by predicting invisible tokens of the input. This line of approaches has repainted the field since about 3 years ago <ref type="bibr" target="#b11">[12]</ref>, that it enables the learning of very large language models and generalizes well on broad language understanding and generation tasks by leveraging huge data.</p><p>Masked image modeling (MIM) Masked image modeling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">45]</ref> progressed in parallel with the MLM task in NLP but located in a non-mainstream position for a long time. The context encoder approach <ref type="bibr" target="#b35">[36]</ref> is a pioneer work in this direction, which masks a rectangle area of the original images, and predicts the missing pixels. CPC <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">45]</ref> predicts patches via a verification task in each batch with a contrastive predictive coding loss. Recently, iGPT <ref type="bibr" target="#b6">[7]</ref>, ViT <ref type="bibr" target="#b14">[15]</ref> and BEiT <ref type="bibr" target="#b0">[1]</ref> recall this learning approach on the modern vision Transformers, and show strong potential in representation learning by introducing special designs on some components, such as clustering on pixels <ref type="bibr" target="#b6">[7]</ref>, prediction of mean color <ref type="bibr" target="#b14">[15]</ref>, and tokenization via an additional dVAE network with a block-wise masking strategy <ref type="bibr" target="#b0">[1]</ref>. In contrary to these complex designs, we present an extremely simple framework, SimMIM, which shows similar or even slightly better effectiveness.</p><p>Reconstruction based methods are also related to our approach, particularly the auto-encoder approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b45">47]</ref>. Similar as in our approach, they adopt a reconstruction task to recover the original signals. However, they are based on a different philosophy of visible signal reconstruction, other than the creation or prediction of invisible signals as in our approach. They thus progress in a very different path, by studying how to effectively regularize the task learning by proper regularization or architecture bottlenecks.</p><p>Image inpainting methods Beyond representation learning, masked image modeling is a classical computer vision problem, named image inpainting. This problem has been extensively studied in computer vision for a long time <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b51">53]</ref>, aiming for improving the inpainting quality and without connecting to self-supervised representation learning. While we advocate image inpainting as a strong self-supervised pre-text task, we also find stronger inpainting capability does not necessarily leads to stronger finetuning performance on down-stream tasks.</p><p>Compressed sensing The approach in this paper is also related to compressed sensing <ref type="bibr" target="#b13">[14]</ref>, which affirms most of the data we acquire including image signals can be thrown away with almost no perceptual loss. Such claim is also partly supported by recent works of sparse inference <ref type="bibr" target="#b18">[19]</ref> that the recognition accuracy has very little drop after throwing a large portion of image features <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b49">51]</ref>. This paper also finds that by masking a large portion of an image. The observation in this paper goes further for the input signals, that with an extremely small portion of randomly selected input image patches as input, i.e., 10%, the inpainting task can still be learnt to produce good visual representations.</p><p>Other self-supervised learning approaches During the last two decades, there have been numerous pretext tasks to learn visual representation in a self-supervised way: grayscale image colorization <ref type="bibr" target="#b55">[57]</ref>, jigsaw puzzle solving <ref type="bibr" target="#b33">[34]</ref>, split-brain auto-encoding <ref type="bibr" target="#b56">[58]</ref>, rotation prediction <ref type="bibr" target="#b16">[17]</ref>, learning to cluster <ref type="bibr" target="#b3">[4]</ref>. Though very different from masked image modeling, some of them interestingly also follow a philosophy of predicting the invisible parts of signals, e.g., <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref> use one or two color channels as input to predict values of other channels. Another large portion of works lie in the contrastive learning approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b48">50]</ref>, which are the previous mainstream. We hope our work can encourage the study of masked language modeling as a pretext task for self-supervised visual representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A Masked Image Modeling Framework</head><p>Our approach SimMIM learns representation through masked image modeling, which masks a portion of input image signals and predicts the original signals at masked area. The framework consists of 4 major components:</p><p>1) Masking strategy. Given an input image, this component designs how to select the area to mask, and how to implement masking of selected area. The transformed image after masking will be used as the input.</p><p>2) Encoder architecture. It extracts a latent feature representation for the masked image, which is then used to predict the original signals at the masked area. The learnt encoder is expected to be transferable to various vision tasks. In this paper, we mainly consider two typical vision Transformer architectures: a vanilla ViT <ref type="bibr" target="#b14">[15]</ref> and Swin Transformer <ref type="bibr" target="#b31">[32]</ref>.</p><p>3) Prediction head. The prediction head will be applied on the latent feature representation to produce one form of the original signals at the masked area.</p><p>4) Prediction target. This component defines the form of original signals to predict. It can be either the raw pixel values or a transformation of the raw pixels. This component also defines the loss type, with typical options including the cross-entropy classification loss and the 1 or 2 regression losses. In the following subsections, we will present typical options of each component. These options are then systematically studied. By combining simple designs of each component, we have been able to achieve strong representation learning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Masking Strategy</head><p>For input transformation of masked area, we follow the NLP community <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> and BEiT <ref type="bibr" target="#b0">[1]</ref> to use a learnable mask token vector to replace each masked patch. The token vector dimension is set the same as that of the other visible patch representation after patch embedding. For masking area selection, we study the following masking strategies (illustrated in Figure <ref type="figure" target="#fig_1">2</ref>): Patch-aligned random masking We first present a patch-aligned random masking strategy. Image patches are the basic processing units of vision Transformers, and it is convenient to operate the masking on patch-level that a patch is either fully visible or fully masked. For Swin Transformer, we consider equivalent patch sizes of different resolution stages, 4×4∼32×32, and adopt 32×32 by default which is the patch size of the last stage. For ViT, we adopt 32×32 as the default masked patch size. Other masking strategies We also try other masking strategies in previous works: 1) <ref type="bibr" target="#b35">[36]</ref> introduces a central region masking strategy. We relax it to be randomly movable on the image. 2) <ref type="bibr" target="#b0">[1]</ref> introduces a complex block-wise masking strategy. We try this mask strategy on two masked patch sizes of 16 × 16 and 32 × 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Prediction Head</head><p>The prediction head can be of arbitrary form and capacity, as long as its input conforms with the encoder output and its output accomplishes the prediction target. Some early works follow auto-encoders to employ a heavy prediction head (decoder) <ref type="bibr" target="#b35">[36]</ref>. In this paper, we show that the prediction head can be made extremely lightweight, as light as a linear layer. We also try heavier heads such as a 2-layer MLP, an inverse Swin-T, and an inverse Swin-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Prediction Targets</head><p>Raw pixel value regression The pixel values are continuous in the color space. A straight-forward option is to predict raw pixels of the masked area by regression. In general, vision architectures usually produce feature maps of downsampled resolution, e.g., 16× in ViT and 32× for most other architectures. To predict all pixel values at a full resolution of input images, we map each feature vector in feature map back to the original resolution, and let this vector take charge of the prediction of corresponding raw pixels.</p><p>For example, on the 32× down-sampled feature maps produced by a Swin Transformer encoder, we apply a 1 × 1 convolution (linear) layer with output dimension of 3072 = 32×32×3 to stand for the RGB values of 32×32 pixels. We also consider lower resolution targets by downsampling the original images by {32×, 16×, 8×, 4×, 2×}, respectively.</p><p>An 1 -loss is employed on the masked pixels:</p><formula xml:id="formula_0">L = 1 Ω(x M ) y M − x M 1 ,<label>(1)</label></formula><p>where x, y ∈ R 3HW ×1 are the input RGB values and the predicted values, respectively; M denotes the set of masked pixels; Ω(•) is the number of elements. We also consider 2 and smooth-1 loss in experiments which perform similarly well, and 1 loss is adopted by default.</p><p>Other prediction targets Previous approaches mostly convert the masked signals to clusters or classes, and then perform a classification task for masked image prediction.</p><p>• Color clustering. In iGPT <ref type="bibr" target="#b6">[7]</ref>, the RGB values are grouped into 512 clusters by k-means using a large amount of natural images. Each pixel is then assigned to the closest cluster center. This approach requires an additional clustering step to generate the 9-bit color palette. In our experiments, we use the 512 cluster centers learnt in iGPT.</p><p>• Vision tokenization. In BEiT <ref type="bibr" target="#b0">[1]</ref>, a discrete VAE (dVAE) network <ref type="bibr" target="#b37">[38]</ref> is employed to transform image patches to dVAE tokens. The token identity is used as the classification target. In this approach, an additional dVAE network needs to be pre-trained.</p><p>• Channel-wise bin color discretization. The R, G, B channels are separately classified, with each channel discretized into equal bins, e.g., 8 and 256 bins used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation protocols</head><p>We follow <ref type="bibr" target="#b0">[1]</ref> to mainly evaluate the quality of learnt representations by fine-tuning on ImageNet-1K image classification, which is a more usable scenario in practice. We will mainly account for this metric in our ablations.</p><p>In the system-level comparison, we also follow previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> to report the performance on previous dominant metric of linear probing. Nevertheless, we will not account on this linear probing metric, as our main goal is to learn representations which can well complement the following down-stream tasks. We adopt Swin-B <ref type="bibr" target="#b31">[32]</ref> as the default backbone in our ablation study. To reduce experimental overhead, we use a default input image size of 192 2 and adapt the window size as 6 to accommodate the changed input image size. The ImageNet-1K image classification dataset is used for both pre-training and fine-tuning. In self-supervised pre-training, we employ an AdamW optimizer <ref type="bibr" target="#b26">[27]</ref> with a cosine learning rate scheduler, and train for 100 epochs. The training hyper-parameters are: the batch size as 2048, base learning rate as 8e-4, weight decay as 0.05, β 1 = 0.9, β 2 = 0.999, warm-up for 10 epochs. A light data augmentation strategy is used: random resize cropping with scale range of [0.67, 1] and a aspect ratio range of [3/4, 4/3], followed by a random flipping and a color normalization steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The default options for the components of SimMIM are: a random masking strategy with a patch size of 32×32 and a mask ratio of 0.6; a linear prediction head with a target image size of 192 2 ; an 1 loss for masked pixel prediction. Our ablation is conducted by varying one option and keeping other settings the same as that of the default.</p><p>In fine-tuning, we also employ an AdamW optimizer, 100-epoch training, and a cosine learning rate scheduler with 10-epoch warm-up. The fine-tunig hyper-parameters are: the batch size as 2048, a base learning rate of 5e-3, a weight decay of 0.05, β 1 = 0.9, β 2 = 0.999, a stochastic  depth <ref type="bibr" target="#b23">[24]</ref> ratio of 0.1, and a layer-wise learning rate decay of 0.9. We follow the same data augmentation used in <ref type="bibr" target="#b0">[1]</ref>,</p><p>including RandAug <ref type="bibr" target="#b9">[10]</ref>, Mixup <ref type="bibr" target="#b54">[56]</ref>, Cutmix <ref type="bibr" target="#b52">[54]</ref>, label smoothing <ref type="bibr" target="#b41">[43]</ref>, and random erasing <ref type="bibr" target="#b57">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Masking Strategy</head><p>We first study how different masking strategies affect the effectiveness of representation learning. The fine-tuning accuracy of different approaches under multiple masking ratios are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We first notice that the best accuracy of our simple random masking strategy reaches 83.0%, which is +0.3% higher than the best of other more specially designed strategies such as the block-wise masking as in <ref type="bibr" target="#b0">[1]</ref>.</p><p>In addition, when a large masked patch size of 32 is adopted, this simple strategy performs stably well on a broad range of masking ratios (10%-70%). We hypothesize that the centering pixel of a large masked patch may be distant enough to visible pixels. Thus it enforces the network to learn relatively long-range connections, even when a low masking ratio is used (e.g., 10%) or all patches around are not masked. Another way to increase the prediction distance is to use larger masking ratio, which also shows to benefit the fine-tuning performance of relatively small patch sizes. By increasing the masking ratio from 0.4 to 0.8 at a patch size of 4, 8 and 16, the accuracy is smoothly improved by +0.2% (from 81.9% to 82.1%), +0.4% (from 82.0% to 82.4%), and +0.4% (from 82.4% to 82.8%), respectively. Nevertheless, the overall accuracy at these smaller patches is not as high as that at a larger patch size of 32. Further increasing the patch size to 64 is observed with degraded accuracy, probably due to the too large prediction distance.</p><p>The above observations and analyses can also be well reflected by a newly proposed AvgDist metric, which measures the averaged Euclidean distance of masked pixels to the nearest visible ones. The AvgDist of different masking strategies w.r.t. varying masking ratios are shown in Figure <ref type="figure" target="#fig_3">3(a)</ref>. From this figure, we observe that the AvgDist of all masking strategies is smoothly increased with growing masking ratios. For random masking strategy, when the masked patch size is low, e.g., 4 or 8, the AvgDist is relatively low and grows slowly with increasing masking ratios. On the other hand, when the patch size is large, e.g., 64, very small masking ratio (e.g. 10%) still makes relatively large AvgDist. The square and block-wise methods produce similarly high AvgDist values as of patch size 64.</p><p>Figure <ref type="figure" target="#fig_3">3</ref>(b) plots the relationship between fine-tuning accuracy and the AvgDist measure, which follows a ridge shape. The entries of high fine-tuning accuracy roughly distribute in a range of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> of AvgDist, while entries with smaller or higher AvgDist perform worse. This indicates that the prediction distance in masked image modeling is encouraged to be moderate, neither too large nor too small. Probably, small distance in masked prediction may let the network learn too much short connections, while large distance may be too difficult to learn. These results also indicate that AvgDist may be a good indicator for the effectiveness of masked image modeling.</p><p>In our experiments, we adopt a masking ratio of 0.6 on patch size of 32 by default, due to its stable performance. Also note that the masking strategies and ratios in the language domain are very different from what explored in our work, which usually adopts a small masking ratio of 15%. We hypothesize that different degrees of information redundancy by two modalities may lead to the different behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Prediction Head</head><p>Table <ref type="table" target="#tab_1">2</ref> ablates the effect of different prediction heads, including a linear layer, a 2-layer MLP, an inverse Swin-T and an inverse Swin-B. While generally heavier heads produce slightly lower losses, for example, 0.3722 (inverse Swin-B) versus 0.3743 (a linear layer), the transferring performances on the down-stream ImageNet-1K task are lower. It indicates that stronger inpainting capability does not necessarily result in better down-stream performance. It is probably because that the capacity is largely wasted in the prediction head, which will not be used in down-stream tasks. There is also a practical drawback, that a heavier prediction head brings higher training costs, e.g., the training cost of using an inverse Swin-B is 2.3× of that by a linear layer.</p><p>Also note that in previous contrastive learning approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>, it is a common practice to use a multilayer MLP head in the pre-text tasks, instead of a linear layer, which makes the latent feature produced by the en- coder moderately distant to the pre-text target, and shows beneficial for the linear probing evaluation metric. In our work, we show that a single linear layer head in our approach, under a fine-tuning metric, has shown competitive or even the optimal transferring performance. It indicates that if our aim is to learn good features for finetuning, the important exploration on head designing in contrastive learning approaches may not be necessary for that of masked image modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Prediction Resolution</head><p>Table <ref type="table" target="#tab_2">3</ref> ablates the effect of varying target resolution. It shows that a large range of resolutions (e.g., 12 2 -192 2 ) perform equally well. The transferring performance drops only at a low resolution of 6 2 , probably because this option throws too much information away. These results imply the information granularity required by the down-stream image classification task. The effects to other more fine-grained down-stream tasks such as object detection or semantic segmentation will be explored in our future study. Note that we adopt a default target resolution of 192 2 in our experiments, due to the equally best transferring accuracy and the negligible computation overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Prediction Target</head><p>Table <ref type="table" target="#tab_4">5</ref> compares the effects of different prediction targets. Several observations can be drawn as follows:</p><p>• The three losses of 1 , smooth-1 , and 2 perform similarly well; • Carefully defined classes by color clustering <ref type="bibr" target="#b6">[7]</ref> or tokenization <ref type="bibr" target="#b0">[1]</ref> perform slightly worse than ours; • A simple color discretization approach by channelwise equal-sized bins (proposed as an alternative option) performs competitive to 1 loss, but it requires a careful tuning of the bin number (e.g., 8-bin). It reveals that it is not necessary to align the target of masked image modeling to be the same classification based Training costs are counted in relative to our approach. † BEiT requires an additional stage to pre-train dVAE, which is not counted.</p><p>as masked language modeling. It is good to align the approach to the own nature of visual signals.</p><p>Prediction or reconstruction? While both auto-encoders and masked image modeling approaches learn a network by recovering the original signals, they are built on different philosophies of visible signal reconstruction and prediction of invisible signals. In our framework, we can instantiate a reconstruction task by also regress the raw pixel values of visible patches in the input. Table <ref type="table" target="#tab_3">4</ref> compares the approach which predicts only the masked area as in our default setting and an alternative to recover both masked and unmasked area. The approach predicting the masked area performs significantly better than that recovering all image pixels as 82.8% vs. 81.7%. This implies that the two tasks are fundamentally different in their internal mechanisms, and the task to predict might be a more promising representation learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to Previous Approaches on ViT-B</head><p>As previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> performed experiments on the ViT architectures, for fair comparison, we also conduct experiments using the ViT-B architecture.</p><p>In pre-training, 800 epochs with a cosine learning rate scheduler and a 20-epoch linear warm-up procedure are em- ployed. All other hyper-parameters strictly follow the same settings as in the ablation study, except that we use a 224 2 input resolution to be the same as in previous approaches.</p><p>In fine-tuning, we adopt a layer-wise learning rate decay of 0.65 following <ref type="bibr" target="#b0">[1]</ref>, and keep all other settings strictly the same as in our ablation study. In linear probing, we follow <ref type="bibr" target="#b0">[1]</ref> to choose an inter-mediate layer of ViT-B which produces the best linear probing accuracy. 100-epoch training with a 5-epoch linear warm-up step is employed. Table <ref type="table" target="#tab_5">6</ref> compares our approach to previous ones on both metrics of fine-tuning and linear probing using ViT-B. Our approach achieves a top-1 accuracy of 83.8% by fine-tuning, which is +0.6% higher than previous best approach <ref type="bibr" target="#b0">[1]</ref>. Also note that our approach reserves the highest training efficiency than others thanks to its simplicity, that it is 2.0×, 1.8×, ∼4.0×, and 1.5× more efficient than that of DINO <ref type="bibr" target="#b4">[5]</ref>, MoCo v3 <ref type="bibr" target="#b8">[9]</ref>, ViT <ref type="bibr" target="#b14">[15]</ref>, and BEiT <ref type="bibr" target="#b0">[1]</ref> (not counting the time for dVAE pre-training), respectively.</p><p>Though our main focus is to learn representations that are better for fine-tuning, we also report the linear probing accuracy of different approaches for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Scaling Experiments with Swin Transformer</head><p>We adopt Swin Transformer of different model sizes for experiments, including Swin-B, Swin-L, SwinV2-H, and SwinV2-G <ref type="bibr" target="#b30">[31]</ref>. To reduce experimental overheads, we adopt a smaller image size of 192 2 in pre-training, and a step learning rate scheduler that the experiments of different training lengths can reuse model training of the first step. The base learning rate of the first learning rate step is set 4e-4 and lasts for 7/8 of the total training epochs. The learning rate is divided by 10 for the remaining epochs. For model sizes of H and G, we use the variants introduced in <ref type="bibr" target="#b30">[31]</ref>, which have stronger stability than the original version. All models use the ImageNet-1K dataset for training, except that SwinV2-G uses a larger and privately collected ImageNet-22K-ext dataset, as detailed in <ref type="bibr" target="#b30">[31]</ref>.</p><p>When using ImageNet-1K for pre-training, all models are trained by 800 epochs, with most other hyper-  Table <ref type="table" target="#tab_6">7</ref> lists the results of our approach with different model sizes, compared to the supervised counterparts. With SimMIM pre-training, all of Swin-B, Swin-L, and SwinV2-H achieve significantly higher accuracy than their supervised counterparts. In addition, the SwinV2-H model with a larger resolution of 512 2 achieves 87.1% top-1 accuracy on ImageNet-1K, which is the highest number among methods that use ImageNet-1K data only.</p><p>While all previous billion-level vision models rely on Google's JFT-3B dataset for model training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b53">55]</ref>, the proposed SimMIM approach is used to aid the training of a 3B SwinV2-G model <ref type="bibr" target="#b30">[31]</ref> by using ∼40× smaller data than that of JFT-3B. It achieves strong performance on four representative vision benchmarks: 84.0% top-1 accuracy on ImageNet-V2 classification <ref type="bibr" target="#b38">[40]</ref>, 63.1/54.4 box/mask mAP on COCO object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>, 59.9 mIoU on ADE20K semantic segmentation <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b58">60]</ref>, and 86.8% top-1 acc on Kinetics-400 action recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref>. More details are described in <ref type="bibr" target="#b30">[31]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>In this section, we attempt to understand the proposed approach as well as some critical designs through visualizations. All example images are from the ImageNet-1K validation set.</p><p>What capability is learned? Figure <ref type="figure" target="#fig_4">4</ref> shows the recovered images with several human-designed masks, to understand what capability is learnt through masked image modeling. The human-designed masks (from left to right) consist of a random mask, a mask to remove most parts of a major object, and a mask to remove all of the major object, respectively. We can draw the following observations: 1) by random masking moderate parts of the major object, both the shape and texture of masked parts can be well recovered, as shown by the penguin, the mountain, the sailboat, and the persons. On the unmasked area, there is a severe checkerboard artifact due to that the recovery of unmasked area is not learnt during training; 2) by masking most parts of a major object (larger than 90%), the model can still predict an existence of object by the negligible clues; 3) when the objects are fully masked out, the masked area will be inpainted with background textures.</p><p>These observations indicate that the approach has learnt strong reasoning ability of objects, and the ability is not due to memorization of image identities or the simple copying of nearby pixels. Prediction v.s. reconstruction We have shown the comparison of the representations learnt by a masked prediction task (our approach), and a joint masked prediction and visible signal reconstruction task in Table <ref type="table" target="#tab_3">4</ref>, which reveals that the pure masked prediction task performs significantly better. Figure <ref type="figure" target="#fig_5">5</ref> compares the recovery effects by two approaches. It shows that the latter approach makes better looking, however, probably the model capacity is wasted at the recovery of the unmasked area which may not be that useful for fine-tuning.</p><p>Effects of masked patch size Figure <ref type="figure" target="#fig_6">6</ref> shows the recovery of an image with different masked patch size under a fixed masking ratio of 0.6. It can be seen that the details can be much better recovered when the masked patch size is smaller, however, the learnt representations transfer worse. Probably, with smaller patch size, the prediction task can be easily accomplished by close-by pixels or textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a simple yet effective self-supervised learning framework, SimMIM, to leverage masked image modeling for representation learning. This framework is made as simple as possible: 1) a random masking strategy with a moderately large masked patch size; 2) predicting raw pixels of RGB values by direct regression task; 3) the prediction head can be as light as a linear layer. We hope our strong results as well as the simple framework can facilitate future study of this line, and encourage in-depth interaction between AI fields.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An illustration of our simple framework for masked language modeling, named SimMIM. It predicts raw pixel values of the randomly masked patches by a lightweight one-layer head, and performs learning using a simple 1 loss.</figDesc><graphic url="image-1.png" coords="1,308.86,227.73,236.25,118.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Illustration of masking area generated by different masking strategies using a same mask ratio of 0.6: square masking<ref type="bibr" target="#b35">[36]</ref>, block-wise masking<ref type="bibr" target="#b0">[1]</ref> apply on 16-sized patches, and our simple random masking strategy on different patch sizes (e.g., 4, 8, 16 and 32).</figDesc><graphic url="image-2.png" coords="4,50.11,72.00,495.01,84.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) AvgDist (averaged distance of masked pixels to the nearest visible pixels) w.r.t. different masking ratios using different masking strategies and different masked patch sizes; (b) finetuning performance (top-1 accuracy) w.r.t. AvgDist.</figDesc><graphic url="image-3.png" coords="5,308.86,72.00,236.22,95.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Recovered images using three different mask types (from left to right): random masking, masking most parts of a major object, and masking the full major object.</figDesc><graphic url="image-4.png" coords="8,50.11,72.00,236.25,136.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Recovered images by two different losses of predicting only the masked area or reconstructing all image area, respectively. For each batch, images from left to right are raw image, masked image, prediction of masked patches only, and reconstruction of all patches, respectively.</figDesc><graphic url="image-5.png" coords="8,50.11,250.60,236.25,62.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. An example of recovered image using masked patch sizes of 4, 8, 16, 32 and 64, and a fixed masked ratio of 0.6.</figDesc><graphic url="image-6.png" coords="8,308.86,72.00,236.24,86.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Mask</cell><cell>Masked</cell><cell>Mask</cell><cell>Top-1</cell></row><row><cell>Type</cell><cell>patch size</cell><cell>ratio</cell><cell>acc (%)</cell></row><row><cell></cell><cell>32</cell><cell>0.11 (2×2)</cell><cell>82.6</cell></row><row><cell>square</cell><cell>32</cell><cell>0.25 (3×3)</cell><cell>82.5</cell></row><row><cell></cell><cell>32</cell><cell>0.44 (4×4)</cell><cell>82.5</cell></row><row><cell></cell><cell>16</cell><cell>0.4</cell><cell>82.7</cell></row><row><cell>block-wise</cell><cell>16</cell><cell>0.6</cell><cell>82.6</cell></row><row><cell></cell><cell>16</cell><cell>0.8</cell><cell>82.4</cell></row><row><cell></cell><cell>32</cell><cell>0.4</cell><cell>82.7</cell></row><row><cell>block-wise</cell><cell>32</cell><cell>0.6</cell><cell>82.6</cell></row><row><cell></cell><cell>32</cell><cell>0.8</cell><cell>82.5</cell></row><row><cell></cell><cell>4</cell><cell>0.4</cell><cell>81.9</cell></row><row><cell>random</cell><cell>4</cell><cell>0.6</cell><cell>82.0</cell></row><row><cell></cell><cell>4</cell><cell>0.8</cell><cell>82.1</cell></row><row><cell></cell><cell>8</cell><cell>0.4</cell><cell>82.0</cell></row><row><cell>random</cell><cell>8</cell><cell>0.6</cell><cell>82.1</cell></row><row><cell></cell><cell>8</cell><cell>0.8</cell><cell>82.4</cell></row><row><cell></cell><cell>16</cell><cell>0.4</cell><cell>82.4</cell></row><row><cell>random</cell><cell>16</cell><cell>0.6</cell><cell>82.7</cell></row><row><cell></cell><cell>16</cell><cell>0.8</cell><cell>82.8</cell></row><row><cell></cell><cell>32</cell><cell>0.1</cell><cell>82.7</cell></row><row><cell></cell><cell>32</cell><cell>0.2</cell><cell>82.8</cell></row><row><cell></cell><cell>32</cell><cell>0.3</cell><cell>82.8</cell></row><row><cell></cell><cell>32</cell><cell>0.4</cell><cell>82.9</cell></row><row><cell>random</cell><cell>32</cell><cell>0.5</cell><cell>83.0</cell></row><row><cell></cell><cell>32</cell><cell>0.6</cell><cell>82.8</cell></row><row><cell></cell><cell>32</cell><cell>0.7</cell><cell>82.7</cell></row><row><cell></cell><cell>32</cell><cell>0.8</cell><cell>82.4</cell></row><row><cell></cell><cell>32</cell><cell>0.9</cell><cell>82.4</cell></row><row><cell>random</cell><cell>64 64</cell><cell>0.1 0.2</cell><cell>82.6 82.6</cell></row></table><note>Ablation on different masking strategies (i.e., square, block-wise, and random) with different masked patch sizes (i.e., 4, 8, 16, 32 and 64).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>. AvgDist. Ablation on different prediction heads. A simple linear layer performs the best with lower training costs.</figDesc><table><row><cell>Head</cell><cell cols="3">#params Training costs Top-1 acc (%)</cell></row><row><cell>Linear</cell><cell>89.9M</cell><cell>1×</cell><cell>82.8</cell></row><row><cell>2-layer MLP</cell><cell>90.9M</cell><cell>1.2×</cell><cell>82.8</cell></row><row><cell>inverse Swin-T</cell><cell>115.2M</cell><cell>1.7×</cell><cell>82.4</cell></row><row><cell>inverse Swin-B</cell><cell>174.8M</cell><cell>2.3×</cell><cell>82.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation on different prediction resolutions. A moderately large resolution (no less than 1/16 all perform well.</figDesc><table><row><cell>Image size</cell><cell>6 2</cell><cell>12 2</cell><cell>24 2</cell><cell>48 2</cell><cell>96 2</cell><cell>192 2</cell></row><row><cell>(ratio of inputs)</cell><cell>(1/32)</cell><cell>(1/16)</cell><cell>(1/8)</cell><cell>(1/4)</cell><cell>(1/2)</cell><cell>(1/1)</cell></row><row><cell cols="2">Top-1 acc (%) 82.3</cell><cell cols="5">82.7 82.8 82.7 82.8 82.8</cell></row><row><cell cols="5">Scope to predict Top-1 acc (%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">masked area</cell><cell cols="2">82.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">full image</cell><cell cols="2">81.7</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation on different performing areas of prediction loss. If the loss is computed at masked area, it performs a pure prediction task. If it is computed on the whole image (both masked &amp; unmasked areas), it performs a joint prediction and reconstruction task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation on different prediction targets.</figDesc><table><row><cell>Loss</cell><cell cols="4">Pred. Resolution Top-1 acc (%)</cell></row><row><cell></cell><cell></cell><cell>Classification</cell><cell></cell><cell></cell></row><row><cell>8-bin</cell><cell></cell><cell>192 2</cell><cell>82.7</cell><cell></cell></row><row><cell>8-bin</cell><cell></cell><cell>48 2</cell><cell>82.7</cell><cell></cell></row><row><cell>256-bin</cell><cell></cell><cell>192 2</cell><cell>N/A</cell><cell></cell></row><row><cell>256-bin</cell><cell></cell><cell>48 2</cell><cell>82.3</cell><cell></cell></row><row><cell cols="2">iGPT cluster</cell><cell>192 2</cell><cell>N/A</cell><cell></cell></row><row><cell cols="2">iGPT cluster</cell><cell>48 2</cell><cell>82.4</cell><cell></cell></row><row><cell>BEiT</cell><cell></cell><cell>-</cell><cell>82.7</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Regression</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell>192 2</cell><cell>82.7</cell><cell></cell></row><row><cell>smooth-1</cell><cell></cell><cell>192 2</cell><cell>82.7</cell><cell></cell></row><row><cell>1</cell><cell></cell><cell>192 2</cell><cell>82.8</cell><cell></cell></row><row><cell>1</cell><cell></cell><cell>48 2</cell><cell>82.7</cell><cell></cell></row><row><cell>1</cell><cell></cell><cell>6 2</cell><cell>82.3</cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">Input Fine-tuning Linear eval Pre-training Size Top-1 acc (%) Top-1 acc (%) costs</cell></row><row><cell cols="2">Sup. baseline [44] 224 2</cell><cell>81.8</cell><cell>-</cell><cell>-</cell></row><row><cell>DINO [5]</cell><cell>224 2</cell><cell>82.8</cell><cell>78.2</cell><cell>2.0×</cell></row><row><cell cols="2">MoCo v3 [9] 224 2</cell><cell>83.2</cell><cell>76.7</cell><cell>1.8×</cell></row><row><cell>ViT [15]</cell><cell>384 2</cell><cell>79.9</cell><cell>-</cell><cell>∼4.0×</cell></row><row><cell>BEiT [1]</cell><cell>224 2</cell><cell>83.2</cell><cell>56.7</cell><cell>1.5×  †</cell></row><row><cell>Ours</cell><cell>224 2</cell><cell>83.8</cell><cell>56.7</cell><cell>1.0×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>System-level comparison using ViT-B as the encoder.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Scaling experiments with Swin Transformer as backbone architectures. All our models are pre-trained with input of 192 2 . Different to other models, Swin-G is trained on a privately collected ImageNet-22K-ext dataset, with details described in<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table><row><cell cols="6">Methods Pre-train Fine-tune Backbone Top-1 acc (%) Param</cell></row><row><cell>Sup.</cell><cell>192 2</cell><cell>224 2</cell><cell>Swin-B</cell><cell>83.3</cell><cell>88M</cell></row><row><cell>Sup.</cell><cell>192 2</cell><cell>224 2</cell><cell>Swin-L</cell><cell>83.5</cell><cell>197M</cell></row><row><cell>Sup.</cell><cell>192 2</cell><cell cols="2">224 2 SwinV2-H</cell><cell>83.3</cell><cell>658M</cell></row><row><cell>Ours</cell><cell>192 2</cell><cell>224 2</cell><cell>Swin-B</cell><cell>84.0</cell><cell>88M</cell></row><row><cell>Ours</cell><cell>192 2</cell><cell>224 2</cell><cell>Swin-L</cell><cell>85.4</cell><cell>197M</cell></row><row><cell>Ours</cell><cell>192 2</cell><cell cols="2">224 2 SwinV2-H</cell><cell>85.7</cell><cell>658M</cell></row><row><cell>Ours</cell><cell>192 2</cell><cell cols="2">512 2 SwinV2-H</cell><cell>87.1</cell><cell>658M</cell></row><row><cell>Ours</cell><cell>192 2</cell><cell cols="2">640 2 SwinV2-G</cell><cell>90.2</cell><cell>3.0B</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank many colleagues at Microsoft for their help, in particular, Li Dong, Furu Wei, Eric Chang, Lidong Zhou, Jing Tao, Aaron Zhang, Edward Cui for useful discussion and the help on GPU resources and datasets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2007">2021. 2, 3, 4, 5, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parametric instance classification for unsupervised visual feature learning</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2006">2020. 1, 2, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations. ICML</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2020. 1, 3, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><surname>David L Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2006">2020. 3, 4, 6</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yizeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04906</idno>
		<title level="m">Dynamic neural networks: A survey</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning. CVPR</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2020. 1, 3, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09844</idno>
		<title level="m">Multi-scale dense networks for resource efficient image classification</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><forename type="middle">N</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">report</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Video swin transformer</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Neural discrete representation learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2003 Papers</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Marc' Aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2021. 2007</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Zero-shot text-to-image generation</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susano</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05974</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susano</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining for image embedding</title>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Trieu H Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Selfie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02940</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16684" to="16693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Spatially adaptive inference with stochastic feature sampling and interpolation</title>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Teck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003">July 2017. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4471" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
