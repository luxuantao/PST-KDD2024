<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local and Global Structure Preservation for Robust Unsupervised Spectral Feature Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaofeng</forename><surname>Zhu</surname></persName>
							<idno type="ORCID">0000-0001-6840-0578</idno>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
							<email>zhangsc@mailbox.gxnu.edu.cn</email>
							<idno type="ORCID">0000-0001-9981-2970</idno>
						</author>
						<author>
							<persName><forename type="first">Rongyao</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yonghua</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
							<email>jingkuan.song@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Guangxi Key Lab of Multi-source Information Mining &amp; Security</orgName>
								<orgName type="institution">Guangxi Normal University</orgName>
								<address>
									<postCode>541004</postCode>
									<settlement>Guilin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer, Electronics, and Information</orgName>
								<orgName type="institution">Guangxi University</orgName>
								<address>
									<postCode>530004</postCode>
									<settlement>Nanning</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>Chengdu Shi</addrLine>
									<postCode>610051</postCode>
									<settlement>Sichuan Sheng</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Local and Global Structure Preservation for Robust Unsupervised Spectral Feature Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">27CEDE463F712CD85320860B01747F07</idno>
					<idno type="DOI">10.1109/TKDE.2017.2763618</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature selection</term>
					<term>graph matrix</term>
					<term>dimensionality reduction</term>
					<term>subspace learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a new unsupervised spectral feature selection method to preserve both the local and global structure of the features as well as the samples. Specifically, our method uses the self-expressiveness of the features to represent each feature by other features for preserving the local structure of features, and a low-rank constraint on the weight matrix to preserve the global structure among samples as well as features. Our method also proposes to learn the graph matrix measuring the similarity of samples for preserving the local structure among samples. Furthermore, we propose a new optimization algorithm to the resulting objective function, which iteratively updates the graph matrix and the intrinsic space so that collaboratively improving each of them. Experimental analysis on 12 benchmark datasets showed that the proposed method outperformed the state-of-the-art feature selection methods in terms of classification performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE goal of feature selection is designed to reduce the dimensionality of the data and keep useful information as much as possible <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The most popular solution for feature selection is to select the features with high-scores based on predefined metric. This makes feature selection certainly remove irrelevant (or uninformative) features, reduce the dimensionality, speed up the execution time, decrease the storage cost, improve the performance of learning models, and so on <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. However, it is still a challenging issue to improve the effectiveness of feature selection. To address this, this paper designs a robust unsupervised spectral feature selection method to preserve the local and global structure among training samples and their corresponding features.</p><p>It is difficult to obtain enough labelled information in many real applications <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, so unsupervised dimensionality reduction is a very practical technique. Therefore, unsupervised spectral feature selection (USFS) method has been developed to incorporate the feature selection and subspace learning into a framework, so as to generate interpretable and robust feature selection models. The USFS has thus attracted extensive research interests <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and has been successfully applied in the domains of data mining and machine learning. For example, Cai et al. <ref type="bibr" target="#b15">[16]</ref> and Zhao et al. <ref type="bibr" target="#b0">[1]</ref> advocated a two-phase USFS method to measure the importance of features in a dataset. They first conduct an eigenvalue decomposition on original data to obtain a graph representation, and then a least square regression between the derived graph representation and the original data is performed with an ' 1 -norm regularizer <ref type="bibr" target="#b15">[16]</ref> and an ' 2;1 -norm regularizer <ref type="bibr" target="#b0">[1]</ref>, respectively. Recently, Du et al. <ref type="bibr" target="#b11">[12]</ref> proposed to learn both the adaptive global structure and local structure among samples in a feature selection model, in which an ' 2;1 -norm regularizer was employed to select important features.</p><p>A common characteristic among previous USFS methods is the construction of the graph matrix on original data. While these feature selection methods have displayed pretty promising in unsupervised spectral feature selection, there are still some limitations that should be addressed for real applications. First, because there are usually noise and redundancy in original data, the constructed graph matrix may be of lowquality to degrade the effectiveness of feature selection models. Second, in some USFS methods, the construction of the graph matrix gives a consideration of preserving either the local structure or the global structure among the samples. This paper advocates to preserve both the local and global structure among training samples because these two kinds of geometry structure have been demonstrated to strengthen the performance of USFS methods due to providing complimentary information to each other <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Third, although the correlation among features has been shown its importance in constructing robust feature selection models <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b20">[21]</ref>, existing USFS methods did not consider that. Lastly, the learning of the graph matrix and the feature selection are carried out in two separated processes. This can easily lead to a suboptimal result, even though each of these two processes could achieve their individual optimization.</p><p>To deal with the above four limitations, in this paper we propose a robust USFS method. We list its main contributions as follows.</p><p>First, different from previous USFS methods, the proposed method efficiently utilizes the feature-level representation property and the low-rank constraint on the weight matrix, respectively, to consider both the local correlation and the global correlation of features for feature selection. As a result, these two kinds of correlations can provide complementary information to each other.</p><p>Second, the proposed method uses the low-rank constraint to preserve the global structure of training samples, and to learn the graph matrix for preserving the local structure of training samples. Accordingly, these two kinds of structure preservations can provide complementary information to each other so that the effectiveness of feature selection is well improved. It is noteworthy that previous USFS methods (e.g., <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>) only preserve one of them.</p><p>Third, in the proposed robust USFS method, the correlation among features and the correlation among training samples are both identified from the intrinsic low-dimensional space of original data. This delivers the profit of avoiding the adversely impact of noise and redundancy in original data. To the best of our knowledge, there is no literature focused on simultaneously learning the feature correlations and the sample correlations. And there is only a few literature such as <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> focused on learning the graph matrix to preserve the local structure of training samples from the intrinsic space, while a number of USFS methods were designed to learn the graph matrix from original data such as <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>.</p><p>Lastly, the proposed method jointly and iteratively performs the graph matrix construction and the feature selection in the intrinsic low-dimensional space. Moreover, our method uses the low-rank constraint to avoid the influence of noise and redundancy, which is not well solved in previous methods. For example, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref> separately carried out them and often resulted in suboptimal results. <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b26">[27]</ref> jointly optimized the graph matrix construction and the feature selection, and learned the graph matrix from original data. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref> jointly and iteratively optimized them and learned the graph matrix from the intrinsic low-dimensional space, but ignoring the influence of noise and redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we first review previous feature selection methods, and then analyze previous USFS methods in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Selection</head><p>In the domain of dimensionality reduction, feature selection methods try to find a subset of the original features, while subspace learning methods transform the high-dimensional data to their low-dimensional space, including linear transformation (such as Principal Component Analysis (PCA) <ref type="bibr" target="#b27">[28]</ref>, Fisher's Linear Discriminant Analysis <ref type="bibr" target="#b28">[29]</ref>, Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b29">[30]</ref>, and Locality Preserving Projection (LPP)) and nonlinear transformation (such as kernel PCA <ref type="bibr" target="#b30">[31]</ref>, kernel LDA <ref type="bibr" target="#b31">[32]</ref>, and kernel CCA <ref type="bibr" target="#b29">[30]</ref>). Usually, feature selection outputs interpretable result and subspace learning leads to robust models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p><p>Since many real applications prefer the interpretable ability of dimensionality reduction, this paper focuses on the study of feature selection. Existing feature selection methods can be partitioned into different categories according to various perspectives. For example, existing feature selection methods can be partitioned into three subgroups via the learning models, such as filter methods (i.e., which selects the important features independent on the learning model <ref type="bibr" target="#b33">[34]</ref>), wrapper methods (which searches the important features guided by accuracy <ref type="bibr" target="#b34">[35]</ref>), and embedded methods (where features are selected to be removed based on the prediction errors during the process of the construction of the models <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>). Feature selection methods can also be parted into unsupervised feature selection methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, supervised feature selection methods <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> and semi-supervised feature selection methods <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, according to the label information.</p><p>In this paper, we focus on the study of USFS since 1) it is an embedded method which has been shown to outperform either filter methods or wrapper methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b42">[43]</ref>; 2) the label information is difficult to be obtained due to all kinds of reasons, such as limited sources and costs <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>; 3) spectral feature selection has been demonstrated to output robust and interpretable result <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Spectral Feature Selection</head><p>USFS methods usually belong to embedded methods and include two key components, i.e., the graph matrix learning to conduct subspace learning and a sparsity-inducing regularizer (such as an ' 1 -norm regularizer and an ' 2;1 -norm regularizer) to conduct feature selection. According to the modes of conducting these two key components, previous USFS methods can be classified into three categorizes, i.e., Sequential USFS methods, joint USFS methods, iteratively joint USFS methods.</p><p>Sequential USFS methods first conduct subspace learning to obtain the graph representation of the data, and then conduct a sparse feature selection between the resulting graph representation and the original data by sparsity-inducing regularizers. For example, the Multi-Cluster Feature Selection (MCFS) method <ref type="bibr" target="#b15">[16]</ref> uses an ' 1 -norm regularizer, while both the Minimize the feature Redundancy for spectral Feature Selection (MRFS) method <ref type="bibr" target="#b44">[45]</ref> and the joint Feature Selection and Subspace Learning (FSFL) method <ref type="bibr" target="#b43">[44]</ref> use the ' 2;1 -norm regularizer. Both MRFS and FSFL outperformed MCFS due to considering the global correlation among the features via the group sparsity, i.e., the ' 2;1 -norm regularizer. The difference between MRFS and FSFL is that MRFS preserves the pairwise sample similarity, i.e., the global correlation among the samples, via a kernel matrix of the feature matrix, while FSFL constructs a sparse kNN graph to preserve the local structure among the samples, i.e., the local correlation among the samples.</p><p>Joint USFS methods jointly conduct subspace learning and sparse feature selection in a framework. Their difference is the method of conducting subspace learning, i.e., the method of the construction of the graph matrix. For example, the Joint Embedding Learning and Sparse Regression (JELSR) method <ref type="bibr" target="#b17">[18]</ref> uses the kNN-based graph Laplacian regularizer to preserve the local structure of the samples, the Robust Spectral learning framework for unsupervised Feature Selection (RSFS) method <ref type="bibr" target="#b16">[17]</ref> utilizes the local kernel regression to capture the nonlinear geometrical information of the samples, and the Nonnegative Discriminative Feature Selection (NDFS) method <ref type="bibr" target="#b35">[36]</ref> learns a pseudo cluster labels and then uses it to learn the graph matrix.</p><p>Iteratively joint USFS methods claim that the selected features highly depend on the learned graph matrix, so they iteratively update the graph matrix and the selected features until the algorithm converges. For example, the unsupervised Feature Selection with Adaptive Structure Learning (FSASL) method <ref type="bibr" target="#b11">[12]</ref> iteratively obtains the adaptive graph matrix and the adaptive features until both of them stop changing. Different from FSASL, the Structured Optimal Graph Feature Selection (SOGFS) method <ref type="bibr" target="#b21">[22]</ref> adds one more constraints (i.e., the consistency of the graph matrix) to iteratively and jointly perform feature selection and the graph matrix learning.</p><p>Finally, the difference between previous USFS methods and our proposed method is listed in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>In this paper, we denote matrices, vectors, and scalars, respectively, as boldface uppercase letters, boldface lowercase letters, and normal italic letters. We summarize other notations used in this paper in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Feature Correlation</head><p>Let the feature matrix X ¼ ½x 1 ; . . . ; x n ¼ ½x 1 ; . . . ; x d 2 R nÂd represent n d-dimensional samples. Motivated by the widely used sample-level self-expressiveness property sparsely representing each sample by other samples <ref type="bibr" target="#b45">[46]</ref>, we utilize the feature-level self-expressiveness property to represent each feature by all features with the following formulation:</p><formula xml:id="formula_0">x i % X d j¼1 x j z j;i ; i ¼ 1; . . . ; d;<label>(1)</label></formula><p>where the element z j;i of the weight matrix Z 2 R dÂd is the weight between the ith feature x i and the jth feature x j . The assumption of Eq. ( <ref type="formula" target="#formula_0">1</ref>) is that 1) the important features should be used to represent other features and should not be represented by the uninformative features, and the uninformative features should be represented by the important features and should be removed out the representation of all the features. With this assumption, Eq. (1) outputs small (or even zero) weight and large weight, respectively, to uninformative features and important features in the right side of Eq. ( <ref type="formula" target="#formula_0">1</ref>). Obviously, Eq. ( <ref type="formula" target="#formula_0">1</ref>) meets the assumption of feature selection, i.e., features are related or redundant on high-dimensional data <ref type="bibr" target="#b7">[8]</ref>.</p><p>By regarding the prediction of each feature as a task and constraining the sparsity across tasks with an ' 2;1 -norm regularizer, we change Eq. ( <ref type="formula" target="#formula_0">1</ref>) to its matrix form and thus have the following least square objective function</p><formula xml:id="formula_1">min Z kX À XZk 2 F þ gkZk 2;1 ;<label>(2)</label></formula><p>where g is a tuning parameter. The ' 2;1 -norm regularizer on Z (i.e., kZk 2;1 ) penalizes Z by encouraging the row sparsity, i.e., elements of some rows of Z are all zeros, to un-select the corresponding features in X.</p><p>Eqs. ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>) indicate that each feature (e.g., x i in lefthand side of Eq. ( <ref type="formula" target="#formula_0">1</ref>)) is represented by a linear combination of a subset of all features in right-hand side of Eq. ( <ref type="formula" target="#formula_0">1</ref>), and the corresponding weight vector is the ith column z i of Z in Eq. ( <ref type="formula" target="#formula_1">2</ref>). Obviously, the larger the values in the z i , the more the corresponding features involve in the representation of the feature x i . In particular, if there is a zero-row in z j (where z j ¼ ½z j;1 ; . . . ; z j;d ), then the corresponding feature (i.e., x j in right-hand side of Eq. ( <ref type="formula" target="#formula_0">1</ref>)) will not participate in the representation of features. That is, the features participating in the representation of all features should be important, while those not participating in the representation process should be discarded by means of feature selection, i.e., kZk 2;1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 2 The Used Notations in This Paper</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>the feature matrix of the training data x a vector of X x i the ith row of X x j the jth column of X x i;j the element in the ith row and the jth column of X jjXjj F the Frobenius norm of X, i.e., jjXjj F ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi P i;j x 2 i;j q jjXjj 2;1 the ' 2;1 -norm of X , i.e., jjXjj 2;1 ¼ P</p><formula xml:id="formula_2">i ffiffiffiffiffiffiffiffiffiffiffiffiffiffi P j x 2 i;j q rankðXÞ the rank of X X T the transpose of X trðXÞ the trace of X X À1</formula><p>the inverse of X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Global Feature Correlation &amp; Global Structure Preservation</head><p>Eq. ( <ref type="formula" target="#formula_1">2</ref>) finds the sparse representation of each feature individually (namely, local feature correlation) but no global constraint on its solution, and thus may be inaccurate at capturing the global structure of the features to largely depress the performance of feature selection on the grossly corrupted features. Since the corrupted data (including noisy/redundant features and outliers) have been indicated to largely increase its rank in real applications, the low-rank constraint has been used to help correct corruption via a low-rank constraint to output robust feature selection models <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Specifically, given a low-rank assumption on Z, i.e., Z ¼ AB, where A 2 R dÂr , B 2 R rÂd , and r minðn; dÞ, Eq. ( <ref type="formula" target="#formula_1">2</ref>) is changed to</p><formula xml:id="formula_3">min A;B kX À XABk 2 F þ gkABk 2;1 ;<label>(3)</label></formula><p>In Eq. ( <ref type="formula" target="#formula_3">3</ref>), the reduced matrix XA 2 R nÂr , which is then multiplied by B to represent the feature matrix X (i.e., the first X in X À XAB), has less than r latent factors. Geometrically, A (or B) has the effect of transforming X (or XA) to a new space, i.e., conducting subspace learning by considering the correlation among d (or r) features (i.e., all features as a group), namely, global feature correlation. Therefore, the low-rank constraint on A (or B) has the effect of subspace learning by considering the global feature correlation. In particular, Eq. ( <ref type="formula" target="#formula_15">13</ref>) further indicates that such subspace learning actually conducts LDA via considering the global feature correlation to preserve the global structure of the samples.</p><p>Unlike Eq. ( <ref type="formula" target="#formula_1">2</ref>) using the feature-level self-expressiveness property to only consider the local feature correlation, Eq. ( <ref type="formula" target="#formula_3">3</ref>) simultaneously uses the feature-level self-expressiveness property and a low-rank constraint (i.e., replacing Z by A and B) to consider the local feature correlation and the global feature correlation, respectively, and thus resulting in local self-expressiveness and global self-expressiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Local Structure Preservation</head><p>Previous literatures have shown that both the global structure and the local structure of the samples may provide complementary information to strengthen the performance of dimensionality reduction <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, so this paper proposes to preserve the global structure of the samples via a low-rank constraint in Eq. ( <ref type="formula" target="#formula_3">3</ref>) and also to preserve the local structure of the samples via learning a graph matrix S 2 R nÂn on a low-dimensional space. Intuitively, given the feature matrix X and its weight matrix W, we follow the literature <ref type="bibr" target="#b6">[7]</ref> to have the following objective function:</p><formula xml:id="formula_4">min W X n i;j kx i W À x j Wk 2 2 s i;j ;<label>(4)</label></formula><p>where W 2 R dÂd is also a transformation matrix transferring the high-dimensional data X to a new space spanned by XW, and the element s i;j of the graph matrix S denotes the similarity between the ith sample x i and the jth sample x j . Moreover, if the ith sample x i is one of the k-nearest neighbors of the jth sample x j , then the value of the heat kernel (i.e., fðx i ;</p><formula xml:id="formula_5">x j Þ ¼ expðÀ kx i Àx j k 2 2 2s 2 Þ</formula><p>where s is a tuning parameter) is regarded as the value of s i;j ; otherwise s i;j ¼ 0.</p><p>Although Eq. ( <ref type="formula" target="#formula_4">4</ref>) has been widely used in previous USFS methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b44">[45]</ref>, it learns a fixed graph matrix S from original high-dimensional data X before learning W. That is, the graph matrix learning is independent on the low-dimensional space learning. In this way, if original data are corrupted by noise and redundancy (it always true in real applications), then an incorrect graph matrix may be outputted. Moreover, Eq. ( <ref type="formula" target="#formula_4">4</ref>) needs tune two parameters (i.e., k and s), which is time-consuming. In particular, the quality of S has been reported very sensitive to the tuning of s <ref type="bibr" target="#b49">[50]</ref>. This motivates us to learn the graph matrix from the 'clean' data (i.e., a low-dimensional space with noise and redundancy as less as possible) and to reduce the number of the tuning parameters. However, the truth is that neither the graph matrix nor the low-dimensional space are known in advance. To address this, we couple the graph matrix learning with the low-dimensional space learning together to iteratively optimize them so that achieving their individually optimal result. As a result, we may learn the graph matrix by following the distribution of the samples, rather than using the heat kernel function to learn a fixed graph matrix, which also needs tune the parameter s. We thus devise the following objective function:</p><formula xml:id="formula_6">min S;W X n i;j ðkx i W À x j Wk 2 2 s i;j þ bks i k 2 2 Þ; s:t:; 8i; s T i 1 ¼ 1; s i;i ¼ 0; s i;j ! 0 if j 2 N ðiÞ; otherwise 0; (5)</formula><p>where b is a tuning parameter, k Á k 2 is the ' 2 -norm of a vector, ks i k 2  2 is used to avoid the trivial solution, 1 and N ðiÞ represent an all-one-element vector and the set of the nearest neighbors of the ith sample, respectively, and the constraint s T i 1 ¼ 1 is used to obtain shift invariant similarity. As a consequence, Eq. (5) outputs small value (i.e., similarity) of s i;j for distant samples and large value of s i;j for close samples.</p><p>Unlike that the USFS methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b44">[45]</ref> use Eq. ( <ref type="formula" target="#formula_4">4</ref>) to learn a fixed graph matrix by tuning two parameters, Eq. ( <ref type="formula">5</ref>) learns a dynamic graph matrix by only tuning a parameter k since the similarity among the samples is learnt according to the distribution of the samples, i.e., the learnt lowdimensional space spanned by XW. Moreover, the dynamic graph matrix is iteratively learnt according to the optimized low-dimensional space so that learning a graph matrix from 'clean' data. Different from the dynamic graph matrix in <ref type="bibr" target="#b21">[22]</ref> representing each sample by all samples, Eq. ( <ref type="formula">5</ref>) represents each sample by only k nearest neighbor samples. Obviously, our proposed method easily avoids the influence of outliers which are usually far away its k nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Objective Function</head><p>Although the graph matrix S is learnt from the lowdimensional space spanned by XW in Eq. ( <ref type="formula">5</ref>), neither the graph matrix S nor the low-dimensional space are known. As a result, Eq. ( <ref type="formula">5</ref>) may output unreliable models. This paper combines the constraints in Eq. (3) with Eq. ( <ref type="formula">5</ref>) to address this issue. Specifically, by regarding the weight matrix W in Eq. ( <ref type="formula">5</ref>) as the low-rank weight matrices AB, i.e., W ¼ AB, we combine Eq. ( <ref type="formula" target="#formula_3">3</ref>) with Eq. ( <ref type="formula">5</ref>) to yield our final objective function as follows:</p><formula xml:id="formula_7">min A;B;S X n i;j kx i AB À x j ABk 2 2 s i;j þ akX À XABk 2 F þ b X n i ks i k 2 2 þ gkABk 2;1 s:t:; 8i; s T i 1 ¼ 1; s i;i ¼ 0; s i;j ! 0 if j 2 N ðiÞ; otherwise 0; (6)</formula><p>where a; b and g are tuning parameters. Eq. ( <ref type="formula">6</ref>) iteratively updates the graph matrix S and the low-rank transformation matrix AB until all of variables achieve their individually optimal result. In this way, Eq. ( <ref type="formula">6</ref>) uses the second term (i.e., the local self-expressiveness of features) and the lowrank constraint on both A and B (i.e., the global selfexpressiveness of features) to consider the local feature correlation and the global feature correlation, respectively, and uses the iteratively updated graph matrix and the lowrank constraint to preserve the local structure of the samples and the global structure of the samples, respectively.</p><p>As a consequence, given the optimal A and B, we calculate the ' 2 -norm values of ðABÞ i ; i ¼ 1; . . . ; d, and then sort them in descending order. We finally select top r features corresponding to the top r ranked ' 2 -norm values as the final result of our proposed feature selection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Optimization</head><p>Eq. ( <ref type="formula">6</ref>) is not jointly convex to all the variables (i.e., A, B, and S), but is convex for each variable while fixing the others. In this paper, we employ the alternative optimization strategy to optimize Eq. ( <ref type="formula">6</ref>), i.e., iteratively optimizing each variable while fixing the others until the algorithm converges. We list the resulting pseudo in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Update B and A by Fixing S</head><p>The optimizations of Eq. ( <ref type="formula">6</ref>) on the variables A and B are convex but non-smooth due to the ' 2;1 -norm regularizer on AB. In this paper, we employ the framework of Iteratively Reweighted Least Square (IRLS) <ref type="bibr" target="#b51">[52]</ref> to optimize Eq. ( <ref type="formula">6</ref>) via iteratively optimizing A and B until the predefined stopping criteria is satisfied.</p><p>With the fixed S, Eq. ( <ref type="formula">6</ref>) is changed to</p><formula xml:id="formula_8">min A;B X i;j kx i AB À x j ABk 2 2 s i;j þ akX À XABk 2 F þ gkABk 2;1 :<label>(7)</label></formula><p>By following the IRLS framework, we rewrite Eq. ( <ref type="formula" target="#formula_8">7</ref>) as</p><formula xml:id="formula_9">min A;B trðB T A T X T LXABÞ þ akX À XABk 2 F þ gtrðB T A T PABÞ;<label>(8)</label></formula><p>where L ¼ Q À S 2 R nÂn is a Laplacian matrix and Q is a diagonal matrix with its ith element q i;i ¼ P n j¼1 s i;j , and the ith element of the diagonal matrix P 2 R dÂd is defined as</p><formula xml:id="formula_10">p ii ¼ 1 2kðABÞ i k 2 2 ; i ¼ 1; . . . ; d<label>(9)</label></formula><p>where ðABÞ i is the ith row of AB. By fixing A, we set the derivative of Eq. ( <ref type="formula" target="#formula_9">8</ref>) with respect to B to zero and solve the resulting equation to obtain</p><formula xml:id="formula_11">B Ã ¼ ðA T S t AÞ À1 A T X T X;<label>(10)</label></formula><p>where</p><formula xml:id="formula_12">S t ¼ X T LX þ aX T X þ gP.</formula><p>Then, we rewrite the Eq. ( <ref type="formula" target="#formula_9">8</ref>) to the following expression,</p><formula xml:id="formula_13">min A;B trðB T A T X T LXABÞ þ atrðX T X À X T XAB À B T A T X T X þ B T A T X T XABÞ þ gtrðB T A T PABÞ:<label>(11)</label></formula><p>By substituting Eq. ( <ref type="formula" target="#formula_11">10</ref>) back into Eq. ( <ref type="formula" target="#formula_13">11</ref>), and Eq. ( <ref type="formula" target="#formula_13">11</ref>) is changed to</p><formula xml:id="formula_14">max A trðX T XAðA T S t AÞ À1 A T X T X þ X T XAðA T S t AÞ À1 A T X T XÞ , max A trðA T S t AÞ À1 A T X T XX T XA:<label>(12)</label></formula><p>Further, we obtain</p><formula xml:id="formula_15">max A trðA T S t AÞ À1 A T S b A;<label>(13)</label></formula><p>where S b ¼ X T XX T X. S t and S b , respectively, are similar to the total-class scatter matrix and the between-class scatter matrix defined in the LDA method <ref type="bibr" target="#b52">[53]</ref>. Therefore, the solution of Eq. ( <ref type="formula" target="#formula_15">13</ref>) can be solved via eigenvalue decomposition, i.e., the global optimal solution of Eq. ( <ref type="formula" target="#formula_15">13</ref>) is the top r eigenvectors of S À1 t S b corresponding to r nonzero eigenvalues. Moreover, similar to the between-class scatter matrix, S b in this work can be regarded as the between-sample correlation matrix, which preserves the global structure of the samples.</p><p>In this way, we can yield A by solving Eq. ( <ref type="formula" target="#formula_15">13</ref>) and then yield B by Eq. <ref type="bibr" target="#b9">(10)</ref>. Moreover, we iteratively update A and B until the resulting objective function value is stable. The detail of optimizing A and B is listed in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Update S by Fixing B and A</head><p>Given the fixed A and B, Eq. ( <ref type="formula">6</ref>) becomes</p><formula xml:id="formula_16">min S X n i;j jjx i AB À x j ABjj 2 2 s i;j þ b X n i;j s 2 i;j s:t:; 8i; s T i 1 ¼ 1; s i;i ¼ 0; s i;j ! 0 if j 2 N ðiÞ; otherwise 0;<label>(14)</label></formula><p>We first yield k nearest neighbors of all samples via calculating their euclidean distance, and then set the value of s i;j as 0 if the jth sample does not belong to one of k nearest neighbors of the ith sample, otherwise, the values s i;j is obtained by Eq. <ref type="bibr" target="#b14">(15)</ref>.</p><p>Since the optimization of S is equal to independently optimize each vector s i ; i ¼ 1; . . . ; n, we further change Eq. ( <ref type="formula" target="#formula_16">14</ref>) to individually optimize s i ; i ¼ 1; . . . ; n, as follows:</p><formula xml:id="formula_17">min s T i 1¼1;s i;i ¼0;s i;j !0 X n j ðkx i AB À x j ABk 2 2 s i;j þ bs 2 i;j Þ:<label>(15)</label></formula><p>By denoting F 2 R nÂn where f i;j ¼ jjx i AB À x j ABjj 2 2 , we rewrite Eq. ( <ref type="formula" target="#formula_17">15</ref>) as follows:</p><formula xml:id="formula_18">min s T i 1¼1;s i;i ¼0;s i;j !0 s i þ 1 2b f i 2 2 ; (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>We further obtain the Lagrangian function of Eq. ( <ref type="formula" target="#formula_18">16</ref>) as, min</p><formula xml:id="formula_20">s i ;t;h s i þ 1 2b f i 2 2 À tðs T i 1 À 1Þ À h T s i ;<label>(17)</label></formula><p>where t and h are the Lagrangian multipliers. According to the Karush-Kuhn-Tucker (KKT) conditions <ref type="bibr" target="#b53">[54]</ref>, we yield the closed-form solution of s i;j ; j ¼ 1; . . . ; n as</p><formula xml:id="formula_21">s i;j ¼ ðÀ 1 2b f i;j þ tÞ þ :<label>(18)</label></formula><p>Algorithm 1. The Pseudo Code of Solving Eq. ( <ref type="formula">6</ref>) In this section, we first prove the convergence of both Algorithms 2 and 1, and then analyze the complexity of Algorithm 1. Finally, we discuss parameters' determination of Algorithm 1.</p><formula xml:id="formula_22">Input: X 2 R nÂd ; a,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.1">Convergence Analysis of Algorithm 2</head><p>We first list the following Lemma:</p><formula xml:id="formula_23">Lemma 1. The inequality ffiffiffi u p À u 2 ffiffi ffi v p ffiffi ffi v p À v 2 ffiffi ffi v p ; (<label>19</label></formula><formula xml:id="formula_24">)</formula><p>is always hold for all positive real numbers of u and v <ref type="bibr" target="#b5">[6]</ref>.</p><p>We then prove the convergence of Algorithm 2 by the following Theorem 1: </p><p>which indicates that By denoting W ¼ AB, we obtain W ðtÞ ¼A ðtÞ B ðtÞ , W ðtþ1Þ ¼A ðtþ1Þ B ðtþ1Þ . According to Eq. ( <ref type="formula" target="#formula_10">9</ref>), Eq <ref type="bibr" target="#b20">(21)</ref> can further be rewritten as follows:</p><formula xml:id="formula_26">trðB ðtþ1Þ T A ðtþ1Þ T X T LXA ðtþ1Þ B ðtþ1Þ Þ þ a X À XA ðtþ1Þ B ðtþ1Þ 2 F þ gtrðB<label>ðtþ1Þ</label></formula><formula xml:id="formula_27">trðW ðtþ1Þ T X T LXW ðtþ1Þ Þ þ a X À XW ðtþ1Þ 2 F þg X d i¼1 w iðtþ1Þ 2 2 w iðtÞ k k 2 trðW ðtÞ T X T LXW ðtÞ Þ þ a X À XW ðtÞ 2 F þgtr X d i¼1 w iðtÞ 2 2 w iðtÞ k k 2 ; (<label>22</label></formula><formula xml:id="formula_28">)</formula><p>where w iðtÞ and w iðtþ1Þ denote ith row of W ðtÞ and W ðtþ1Þ , respectively. According to Lemma 1, we have</p><formula xml:id="formula_29">w iðtþ1Þ 2 À w iðtþ1Þ 2 2 w iðtÞ k k 2 w iðtÞ 2 À w iðtÞ 2 2 w iðtÞ k k 2 :<label>(23)</label></formula><p>By plugging Eq. ( <ref type="formula" target="#formula_29">23</ref>) into Eq. ( <ref type="formula" target="#formula_27">22</ref>), we have</p><formula xml:id="formula_30">trðW ðtþ1Þ T X T LXW ðtþ1Þ Þ þ a X À XW ðtþ1Þ 2 F þg X d i¼1 w iðtþ1Þ 2 trðW ðtÞ T X T LXW ðtÞ Þ þ a X À XW ðtÞ 2 F þg X d i¼1 w iðtÞ 2 :<label>(24)</label></formula><p>We finally have</p><formula xml:id="formula_31">trðW ðtþ1Þ T X T LXW ðtþ1Þ Þ þ a X À XW ðtþ1Þ 2 F þg W<label>ðtþ1Þ</label></formula><formula xml:id="formula_32">2 trðW ðtÞ T X T LXW ðtÞ Þ þ a X À XW ðtÞ 2 F þg W ðtÞ 2 :<label>(25)</label></formula><p>According to Eq. ( <ref type="formula" target="#formula_32">25</ref>), we can know that Algorithm 2 is going to be convergent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.2">Convergence Analysis of Algorithm 1</head><p>We prove the convergence of Algorithm 1 by the following Theorem 2:</p><p>Theorem 2. The objective function value of Eq. ( <ref type="formula">6</ref>) monotonically decreases until Algorithm 1 converges.</p><p>Proof. After the tth iteration, we have obtained the optimal A ðtÞ , B ðtÞ and S ðtÞ . In the (t + 1)th iteration, we need to optimize S ðtþ1Þ by fixing A ðtÞ and B ðtÞ . t u</p><p>According to Eq. ( <ref type="formula" target="#formula_21">18</ref>), we know that s ðtþ1Þ i;j has a closedform solution, i.e., global solution, for all i; j ¼ 1; . . . ; n. Thus we have the following inequality: </p><formula xml:id="formula_33">X n i;j jjx i A ðtÞ B ðtÞ À x j A ðtÞ B ðtÞ jj 2 2 s<label>ðtþ1Þ</label></formula><formula xml:id="formula_34">i;j þ ajjX À XA ðtÞ B ðtÞ jj 2 F þ b X n i jjs<label>ðtþ1Þ</label></formula><p>When fixing S ðtþ1Þ to update A ðtþ1Þ and B ðtþ1Þ , we have the following inequality according to Theorem 1 </p><formula xml:id="formula_36">X n i;j jjx i A ðtþ1Þ B ðtþ1Þ À x j A ðtþ1Þ B ðtþ1Þ jj 2 2 s<label>ðtþ1Þ</label></formula><formula xml:id="formula_37">i;j þ ajjX À XA ðtþ1Þ B ðtþ1Þ jj 2 F þ b X n i jjs ðtþ1Þ i jj 2 2 þ gjjA ðtþ1Þ B ðtþ1Þ jj 2;1 X n i;j jjx i A ðtÞ B ðtÞ À x j A ðtÞ B ðtÞ jj 2 2 s<label>ðtþ1Þ</label></formula><formula xml:id="formula_38">i;j þ ajjX À XA ðtÞ B ðtÞ jj 2 F þ b X n i jjs<label>ðtþ1Þ</label></formula><p>By integrating Eq. ( <ref type="formula" target="#formula_35">26</ref>) with Eq. ( <ref type="formula" target="#formula_39">27</ref>), we obtain</p><formula xml:id="formula_40">X n i;j jjx i A ðtþ1Þ B ðtþ1Þ À x j A ðtþ1Þ B ðtþ1Þ jj 2 2 s<label>ðtþ1Þ</label></formula><formula xml:id="formula_41">i;j þ ajjX À XA ðtþ1Þ B ðtþ1Þ jj 2 F þ b X n i jjs ðtþ1Þ i jj 2 2 þ gjjA<label>ðtþ1Þ</label></formula><p>B ðtþ1Þ jj 2;1 X n i;j jjx i A ðtÞ B ðtÞ À x j A ðtÞ B ðtÞ jj 2 2 s</p><formula xml:id="formula_42">ðtÞ i;j þ ajjX À XA ðtÞ B ðtÞ jj 2 F þ b X n i jjs ðtÞ i jj 2 2 þ</formula><p>gjjA ðtÞ B ðtÞ jj 2;1 :</p><p>From Eq. ( <ref type="formula" target="#formula_43">28</ref>), we know that the objective function value of Eq. ( <ref type="formula">6</ref>) decreases after each iteration of Algorithm 1. Hence, Theorem 2 has been proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.3">Complexity Analysis</head><p>In each iteration, the time cost of Algorithm 1 focuses on the computation cost of X T LX þ aX T X þ gP, ðA T S t AÞ À1 A T X T X in Eq. ( <ref type="formula" target="#formula_11">10</ref>), and f i;j in Eq. ( <ref type="formula" target="#formula_21">18</ref>), and their corresponding complexity are maxfOðnd 2 Þ; Oðn 2 dÞg, Oðr 3 Þ, and Oðnd 2 Þ, where n, d, and r, respectively, are the number of the samples, the features, and the rank of the feature matrix X.</p><p>In our experiments, our method usually converges within 30 iterations, so the time complexity of Algorithm 1 is maxfOðnd 2 Þ; Oðn 2 dÞg (n; d ) r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.4">Parameters' Determination</head><p>The parameter b determines the number of nearest neighbors of samples in the graph representation. Specifically, b ¼ 0 means that only one element in s i does not equal to zero, i.e., the number of nearest neighbors k is 1. b ! 1 means that all elements in s i are non-zero, i.e., the number of nearest neighbors k is n (the number of samples).</p><p>In this paper, we assume that there are k nearest neighbors for each sample. By denoting fi ¼ f fi;1 ; . . . ; fi;n g as a descend order of f i , i ¼ 1; . . . ; n, we know that Eq. ( <ref type="formula" target="#formula_21">18</ref>) indicates the following constraint, i.e., s i;kþ1 ¼ 0 and s i;k &gt; 0. That is</p><formula xml:id="formula_44">À 1 2b fi;kþ1 þ t 0 À 1 2b fi;k þ t &gt; 0 8 &lt; : :<label>(29)</label></formula><p>Based on the constraint s T i 1 ¼ 1, then we have</p><formula xml:id="formula_45">X k j¼1 À 1 2b fi;k þ t ¼ 1 ) t ¼ 1 k þ 1 2kb X k j¼1 fi;k :<label>(30)</label></formula><p>By combining Eq. ( <ref type="formula" target="#formula_44">29</ref>) with Eq. ( <ref type="formula" target="#formula_45">30</ref>), we can obtain with the following inequality with respect to b,</p><formula xml:id="formula_46">k 2 fi;k À 1 2 X k j¼1 fi;j &lt; b k 2 fi;kþ1 À 1 2 X k j¼1 fi;j :<label>(31)</label></formula><p>Finally, we yield a closed-form solution s i which has k nonzero elements, so we ultimately make b set as</p><formula xml:id="formula_47">b ¼ k 2 fi;kþ1 À 1 2 X k v¼1 fi;v ; (<label>32</label></formula><formula xml:id="formula_48">)</formula><p>where k is the number of nearest neighbors of ith samples and can be tuned by cross-validation methods.</p><p>For each i; i ¼ 1; . . . ; n, we totally have Eq. ( <ref type="formula" target="#formula_47">32</ref>) so that we have n different values on b. Hence, in our implementation, we follow the literature <ref type="bibr" target="#b19">[20]</ref> to set the final value of b as the average of n different values on b, i.e.,</p><formula xml:id="formula_49">b ¼ 1 n X n i¼1 k 2 fi;kþ1 À 1 2 X k v¼1 fi;v :<label>(33)</label></formula><p>After fixing b, Eq. ( <ref type="formula">6</ref>) needs to tune the parameters k, a, and g. In this paper, we empirically determine the value of k since Eq. ( <ref type="formula">6</ref>) can automatically adjust the value of s i;j (via assign small value to the neighbors far from the sample) if we set large value to k. a is used to balance the magnitude between P n i;j kx i AB À x j ABk 2 2 s i;j and kX À XABk 2 F , and b is used for controlling the sparsity of AB. In this paper, we employ a cross-validation method to estimate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate our proposed method by comparing with eight comparison methods on twelve data sets in terms of classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We downloaded the data sets (such as HillValley, Ecoli, Cane, and Isolet) and the data sets (such as Yale-32, Colon, WarpAR, Pixraw, Coil, DBWorld and Orl), respectively, from UCI Machine Learning Repository 1 and the website of Feature Selection Data sets. 2 We also downloaded the data set Lung from <ref type="bibr" target="#b54">[55]</ref>.</p><p>These data sets come from all kinds of applications, such as text data (such as Cane, DBWorld, and Isolet), biological data (such as Colon, Ecoli, and Lung), and image data (such as HillValley, Yale-32, WarpAR, Coil, Orl, and Pixraw). Moreover, three of them (such as Colon, DBWorld, and Hill-Valley) are binary data sets and the others are multi-class data sets. The number of features is from 100 to 10,000, and the number of samples varies from 62 to 1,559. In particular, the number of features of seven data sets is larger than the number of samples, such as Colon, DBWorld, Lung, Pixraw, WarpAR, Yale-32, and Orl. This makes the construction of feature selection very challengeable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Methods</head><p>Multi-Cluster Feature Selection <ref type="bibr" target="#b15">[16]</ref> first solves an eigenvalue problem to construct the graph representation, and then utilizes the least square regression to connect the derived graph representation and the original data to rank the features.</p><p>Minimize the feature Redundancy for spectral Feature Selection <ref type="bibr" target="#b0">[1]</ref> uses the ' 2;1 -norm regularizer to replace the ' 1 -norm regularizer in MCFS to rank the features via considering the correlations among the features.</p><p>Nonnegative Discriminative Feature Selection <ref type="bibr" target="#b35">[36]</ref> jointly learns the local geometric structure of the data and the sparse linear regression with an ' 2;1 -norm regularizer.</p><p>Joint Embedding Learning and Sparse Regression <ref type="bibr" target="#b36">[37]</ref> simultaneously takes into account a Laplacian regularizer and the weight matrix to rank the scores of the features.</p><p>Trace Ratio formulation unsupervised Feature Selection (TRFS) <ref type="bibr" target="#b22">[23]</ref> extends the criterion of trace ratio to unsupervised feature selection framework, via combining the k-means method with an ' 2;1 -norm regularizer into the proposed feature selection model.</p><p>Unsupervised Feature Selection with Adaptive Structure Learning <ref type="bibr" target="#b11">[12]</ref> first utilizes the adaptive structure of the data to construct both the global learning and the local learning, and then integrates them with an ' 2;1 -norm regularizer to select the significant features.</p><p>Structured Optimal Graph Feature Selection <ref type="bibr" target="#b21">[22]</ref> learns the global structure among the samples from the lowdimensional feature space to select important features.</p><p>Regularized Self-Representation (RSR) <ref type="bibr" target="#b7">[8]</ref> uses the featurelevel self-representation property to represent each feature by the important features, and then employs the ' 2;1 -norm regularizer to conduct group sparsity on the coefficient matrix, such that filtering the redundant and irrelative features.</p><p>The comparison methods include two sequential USFS methods (such MCFS and MRFS), three joint USFS methods (such as NDFS, JELSR, TRFS), two iteratively joint USFS methods (such as FSASL and SOGFS), and a newly unsupervised feature selection method (i.e., RSR). We also regarded the method using all features to conduct classification tasks as Baseline.</p><p>In the comparison methods, first, RSR only utilizes the feature-level self-representation property to consider the local feature correlation for feature selection. Second, two sequential USFS methods only consider the local structure of the samples. Moreover, they learn the local structure from the original feature space, which may contain noise and redundancy. Furthermore, the sequential steps may result in suboptimal feature selection result. Last, these five joint USFS methods (i.e., NDFS, JELSR, TRFS, FSASL and SOGFS) jointly learn the geometry (either local or global) structure and conduct the selection of the features to avoid the suboptimal issue of sequential USFS methods. By contrast, our proposed method preserves both the global structure and the local structure among the samples, by considering both the global feature correlation and the local feature correlation among the features. Moreover, both the feature correlation and the sample correlation are learnt from the 'clean' data, i.e., the intrinsic low-dimensional space of the original high-dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Setting</head><p>In our experiments, we first used all feature selection methods to selection features, and then ran the SVM classifier on the selected features to conduct classification tasks. For the method Baseline, we directly ran SVM to obtain the classification result.</p><p>We used 10-fold cross-validation to compare all methods. Specifically, we first randomly partitioned the whole data set into 10 subsets. We then selected one subset for testing and used the remaining 9 subsets for training. We repeated the whole process 10 times to avoid the possible bias during data set partitioning for cross-validation. The final result was computed by averaging results from all experiments. We conduct 5-fold cross-validation on the training data to conduct model selection. That is, we separated the training data into five parts, where one of parts is used to validate the model built by the left four parts. In the validation step, we used the grid search method to search the best parameters' combination by the given ranges of the parameters. We selected the parameters' combination with the best classification performance in the validation step to test the testing data. In particular, we empirically set the value of k as 15 and other parameters' range as f10 À3 ; . . . ; 10 3 g for all methods to make fair comparison, where all the methods obtained their best performance.</p><p>We evaluated our method with all the comparison methods via the evaluation metric Average Classification Accuracy (ACA). We also investigated the robustness of our proposed method in terms of three aspects, such as the effect of low-rank constraint, the influence of parameters' setting, and the convergence of our proposed Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Result on Classification Accuracy</head><p>Fig. <ref type="figure" target="#fig_3">1</ref> reported the ACA result of all methods, where the horizontal axis represented the number of the left dimensions after conducting feature selection.</p><p>Obviously, our proposed method achieved the best performance, followed by SOGFS, FSASL, RSR, NDFS, JELSR, TRFS, MRFS, MCFS, and Baseline. For example, our proposed method improved by 10.1 and 24.7 percent, respectively, compared to SOGFS (the best comparison method) in data set Ecoli and MCFS in data set Coil (the worst feature selection method). Besides, we had the following observation.</p><p>First, the classification performance of all feature selection methods first increased and then began to decrease with the increase of the selected features. For example, the ACA results were about 65 and 82 percent, respectively, while keeping the left features as 50 and 200, and then went down to 65 percent while keeping the left features as 300, at the data set Yale-32. This indicated that it is necessary to conduct feature selection for dealing with high-dimensional data since high-dimensional data contain noise or redundancy.</p><p>Second, most of feature selection methods outperformed Baseline, which used all features to conduct classification. For example, our proposed method and MCFS (the worst feature selection method), respectively, improved on average by 17.9 and 7.4 percent, compared to Baseline. This verified the necessary of conducting feature selection for dealing with high-dimensional data again.</p><p>Last but not least, sequential USFS methods (i.e., MCFS and MRFS) were worse than joint USFS methods (i.e., NDFS, TRFS, FSASL, SOGFS and JELSR). For example, the average classification accuracy of our proposed method on average increased by 23.21, 18.83, 4.41, 6.40, 4.33, 15.39, 9.28, 6.53 and 2.36 percent, respectively, than the performance of NDFS, TRFS, RSR, FSASL, SOGFS, JELSR, MCFS and the Baseline, on the data set Ecoli. The reason may be that sequential USFS methods sequentially conduct subspace learning and feature selection to possible result in suboptimal result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Low-Rank Constraint</head><p>We investigated the influence of the effect of different number of ranks (i.e., r 2 f1; 3; 5; 7; 9g) in Eq. ( <ref type="formula">6</ref>) at different data sets, and reported the ACA result in Fig. <ref type="figure" target="#fig_1">2</ref>, where the horizonal axis indicates the number of kept ranks. It is worth noting that the number of real classes in both the binary data sets (such as Colon, HillValley, and DBWorld) and the multi-class data sets (such as Lung and Ecoli) is less than 9, but we still set the rank of their feature matrices as 9 since the real rank of these corresponding data sets is large than 9.</p><p>From Fig. <ref type="figure" target="#fig_1">2</ref>, we observed that the performance with a low-rank constraint in most of cases outperformed the performance of the cases with full-rank. For example, the average classification accuracy of the proposed method with low-rank constraints increased by 1.12, 4.67, 0.27, 1.17 and 1.9 percent, respectively, compared to the results of our proposed method with the full-rank constraint on the data sets Lung, Yale-32, Isolet, Coil, and Orl. This manifested that it is reasonable to analyze high-dimensional data with a lowrank constraint in feature selection. The reason is that the low-rank constraint conducting subspace learning helped find the low-dimensional space of high-dimensional data via considering the global feature correlation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Parameters' Sensitivity</head><p>We tuned the parameters a and g within the range of f10 ðÀ3Þ ; 10 ðÀ2Þ ; . . . ; 10 3 g and listed the results in Fig. <ref type="figure">3</ref>.</p><p>As shown in Fig. <ref type="figure">3</ref>, the proposed method is sensitive to the parameters' setting. That is, different parameter combinations output different classification results. Hence, it is necessary to tune the parameters in our methods. More specifically, a is used to control the magnitude between the local representation term P n i;j kx i AB À x j ABk 2 2 s i;j and the global representation term kX À XABk 2 F , while g in Eq. ( <ref type="formula">6</ref>) is used to adjust the sparsity of AB. In Fig. <ref type="figure">3</ref>, we can find that our method achieves the best performance on the data sets Ecoli and Isolet while setting a ¼ 1, and g ¼ 100. However, our method produces the best ACA 96.03 percent with a ¼ 0:01, and g ¼ 1 for the data set Yale-32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Convergence</head><p>Fig. <ref type="figure" target="#fig_6">4</ref> shows the behavior of the objective values of our proposed optimization algorithm (i.e., Algorithm 1) with respect to the increase of the iterations. In our experiments, we set the stop criteria of both Algorithms 1 and 2 as 10 À3 , i.e., From Fig. <ref type="figure" target="#fig_6">4</ref> we can find 1) the proposed Algorithm 1 to optimize the proposed objective function in Eq. ( <ref type="formula">6</ref>) monotonically decreases the objective function values until Algorithm 1 achieves converges; 2) the proposed Algorithm 1 needs a few iterations (i.e., less than 20) to reach the convergence, which is very efficient.</p><p>It is noteworthy that our proposed Algorithm 2 also achieves convergence within 30 iterations at all data sets. We did not list them due to the limited space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper has proposed a novel unsupervised spectral features selection method by iteratively learning the graph matrix and selecting the features. Specifically, we embedded the feature-level self-expressiveness property, a low-rank constraint, the graph matrix learning, and an ' 2;1 -norm regularizer in a framework, to yield an interpretable and robust low-dimensional space and the graph matrix measuring the similarity in the learnt low-dimensional space. Experimental results on real data sets verified that our proposed method achieved the best classification performance, compared to the state-of-the-art feature selection methods.</p><p>In the future work, we will extend our proposed framework to conduct feature selection on the high-dimensional data with incomplete data since incomplete data sets are often found in industrial applications. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 . 2 F</head><label>12</label><figDesc>The objective function value of Eq. (7) monotonically decreases until Algorithm 2 converges.Proof. While fixing S, we denote the tth iteration of a matrix A as A ðtÞ and A ðtþ1Þ , respectively. According to Algorithm 2, we have &lt; A ðtþ1Þ ; B ðtþ1Þ &gt; ¼ arg min A;B trðB ðtÞ T A ðtÞ T X T LXA ðtÞ B ðtÞ Þ þ a X À XA ðtÞ B ðtÞ þgtrðB ðtÞ T A ðtÞ T P ðtÞ A ðtÞ B ðtÞ Þ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 F</head><label>2</label><figDesc>T A ðtþ1Þ T P ðtÞ A ðtþ1Þ B ðtþ1Þ Þ trðB ðtÞ T A ðtÞ T X T LXA ðtÞ B ðtÞ Þ þ a X À XA ðtÞ B ðtÞ þgtrðB ðtÞ T A ðtÞ T P ðtÞ A ðtÞ B ðtÞ Þ:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>i jj 2 2 þ</head><label>2</label><figDesc>gjjA ðtÞ B ðtÞ jj 2;1 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. ACA result of all methods on all data sets at different number of selected features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. ACA result of our proposed method at different number of ranks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>kobjðtþ1ÞÀobjðtÞk 2 2 objðtÞ 10 À3</head><label>210</label><figDesc>, where objðtÞ represents the tth iteration objective function value of Eq. (6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. ACA result of our proposed Algorithm 1 at different iterations on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 The</head><label>1</label><figDesc>Summarization Between Previous USFS Methods and Our Proposed MethodThis table has five blocks and the last four blocks describe the characteristic of sequential USFS methods, joint USFS methods, iteratively joint USFS methods, and our proposed method, respectively.</figDesc><table><row><cell>Methods</cell><cell>Feature</cell><cell>Sample</cell><cell>Noise &amp;</cell><cell>Dynamic</cell><cell>Joint</cell><cell>Learning</cell></row><row><cell></cell><cell>correlation</cell><cell>correlation</cell><cell>redundancy</cell><cell>graph matrix</cell><cell>learning</cell><cell>space</cell></row><row><cell>MCFS [16]</cell><cell>Â</cell><cell>Local</cell><cell>Â</cell><cell>Â</cell><cell>Â</cell><cell>Original space</cell></row><row><cell>FSFL [44]</cell><cell>Global</cell><cell>Local</cell><cell>Â</cell><cell>Â</cell><cell>Â</cell><cell>Original space</cell></row><row><cell>MRFS [45] JELSR [18] RSFS [17] NDFS [36] FSASL [12] SOGFS [22] Proposed</cell><cell>Global Global Global Global Global Global Local &amp; Global</cell><cell>Global Local Global Local Local Local Local &amp; Global</cell><cell>Â Â Â Â Â Â p</cell><cell>Â Â Â Â p p p</cell><cell>Â p p p p p p</cell><cell>Original space Original space Original space Original space Intrinsic space Intrinsic space Intrinsic space</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>g, and r; Output: A 2 R dÂr , B 2 R rÂd , and S 2 R nÂn ; The Pseudo Code of Solving A and B Input: X 2 R nÂd , L 2 R nÂn , a, k, and r; Output: A 2 R dÂr and B 2 R rÂd ; 1. Initialize P ¼ I 2 R dÂd ;</figDesc><table><row><cell>1. Calculate k nearest neighbors of all samples;</cell></row><row><cell>2. Initialize S by Eq. (5) where W is an identity matrix;</cell></row><row><cell>3. repeat:</cell></row><row><cell>3.1. Update A and B via Algorithm (2);</cell></row><row><cell>3.2. Update S by Eq. (16);</cell></row><row><cell>3.3. Calculate L ¼ Q À S T þS 2 ;</cell></row><row><cell>until converge</cell></row><row><cell>Algorithm 2. 2. repeat:</cell></row><row><cell>2.1. Calculate A by Eq. (13);</cell></row><row><cell>2.2. Calculate B by Eq. (10);</cell></row><row><cell>2.3. Calculate P by Eq. (9);</cell></row><row><cell>until converge</cell></row><row><cell>3.7 Convergence Analysis, Complexity, and</cell></row><row><cell>Parameters' Determination</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>jjx i A ðtÞ B ðtÞ À x j A ðtÞ B ðtÞ jj 2 2 s</figDesc><table><row><cell>i i;j ðtÞ jj 2 2 þ gjjA ðtÞ B ðtÞ jj 2;1 X n i;j</cell></row><row><cell>þ ajjX À XA ðtÞ B ðtÞ jj 2 F þ b X n i jjs ðtÞ i jj 2 2 þ gjjA ðtÞ B ðtÞ jj 2;1 :</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the China Key Research Program (Grant No: 3722016YFB1000905), the Nation Natural Science Foundation of China (Grants No: 61573270 and 61672177), the China 1000-Plan National Distinguished Professorship, the Guangxi Natural Science Foundation (Grant No: 2015GXNSFCB139011), the Guangxi High Institutions Program of Introducing 100 High-Level Overseas Talents, the Guangxi Collaborative Innovation Center of Multi-Source Information Integration and Intelligent Processing, the Research Fund of Guangxi Key Lab of MIMS (16-A-01-01 and 16-A-01-02), the Innovation Project of Guangxi Graduate Education (YCSW2017039), and the Guangxi Bagui Teams for Innovation and Research. Rongyao Hu and Yonghua Zhu have equivalent contributions to this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On similarity preserving feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="619" to="632" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph self-representation method for unsupervised feature selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Missing value estimation for mixed-attribute data sets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="121" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparse Bayesian classification of EEG for brain-computer interface</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2256" to="2267" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient kNN classification with different numbers of nearest neighbors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2017.2673241</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Block-row sparse multiview multilabel learning for image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="450" to="461" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf</title>
		<meeting>Int. Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection by regularized self-representation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C K</forename><surname>Shiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="438" to="446" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse Bayesian learning for obtaining sparsity of eeg frequency bands based feature vectors in motor imagery classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page">1650032</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selftaught dimensionality reduction on the high-dimensional smallsized data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="229" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning k for kNN classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection with adaptive structure learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A sparse embedding and least variance encoding approach to hashing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3737" to="3750" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Frequency recognition in SSVEP-based BCI using multiset canonical correlation analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page">1450013</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint hypergraph learning and sparse regression for feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="291" to="309" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for multi-cluster data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust spectral learning for unsupervised feature selection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Mining</title>
		<meeting>IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="977" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint embedding learning and sparse regression: A framework for unsupervised feature selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="793" to="804" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Think globally, fit locally: Unsupervised learning of low dimensional manifolds</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="119" to="155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph PCA hashing for similarity search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2033" to="2044" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel relational regularization feature selection method for joint regression and classification in AD diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="205" to="214" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection with structured graph optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1302" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection via unified trace ratio formulation and K-means clustering (TRACK)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases</title>
		<meeting>Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="306" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Subspace regularized sparse multitask learning for multiclass neurodegenerative disease identification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="607" to="618" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantization-based hashing: A general framework for scalable image and video retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.03.021</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2017.03.021" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Low-rank graphregularized structured sparse regression for identifying genetic biomarkers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Big Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="414" />
			<date type="published" when="2017-12">Oct.-Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust joint graph sparse coding for unsupervised spectral feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1263" to="1275" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Wiley</publisher>
			<pubPlace>Hoboken, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Mullert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw. Signal Process. IX</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by mixed kernel canonical correlation analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3003" to="3016" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kernel principal component analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch€ Olkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R. M€</forename><surname>Uller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artif. Neural Netw</title>
		<meeting>Int. Conf. Artif. Neural Netw</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">KPCA plus LDA: A complete kernel fisher discriminant framework for feature extraction and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="244" />
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video captioning with attention-based LSTM and semantic consistency</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2045" to="2055" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection using nonnegative spectral analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1026" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint embedding learning and sparse regression: A framework for unsupervised feature selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="793" to="804" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Toward integrating feature selection algorithms for classification and clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pitfalls of supervised feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smialowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinf</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="440" to="443" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient semi-supervised feature selection: Constraint, relevance, and redundancy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Benabdeslem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hindawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1131" to="1143" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semi-supervised feature selection for graph classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discriminative semisupervised feature selection via manifold regularization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-T</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1033" to="1047" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised feature selection methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikhpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sarram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gharaghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A Z</forename><surname>Chahooki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint feature selection and subspace learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1294</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On similarity preserving feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="619" to="632" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sparse reduced-rank regression for simultaneous dimension reduction and variable selection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statistical Assoc</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">500</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Matrix completion from noisy entries</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2057" to="2078" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Global versus local methods in nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
		<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="705" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Globally and locally consistent unsupervised projection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1328" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Globally maximizing, locally minimizing: Unsupervised discriminant projection with applications to face and palm biometrics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="650" to="664" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Iteratively reweighted least squares minimization for sparse recovery</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fornasier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>G€</surname></persName>
		</author>
		<author>
			<persName><surname>Urk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<meeting><address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">His current research interests include large-scale multimedia retrieval, feature selection, sparse learning, data preprocess, and medical image analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="209" />
			<date type="published" when="2002">2002</date>
			<pubPlace>China</pubPlace>
		</imprint>
	</monogr>
	<note>Gene expression correlates of clinical prostate cancer behavior</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">His research interests include data mining and partitioning. He is a senior member of the IEEE</title>
		<imprint>
			<publisher>ACM</publisher>
			<pubPlace>China</pubPlace>
		</imprint>
	</monogr>
	<note>Shichao Zhang is a China 1000-Plan distinguished professor with the Guangxi Normal University</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Rongyao Hu is working toward the master&apos;s degree at Guangxi Normal University, China. His current research interests include data mining and pattern recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Yonghua Zhu is working toward the master&apos;s degree at Guangxi University</title>
		<imprint>
			<pubPlace>China</pubPlace>
		</imprint>
	</monogr>
	<note>His current research interests include data mining and machine learning</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">For more information on this or any other computing topic, please visit our Digital Library at</title>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<ptr target="www.computer.org/publications/dlib" />
		<imprint>
			<pubPlace>Chengdu, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Electronic Science and Technology of China</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include multimedia data analysis and image retrieval</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
