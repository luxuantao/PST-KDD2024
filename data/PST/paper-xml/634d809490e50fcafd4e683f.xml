<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A METHOD FOR ESTIMATING THE GROUPING OF PARTICIPANTS IN CLASSROOM GROUP WORK USING ONLY AUDIO INFORMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Osamu</forename><surname>Ichikawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shiga University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuuto</forename><surname>Shima</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shiga University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takahiro</forename><surname>Nakayama</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hajime</forename><surname>Shirouzu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National Institute for Educational Policy Research</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A METHOD FOR ESTIMATING THE GROUPING OF PARTICIPANTS IN CLASSROOM GROUP WORK USING ONLY AUDIO INFORMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICASSP43922.2022.9746121</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-channel signal processing</term>
					<term>spectral clustering</term>
					<term>correlation analysis</term>
					<term>graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel method for estimating which microphone belongs to the same group in a situation where there are multiple discussion groups in one room, using only audio information. The assumption is that each member wears one close-talk microphone, and that the audio is recorded on their own audio track. Each microphone records the main speech of the associated speaker, as well as the speech of neighboring others that have leaked into the microphone. If the neighboring speaker's leaked speech is coming in clearly, the neighboring speaker can be in proximity. An undirected network is constructed with speakers as nodes and the degrees of leaked speech as the edge weights. Given a target number of discussion groups in a room, network clustering can be applied to obtain subgroup information about which audio tracks belong to the same group. An evaluation experiment was conducted using audio data recorded in workgroup classes at the actual junior high school. In this experiment, the average Rand index of the grouping was 0.995, confirming that practical accuracy can be obtained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Active learning, in which students think and learn on their own, has been proposed as a new form of education. The "knowledge constructive jigsaw methodology," promoted by Consortium for Renovating Education of the Future (CoREF) promotes heuristic learning by dividing the students in a classroom into small groups of three to seven for mutual discussions. To visualize this learning process, each student is fitted with a close-talk microphone to record what they speak, and subsequently, ASR is used to convert their speech data into text <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>. By observing the interactions between participants in a group, we can learn how students gain understanding and discover further questions.</p><p>Because there were multiple discussion groups in one classroom, the use of far-field microphones arrays placed on the centers of desks is not recommended. Therefore, the use of close-talk microphones has become the de facto standard in this field. In that form, a single audio track is associated with a corresponding single participant. Since the analysis of the interactions between participants is done on a group basis, the recorded tracks need to be linked to the respective groups.</p><p>Managing grouping information has become a major workload for teachers. For example, the recording device is assumed to be an IC recorder or tablet PC. In that case, the students can use a recording device with the same number as in the class membership list. Since the students will belong to one of the groups, the teachers should prepare a table of which students (recorders) belong to which group before starting the class. However, even if the grouping is prepared in advance, if there is an imbalance in the number of groups due to a sudden absence of students, etc., it is quite possible to order a temporary move. To reduce the workload of managing the grouping information, it is required to provide a function to automatically estimate which students (recorders) were in the same group after recording.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CONVENTIONAL TECHNOLOGY</head><p>Instead of estimating groupings, methods for estimating the location of speakers and microphones have been actively researched. Using a microphone array placed in the center of the desk, the position and direction of the speaker can be estimated by the MUSIC <ref type="bibr" target="#b2">[3]</ref>[4] method or other methods. Research is also being conducted to estimate the location of sound sources from data collected by multiple microphones distributed on a desk <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref>. Speech separation is also explored <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. However, it cannot work in the case where a speaker with a directional microphone attached to his/her mouth turns his/her face in various directions, as in this study.</p><p>One possible solution would be to record a reference voice, such as a teacher's voice or a chime, at a high level on any of the recorder's microphones and estimate the distance from the source by measuring the time difference between the recorders. If there are multiple reference voices and their physical locations are known, the location of the microphones on the recorders can be estimated. There is a study combining this technology with watermarking <ref type="bibr" target="#b9">[10]</ref>. If the locations of the speakers are determined, we can presume that the people nearby are in one group. However, this method assumes that the times of multiple recorders are perfectly synchronized, which is difficult to achieve with IC recorders that are manually switched on. It could be possible to use special software to synchronize the start of recording, but the problem is that the time synchronization gradually slips due to subtle differences in the clock frequency in the equipment. There is a technique to correct it sequentially <ref type="bibr" target="#b10">[11]</ref>, but it is difficult to compensate for the time synchronization of devices located far enough apart in a classroom that they do not mix with each other's audio.</p><p>Previous studies focusing on sound source localization often implicitly assume that there is only one group in a room and cannot cover the case of multiple discussion groups in a single classroom, as in this study.</p><p>In the following explanation, students are referred to as speakers, and the audio data from the recording device (microphone) associated with one speaker is referred to as the audio track. The challenge is to estimate the group to which a speaker belongs while the speaker and the audio track are tied together. This may be accomplished by linguistic or acoustic approaches.</p><p>In the linguistic approach, the speech tracks of all speakers are converted to text by automatic speech recognition, and groupings are estimated by accumulating pairs of utterances that match the context of the texts. In discourse analysis, the context is discussed as cohesion and coherence <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. However, it is very difficult to judge the context because everyone from different groups may be discussing the same topic. Also, since most utterances are short, only a few words, it is difficult to track the complete context. Also, it should be noted that there may be taciturn people who never say a word, and it is almost impossible to estimate which group they belong to, in the linguistic approach.</p><p>Therefore, the acoustic approach is explored in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head><p>The method proposed in this paper focuses on the "leaked speech" of others that are mixed into each person's microphone. It is expected that each audio track contains the target speaker's voice at a sufficient volume. On the other hand, it has been found that the voices of the surrounding speakers also come in at a very low volume. This is called leaked speech. Even with a noise-canceling close-talk microphone, leakage cannot be completely eliminated. Surrounding speakers vary in loudness, but in general we could say that nearby speakers produce loud leaked speech, while distant speakers produce small leaked speech. In other words, the volume of the leaked speech of speakers in the group is expected to be relatively high, while the volume of the leaked speech of speakers outside the group is expected to be low.</p><p>In the above, the degree of leaked speech was explained as volume, however it can be measured stably by correlation, especially in a noisy space such as a classroom. In this paper, we employed the cross-spectrum phase analysis (CSP) <ref type="bibr" target="#b14">[15]</ref>, which can sensitively detect correlations without depending on the volume or tone. In other words, the CSP coefficient between the leaked speech and its source speech are calculated and used as a metric of the distance between the speakers. Compute this for all speaker pairs and set it as an adjacency matrix as in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>An undirected graph is constructed by using speakers as nodes and giving the weights (edges) between nodes with reference to the adjacency matrix as shown in Figure <ref type="figure" target="#fig_1">2</ref>. By performing graph clustering and estimating the subgraphs within it, we can estimate multiple subgroups connected by large weights of edges as shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Note that the target number of clusters should be given in the above graph clustering. This is because it is natural for the teacher to know the total number of groups. Also, if the target number of clusters is not given, the following problems might be expected. Let's consider one speaker who speaks very loudly. His/her speech will be mixed into everyone else's microphones at a high level. Then, it may happen that the speaker acts as a hub connecting multiple clusters, and as a result, only one cluster can be estimated. Given a target number of clusters, such speaker can also be expected to belong to one cluster with the highest affinity.     The block diagram of the proposed method is shown in Figure <ref type="figure" target="#fig_3">4</ref>. The details of each function are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spectral Analysis</head><p>The input data is the PCM data (audio waveform data) of M tracks, which is stored on the computer. In our experiment, the audio data was recorded at a sampling frequency of 16 kHz. Each audio track is windowed with a frame shift of 10 ms and the complex spectrum is obtained using a 512-point Discrete Fourier Transform (DFT). The complex spectrum of the t-th frame of the m-th audio track is denoted by X(m,k,t). k is the index corresponding to the frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VAD</head><p>For each audio track, a flag is determined for each frame as to whether the speaker associated with that track is speaking or not, and set as Voice Activity Detection (VAD) information V(m,t). Here, a value of 1 indicates that the m-th speaker is speaking at frame t, and a value of 0 indicates that the speaker is not speaking. The determined speech segment will be used as a segment to detect leaks to other audio tracks.</p><p>There are two main types of VAD techniques: modelbased methods <ref type="bibr" target="#b15">[16]</ref> that use MFCCs and spectral features, and power-based methods <ref type="bibr" target="#b16">[17]</ref>[18] that use speech power as a feature. In the assumed scenario, the voice of the speakers next to the subject speaker will also be mixed into the microphone, so we used a power VAD with stricter judgment criteria. In other words, the threshold should be set high enough so that the neighboring speaker's speech is not judged as the target speaker's speech. Weak utterances of the target speaker may be dropped by the setting, this is not a problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adjacency Matrix Estimation</head><p>In this part of the process, the CSP coefficients for each pair of audio tracks are obtained for all the M×(M-1)/2 possible pairs of audio tracks. As the CSP coefficients have dimensions corresponding to the time shift, the largest value in the time dimension is selected and set to the corresponding part of the adjacency matrix. In other words, when the adjacency matrix is α and the indices of the two specified audio tracks are p and q, assign the same value to α(p,q) and α(q,p).</p><p>To calculate the CSP coefficients, it is first necessary to decide which of the two specified audio tracks p and q should be the primary track and which should be the secondary track. The primary and secondary correspond to the direction of speech "leakage". We consider that the speech uttered by the speaker of the primary track is mixed (leaked) into the speech of the secondary track. Leakage essentially occurs in both directions, but it is necessary to consider that there are also silent students in groups. Therefore, the speech of the student with the longer speech segments is treated as the primary track, while the speech of the more reticent student is treated as the secondary track.</p><p>Therefore, if the Equation ( <ref type="formula" target="#formula_0">1</ref>) holds, p should be the primary track and q should the secondary track. (</p><formula xml:id="formula_0">)<label>1</label></formula><p>The CSP coefficients are obtained as a vector of the same dimension as the input width of the Discrete Fourier Transform. This dimension corresponds to the time in sampling units and represents the time difference in synchronization between the two inputs. However, it should be noted that the time difference of synchronization identified by CSP is only within the width of a single audio frame. Since it is necessary to consider the possibility of temporal misalignment beyond the frame between the two audio tracks, we shift the frame of one of the tracks in the range of -R to +R to find the maximum CSP coefficient. The value of R can be given arbitrarily depending on the recordings.</p><p>A new metric representing the entire tracks is obtained as an average of the maximum CSP coefficients over the segments U that is the leak detection target, where the speaker of the primary track is speaking, and the speaker of the secondary is not.</p><p>If p is the primary track and q is the secondary track, then U is given by Equation <ref type="bibr" target="#b1">(2)</ref>.</p><p>𝑈(𝑝, 𝑞, 𝑡, 𝑟) = 𝑉(𝑝, 𝑡)(1 − 𝑉(𝑞, 𝑡 + 𝑟))</p><p>(2) where r is the frame shift varying from -R to +R (see Figure <ref type="figure" target="#fig_7">5</ref>).</p><p>The CSP coefficient ϕ for frame t with frame shift r is obtained by Equation (3). IDFT is an inverse discrete Fourier transform, where * denotes complex conjugate. (3) ϕ is the vector, whose dimension is mapped to the time axis by IDFT. Let C be the maximum value of elements of ϕ. C represents the degree of correlation between both tracks for the frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝜙(𝑝</head><p>𝐶(𝑝, 𝑞, 𝑡, 𝑟) = max 𝑑 𝜙 𝑑 (𝑝, 𝑞, 𝑡, 𝑟) .</p><p>The average value of C is obtained by collecting the C of the frames in the segments where the leak detection is made. This is named 𝐶 ́.</p><p>𝐶 ́(𝑝, 𝑞, 𝑟) = ∑ 𝐶(𝑝, 𝑞, 𝑡, 𝑟)𝑈(𝑝, 𝑞, 𝑡, 𝑟) 𝑡 ∑ 𝑈(𝑝, 𝑞, 𝑡, 𝑟) 𝑡 .</p><p>(</p><formula xml:id="formula_2">)<label>5</label></formula><p>𝐶 ́ is the CSP coefficient value when the frame is shifted by r.</p><p>For this variable 𝐶 ́ , find the maximum value 𝐶 𝑚𝑎𝑥 by varying r from -R to R.</p><p>𝐶 𝑚𝑎𝑥 (𝑝, 𝑞) = max 𝑟 𝐶 ́(𝑝, 𝑞, 𝑟).</p><p>Thus, this is the maximum value of the CSP coefficient set to the adjacency matrix α as 𝛼(𝑝, 𝑞) = 𝛼(𝑞, 𝑝) = { 𝐶 𝑚𝑎𝑥 (𝑝, 𝑞) , 𝑝 ≠ 𝑞 0 , 𝑝 = 𝑞 .</p><p>(7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Clustering</head><p>This process uses the adjacency matrix estimated in the previous step to cluster the audio tracks into multiple subgroups. There are various methods for network clustering, but here we will use spectral clustering <ref type="bibr" target="#b18">[19]</ref>, which allows us to specify the target number of clusters.</p><p>For the adjacency matrix α, α(p,q) is the edge weight that links node p to node q. They correspond to speaker p to speaker q. The matrix α is a symmetric matrix and the diagonal terms are zeros. Since the number of speakers is M, the matrix has M × M dimensions.</p><p>In spectral clustering, the graph Laplacian φ is first estimated from the adjacency matrix α. Here we use the unnormalized graph Laplacian matrix.  Γ 𝑇 = (𝜌 1 , 𝜌 1 , ⋯ , 𝜌 𝑀 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝜑 = 𝜈 − 𝛼</head><p>(11) This can be divided into N groups using a vector clustering method such as k-means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION 4.1. Data</head><p>For the evaluation, we used the real audio data recorded in group work classes in the actual junior high school working with CoREF. The classrooms were furnished with woods. It is the standard size of a Japanese junior high school. The desks were about 60 cm wide. The desks of students in the same group were closely arranged. The closest distance between students of different groups was about 1 meter, as interpreted from the pictures. There were two group work sessions in one class, the first half is called Expert session and the second half is called Jigsaw session. The length of the former is about 5 to 8 minutes, and the latter about 20 to 24 minutes. Three classes (mathematics, science, and Japanese) were evaluated. We measured the performance of automatic grouping for a total of six sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>Examples of the results of automatic grouping are shown in Figure <ref type="figure" target="#fig_10">6</ref> and Figure <ref type="figure" target="#fig_11">7</ref>. Each node in the figure has an ID such as "A-06" or "01-A" (corresponding to the audio track name of the speaker), so you can read from the ID whether it belongs to the correct group or not.</p><p>For each session, we calculated the Rand index, which is a metric of the accuracy of the grouping. Table <ref type="table" target="#tab_0">1</ref> shows the results. The grouping error occurred only once in all six sessions, where it resulted in one node belonging to the wrong group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>Assuming that there are multiple discussion groups in a classroom, we have proposed a novel method to estimate the grouping using only audio information, recorded by the microphones attached to each student. An evaluation experiment was conducted using audio data recorded in actual junior high school classes, and an average Rand index of 0.995 was obtained. This means that out of the six sessions, only one person was misassigned. It was verified that the method has a practical level of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>This work was supported by JSPS KAKENHI (Grant Numbers JP17H06107 and JP19K02999).   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Correlation analysis of audio tracks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Constructing undirected graph.</figDesc><graphic url="image-3.png" coords="2,291.70,357.00,265.11,149.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Image of graph clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Block diagram of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, 𝑞, 𝑡, 𝑟) = 𝐼𝐷𝐹𝑇 [ 𝑋(𝑝, 𝑘, 𝑡)𝑋(𝑞, 𝑘, 𝑡 + 𝑟) * |𝑋(𝑝, 𝑘, 𝑡)||𝑋(𝑞, 𝑘, 𝑡 + 𝑟)|].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Configuring segments for leak detection.</figDesc><graphic url="image-4.png" coords="3,329.09,77.24,213.44,72.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 8 )</head><label>8</label><figDesc>where the order matrix ν is given by𝜈(𝑝, 𝑞) = { ∑ 𝛼(𝑝, 𝑞́) 𝑞́, 𝑝 = 𝑞 0 , 𝑝 ≠ 𝑞 . (9)Next, find the eigenvalues of the graph Laplacian φ, select N eigenvalues in order of decreasing eigenvalue, and find N corresponding eigenvectors. Let them be 𝛾 1 , 𝛾 1 , ⋯ , 𝛾 𝑁 .These vectors are aligned in the column direction to form the matrix Γ. This will be a matrix with M-dimensional rows and N-dimensional columns. Γ = (𝛾 1 , 𝛾 1 , ⋯ , 𝛾 𝑁 ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 10 )</head><label>10</label><figDesc>Next, take the matrix Γ in the row direction and construct M N-dimensional vectors. These are denoted as 𝜌 1 , 𝜌 1 , ⋯ , 𝜌 𝑀 . Each of them corresponds to M speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Results of automatic grouping in Expert Session of the science class.</figDesc><graphic url="image-5.png" coords="4,351.82,327.50,151.50,99.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Results of automatic grouping in Jigsaw Session of the mathematics class.</figDesc><graphic url="image-6.png" coords="4,351.38,459.50,154.89,100.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Experimental Results</cell><cell></cell></row><row><cell>Class</cell><cell>Session</cell><cell>Num. groups</cell><cell>Rand index</cell></row><row><cell>Mathamatics</cell><cell>Expert Jigsaw</cell><cell>7 7</cell><cell>1.00 1.00</cell></row><row><cell>Science</cell><cell>Expert Jigsaw</cell><cell>3 6</cell><cell>1.00 1.00</cell></row><row><cell>Japanese</cell><cell>Expert Jigsaw</cell><cell>6 6</cell><cell>0.97 1.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 09:17:49 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Renovating Assessment for the Future: Design-Based Implementation Research for a Learning-in-Class Monitoring System Based on the Learning Sciences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shirouzu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iikubo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference of the Learning Sciences</title>
				<meeting>International Conference of the Learning Sciences</meeting>
		<imprint>
			<publisher>ICLS</publisher>
			<date type="published" when="2018-07">2018. Jul. 2018</date>
			<biblScope unit="page" from="1807" to="1814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://cloud.ibm.com/apidocs/speech-to-text" />
		<title level="m">IBM Cloud API Docs / Speech to Text</title>
				<meeting><address><addrLine>Last</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple emitter location and signal parameter estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Antennas and Propagation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="276" to="280" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sound Source Localization and Separation in Near Field</title>
		<author>
			<persName><forename type="first">F</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asoh</surname></persName>
		</author>
		<author>
			<persName><surname>Matui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transaction</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2286" to="2294" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Microphone Array Position Self-Calibration from Reverberant Speech Input</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmalenstroeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Acoustic Signal Enhancement</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note>IWAENC</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple sound source localization with iterative updates of DOA permutations in distributed microphone arrays</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wakabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miyazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings in Spring Meeting of Acoustical Society Japan 2021</title>
				<meeting>in Spring Meeting of Acoustical Society Japan 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-talker Speech Recognition Based on Blind Source Separation with Ad Hoc Microphone Array Using Smartphones and Cloud Storage</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ochi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2016</title>
				<meeting>Interspeech 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3369" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Amplitude-based Speech Enhancement with Nonnegative Matrix Factorization for Asynchronous Distributed Recording</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Acoustic Signal Enhancement (IWAENC)</title>
				<meeting>International Workshop on Acoustic Signal Enhancement (IWAENC)</meeting>
		<imprint>
			<date type="published" when="2014-09">2014. Sep. 2014</date>
			<biblScope unit="page" from="203" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meeting Recognition with Asynchronous Distributed Microphone Array Using Block-Wise Refinement of Mask-Based MVDR Beamformer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="5694" to="5698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Watermarked Movie Soundtrack Finds the Position of the Camcorder in a Theater</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Babaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="454" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meeting Recognition with Asynchronous Distributed Microphone Array</title>
		<author>
			<persName><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop</title>
				<meeting>IEEE Automatic Speech Recognition and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discourse parsing: a decision tree approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nomoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Very Large Corpora</title>
				<meeting>the 6th Workshop on Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical investigation of the relation between discourse structure and coreference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tablan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th conference on Computational linguistics (COLING 2000)</title>
				<meeting>the 18th conference on Computational linguistics (COLING 2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acoustic Event Localization Using a Crosspower-Spectrum Phase Based Technique</title>
		<author>
			<persName><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Svaizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="273" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A statistical model-based voice activity detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999-01">Jan. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple but efficient real-time Voice Activity Detection algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Moattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Homayounpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Signal Processing Conference</title>
				<meeting>European Signal Processing Conference</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="2549" to="2553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-Channel VAD for Transcription of Group Discussion</title>
		<author>
			<persName><forename type="first">K</forename><surname>O. Ichikawa1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><surname>Shirouzu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2021</title>
				<meeting>Interspeech 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="336" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
