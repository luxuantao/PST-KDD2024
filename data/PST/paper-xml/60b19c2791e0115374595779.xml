<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence Parallelism: Making 4D Parallelism Possible</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-26">26 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
							<email>xuefuzhao@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
							<email>yongbinli@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>You</surname></persName>
							<email>youy@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence Parallelism: Making 4D Parallelism Possible</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-26">26 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2105.13120v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Within Transformer, self-attention is the key module to learn powerful contextaware representations. However, self-attention suffers from quadratic memory requirements with respect to the sequence length, which limits us to process longer sequence on GPU. In this work, we propose sequence parallelism, a memory efficient parallelism method to help us break input sequence length limitation and train with longer sequence on GPUs. Compared with existing parallelism, our approach no longer requires a single device to hold the whole sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we communicate attention embeddings among GPUs. Inspired by ring all-reduce, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Our implementation is fully based on PyTorch. Without extra compiler or library changes, our approach is compatible with data parallelism and pipeline parallelism. Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7× and 3.0× maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. We plan to integrate our sequence parallelism with data, pipeline and tensor parallelism to further train large-scale models with 4D parallelism in our future work. 2 * Equally-contributed first authors. 2 We will release our code.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer-based language models <ref type="bibr" target="#b0">[Radford et al., 2019</ref><ref type="bibr" target="#b1">, Brown et al., 2020</ref><ref type="bibr" target="#b2">, Devlin et al., 2018]</ref> have achieved impressive performance on various natural language understanding and generation tasks (e.g., Q&amp;A <ref type="bibr" target="#b3">[Qu et al., 2019</ref><ref type="bibr">, Yang et al., 2020]</ref>, relation extraction <ref type="bibr" target="#b5">[Xue et al., 2020a</ref><ref type="bibr">,b, Zhou et al., 2020]</ref>). Recently, Transformer also achieved promising results on computer vision tasks <ref type="bibr" target="#b8">[Dosovitskiy et al., 2020</ref><ref type="bibr" target="#b9">, Zhang et al., 2020</ref><ref type="bibr" target="#b10">, 2021a]</ref> and even on bioinformatics tasks <ref type="bibr" target="#b11">[Elnaggar et al., 2020</ref><ref type="bibr" target="#b12">, Wang et al., 2021]</ref>. These Transformer-based models learn powerful context-aware representation by applying self-attention <ref type="bibr" target="#b13">[Vaswani et al., 2017]</ref> to all pairs of tokens from the input sequence. This mechanism helps Transformed-based models capture long-term dependencies at the token level for sequence modeling. However, despite its effectiveness, self-attention suffers from quadratic memory requirements with respect to sequence length, which limits the length of input sequence when we train the models on GPUs, and unfortunately, long sequence is common in real world applications (e.g., document-level information extraction <ref type="bibr" target="#b7">[Zhou et al., 2020]</ref>, speech separation <ref type="bibr" target="#b14">[Luo et al., 2020]</ref>).</p><p>In this paper, we designed and implemented sequence parallelism, a novel parallelism aiming at training transformer-based models with longer sequences and a larger batch size. In sequence parallelism, we first split the input sequence into multiple chunks along the sequence dimension and feed each sub-sequence chunk to one corresponding GPU. Each GPU thus only holds a part of the full sentence. To apply self-attention to the tokens from different chunks, the main challenge is to compute attention scores and outputs across GPUs. To tackle this problem, we proposed Ring Self-Attention (RSA), which circulates key and value embeddings across GPUs in a ring manner. In this case, each device is just required to keep the attention embeddings corresponding to its own sub-sequence. As a result, our sequence parallelism is memory efficiency, especially for long input sequences.</p><p>To model long sequences, existing works mainly focus on sparse attention <ref type="bibr" target="#b15">[Beltagy et al., 2020</ref><ref type="bibr" target="#b16">, Zaheer et al., 2020</ref><ref type="bibr" target="#b17">, Zhang et al., 2021b]</ref>. These works target at designing an attention mechanism that scales not quadratically but linearly with the sequence length. Then it would help to process documents with thousands of tokens or longer. In this paper, we aim to solve the long sequence modeling problem from the distributed system angle. Compared with sparse attention, we devote to design and implement a system instead of a deep learning algorithm to train Transformer-based model with longer sequences. Current system-level solutions mainly rely on holding fewer trainable parameters on each device. The existing pipeline parallelisms (e.g., pipeline parallelism <ref type="bibr" target="#b18">[Huang et al., 2018]</ref> and tensor parallelism <ref type="bibr" target="#b19">[Shoeybi et al., 2019]</ref>) are designed to cope with a larger model size instead of longer sequences, although they can still process longer sequence to some extend. However, the challenge is, these existing parallelism methods keep the whole sequence on single device, which results in memory redundancy and limits the maximum length of the input sequence. In contrast, our approach splits the whole sequence into multiple devices, making it possible to fit longer input data on the device.</p><p>In summary, our main contributions are as follow:</p><p>• To our best knowledge, our system first proposed to use distributed system to handle long sequence training in Transformer-style models. Our implementation is fully based on PyTorch, which is compatible with data parallelism and pipeline parallelism without extra compiler or library. This makes it possible to integrate sequence parallelism with data, pipeline and tensor parallelism into 4D parallelism to train large-scale models in our future work.</p><p>• Our system breaks the length limitation of Transformer model training. Sequence parallelism splits long sequences into multiple chunks and feed into different devices. It is memory efficient because each device only keep the attention embeddings corresponding to its own sub-sequences.</p><p>• Our system achieves 3.0× maximum sequence length than SoTA (i.e., tensor parallelism) when scaling up 64 NVIDIA P100 GPUs. On shorter sequence modeling, our system is still more memory efficient, which achieves 13.7× maximum batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Self-attention We first briefly review self-attention mechanism from Transformer. For an input sentence X = {x 1 , . . . , x N } with N tokens, we encode every token x into three attention embeddings (i.e., query q, key k, value v). To model the dependency among tokens, self-attention computes the attention scores for each token x i against all other tokens in X by multiplying q i with k of all tokens. The attention scores are then multiplied with v and summed up to give the attention output. For parallel computing, q, k and v of all tokens are combined into three matrices: Q, K and V . The self-attention of an input sentence X is computed by the following formula:</p><formula xml:id="formula_0">Attention(Q, K, V ) = sof tmax( QK T √ d k )V (1)</formula><p>where d k is the dimension of the key.</p><p>Multi-head attention is designed to jointly consider the information from different subspaces of embedding. Compared with self-attention below, multi-head attention has h query, key and value embeddings instead of the single one, where h denotes the number of heads. We obtain these embeddings with identical shapes by linear transformations. The multi-head attention can be described as:</p><formula xml:id="formula_1">M ultiHead(Q, K, V ) = Concat(head 1 , ..., head h )W O (2)</formula><p>where head i = Attention(Q i , K i , V i ) and W denotes the linear transformations. All heads are concatenated and further projected by linear transformation W O .</p><p>Pipeline parallelism Large-scale deep neural networks <ref type="bibr" target="#b20">[Fedus et al., 2021</ref><ref type="bibr" target="#b21">, Raffel et al., 2020]</ref> have shown their effectiveness on various tasks. However, it is challenging to hold the whole model on one single device due to memory limitations. To overcome this difficulty, <ref type="bibr" target="#b18">Huang et al. [2018]</ref> proposed pipeline parallelism, a model parallelism which splits the model layers into different partitions on separate accelerators. As shown in Figure <ref type="figure">1a</ref>, they split the data along the batch dimension into microbatches, and each device can process one microbatch received from the previous device at a time. When computation is pipelined across microbatches, pipelining schemes need to ensure that inputs use consistent weight versions for both forward and backward computation to ensure correct weight update and model convergence <ref type="bibr" target="#b22">[Narayanan et al., 2021]</ref>.</p><p>Tensor parallelism Different from pipeline parallelism which splits models by layer, tensor parallelism (i.e., Megatron) <ref type="bibr" target="#b19">[Shoeybi et al., 2019]</ref>) introduces tensor splitting, where individual layers of the model are partitioned over multiple devices. Similar to our sequence parallelism, tensor parallelism is also designed for Transformer-based models. Each Transformer layer includes a self-attention block and a two-layer multi-layer perceptron (MLP) block. The MLP block can be formalized as: where GeLU is non-linearity activation function, X is the input data, Z and Y are the outputs. Tensor parallelism splits the weight matrices A and B along columns and rows respectively. Then, the first and second GEMM in the MLP block above can be writen as:</p><formula xml:id="formula_2">Y = GeLU(XA), Z = Y B<label>(3)</label></formula><formula xml:id="formula_3">[A] = [ A 1 A 2 ] , [ Y 1 Y 2 ] = [ GeLU(XA 1 ) GeLU(XA 2 ) ] [B] = B 1 B 2 , Z = [ Z 1 + Z 2 ] = [ Y 1 Y 2 ] B 1 B 2<label>(4)</label></formula><p>At the second GEMM, Z 1 and Z 2 need to undergo an all-reduce operation to give the final output before the dropout layer in Transformer.</p><p>Similarly, Megatron splits the tensors in the self-attention layer. For multi-head attention, attention heads are split by column and allocated equally to the devices. The linear layer after the self-attention computation is split by row. An all-reduce operation is needed at the linear layer output to aggregate attention output from all devices. Please refer to Megatron <ref type="bibr" target="#b19">[Shoeybi et al., 2019]</ref> for more details about tensor parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence parallelism</head><p>We propose sequence parallelism for training Transformer with longer sequences. The overview of sequence parallelism is shown in Figure <ref type="figure">2</ref>. Input sequences are split into multiple chunks and the sub-sequences are fed to different corresponding devices. All devices are holding the same trainable parameters but different sub-sequence input chunks. We will introduce and analyse sequence parallelism in detail below. We use the following notation in this section: (1) B: batch size; (2) L: sequence length;</p><p>(3) H: hidden size of linear layers; (4) A: attention head size; (5) Z: number of attention heads; (6) N: number of GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ring Self-Attention</head><p>The main challenge to distribute sub-sequences to multiple devices lies in calculating attention scores across devices. Therefore, we propose Ring Self-Attention (RSA) to compute attention output in a distributed setting. There are two steps in RSA to obtain the final output. Please note, we only consider bidirectional self-attention here to introduce RSA succinctly. We treat all heads equally so it can be extended to multi-head attention directly. </p><formula xml:id="formula_4">(H, 4H N ) (B, L, 4H N ) 32H 2 N + 4BLH N + BLH 2nd linear (B, L, 4H N ) ( 4H N , H) (B, L, H) Sequence parallelism 1st linear (B, L N , H) (H, 4H) (B, L N , 4H) 32H 2 + 5BLH N 2nd linear (B, L N , 4H) (4H, H) (B, L N , H)</formula><p>Given query embeddings {q 1 1 , q 1 2 , ..., q N L }, key embeddings</p><formula xml:id="formula_5">{k 1 1 , k 1 2 , ..., k N L } and value embeddings {v 1 1 , v 1 2 , ..., v N L }</formula><p>, where q n s represents the key embedding of the s th token in the the sequence which is on n th device. We define all key embeddings on n th device as K n . In RSA, n th device holds the corresponding query embeddings Q n , key embeddings K n and value embeddings V n . The embeddings on n th device correspond to the n th chunk whose sub-sequence length is L/N . Our goal is to obtain Attention n (Q n , K, V ) which is the self-attention layer output on n th device. To this end, as shown in Figure <ref type="figure">3a</ref>, we first transmit the key embeddings among devices to calculate the attention scores QK T in a circular fashion. Such communication needs to be conducted N − 1 times to make sure the query embeddings of each sub-sequence can multiply all the key embeddings. To be more specific, each device will compute the partial attention scores based on its local query and key embeddings first. Afterwards, it will receive different key embeddings from the previous device and calculate the partial attention scores with respect to the new key embeddings for each ring-style communication. As a result, all query embeddings {Q 1 , Q 2 , ..., Q N } collected their corresponding attention scores {S 1 , S 2 , ..., S N } on their own devices.</p><p>In the second stage of RSA, we can calculate the self-attention layer output {O 1 , O 2 , ..., O N } based on {S 1 , S 2 , ..., S N } and {V 1 , V 2 , ..., V N }. Since computing O n requires S n and all value embeddings, as we described in Figure <ref type="figure">3b</ref>, we transmit all value embeddings instead of key embeddings in a similar way. For O n , we calculate S n V by:</p><formula xml:id="formula_6">O n = S n V = N i=1 S n i V i<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">V i = V n , S n i is S n after column splitting, which means S n i ∈ R L/N ×L/N but S n ∈ R L/N ×L .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modelling</head><p>We analyse and compare our sequence parallelism with our direct baseline (i.e., tensor parallelism).</p><p>To our best knowledge, sequence parallelism is the first system designed for breaking the length limitation of sequence. Therefore, as a distributed training system designed for Transformer, our direct baseline should be a SoTA model parallelism. Recently, <ref type="bibr" target="#b22">Narayanan et al. [2021]</ref> proposed Megatron2. They combined data, pipeline and tensor parallelism and achieved SoTA scaling performance on language model training. Our sequence parallelism is also compatible with both data parallelism and pipeline parallelism. We thus select tensor parallelism proposed in Megatron as our strong and direct baseline in both theoretical modeling and experiments. We expect our system can outperform tensor parallelism with and without pipeline parallelism.</p><p>We mainly focus on two aspects, memory usage and communication cost. According to the architecture of Transformer, the comparison is divided into two parts, MLP block and multi-head attention block. In this part, we consider multi-head attention instead of self-attention for a fair and accurate comparison. We assume the optimizer is Adam which is used in Megatron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP block</head><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, for the MLP blocks, tensor parallelism stores the matrixes after row or column-style splitting of whole sequence. Our sequence parallel stores the matrixes without  </p><formula xml:id="formula_8">Q/K/V (B, L, H) (H, ZA N ) (B, Z N , L, A) QK T (B, Z N , L, A) (B, Z N , L, A) (B, Z N , L, L) 16AZH N + 4BLZA N AV (B, Z N , L, L) (B, Z N , L, A) (B, Z N , L, A) + BZL 2 N + BLH Linear (B, Z N , L, A) ( AZ N , H) (B, L, H) Sequence parallelism Q/K/V (B, L N , H) (H, AZ) (B, Z, L N , A) Ring-QK T (B, Z, L N , A) (B, Z, L N , A) (B, Z, L N , L) 16AZH + 4BZLA N Ring-AV (B, Z, L N , L) (B, Z, L N , A) (B, Z, L N , A) + BZL 2 N + BLH N Linear (B, Z, L N , A) (AZ, H) (B, L N , H) ,WHUDWLRQ</formula><formula xml:id="formula_9">32H 2 N + 4BLH N + BLH &gt; 32H 2 + N<label>(6)</label></formula><p>We can find that, in MLP block, sequence parallelism is more memory efficient when BL &gt; 32H.</p><p>As for communication, an all-reduce operation is needed in both the forward pass and backward pass in the MLP block of Megatron due to tensor splitting. As our sequence parallelism does not split the linear layer weights, no all-reduce is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head attention block</head><p>We compare the memory usage of multi-head attention block in Table <ref type="table" target="#tab_1">2</ref>. Tensor parallelism splits the attention heads here, but our sequence parallelism still splits the length dimension of the sequence data. By comparing the memory usages of tensor parallelism and sequence parallelism of multi-head attention block, we can find sequence parallelism is more memory efficient if BL &gt; 16AZ. As for communication, tensor parallelism needs an all-reduce operation in both the forward pass and backward pass when calculating the attention output. In our RSA, to facilitate tensor exchange between devices, our communication is equivalent to 2 all-reduce operations in the forward pass and 4 all-reduce operations in the backward pass. The extra communication cost of RSA can be offset by the lack of communication cost in the MLP block.</p><p>In both MLP block and multi-head attention block, our sequence parallelism is more memory efficient when we train Transformer with a longer sequence and a larger batch size. We conducted our experiments on the Piz Daint supercomputer provided by Swiss National Supercomputing Center (CSCS). The Piz Daint supercomputer provides one P100 GPU (16GB GPU RAM) for each compute node and the compute nodes are connected by a high-bandwidth network. We chose two bidirectional language models, namely BERT Large and BERT Base, to evaluate our sequence parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convergence performance</head><p>We first verified the convergence performance of sequence parallelism. We used the Wikipedia dataset <ref type="bibr" target="#b2">[Devlin et al., 2018]</ref> and evaluated Megatron and our model on the development set every 1k iterations. We trained the BERT Large model for 50k iterations with the default hyper-parameters used by Megatron. Our goal here is to verify the correctness of our implementation so we trained the model for fewer steps. We set parallel size as 4 for tensor parallelism in Megatron and sequence parallelism in our model. No pipelining was used for both models. In Figure <ref type="figure" target="#fig_2">4</ref>, Our sequence parallelism shows good convergence on both the masked language modeling (MLM) loss and the sentence order prediction (SOP) loss. Compared with Megatron, sequence parallelism has a similar trend in convergence and achieved lower values for both MLM loss and SOP loss for 50k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Maximum batch size</head><p>As we discussed in Section 3.2, since our sequence parallelism is memory efficient to handle larger batch size, we first investigated the maximum batch size we can reach with sequence parallelism. In this section, for a comprehensive comparison, we scaled with both the tensor and sequence parallelism and added pipeline parallelism to evaluate the performance on the BERT Base and BERT Large models. We used tokens per second as the metric for throughput. To this end, we trained the model for 150 iterations in total, and then we calculate the mean tokens processed per second within last 100 iterations. Scaling with sequence/tensor parallelism In this section, we fixed all hyper-parameters except the batch size and the tensor or sequence parallel size. We trained the model with a sequence length 512 and no pipeline parallelism is used. The tensor parallel size in Megatron is limited by the number of attention heads and hidden size, because these two hyper-parameters are required to be divisible by the tensor parallel size. Among them, the number of attention heads is small so it limits the tensor parallel size. Thus, the tensor parallel size is maximum 12 for the BERT Base model and 16 for the BERT Large model in Megatron. In contrast, for our sequence parallelism, only the sequence length is required to be divisible by the sequence parallel size so that we can scale sequence parallel to larger size since it is a much larger hyper-parameter than number of attention heads.</p><p>Our sequence parallelism outperforms tensor parallelism in terms of memory consumption. Our method achieved 2.7 times larger batch size for BERT Large on 16 GPUs as shown in Figure <ref type="figure">5b</ref>, and the batch size of sequence parallelism on 64 GPUs is 10.2 times larger than that of tensor parallelism on 16 GPUs. Even if we combine data and tensor parallelism to scale up to 64 GPUs for Megatron, our system would still support larger batch size. As for BERT Base, Figure <ref type="figure">5a</ref> shows that our model on 64 GPUs can achieve 13.7 times larger batch size than Megatron on 12 GPUs. In Figure <ref type="figure">5c and 5d</ref>, we can observe our sequence parallelism achieved comparable throughput with the same parallel size, and our system can extend to larger parallel size to achieve better performance.</p><p>Scaling with pipeline parallelism To verify the compatibility with pipeline parallelism, we fixed the tensor or sequence parallel size as 4 and scale the pipeline parallel size. We can observe that sequence parallelism still outperforms tensor parallelism on the maximum batch size in Figure <ref type="figure" target="#fig_4">6a</ref> and 6b. It can be noted that sequence parallelism also achieved higher throughput when using more pipeline stages as shown in Figure <ref type="figure" target="#fig_4">6c</ref> and 6d. This is because that Megatron incurs extra communication cost between pipeline stages. Megatron holds the activation for the full sequence on each device. Thus, it needs to split the activation, transmit the partial activation to the next device, and gather back the partial activation when sending the activation between pipelines. This incurs less communication overhead compared to transmit the whole activation between pipelines. However, this still brings more communication cost than our pipeline parallelism as no splitting and all-gather operation are needed for our sub-sequence intermediate activation. Therefore, our sequence parallelism achieved better throughput when scaling along pipeline parallel size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Maximum sequence length</head><p>Sequence parallelism is designed for training Transformer-based models with longer input sequences so we investigated the maximum sequence length it can handle. Similarly, we still compared with tensor parallelism with and without pipeline parallelism. Scaling on both the BERT Base and BERT Large are covered in our experiments. We fixed batch size as 64 for BERT Base and 16 for BERT Large. No pipeline parallelism was used.</p><p>We show the maximum sequence length of the BERT Base and Large models in Figure <ref type="figure" target="#fig_5">7</ref>. If we scale up to 64 GPUs, we can achieve around 3× and 2× maximum sequence length on BERT base and BERT large, respectively. Another observation is splitting along number of attention heads limits the input sequence length of tensor parallelism in Megatron, but our sequence parallelism can scale easily by splitting a long sequence into multiple chunks. When using the same 16 GPUs, our sequence parallelism still can achieve 1.4 times larger sequence length than tensor parallelism. The gap is expected to widen if we use 32GB GPUs instead of 16GB GPUs. Also, in Appendix A, we investigated the maximum sequence length our system can handle when the we use smaller batch size.</p><p>Our RSA focuses on full self-attention in this paper, and we leave extending our system to sparse attention as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Weak scaling</head><p>Both sequence and tensor parallelism are to handle longer sequences in this paper. Strong scaling limits the upper bound of batch size and sequence length within single device in the beginning so we mainly discuss weak scaling on BERT Large here. In weak scaling experiments, we scale the batch size and sequence size separately when increasing the number of nodes. We fixed the pipeline parallel size as 8. In Table <ref type="table" target="#tab_2">3</ref>, we can observe sequence parallelism achieved almost constant memory usage when scaling along the global batch size, which outperforms tensor parallelism by a large margin. As for weak scale along the sequence length dimension, our method still uses much less memory with comparable throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and discussion</head><p>In this paper, we proposed sequence parallelism for training Transformer-based models with a longer sequence. Sequence parallelism is designed to break the limitation of sequence length on a single device (i.e., GPU). We show that sequence parallelism can handle longer sequence and is more memory-efficient than SoTA. In particular, sequence parallelism achieves 3.0× maximum sequence and 13.7× maximum batch size length than tensor parallelism when scaling up to 64 GPUs. Experiments also show better compatibility than tensor parallelism. Unlike both tensor and pipeline parallelism, sequence parallelism is not limited by the smaller hyper-parameters (e.g., number of attention heads, number of layers). Therefore, our sequence parallelism can be adapted as long as the sequence length is divisible by sequence parallel size. We used a language model (i.e., BERT) to evaluate our system. However, sequence parallelism can also be adapted to computer vision tasks. This work paves the way to process large images <ref type="bibr" target="#b23">[Hou et al., 2019]</ref> by ViT <ref type="bibr" target="#b8">[Dosovitskiy et al., 2020]</ref> and MLP-Mixer <ref type="bibr" target="#b24">[Tolstikhin et al., 2021]</ref> as larger image means more patches or longer sequences.</p><p>In the future, we plan to integrate data, pipeline, tensor and sequence parallelism <ref type="bibr" target="#b22">[Narayanan et al., 2021]</ref> to construct 4D parallelism. This would enable us to train extremely large models with very long sequences. Maximum sequence length To investigate the maximum sequence length our system can handle on the cluster with 64 P100 GPUs, we set both data and pipeline parallel size as 1 and global batch size as 16. Please note that we set the batch size as 64 in Section 4.4. We select BERT base as the Transformer based model. As shown in Figure <ref type="figure" target="#fig_6">8</ref>, our sequence parallelism can even handle the sequence with over 5000 tokens using full multi-head attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Different model parallel approaches of Transformer-based models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3: Ring Self-Attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence performance comparison between tensor parallelism and ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5: Scaling with sequence/tensor parallelism</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Scaling with pipeline parallelism</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Scaling with sequence length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Scaling with sequence length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MLP block memory usage comparison. M1 means the matrix before linear layer, and M2 is the trainable matrix of linear layer.</figDesc><table><row><cell></cell><cell>GEMM</cell><cell>M1</cell><cell>M2</cell><cell>output</cell><cell>Memory</cell></row><row><cell>Tensor parallelism</cell><cell>1st linear</cell><cell>(B, L, H)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Multi-head attention block memory usage comparison</figDesc><table><row><cell>Operation</cell><cell>M1</cell><cell>M2</cell><cell>output</cell><cell>Memory</cell></row><row><cell>Tensor</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>parallelism</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Weak scaling results. P is the tensor or sequence parallel size. B and S are global batch size and sequence length, respectively. M and T denote max allocated memory/MB and tokens processed per second. OOM means that CUDA out of memory occurs.</figDesc><table><row><cell>P</cell><cell>B</cell><cell>S</cell><cell cols="2">Tensor parallelism M T</cell><cell cols="2">Sequence parallelism M T</cell></row><row><cell cols="2">1 64</cell><cell>512</cell><cell>8477.28</cell><cell>9946.15</cell><cell>8477.53</cell><cell>9261.04</cell></row><row><cell cols="3">2 128 512</cell><cell>9520.47</cell><cell cols="2">15510.19 8478.76</cell><cell>13938.22</cell></row><row><cell cols="3">4 256 512</cell><cell cols="3">12232.52 20701.96 8481.26</cell><cell>21269.91</cell></row><row><cell cols="3">8 512 512</cell><cell>OOM</cell><cell>OOM</cell><cell>8490.75</cell><cell>26401.64</cell></row><row><cell cols="2">1 64</cell><cell>256</cell><cell>3707.39</cell><cell>9752.61</cell><cell>3707.01</cell><cell>9340.13</cell></row><row><cell cols="2">2 64</cell><cell>512</cell><cell>4993.43</cell><cell cols="2">14195.17 4670.64</cell><cell>13144.16</cell></row><row><cell cols="2">4 64</cell><cell cols="2">1024 8175.93</cell><cell cols="2">19879.27 6601.88</cell><cell>18243.82</cell></row><row><cell cols="2">8 64</cell><cell cols="3">2048 14862.09 22330.5</cell><cell cols="2">10536.38 21625.51</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert with history answer embedding for conversational question answering</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1133" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert representations for video question answering</title>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haruo</forename><surname>Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
				<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1556" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gdpnet: Refining latent multi-view graph for relation extraction</title>
		<author>
			<persName><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eng</forename><surname>Siong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chng</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06780</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An embarrassingly simple model for dialogue relation extraction</title>
		<author>
			<persName><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eng</forename><surname>Siong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chng</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13873</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11304</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Span-based localizing network for natural language video localization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.585</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.585" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="6543" to="6554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language video localization: A revisit in span-based question answering framework</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangli</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Siow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mong</forename><surname>Goh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3060449</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Prottrans: Towards cracking the language of life&apos;s code through self-supervised deep learning and high performance computing</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghalia</forename><surname>Rihawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06225</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pssm-distil: Protein secondary structure prediction (pssp) on lowquality pssm by knowledge distillation with contrastive learning</title>
		<author>
			<persName><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenlei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for timedomain single-channel speech separation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Poolingformer: Long document modeling with pooling attention</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04473</idno>
		<title level="m">Efficient large-scale language model training on gpu clusters</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">High resolution medical image analysis with spatial partitioning</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Korfiatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Blezek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
