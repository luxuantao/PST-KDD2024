<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evolving an artificial neural network classifier for condition monitoring of rotating mechanical systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Abhinav</forename><surname>Saxena</surname></persName>
							<email>asaxena@ece.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashraf</forename><surname>Saad</surname></persName>
							<email>ashraf.saad@gatech.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<postCode>31407</postCode>
									<settlement>Savannah</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evolving an artificial neural network classifier for condition monitoring of rotating mechanical systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F0FF29B47AB5034F1C946BF112F16EFF</idno>
					<idno type="DOI">10.1016/j.asoc.2005.10.001</idno>
					<note type="submission">Received 6 September 2005; received in revised form 28 September 2005; accepted 25 October 2005</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Genetic algorithms</term>
					<term>Artificial neural networks</term>
					<term>Hybrid techniques</term>
					<term>Fault diagnosis</term>
					<term>Condition monitoring</term>
					<term>Rotating mechanical systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the results of our investigation into the use of genetic algorithms (GAs) for identifying near optimal design parameters of diagnostic systems that are based on artificial neural networks (ANNs) for condition monitoring of mechanical systems. ANNs have been widely used for health diagnosis of mechanical bearing using features extracted from vibration and acoustic emission signals. However, different sensors and the corresponding features exhibit varied response to different faults. Moreover, a number of different features can be used as inputs to a classifier ANN. Identification of the most useful features is important for an efficient classification as opposed to using all features from all channels, leading to very high computational cost and is, consequently, not desirable. Furthermore, determining the ANN structure is a fundamental design issue and can be critical for the classification performance. We show that a GA can be used to select a smaller subset of features that together form a genetically fit family for successful fault identification and classification tasks. At the same time, an appropriate structure of the ANN, in terms of the number of nodes in the hidden layer, can be determined, resulting in improved performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the increase in production capabilities of modern manufacturing systems, plants are expected to run continuously for extended hours. As a result, unexpected downtime due to machinery failure has become more costly than ever before. Therefore, condition monitoring is gaining importance in industry because of the need to increase machine availability and health trending, to warn of impending failure and/or to shut down a machine in order to prevent further damage. It is required to detect, identify and then classify different kinds of failure modes that can occur within a machine system. Often several different kinds of sensors are employed at different positions to sense a variety of possible failure modes. Features are then calculated to analyze the signals from all these sensors to assess the health of the system.</p><p>Traditionally, two basic approaches have been used: the use of a single feature to assess a very general indication of the existence of a fault without any indication of the nature of the fault and, alternatively, the use of more detailed frequency derived indicators. Computing such indicators can be time consuming and require detailed knowledge of the internal structure of the machine, in terms of relative speeds of different components, in order to make a good classification of the faults and their locations. Significant research on fault detection in gears and bearings has been carried out so far and has resulted in the identification of a rich feature library with a variety of features from the time, frequency and wavelet domains. However, the requirements in terms of suitable features may differ depending on how these elements are employed inside a complex plant. Moreover, there are several other systems like planetary gears that are far more complex and a good set of features for them has not yet been clearly identified. However, due to some structural similarities, it is natural to search for suitable features from existing feature libraries. Several classification techniques can be employed as required once a suitable set of feature indicators is identified. Different features are needed to effectively discern different faults, but an exhaustive set of features that captures a variety of faults can be very large and is, therefore, prohibitively expensive, from a computational standpoint, to process it. A feature selection process must therefore be identified in order to speed up computation and to also increase the accuracy of classification.</p><p>Genetic algorithms (GAs) offer suitable means to do so, given the vast search space formed by the number of possible combination of all available features in a typical real-world application. GA-based techniques have also been shown to be extremely successful in evolutionary design of various classifiers, such as those based on artificial neural networks (ANNs). Using a reduced number of features that primarily characterize the system conditions along with optimized structural parameters of ANNs have been shown to give improved classification performance <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b18">20]</ref>. However, in all previous works a fixed ANN structure was chosen. The work presented herein uses a GA to determine the parameters of the classifier ANN in addition to obtaining a reduced number of good features. Thus, the structure of the ANN is also dynamically learned. Our results show the effectiveness of the extracted features from the acquired raw and preprocessed signals in diagnosis of machine condition.</p><p>The remaining sections of the paper are as follows: in Section 2, we discuss the rationale behind using GAs as an optimization tool accompanied by ANNs as a classifier. Section 3 discusses the implementation details with respect to the bearing health diagnosis, followed by the results and discussion in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Using GAs and ANNs</head><p>Feature selection is an optimization problem of choosing an optimal combination of features from a large and possibly multi-modal search space. Two major classes of optimization techniques have traditionally been used, namely: calculus-based techniques, that use gradient-based search mechanisms, and enumerative techniques, such as dynamic programming (DP) <ref type="bibr" target="#b22">[24]</ref>. For an ill-defined or multimodal objective function where a global minimum may not be possible or very difficult to achieve, DP can be more useful but its computational complexity makes it unsuitable for effective use in most practical cases. Thus, a non-conventional non-linear search algorithm is desired to obtain fast results, and a GA meets these requirements. From another standpoint the basic problem here is that of high dimensionality, with a large number of features among which it is not known which ones are good for identifying a particular fault type. There are several methods that can be used to reduce the dimensionality of the problem. Principle component analysis (PCA) yields a linear manifold while maximizing the directional variance in an uncorrelated way. It is the most widely used technique among similar techniques such as multi-dimensional scaling (MDS) and singular value decomposition (SVD) <ref type="bibr" target="#b23">[25]</ref>. Other methods approach this problem from different perspectives, such as <ref type="bibr" target="#b13">[15]</ref>: low dimensional projection of the data (projection pursuit, generalized additive models), regression (principle curves) and self-organization (Kohonen maps) just to name a few. PCA can be much less computationally expensive than a GA-based approach. However, all features need to be computed for PCA before a rotated feature space can be created for easier use. Therefore, using PCA still requires computation of all features, demanding a large amount of data processing.</p><p>GA facilitates a better scenario, in which although the computational cost will be very high during the offline training and feature selection phase, much less computing is required for online classification. Other methods for feature selection include forward selection that assesses almost all combinations of different features that are available to determine a set of best features <ref type="bibr" target="#b2">[4]</ref>. The main difficulty in applying forward selection is that in some cases two features perform poorly when used separately, but lead to better results when both are used. The pair of such good features found by forward selection may not necessarily be the best combination, as it chooses the candidates based on their respective individual performances. Moreover, a GA-based method can be used to add more functionality in parameter selection. For instance, it can be used to simultaneously find the optimal structure of an ANN, in terms of concurrently determining the number of nodes in the hidden layers and the connection matrices for evolving the ANNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b15">17]</ref>. Potential applications of ANNs in automated detection and diagnosis have been shown in refs. <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b20">22]</ref>. Similar optimization can be considered irrespective of what classification technique is used, an evolutionary search is expected to provide a better combination, especially in the cases where the dimensionality increases the possible number of combinations exponentially, and hence the computational power needed. Approaches using Support Vector Machines and Neuro-Fuzzy Networks have also been put forward for solving the feature selection problem <ref type="bibr" target="#b17">[19]</ref>, but the use of a GA still remains warranted. A recent survey on the use of evolutionary computing to solving complex real world problems in the manufacturing industry is presented in ref. <ref type="bibr" target="#b14">[16]</ref>. Other related work on using evolutionary computing to design neural networks for mobile robots control is described in ref. <ref type="bibr" target="#b11">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Genetic algorithms</head><p>In 1975, Holland introduced an optimization procedure that mimics the process observed in natural evolution called genetic algorithms <ref type="bibr" target="#b7">[9]</ref>. A GA is a search process that is based on the laws of natural selection and genetics. As originally proposed, a simple GA usually consists of three processes selection, genetic operation and replacement. A typical GA cycle and its high-level description are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The population comprises a group of chromosomes that are the candidates for the solution. The fitness values of all chromosomes are evaluated using an objective function (performance criteria or a system's behavior) in a decoded form (phenotype). A particular group of parents is selected from the population to generate offspring by the defined genetic operations of crossover and mutation. The fitness of all offspring is then evaluated using the same criterion and the chromosomes in the current population are then replaced by their offspring, based on a certain replacement strategy. Such a GA cycle is repeated until a desired termination criterion is reached. If all goes well throughout this process of simulated evolution, the best chromosome in the final population can become a highly evolved and more superior solution to the problem <ref type="bibr" target="#b6">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Artificial neural networks</head><p>An artificial neural network (ANN) is an information processing paradigm that is inspired by the way the human brain processes information. A great deal of literature is available explaining the basic construction and similarities to biological neurons. The discussion here is limited to a basic introduction of several components involved in the ANN implementation. The network architecture or topology, comprising: number of nodes in hidden layers, network connections, initial weight assignments and activation functions, plays a very important role in the performance of the ANN, and usually depends on the problem at hand. Fig. <ref type="figure">2</ref> shows a simple ANN and its constituents. In most cases, setting the correct topology is a heuristic model selection. Whereas the number of input and output layer nodes is generally suggested by the dimensions of the input and the output spaces, determining the network complexity is yet again very important. Too many parameters lead to poor generalization (over fitting), and too few parameters result in inadequate learning (under fitting) <ref type="bibr" target="#b3">[5]</ref>. Some aspects of ANNs are described next.</p><p>Every ANN consists of at least one hidden layer in addition to the input and the output layers. The number of hidden units governs the expressive power of the net and thus the complexity of the decision boundary. For well-separated classes fewer units are required and for highly interspersed data more units are needed. The number of synaptic weights is based on the number of hidden units. It represents the degrees of freedom of the network. Hence, we should have fewer weights than the number of training points. As a rule of thumb, the number of hidden units is chosen as n/10, where n is the number of training points <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b12">14]</ref>. But this may not always hold true and a better tuning might be required depending on the problem.</p><p>Network learning pertains to training an untrained network. Input patterns are exposed to the network and the network output is compared to the target values to calculate the error, which is corrected in the next pass by adjusting the synaptic weights. Several training algorithms have been designed; the most commonly used being the Levenberg-Marquardt (LM) backpropagation algorithm, which is a natural extension of LMS algorithms for linear systems. However, resilient back propagation has been shown to work faster on larger networks <ref type="bibr" target="#b16">[18]</ref>, and has thus been used throughout this study.</p><p>Learning Training weights are adjusted after each pattern is presented. Stopping Criterion indicates when to stop the training process. It can be a predetermined limit of absolute error, minimum square error (MSE) or just the maximum number of training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem description and methods</head><p>A simpler problem of a roller bearing health monitoring has been used to illustrate the effectiveness of a GA in feature selection for fault classification using ANNs. Several bearings with different outer race defects (Fig. <ref type="figure" target="#fig_2">3</ref>) were used in the test setup. Defects were constructed using a die sinking electrical discharge machine (EDM) to simulate eight different crack sizes resulting in nine health conditions; eight faulty and one healthy. The groove geometries used in these tests have been described in ref. <ref type="bibr" target="#b0">[1]</ref>. In all, four sensors (three accelerometers and one acoustic emission sensor) were employed to get the signal from all eight faulty bearings plus one bearing without any defect. These sensors were attached to signal conditioners and a programmable low pass filter such that each had ground-isolated outputs. The defect sizes were further categorized in four groups including a nodefect category. All tests were run at a radial load of 14,730 N and a rotational speed of 1400 rpm. The data was acquired in the time domain as several snapshots of the time series for each case.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows snapshots of the vibration signal for a healthy bearing and two defect conditions. Each impulse in the signal corresponds to an individual roller in the bearing and hence for a bearing with 25 rollers an equal number of such peaks should appear per bearing revolution. Fig. <ref type="figure" target="#fig_3">4</ref> also shows how the vibration signal becomes larger as the defect grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Relevant work</head><p>In a previous study, a self-organizing map (SOM) based approach was used to develop an online bearing health monitoring system <ref type="bibr" target="#b19">[21]</ref>. SOM has the ability to map a high dimensional signal manifold on a low dimensional topographic feature map, with most of the topological relationships of the signal domain preserved <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12]</ref>. It aggregates clusters of input information from the raw data, and projects them on a much simpler twoor three-dimensional network, thereby contributing to relatively comprehensible visualizations.</p><p>The same dataset as described above was used for this study. In this approach the time series data produced by various sensors was mapped onto a twodimensional SOM grid. This required preprocessing of the original data by means of feature extraction. In the absence of any specific guideline, two features, namely kurtosis and curve-length, were chosen out of a large pool of features by means of some trial experiments. These features were extracted from all data channels and presented for SOM training. As shown in Fig. <ref type="figure">5</ref>, a set of good features creates separate regions for mapping data from healthy and faulty systems. Thus, any real time input can be mapped onto this map in order to assess the condition of the bearing. For instance, a set of testing data that consisted of time series corresponding to various defect sizes could be plotted as a trajectory on this map, and as time passed by this trajectory approached the region corresponding to the bearing outer race defect.</p><p>While choosing the appropriate features it was observed that poor pairs of features did not result in a conclusive SOM that could be used for health monitoring. A lot of time and energy was spent in selecting a set of good features and in the end the two features mentioned above were selected. The need for an automated technique to choose appropriate features was felt which could be further used to train these SOMs. This effort was further expanded and a GA was considered an appropriate choice to carry out the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature extraction</head><p>As mentioned earlier, published research has been conducted on diagnostic feature extraction for gears and bearing defects. Statistical features provide a compact representation of long time data series. A great deal of information regarding the distribution of data points can be obtained using first, second and higher order transformations of raw sensory data, such as various moments and cumulants of the data. Features such as mean, variance, standard deviation and kurtosis (normalized fourth central moment) are the most common features employed for rotating mechanical components. It is also worthwhile sometimes to explore other generalized moments and cumulants for which such common names do not exist. An approach suggested in ref. <ref type="bibr" target="#b8">[10]</ref> has been adapted to create a variety of such features in a systematic manner. Four sets of features were obtained based on the data preprocessing and feature type. These features were calculated from the data obtained from all four sensors, and for all four fault levels, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Plain statistical features</head><p>A number of features were calculated based on moments ðm ðiÞ x Þ and cumulants ðC ðiÞ x Þ of the vibration data obtained from the sensors. Where the ith moment ðm ðiÞ x Þ of a random variable X is given by the expectation E(X i ) and the cumulants C ðiÞ x are defined as described below in Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula" target="#formula_1">4</ref>):</p><formula xml:id="formula_0">C ð1Þ x ¼ m<label>ð1Þ</label></formula><p>x (1)</p><formula xml:id="formula_1">C ð2Þ x ¼ m ð2Þ x À ðm ð1Þ x Þ 2 (2) C ð3Þ x ¼ m ð3Þ x À 3m ð2Þ x m ð1Þ x þ 2ðm ð1Þ x Þ 3 (3) C ð4Þ x ¼ m ð4Þ x À 3ðm ð2Þ x Þ 2 À 4m ð3Þ x m ð1Þ x þ 12m ð2Þ x ðm ð1Þ x Þ 2 À 6ðm ð1Þ x Þ 4<label>(4)</label></formula><p>The three accelerometers were used to measure the individual vibration components in three directions x, y and z. In order to compute the combined effect, another signal w was generated as given by Eq. ( <ref type="formula">5</ref>). Cumulants as described in Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula" target="#formula_1">4</ref>) were computed for all x, y, z, w and a (acoustic emission) signals, and a 38-element feature vector was formed as described by Eq. ( <ref type="formula">6</ref>). In total, 128 snapshot segments from each of the nine experiments were used to form a sequence of 1152 (128 Â 9) sample points. Each sample point consists of four rows of time series data from the four sensors, and the subsequent feature calculation yields a 38-element feature vector as determined by Eq. ( <ref type="formula">6</ref>). Thus, a 1152 Â 38 feature matrix was obtained for conducting the experiments with statistical features.</p><formula xml:id="formula_2">w ¼ Vðx 2 þ y 2 þ z 2 Þ (5) v ¼ ½m<label>ð1Þ</label></formula><formula xml:id="formula_3">x m<label>ð1Þ</label></formula><formula xml:id="formula_4">y m ð1Þ z C<label>ð2Þ</label></formula><formula xml:id="formula_5">x C<label>ð2Þ</label></formula><formula xml:id="formula_6">y C ð2Þ z C ð1ÞÃ x C<label>ð1Þ</label></formula><formula xml:id="formula_7">y C ð1ÞÃ y C ð1Þ z C ð1ÞÃ z C<label>ð1Þ</label></formula><formula xml:id="formula_8">x C<label>ð3Þ</label></formula><formula xml:id="formula_9">x C<label>ð3Þ</label></formula><formula xml:id="formula_10">y C ð3Þ z C ð1ÞÃ x C<label>ð2Þ</label></formula><formula xml:id="formula_11">y C ð2ÞÃ x C<label>ð1Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Signal differences and sums</head><p>As a further preprocessing on raw data, sum and difference signals were calculated for all four sensor channels in order to highlight high and low frequency content of the raw signals, as shown in Eqs. ( <ref type="formula">7</ref>) and <ref type="bibr" target="#b6">(8)</ref>. Difference signals should increase whenever the high frequency changes take place, and the sum signals would show similar effects for the low frequency content.</p><formula xml:id="formula_12">dðnÞ ¼ xðnÞ À xðn À 1Þ (7) iðnÞ ¼ fxðnÞ À m ð1Þ x g þ iðn À 1Þ<label>(8)</label></formula><p>where m</p><p>x is the mean of the sequence x. Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula">6</ref>) were applied to both d(n) and i(n) to create two more 38 Â 1152 feature matrices. Since these sum and difference signals consist of only first-Fig. <ref type="figure">5</ref>. Detailed view of accelerometer (z-axis) response (mounted on the top of the housing). Features kurtosis and curve-length on this accelerometer nicely capture most of the states in the evolution of the fault, and thus the trajectory clearly seems to approach lighter (faulty) region, as time progresses. order derivatives, computing them is fairly fast for the online processing requirements, and hence are suitable candidates as signals for feature calculation. Moreover, several sensors are capable of computing these measurements internally (using hardware filtering) thereby eliminating the requirement of further data processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Spectral features</head><p>Frequency domain features are very informative for rotating components, since well-defined frequency components are associated with them. Any defect associated with the balls or the inner race of bearings expresses itself as a high frequency component. For each of the four channels, a 64-point fast Fourier transform (FFT) was carried out. Based on visual inspection, or simple threshold test, it was observed that the frequency components were relatively small in magnitudes beyond the first 32 values. Consequently, only the first 32 values from each channel (i.e., a total of 128 values from all four channels) were retained. Although this could be done automatically using some preset threshold, a fixed value was used for this study. This results in another 128 Â 1152 feature matrix for spectral features. While other features can be computed based on FFT of the raw signal, these were considered sufficient in order to show the effectiveness of a GA in selecting the best features among the ones available.</p><p>Five feature sets were defined: (  <ref type="formula">5</ref>) all the features considered together (242 values). The raw data was normalized using Eq. ( <ref type="formula">9</ref>) prior to the feature calculation. It has been shown that the normalized data performs much better in terms of training time and training success <ref type="bibr" target="#b8">[10]</ref>. This helps in fast and uniform learning of all categories and results in small training errors <ref type="bibr" target="#b3">[5]</ref>.</p><formula xml:id="formula_14">x i ¼ x i À m x s x (9)</formula><p>where m x is the mean and s x is the variance of the sequence x.</p><p>As can be realized, a large feature set was obtained with only limited feature calculation techniques and a sensor suit mounted only at one location. In practice, several such sensor suites are mounted at different locations to monitor for the occurrence of various faults, and this further increases the dimensionality of the problem. Almost all ANN-based classification techniques would take a long time to train such large networks and may not still achieve a good performance. Forming all combinations of a reduced feature set and testing them exhaustively is practically impossible. Furthermore, searching for an optimal structure of the ANN increases the complexity of the search space considerably. In such a vast search spacea GA is the most appropriate non-linear optimization technique. The details of implementation for this research are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>The experiments were conducted using an actual test bed and the computer implementation was done using Matlab on a PC with Pentium Celeron 2.2 GHz processor and 512 MB RAM. The GA implementation is described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Chromosome encoding</head><p>A binary chromosomal representation was adopted for the problem. The length of the chromosome depends directly on the number of features required in the solution set for inputs to the ANN. For this study, this number was fixed at 10. This number is usually defined by the amount of time and computational power available. However, a GA-based search augmented with additional constraints and cost functions for fitness evaluation can be used to dynamically find an optimal value for the number of features to use, rather than prespecifying it explicitly. Since the selection should be made from 38 (76, 128 or 242 depending on the feature set used) values, each gene consists of 6 (7, 7 or 8) bits (respectively). As a result, the total length of the chromosome becomes 60 (6 Â 10) and likewise (70, 70 or 80, respectively) in the other cases. In order to take care of genotypes without meaningful corresponding phenotypes, the numbers generated were wrapped around the maximum number of features in the corresponding feature set. Several experiments were carried out in order to find an appropriate population size using trial and error. It was found that a population size of 20 was appropriate for convergence in reasonable time and was therefore kept fixed at that size for all of the experiments. It was later realized that an integer coding of size 10 would be a faster and more efficient method, which would also take care of genotypes without meaningful phenotypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Crossover</head><p>We implemented a multi-point crossover. The location of crossover points is determined randomly. All members of the population were made to crossover after they were paired based on their fitness. The chromosomes are then sorted in the order of decreasing fitness, and pairs were formed with immediate neighbors; i.e., the best chromosome was paired with the next best and the third best chromosome with the next one on the list, and so on. Fig. <ref type="figure">6a</ref> illustrates a four-point crossover.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Mutation</head><p>A multi-point bit-flip mutation based on a prespecified probability of mutation ( p m ) was implemented. Again the location of mutation is randomly determined every time the mutation operator is applied to a chromosome. Fig. <ref type="figure">6b</ref> illustrates a single point mutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">Fitness function</head><p>Two fitness functions were used to capture the system performance. The number of correct classifications by the ANN was used as the first fitness function. The output of the ANN was expected to be 1, 2, 3 or 4 in value corresponding to one of four different fault levels, based on which fault level is recognized. The ANN output was rounded to the nearest integer to make a decision as shown in Fig. <ref type="figure">6c</ref>. This represents the classification success of the ANN and rejects the magnitude of training error. However, in order to evaluate the performance in terms of training efficiency, the second criterion used is the total absolute error. The total absolute error represents the accuracy of the ANN training. No direct penalty was made for incorrect classifications. However, other fitness functions can also be implemented that might involve mean squared error or some other penalty scheme over incorrect classifications.</p><p>After a new generation of offspring is obtained, the fitness of all chromosomes (both parents and offspring) is evaluated, and the ones with the highest fitness are carried to the next generation for the next genetic cycle. The detailed procedure is illustrated in Fig. <ref type="figure">7</ref>. Two experiments were conducted in order to compare the performance of standalone ANNs and GA-evolved ANNs.</p><p>First, the comparison was made between the performance of a fixed size ANN with and without GA optimization. Thus, an ANN with all features used as input is compared to an ANN where the 10 best features selected by the GA are used as input. The number of hidden nodes was kept fixed at 12 nodes in both cases. It was found that in most cases the best ANN performance without GA optimization was achieved with 12 hidden nodes. The size of standalone ANNs is larger with several input nodes more than the GA-evolved ANNs, which have only 10 inputs. In the next comparison, the number of hidden nodes is also evolved using the GA, and thus the ANN size further changed every time a new chromosome is obtained. Each ANN is trained for 50-100 epochs before using the test data to measure its performance. To limit the computational time to reasonable extents, the stopping criterion was preset to the number of epochs, since each generation required training 200 ANNs, and in each case the GA is allowed to evolve over 100 generations. A resilient back-propagation training algorithm has been shown to work faster on larger networks by Riedmiller and Braun <ref type="bibr" target="#b16">[18]</ref> and has been used throughout this study. A batch training strategy is employed with tan-sigmoid activation functions for the input and hidden layers and a pure-linear activation function for the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and discussion</head><p>Experiments were done with constant crossover and mutation parameters, so that the results can be compared between evolved ANNs and fixed sized ANNs. The numbers in the first column represent the dataset. The next columns describe the training conditions, represented by alphabets further explained below the table. In Table <ref type="table" target="#tab_1">2</ref>, an extra column for n shows the number of hidden nodes the GA converged to. Fig. <ref type="figure" target="#fig_6">8</ref> shows the resulting convergence curves for both fitness functions. The results have been summarized in Tables <ref type="table">1</ref> and<ref type="table" target="#tab_1">2</ref>. It can be seen in all cases that GA-evolved ANNs clearly outperform the standalone ANNs. In some cases, for instance, when using features on Sum signal (dataset 2), the ANN completely failed due to the many null entries in the feature set. That is, the feature values were zero in  many cases due to very small low-frequency content change in the signal and, as a result, the sum signal did not show any significant differences over time. That in turn causes the statistical features computed based on it to diminish in most cases. However, the GA extracts only the significant (non-zero) features to train the ANN and thereby results in significantly better performance. In case 5, that also includes the data from case 2, ANNs do not get trained very well, and, once again, the GA-evolved ANNs result in superior performance. Table <ref type="table" target="#tab_1">2</ref> compares the performance of standalone ANNs with GA-evolved ANNs (with optimized number of hidden nodes). The number of hidden nodes was found to be around seven nodes in almost all cases. For the dataset used in this study, it can be seen in Fig. <ref type="figure" target="#fig_7">9</ref> that statistical features give the best performance, with minimum absolute errors and maximum classification success. However, when all features are collectively used (dataset 5) to search for the best solution, the performance improves furthermore, and this could be regarded as a direct result of the fact that some features from other datasets also work very well and, in conjunction with good statistical features, they give the best performance. In this study the data was used only from one loading condition at a radial load of 14,730 N and a rotational speed of 1400 rpm. The operating and loading conditions are expected to influence the characteristics of the sensor signals, which in turn will be reflected in feature values. However, the concept presented in this paper suggests a scenario in which a good set of training data must be used for training purposes. This means that if an ANN is trained with a wider variety of signals it should be able to classify more conditions. Since the learning process does not depend on specific data characteristics, the learning is expected to be adequate as long as training data from desired conditions is available and labeled correctly. The ANN can learn using as many conditions as it is exposed to during training and the GA will assist in choosing its optimal structure.</p><p>Further for the purpose of this study a fixed number of features <ref type="bibr" target="#b8">(10)</ref> was chosen for determining the ANN structure. This number was arbitrarily chosen based on the rationale that the number of features should be decided by the available computational resources and available processing time. For some applications a classification once every 5 min may be acceptable whereas for others computations may be required every fifteen seconds to consider it real-time. Thus, depending on the frequency of update and computational complexity for different features this number may vary and can be accordingly decided. However, by including an extra constraint of 'time limitation' in the GA fitness functions this number could also be dynamically generated. However, this is beyond the scope of this study and hence was not considered.</p><p>Since the training times for all these different cases were too high, only 50 epochs were used in most cases, sufficient to show the resulting improvement upon using the GA. Overall, it can be concluded that GAevolved ANNs perform much better, and can lead to considerable savings of computational expenses for effective bearing health monitoring. Therefore, once these optimal parameters have been found, such a system can be used for online classification of the faults. In that case, we expect to use a select feature set and a fixed ANN structure to identify and classify the fault. Such optimal parameters can also be recomputed at fixed time intervals to take into account the nonstationary nature of the process and update the diagnostic module. These kinds of calibrations can be done concurrently offline and employed once the new parameters are available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>It has been shown that using GAs for selecting an optimal feature set for a classification application of ANNs is a very powerful technique. Irrespective of the vastness of the search space, GAs can successfully identify the required number of good features. The definition of ''good'' is usually specific to each classification technique, and a GA optimizes the best combination based on the performance obtained directly from the success of the classifier. The success of the ANNs in this study was measured in both terms; i.e., the training accuracy and the classification success. Although the end result depends on the classification success, the training accuracy is the key element to ensure a good classification success. Having achieved that objective, further scope of this research is in two directions. First, to make the ANN structure more adaptive so that not only the number of hidden nodes but also the number of input nodes and the connection matrix of the ANNs can be evolved using GA. Further, as mentioned in the discussion above, the additional constraint of available computational time can be included in a GA's fitness function which can be used to determine the optimal number of features to be selected. Second, to use this technique to find out a desired set of good features for more complex systems, such as planetary gear systems, where some faults cannot be easily distinguished.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Genetic algorithm cycle and a simple top level description.</figDesc><graphic coords="3,61.00,487.50,417.55,158.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig.2. A general ANN with two hidden layers and its main components.</figDesc><graphic coords="4,94.34,489.26,360.68,156.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Outer race roller bearing defect. (b) Experimental setup summarized.</figDesc><graphic coords="5,91.90,91.16,355.67,154.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Snapshots of the vibration signals for different defect conditions at 1400 rpm and 14,730 N radial load. Each impulse corresponds to individual rollers in the bearing.</figDesc><graphic coords="6,122.06,91.16,305.24,334.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Basic building blocks and design parameters in genetic algorithm implementation.</figDesc><graphic coords="9,74.55,91.16,390.22,104.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. GA convergence curves for the two fitness functions corresponding to absolute error and classification success.</figDesc><graphic coords="11,282.39,91.16,213.82,145.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparing standalone fixed size ANNs with GA-evolved NNs.</figDesc><graphic coords="12,99.44,91.16,350.26,98.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Comparing the performance of fixed size ANNs with GA-evolved ANNs</figDesc><table><row><cell cols="2">Table 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Comparing the performance of fixed size ANNs without and with GA-selected features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Training conditions</cell><cell></cell><cell></cell><cell cols="2">Minimum absolute</cell><cell cols="2">Mean absolute</cell><cell cols="2">Best classification</cell><cell cols="2">Mean classification</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>error</cell><cell></cell><cell>error</cell><cell></cell><cell>success (%)</cell><cell></cell><cell>success (%)</cell><cell></cell></row><row><cell>D a</cell><cell cols="2">E/G/P/C/M b</cell><cell></cell><cell></cell><cell>ANN-12</cell><cell>GANN-12</cell><cell>ANN-12</cell><cell>GANN-12</cell><cell>ANN-12</cell><cell>GANN-12</cell><cell>ANN-12</cell><cell>GANN-12</cell></row><row><cell>1</cell><cell>100</cell><cell>100</cell><cell>20</cell><cell>5/3</cell><cell>84.90</cell><cell>31.9</cell><cell>131</cell><cell>47.9</cell><cell>97</cell><cell>100</cell><cell>92</cell><cell>98.2</cell></row><row><cell>2</cell><cell>75</cell><cell>100</cell><cell>20</cell><cell>5/1</cell><cell>815.00</cell><cell>32.6</cell><cell>816</cell><cell>62.2</cell><cell>22.2</cell><cell>88</cell><cell>22</cell><cell>82</cell></row><row><cell>3</cell><cell>50</cell><cell>100</cell><cell>20</cell><cell>5/3</cell><cell>97.85</cell><cell>64.99</cell><cell>115.3</cell><cell>94.79</cell><cell>98.5</cell><cell>99.7</cell><cell>97.2</cell><cell>97.7</cell></row><row><cell>4</cell><cell>50</cell><cell>100</cell><cell>20</cell><cell>5/3</cell><cell>82.11</cell><cell>57.27</cell><cell>601.1</cell><cell>114.45</cell><cell>95.1</cell><cell>98.1</cell><cell>65.2</cell><cell>95.5</cell></row><row><cell>5</cell><cell>50</cell><cell>100</cell><cell>20</cell><cell>5/3</cell><cell>2048</cell><cell>18.42</cell><cell>2048</cell><cell>39.63</cell><cell>22</cell><cell>100</cell><cell>22</cell><cell>99.7</cell></row><row><cell cols="13">a (D) Dataset; (1) statistical features (38), (2) sum features (38), (3) difference features (38), (4) spectral features (128) and (5) all the features</cell></row><row><cell>(242).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Training conditions</cell><cell></cell><cell></cell><cell cols="2">Minimum absolute</cell><cell cols="2">Mean absolute error</cell><cell cols="2">Best classification</cell><cell cols="2">Mean classification</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>error</cell><cell></cell><cell></cell><cell></cell><cell>success (%)</cell><cell></cell><cell>success (%)</cell><cell></cell></row><row><cell>D a</cell><cell cols="2">E/G/P/C/M b</cell><cell>n</cell><cell></cell><cell>ANN-12</cell><cell>GANN-n</cell><cell>ANN-12</cell><cell>GANN-n</cell><cell>ANN-12</cell><cell>GANN-n</cell><cell>ANN-12</cell><cell>GANN-n</cell></row><row><cell>1</cell><cell cols="2">50/50/20/5/5</cell><cell>7</cell><cell></cell><cell>84.90</cell><cell>12.12</cell><cell>131</cell><cell>22.53</cell><cell>97</cell><cell>100</cell><cell>92</cell><cell>99.5</cell></row><row><cell>2</cell><cell cols="2">50/50/20/5/5</cell><cell>7</cell><cell></cell><cell>815.00</cell><cell>111.8</cell><cell>816</cell><cell>155.8</cell><cell>23</cell><cell>99.2</cell><cell>22</cell><cell>96.6</cell></row><row><cell>3</cell><cell cols="2">50/50/20/5/5</cell><cell>8</cell><cell></cell><cell>97.85</cell><cell>45.76</cell><cell>115.3</cell><cell>87.12</cell><cell>98.5</cell><cell>100</cell><cell>97.2</cell><cell>99.3</cell></row><row><cell>4</cell><cell cols="2">50/50/20/5/5</cell><cell>8</cell><cell></cell><cell>82.11</cell><cell>91.35</cell><cell>601.1</cell><cell>115.16</cell><cell>95.1</cell><cell>95.5</cell><cell>65.2</cell><cell>93.5</cell></row><row><cell>5</cell><cell cols="2">50/50/20/5/5</cell><cell>7</cell><cell></cell><cell>2048</cell><cell>11.9</cell><cell>2048</cell><cell>23</cell><cell>22</cell><cell>100</cell><cell>22</cell><cell>99.9</cell></row><row><cell cols="4">n: Number of hidden node.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">a (D) Dataset; (1) statistical features (38), (2) sum features (38), (3) difference features (38), (4) spectral features (128) and (5) all the features</cell></row><row><cell>(242).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">b (E) No. of epochs for ANN training, (G) number of generations, (P) population size, (C) crossover points and (M) mutation points.</cell></row></table><note><p>b (E) No. of epochs for ANN training, (G) number of generations, (P) population size, (C) crossover points and (M) mutation points.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>A.Saxena, A. Saad / Applied Soft Computing 7 (2007) 441-454</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the Georgia Institute of Technology Woodroof School of Mechanical Engineering and the Intelligent Control Systems Laboratory at the School of Electrical and Computer Engineering for the experimental datasets obtained for the work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sensor and machine condition effects in roller bearing diagnostics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billington</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Department of Mechanical Engineering, Gerogia Institute of Technology, Atlanta</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Evolutionary Design of Neural Architecture-A Preliminary Taxonomy and Guide to Literature</title>
		<author>
			<persName><forename type="first">K</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
		<idno>#95-01</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Artificial Intelligence Research group, Iowa State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">CS Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Little Handbook of Statistical Practice</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dallal</surname></persName>
		</author>
		<ptr target="http://www.tufts.edu/$gdallal/LHSP.HTM" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>Wiley-Interscience Publications</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An evolutionary method for the design of generic neural networks CEC &apos;02</title>
		<author>
			<persName><forename type="first">D</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Congress on Evolutionary Computation</title>
		<meeting>the 2002 Congress on Evolutionary Computation</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1769" to="1774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evolutionary design of MLP neural network architectures</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F M</forename><surname>Filho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IVth Brazilian Symposium on Neural Networks</title>
		<meeting>IVth Brazilian Symposium on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Genetic algorithm in search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization and Machine Learning</title>
		<imprint>
			<publisher>Addison Wesley Publishing Company</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adaptation In Natural and Artificial Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>The University of Michigan Press</publisher>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Genetic algorithms for feature selection in machine condition monitoring with vibration signals</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings, Image Signal Processing</title>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="205" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">New developments and applications of selforganizing maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Engineering applications of the self-organizing maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1996-10">October 1996</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1358" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evolutionary design and behavior analysis of neuromodulatory neural networks for mobile robots control</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kondo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="189" to="202" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lessons in neural network training: overfitting may be harder than expected</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth National Conference on Artificial Intelligence</title>
		<meeting>the fourth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page">97</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Perpin ˜a ´n, A Review of Dimension Reduction Techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carreira</surname></persName>
		</author>
		<idno>CS-96-09</idno>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evolutionary computing in manufacturing industry: an overview of recent applications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Oduguwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="299" />
			<date type="published" when="2005-03">March 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Saxe ´n, A genetic algorithms based multi-objective neural net applied to noisy blast furnace data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chakraborti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="387" to="397" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A direct adaptive method for faster backpropagation learning: the RPROP algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gear Fault detection using artificial neural networks and support vector machines with genetic algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Samanta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="625" to="644" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Artificial neural networks and genetic algorithms for gear fault detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Samanta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1273" to="1282" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fault diagnosis in rotating mechanical systems using self-organizing maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks in Engineering (ANNIE04)</title>
		<meeting><address><addrLine>St. Louis, Missouri</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bearing condition diagnostics via vibration &amp; acoustic emission measurements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shiroishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurfess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Danyluk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="693" to="705" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sunghwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Dagli</surname></persName>
		</author>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3218" to="3222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Genetic algorithms and their applications</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="page" from="21" to="37" />
			<date type="published" when="1996-11">November. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction techniques for classification and visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kollios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 8th SIGKDD</title>
		<meeting>8th SIGKDD<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
