<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices by Convex Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><surname>Wright</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yigang</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
							<email>mayi@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shankar</forename><surname>Rao</surname></persName>
							<email>srrao@uiuc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Computing Group Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Coordinated Science Laboratory</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices by Convex Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7094E54EA6F523F2F6A98F51CAFF7B52</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis. However, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations. This paper considers the idealized "robust principal component analysis" problem of recovering a low rank matrix A from corrupted observations D = A + E. Here, the corrupted entries E are unknown and the errors can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse. We prove that most matrices A can be efficiently and exactly recovered from most error sign-and-support patterns by solving a simple convex program, for which we give a fast and provably convergent algorithm. Our result holds even when the rank of A grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors E grows in proportion to the total number of entries in the matrix. A by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries. Simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of finding and exploiting low-dimensional structure in high-dimensional data is taking on increasing importance in image, audio and video processing, web search, and bioinformatics, where datasets now routinely lie in thousand-or even million-dimensional observation spaces. The curse of dimensionality is in full play here: meaningful inference with limited number of observations requires some assumption that the data have low intrinsic complexity, e.g., that they are low-rank <ref type="bibr" target="#b0">[1]</ref>, sparse in some basis <ref type="bibr" target="#b1">[2]</ref>, or lie on some low-dimensional manifold <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Perhaps the simplest useful assumption is that the observations all lie near some low-dimensional subspace. In other words, if we stack all the observations as column vectors of a matrix M ∈ R m×n , the matrix should be (approximately) low rank. Principal component analysis (PCA) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> seeks the best (in an 2 -sense) such low-rank representation of the given data matrix. It enjoys a number of optimality properties when the data are only mildly corrupted by small noise, and can be stably and efficiently computed via the singular value decomposition.</p><p>One major shortcoming of classical PCA is its brittleness with respect to grossly corrupted or outlying observations <ref type="bibr" target="#b4">[5]</ref>. Gross errors are ubiquitous in modern applications in imaging and bioinformatics, where some measurements may be arbitrarily corrupted (e.g., due to occlusion or sensor failure) or simply irrelevant to the structure we are trying to identify. A number of natural approaches to robustifying PCA have been explored in the literature. These approaches include influence function techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, multivariate trimming <ref type="bibr" target="#b7">[8]</ref>, alternating minimization <ref type="bibr" target="#b8">[9]</ref>, and random sampling techniques <ref type="bibr" target="#b9">[10]</ref>. Unfortunately, none of these existing approaches yields a polynomial-time algorithm with strong performance guarantees. <ref type="foot" target="#foot_0">1</ref>In this paper, we consider an idealization of the robust PCA problem, in which the goal is to recover a low-rank matrix A from highly corrupted measurements D = A + E. The errors E can be arbitrary in magnitude, but are assumed to be sparsely supported, affecting only a fraction of the entries of D. This should be contrasted with the classical setting in which the matrix A is perturbed by small (but densely supported) noise. In that setting, classical PCA, computed via the singular value decomposition, remains optimal if the noise is Gaussian. Here, on the other hand, even a small fraction of large errors can cause arbitrary corruption in PCA's estimate of the low rank structure, A.</p><p>Our approach to robust PCA is motivated by two recent, and tightly related, lines of research. The first set of results concerns the robust solution of over-determined linear systems of equations in the presence of arbitrary, but sparse errors. These results imply that for generic systems of equations, it is possible to correct a constant fraction of arbitrary errors in polynomial time <ref type="bibr" target="#b10">[11]</ref>. This is achieved by employing the 1 -norm as a convex surrogate for the highly-nonconvex 0 -norm. A parallel (and still emerging) line of work concerns the problem of computing low-rank matrix solutions to underdetermined linear equations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. One of the most striking results concerns the exact completion of low-rank matrices from only a small fraction of their entries <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. <ref type="foot" target="#foot_1">2</ref> There, a similar convex relaxation is employed, replacing the highly non-convex matrix rank with the nuclear norm (or sum of singular values).</p><p>The robust PCA problem outlined above combines aspects of both of these lines of work: we wish to recover a low-rank matrix from large but sparse errors. We will show that combining the solutions to the above problems (nuclear norm minimization for low-rank recovery and 1 -minimization for error correction) yields a polynomial-time algorithm for robust PCA that provably succeeds under broad conditions: With high probability, solving a simple convex program perfectly recovers a generic matrix A ∈ R m×m of rank as large as C m log(m) , from errors affecting up to a constant fraction of the m 2 entries. This conclusion holds with high probability as the dimensionality m increases, implying that in high-dimensional observation spaces, sparse and low-rank structures can be efficiently and exactly separated. This behavior is an example of the so-called the blessing of dimensionality <ref type="bibr" target="#b16">[17]</ref>.</p><p>However, this result would remain a theoretical curiosity without scalable algorithms for solving the associated convex program. To this end, we discuss how a near-solution to this convex program can be obtained relatively efficiently via proximal gradient <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and iterative thresholding techniques, similar to those proposed for matrix completion in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. For large matrices, these algorithms are significantly faster and more scalable than general-purpose convex program solvers.</p><p>Our analysis also implies an extension of existing results for the low-rank matrix completion problem, and including the first results applicable to the proportional growth setting where the rank of the matrix grows as a constant (non-vanishing) fraction of the dimensionality: With overwhelming probability, solving a simple convex program perfectly recovers a generic matrix A ∈ R m×m of rank as large as Cm, from observations consisting of only a fraction ρm 2 (ρ &lt; 1) of its entries.</p><p>Organization of this paper. This paper is organized as follows. Section 2 formulates the robust principal component analysis problem more precisely and states the main results of this paper, placing these results in the context of existing work. The proof (available in <ref type="bibr" target="#b21">[22]</ref>) relies on standard ideas from linear algebra and concentration of measure, but is beyond the scope of this paper. Section 3 extends existing proximal gradient techniques to give a simple, scalable algorithm for solving the robust PCA problem. In Section 4, we perform simulations and experiments corroborating the theoretical results and suggesting their applicability to real-world problems in computer vision. Finally, in Section 5, we outline several promising directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Setting and Main Results</head><p>We assume that the observed data matrix D ∈ R m×n was generated by corrupting some of the entries of a low-rank matrix A ∈ R m×n . The corruption can be represented as an additive error E ∈ R m×n , so that D = A + E. Because the error affects only a portion of the entries of D, E is a sparse matrix. The idealized (or noise-free) robust PCA problem can then be formulated as follows: Problem 2.1 (Robust PCA). Given D = A + E, where A and E are unknown, but A is known to be low rank and E is known to be sparse, recover A.</p><p>This problem formulation immediately suggests a conceptual solution: seek the lowest rank A that could have generated the data, subject to the constraint that the errors are sparse: E 0 ≤ k. The Lagrangian reformulation of this optimization problem is</p><formula xml:id="formula_0">min A,E rank(A) + γ E 0 subj A + E = D.<label>(1)</label></formula><p>If we could solve this problem for appropriate γ, we might hope to exactly recover the pair (A 0 , E 0 ) that generated the data D. Unfortunately, ( <ref type="formula" target="#formula_0">1</ref>) is a highly nonconvex optimization problem, and no efficient solution is known. <ref type="foot" target="#foot_2">3</ref> We can obtain a tractable optimization problem by relaxing (1), replacing the 0 -norm with the 1 -norm, and the rank with the nuclear norm A * = i σ i (A), yielding the following convex surrogate:</p><formula xml:id="formula_1">min A,E A * + λ E 1 subj A + E = D.<label>(2)</label></formula><p>This relaxation can be motivated by observing that A * + λ E 1 is the convex envelope of rank(A) + λ E 0 over the set of (A, E) such that max( A 2,2 , E 1,∞ ) ≤ 1. Moreover, recent advances in our understanding of the nuclear norm heuristic for low-rank solutions to matrix equations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> and the 1 heuristic for sparse solutions to underdetermined linear systems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, suggest that there might be circumstances under which solving the tractable problem (2) perfectly recovers the low-rank matrix A 0 . The main result of this paper will be to show that this is indeed true under surprisingly broad conditions. A sketch of the result is as follows: For "almost all" pairs (A 0 , E 0 ) consisting of a low-rank matrix A 0 and a sparse matrix E 0 ,</p><formula xml:id="formula_2">(A 0 , E 0 ) = arg min A,E A * + λ E 1 subj A + E = A 0 + E 0 ,</formula><p>and the minimizer is uniquely defined. That is, under natural probabilistic models for low-rank and sparse matrices, almost all observations D = A 0 + E 0 generated as the sum of a low-rank matrix A 0 and a sparse matrix E 0 can be efficiently and exactly decomposed into their generating parts by solving a convex program. <ref type="foot" target="#foot_3">4</ref>Of course, this is only possible with an appropriate choice of the regularizing parameter λ &gt; 0.</p><p>From the optimality conditions for the convex program (2), it is not difficult to show that for matrices D ∈ R m×m , the correct scaling is λ = O m -1/2 . Throughout this paper, unless otherwise stated, we will fix λ = m -1/2 . For simplicity, all of our results in this paper will be stated for square matrices D ∈ R m×m , although there is little difficulty in extending them to non-square matrices.</p><p>It should be clear that not all matrices A 0 can be successfully recovered by solving the convex program <ref type="bibr" target="#b1">(2)</ref>. Consider, e.g., the rank-1 case where U = [e i ] and V = [e j ]. Without additional prior knowledge, the low-rank matrix A = U SV * cannot be recovered from even a single gross error. We therefore restrict our attention to matrices A 0 whose row and column spaces are not aligned with the standard basis. This can be done probabilistically, by asserting that the marginal distributions of U and V are uniform on the Stiefel manifold W m r : Definition 2.2 (Random orthogonal model <ref type="bibr" target="#b12">[13]</ref>). We consider a matrix A 0 to be distributed according to the random orthogonal model of rank r if its left and right singular vectors are independent uniformly distributed m×r matrices with orthonormal columns. <ref type="foot" target="#foot_4">5</ref> In this model, the nonzero singular values of A 0 can be arbitrary.</p><p>Our model for errors is similarly natural: each entry of the matrix is independently corrupted with some probability ρ s , and the signs of the corruptions are independent Rademacher random variables. Definition 2.3 (Bernoulli error signs and support). We consider an error matrix E 0 to be drawn from the Bernoulli sign and support model with parameter ρ s if the entries of sign(E 0 ) are independently distributed, each taking on value 0 with probability 1 -ρ s , and ±1 with probability ρ s /2 each. In this model, the magnitude of the nonzero entries in E 0 can be arbitrary.</p><p>Our main result is the following (see <ref type="bibr" target="#b21">[22]</ref> for a proof): Theorem 2.4 (Robust recovery from non-vanishing error fractions). For any p &gt; 0, there exist constants (C 0 &gt; 0, ρ s &gt; 0, m 0 ) with the following property: if m &gt; m 0 , (A 0 , E 0 ) ∈ R m×m × R m×m with the singular spaces of A 0 ∈ R m×m distributed according to the random orthogonal model of rank</p><formula xml:id="formula_3">r ≤ C 0 m log(m)<label>(3)</label></formula><p>and the signs and support of E 0 ∈ R m×m distributed according to the Bernoulli sign-and-support model with error probability ≤ ρ s , then with probability at least 1 -Cm -p</p><formula xml:id="formula_4">(A 0 , E 0 ) = arg min A * + 1 √ m E 1 subj A + E = A 0 + E 0 ,<label>(4)</label></formula><p>and the minimizer is uniquely defined.</p><p>In other words, matrices A 0 whose singular spaces are distributed according to the random orthogonal model can, with probability approaching one, be efficiently recovered from almost all corruption sign and support patterns without prior knowledge of the pattern of corruption.</p><p>Our line of analysis also implies strong results for the matrix completion problem studied in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>. We again refer the interested reader to <ref type="bibr" target="#b21">[22]</ref> for a proof of the following result: Theorem 2.5 (Matrix completion in proportional growth). There exist numerical constants m 0 , ρ r , ρ s , C all &gt; 0, with the following property: if m &gt; m 0 and A 0 ∈ R m×m is distributed according to the random orthogonal model of rank</p><formula xml:id="formula_5">r ≤ ρ r m, (<label>5</label></formula><formula xml:id="formula_6">) and Υ ⊂ [m] × [m] is an independently chosen subset of [m] × [m]</formula><p>in which the inclusion of each pair (i, j) is an independent Bernoulli(1 -ρ s ) random variable with ρ s ≤ ρ s , then with probability at least 1 -exp (-Cm),</p><formula xml:id="formula_7">A 0 = arg min A * subj A(i, j) = A 0 (i, j) ∀ (i, j) ∈ Υ,<label>(6)</label></formula><p>and the minimizer is uniquely defined.</p><p>Relationship to existing work. Contemporaneous results due to <ref type="bibr" target="#b24">[25]</ref> show that for A 0 distributed according to the random orthogonal model, and E 0 with Bernoulli support, correct recovery occurs with high probability provided</p><formula xml:id="formula_8">E 0 0 ≤ C m 1.5 log(m) -1 max(r, log m) -1/2 . (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>This is an interesting result, especially since it makes no assumption on the signs of the errors. However, even for constant rank r it guarantees correction of only a vanishing fraction o(m 1.5 ) m 2 of errors. In contrast, our main result, Theorem 2.4, states that even if r grows proportional to m/ log(m), non-vanishing fractions of errors are corrected with high probability. Both analyses start from the optimality condition for the convex program (2). The key technical component of this improved result is a probabilistic analysis of an iterative refinement technique for producing a dual vector that certifies optimality of the pair (A 0 , E 0 ). This approach extends techniques used in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>, with additional care required to handle an operator norm constraint arising from the presence of the nuclear norm in <ref type="bibr" target="#b1">(2)</ref>. For further details we refer the interested reader to <ref type="bibr" target="#b21">[22]</ref>.</p><p>Finally, while Theorem 2.5 is not the main focus of this paper, it is interesting in light of results by <ref type="bibr" target="#b14">[15]</ref>. That work proves that in the probabilistic model considered here, a generic m × m rank-r matrix can be efficiently and exactly completed from a subset of only Cmr log 8 (m) (8) entries. For r &gt; m polylog(m) , this bound exceeds the number m 2 of possible observations. A similar result for spectral methods <ref type="bibr" target="#b13">[14]</ref> gives exact completion from O(m log(m)) measurements when r = O(1). In contrast, our Theorem 2.5 implies that for certain scenarios with r as large as ρ r m, the matrix can be completed from a subset of (1 -ρ s )m 2 entries. For matrices of large rank, this is a significant extension of <ref type="bibr" target="#b14">[15]</ref>. However, our result does not supersede (8) for smaller ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scalable Optimization for Robust PCA</head><p>There are a number of possible approaches to solving the robust PCA semidefinite program <ref type="bibr" target="#b1">(2)</ref>. For small problem sizes, interior point methods offer superior accuracy and convergence rates. However, off-the-shelf interior point solvers become impractical for data matrices larger than about 70 × 70, due to the O(m<ref type="foot" target="#foot_5">6</ref> ) complexity of solving for the step direction. For the experiments in this paper we use an alternative first-order method based on the proximal gradient approach of <ref type="bibr" target="#b17">[18]</ref>, 6 which we briefly introduce here. For further discussion of this approach, as well as alternatives based on duality, please see <ref type="bibr" target="#b26">[27]</ref>. This algorithm solves a slightly relaxed version of (2), in which the equality constraint is replaced with a penalty term:</p><formula xml:id="formula_10">min µ A * + λµ E 1 + 1 2 D -A -E 2 F .<label>(9</label></formula><p>) Here, µ is a small constant; as µ 0, the solutions to (9) approach the solution set of (2).</p><p>The approach of <ref type="bibr" target="#b17">[18]</ref> minimizes functions of this type by forming separable quadratic approximations to the data fidelity term D-A-E 2 F at a special set of points ( Ãk , Ẽk ) that are conspicuously chosen to obtain a convergence rate of O k -2 . The solutions to these subproblems,</p><formula xml:id="formula_11">A k+1 = arg min A µ A * + A -Ãk -1 4 ∇ A D -A -E 2 F Ãk , Ẽk 2 F ,<label>(10)</label></formula><formula xml:id="formula_12">E k+1 = arg min E λµ E 1 + E -Ẽk -1 4 ∇ E D -A -E 2 F Ãk , Ẽk 2 F ,<label>(11)</label></formula><p>can be efficiently computed via the soft thresholding operator (for E) and the singular value thresholding operator (for A, see <ref type="bibr" target="#b19">[20]</ref>). We terminate the iteration when the subgradient</p><formula xml:id="formula_13">Ãk -A k+1 + E k+1 -Ẽk , Ẽk -E k+1 + A k+1 -Ãk ∈ ∂ µ A * + λµ E 1 + 1 2 D -A -E 2 F A k+1 ,E k+1</formula><p>has sufficiently small Frobenius norm. <ref type="foot" target="#foot_6">7</ref> In practice, convergence speed is dramatically improved by employing a continuation strategy in which µ starts relatively large and then decreases geometrically at each iteration until reaching a lower bound, μ (as in <ref type="bibr" target="#b20">[21]</ref>).</p><p>The entire procedure is summarized as Algorithm 1 below. We encourage the interested reader to consult <ref type="bibr" target="#b17">[18]</ref> for a more detailed explanation of the choice of the proximal points ( Ãk , Ẽk ), as well as a convergence proof ([18] Theorem 4.1). As we will see in the next section, in practice the total number of iterations is often as small as 200. Since the dominant cost of each iteration is computing the singular value decomposition, this means that it is often possible to obtain a provably robust PCA with only a constant factor more computational resources than required for conventional PCA.</p><p>Algorithm 1: Robust PCA via Proximal Gradient with Continuation</p><formula xml:id="formula_14">1: Input: Observation matrix D ∈ R m×n , weight λ. 2: A 0 , A -1 ← 0, E 0 , E -1 ← 0, t 0 , t -1 ← 1, µ 0 ← .99 D 2,2 , μ ← 10 -5 µ 0 . 3: while not converged 4: Ãk ← A k + t k-1 -1 t k (A k -A k-1 ), Ẽk ← E k + t k-1 -1 t k (E k -E k-1</formula><p>).</p><p>5:</p><formula xml:id="formula_15">Y A k ← Ãk -1 2 Ãk + Ẽk -D . 6: (U, S, V ) ← svd(Y A k ), A k+1 ← U S -µ 2 I + V * . 7: Y E k ← Ẽk -1 2</formula><p>Ãk + Ẽk -D .</p><p>8:</p><formula xml:id="formula_16">E k+1 ← sign[Y E k ] • |Y E k | -λµ 2 11 * + . 9: t k+1 ← 1+ √ 1+4t 2 k 2</formula><p>, µ ← max(.9µ, μ). 10: end while 11: Output: A, E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Simulations and Experiments</head><p>In this section, we first perform simulations corroborating our theoretical results and clarifying their implications. We then sketch two computer vision applications involving the recovery of intrinsically low-dimensional data from gross corruption: background estimation from video and face subspace estimation under varying illumination. <ref type="foot" target="#foot_7">8</ref>Simulation: proportional growth. We first demonstrate the exactness of the convex programming heuristic, as well as the efficacy of Algorithm 1, on random matrix examples of increasing dimension. We generate A 0 as a product of two independent m × r matrices whose elements are i.i.d. N (0, 1) random variables. We generate E 0 as a sparse matrix whose support is chosen uniformly at random, and whose non-zero entries are independent and uniformly distributed in the range [-500, 500]. We apply the proposed algorithm to the matrix D . = A 0 + E 0 to recover Â and Ê. The results are presented in Table <ref type="table" target="#tab_0">1</ref>. For these experiments, we choose λ = m -1/2 . We observe that the proposed algorithm is successful in recovering A 0 even when 10% of its entries are corrupted. Simulation: phase transition w.r.t. rank and error sparsity. We next examine how the rank of A and the proportion of errors in E affect the performance our algorithm. We fix m = 200, and vary ρ r . = rank(A0) m and the error probability ρ s between 0 and 1. For each ρ r , ρ s pair, we generate 10 pairs (A 0 , E 0 ) as in the above experiment. We deem (A 0 , E 0 ) successfully recovered if the recovered Â satisfies Â-A0 F A0 F &lt; 0.01. Figure <ref type="figure" target="#fig_1">1</ref> (left) plots the fraction of correct recoveries. White denotes perfect recovery in all experiments, and black denotes failure for all experiments. We observe that there is a relatively sharp phase transition between success and failure of the algorithm roughly above the line ρ r + ρ s = 0.35. To verify this behavior, we repeat the experiment, but only vary ρ r and ρ s between 0 and 0.4 with finer steps. These results, seen in Figure <ref type="figure" target="#fig_1">1</ref> (right), show that phase transition remains fairly sharp even at higher resolution.  Experiment: background modeling from video. Background modeling or subtraction from video sequences is a popular approach to detecting activity in the scene, and finds application in video surveillance from static cameras. Background estimation is complicated by the presence of foreground objects such as people, as well as variability in the background itself, for example due to varying illumination. In many cases, however, it is reasonable to assume that these background variations are low-rank, while the foreground activity is spatially localized, and therefore sparse. If the individual frames are stacked as columns of a matrix D, this matrix can be expressed as the sum of a low-rank background matrix and a sparse error matrix representing the in the scene. We illustrate this idea using two examples from <ref type="bibr" target="#b28">[29]</ref> (see Figures 2). In Figure <ref type="figure">2</ref>(a)-(c), the video sequence consists of 200 frames of a scene in an airport. There is no significant change in illumination in the video, but a lot of activity in the foreground. We observe that our algorithm is very effective in separating the background from the activity. In Figure <ref type="figure">2</ref>(d)-(f), we have 550 frames from a scene in a lobby. There is little activity in the video, but the illumination changes drastically towards the end of the sequence. We see that our algorithm is once again able to recover the background, irrespective of the illumination change.</p><formula xml:id="formula_17">m rank(A0) E0 0 Â-A 0 F A 0 F rank( Â) Ê 0 #iterations</formula><p>Experiment: removing shadows and specularities from face images. Face recognition is another domain in computer vision where low-dimensional linear models have received a great deal of attention, mostly due to the work of <ref type="bibr" target="#b29">[30]</ref>. The key observation is that under certain idealized circumstances, images of the same face under varying illumination lie near an approximately ninedimensional linear subspace known as the harmonic plane. However, since faces are neither perfectly convex nor Lambertian, face images taken under directional illumination often suffer from self-shadowing, specularities, or saturations in brightness.</p><p>Given a matrix D whose columns represent well-aligned training images of a person's face under various illumination conditions, our Robust PCA algorithm offers a principled way of removing such spatially localized artifacts. Figure <ref type="figure">3</ref> illustrates the results of our algorithm on images from subsets 1-3 of the Extended Yale B database <ref type="bibr" target="#b30">[31]</ref>. The proposed algorithm algorithm removes the specularities in the eyes and the shadows around the nose region. This technique is potentially useful for pre-processing training images in face recognition systems to remove such deviations from the low-dimensional linear model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>Our results give strong theoretical and empirical evidences for the efficacy of using convex programming to recover low-rank matrices from corrupted observations. However, there remain many fascinating open questions in this area. From a mathematical perspective, it would be interesting to know if it is possible to remove the logarithmic factor in our main result. The phase transition experiment in Section 4 suggests that convex programming actually succeeds even for rank(A 0 ) &lt; ρ r m and E 0 0 &lt; ρ s m 2 , where ρ r and ρ s are sufficiently small positive constants. Another interesting and important question is whether the recovery is stable in the presence of small dense noise. That is, suppose we observe D = A 0 + E 0 + Z, where Z is a noise vector of small 2 -norm (e.g., Gaussian noise). A natural approach is to now minimize A * + λ E 1 , subject to a relaxed constraint D -A -E F ≤ ε. For matrix completion, <ref type="bibr" target="#b15">[16]</ref> showed that a similar relaxation gives stable recovery -the error in the solution is proportional to the noise level. Finally, while this paper has sketched several examples on visual data, we believe that this powerful new tool pertains to a wide range of high-dimensional data, for example in bioinformatics and web search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Phase transition wrt rank and error sparsity. Here, ρr = rank(A)/m, ρs = E 0/m 2 . Left: (ρr, ρs) ∈ [0, 1] 2 . Right: (ρr, ρs) ∈ [0, 0.4] 2 .</figDesc><graphic coords="7,210.32,170.51,87.91,96.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Background modeling. (a) Video sequence of a scene in an airport. The size of each frame is 72 × 88 pixels, and a total of 200 frames were used. (b) Static background recovered by our algorithm. (c) Sparse error recovered by our algorithm represents activity in the frame. (d) Video sequence of a lobby scene with changing illumination. The size of each frame is 64 × 80 pixels, and a total of 550 frames were used. (e) Static background recovered by our algorithm. (f) Sparse error. The background is correctly recovered even when the illumination in the room changes drastically in the frame on the last row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Proportional growth. Here the rank of the matrix grows in proportion (5%) to the dimensionality m; and the number of corrupted measurements grows in proportion to the number of entries m 2 , top 5% and bottom 10%, respectively. The time reported is for Matlab implementation run on a 2.8 GHz MacBook Pro.</figDesc><table><row><cell>time (s)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Random sampling approaches guarantee near-optimal estimates, but have complexity exponential in the rank of the matrix A0. Trimming algorithms have comparatively lower computational complexity, but guarantee only locally optimal solutions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A major difference between robust PCA and low-rank matrix completion is that here we do not know which entries are corrupted, whereas in matrix completion the support of the missing entries is given.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In a sense, this problem subsumes both the low rank matrix completion problem and the 0 -minimization problem, both of which are NP-hard and hard to approximate<ref type="bibr" target="#b22">[23]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Notice that this is not an "equivalence" result for (1) and (2) -rather than asserting that the solutions of these two problems are equal with high probability, we directly prove that the convex program correctly decomposes D = A0 + E0 into (A0, E0). A natural conjecture, however, is that under the conditions of our main result, (A0, E0) is also the solution to (1) for some choice of γ.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>I.e., distributed according to the Haar measure on the Stiefel manifold W m r .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>That work is similar in spirit to the work of<ref type="bibr" target="#b18">[19]</ref>, and has also applied to matrix completion in<ref type="bibr" target="#b20">[21]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>More precisely, as suggested in<ref type="bibr" target="#b20">[21]</ref>, we terminate when the norm of this subgradient is less than 2 max(1, (A k+1 , E k+1 ) F ) × τ . In our experiments, we set τ = 10 -7 .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Here, we use these intuitive examples and data illustrate how our algorithm can be used as a simple, general tool to effectively separate low-dimensional and sparse structures occurring in real visual data. Appropriately harnessing additional structure (e.g., the spatial coherence of the error<ref type="bibr" target="#b27">[28]</ref>) may yield even more effective algorithms.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* For more information, see http://perception.csl.illinois.edu/matrix-rank/home.html. This work was partially supported by NSF IIS 08-49292, NSF ECCS 07-01676, and ONR N00014-09-1-0230.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<title level="m">Robust Statistics</title>
		<meeting><address><addrLine>New York, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A framework for robust subspace learning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="117" to="142" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust estimates, residuals, and outlier detection with multiresponse data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gnanadesikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kettenring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="124" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="385" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Thy</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4203" to="4215" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Guaranteed minimum rank solution of matrix equations via nuclear norm minimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimzation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Matrix completion from a few entries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The power of convex relaxation: Near-optimal matrix completion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix completion with noise</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Plan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">High-dimensional data analysis: The curses and blessings of dimensionality</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>AMS Math Challenges Lecture</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Science</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smooth minimization of non-smooth functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/0810.3286" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<ptr target="http://math.nus.edu.sg/˜matys/apg.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="260" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="797" to="829" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparse and low-rank matrix decompositions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFAC Symposium on System Identification</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dense error correction via 1 -minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse signal recovery using markov random fields</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Statistical modeling of complex backgrounds for foreground object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lambertian reflection and linear subspaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
