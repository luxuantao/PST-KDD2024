<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight Run-Time Working Memory Compression for Deployment of Deep Neural Networks on Resource-Constrained MCUs</title>
				<funder ref="#_FYpGVUg #_DgASE6S #_3NWr9tU">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Extreme Science and Engineering Discovery Environment (XSEDE)</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Pittsburgh Center for Research Computing</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhepeng</forename><surname>Wang</surname></persName>
							<email>zhepeng.wang@pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<region>Pennsylvania, US</region>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Lightweight Run-Time Working Memory Compression for Deployment of Deep Neural Networks on Resource-Constrained MCUs. In 26th Asia and South Pacific Design Automation Conference (ASPDAC &apos;21)</orgName>
								<address>
									<addrLine>January 18-21, 8 pages</addrLine>
									<postCode>2021</postCode>
									<settlement>Tokyo, New York</settlement>
									<region>NY</region>
									<country>Japan. ACM, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yawen</forename><surname>Wu</surname></persName>
							<email>yawen.wu@pitt.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<region>Pennsylvania, US</region>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Lightweight Run-Time Working Memory Compression for Deployment of Deep Neural Networks on Resource-Constrained MCUs. In 26th Asia and South Pacific Design Automation Conference (ASPDAC &apos;21)</orgName>
								<address>
									<addrLine>January 18-21, 8 pages</addrLine>
									<postCode>2021</postCode>
									<settlement>Tokyo, New York</settlement>
									<region>NY</region>
									<country>Japan. ACM, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenge</forename><surname>Jia</surname></persName>
							<email>zhenge.jia@pitt.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<region>Pennsylvania, US</region>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Lightweight Run-Time Working Memory Compression for Deployment of Deep Neural Networks on Resource-Constrained MCUs. In 26th Asia and South Pacific Design Automation Conference (ASPDAC &apos;21)</orgName>
								<address>
									<addrLine>January 18-21, 8 pages</addrLine>
									<postCode>2021</postCode>
									<settlement>Tokyo, New York</settlement>
									<region>NY</region>
									<country>Japan. ACM, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<settlement>Indiana</settlement>
									<region>US</region>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Lightweight Run-Time Working Memory Compression for Deployment of Deep Neural Networks on Resource-Constrained MCUs. In 26th Asia and South Pacific Design Automation Conference (ASPDAC &apos;21)</orgName>
								<address>
									<addrLine>January 18-21, 8 pages</addrLine>
									<postCode>2021</postCode>
									<settlement>Tokyo, New York</settlement>
									<region>NY</region>
									<country>Japan. ACM, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingtong</forename><forename type="middle">2021</forename><surname>Hu</surname></persName>
							<email>jthu@pitt.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<region>Pennsylvania, US</region>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Lightweight Run-Time Working Memory Compression for Deployment of Deep Neural Networks on Resource-Constrained MCUs. In 26th Asia and South Pacific Design Automation Conference (ASPDAC &apos;21)</orgName>
								<address>
									<addrLine>January 18-21, 8 pages</addrLine>
									<postCode>2021</postCode>
									<settlement>Tokyo, New York</settlement>
									<region>NY</region>
									<country>Japan. ACM, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lightweight Run-Time Working Memory Compression for Deployment of Deep Neural Networks on Resource-Constrained MCUs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394885.3439194</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computer systems organization ? Embedded software</term>
					<term>? Computing methodologies ? Neural networks</term>
					<term>Supervised learning by classification Neural Network Deployment, Neural Network Compression, Edge Computing, Artificial Intelligence of Things (AIoT)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work aims to achieve intelligence on embedded devices by deploying deep neural networks (DNNs) onto resource-constrained microcontroller units (MCUs). Apart from the low frequency (e.g., 1-16 MHz) and limited storage (e.g., 16KB to 256KB ROM), one of the largest challenges is the limited RAM (e.g., 2KB to 64KB), which is needed to save the intermediate feature maps of a DNN. Most existing neural network compression algorithms aim to reduce the model size of DNNs so that they can fit into limited storage. However, they do not reduce the size of intermediate feature maps significantly, which is referred to as working memory and might exceed the capacity of RAM. Therefore, it is possible that DNNs cannot run in MCUs even after compression. To address this problem, this work proposes a technique to dynamically prune the activation values of the intermediate output feature maps in the runtime to ensure that they can fit into limited RAM. The results of our experiments show that this method could significantly reduce the working memory of DNNs to satisfy the hard constraint of RAM size, while maintaining satisfactory accuracy with relatively low overhead on memory and run-time latency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the fast-growing world of Internet of Things (IoT), which connects and shares data across a vast network of devices or "things", analytics is the key to extract the most valuable information from raw data in a broad spectrum of applications -from manufacturing and retailing to energy, smart cities, health care and beyond. For its ability to make rapid decisions and uncover deep insights as it learns from massive volumes of IoT data, Artificial Intelligence (AI) is an essential form of analytics to expand the value of IoT. In Artificial Intelligence of Things (AIoT), AI adds value to IoT through machine learning and improves decision making while IoT adds value to AI through connectivity, signaling, and data exchange. According to a recent market research report, embedded AI in support of IoT Things/Objects will reach $4.6B globally by 2024 <ref type="bibr" target="#b13">[14]</ref>.</p><p>In recent years, deep neural networks (DNNs) have been proved to be a promising technique of AI with its powerful capability to make accurate inferences based on complex and noisy inputs. While many dedicated DNN accelerators such as TPU <ref type="bibr" target="#b8">[9]</ref> have been designed, the majority of IoT devices are still using low-cost, low-power, and resource-constrained microcontroller units (MCUs). Therefore, to realize the vision of AIoT, it is essential to deploy intelligence into the prolific embedded devices via deploying DNNs on MCUs. However, typical MCUs are resource-constrained, which have limited storage (e.g., ROM and Flash memory) capacity and run in low frequency (several or tens of MHz), while a typical DNN usually has tens of millions of weights and uses billions of operations to finish one inference. Even a lightweight DNN (e.g., MobileNetV2 <ref type="bibr" target="#b14">[15]</ref>) has over a million weights and millions of operations. The gap between the model size of DNNs and the storage capacity of MCUs makes the deployment of DNNs onto MCUs infeasible. Therefore, neural network compression techniques such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> have been adopted by some works such as GENESIS <ref type="bibr" target="#b3">[4]</ref> to deploy DNNs on resource-constrained MCUs. However, even if the DNN could be fit into the limited storage with compression, it still cannot run successfully if the intermediate data (i.e., feature maps) exceeds the size of limited RAM (e.g., 2KB -64KB). Although we can constantly spill out the intermediate data to fast non-volatile memories such as FRAM in some cases <ref type="bibr" target="#b3">[4]</ref>, it is either too expensive or infeasible for Flash memory or ROM in most of the off-the-shelf commercial MCUs.</p><p>The necessary space to save the intermediate results of a DNN is referred to as working memory ?. We use ? ? to denote the memory requirement of layer ? of a specific DNN. It is defined as:</p><formula xml:id="formula_0">? ? = |? ? | + |? ? | , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where According to Eq. ( <ref type="formula" target="#formula_0">1</ref>), we can conclude that ? ? is related to the shape and number of filters of layer ?. Existing neural network compression techniques such as fine-grained unstructured pruning in <ref type="bibr" target="#b4">[5]</ref> focus on pruning the insignificant weights of filters, which could not reduce the working memory of a DNN since it does not change the shape or number of filters. Structured pruning <ref type="bibr" target="#b10">[11]</ref>, which removes certain number of filters in each layer (as shown in Figure <ref type="figure" target="#fig_1">1</ref>(a)(b)), could lead to the reduction of working memory. However, this method is not only coarse-grained but also static and invariant to different inputs. For instance, in Figure <ref type="figure" target="#fig_1">1</ref>(a)(b), the filter in grey color is removed permanently and thus reduce the working memory. This pruning operation might not affect the inference of some inputs like input 0. However, the removed filter might be very important for the inference of some inputs like input 1. And the removal of this filter could degrade the accuracy of the inference of this kind of inputs. Thus, static structured pruning would weaken the representation capability of the original DNNs especially those already small DNNs designed for MCUs, and thus lower the accuracy by a large margin. Although <ref type="bibr" target="#b11">[12]</ref> proposes dynamic structured pruning to mitigate the loss of accuracy incurred by this coarse-grained pruning, it is not suitable to be used in MCUs since it needs an extra DNN to decide the policy of pruning in the runtime, which makes it prohibitive for resource-constrained MCUs. Quantization <ref type="bibr" target="#b15">[16]</ref> is another compression technique that could reduce working memory by using fewer bits to represent the activation values of output feature maps. However, the working memory of lots of DNNs could not satisfy the constraint of RAM even after their activation values being quantized to one byte, which will be shown in Section 4. <ref type="bibr" target="#b15">[16]</ref> proposes to quantize the activation values to less than one byte. However, this kind of quantization is not hardware friendly and might cause problems in accessing the memory of MCUs without extra hardware support. Moreover, quantization is also static and thus insensitive to the inputs in the runtime.</p><formula xml:id="formula_2">|?</formula><p>To deploy a given DNN that could not directly run on MCUs due to the limited size of RAMs, we developed a lightweight run-time working memory compression algorithm to dynamically prune the intermediate output feature maps such that they could fit into RAMs in the output feature maps in the runtime. Since which values to be pruned are dynamically decided based on the current input, the method is sensitive to the input and thus minimize the accuracy degradation incurred by pruning for each input.</p><p>The main advantages of our method are as follows.</p><p>? Effectiveness. To the best of our knowledge, this is the first work to guarantee that a specific DNN with oversized working memory could fit into resource-constrained MCUs without changing the architecture of the deployed DNN. Since the complete architecture is reserved, the loss of accuracy incurred by pruning is reduced compared with static pruning that modifies the architecture of original DNNs, which shows the effectiveness of our algorithm. ? Simplicity. The method we proposed could be implemented easily on the off-the-shelf commercial MCUs without any extra hardware support. ? Lightweight. The method is also lightweight since the incurred memory overhead is negligible and the overhead on the run-time latency is moderate, which will be shown in Section 4.</p><p>Besides, the DNN running with our compression algorithm is also a good complement to the recent neural architecture search (NAS) specified for MCUs <ref type="bibr" target="#b2">[3]</ref>, which will be illustrated in Section 2.</p><p>According to the experimental results, our method could guarantee that the DNN with oversized working memory could fit into limited RAM while maintaining satisfactory accuracy with relatively low overhead on memory and run-time latency.</p><p>The remainder of the paper is organized as follows. Section 2 reviews the related works and Section 3 describes the run-time working memory compression in detail. Experimental results are given in Section 4 and the concluding remarks are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deployment of DNNs on Resource-Constrained MCUs. DNNs were once thought to be unsuitable for deployment on resourceconstrained MCUs due to the gap between its complexity and the limited resources of MCUs. However, more attention has been paid to running DNNs on resource-constrained MCUs in recent years. <ref type="bibr" target="#b6">[7]</ref> designs and deploys a DNN on MCUs to detect ventricular arrhythmias and achieves better performance than conventional algorithms. However, the simple architecture of the DNN hinders it from being applied to more complex tasks such as image classification. <ref type="bibr" target="#b3">[4]</ref> is the first work to successfully deploy DNNs on energy harvesting powered MCUs. However, instead of ROM, this kind of MCUs use FRAM as the storage component, which is more expensive and allows frequent writing operations. The problem of oversized working memory was overcome by constantly spilling out intermediate data from SRAM to FRAM. However, this strategy is either too expensive or infeasible for the MCUs with Flash memory or ROM. Different from the above works that focus on the inference of DNNs on MCUs, <ref type="bibr" target="#b18">[19]</ref> proposes a framework for the efficient on-device training of DNNs. Based on their framework, the online training of LeNet could be achieved on MCUs, which helps to enable more use cases of DNNs on MCUs. DNN Compression. DNN compression is a technique to reduce the size of a specific DNN so that it could fit into the memory of mobile or embedded devices with negligible loss of accuracy <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>. Pruning is one of the common compression techniques, which could be divided into structured pruning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> and unstructured pruning <ref type="bibr" target="#b4">[5]</ref>. However, directly applying unstructured pruning to DNNs on MCUs could not solve the problem of oversized working memory, as we discussed in Section 1. Structured pruning and quantization <ref type="bibr" target="#b15">[16]</ref> could reduce the working memory of a given DNNs since they effectively decrease the size of intermediate feature maps. And <ref type="bibr" target="#b17">[18]</ref> is the first work to combine these two techniques to deploy DNNs on energy harvesting powered MCUs. However, the framework they propose is optimized for energy harvesting settings and specified for a special kind of DNNs, i.e., multi-exit DNNs. To satisfy the more strict energy consumption constraint of energy harvesting powered devices, the accuracy of DNNs is sacrificed. Therefore, a more general method is needed to solve the problem of oversized working memory of DNNs on MCUs while maintaining the accuracy as much as possible. Hardware-Aware Neural Architecture Search (NAS). Hardwareaware NAS is an emerging technique that could automatically generate the architecture of DNNs with the best accuracy for a particular application while satisfying the hardware constraints of target platforms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref>. While most of the existing works focus on mobile devices or FPGAs, little attention has been paid to the design of DNNs on resource-constrained MCUs. Recently, <ref type="bibr" target="#b2">[3]</ref> proposes a hardware-aware NAS customized for MCUs. It takes the model size and working memory of DNNs into consideration in the search process. It could eliminate the DNNs with oversized working memory since they are regarded as not being able to run on MCUs by default in the search process. However, equipped with our runtime working memory compression, this kind of eliminated DNNs actually can run on the target MCUs successfully. And they might have better accuracy compared with those DNNs having smaller working memory since a larger working memory is usually related to a more complicated architecture with stronger representation capability. Due to the simplicity of our compression algorithm, it is convenient to be merged into the NAS framework in <ref type="bibr" target="#b2">[3]</ref> and thus expand their search space, which could lead to better results. Therefore, we can conclude that the DNN running with our compression algorithm is a good complement to <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RUN-TIME WORKING MEMORY COMPRESSION</head><p>Traditional static structured pruning <ref type="bibr" target="#b10">[11]</ref> aims to reduce the computational cost by removing certain filters on selected layers of DNNs, which could reduce the memory requirement of the corresponding layers at the same time. However, since it is usually applied to DNNs on mobile or cloud platforms, where the memory to store the intermediate data is sufficient, the reduction of working memory is only a side effect of this method and thus it is not optimized for the saving of working memory. In the original setting of structured pruning, if some layers with large memory requirements are sensitive to pruning, the algorithm could choose to prune less or even no filters at those layers in order to maintain the accuracy. However, the limited RAM size in MCUs poses hard constraints on the working memory of the deployed DNNs. Even if some layers are sensitive to pruning, they will have to be pruned heavily if their memory requirements exceed the RAM size by a large margin. Therefore, we propose a run-time working memory (WM) compression specified for the deployment of DNNs on resource-constrained MCUs. It could reserve the complete architecture of the deployed DNNs if their weights could fit into the storage and dynamically prune the insignificant activation values of intermediate output feature maps in the runtime to satisfy the hardware constraint of RAM size. Therefore, our method could make full use of the representation capability of the original DNN and thus lower the accuracy loss incurred by pruning as much as possible for some layers sensitive to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview and Execution Model</head><p>Our run-time WM compression mainly consists of two parts, i.e., the offline part and the online part, which will be illustrated in Section 3.2 and 3.3, respectively. The offline part will decide the amount of activation values to prune in each layer of the DNN before deployment. If the memory requirement does not exceed the RAM size for a specific layer, no activation values need to be pruned and thus the WM compression will not be triggered in that layer in the runtime, which reduces the online overhead. The system overview of the online part is shown in Figure <ref type="figure" target="#fig_2">2</ref>. And the online part will decide which activation values to prune dynamically in the runtime according to the output feature maps of the specific layer. Note that the choices could be distinct for different input data. When the online part is triggered for layer ?, it means that RAM cannot hold the complete output feature maps from layer ?. Therefore, we reserve a tiny buffer, which occupies a small space in RAM, to process the output feature maps progressively. The calculated activation values in ? ? will be sent to tiny buffer first after the inference operation. When the tiny buffer is full, the MCU will be notified to execute the code of the online part of our runtime WM compression. The program will decide ?, the minimum number of activation values to prune for the specific data in the buffer. We introduce a mechanism with threshold ? to adapt ? in the runtime, which will be discussed in Section 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Offline Part of Compression Algorithm</head><p>Algorithm 1 presents the steps of the offline part of run-time WM compression in detail. The purpose of this part is to decide the amount of activation values to prune in the corresponding output feature maps of each layer in the DNN with an online pruning array ? ? as output. Since this process is done before the deployment of a DNN on the MCU, it is offline and thus reduce the online overhead. Note that in our WM compression, we only need to consider the convolution layers for compression. For the pooling layer, it is used to downsample the output ? from the convolution layer. If ? could fit into RAM, then the downsampled feature map must be able to fit into RAM. For the fully connected (FC) layer, the size of its output is much smaller than that of the convolution layer. Thus, there is little possibility that the memory requirement of the FC layer could exceed RAM size. As for the input of Algorithm 1, RAM size ? ? and tiny buffer size ? ? are both converted to the number of activation values they can hold.</p><p>In the statements of Algorithm 1, function GetInSize(M) returns an array containing the number of activation values in the input feature maps of each convolution layer of DNN ?, while function GetOutSize(M) returns an array with the number of activation values in the output feature maps of each convolution layer. The decision process of ? ? iterates through the ? convolution layers. For a specific layer ?, there are two constraints on the output feature maps and only one of them would be the bottleneck in different cases. The first one is the hard constraint of RAM size ? ? . When the memory requirement of layer ? exceeds ? ? , according to the memory layout shown in Figure <ref type="figure" target="#fig_2">2</ref>, the hard constraint could be formulated as</p><formula xml:id="formula_3">? ? = ? (?) ?? + ? (?) ??? -? (?) ? + ? ?? + ? ? + ? ??? , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where ?</p><formula xml:id="formula_5">(?)</formula><p>?? is the size of input of layer ?, while ? ??? is quite large for layer ?, the remaining spaces for ? (?+1) ??? might be very tight if we only consider the hard constraint in Eq. ( <ref type="formula" target="#formula_3">2</ref>) for pruning and thus degrading the accuracy significantly. Therefore, we introduce a predefined output threshold ? to ensure that the space occupied by the output of layer ? does not exceed ? * ? ? after pruning. And the constraint could be formulated as,</p><formula xml:id="formula_6">? * ? ? = ? (?) ??? -? (?) ? + ? ?? + ? ??? .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? (?)</head><p>? acquired based on Eq. ( <ref type="formula">3</ref>), corresponds to the option 2 (Opt. </p><formula xml:id="formula_7">= min ? (?) ??? , ? ???? , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where ? ???? is the size of the output from the pooling layer following convolution layer ?, returned by the function GetPoolSize(?, ?). After ? iterations, the resulted ? ? is the output of Algorithm 1 and would be used as one of the inputs of the online part of run-time WM compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Online Part of Compression Algorithm</head><p>Algorithm 2 presents the steps of the online part of run-time WM compression in detail for a specific input data ?  If ? ? ? is larger than any value in K, then ? ? ? and ? will be added to K and K ?? , respectively. And the last value in K and its corresponding index will be removed from Top ? cache. Otherwise, K ?? and K will keep unchanged. Besides, ? ? ? should also be compared with pruning threshold ?. If ? ? ? is less than ?, then it will be pruned and it corresponding bit in bitmap ?? will be set to one through function SetBitmap(??, ? * (? ? -1) + ?), where ? is the index of the batch for ? ? ? . After all the values in the batch are processed by the steps mentioned above, if the activation values pruned by the threshold ? are more than the preset number ? ? , then we can reduce ? ? for the following batches, where ? ? is the average number of the total amount of activation values to prune in the rest of the output feature maps. Therefore, we can reserve more important features in the following batches and thus mitigate the loss of accuracy. Otherwise, the program will prune all the activation values recorded in the Top ? cache and set the corresponding bits to one in bitmap ??. Then, the pruned values ? ? in the batch will be moved to the corresponding position in the space for compressed output feature maps ? ? . After all the batches are processed, our run-time WM compression for convolution layer ? is finished. If there is a pooling layer after layer ?, ? ? will be downsampled through the function Pool(? ? , ?, ?). And the downsampled ? ? will be used as the input ? ?+1 for the next layer ? + 1.</p><p>In the end, all of the ? convolution layers are processed by our compression method. The generated features ? ? +1 would be the input of the remaining fully connected layers in the deployed DNN. And the final prediction ? would be calculated through function FC(? ? +1 , ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>This section reports the experimental results of our proposed runtime working memory compression on CIFAR-10. The results show that our methods could reduce the working memory of a given DNN effectively with accuracy higher than the original DNN or with acceptable accuracy loss. Moreover, the accuracy of the DNN compressed by our method outperforms the DNN compressed with static structured pruning by a large margin. Besides, we also conduct sensitivity analysis for the two hyperparameters in our algorithm, i.e., tiny buffer size ? ? and pruning threshold ?, to explore the impact of them on the performance of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset. The dataset we used in the following experiments is CIFAR-10, which contains 60000 images in 10 classes. In our experiments, the size of training set, validation set and test set are 45000, 5000, and 10000, respectively. The pre-processing performed on the images are mean subtraction and division by the standard deviation. DNNs. The DNNs evaluated in our experiments are LeNet-A, SpArSe-Net-A and SonicNet-A, which are the adapted versions of three lightweight DNNs suitable for resource-constrained MCUs, i.e., LeNet <ref type="bibr" target="#b9">[10]</ref>, SpArSeNet <ref type="bibr" target="#b2">[3]</ref> and SonicNet <ref type="bibr" target="#b3">[4]</ref>, respectively. The details of these DNNs are listed in Table <ref type="table" target="#tab_6">1</ref>. MS represents the model size of the DNN while MS represents its working memory. # Filters records the number of filters for each convolution layer and kernel shape records the shape of kernels of the corresponding filters. Note that all of the convolution layers of the evaluated DNNs use valid padding with stride equal to 1. Pool position is a list of the placement of pooling layers, i.e., the indices of convolution layers followed by the pooling layers. FC config provides the size of output features of each fully connected layer, which is after the series of convolution layers and pooling layers. For example, LeNet-A has two convolution layers. Both of them have 5 ? 5 kernels. The first layer has 6 filters while the second one has 32 filters. They are both followed by a pooling layer. Note that only the second pooling layer of LeNet-A is global average pooling. The other pooling layers of the evaluated DNNs are max-pooling by default. LeNet-A has three fully-connected layers after all the convolution and pooling layers.</p><p>And the output feature of them are 120, 84, 10, respectively. Moreover, the weights and the activation values of these three DNNs are all quantized to one byte under Arm configuration.</p><p>Baseline. According to Section 2, static structured pruning and quantization could also effectively reduce the working memory of DNNs on resource-constrained MCUs. Since we have already quantized the DNNs to one byte in our experiments, only static structured pruning could reduce working memory further. Therefore, we implement the method in <ref type="bibr" target="#b10">[11]</ref> as the baseline. Note that in <ref type="bibr" target="#b10">[11]</ref>, the layers to prune are decided manually since the goal of their work is to reduce the computational cost and there is no memory constraint in their work. But in our implementations, pruning will be triggered when the memory requirement exceeds RAM size. Output Threshold ? . Output threshold ? is a predefined hyperparameter in our run-time working memory compression, which is mentioned in Section 3.2. In our experiments, we set the value of ? to 0.8, 0.5 and 0.8 for LeNet-A, SpArSeNet-A and SonicNet-A, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of Run-Time WM Compression</head><p>Table <ref type="table" target="#tab_4">2</ref> shows the experimental results of the evaluation of the three mentioned DNNs running with our proposed run-time working memory compression (RTWMC) and compares it with the baseline, i.e., static structured pruning (SSP). The basic memory configurations of the target MCUs are shown with storage size and RAM size. After quantization, all of the three DNNs could fit into the storage of the target MCUs while the working memory of them still exceeds the corresponding RAM size. Therefore, to run on the target MCUs, pruning the intermediate feature maps is necessary.</p><p>Original ACC represents the accuracy of the DNNs without pruning, while pruned ACC is the accuracy after pruning. According to Table <ref type="table" target="#tab_4">2</ref>, for LeNet-A and SonicNet-A, the accuracy after pruning with our proposed method (i.e., RTWMC) is 0.62% and 2.3% higher than the original accuracy, respectively. It might be due to the regularization functionality provided by our pruning method, which could improve the generalization capability of the original DNNs. In addition, the accuracy of our method on LeNet-A and SonicNet-A outperforms that of the baseline (i.e., SSP) by 18.58% and 14.87%, respectively. As for SpArSeNet-A, our pruning method incurs 3.61% accuracy loss, while the accuracy loss of the baseline is 7.59%. The accuracy of our method outperforms the baseline by 3.98%. It shows that our method could reduce the accuracy loss incurred by pruning as much as possible. The main overheads of RTWMC are the overhead on memory and run-time latency. For SSP, all of the space of RAM are used to store the intermediate feature maps, while for RTWMC, some of them are reserved for the tiny buffer and Top ? cache as shown in Figure <ref type="figure" target="#fig_2">2</ref>. But the total memory overhead is quite small, which are 0.02 KB, 0.09 KB and 0.07 KB, respectively. And it is negligible compared with the corresponding RAM size. Moreover, the accuracy of our RTWMC with these memory overheads is still much higher than that of SSP as shown in Table <ref type="table" target="#tab_4">2</ref>. Besides, we also estimate the runtime latency of RTWMC, which is represented in the form of the ratio of the latency of RTWMC to that of running the DNNs without pruning. The ratio is 1.08x, 1.26x and 1.17x, respectively, which is moderate for running DNNs on MCUs. Therefore, we can claim that our RTWMC is lightweight for the deployment of DNNs on resource-constrained MCUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sensitivity Analysis of Hyperparameters</head><p>In this section, we report the experimental results about the sensitivity analysis of hyperparameters in RTWMS. Section 4.3.1 shows the analysis of pruning threshold ?, while Section 4.3.2 is about the analysis of buffer size ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Sensitivity</head><p>Analysis of Pruning Threshold ?. Figure <ref type="figure" target="#fig_4">3</ref> shows the impact of pruning threshold ? on the top-1 accuracy of the DNNs pruned by RTWMC, when the buffer size ? ? is fixed. For LeNet-A, SpArSeNet and SonicNet, the buffer size is 70 B, 30 B and 90 B, respectively. When the pruning threshold is 0, each output feature map is pruned equally, which means the number of activation values to be pruned is the same for each feature map. And the achieved accuracy is already relatively high. For LeNet-A and SonicNet-A, it is 18.32% and 14.59% higher than the baseline, respectively. And for SpArSeNet-A, it is only 0.78% lower than the baseline. This result shows the advantage of fine-grained dynamic unstructured pruning over the coarse-grained static structured pruning. Moreover, the accuracy could be further improved with an appropriate pruning threshold. The highest accuracy is 60%, 71.46% and 71% for LeNet-A, SpArSeNet and SonicNet, which is achieved at 0.7, 0.4 and 0.3, respectively. And all of them outperform the baseline by a large margin. Besides, when the threshold is large (i.e., 10 in our experiments), the accuracy of the evaluated DNNs drops dramatically and is less than 25% for all these three DNNs. More specifically, the corresponding accuracy is 20.28%, 11.22% and 17.31% for LeNet-A, SpArSeNet and SonicNet-A, respectively. In this case, the first several output feature maps are almost removed completely for inference, which reduces the accuracy significantly. This case is quite similar to the case where the structured pruning is applied in the run-time and thus without the chance to retrain the weights. Therefore, this result could imply the advantage of fine-grained dynamic unstructured pruning over the naive coarse-grained dynamic structured pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Sensitivity</head><p>Analysis of Buffer Size ? ? . Figure <ref type="figure" target="#fig_5">4</ref> shows the impact of buffer size ? ? on the top-1 accuracy of the DNNs pruned by RTWMC, when the pruning threshold ? is fixed. For LeNet-A, SpArSeNet and SonicNet, the pruning threshold ? is 0.3, 0.3 and 0.1, respectively. Since the complete output feature maps could not fit into the RAM, pruning could not be executed based on the global information of the feature maps. And in RTWMC, pruning is done in the unit of buffer size. If the buffer size is too small, the decision of pruning is only based on the information of a small number of activation values, which might lead to suboptimal solutions. Therefore, a bigger buffer size is helpful to make wiser decisions about pruning. But if the buffer size is too large, the memory overhead and the run-time overhead will be increased, which leads to the larger total number of activation values to be pruned and longer run-time latency. Thus, we need to choose an appropriate buffer size in order to get the best performance. The highest accuracy is 60.04%, 72.01% and 71.11% for LeNet-A, SpArSeNet and SonicNet, which is achieved with the buffer size of 10 B, 40 B and 30 B, respectively, For LeNet-A, the best performance could be achieved with quite small buffer size while for SpArSeNet and SonicNet, increasing the buffer size could improve the accuracy at the early stage. When the best accuracy is achieved, the trend of the three curves in Figure <ref type="figure" target="#fig_5">4</ref> becomes stable with little change in the accuracy. It means that the positive and negative impacts brought by a larger buffer size achieve a subtle balance. And thus, increasing the buffer size is not necessary for better accuracy. Therefore, we can conclude that for RTWMC, the best performance could be achieved with a relatively small buffer size, which justifies the small memory overhead and the moderate run-time overhead we claimed in Section 4.2 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>This work aims to enable the deployment of DNNs on resourceconstrained MCUs when their working memory exceeds the RAM size. It proposes a lightweight run-time working memory compression to dynamically prune the activation values on the intermediate output feature maps, such that the working memory is reduced to a size lower than the RAM size. Experimental results show that without incurring heavy overhead on memory and run-time latency, the compressed DNNs could maintain the original accuracy or run with moderate accuracy loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>? | denotes the number of activation values of input feature maps of layer ?, and |? ? | denotes the number of activation values of output feature maps of layer ?, which is equivalent to |? ?+1 |. For a DNN consisting of ? layers, its working memory ? is defined as max ? ?1,...,? ? ? . And the working memory ? of a specific DNN is oversized when ? exceeds the RAM size of the target MCU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Static Pruning vs. Dynamic Pruning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System Overview of Run-Time WM Compression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>? over all the ? batches initially. Note that although the output feature maps and their related bitmaps are made up of three dimensions, they are flattened to one dimension in Algorithm 2 for the simplicity of index. Function Conv(? ? , ?, ?, ?, ?) calculates the activation values starting from ? + 1 to ? through convolution operations, which might include batch normalization and ReLU functions. And the result ? ? is saved in the tiny buffer. The first ? ? activation values and their corresponding indices in the batch are sorted in descending order and used to initialize the Top ? cache as K and K ?? , respectively. Then each value ? ? ? in ? ? is compared with the values K through function TopKAdd(K ?? , K, ? ? ? , ?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top-1 Accuracy of Run-Time Working Memory Compression (RTWMC) with Different Pruning Threshold ? on Three Evaluated DNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top-1 Accuracy of Run-Time Working Memory Compression (RTWMC) with Different Buffer Size ? ? on Three Evaluated DNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b2">3</ref>. Therefore, we need a tiny space called Top ? cache, to keep track of the smallest ? activation values among the data in the buffer. Then, at least these ? values are pruned and only the remaining part of the activation values are saved in the space for output feature maps. Besides, there is a bitmap related to the pruned output feature maps ? ? . It uses one bit to indicate whether the corresponding activation values are pruned in ? ? . The bit will be set to one if the related activation value is pruned. When the MCU accesses the input feature maps of the next layer ? + 1, it will first query the bitmap, and use zero to represent the activation value whose bit is set to one for the following inference operations. Otherwise, it will employ the original values saved in ? ? . By introducing a tiny buffer and considering all the incurred memory overhead in the offline part of our algorithm, our run-time WM compression can guarantee that the DNN with oversized working memory could fit into RAM appropriately and run successfully on the target MCU.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.</head><label></label><figDesc>??? is the original size of output of layer ?. ? ? denotes the size of tiny buffer. And ? ?? is the size of bitmap, where? ?? = ? ??? [? ] ??and ?? is the bit width of an activation value. ? ??? is the size of Top ? cache, where Since we need to record both the indices and the values in the Top ? cache, there is a multiplier 2 to calculate ? ??? . Based on Eq. (2), we can get ? ? , which records the number of activation values to prune for layer ?. ? ? obtained in this case corresponds to the option 1 (Opt. 1) for ??? to calculate ?</figDesc><table><row><cell></cell><cell></cell><cell>(?)</cell></row><row><cell>? ??? = 2  *</cell><cell>(?</cell><cell>(? ) ??? -? (? ) (? ) ? ) * ? ? ? ???</cell></row><row><cell></cell><cell></cell><cell>(?)</cell></row><row><cell></cell><cell></cell><cell>(?)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(?) ? in</cell></row><row><cell cols="3">Algorithm 1.</cell></row><row><cell cols="3">Another constraint is from the observation that if ? and ? (?)</cell><cell>(?) ?? is small</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2) for ??? to calculate ?</figDesc><table><row><cell></cell><cell cols="4">(?) ? in Algorithm 1. Besides, if there is a</cell></row><row><cell cols="2">pooling layer after layer ?, ? we have</cell><cell>(?+1) ??</cell><cell>might not be equal to ?</cell><cell>(?) ??? . And</cell></row><row><cell>?</cell><cell>(?+1) ??</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Offline Part of Run-Time WM Compression Input: Original DNN ? with ? convolution layers, output threshold ?, RAM size ? ? , tiny buffer size ? ? , bit width ?? of an activation value Output: Online pruning array ? ? ? &gt; 0 for ? 1 . For layer ?, the activation values are pruned in the unit of batch ?, whose size is ? ? . The amount of values pruned ? ? for a batch is the mean of ?</figDesc><table><row><cell cols="4">1 ? ?? ? GetInSize(?);</cell></row><row><cell cols="4">2 ? ??? ? GetOutSize(?);</cell></row><row><cell cols="3">3 ? ??? ? ? ??? ;</cell><cell></cell></row><row><cell cols="3">4 for ? = 1, ..., ? do</cell><cell></cell></row><row><cell>5</cell><cell>? ?? ?</cell><cell>? ??? [? ] ??</cell><cell>;</cell></row><row><cell>20</cell><cell>else</cell><cell></cell><cell></cell></row><row><cell>21</cell><cell cols="2">??? ? 0;</cell><cell></cell></row><row><cell>22</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell>23</cell><cell cols="3">? ? [?] ? ??? ??? ;</cell></row><row><cell>24</cell><cell cols="2">if p then</cell><cell></cell></row><row><cell>31</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell>32</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell cols="2">33 end</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>(?)</cell><cell></cell><cell></cell></row></table><note><p><p><ref type="bibr" target="#b0">1</ref> </p>. The purpose of this part is to decide which activation values to prune dynamically according to the output feature maps of specific layer ? with Algorithm 1: 6 ? ??? [?] ? ? ??? [?] + ? ?? ; 7 ? ?? ? ? ?? [?] + ? ??? [?]; 8 ??? ? 1 -2 * (? ? /? ??? [?]); 9 ? ? False; 10 if ? ??? &gt; ? * ? ? then 11 ? ? True; 12 if ? ?? [?] + ? ? ? (1 -?) * ? ? then 13 ??? ? ? ?? [?] + ? ??? [?] + ? ? -? ? ; // Opt. 1 14 else 15 ??? ? ? ??? [?] -? * ? ? ; // Opt. 2 16 end 17 else if ? ?? &gt; ? ? then 18 ? ? True; 19 ??? ? ? ?? [?] + ? ??? [?] + ? ? -? ? ; // Opt. 1 25 ? ??? [?] ? ? ??? [?] -? ? [?]; 26 if there is a pooling layer after layer l then 27 ? ???? ? GetPoolSize(?, ?); 28 ? ?? [? + 1] ? min ? ??? [?], ? ???? ; 29 else 30 ? ?? [? + 1] ? ? ??? [?];</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Algorithm 2 :</head><label>2</label><figDesc>Online Part of Run-time WM Compression Input: Input data ? 1 , original DNN ? with ? convolution layers, online pruning array ? ? , pruning threshold ?, tiny buffer size ? ? , bitmap ?? initialized with zeros Output: Prediction ?</figDesc><table><row><cell cols="2">1 ? ??? ? 0;</cell></row><row><cell cols="2">2 ? ??? ? GetOutSize(?);</cell></row><row><cell cols="2">3 for ? = 1, ..., ? do</cell></row><row><cell>4 5 6</cell><cell>if ? ? [?] &gt; 0 then ? ? ? ??? [? ] ; ? ? ? ? ? ? ? [? ] ? ;</cell></row><row><cell>7</cell><cell>? ? ? ? ;</cell></row><row><cell>8</cell><cell>for ? = 1, ...,? do</cell></row><row><cell>9</cell><cell>if i == T then</cell></row><row><cell>23</cell><cell>end</cell></row><row><cell>24</cell><cell>end</cell></row><row><cell>25</cell><cell>if ? 0 &gt; ? ? then</cell></row></table><note><p>10 ? ? ? ? ? [?] -? ??? ; 11 ? ? ? ??? [?] mod ? ? ; 12 end 13 ?, ? ? (? -1) * ? ? , min {? * ? ? , ? ??? [?]}; 14 ? ? ? Conv(? ? , ?, ?, ?, ?); 15 K ?? , K ? Sort(? ? [0, ? ? ]); 16 ? 0 ? 0; 17 for ? = 1, ..., ? do 18 TopKAdd(K ?? , K, ? ? [ ?], ?); 19 if ? ? [ ?] &lt; ? then 20 SetBitmap(??, ? * (? ? -1) + ?); 21 ? ? [ ?] ? 0; 22 ? 0 ? ? 0 + 1; 26 ? ??? ? ? ??? + ? 0 ; 27 ? ? ? ? ? [? ]-? ??? ? -?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>28 else 29 for</head><label></label><figDesc></figDesc><table><row><cell>36</cell><cell>end</cell></row><row><cell>37</cell><cell>else</cell></row><row><cell cols="2">38 ? 39 end</cell></row><row><cell>40</cell><cell>if there is a pooling layer after layer ? then</cell></row><row><cell>41</cell><cell>? ? ? Pool(? ? , ?, ?);</cell></row><row><cell>42</cell><cell>end</cell></row><row><cell>43</cell><cell>? ?+1 ? ? ? ;</cell></row></table><note><p>? ? K ?? do 30 SetBitmap(??, ? * (? ? -1) + ?); 31 ? ? [?] ? 0; 32 end 33 ? ??? ? ? ??? + ? ? ; 34 end 35 ? ? [? : ?] ? ? ? ; ? ? Conv(? ? , ?, ?, 0, ? ??? [?]); 44 end 45 ? ? FC(? ? +1 , ?);</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 : Configuration of Evaluated DNNs</head><label>1</label><figDesc></figDesc><table><row><cell>Network</cell><cell cols="2">MS (KB) WM (KB)</cell><cell># Filters</cell><cell cols="3">Kernel Shape Pool Position FC Config</cell></row><row><cell>LeNet-A</cell><cell>19.79</cell><cell>4.59</cell><cell>[6, 32]</cell><cell>[5, 5]</cell><cell>[1, 2]</cell><cell>[120, 84, 10]</cell></row><row><cell>SpArSeNet-A</cell><cell>24.33</cell><cell>15.74</cell><cell>[9, 11, 17, 39]</cell><cell>[3, 4, 1, 5]</cell><cell>[2, 4]</cell><cell>[10]</cell></row><row><cell>SonicNet-A</cell><cell>60.17</cell><cell>15.31</cell><cell>[20, 80]</cell><cell>[5, 5]</cell><cell>[1, 2]</cell><cell>[10]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 : Evaluation of the Three DNNs with Run-Time WM Memory Compression</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Network</cell><cell></cell><cell></cell><cell cols="2">Storage Size (KB)</cell><cell cols="2">RAM Size (KB)</cell><cell cols="2">Original ACC (%)</cell><cell>Pruning Method</cell><cell>Pruned ACC (%)</cell><cell>Memory Overhead (KB)</cell><cell>Estimated Runtime Latency</cell><cell>Buffer Size (B)</cell><cell>Pruning Threshold</cell></row><row><cell></cell><cell>LeNet-A</cell><cell></cell><cell></cell><cell>32</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell>59.44</cell><cell>RTWMC</cell><cell>60.06</cell><cell>0.02</cell><cell>1.08x</cell><cell>10</cell><cell>0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSP</cell><cell>41.48</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">SpArSeNet-A</cell><cell></cell><cell>32</cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell>75.62</cell><cell>RTWMC</cell><cell>72.01</cell><cell>0.09</cell><cell>1.26x</cell><cell>40</cell><cell>0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSP</cell><cell>68.03</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">SonicNet-A</cell><cell></cell><cell>64</cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell>68.81</cell><cell>RTWMC</cell><cell>71.11</cell><cell>0.07</cell><cell>1.17x</cell><cell>30</cell><cell>0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSP</cell><cell>56.24</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>71</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 Accuracy (%)</cell><cell>61 63 65 67 69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LeNet-A SpArSeNet-A SonicNet-A</cell></row><row><cell></cell><cell>59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100 150 200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Buffer Size (B)</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was supported in part by the <rs type="funder">National Science Foundation</rs> under Grant <rs type="grantNumber">CNS-2007274</rs> and in part by the <rs type="funder">University of Pittsburgh Center for Research Computing</rs> through the resources provided. It used the <rs type="funder">Extreme Science and Engineering Discovery Environment (XSEDE)</rs> [15], which is supported by <rs type="funder">National Science Foundation</rs> grant number <rs type="grantNumber">ACI-1548562</rs>. Specifically, it used the Bridges system, which is supported by <rs type="funder">NSF</rs> award number <rs type="grantNumber">ACI-1445606</rs>, at the <rs type="institution">Pittsburgh Supercomputing Center (PSC)</rs> [13].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FYpGVUg">
					<idno type="grant-number">CNS-2007274</idno>
				</org>
				<org type="funding" xml:id="_DgASE6S">
					<idno type="grant-number">ACI-1548562</idno>
				</org>
				<org type="funding" xml:id="_3NWr9tU">
					<idno type="grant-number">ACI-1445606</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparsification and separation of deep learning layers for constrained resource inference on wearables</title>
		<author>
			<persName><forename type="first">Sourav</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM Conference on Embedded Network Sensor Systems CD-ROM</title>
		<meeting>the 14th ACM Conference on Embedded Network Sensor Systems CD-ROM</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="176" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse: Sparse architecture search for cnns on resource-constrained microcontrollers</title>
		<author>
			<persName><forename type="first">Igor</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Whatmough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4977" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intelligence beyond the edge: Inference on intermittent embedded systems</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Gobieski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Lucia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="199" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Personalized Deep Learning for Ventricular Arrhythmias Detection on Medical IoT Systems</title>
		<author>
			<persName><forename type="first">Zhenge</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhepeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichuan</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtong</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08060</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accuracy vs. efficiency: Achieving both through fpga-implementation aware neural architecture search</title>
		<author>
			<persName><forename type="first">Weiwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-M</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Design Automation Conference</title>
		<meeting>the 56th Annual Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Motivation for and evaluation of the first tensor processing unit</title>
		<author>
			<persName><forename type="first">Norman</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<title level="m">Pruning filters for efficient convnets</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2181" to="2191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bridges: a uniquely flexible HPC resource for new communities and data analytics</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Nicholas A Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><forename type="middle">Z</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Ray</forename><surname>Roskies</surname></persName>
		</author>
		<author>
			<persName><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure</title>
		<meeting>the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Artificial Intelligence (AI) in Big Data</title>
		<author>
			<persName><forename type="first">Markets</forename><surname>Research</surname></persName>
		</author>
		<ptr target="https://www.researchandmarkets.com/reports/4858130/artificial-intelligence-ai-in-big-data-data-as" />
	</analytic>
	<monogr>
		<title level="m">Data as a Service (DaaS), AI Supported IoT (AIoT), and AIoT DaaS</title>
		<imprint>
			<date type="published" when="2019">2019. 2019 -2024. Retrieved Nov. 27. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">John</forename><surname>Towns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Cockerill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maytal</forename><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Gaither</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Grimshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hazlewood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Lifka</surname></persName>
		</author>
		<author>
			<persName><surname>Gregory D Peterson</surname></persName>
		</author>
		<idno type="DOI">10.1109/mcse</idno>
		<ptr target="https://doi.org/10.1109/mcse" />
	</analytic>
	<monogr>
		<title level="j">XSEDE: Accelerating scientific discovery Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="62" to="74" />
			<date type="published" when="2014-09">2014. sep 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HAQ: Hardware-Aware Automated Quantization with Mixed Precision</title>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8612" to="8620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Intermittent Inference with Nonuniformly Compressed Multi-Exit Neural Network for Energy Harvesting Powered Devices</title>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhepeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenge</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtong</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/DAC18072.2020.9218526</idno>
		<ptr target="https://doi.org/10.1109/DAC18072.2020.9218526" />
	</analytic>
	<monogr>
		<title level="m">2020 57th ACM/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enabling On-Device CNN Training by Self-Supervised Instance Filtering and Error Map Pruning</title>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhepeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtong</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCAD.2020.3012216</idno>
		<ptr target="https://doi.org/10.1109/TCAD.2020.3012216" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3445" to="3457" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
