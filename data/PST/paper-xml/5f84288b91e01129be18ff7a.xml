<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty-Aware Few-Shot Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
							<email>zhizheng@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
							<email>culan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
							<email>chenzhibo@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty-Aware Few-Shot Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot image classification aims to learn to recognize new categories from limited labelled data. Recently, metric learning based approaches have been widely investigated which classify a query sample by finding the nearest prototype from the support set based on the feature similarities. For few-shot classification, the calculated similarity of a query-support pair depends on both the query and the support. The network has different confidences/uncertainty on the calculated similarities of the different pairs and there are observation noises on the similarity. Understanding and modeling the uncertainty on the similarity could promote better exploitation of the limited samples in optimization. However, this is still underexplored in few-shot learning. In this work, we propose Uncertainty-Aware Few-Shot (UAFS) image classification by modeling uncertainty of the similarities of querysupport pairs and performing uncertainty-aware optimization. Particularly, we design a graph-based model to jointly estimate the uncertainty of similarities between a query and the prototypes in the support set. We optimize the network based on the modeled uncertainty by converting the observed similarity to a probabilistic similarity distribution to be robust to observation noises. Extensive experiments show our proposed method brings significant improvements on top of a strong baseline and achieves the state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The strong capability of deep learning in part relies on a large number of labeled data for training, while humans are more readily able to learn knowledge quickly given a few examples. Few-shot learning <ref type="bibr" target="#b33">(Vinyals et al. 2016;</ref><ref type="bibr" target="#b11">Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b18">Karlinsky et al. 2019;</ref><ref type="bibr" target="#b10">Dong and Xing 2018;</ref><ref type="bibr" target="#b24">Mishra et al. 2018)</ref> strives to a further step to develop deep learning approaches which aim to generalize well to unseen (but related) tasks/classes with just few labeled samples (i.e., requiring/using limited labeled data). Among them, few-shot image classification <ref type="bibr" target="#b33">(Vinyals et al. 2016;</ref><ref type="bibr" target="#b30">Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b31">Sung et al. 2018;</ref><ref type="bibr" target="#b25">Oreshkin, López, and Lacoste 2018;</ref><ref type="bibr" target="#b11">Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b25">Nichol, Achiam, and Schulman 2018;</ref><ref type="bibr" target="#b22">Lee et al. 2019</ref>) has attracted much attention, which aims to classify data (query samples) of new categories given only a few labeled samples of these categories as examples (support samples).</p><p>For the few-shot image classification, several latest research works reveal that good feature embedding is amenable to deliver favorable classification performance for the similarity-based classification <ref type="bibr" target="#b16">(Chen et al. 2019</ref><ref type="bibr" target="#b6">(Chen et al. , 2020;;</ref><ref type="bibr" target="#b17">Huang and Tao 2019;</ref><ref type="bibr" target="#b32">Tian et al. 2020)</ref>. They average the feature embeddings of the support samples of a category to be the prototype of this category. Thus, with each prototype being category-specific, the set of prototypes plays the role of similarity-based classifier, which could identify the class of a query sample by finding the nearest prototype over the support set in the embedding space. During training, for a query sample, the logit vector (classification probability) is obtained through feeding the similarities between the query sample and the prototypes to a SoftMax function. Hence, the representativeness of the learned feature embedding and the reliability of the estimated similarity is vital to the classification performance.</p><p>The quality of the output of the network has been investigated and modeled with uncertainty in regression, classification, segmentation, multi-task learning <ref type="bibr" target="#b19">(Kendall, Badrinarayanan, and Cipolla 2015;</ref><ref type="bibr" target="#b20">Kendall and Gal 2017;</ref><ref type="bibr" target="#b3">Chang et al. 2020;</ref><ref type="bibr" target="#b20">Kendall, Gal, and Cipolla 2018)</ref> to benefit the optimization. Aleatoric uncertainty identifies the confidence level of the output of the network and it captures the noise inherent in the observations. For few-shot image classification, the network actually has different uncertainties on the calculated similarities of different query-prototype pairs. An observed similarity of a query-prototype pair, being a one time sampling, suffers from observation noise, where the higher of the uncertainty, the less reliable of the estimated similarity. For each few-shot task during the optimization, the number of experienced query-prototype pairs is limited and thus the side effect of observation noises due to the high uncertainty limits the optimization. Therefore, modeling uncertainty is especially vital for few-shot learning with the limited samples, but still under-explored.</p><p>In this paper, we propose an efficient uncertainty-aware few-shot image classification framework by modeling uncertainty and performing uncertainty-aware optimization. For the similarity-based few-shot image classification, particularly, we model the data-dependent uncertainty of the similarities between a query sample and the N prototypes of the support set through a graph-based model. To be robust to observation noises, for each query-prototype pair, we convert the observed similarity to a distribution (i.e., probabilistic similarity). The similarity-based classification losses are calculated based on the Monte Carlo sampling on the similarity distributions to alleviate the influence of observation noises.</p><p>In summary, our contributions lie in the following aspects: • We are the first to explore and model the uncertainty of the similarity of a query-prototype pair in few-shot image classification, where a better understanding of the limited data is particularly important. • We design a model to jointly estimate the uncertainty of the similarities between a query image and the prototypes in the support set. • We perform uncertainty-aware optimization to make the few-shot learning more robust to observation noises. We conduct comprehensive experiments and demonstrate the effectiveness of our proposed uncertainty-aware fewshot classification method. We conduct all our experiments in the popular inductive (non-transductive) setting, where each query sample is evaluated independently from other query samples. Our scheme achieves the state-of-the-art performance on multiple public datasets. We will release our code and models when the paper is accepted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-shot Image Classification</head><p>Few-shot image classification aims to recognize novel (unseen) classes upon limited labeled examples, which requires to generalize the accrued knowledge from base/seen classes to novel (unseen) classes. Representative approaches can be grouped in four categories.</p><p>Classification-based methods train both a feature extractor and classifiers with meta-learning and learn a new classifier (e.g., linear softmax classifier) for the novel classes during meta-testing <ref type="bibr" target="#b16">(Chen et al. 2019;</ref><ref type="bibr" target="#b27">Ren et al. 2019;</ref><ref type="bibr" target="#b22">Lee et al. 2019)</ref>. They in general need to update/fine-tune the network given a few-shot testing task.</p><p>Optimization-based methods exploit more efficient meta-learning optimization strategies for few-shot learning <ref type="bibr" target="#b11">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b14">Grant et al. 2018;</ref><ref type="bibr" target="#b25">Nichol, Achiam, and Schulman 2018;</ref><ref type="bibr" target="#b22">Lee et al. 2019)</ref>. MAML <ref type="bibr" target="#b11">(Finn, Abbeel, and Levine 2017</ref>) is a representative work, which trains models on a variety of learning tasks, such that only a few iterations are needed to achieve rapid adaptation for a new task. It achieves this by updating network parameters with the gradients jointly derived from several tasks.</p><p>Hallucination-based methods enhance the performance by augmenting data, which alleviates data deficiency by using the generated data. The generators transfer appearance variations <ref type="bibr" target="#b15">(Hariharan and Girshick 2017;</ref><ref type="bibr" target="#b12">Gao et al. 2018)</ref> or styles <ref type="bibr" target="#b0">(Antoniou, Storkey, and Edwards 2018)</ref> from base classes to novel classes. Wang et al. create an augmented training set through a hallucinator/generator which is trained end-to-end along with the classification algorithm <ref type="bibr" target="#b37">(Wang et al. 2018b)</ref>.</p><p>Metric-based methods classify an unseen instance to its nearest class based on the similarities with a few labeled examples. They aim to learn to project different instances to a metric space wherein the similarities among instances of the same category are larger than that of instances of different categories. Many approaches follow the meta-learning framework, which trains the model with few-shot tasks sampled in training data. Matching networks <ref type="bibr" target="#b33">(Vinyals et al. 2016)</ref> propose to assign an unlabeled data with the label of its nearest labeled data in one-shot learning. Furthermore, the prototypical network <ref type="bibr" target="#b30">(Snell, Swersky, and Zemel 2017)</ref> averages the features of several samples of the same category as the prototype of this class and classify unseen data by finding the nearest prototype in the support set. Here, the prototype is taken as the representation of a class and plays a role of the class centroid. Relation networks <ref type="bibr" target="#b31">(Sung et al. 2018)</ref> propose an additional relation module as a learnable metric jointly trained with deep feature representations. Some recent metric-based works reveal that training/pretraining the network with a (fully connected layer) classifier with cosine metric is helpful for learning good feature representations for few-shot tasks <ref type="bibr" target="#b16">(Chen et al. 2019;</ref><ref type="bibr" target="#b32">Tian et al. 2020;</ref><ref type="bibr" target="#b6">Chen et al. 2020)</ref>. They thus propose strong baselines by training the network with classification supervision over all base classes and further performing metalearning on sampled categories to simulate the few-shot setting. Chen et al. confirmed that the classification-based pretraining can provide extra transferability from base classes to novel classes in the meta-learning stage <ref type="bibr" target="#b6">(Chen et al. 2020)</ref>.</p><p>In this paper, on top of such a strong baseline, we look into an important but still under-explored issue, i.e., modeling and exploiting the inherent uncertainty of the calculated similarities for better optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Uncertainty in Deep Learning</head><p>For deep neural network, there are two main types of uncertainty that can be modeled: aleatoric uncertainty, and epistemic uncertainty <ref type="bibr" target="#b20">(Kendall and Gal 2017;</ref><ref type="bibr" target="#b12">Gal 2016;</ref><ref type="bibr" target="#b20">Kendall, Gal, and Cipolla 2018)</ref>. Epistemic uncertainty captures model-related stochasticity, including parameters choices, architecture, etc. Aleatoric uncertainty captures the noise inherent in the observations <ref type="bibr" target="#b20">(Kendall and Gal 2017;</ref><ref type="bibr" target="#b12">Gal 2016;</ref><ref type="bibr" target="#b20">Kendall, Gal, and Cipolla 2018)</ref>. The aleatoric uncertainty can be further divided into two sub-categories.</p><p>Heteroscedastic uncertainty, as a sub-category of aleatoric uncertainty, is data-dependent, which depends on the input data of the model, with some noisy inputs or rarely seen inputs potentially having lower confidence on the output/prediction. For per-pixel semantic segmentation and depth regression tasks, input-dependent heteroscedastic uncertainty are considered as loss attenuation for better optimization <ref type="bibr" target="#b20">(Kendall and Gal 2017)</ref>. In contrast, homoscedastic uncertainty is a quantity which is the same for all input data and varies between different tasks. It is thus also described as task-dependent uncertianty. It has been adopted in weighting the loss function for multi-task learning <ref type="bibr" target="#b20">(Kendall, Gal, and Cipolla 2018)</ref>.</p><p>For few-shot learning classification, the "hardness" of a few-shot episode is quantified with a metric in <ref type="bibr">(Dhillon et</ref>    <ref type="formula" target="#formula_6">5</ref>)). <ref type="formula">2020</ref>). However, this metric is only used to evaluate few-shot algorithms rather than improve them in training. In <ref type="bibr" target="#b36">(Wang et al. 2020)</ref>, a statistical approach is adopted to measure the credibility of predicted pseudo-labels for unlabeled samples in the semi-supervised or transductive few shot learning setting, where the modeled credibility is used for selecting the most trustworthy pseudo-labeled instances to update the classifier. There is still a lack of exploration of the uncertainty for more general inductive few-shot setting, where the network does not use/need testing samples for updating.</p><p>In this paper, we model the data-dependent uncertainty of the calculated similarities between a query sample and the prototypes, and leverage it to better optimize the network in the training. To the best of our knowledge, we are the first to explore the uncertainty of pair-similarity in few-shot classification for a more efficient optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Uncertainty-aware Few-shot Image Classification</head><p>We propose a Uncertainty-Aware Few-Shot (UAFS) image classification framework (see Figure <ref type="figure" target="#fig_2">2</ref>). In order to efficiently exploit the limited samples in the episodic training, we model the uncertainty of the estimated similarities between a query and the prototypes to eliminate the side effects of observation noises. Particularly, given a query and the N prototypes, we use a graph-based network to jointly learn the N uncertainty values for the similarities of the N query-prototype pairs. Based on the estimated uncertainty, we represent the similarity as a distribution for each queryprototype pair. Then, we perform Monte Carlo sampling on the distributions of similarities for more accurate loss estimation and optimization.</p><p>To be self-included, we first introduce the problem formulation and a strong baseline in Section 3.1. Then, we elaborate our proposed uncertainty-aware few-shot image classi-fication in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries and a Strong Baseline</head><p>Problem Formulation. For few-shot image classification, all the categories/classes of the dataset are divided into base classes C base for training and novel classes C novel for testing without class overlapping <ref type="bibr" target="#b30">(Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b16">Chen et al. 2019;</ref><ref type="bibr">Dhillon et al. 2020)</ref>. In few-shot learning, episodic training is widely used. Each N -way K-shot task randomly sampled from base classes is defined as an episode, where the support set S includes N classes with K samples per class, and the query set Q contains the same N classes with M samples per class. Our method belongs to the metric-based few-shot learning which performs similaritybased classification based on matching the prototypes in the support set for a given query sample.</p><p>Given an image x i , we feed it to a Convolution Neural Network (CNN) to obtain a feature map, and perform global averaging pooling to get a D-dimensional feature vector z i = f φ (x i ) ∈ R D as the embedding result of x i , where f φ (•) denotes the feature extractor. The prototype of a class indexed by k is calculated by averaging the feature embeddings of all the samples in the support set of this class as:</p><formula xml:id="formula_0">c k = 1 |S k | (xi,yi)∈S k f φ (x i ),<label>(1)</label></formula><p>where x i denotes the sample indexed by i and y i denotes its label, S k denotes the set of samples of the k-th class in the support set</p><formula xml:id="formula_1">S = {S 1 , • • • , S N }, |S k | denotes the number of samples in S k .</formula><p>Given a query image x, the probability of classifying it to the class k is:</p><formula xml:id="formula_2">p(y i = k/x i ) = exp(τ • s(f φ (x i ), c k )) N j=1 exp(τ • s(f φ (x i ), c j )) ,<label>(2)</label></formula><p>where the temperature parameter τ is a hyper-parameter, and N is the number of classes in the support set (i.e., Nway). The s(f φ (x i ), c k ) represents the similarity between the given query-sample x i and the prototype c k of the class k. For metric-based few-shot image classification, the N prototypes of N classes in the support set construct the similarity-based classifier.</p><p>Baseline. Many works <ref type="bibr" target="#b33">(Vinyals et al. 2016;</ref><ref type="bibr" target="#b30">Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b1">Bauer et al. 2017;</ref><ref type="bibr" target="#b16">Chen et al. 2019;</ref><ref type="bibr" target="#b23">Li et al. 2019;</ref><ref type="bibr" target="#b6">Chen et al. 2020)</ref> reveal that the core of metricbased few-shot learning is to learn a good feature embedding function f φ (•) from base classes C base which could generalize well on unseen classes. We follow the latest work <ref type="bibr" target="#b6">(Chen et al. 2020)</ref> which simply leverage both the classificationbased pre-training and the meta-learning training to build the strong baseline with the two stages:</p><p>Stage-1: Classification-based pre-training. We employ a Fully-Connected (FC) layer as the classifier with cosine similarity metric over all base classes. We train the feature extractor f φ (•) (backbone network) and the classifier in an endto-end manner using a standard cross-entropy loss.</p><p>Stage-2: Meta-learning. For each episode/task, we sample N -way K-shot from base classes for training, which can be understood as a simulation of few-shot test process. In each task, N × K images and N × M images are sampled to constitute the support set and the query set, respectively. We adopt the cross-entropy loss formulated by <ref type="formula" target="#formula_2">2</ref>) with s(•, •) instantiated by cosine similarity.</p><formula xml:id="formula_3">L = − N ×M i=1 y i • log(p(y i /x i )), where p(y i /x i ) is calcu- lated in Eq. (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Uncertainty-aware Few-shot Learning</head><p>For a given model, it has different uncertainties regarding its outputs for different input data and there will be observation noises for an instantiated output. For similarity-based few-shot image classification, the class of a query (instance) is determined based on its similarity with the prototypes in the support set. The model has uncertainty on the estimated similarities for the query-prototypes pairs. The small number of samples in each few-shot episode exacerbate the side effect of observation noises. To address this, we model the data-dependent uncertainty of similarities and leverage it for better optimizing the network. Probabilistic Similarity Representation. To model the uncertainty of similarity of a query-prototype pair, we convert the similarity representation of this query-prototype pair from a deterministic value to a probabilistic representation (i.e., a distribution). In this way, the similarity uncertainty can be modeled by the parameters of the distribution. Given a query sample x i (with its feature z i = f φ (x i ) in the latent space) and a prototype c j obtained by Eq. ( <ref type="formula" target="#formula_0">1</ref>), instead of being a deterministic value, we model the their similarity as a probability distribution. Similar to many other works <ref type="bibr" target="#b20">(Kendall and Gal 2017;</ref><ref type="bibr" target="#b12">Gal 2016</ref>), we simply model the distribution of the similarity between this query and the prototype c j by a Gaussian distribution with mean µ ij and variance σ 2 ij :</p><formula xml:id="formula_4">p(s ij |z i , C) = N (s ij ; µ ij , σ 2 ij I),<label>(3)</label></formula><p>where C = {c 1 , • • • , c N } denotes the N prototypes in the support set. We estimate the mean of the similarity by their inner product, i.e., µ ij = z i , c j , which denotes the most likely value of the similarity between z i and c i . The variance σ 2 ij denotes the uncertainty of this pairwise similarity. We estimate it by a graph neural network with trainable parameters, which will be elaborated in the subsection "Similarity Uncertainty Modeling". With the similarity of a querysupport pair being a distribution rather than a deterministic value, we enable a differentiable instantiated representation of the probabilistic similarity s ij by re-parameterizing it as:</p><formula xml:id="formula_5">s ij,t = µ ij + σ ij t , t ∈ N (0, 1).<label>(4)</label></formula><p>Uncertainty-aware Optimization. To reduce the side effect of observation noises in the optimization of an few shot episode, we calculate the loss based on the distributions of similarities (i.e., probabilistic similarities). However, the analytic solution of integrating over these distributions for the optimization of loss function is difficult. We thus approximate the objective by Monte Carlo integration. We achieve this by using Monte Carlo sampling on the N similarity distributions. Particularly, for a given query image x i and N prototypes c j , j = 1, • • • , N which form N similarity pairs, we repeat T random sampling over the similarity distributions for each query-prototype pair s ij (see (4) with t = 1, • • • , T ) to obtain statistical results. Therefore, for a given query image x i with groundtruth label y i = k, we obtain its corresponding classification loss as:</p><formula xml:id="formula_6">L(x i , y i = k) = −log( 1 T T t=1 (e s ik,t / N j=1 e sij,t )).<label>(5)</label></formula><p>Similarity Uncertainty Modeling. We propose a Graph Neural Network (GCN) based model to jointly estimate the similarity uncertainties (denoted by the variances of the similarity distributions) for N query-prototype pairs. To estimate the uncertainty σ ij of the similarity between the query image x i and the prototype c j of the class j, one solution is to use the information of this pair to infer σ ij . However, similarity-based N class classification is a joint determination process, where the query is compared with all the N prototypes and the class of the prototype with the highest similarity value is taken as the predicted class. The joint determination nature inspires us to estimate uncertainty σ ij for each pair after taking a global glance of the N pairs with j = 1, • • • , N . Therefore, we design a graph-based model as the similarity uncertainty estimator to jointly infer all the σ ij for a given query x i and N prototypes c j . This enables the exploration of global scope information for the current task to jointly estimate the similarity uncertainties. Unlike the previous work <ref type="bibr" target="#b13">(Garcia and Bruna 2018)</ref>, which adopts graph-based model to propagate information from labeled samples to unlabeled ones thus delivers a transductive solution, we use graph-based model to achieve the joint estimation of uncertainty of N query-prototype pairs for a given query (non-transductive), where the uncertainty is used to better optimize the network. Our graph-based model is discarded in testing. The GCN in <ref type="bibr" target="#b13">(Garcia and Bruna 2018)</ref> is a part of their network and is needed during testing.</p><p>The feature of a query image x i is denoted as z i = f φ (x i ) ∈ R D and the prototypes of its support set are denoted by c j ∈ R D , j = 1, • • • , N . We use the group-wise relations/similarities between the query and the j th prototype feature to represent the information of this query-prototype pair. Specifically, we split both z i and c j into L groups along their channel dimensions to have {z l i |l = 1, • • • , L} and {c l j |l = 1, • • • , L}, where L ≤ D. We calculate the cosine similarity of the l th group as r l ij =</p><formula xml:id="formula_7">z l i c l j T z l i c l j</formula><p>, and stack them to be a relation feature vector</p><formula xml:id="formula_8">v ij = [r 1 ij , • • • , r L ij ] ∈ R L .</formula><p>In this simple way, we make use of the similarities with respect to different semantics of feature (in different channel groups) to infer the uncertainty of similarity.</p><p>Given a query and N prototypes, we build a graph of N nodes, with the j th node denoted by the information of the query and the j th prototype. The output of the node is the predicted uncertainty of the similarity of this queryprototype pair. The N nodes in the graph are represented by a matrix</p><formula xml:id="formula_9">V i = (v i1 ; • • • ; v iN ) ∈ R N ×L .</formula><p>We use graph convolutional network (GCN) to learn the similarity uncertainty for each node. Similar to <ref type="bibr" target="#b29">(Shi et al. 2019;</ref><ref type="bibr" target="#b34">Wang et al. 2018a;</ref><ref type="bibr" target="#b35">Wang and Gupta 2018)</ref>, the edge from the j th node to the j th node is modeled by their affinity in the embedded space, which is formulated as:</p><formula xml:id="formula_10">E i (j, j ) = ϕ 1 (v ij )ϕ 2 (v ij ) T ,<label>(6)</label></formula><p>where ϕ 1 and ϕ 2 denote two embedding functions implemented by FC layers. All edges constitute an adjacent matrix denoted by E i ∈ R N ×N . We normalize each row of E i with SoftMax function so that all the edge values connected to a target node is 1, yielding a numerically-stable message passing through the modeled graph. Similar to <ref type="bibr" target="#b34">(Wang et al. 2018a)</ref>, we denote the normalized adjacent matrix by G i and update the nodes through GCN as:</p><formula xml:id="formula_11">V i = V i + Y i W y , where Y i = G i V i W υ ,<label>(7)</label></formula><p>W υ ∈ R L×L and W y ∈ R L×L are two learnable transformation matrices. W υ is implemented by a 1 × 1 convolutional layer. W y is implemented by two stacked blocks. Each block consists of a 1 × 1 convolution layer, followed by a Batch Normalization (BN) layer and an LeakyReLU activation layer. We thus infer the similarity uncertainty vector</p><formula xml:id="formula_12">u i = [σ i1 , • • • , σ iN ] ∈ R N</formula><p>for the N pairs from the updated graph nodes, which can be formulated as:</p><formula xml:id="formula_13">u i = α(BN(V i W u1 ))W u2 ,<label>(8)</label></formula><p>where the j th -dimension of u i is the aforementioned σ ij , denoting the similarity uncertainty for the query x i and the j th prototype. W u1 ∈ R L×L and W u2 ∈ R L×1 are transformation matrices both implemented by a 1 × 1 convolutional layer. "BN" refers to the Batch Normalization layer and α(•) refers to the LeakyReLU activation function. Discussion: For similarity-based few-shot classification, the class estimation of a query image is influenced by not only the query itself but also the prototypes in the support set. Therefore, different from the uncertainty modeling in depth regression, segmentation <ref type="bibr" target="#b20">(Kendall and Gal 2017)</ref> which model the uncertainty based on the sample itself, we model the uncertainty of similarity based on both the query sample and the prototypes. We further conduct extensive experiments to compare the effectiveness of modeling feature uncertainty (query itself) vs. similarity uncertainty (query and prototypes) for few-shot image classification in our experiments. Moreover, in our ablation study, we also compare our graph-based modeling with non-graph-based modeling (i.e.Conv-based), which infers the uncertainty of a queryprototype pair based on the pair itself.</p><p>Besides, our proposed similarity uncertainty estimator is scalable to the number of classes for classification, which allow us to optimize it during different training stages as introduced in the next subsection. Joint Training. Similar to the baseline configuration described in Section 3.1, we train the entire network in two stages: classification-based pre-training and meta-learning.</p><p>In the classification-based pre-training stage, we perform classification over all classes of C base . The learned FC layer coefficients of the classifier play a role of class centers, which can be considered as the prototypes in similaritybased classification. The classification scores are obtained by calculating the cosine similarity between the instance feature and those class centers (prototypes). From this perspective, the classification in the two stages are unified.</p><p>In the meta-learning stage, we sample N -way K-shot episodic tasks from base classes and perform episodic training. Actually, our proposed similarity uncertainty modeling and optimization is applicable to both stages. In the classification-based pre-training stage, the FC coefficients can be considered as the prototypes and the number of input nodes in GCN is the total number of classes in C base . In the meta-learning stage, the number of input nodes in GCN is N . The parameters in GCN are shared in the two stages. We fine-tune the backbone and the similarity uncertainty estimator in the meta training stage.</p><p>Note that we do not introduce any complexity increase in the testing where the uncertainty estimator is discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Implementation</head><p>Datasets: For few-shot image classification, we conduct experiments on four public benchmark datasets: mini-ImageNet <ref type="bibr" target="#b33">(Vinyals et al. 2016)</ref>, tiered-ImageNet <ref type="bibr" target="#b27">(Ren et al. 2018)</ref>, CIFAR-FS <ref type="bibr" target="#b2">(Bertinetto et al. 2018)</ref>, and FC100 (Oreshkin, López, and Lacoste 2018) (See our Supplementary for more detailed introduction). Both mini-ImageNet and tiered-ImageNet are built upon ImageNet <ref type="bibr" target="#b7">(Deng et al. 2009</ref>) and we use the commonly used 84 × 84 spatial resolution. CIFAR-FS and FC100 are built on CIFAR-100 <ref type="bibr" target="#b21">(Krizhevsky, Hinton et al. 2009</ref>) and 32 × 32 spatial resolution is used.</p><p>Networks and Training: Following recent common practices in this field (Oreshkin, López, and Lacoste 2018; <ref type="bibr" target="#b22">Lee et al. 2019)</ref>, we adopt a wider ResNet-12 with more channels as the backbone in our work unless claimed otherwise. We build our strong baseline (see Section 3.1) by following <ref type="bibr" target="#b6">(Chen et al. 2020)</ref>  epochs with the batch size of 512 for tiered-ImageNet and 100 epochs with the batch size of 128 for the other three datasets. For meta-learning stage, we uniformly sample 4 episodes/tasks per iteration for mini-ImageNet and tiered-ImageNet, and 16 episodes per iteration for CIFAR-FS and FC100. We sample 15 query samples per class (i.e., Q = 15) within each episode in training. We adopt the SGD optimizer with the initial learning rate of 0.1 and the weight decay of 5.e-4. For all our models, the classification temperature τ in (2) is a trainable parameter which is initialized with 10. We perform data augmentation, including horizontal flip, random crop , and color jitter during training. For the parameter L, we found the performance is very similar when it is set within the range of 16 to 64 and we set it to 32. Evaluation: We do all experiments in the inductive (nontransductive) setting (i.e., each query sample is evaluated independently from other query samples). We create each few-shot episode by uniformly sampling 5 classes (i.e.N =5) from the test set and further uniformly sampling support and query samples for each sampled class accordingly. Consistent with other works, we report the mean accuracy and the standard deviation with a 95% confidence interval over 1000 randomly sampled few-shot episodes. Besides, we do not perform fine-tuning using the data on the novel classes (i.e., support set) in testing to make the system simple/easy to use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Effectiveness of Proposed UAFS. Table <ref type="table" target="#tab_1">1</ref> shows the ablation study on the effectiveness of the proposed similar-ity uncertainty modeling and optimization. Model 3 corresponds to our strong baseline Meta-Base (see Table <ref type="table">3</ref>) with two-stage training. Model 6 corresponds to our final scheme Meta-UAFS with the proposed similarity uncertainty modeling and optimization (UMO).</p><p>We have the following observations. 1) Based on the strong baseline, our proposed Meta-UAFS (Model-6) improves Meta-Base (Model-3) by 1.12%, 1.41%, 1.72% and 1.76% in 1-shot classification on four datasets respectively, which demonstrates the effectiveness of our UMO.</p><p>2) Model-3 obviously outperforms Model-1, demonstrating the effectiveness of two-stage training as a strong baseline.</p><p>3) Model-2 with UMO obviously outperforms Model-1 but Model-4 only slightly outperforms Model-3. The higher the performance, the more difficult it is to obtain gain. On top of the strong baseline, the UMO is not helpful for Stage-1 which does not suffer from limited training data as Stage-2. 4) Applying the UMO only for the meta-learning stage does not always bring improvements. Applying the UMO on both stages brings significant improvements. In the meta-learning stage for model-5, the backbone is pre-trained but the similarity uncertainty estimator is trained from scratch, resulting in the difficulty in optimizing the estimator with limited data. Since our proposed graph-based similarity uncertainty estimator is scalable to the number of classes, this allows us to use the estimator with shared parameters in classificationbased pre-training stage, which effectively reduces the difficulty of optimization (playing a pre-training role for the similarity uncertainty estimator). Therefore, our final proposed model (Model-6) delivers the best results. Note that in meta-learning state, the number of training samples in a task is small and the side-effect of observation noise is more harmful. Observation 3) and 4) show that a well designed UMO (with pre-training) is very helpful for meta-learning where the number of training samples in a task is small. Similarity Uncertainty Estimator Designs. For a query and the N prototypes, we can independently estimate the uncertainty value for each query-prototype pair. However, since the class of the query is actually jointly determined by comparing it with the N prototypes. Thus, we propose to use a GCN to jointly estimate the uncertainty values. Table <ref type="table" target="#tab_2">2</ref> shows that our Graph-based similarity uncertainty estimator (last row) obviously outperforms the Conv-based similarity uncertainty estimator (the penultimate row) which uses 1×1 convolutional layers to independently infer the uncertainty Table <ref type="table">3</ref>: Accuracy (%) comparison for 5-way few-shot classification of our schemes, our baselines and the state-of-the-arts on four benchmark datasets. For the backbone network, "l 1 -l 2 -l 3 -l 4 " denotes a 4-layer convolurional network with the number of convolutional filters in each layer as l 1 , l 2 , l 3 , l 4 , respectively. classification-based pre-training stage. "Meta-Base" and "Meta-UAFS" refer to the strong baseline model and our final UAFS model, which are both trained with two stages (the second stage: meta-learning stage). The superscript † refers to that the model is trained on the combination of training and validation sets while others only use the training set. Note that for fair comparisons, all the presented results are from inductive (non-transductive) setting. Bold numbers denotes the best performance while numbers with underlines denotes the second best performance. for each query-prototype pair. Sample Uncertainty vs. Pair Similarity Uncertainty. Table 2 shows that the scheme B+SimiU with Graph-based uncertainty estimator which models the uncertainty of the query-prototype similarity is more effective than the scheme B+SampleU which models the uncertainty of a sample. That is because in the few shot classification, for a given query, its estimated class depends on both the query and the N prototypes. Therefore, we model the uncertainty of the similarity between the query and the prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-arts</head><p>In Table <ref type="table">3</ref>, we compare our proposed Meta-UAFS with the state-of-the-art approaches. Note that our design of baseline network Meta-Base is the same as MetaBaseline 1 <ref type="bibr" target="#b6">(Chen et al. 2020)</ref> and they have the similar performance. In comparison with Meta-Base <ref type="bibr" target="#b6">(Chen et al. 2020)</ref>, our Meta-UAFS achieves significant improvement of 1.12%, 1.41%, 1.72%, and 1.76% in 1-shot accuracy on mini-ImageNet, tiered-ImageNet, CIFAR-FS, and FC00, respectively for 1-shot classification. Our Meta-UAFS achieves the state-of-the-art performance. CAN <ref type="bibr" target="#b16">(Hou et al. 2019)</ref> introduces cross attention module to highlight the target object regions, making the extracted feature more discriminative. Our uncertainty modeling and optimization which reduces the side effects of observation noises is complementary to their attention design and incorporating their attention design into our framework would lead to superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization of Learned Uncertainty</head><p>Our exploration of uncertainty could reduce the side effect of observation noises on the similarity during optimization. We visualize the learned uncertainty of many randomly selected query-prototype tasks and find they are consistent with human intuition. Figure <ref type="figure" target="#fig_2">2</ref> shows two examples. Given 1 In their test code, rather than using the more commonly used number of testing episodes, i.e., 1000, they use 8000. In contrast, we report Meta-Base using 1000.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose an Uncertainty-Aware Few-Shot image classification approach where data-dependent uncertainty modeling is introduced to alleviate the side effect of observation noise. Particularly, we convert the similarity between a query sample and a prototype from a deterministic value to a probabilistic representation (using a distribution) and optimize the networks by exploiting the similarity uncertainty. We design a graph-based uncertainty estimator to jointly estimate the similarity uncertainty of the querysupport pairs for a given query sample. Extensive experiments demonstrate the effectiveness of our proposed method and our scheme achieves state-of-the-arts performance on all the four benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>We are the first to introduce aleatoric uncertainty modeling into few-shot learning. Specifically, we model datadependent uncertainty for the similarity of query-support pairs in few-shot image classification. The broader impact of our work lies in the following aspects:</p><p>For research community, we point out the importance of uncertainty exploration in deep learning with limited labeled data (few shot learning) and our solution bring improvement for few-shot image classification. This is overlooked in previous works in this domain. Despite the main focus of our this work is on the modeling of data-dependent uncertainty, we believe it will inspire more studies on uncertainty exploration in the field of few-shot learning. It is exciting to see more extensions to other few-shot tasks, e.g. few-shot object detection, few-shot scene segmentation.</p><p>For industry/society, besides the improvements on fewshot image classification accuracy, this work makes contributions to developing a risk-controlled few-shot recognition system. In many scenarios, we not only expect a higher performance of our system but also want to make sure the risks of using this system can be controlled when facing complex especially health-related application scenarios. Therefore, we would expect the model to reject the results by itself or push it to human decision if it is not confident enough. Intuitively, the output with low confidence is more likely to be given by models trained with very limited data. Thus, it becomes critical for us to address this problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Pipeline of proposed Uncertainty-Aware Few-Shot (UAFS) image classification. To alleviate the influence of observation noises on the similarity of a query-prototype pair, rather than a scalar value, we model the similarity between a query feature z i and a prototype feature c j (where j = 1, • • • , N ) by a distribution, i.e., probabilistic similarity, based on graph-based similarity uncertainty estimation. For a query sample x i and N prototypes in the support set, we take each queryprototype pair as a node and employ a graph-based model to jointly infer the similarity uncertainty σ i,j for each query-prototype pair. The network is optimized with the similarity-based classification losses which exploit the estimated uncertainty (not shown in this figure, please see the subsection around (5)).</figDesc><graphic url="image-2.png" coords="3,55.23,55.45,108.91,146.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of learned uncertainty values for two examples. a fixed query sample of class "Box", four negative support samples (last four columns) and a positive support sample, when compare (a) and (b), we find the positive pair in (b) has less visual ambiguity than that in (a). Consistently, our learned uncertainty value of 0.069 in (b) is much lower than that (0.283) in (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>al.</figDesc><table><row><cell>query image</cell><cell>𝒙 𝒊</cell><cell>query Embedding Space</cell><cell></cell><cell>𝜇 𝑖,5</cell><cell>Modeling Similarity Uncertainty</cell></row><row><cell></cell><cell></cell><cell>𝒛 𝑖 = 𝑓 𝜙 (𝒙 𝑖 )</cell><cell cols="2">𝜎 𝑖,5</cell></row><row><cell>support-set</cell><cell>Feature Extraction</cell><cell cols="3">probabilistic similarity</cell><cell>𝜎 𝑖,3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>𝜎 𝑖,1</cell><cell>𝜎 𝑖,5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>prototypes</cell><cell>𝜎 𝑖,2</cell><cell>𝜎 𝑖,4</cell></row><row><cell></cell><cell></cell><cell cols="2">𝒄 1 𝒄 2 𝒄 3 𝒄 4 𝒄 5</cell><cell>(classifier)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>with two-stage training. For classification-based pre-training stage, we train 120 Comparison on applying the proposed similarity uncertainty modeling and optimization in different training stages. "Stage1" refers to the classification-based pre-training stage while "Stage2" refers to the meta-learning stage as described in the manuscript. "no" denotes the corresponding training stage is not used. "w U" denotes that we apply the proposed similarity uncertainty modeling and optimization in the corresponding training stage while the "w/o U" denotes we do not apply it.</figDesc><table><row><cell cols="3">Model Stage1 Stage2</cell><cell cols="2">mini-ImageNet</cell><cell cols="2">tiered-ImageNet</cell><cell cols="2">CIFAR-FS</cell><cell>FC-100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>1</cell><cell>w/o U</cell><cell>no</cell><cell cols="6">59.28 ± 0.81 78.26 ± 0.60 67.02 ± 0.84 83.56 ± 0.61 71.95 ± 0.81 84.21 ± 0.58 39.42 ± 0.77 54.12 ± 0.79</cell></row><row><cell>2</cell><cell>w U</cell><cell>no</cell><cell cols="6">61.53 ± 0.83 78.83 ± 0.60 68.77 ± 0.82 83.92 ± 0.42 73.81 ± 0.78 84.54 ± 0.63 41.76 ± 0.52 56.63 ± 0.24</cell></row><row><cell>3</cell><cell cols="8">w/o U w/o U 63.10 ± 0.85 79.44 ± 0.65 67.72 ± 0.80 83.61 ± 0.62 72.36 ± 0.67 84.43 ± 0.50 40.23 ± 0.22 56.16 ± 0.56</cell></row><row><cell>4</cell><cell>w U</cell><cell cols="7">w/o U 63.16 ± 0.66 79.56 ± 0.67 68.92 ± 0.77 83.96 ± 0.56 73.10 ± 0.70 84.68 ± 0.48 40.34 ± 0.59 56.24 ± 0.55</cell></row><row><cell>5</cell><cell>w/o U</cell><cell>w U</cell><cell cols="6">62.57 ± 0.33 79.35 ± 0.49 69.11 ± 0.83 84.12 ± 0.55 72.51 ± 0.68 84.46 ± 0.49 39.49 ± 0.33 53.24 ± 0.24</cell></row><row><cell>6</cell><cell>w U</cell><cell>w U</cell><cell cols="6">64.22 ± 0.67 79.99 ± 0.49 69.13 ± 0.84 84.33 ± 0.59 74.08 ± 0.72 85.92 ± 0.42 41.99 ± 0.58 57.43 ± 0.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison on different uncertainty modelling methods. "SampleU" denotes that we estimate the uncertainty of a query sample while "SimiU" denotes that we estimate the uncertainty of the similarity between a query and its prototype. Besides, we compare different designs of the uncertainty estimator (U-Estimator), including using CNNs ("Conv-based") to process each sample or each query-prototype pair independently, using GCNs ("Graphbased") to process the N query-prototype pairs jointly. "B" denotes the abbreviation of "Meta-Base". ± 0.85 79.44 ± 0.65 72.36 ± 0.67 84.43 ± 0.50 B+ SampleU Conv-based 63.25 ± 0.81 79.39 ± 0.63 72.11 ± 0.80 84.26 ± 0.52 B+ SimiU Conv-based 63.31 ± 0.68 79.63 ± 0.46 73.22 ± 0.75 84.40 ± 0.69 B+ SimiU Graph-based 64.22 ± 0.67 79.99 ± 0.49 74.08 ± 0.72 85.92 ± 0.42</figDesc><table><row><cell>Methods</cell><cell>U-Estimator</cell><cell cols="2">mini-ImageNet</cell><cell>CIFAR-FS</cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Meta-Base</cell><cell>-</cell><cell>63.10</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Ravichandran, Bhotika, and Soatto 2019) ResNet-12 59.04 ± 0.43 77.64 ± 0:39 66.87 ± 0.43 82.64 ± 0.39 69.20 ± 0.40 84.70 ± 0.40 --MetaOptNet (Lee et al. 2019) ResNet-12 62.64 ± 0.61 78.63 ± 0.46 65.99 ± 0.72 81.56 ± 0.53 72.00 ± 0.70 84.20 ± 0.50 41.10 ± 0.60 55.50 ± 0.60 CAN (Hou et al. 2019) ResNet-12 63.85 ± 0.48 79.44 ± 0.34 69.89 ± 0.51 84.23 ± 0.37 ± 0.64 73.31 ± 0.53 67.45 ± 0.70 82.88 ± 0.53 70.26 ± 0.70 83.82 ± 0.49 36.82 ± 0.51 49.72 ± 0.55 MetaBaseline (Chen et al. 2020) 1 ± 0.85 79.44 ± 0.65 67.72 ± 0.80 83.61 ± 0.62 72.36 ± 0.67 84.43 ± 0.50 40.23 ± 0.22 56.16 ± 0.56 Meta-UAFS ResNet-12 64.22 ± 0.67 79.99 ± 0.49 69.13 ± 0.84 84.33 ± 0.59 74.08 ± 0.72 85.92 ± 0.42 41.99 ± 0.58 57.43 ± 0.38</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell cols="2">mini-ImageNet</cell><cell cols="2">tiered-ImageNet</cell><cell cols="2">CIFAR-FS</cell><cell>FC-100</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>MatchingNet (Vinyals et al. 2016)</cell><cell>64-64-64-64</cell><cell>46.6</cell><cell>60.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MAML (Finn, Abbeel, and Levine 2017)</cell><cell>32-32-32-32</cell><cell cols="4">48.70 ± 1.84 63.11 ± 0.92 51.67 ± 1.81 70.30 ± 1.75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ProtoNet  † (Snell, Swersky, and Zemel 2017)</cell><cell>64-64-64-64</cell><cell cols="4">49.42 ± 0.78 68.20 ± 0.66 53.31 ± 0.89 72.69 ± 0.74</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RelationNet (Sung et al. 2018)</cell><cell cols="5">64-96-128-256 50.44 ± 0.82 65.32 ± 0.70 54.48 ± 0.93 71.32 ± 0.78</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TADAM (Oreshkin, López, and Lacoste 2018)</cell><cell>ResNet-12</cell><cell cols="2">58.50 ± 0.30 76.70 ± 0.30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">40.10 ± 0.40 56.10 ± 0.40</cell></row><row><cell>LEO  † (Rusu et al. 2019)</cell><cell>WRN-28-10</cell><cell cols="4">61.76 ± 0.08 77.59 ± 0.10 66.33 ± 0.05 81.44 ± 0.09</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TapNet (Yoon, Seo, and Moon 2019)</cell><cell>ResNet-12</cell><cell cols="4">61.65 ± 0.15 76.36 ± 0.10 63.08 ± 0.15 80.26 ± 0.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">Shot-free (-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline2020 (Dhillon et al. 2020)</cell><cell cols="5">WRN-28-10 56.17 ResNet-12 63.17 ± 0.23 79.26 ± 0.17 68.62 ± 0.27 83.29 ± 0.18</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Meta-Base</cell><cell>ResNet-12</cell><cell>63.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work was done when Zhizheng Zhang was an visiting student at Columbia University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Data augmentation generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Discriminative k-shot learning using probabilistic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Świ Ątkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00326</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08136</idno>
		<title level="m">Meta-learning with differentiable closed-form solvers</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">Data Uncertainty Learning in Face Recognition</title>
				<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<imprint>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04390</idno>
		<title level="m">A new meta-baseline for few-shot learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification</title>
		<imprint>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Few-Shot Semantic Segmentation with Prototype Learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>1126- 1135. JMLR. org</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-shot learning via covariance-preserving adversarial augmentation networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016. 2018</date>
			<biblScope unit="page" from="975" to="985" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>Uncertainty in deep learning</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08930</idno>
		<title level="m">Recasting gradient-based meta-learning as hierarchical bayes</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4003" to="4014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">All you need is a good representation: A multi-level and classifier-centric representation for few-shot learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12476</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RepMet: Representative-based metric learning for classification and fewshot object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<title level="m">Bayesian segnet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017. 2018</date>
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Metalearning with differentiable convex optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for few-shot learning by category traversal</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arulkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>On first-order metalearning algorithms</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Few-shot Learning with Embedded Class Models and Shot-Free Meta Training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incremental few-shot learning with attention attractor networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2019. 2018</date>
			<biblScope unit="page" from="5275" to="5285" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Metalearning for semi-supervised few-shot classification</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11539</idno>
		<title level="m">Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<title level="m">Instance Credibility Inference for Few-Shot Learning</title>
				<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Tapnet: Neural network augmented with task-adaptive projection for few-shot learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06549</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
