<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 4-6, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Peking University ‡ ShanghaiTech University Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenxiang</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Peking University ‡ ShanghaiTech University Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Zhiqiang Xie</orgName>
								<orgName type="institution">Peking University and Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ShanghaiTech University and Microsoft Research</orgName>
								<address>
									<settlement>Zhi Yang</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">November 4-6, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Performing Deep Neural Network (DNN) computation on hardware accelerators efficiently is challenging. Existing DNN frameworks and compilers often treat the DNN operators in a data flow graph (DFG) as opaque library functions and schedule them onto accelerators to be executed individually. They rely on another layer of scheduler, often implemented in hardware, to exploit the parallelism available in the operators. Such a two-layered approach incurs significant scheduling overhead and often cannot fully utilize the available hardware resources. In this paper, we propose RAM-MER, a DNN compiler design that optimizes the execution of DNN workloads on massively parallel accelerators. RAM-MER generates an efficient static spatio-temporal schedule for a DNN at compile time to minimize scheduling overhead. It maximizes hardware utilization by holistically exploiting parallelism through inter-and intra-operator co-scheduling. RAMMER achieves this by proposing several novel, hardware neutral, and clean abstractions for the computation tasks and the hardware accelerators. These abstractions expose a much richer scheduling space to RAMMER, which employs several heuristics to explore this space and finds efficient schedules. We implement RAMMER for multiple hardware backends such as NVIDIA GPUs, AMD GPUs, and Graphcore IPU. Experiments show RAMMER significantly outperforms stateof-the-art compilers such as TensorFlow XLA and TVM by up to 20.1×. It also outperforms TensorRT, a vendor optimized proprietary DNN inference library from NVIDIA, by up to 3.1×.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural network (DNN) is now a widely adopted approach for image classification, natural language processing, and many other AI tasks. Due to its importance, many computational devices, such as CPU, GPU, FPGA, and specially designed DNN accelerators have been leveraged to * Both authors contributed equally. perform DNN computation. Efficient DNN computation on these devices is an important topic that has attracted much research attention in recent years <ref type="bibr" target="#b18">[23,</ref><ref type="bibr" target="#b23">28,</ref><ref type="bibr" target="#b27">32,</ref><ref type="bibr" target="#b35">40,</ref><ref type="bibr" target="#b47">52]</ref>. One of the key factors that affect the efficiency of DNN computation is scheduling, i.e. deciding the order to perform various pieces of computation on the target hardware. The importance of scheduling in general is well known and has been thoroughly studied <ref type="bibr" target="#b15">[20,</ref><ref type="bibr" target="#b34">39]</ref>. However, there is little work discussing scheduling for DNN computation on hardware devices specifically.</p><p>The computational pattern of a deep neural network is usually modeled as a data flow graph (DFG), where each node corresponds to an operator, which represents a unit of computation such as matrix multiplication, while an edge depicts the dependency between operators. This representation naturally contains two levels of parallelism. The first level is the inter-operator parallelism, where operators that do not have dependencies in the DFG may run in parallel. The second level is the intra-operator parallelism, where an operator such as matrix multiplication has inherent internal data parallelism and can leverage hardware accelerators that can perform parallel computation, such as a GPU.</p><p>To exploit the two levels of parallelism, current practice adopts a two-layered scheduling approach. An inter-operator DFG layer scheduler takes the data flow graph and emits operators that are ready to be executed based on the dependencies. In addition, an intra-operator scheduler takes an operator and maps it to the parallel execution units in the accelerator. This layering design has a fundamental impact on the system architectures of the existing DNN tool sets. For example, the DFG layer scheduler is typically implemented in deep learning frameworks such as TensorFlow <ref type="bibr" target="#b13">[18]</ref> or ONNX Runtime <ref type="bibr" target="#b11">[14]</ref>. The operator layer scheduler, on the other hand, is often hidden behind the operator libraries such as cuDNN <ref type="bibr" target="#b9">[12]</ref> and MKL-DNN <ref type="bibr" target="#b6">[9]</ref>, and sometimes implemented directly in hardware, as is the case for GPUs.</p><p>While widely adopted by existing frameworks and accelerators, such a two-layer scheduling approach incurs fundamental performance limitations. The approach works well only USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 881 when the overhead of emitting operators is largely negligible compared to the execution time of operators, and when there is sufficient intra-operator parallelism to saturate all processing units in an accelerator. This unfortunately is often not the case in practice. DNN accelerators keep on increasing performance at a much faster pace than CPUs, thus making the operator emitting overhead more and more pronounced. This is exacerbated for DNN inference workloads when the batch size is small, which limits the intra-operator parallelism. Moreover, the two-layer scheduling approach overlooks the subtle interplay between the upper and lower layers: to optimize the overall performance, a system could reduce the degree of intra-operator parallelism in order to increase the level of inter-operator parallelism ( § 2).</p><p>To mitigate these limitations, we present RAMMER, a deep learning compiler that takes a holistic approach to manage the parallelism available in the DNN computation for scheduling. It unifies the inter-and intra-operator scheduling through a novel abstraction called rTask. rTask enables the scheduler to break the operator boundary and allows fine-grained scheduling of computation onto devices. Instead of the existing design that breaks scheduling into two pieces managed by software and hardware separately, RAMMER is a unified software-only solution, which makes it less dependent on underlying hardware and thus can be adopted by diverse DNN accelerators. In RAMMER, we make the following design decisions.</p><p>First, to exploit the intra-operator parallelism through a software compiler, RAMMER redefines a DNN operator as an rTask-operator or rOperator. An rOperator consists of multiple independent, homogeneous rTasks, each is a minimum schedulable unit runs on a single execution unit of an accelerator (e.g., a streaming multiprocessor SM in a GPU). Thus, rTask as the fine-grained intra-operator information is exposed to the RAMMER scheduler. RAMMER treats a DNN as a data flow graph of rOperator nodes, hence it can still see the coarse-grained inter-operator (DFG) dependencies.</p><p>Unfortunately, certain modern accelerators such as GPU do not expose interfaces for intra-operator (i.e., rTask) scheduling. To address this challenge, as a second design decision RAMMER abstracts a hardware accelerator as a virtualized parallel device (vDevice), which contains multiple virtualized execution units (vEU). The vDevice allows several rTasks, even from different operators, to run on a specified vEU in a desired order. Moreover, a vEU can run a barrier rTask that waits for the completion of a specified set of rTasks, thus ensuring the correct execution of rTasks from dependent operators. The vDevice maps a vEU to one of the physical execution units in an accelerator to perform the actual computation of rTasks.</p><p>Finally, fine-grained scheduling could incur significant runtime overheads, even more so than the operator scheduling overhead discussed previously. To address this issue, RAM-MER moves the scheduling decision from runtime to compile time. This is driven by the observation that most DNN's DFG is available at the compile time, and the operators usually exhibit deterministic performance characteristics. Therefore, the runtime performance can be obtained through compile time profiling <ref type="bibr" target="#b40">[45]</ref>. This not only avoids unnecessary runtime overheads, but also allows a more costly scheduling policy to fully exploit the inter-and intra-operator parallelism together.</p><p>RAMMER is compatible with optimizations developed in existing DNN compilers. RAMMER can import a data-flow graph from other frameworks like TensorFlow. Such a DFG can be optimized with techniques employed by a traditional graph optimizer such as <ref type="bibr" target="#b13">[18]</ref>. An rOperator can also be optimized by an existing kernel tuner <ref type="bibr" target="#b18">[23]</ref>. Our experience shows that, on top of existing optimizations, RAMMER can provide significant additional performance improvement, especially for DNN inference workloads.</p><p>RAMMER is hardware neutral. The abstractions proposed, such as rTask, rOperator and vEU are applicable to any massively parallel computational devices with homogeneous execution units. This includes almost all the computational devices proposed for DNN workloads. In this paper, in addition to describe in detail how RAMMER is implemented on NVIDIA GPUs, we will also discuss our experience retargeting RAMMER for several alternative computing devices.</p><p>We have implemented RAMMER with 52k lines of C++ code and open-sourced the code<ref type="foot" target="#foot_0">1</ref> . Our evaluation on 6 DNN models shows that RAMMER significantly outperforms stateof-the-art compilers like XLA and TVM on both NVIDIA and AMD GPUs, with up to 20.1× speedup. RAMMER even outperforms TensorRT <ref type="bibr" target="#b10">[13]</ref>, a vendor optimized DNN inference library from NVIDIA, with up to 3.1× gain.</p><p>Our experience on RAMMER strongly suggests that the current industry-prevalent practice of vendor supplying highly optimized DNN operator implementations in a library form (such as cuDNN and MKL-DNN) is sub-optimal. This practice will incur significant efficiency cost for DNN workloads. The situation will become even worse in the coming years as modern accelerators keep on increasing the available hardware parallelism while new DNN architectures strive to save computation by replacing larger operators with many smaller ones <ref type="bibr" target="#b44">[49,</ref><ref type="bibr" target="#b49">54]</ref>. We recommend vendors to supply optimized implementations in other forms, such as our proposed rOperator and vEU abstractions, in order to enable holistic optimization that can fully utilize hardware resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>In this section, we highlight some results to illustrate the limitation of the two-layer design of existing deep learning frameworks. Without loss of generality, we experiment with TensorFlow <ref type="bibr" target="#b13">[18]</ref>, a state-of-the-art DNN framework, on an NVIDIA GPU, using the same settings as in §5.     Hardware-managed intra-operator scheduling leads to low GPU utilization. The two-layer design delegates the intra-operator scheduling to the hardware scheduler in an accelerator like GPU. Figure <ref type="figure" target="#fig_1">1</ref> shows that such an approach could lead to low GPU utilization across different DNN models. When the batch size is 1, the GPU utilization could be as low as 2% for Seq2Seq model. Even when the batch size is increased to 16, the average GPU utilization across the 6 models is merely 40%<ref type="foot" target="#foot_1">2</ref> . To improve the scheduling efficiency, modern GPUs support the multi-streaming mechanism that allows independent operators to run concurrently. However, our measurement in §5 shows that multi-streaming often hurts rather than improves the overall performance.</p><p>High inter-operator scheduling overheads. The twolayer approach also incurs a higher inter-operator scheduling overheads. Here, we regard the time not spent doing actual computation in the GPU as the overhead for inter-operator scheduling. This overhead includes various operations to support operator emitting, including kernel launching, context initialization, communication between host and GPU, and so on. The percentage shown above each bar in Figure <ref type="figure" target="#fig_2">2</ref> depicts how much time the DNN model is not spent in the actual GPU computation. From the figure it is clear that the overhead of inter-operator scheduling is quite significant. When batch size is 1, the average overhead is 55% across the 6 DNN Increasing the batch size to 16 slightly improves the situation, while the overhead is still not negligible (between 16% and 55%). Modern DNN compilers, including the one in TensorFlow, employs a technique called kernel fusion <ref type="bibr" target="#b12">[17,</ref><ref type="bibr" target="#b18">23]</ref>, which merges several DNN operators into a single one when allowed. However, our results in §5 show that this technique cannot reduce the overhead significantly.</p><p>Interplay between inter-and intra-operator scheduling.</p><p>Separating scheduling into two layers ignores the subtle interplay between inter-operator and intra-operator scheduling, which may lead to suboptimal performance. For example, Figure <ref type="figure">3</ref>(a) shows two independent operators being scheduled to a GPU. For operator 0, to maximize its performance, the system may choose the fastest implementation with a high degree of parallelism. Thus operator 0 could greedily span all the parallel execution units (EUs) of an accelerator ( in this case the streaming multiprocessors of the GPU), while each EU may not be fully utilized. Since operator 0 occupies all the EUs, operator 1 has to wait for available resource. A better scheduler could reduce the degree of parallelism of operator 0 to increase the level of inter-operator parallelism, by mapping operator 1 alongside operator 0, as illustrated by Figure <ref type="figure">3</ref>(b). We will discuss more details of this issue in §3.3 and §5.</p><p>Opportunities. Given the fundamental limitations of the two-layer design observed above, it is desirable to manage the scheduling of inter and intra-operator together. However, a naive implementation of this approach may incur even higher overheads than the already significant inter-operator scheduling overheads. the compile time, and the operators often exhibit deterministic performance, therefore, their execution times can be obtained through compile time profiling <ref type="bibr" target="#b40">[45]</ref>. For example, Figure <ref type="figure">4</ref> shows the averaged GPU kernel time and the variance of all the operators in the ResNeXt <ref type="bibr" target="#b44">[49]</ref> model. The kernel run-time weighted average of standard deviations among all operators is only 7%. This allows us to move the scheduling from runtime to compile-time, by generating an offline schedule plan to reduce runtime overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RAMMER's Design</head><p>The observations in §2 motivate RAMMER, a DNN compiler framework that manages both inter and intra-operator scheduling. Figure <ref type="figure">5</ref> shows the key differences between an existing deep learning framework and RAMMER. First, the input to RAMMER is a data-flow graph where a node is an rOperator, rather than a traditional operator. An rOperator explicitly exposes rTask, a fine-grained computation unit that could run on a parallel execution unit in an accelerator. We discuss details of rTask in §3.1. Second, instead of separating the two-layer scheduling between software and hardware, RAMMER introduces rTask-aware DFG compiler to manage the inter and intra-operator scheduling in one place. The rTask-aware DFG compiler will generate a static execution plan for runtime execution. Often, it is not efficient or not possible to pack the entire DNN computation in a single accelerator device invocation. Therefore, the execution plan is breaking into  multiple rPrograms, each contains a piece of computation to be carried out on the hardware. Instead of emitting one operator at a time for an accelerator, RAMMER emits an rProgram at a time. The details of the rTask-aware DFG compiler will be discussed in §3.3. To carry out the execution plan, RAMMER abstracts a hardware accelerator as a virtualized parallel device (vDevice), which includes multiple virtualized execution units (vEUs). The vDevice provides the scheduling and synchronization capabilities at the rTask level so that the rProgram can be mapped to the corresponding vEUs at compile time. The vEUs, together with the vDevice will be mapped to the hardware at runtime. We introduce virtualized device in §3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">rOperator</head><p>An rOperator is defined as a group of independent, homogeneous rTasks (short for RAMMER task), where an rTask is the minimum computation unit in an operator to be executed on a processing element of the accelerator device. The concept of rTask naturally aligns with the parallel architecture of DNN accelerators, e.g., the SIMD architecture of GPU. To maximize efficiency, the computation on such an accelerator needs to be divided into multiple parallel (homogeneous) tasks. Each of these parallel tasks can be represented by an rTask, thereby exposing the intra-operator parallelism not only to the underlying hardware, but to the RAMMER compiler. Given that an rTask is logically identical to a parallel task, RAMMER relies on external tools to partition an rOperator into rTasks (e.g., TVM <ref type="bibr" target="#b18">[23]</ref>). In another word, RAMMER uses external heuristics to decide a reasonable granularity of rTask.</p><p>As a concrete example, a matrix multiplication operator can be divided into multiple homogeneous rTasks, each computes a tile of the output matrix, while the tiling strategy is assumed to be given. If a complicated DNN operator can hardly be divided into independent homogeneous rTasks (e.g., Separa-bleConv2D <ref type="bibr" target="#b4">[7]</ref>), it can be represented as multiple dependent rOperators, each can be partitioned into rTasks.</p><p>An rTask is indexed by a logical rtask_id. The rTasks in an rOperator are numbered continuously. To execute an rTask, the parallel execution unit could call the compute_rtask() interface (line 3 Figure <ref type="figure" target="#fig_4">6</ref>). To generate an rProgram, RAMMER needs to know the total number of rTasks in an operator. This is available through the interface get_total_rtask_num(). In contrast, a traditional opera-tor has only one interface compute()(line 1 Figure <ref type="figure" target="#fig_4">6</ref>). The implementation of an rOperator is called rKernel, which realizes the concrete rTask computation logics and decides the total number of rTasks. One rOperator might have multiple versions of rKernels based on different tiling strategies, e.g., trading off between resource efficiency and overall execution time.</p><p>The rOperator abstraction allows RAMMER to expose both inter-and intra-operator parallelisms. This opens up a new space to optimize DNN computation holistically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Virtualized Parallel Device</head><p>Modern accelerators do not provide interfaces to map an rTask to a desired execution unit directly. For example, a GPU only allows to execute one operator (in the form of a kernel) at a time. To address this challenge, RAMMER abstracts a hardware accelerator as a software-managed virtual device called virtualized parallel device (vDevice). A vDevice further presents multiple parallel virtual execution units (vEUs), each of them can execute rTasks independently.</p><p>With vDevice, RAMMER organizes the computation of rTask-aware DFG as an rProgram on a vDevice. An rProgram is represented as two dimensional array of rTask prog[vEU_id][order], where vEU_id denotes the vEU the rTask is assigned to, and order denotes the execution order of the rTask in this vEU. For example, prog[0][0] denotes the first rTask to be executed in vEU 0. To ensure the correct execution of dependent rTasks in a plan, RAMMER introduces barrier-rTask. A barrier-rTask takes the argument of a list of pairs &lt;vEU_id, order&gt;. The barrier-rTask will wait until the completion of all rTasks indexed by each pair. The barrier-rTask provides a fine-grained synchronization mechanism to enable rTask schedule plan execution.</p><p>For the execution of DNN computation, a vDevice needs to be mapped to a physical accelerator at runtime. We will discuss how RAMMER implements the mapping of vDevice to different hardware accelerators in §4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">rTask-aware DFG Compiler</head><p>The rTask abstraction and the fine-grained rTask execution capability exposed by the vDevice open up a large optimization space. RAMMER aims to generate a high-quality schedule in this space, represented as a sequence of rPrograms. To this end, the rTask-aware DFG compiler separates the scheduling mechanism from its policy. On the mechanism side, it provides two capabilities: (1) Two scheduling interfaces for a policy to generate an execution plan. (2) A profiler to supply profiling information requested by a scheduling policy. an operator to the specified vEU in a sequential order. Here task_uid is a global identifier for an rTask, which is essentially the operator id combined with the rtask_id within the operator. The second API, namely Wait(wtask_uid, list&lt;task_uid&gt;), allows an rTask specified by wtask_uid to wait for rTasks in list&lt;task_uid&gt;. The Wait interface will implicitly Append a barrier-rTask (discussed in §3.2) right before the rTask wtask_uid. As an optimization, when waiting for multiple consecutive rTasks r 1 , r 2 , ..., r n sequentially appended to the same vEU, the rTask only need to include the last one, i.e., r n , in the waiting list.</p><p>Compile-time profiling. RAMMER profiler provides the following three types of information: 1) individual rTask execution time on a vEU; 2) resource usage of an rTask such as the local memory or registers used and 3) the overall execution time of an rProgram. This profiling information can guide a policy to generate an efficient scheduling plan.</p><p>Scheduling policy. Algorithm 1 illustrates how to use the above scheduling interfaces and the profiler to implement a scheduling policy to exploit both inter-and intra-operator parallelisms. This policy takes an rTask-aware DFG and schedules operators in waves <ref type="bibr" target="#b32">[37]</ref>. The operators in a wave are the fringe nodes of a breadth-first-search on the DFG. The policy will include a wave's operators in the current rProgram if the profiling results (denoted by time()) suggest it will reduce the total execution time. Otherwise, the policy will create a separate rProgram (line 2-10).</p><p>First of all, we assume that each rOperator has one or more implementations called rKernels, each rKernel is a way to break the operator into rTasks with different resource and runtime trade-offs. Among the rKernels of a particular rOperator, there is the fastest one with the smallest runtime, and there is the most efficient one with the smallest product of runtime and the total number of rTasks.</p><p>For each wave, the policy selects the implementations of the operators through SelectRKernels() (line 13), with the following heuristics: If combine all the rTasks in the wave with the fastest operator implementations still cannot occupy all the parallel execution units in the accelerator, the policy will just select them. Otherwise, the policy will find the most efficient rKernels and then perform a profiling. The policy will choose these rKernels if the profiling results show better execution time, otherwise it will stick with the fastest rKernels. This heuristic considers the interplay between the inter-and intra-operator scheduling by evaluating the rOperators (and their rTasks) in a wave, instead of individually. After the rKernel selection, the policy calls SelectvEU() to decide which vEU an rTask should be scheduled to (line 16). Given the current rProgram P, SelectvEU() chooses the vEU that can execute the rTask at the earliest, based on the profiled execution time of each rTask in P. Finally, the policy calls Wait() to ensure rTask level dependency (derived from the DFG) and Append() to assign the rTask to the selected vEU (line <ref type="bibr" target="#b12">[17]</ref><ref type="bibr" target="#b13">[18]</ref>. The policy in Algorithm 1 demonstrates how RAMMER separates the scheduling mechanism from scheduling policy. As shown in §5, this simple policy can already outperform the state-of-the-art, sometimes significantly. We envision the proposed scheduling mechanism could enable future research on more advanced scheduling policies to further explore the optimization space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>We implement RAMMER with 52k lines of C++ code, including 3k lines of code for the core compiler and scheduling function. The input of RAMMER is a DNN model in either TensorFlow <ref type="bibr" target="#b13">[18]</ref> frozen graph, TorchScript [16] or ONNX <ref type="bibr" target="#b11">[14]</ref> format. RAMMER first converts the input model into a DFG of rOperators. Since the input model is often not optimized, like other compilers, we also implemented common graph optimizations such as constant folding, common sub-expression elimination, pattern-based kernel fusion, etc. For each rOperator from an optimized DFG, RAMMER loads one or multiple versions of rKernel implementations from dif-  ferent sources, e.g., auto-kernel generators <ref type="bibr" target="#b18">[23]</ref>, hand-tuned kernels, or converted from existing operators in other frameworks. RAMMER compiler will then partition the DFG into sub-graphs (e.g., based on the policy in Algorithm 1) and compile each of them as an rProgram. As an output, each rProgram is further generated as a device code (e.g., GPU kernels) that runs on the accelerator. Figure <ref type="figure" target="#fig_6">7</ref> summarizes the overall workflow of RAMMER.</p><p>In the rest of this section, we describe the details about RAMMER's implementation for CUDA GPU. We focus on NVIDIA GPUs and the CUDA eco-system because they are the most widely used accelerators for DNN. To demonstrate that the vDevice abstraction enables RAMMER compiler to support different accelerators with an uniform interface, we will also briefly describe our experience with other DNN accelerators, including AMD GPUs and Graphcore IPU, at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RAMMER on NVIDIA CUDA GPUs</head><p>An NVIDIA GPU usually consists of tens to hundreds of streaming multiprocessors (SM), each containing tens of cores. Computation on SM follows the Single Instruction Multiple Thread (SIMT) model. In this paper we assume the readers are familiar with the basic concepts of CUDA <ref type="bibr" target="#b1">[4]</ref>, the programming paradigm introduced by NVIDIA to program their GPUs. A single CUDA program (often referred to as a CUDA kernel) groups multiple threads into blocks, each thread-block is assigned to run on an SM, where the scheduling is performed by GPU hardware. RAMMER naturally maps each vEU to an SM and implements an rTask as a thread-block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">rOperator in CUDA</head><p>Figure <ref type="figure" target="#fig_7">8</ref> shows a naive CUDA implementation of an rOperator that multiplies a M × K matrix A by a K × N matrix B. For simplicity, we assume M and N are evenly divisible by 32. In the code, each rTask computes a 32 × 32 tile of the output matrix C. Line 1-10 in Figure <ref type="figure" target="#fig_7">8</ref> shows the computation of one thread in one rTask. The thread uses rtask_id, a RAMMER assigned id to identify the tile to be computed by this rTask (line 3-4), and uses threadIdx, a CUDA builtin thread index to identify the data element to be computed (line 5-6) by this thread. The identified element is then computed in line 7-9. Line 13 shows the interface exposed by this rOperator, which will be called by a vEU's parallel thread. The total rTasks needed in this operator is determined by the matrix dimension M and N, and can be obtained through the get_total_rtask_num interface (line 16). The key difference between code in Figure <ref type="figure" target="#fig_7">8</ref> and a traditional CUDA code is that an rTask uses rtask_id, a logical index controlled by RAMMER, instead of blockIdx, a built-in thread-block index controlled by the GPU's hardware scheduler. This enables RAMMER to map an rTask to a desired vEU by executing compute_rtask() with a proper rtask_id. Note that the code shown in Figure <ref type="figure" target="#fig_7">8</ref> is for illustrative purpose. The evaluation shown in §5 uses a more complicated tiled version of matrix multiplication rOperator, which further improves the performance through carefully exploiting GPU memory hierarchy, e.g., shared memory and registers <ref type="bibr" target="#b31">[36,</ref><ref type="bibr" target="#b36">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">vDevice and vEU on CUDA GPU</head><p>On a CUDA GPU, the intra-operator scheduling is usually managed by the GPU's built-in scheduler. To bypass the built-in scheduler, RAMMER leverages a persistent threadblock (PTB) <ref type="bibr" target="#b24">[29]</ref> to implement a vEU in a vDevice. PTB is a thread-block containing a group of continuously running threads, where RAMMER is able to "pin" the PTB to the desired SM. Given an rProgram, each thread in the PTB (and hence the vEU) executes the compute_rtask() according to the sequence specified by the rProgram. To execute the compute_rtask() from multiple rTasks continuously in a PTB, a function qualifier __device__ is required by CUDA for comptue_rtask() and any sub functions executed therein (e.g., line 1 and 13 in Figure <ref type="figure" target="#fig_7">8</ref>).</p><p>Figure <ref type="figure" target="#fig_8">9</ref> illustrates the CUDA code for a vDevice with two vEUs, i.e., a CUDA kernel function with two PTBs. This vDevice executes an rProgram compiled from a DFG with three rOperators: a Matmul, a Relu, and a Conv. Specified by the execution plan, the vDevice executes two rTasks of the Matmul operator on vEU 0, and in parallel it also runs four rTasks of the Relu operator on vEU 1. Then a global barrier is inserted to the two vEUs, each runs a barrier-rTask: vEU 0 waits for the 4th rTask on vEU 1, and vEU 1 waits for the 2nd rTask on vEU 0. Finally, the vDevice executes two rTasks of the Conv operator on the two vEUs respectively. On each vEU, RAMMER runs the rTasks sequentially in a code branch, executed only if the current vEU Id matches the one generated by the rProgram.</p><p>Before a lengthy DNN computation, RAMMER dispatches  each vEU (implemented by a PTB) to a desired SM through the GPU scheduler <ref type="bibr" target="#b43">[48]</ref>. To improve hardware utilization, an SM can run multiple vEUs (PTBs) concurrently. Since CUDA uses a SIMT model, all vEU are homogeneous, the number of vEUs an SM can support depends on the most demanding rTask across all the vEUs, i.e., the rTask that requires the most thread number, register number, shared memory size, etc. In practice, we set the number of vEUs on each SM according to the maximum active PTB number provided by the CUDA compiler nvcc <ref type="bibr" target="#b2">[5]</ref>. With the vDevice abstraction, the optimizations in RAMMER become hardware agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Executing rTask on vEU in CUDA</head><p>Executing heterogeneous rTasks. In a CUDA kernel, the number of threads in a thread block is fixed in the entire execution lifecycle. This force RAMMER to require that all the rTasks on a vEU to run on with the same number of persistent threads. In practice, different rOperators may use different number of threads to balance parallelism and perthread resource usage. To address this problem, RAMMER sets the number of threads of a vEU to be the maximum number of threads used by an rTask in the vEU. For an rTask with less threads, RAMMER inserts early-exit logic in the extra threads to skip the unnecessary (and invalid) execution. However, early-exit may lead to dead-lock: a global barrier might never return because early-exit logic may skip the barrier. To avoid this issue, RAMMER can leverage the CUDA cooperative group primitives <ref type="bibr" target="#b0">[3]</ref>, which explicitly controls the scope of threads during a synchronization.</p><p>Implementing barrier-rTask. To implement an efficient barrier-rTask, RAMMER introduces a step array, where each element is an integer tracking the number of finished rTasks in each vEU. When finished, an rTask will use its first thread to increase the corresponding element in the step array by 1. When waiting for a list of rTasks on N vEUs, a barrier-rTask uses its first N threads to poll on the corresponding elements in the step array until the steps are larger than the orders of those rTasks. After that, the barrier-rTask calls __syncthreads to ensure all threads in this vEU are ready to run the next rTask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Transforming Legacy CUDA Operators</head><p>Many operators for DNN are already available as CUDA kernel code. To reduce development efforts, RAMMER introduces a source-to-source converter to transform a legacy CUDA operator into an rOperator. The key insight of the converter lies on the facts that to exploit the intra-operator parallelism, legacy CUDA operators are also implemented as threadblocks, although they use blockIdx and let CUDA GPU hardware control the intra-operator scheduling directly. rOperator can just compute the desired blockIdx from rtask_id without changing computation logic in the legacy kernel.</p><p>One challenge in this transformation is that the threadblocks in existing operator could be laid out in 1, 2, or 3dimensional shape, while in a vEU threads are laid out in a 1-dimensional shape. This means our vEU needs to support rTask with different threads shapes. For example, Figure <ref type="figure" target="#fig_9">10</ref> illustrates a vEU executing two rTasks with the thread shapes of [2 × 2] and [2 × 3] respectively. Our solution is to stick to a 1-D persistent thread shape for a vEU, and apply a thread index remapping to compute the desired threadIdx in the legacy kernel with the vEU's 1-D threadIdx. Notice that, as discussed before, the number of threads of a vEU is the maximum number of threads of all rTask in the vEU, so that such a remapping is always possible. For example, in Figure <ref type="figure" target="#fig_9">10</ref> we configure the vEU with [1 × 6] persistent threads. When executing rTask 0 with a legacy [2 × 2] thread shape, RAMMER remaps the [2 × 2] shape to the vEU's [1 × 6] thread.</p><p>In summary, to convert a legacy DNN operator to an rOperator, one needs to remap thread and block index, implement the early-exit logic, and use CUDA cooperative group primitive to support local barrier on the active (i.e. not earlyexited) threads. RAMMER implements these changes by inserting a compiler-generated code segment at the entry point of the legacy operator kernel code. With these modifications, RAMMER can preserve the legacy operator implementation, and reuse it as an rTask operator. In RAMMER, we have transformed and implemented total 150 rKernels for 70 rOperators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RAMMER on Other Accelerators</head><p>The design of RAMMER is not limited to CUDA and NVIDIA GPUs. In fact, our rTask, rOperator and vEU abstractions are applicable to any massively parallel computational devices with homogeneous execution units, including most of the devices that used for DNN computation. In this section, we discuss how to port RAMMER to support other devices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">RAMMER on AMD GPUs</head><p>AMD GPUs are similar to NVIDIA GPUs, which also consist of many parallel execution units called compute units (CU). AMD GPU has a HIP programming model <ref type="bibr" target="#b5">[8]</ref>, which is similar to CUDA. AMD provides a hipify tool that can convert a CUDA kernel to a HIP kernel. hipify can help convert most CUDA rOperators to the HIP version. Some CUDA kernel configurations, such as the number of threads per thread-block and size of local memory, are not optimized for AMD GPUs due to the minor architecture differences. We re-implemented 41 rKernels for AMD GPUs for better performance. hipify can also convert the CUDA implementation of vDevice (i.e., PTBs) to the HIP version. The only exception is that AMD GPUs do not support cooperative group primitives. To address this issue, we introduce a new API in rOperator to provide the number of (block-wise) synchronizations S (i.e. calls to __syncthreads). For early-exit threads, instead of exit immediately, RAMMER will insert code to call the __syncthreads primitive S times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">RAMMER on Graphcore IPU</head><p>The Graphcore IPU (Intelligence Processing Unit) <ref type="bibr" target="#b7">[10]</ref> is a state-of-the-art DNN accelerator with an architecture quite different from GPUs. IPU is a massively parallel MIMD processor with a bulk-synchronous-parallel (BSP) communication model. Each IPU contains 1,216 parallel processing units called tiles; a tile consists of a hyper-threaded computing core plus 256 KB of local memory. DNN computation on an IPU is explicitly programmed as a data-flow graph, where each vertex implements the code executed on a tile and each edge depicts the data transfers between vertices. The IPU compiler is responsible for mapping each vertex to a tile.</p><p>RAMMER's rTask abstraction can also map to IPU's MIMD model: a vEU can map to a tile and a vertex can be treated as an rTask. Thus, an rOperator on IPU can be implemented as a set of vertices. More importantly, IPU compiler allows to control the vertex-tile mapping at compile-time. This provides the core functionality required in vDevice abstraction. Restricted by the hardware BSP model, IPU does not provide a finegrained synchronization mechanism. We therefore implement barrier-rTask with a global barrier, which may reduce scheduling space for RAMMER. Even with this limitation, RAMMER still can schedule rTasks of different operators at the same computing step to increase utilization. To evaluate RAMMER, we implemented total 15 rOperators and 18 rKernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">RAMMER on x86 CPUs</head><p>We also implemented RAMMER on multi-core x86 CPUs. However, we see little performance benefit of adopting the RAMMER abstractions on x86-based platforms. On x86, the operator runtime is high due to the relatively low performance of x86 cores for numerical computations, and the small number of cores can be fully occupied by almost any DNN operators. Moreover, scheduling overhead is not significant because kernel launch is just a regular function call. Therefore, RAM-MER cannot provide additional benefit compared with the traditional two-layered scheduling approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we present the detailed evaluation results to demonstrate the effectiveness of RAMMER with comparison with other state-of-the-art frameworks. We compare RAMMER with other DNN frameworks and compilers, including TensorFlow (v1.15.2) representing the state-of-the-art DNN framework, TVM (v0.7) <ref type="bibr" target="#b18">[23]</ref> and TensorFlow-XLA representing the state-of-the-art DNN compilers, and TensorRT (v7.0) (with TensorFlow integration version), a vendor-specific inference library for NVIDIA GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Benchmarks and datasets. Our evaluation is performed using a set of representative DNN models that covers typical deep neural architectures such as CNN and RNN; and different application domains including image, NLP and speech. Among them, ResNeXt <ref type="bibr" target="#b44">[49]</ref> is an improved version of ResNet <ref type="bibr" target="#b25">[30]</ref>; NASNet <ref type="bibr" target="#b49">[54]</ref> is a state-of-the-art CNN model obtained by the neural architecture search; AlexNet <ref type="bibr" target="#b30">[35]</ref> represents a classic CNN model with a simple architecture. LSTM-TC <ref type="bibr" target="#b26">[31]</ref> is an RNN model for text classification; Deep-Speech2 <ref type="bibr" target="#b14">[19]</ref> is a representative speech recognition model; and Seq2Seq <ref type="bibr" target="#b41">[46]</ref> is for neural machine translation. All the implementations of these benchmarks, including the rKernels used in each model, are available in our artifact evaluation repository <ref type="foot" target="#foot_2">3</ref> .</p><p>We focus our evaluation on model inference. There is no fundamental reason limiting RAMMER from model training, except that supporting training requires us to develop more operators. We evaluate these models on a set of datasets including CIFAR-10 [2], ImageNet <ref type="bibr" target="#b21">[26]</ref>, LibriSpeech <ref type="bibr" target="#b8">[11]</ref> and synthetic datasets. Table <ref type="table" target="#tab_6">1</ref> lists the models, hyper-parameters, and the corresponding datasets used. All performance numbers in our experiments are averages over 1,000 runs; in all cases we observed very little variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on CUDA GPUs</head><p>This section answers the following questions: 1) How does RAMMER perform comparing with the state-of-the-art DNN frameworks or compilers? 2) How well does RAMMER utilize the GPU's parallel resource? 3) How much does RAMMER reduce the runtime scheduling overhead? 4) How much performance gain comes from RAMMER's scheduling leveraging both the intra and inter operator parallelism? 5) How effective is the fine-grained synchronization in improving the overall performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">End-to-end Performance</head><p>We first demonstrate the end-to-end efficiency of RAMMER by comparing with TensorFlow (TF), TensorFlow-XLA (TF- XLA), TVM and TensorRT (TF-TRT). To show the benefit of the abstractions introduced in RAMMER, we create a baseline version of RAMMER (called RAMMERBASE), which only implements the optimizations similar to those in existing compilers and still uses a two-layered scheduling approach. Thus, RAMMERBASE can be treated as just another regular DNN compiler implemented in the same codebase of RAMMER. Figure <ref type="figure" target="#fig_10">11</ref> shows the execution time of the benchmarks with batch size of 1. First, RAMMER significantly outperforms TF by 14.29× on average, and up to 33.94× for the LSTM-TC model. The performance improvement of RAMMER against TF is mainly because TF suffers from heavy runtime scheduling overhead at DFG level, especially when the individual operator's execution time is relatively short, as is the case in small batch inference. TF-XLA, as a DNN compiler, can improve TF's performance through DFG level optimizations (e.g., operator fusion) and operator-level code specializations (e.g. customized kernel generation). However, it still cannot fully avoid scheduling overhead, which leads to an average of 11.25× (up to 20.12×) performance gap compared to RAMMER. We observed that TF-XLA incurs even higher overhead for some CNN models such as ResNeXt and NASNet compared with TF. TVM, as another state-of-the-art DNN compiler, mainly leverages a kernel tuning technique to generate a specialized kernel for each operator. In our evaluation, TVM tunes 1,000 steps and chooses the fastest kernel for each operator. With such specialized optimization, TVM can improve the performance significantly compared with TF and TF-XLA. Still, RAMMER can outperform TVM by 3.48× on average and up to 6.46×. Even though TVM can make individual operator run faster through tuning, it still lacks the capability to leverage the fine-grained parallelism as RAMMER. An exception is that, for AlexNet, RAMMER can only achieve comparable performance with TVM. This is mainly because AlexNet, being one of the earliest modern DNN models, can be easily optimized due to its simple sequential model architecture and relatively fewer, but larger operators. Finally, TensorRT is a specialized DNN inference library with highly optimized operators provided by NVIDIA. We use its official TensorFlow-integration version (TF-TRT) to compile and run our models, as its stand-alone version fails to directly compile these benchmarks. However, for RNN models like DeepSpeech2, LSTM-TC and Seq2Seq-NMT, TF-TRT failed  to produce results after compiling for over 50 hours. Thus, we reimplemented these three models with the TensorRT native APIs. Our evaluation shows that RAMMER can outperform the vendor optimized TensorRT on all the benchmarks, with an averaged 2.18× and up to 3.09× lower latency. Finally, compared to RAMMERBASE, RAMMER can further improve the end-to-end performance by 2.59× and up to 6.29×.</p><p>Performance with different batch sizes. We also evaluate RAMMER's performance with larger batch sizes. Figure <ref type="figure" target="#fig_12">12</ref> shows the performance comparison on two representative CNN and RNN models, i.e., ResNeXt and LSTM-TC, with batch sizes of 4 and 16. We limit our benchmarks in this test due to the cost of developing optimized rOperator kernels for RAMMER: we have to hunt for efficient open-sourced operator kernel implementations or perform tuning by hand or through automatic tuning tools, which is time consuming. As it shows, using larger batch sizes can reduce scheduling overhead in existing frameworks due to the increased per-operator execution time. Even so, RAMMER can still outperform all the systems except for TensorRT on the ResNeXt model with batch size of 16. For this case, TensorRT uses some operators whose source codes are not publicly available, and our implementations do not yet match their performance. In fact, implementing operators to match the performance of close-sourced kernels is one of the major challenges for RAMMER. Compared to the other open source frameworks and compilers, RAMMER has a significant gain. For example, when using batch size of 16, RAMMER can outperform TF by 2.25×, and TVM by 1.25× on ResNeXt. For the LSTM-TC model, RAMMER can get 20.08× and 9.0× performance gains compared with TF and TVM respectively.   Performance with larger input sizes. In our default settings, ResNeXt and NASNet are evaluated on images of 32×32 size in the CIFAR-10 dataset. To show RAMMER's performance on larger images, we also evaluate these two models on the ImageNet dataset with the same model hyperparameters in their original papers <ref type="bibr" target="#b44">[49,</ref><ref type="bibr" target="#b49">54]</ref>. Specifically, ResNeXt on ImageNet uses 101 layers with cardinality of 64 and bottleneck width of 4d; and for NASNet, the number of repeated cells is 4 and the number of filters is 1056.</p><p>Figure <ref type="figure" target="#fig_14">13</ref> shows the end-to-end model inference time. From the results, we observe that using larger input size has little impacts on RAMMER's performance gain. For example, using ImageNet, RAMMER can still outperform TF by 18.91×, TVM by 4.96×, and even TF-TRT by 2.06× on ResNeXt. For the NASNet model, RAMMER can also get 6.99×, 1.33× and 2.34× performance gains compared with TF, TVM and TF-TRT respectively. The significant performance improvement is mainly because that the model structure for larger dataset usually have more inter-operator parallelism that can be better leveraged by RAMMER's optimization. For example, the cardinality for ResNeXt is increased from 16 to 64 when replacing the dataset from CIFAR-10 to ImageNet. Note that in the above evaluations, RAMMERBASE can already get a comparable or even better performance than compilers like TF-XLA and TVM. Thus, we will use RAM-MERBASE as the baseline of the state-of-the-art compiler and TF-TRT as the state-of-the-art DNN inference library to evaluate the benefits of RAMMER in the rest of the evaluations. RAMMERBASE can also help remove the side effects caused by different implementations in the performance comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">GPU Utilization</head><p>RAMMER's scheduling enables rTasks from different opera- tors to execute alongside each other to achieve better GPU utilization. We evaluate the utilization improvement by RAM-MER through comparing it with both TF, TF-TRT and RAM-MERBASE. Figure <ref type="figure" target="#fig_15">14</ref> shows the average utilization for the 6 DNN models (with batch size of 1) through their execution time. The average GPU utilization only accounts for kernel execution, excluding other stages like operator emitting. Specifically, we use the metric SM-efficiency provided by NVIDIA profiler nvprof <ref type="bibr" target="#b3">[6]</ref> to measure the utilization, which calculates the percentage of time when at least one warp is active on a multiprocessor. Compared to TF and TF-TRT, RAMMER can improve GPU utilization by 4.32× and 2.45× on average respectively across different models. This improvement comes from both the lower runtime scheduling overhead and the capability to co-schedule operators in RAM-MER. Through comparing RAMMER with highly optimized RAMMERBASE, which uses the same set of kernels, our evaluation shows that RAMMER's scheduling by itself can improve the utilization by 1.61× on average, and up to 2.39× for the LSTM-TC model.</p><p>As mentioned in §2, modern GPUs support the multistreaming mechanism to increase utilization through concurrently scheduling independent kernels. We evaluate the efficiency of multi-streaming by increasing the stream numbers in TF. Figure <ref type="figure" target="#fig_16">15</ref> shows both the end-to-end execution time and the kernel time when using stream number of 1, 2, and 4 for each model. We observe that using more streams can harm the end-to-end performance, a phenomenon observed by others <ref type="bibr" target="#b40">[45]</ref>. For example, using 4 streams increases the end-to-end time by 2.72× on average compared with using a single stream. Moreover, the kernel time in each model only sees very small reduction after enabling multi-streaming, which implies most kernels are still sequentially executed, thus providing little improvement on the GPU utilization. The major reason is because multi-streaming introduces even higher operator scheduling overhead, as shown in Figure <ref type="figure" target="#fig_16">15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Scheduling Overhead</head><p>The techniques proposed by RAMMER can effectively reduce scheduling overhead. To verify this, we evaluate the run-time  scheduling overhead by comparing RAMMER with both TF, TF-TRT and RAMMERBASE. Figure <ref type="figure" target="#fig_4">16</ref> shows the total kernel time and the scheduling overhead (i.e., the time not spent on actual computation) for each model. given rOperator and RAMMER can select kernels based on its policy (i.e., the RAMMER-select), it can further improve the end-to-end performance by 1.44× on average, and up to 2.28× compared with RAMMER-fast, even though the selected kernels may be not the fastest in isolation. In fact, if we use these kernels in RAMMERBASE (i.e., the RAMMERBASEselect), its performance will drop by 1.84× on average. We further perform detailed analysis of the kernels used in LSTM-TC model with batch size of 4. For example, for the Matmul operator, the fastest kernel uses 1,024 rTasks to get the optimal execution time of 4.28 microseconds; while the selected kernel by RAMMER only consists of 16 rTasks and gets a slower execution time of 7.46 microseconds when launched alone. However, RAMMER chooses this kernel to trade a slower individual kernel (by reducing intra-operator parallelism) for a better overall performance (through increasing the inter-operator parallelism), thanks to the holistic scheduling capability of RAMMER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Fine-grained Synchronization</head><p>As a synchronization mechanism, barrier-rTask provides some extra optimization spaces for the DFGs with irregular structure, which is common in the models generated by neural architecture search (NAS) <ref type="bibr" target="#b49">[54]</ref>. To highlight such extra benefit, we leverage NASBench <ref type="bibr" target="#b45">[50]</ref>, a state-of-the-art NAS benchmark, to randomly generate 5,000 modules, where each module is a small DFG that consists of up to 9 operators and 7 edges. We first compare the end-to-end performance of RAMMER and RAMMERBASE on all these modules, which shows RAMMER can improve the performance by 1.28× on average, and up to 3.40× than RAMMERBASE. Among all these modules, our measurement shows that 28.3% of them has obvious irregular structures, e.g., heterogeneous operators in a wave decided by our policy (Algorithm 1). For these modules, we compare the end-to-end performance of using our barrier-rTask implementation and a global barrier. The results show that using the barrier-rTask can provide extra performance speedup of 1.11× on average and up to 1.89×. Figure <ref type="figure" target="#fig_7">18</ref> illustrates one of such modules, where the execution time and rTask number in each operator are also listed.</p><p>For such a DFG, our barrier-rTask provides a possibility to overlap the execution of operators from different waves (e.g., the two STEM-Conv operators from wave 1 and 2) through removing the global barriers between waves and inserting fine-grained rTask-level synchronizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on Other Accelerators</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">End-to-end Performance on ROCm GPUs</head><p>We evaluate the efficiency of RAMMER on AMD ROCm GPUs by comparing it with TF, TVM, and RAMMERBASE. TF-XLA is not included because it cannot be successfully enabled on AMD GPUs in our experiments, and TensorRT is not included because it is proprietary and is exclusive for NVIDIA. Figure <ref type="figure" target="#fig_18">19</ref> shows the end-to-end performance of the 6 benchmarks with batch size of 1. Compared with TF, RAMMER can outperform it by 13.95× on average, and up to 41.14× for the LSTM-TC model. Compared to TVM, RAM-MER can improve the performance by 5.36× on average, and up to 7.57×. Note that we fail to make the TVM auto tuning feature works on ROCm GPUs, so TVM just uses its default kernels in this experiment. Compared with RAMMERBASE, we can see that the proposed scheduling of RAMMER's can bring average of 2.19× and up to 4.12× speedup. Finally, RAMMERBASEK in the figures is exactly the same as RAM-MERBASE, except that it uses kernels from RAMMER. Notice that RAMMER might not always choose the fastest kernel implementations for the rOperators. Though there are little performance change for most models, for the ResNeXt model there is a 3.02× performance drop. This demonstrates the importance of the interplay of scheduling and kernel selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">End-to-end Performance on Graphcore IPU</head><p>We also conduct a preliminary evaluation of RAMMER on a Graphcore IPU. In this experiment, we choose only the three   RNN benchmarks, again, because it takes effort implementing efficient rOperators to support other models. Currently, RAMMER only supports a single IPU device. We leave the multi-IPU support of RAMMER to future work. For the three RNN models, due to the limited memory available on IPU (256 KB on each tile), we configure the layers of these models to 4 in order to fit in a single IPU. Figure <ref type="figure" target="#fig_20">20</ref> shows the end-to-end performance of RAMMER on these models with batch size of 1. It shows that RAMMER's preliminary implementation can bring up to 5.37× performance improvement compared with RAMMERBASE, which demonstrates the applicability and effectiveness of the abstractions of RAMMER on new accelerator architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Having shown the advantages, we discuss some RAMMER's limitations and future work in this section.</p><p>Performance gain on large batch sizes. RAMMER's benefits are more significant when the intra-operator parallelism is insufficient to saturate hardware. This is the case when the input batch size is small, often found in online DNN inference. Moreover, our preliminary experiment shows that this is also the case for some model training workloads with large batch sizes (e.g., 256), such as the LSTM-TC model. Figure <ref type="figure" target="#fig_21">21</ref> shows the training performance of LSTM-TC with batch size of 256. As it shows, with holistic optimizations on both intra-and inter-operator parallelism, RAMMER improves the performance by 2.28× than our baseline implementation RAMMERBASE and 2.36× than TF-XLA. We will leave a more detailed analysis and further optimizations on model training with large batch sizes as our future work.</p><p>Dynamic graph. Currently, RAMMER only supports static graph. For DFGs with dynamic control flow <ref type="bibr" target="#b46">[51]</ref>, RAMMER can compile each of the static sub-graphs, e.g., a branch of conditionals or a body of loops, into individual rPrograms. We leave this implementation to our future work.</p><p>Inter-job scheduling. RAMMER focuses on optimizing a single deep learning job and is orthogonal to inter-job scheduling, e.g., through scheduling multiple models in a batch or precisely controlling each job's hardware resource with vDevice. Nevertheless, it is an interesting topic to explore the possibility to co-schedule rTasks not only from different operators, but from different jobs within an accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>DNN compiler optimization can be generally divided into two classes based on its two-layered representations. DFG-level optimizations, such as operator fusion, are exploited in many DNN frameworks and compilers, e.g., TensorFlow <ref type="bibr" target="#b13">[18]</ref>, Py-Torch [15], TVM <ref type="bibr" target="#b18">[23]</ref>, XLA <ref type="bibr" target="#b12">[17]</ref>, etc. TASO <ref type="bibr" target="#b29">[34]</ref> proposes an automatic graph substitutions approach to optimize the DFG.</p><p>On the operator-level, recent work has leveraged different approaches to tune and generate efficient hardware-specific operator code, e.g., AutoTVM <ref type="bibr" target="#b19">[24]</ref>, Tensor comprehension <ref type="bibr" target="#b42">[47]</ref>, FlexTensor <ref type="bibr" target="#b48">[53]</ref>, Tiramisu <ref type="bibr" target="#b16">[21]</ref>, Halide <ref type="bibr" target="#b38">[43]</ref>, etc. RAMMER is compatible with all these optimizations through taking an optimized DFG as input and generating efficient rKernels with those kernel generators. DNN inference and its optimization have attracted a lot of recent attention. DeepCPU <ref type="bibr" target="#b47">[52]</ref>, BatchMaker <ref type="bibr" target="#b22">[27]</ref>, GRNN <ref type="bibr" target="#b27">[32]</ref>, and NeoCPU <ref type="bibr" target="#b35">[40]</ref> optimize the inference for RNN or CNN specific models on either CPU or GPUs. Jain et al. <ref type="bibr" target="#b28">[33]</ref> proposes to leverage both temporal and spatial multiplexing for multiple inference jobs to improve the GPU utilization. RAMMER differentiates with these works in two aspects: 1) RAMMER can apply to general DNN models and accelerators; and 2) more than just compiler optimizations, RAMMER provides a new abstraction and a larger optimization space for DNN computation. Astra <ref type="bibr" target="#b40">[45]</ref> exploits the predictability of DNN to perform online optimization for DNN training, while RAMMER leverages the same property to reduce the individual rTask scheduling overhead. There are also many inference systems proposed to optimize the overall throughout under the guaranteed query latency, e.g., Nexus <ref type="bibr" target="#b39">[44]</ref>, PRETZEL <ref type="bibr" target="#b33">[38]</ref>, Clipper <ref type="bibr" target="#b20">[25]</ref>, TF-serving <ref type="bibr" target="#b37">[42]</ref>, etc. RAMMER instead focuses on optimizing a single model and is orthogonal to these works. Some other work from the GPU community has proposed software-based schedulers within a GPU to schedule general workload. For example, Juggler <ref type="bibr" target="#b17">[22]</ref> proposes a framework to dynamically execute a job represented as a DAG of tasks. Wu et al. <ref type="bibr" target="#b43">[48]</ref> proposes a software approach to control the job locality on SMs. However, driven by the property of DNN workload, RAMMER proposes a new computation representation with rTask and rOperator; and adopts a compile-time scheduling approach to avoid runtime overhead systemically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>DNN computation suffers from unnecessary overheads due to the fundamental limitations of existing deep learning frameworks, which adopt a two-layer scheduling design that manages the inter-operator scheduling in the framework and delegates intra-operator scheduling to the hardware accelerator. RAMMER addresses this issue with a holistic compiler solution that (1) provides an rTask-operator abstraction that exposes the fine-grained intra-operator parallelism. (2) virtualizes the modern accelerator with parallel execution units to expose the hardware's fine-grained scheduling capability. (3) leverages the predictability of DNN computation to transform run-time scheduling into a problem of generating compile-time rTask execution plans. Our evaluations show that RAMMER can achieve significant improvements compared to native deep learning frameworks, compilation frameworks and even vendor-specific inference engine on GPUs. This positions RAMMER as a new enhancement to the existing ecosystem of DNN compiler infrastructure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The average GPU utilization on different DNN model with different batch size (BS). The utilization only accounts for kernel execution, excluding other stages like operator emitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The average kernel time and end-to-end execution time on different DNN model with different batch size (BS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: An illustration of (a) the inefficiency scheduling in existing approach; and (b) an optimized scheduling plan. 0 10 20 30 40 50 60</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The execution interfaces of traditional operator and rOperator. More details in §4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 P 1 = 5 P 2 = 7 P 12 Function</head><label>4152712</label><figDesc>Scheduling interfaces. RAMMER's rTask-aware DFG compiler introduces two scheduling interfaces, Append and Wait. Append(task_uid, vEU_id) assigns an rTask from Algorithm 1: Wavefront Scheduling Policy Data: G: DFG of rOperator, D: vDevice Result: Plans: rPrograms 1 Function Schedule(G, D): 2 P curr = {}; 3 for W = Wavefront(G) do ScheduleWave(W , P curr , D); ScheduleWave(W , {}, D); 6 if time(P 1 ) ≤ time(P curr ) + time(P 2 ) then ScheduleWave(W, P, D): 13 SelectRKernels(W , P); 14 for op ∈ W do 15 for r ∈ op.rTasks do 16 vEU = SelectvEU(op, P, D); 17 P.Wait(r, Predecessor(op).rTasks); 18 P.Append(r, vEU); 19 return P;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Overall workflow of RAMMER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A CUDA implementation of a naive matrix multiplication with the rOperator abstraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The CUDA code for a vDevice with two vEUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Executing two heterogeneous rTasks on a vEU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: End-to-end model inference time with batch size of 1 on NVIDIA V100 GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: End-to-end model inference time with different batch sizes (BS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: End-to-end model inference time with the batch size of 1 on the ImageNet dataset (image size: 224×224).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Comparison of GPU utilization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: TF performance with different stream(STM) number. (Note: The number atop a bar indicates kernel time.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>Figure 16: GPU scheduling overhead on different models. (The number atop a bar indicates the overhead in percentage.) TF: TensorFlow, TRT: TF-TRT, RB: RAMMERBASE, R: RAMMER</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: End-to-end model inference time with batch size of 1 on AMD MI50 GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: End-to-end model inference time with batch size of 1 on Graphcore IPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: End-to-end model training time of LSTM-TC with batch size of 256 on NVIDIA V100 GPU. Note that TVM and TF-TRT do not support training, hence the data is missing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.01,630.00,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,543.00,630.00,259.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Deep learning models and datasets.</figDesc><table><row><cell>Model</cell><cell>Dataset</cell><cell>Model Type</cell><cell>Note</cell></row><row><cell>ResNeXt</cell><cell>CIFAR-10</cell><cell cols="2">Computer Vision layers: 29, cardinality: 16, bottleneck width: 64d (16×64d, paper parameter)</cell></row><row><cell>NASNet</cell><cell>CIFAR-10</cell><cell cols="2">Computer Vision repeated cells: 6, filters: 768 (6@768, paper parameter)</cell></row><row><cell>AlexNet</cell><cell>ImageNet</cell><cell cols="2">Computer Vision (paper parameter)</cell></row><row><cell>DeepSpeech2</cell><cell>LibriSpeech</cell><cell>Speech</cell><cell>input length: 300; CNN layer: 2; RNN: type: uni-LSTM, layer: 7, hidden size: 256</cell></row><row><cell>LSTM (-TC)</cell><cell>synthetic</cell><cell>Language Model</cell><cell>input length: 100, hidden size: 256, layer: 10</cell></row><row><cell>Seq2Seq (-NMT)</cell><cell>synthetic</cell><cell>Language Model</cell><cell>Encoder: input length: 100, type: uni-LSTM, hidden size: 128, layer: 8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Decoder: output length: 30, type: uni-LSTM, hidden size: 128, layer: 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Figure18: An irregular DFG generated by NASBench nels only for each individual operator, and the kernels selected by RAMMER's scheduling policy. Figure17shows the performance of RAMMER and RAMMERBASE with these two kernel sets on two representative CNN and RNN models, i.e., ResNeXt and LSTM-TC. First, no matter which set of kernels is used, RAMMER can always improve the performance significantly. For example, if RAMMER uses the same fastest kernels (i.e., the RAMMER-fast) as used in RAMMERBASE (i.e., RAMMERBASE-fast), it can improve the performance by 2.89× on average. If more rKernels are available for a</figDesc><table><row><cell></cell><cell></cell><cell>STEM-Conv</cell><cell>BN &amp; Relu</cell><cell>Pool</cell></row><row><cell></cell><cell></cell><cell>rTask: 64</cell><cell>rTask: 128</cell><cell>rTask: 128</cell></row><row><cell></cell><cell></cell><cell>Time: 109 us</cell><cell>Time: 7.9 us</cell><cell>Time: 6.3 us</cell></row><row><cell>Conv</cell><cell>BN &amp; Relu</cell><cell></cell><cell></cell><cell></cell></row><row><cell>rTask: 64</cell><cell>rTask: 128</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time: 4.9 us</cell><cell>Time: 7.9 us</cell><cell>Pool</cell><cell>STEM-Conv</cell><cell>BN &amp; Relu</cell></row><row><cell></cell><cell></cell><cell>rTask: 128</cell><cell>rTask: 64</cell><cell>rTask: 128</cell></row><row><cell></cell><cell></cell><cell>Time: 6.3 us</cell><cell>Time: 109 us</cell><cell>Time: 7.9 us</cell></row><row><cell></cell><cell></cell><cell>Wave 1</cell><cell>Wave 2</cell><cell>Wave 3</cell></row><row><cell>Specifically, com-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pared with TF, RAMMERBASE can reduce the scheduling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>time from an average of 32.29 milliseconds to only 2.27 mil-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>liseconds (overhead percentage from 55.41% to 18.43%) over</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>all models. Even compared with TF-TRT, RAMMERBASE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>can reduce the average scheduling overhead from 31.38% to</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>18.43%. RAMMERBASE achieves this reduction by optimiz-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ing the scheduling execution code path and leveraging opera-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tor fusion to reduce kernel launches. The significant reduction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>demonstrates the heavy overhead of operator scheduling in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>existing DNN frameworks. Compared with RAMMERBASE,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RAMMER can further reduce the average overhead from 2.27</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>milliseconds to 0.37 milliseconds, a 6.14× reduction. This</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>significant reduction is due to static compile-time operator</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>scheduling, i.e. packing operators into rProgram so that sev-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>eral operators can be executed by a single GPU kernel launch.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5.2.4 Interplay of Intra and Inter Operator Scheduling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RAMMER enables scheduling policies to optimize the inter-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>play of intra and inter operator scheduling, instead of just</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>focusing on making individual operators fast. This is im-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>plemented through selecting appropriate rKernel for each</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>rOperator, as introduced in  §3.3. We evaluate the effect of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>such scheduling by using two sets of kernels: the fastest ker-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/microsoft/nnfusion</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Note that the LSTM's GPU utilization is slightly higher when batch size is 1 compared to that when batch size is 16, because TensorFlow uses different kernel implementations of GEMM for different batch sizes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/microsoft/nnfusion/tree/osdi20_ artifact/artifacts</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers and our shepherd, Prof. Jinyang Li, for their extensive suggestions. We thank Jim Jernigan and Kendall Martin from the Microsoft Grand Central Resources team for the support of GPUs. Fan Yang thanks the late Pearl, his beloved cat, for her faithful companion during writing this paper. This work was partially supported by the National Natural Science Foundation of China under Grant No. 61972004.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cooperative</forename><surname>Groups</surname></persName>
		</author>
		<ptr target="https://devblogs.nvidia.com/cooperative-groups/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://docs.nvidia.com/cuda/cuda-driver-api" />
		<title level="m">CUDA Driver API</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nvcc</forename><surname>Cuda</surname></persName>
		</author>
		<ptr target="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="https://docs.nvidia.com/cuda/profiler-users-guide/" />
		<title level="m">CUDA nvprof</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D" />
		<title level="m">Depthwise separable 2D convolution</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<ptr target="https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP-GUIDE.html" />
	</analytic>
	<monogr>
		<title level="j">HIP Programming Guide</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mkl-Dnn</forename><surname>Intel</surname></persName>
		</author>
		<ptr target="https://github.com/oneapi-src/oneDNN" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">'</forename><surname>Ipu Programmer</surname></persName>
		</author>
		<author>
			<persName><surname>Guide</surname></persName>
		</author>
		<ptr target="https://www.graphcore.ai/docs/ipu-programmers-guide" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="http://www.openslr.org/12/" />
		<title level="m">LibriSpeech ASR corpus</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="https://developer.nvidia.com/cudnn" />
		<title level="m">NVIDIA cuDNN</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia Tensorrt</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/tensorrt" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="https://onnx.ai/" />
		<title level="m">ONNX</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="https://www.tensorflow.org/xla" />
		<title level="m">XLA</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<title level="s">USENIX Association</title>
		<meeting><address><addrLine>GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep speech 2 : End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
				<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016-06-22">20-22 Jun 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Remzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<title level="m">Operating Systems: Three Easy Pieces</title>
				<meeting><address><addrLine>North Charleston, SC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CreateSpace Independent Publishing Platform</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tiramisu: A polyhedral compiler for expressing fast and portable code</title>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malek</forename><surname>Ben Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Del Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdurrahman</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</title>
				<meeting>the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Juggler: A dependence-aware task-based execution framework for gpus</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mehmet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyong</forename><surname>Belviranli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laxmi</forename><forename type="middle">N</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><surname>Bhuyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;18</title>
				<meeting>the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="54" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TVM: An automated endto-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
				<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3389" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clipper: A low-latency online prediction serving system</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilio</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)</title>
				<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="613" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Low latency rnn inference with cellular batching</title>
		<author>
			<persName><forename type="first">Pin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference (EuroSys 18)</title>
				<meeting>the Thirteenth EuroSys Conference (EuroSys 18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">[dl] a survey of fpga-based neural network inference accelerators</title>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Reconfigurable Technol. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019-03">March 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A study of persistent threads style gpu programming for gpgpu workloads</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Innovative Parallel Computing (InPar)</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">November 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grnn: Low-latency and scalable rnn inference on gpus</title>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mawhirter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference</title>
				<meeting>the Fourteenth EuroSys Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Joseph Gonzalez, and Ion Stoica. Dynamic space-time scheduling for gpu inference</title>
		<author>
			<persName><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangxi</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harikaran</forename><surname>Subbaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rehan</forename><surname>Sohail Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<idno>CoRR, abs/1901.00041</idno>
		<imprint>
			<date type="published" when="2018-12">Dec 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Taso: Optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP &apos;19</title>
				<meeting>the 27th ACM Symposium on Operating Systems Principles, SOSP &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance upper bound analysis and optimization of sgemm on fermi and kepler gpus</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), CGO &apos;13</title>
				<meeting>the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), CGO &apos;13<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Planning Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><surname>Lavalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PRETZEL: Opening the black box of machine learning prediction serving systems</title>
		<author>
			<persName><forename type="first">Yunseong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Scolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung-Gon</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">Domenico</forename><surname>Santambrogio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Interlandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
				<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="611" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Handbook of Scheduling: Algorithms, Models, and Performance Analysis</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurie</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>CRC Press, Inc., USA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimizing cnn model inference on cpus</title>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference, USENIX ATC &apos;19</title>
				<meeting>the 2019 USENIX Conference on Usenix Annual Technical Conference, USENIX ATC &apos;19<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1025" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An improved magma gemm for fermi graphics processing units</title>
		<author>
			<persName><forename type="first">Rajib</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanimire</forename><surname>Tomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="511" to="515" />
			<date type="published" when="2010-11">November 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tensorflow-serving: Flexible, high-performance ml serving</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Soyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Gorovoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Sukriti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinu</forename><surname>Rajashekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on ML Systems at NIPS 2017</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI &apos;13</title>
				<meeting>the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="519" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nexus: A gpu cluster engine for accelerating dnn-based video analysis</title>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lequn</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthai</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP &apos;19</title>
				<meeting>the 27th ACM Symposium on Operating Systems Principles, SOSP &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploiting predictability to optimize deep learning</title>
		<author>
			<persName><forename type="first">Muthian</forename><surname>Sivathanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapan</forename><surname>Chugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">S</forename><surname>Singapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Astra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;19</title>
				<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="909" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14</title>
				<meeting>the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno>CoRR, abs/1802.04730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enabling and exploiting flexible task assignment on gpu through sm-centric program transformations</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM on International Conference on Supercomputing, ICS &apos;15</title>
				<meeting>the 29th ACM on International Conference on Supercomputing, ICS &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">NAS-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic control flow in large-scale machine learning</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference, EuroSys &apos;18</title>
				<meeting>the Thirteenth EuroSys Conference, EuroSys &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepcpu: Serving rnn-based deep learning models 10x faster</title>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (USENIX ATC 18)</title>
				<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-07">2018. July 2018</date>
			<biblScope unit="page" from="951" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="859" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
