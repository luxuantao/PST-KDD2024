<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NER-to-MRC: Named-Entity Recognition Completely Solving as Machine Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-06">6 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
							<email>tetsuyasakai@acm.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hayato</forename><surname>Yamana</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NER-to-MRC: Named-Entity Recognition Completely Solving as Machine Reading Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-06">6 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.03970v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>(a) Named-Entity Recognition Tasks Only France and Britain backed Fischler &apos;s proposal. B-LOC B-LOC B-PER (b) Machine Reading Comprehension Problems</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named-entity recognition (NER) detects texts with predefined semantic labels and is an essential building block for natural language processing (NLP). Notably, recent NER research focuses on utilizing massive extra data, including pre-training corpora and incorporating search engines. However, these methods suffer from high costs associated with data collection and pre-training, and additional training process of the retrieved data from search engines. To address the above challenges, we completely frame NER as a machine reading comprehension (MRC) problem, called NERto-MRC, by leveraging MRC with its ability to exploit existing data efficiently. Several prior works have been dedicated to employing MRC-based solutions for tackling the NER problem, several challenges persist: i) the reliance on manually designed prompts; ii) the limited MRC approaches to data reconstruction, which fails to achieve performance on par with methods utilizing extensive additional data. Thus, our NER-to-MRC conversion consists of two components: i) transform the NER task into a form suitable for the model to solve with MRC in a efficient manner; ii) apply the MRC reasoning strategy to the model. We experiment on 6 benchmark datasets from three domains and achieve state-of-the-art performance without external data, up to 11.24% improvement on the WNUT-16 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A fundamental topic in natural language processing (NLP) is named entity recognition (NER), which aims to detect the text with predefined semantic labels, such as a person, a position, and others <ref type="bibr" target="#b19">(Li et al., 2022)</ref>. Given its importance, many methods have been published for this task and most of them can be viewed within a sequence labeling framework <ref type="bibr" target="#b20">(Li et al., 2020;</ref><ref type="bibr">Wang et al., 2021b;</ref><ref type="bibr"></ref> Question: What kind of entity is this? Options: <ref type="bibr">[</ref> Figure <ref type="figure">1</ref>: Comparison of the name-entity recognition task and machine reading comprehension problem, where the example text and related labels are the same as <ref type="bibr">CoNLL-2003 dataset(Sang and</ref><ref type="bibr" target="#b34">Meulder, 2003)</ref>. For presentation purposes, we omit the entity label "Other".</p><p>(a) shows a typical solution for NER tasks. (b) presents our NER-to-MRC framework. <ref type="bibr" target="#b11">Fu et al., 2021;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref> that tags the input words, as shown in Fig. <ref type="figure">1</ref> (a). For example, with a given sentence, a properly working NER system should recognize three named entities with its inside-outside-beginning (IOB) format tagging <ref type="bibr" target="#b33">(Ramshaw and Marcus, 1995)</ref>: "France" is a location entity (B-LOC), "Britain" is a location entity (B-LOC), and "Fischler" is a person entity (B-PER). NER tasks provide only a single sequence with limited information as the model's input. Consequently, several approaches <ref type="bibr">(Wang et al., 2021b;</ref><ref type="bibr" target="#b0">Antony and Suryanarayanan, 2015;</ref><ref type="bibr" target="#b31">Nishida et al., 2022;</ref><ref type="bibr" target="#b42">Wang et al., 2022)</ref> consider how to efficiently utilize additional information to enhance the understanding of the input sequence, thereby improving model performance. However, the effective retrieval of supplementary information and training on this extra data require additional cost and effort.</p><p>The above contradiction poses a challenge, and hence the focus of our paper is to address this. The ability of reading comprehension can reflect human understanding and reasoning skills <ref type="bibr" target="#b6">(Davis, 1944;</ref><ref type="bibr">?)</ref>, such as question-answering or multiple-choice. Additionally, recent studies show that machine reading comprehension (MRC) can improve various tasks, such as natural language inference and text classification <ref type="bibr" target="#b51">(Yang et al., 2022;</ref><ref type="bibr" target="#b47">Wei et al., 2022;</ref><ref type="bibr" target="#b35">Sanh et al., 2022)</ref>. Inspired by these, we study the problem of how to harness MRC to address NER, resulting in bridging MRC reasoning strategy with NER task. To begin with, we require high-quality data reconstruction to convert NER as an MRC problem. Our preferred method for addressing this construction involves the use of artificially designed NER-to-MRC queries, including BERT-MRC <ref type="bibr" target="#b20">(Li et al., 2020)</ref> and KGQA <ref type="bibr" target="#b3">(Banerjee et al., 2021)</ref>. However, this solution faces the following challenges, i) low-transferability: the lack of a unified data reconstruction paradigm makes it difficult to migrate to new datasets. ii) hand-crafted: high reproduction difficulty and unstable performance due to hand-crafted question templates; iii) insufficient information: the input ignores label semantics.</p><p>To address the above challenges, we frame the data construction as a multiple choice (MC) problem. Before detailing the solution, we first present an example of MC format as follows: given a (passage, question, options) triplet, the system chooses "Location" as the entity label for "France". The MC format overcomes the counter-intuitive challenge mentioned above by allowing the model to predict possible entity labels through text descriptions. In essence, what matters the most for building the MC format is to generate appropriate questions for prompting language models. Inspired by the design principle (a less manual process) of UniMC <ref type="bibr" target="#b51">(Yang et al., 2022)</ref>, we only provide a single prompt question, which implies stable results as opposed to inconsistent question templates in existing schemes to resolve the hand-crafted challenge. On the other hand, to address the insufficient information challenge, our design introduces label information as options, which conveys essential semantics in a variety of low-resource scenarios <ref type="bibr" target="#b26">(Luo et al., 2021;</ref><ref type="bibr" target="#b28">Mueller et al., 2022)</ref>.</p><p>Recent state-of-the-art (SoTA) NER works rely on the assistance of external data, such as retrieved text from the Google search engine <ref type="bibr">(Wang et al., 2021b)</ref> and pre-training data <ref type="bibr" target="#b50">(Yamada et al., 2020)</ref>. To achieve performance comparable to state-of-the-art external context retrieving approaches without relying on additional retrieved data, merely considering MRC methods in the data reconstruction phase is insufficient. We believe that MRC techniques can learn missing information from the dataset without any extra data. Therefore, we further integrated MRC reasoning strategies into the NER tasks. Specifically, we introduce a powerful human reading strategy, HRCA <ref type="bibr" target="#b52">(Zhang and Yamana, 2022)</ref> (details in Sec. 3.2), instead of only inserting a Pre-trained Language Model (PLM) as BERT-MRC. To this end, we transfer the model choice to entity labels by matching the options.</p><p>To evaluate our framework, we carry out numerous experiments on 6 challenging NER benchmarks, including three domains with general and specialized vocabulary. The results demonstrate that our approach improves SoTA baselines, such as WNUT-16 <ref type="bibr" target="#b39">(Strauss et al., 2016)</ref> (+11.24%), WNUT-17 <ref type="bibr" target="#b7">(Derczynski et al., 2017)</ref> (+0.76%), BC5CDR <ref type="bibr" target="#b18">(Li et al., 2016)</ref>  <ref type="bibr">CoNLL-2003 (Sang and</ref><ref type="bibr" target="#b34">Meulder, 2003)</ref> (+0.33%) and CoNLL++ <ref type="bibr">(Wang et al., 2019a</ref>) (+0.44%). Note that our MRC method achieves such performance without any extra data, which suggests the potential of mining text for intrinsic connections to complete complex tasks.</p><formula xml:id="formula_0">(+1.48%), NCBI (Dogan et al., 2014) (+1.09%),</formula><p>In summary, our contributions are:</p><p>? We propose a new NER-to-MRC reconstruction solution by introducing a simple yet stable data reconstruction rule. ? We apply MRC strategies to solve NER problems without retrieval systems or pre-training NER data, resulting in a concise process. ? Our approach shows the SoTA performance on multiple popular NER datasets, including three domains with general (WNUT-16, <ref type="bibr">WNUT-17, CoNLL-2003 and CoNLL++)</ref> and specialized (BC5CDR and NCBI) vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We introduce the general development trend of the NER field in Sec. 2.1. Treating NLP tasks other than MRC as MRC problems can enhance the performance of corresponding tasks. We demonstrate how such schemes apply the MRC paradigm to other NLP tasks in Sec. 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Named Entity Recognition (NER)</head><p>NER is designed to detect words from passages by predefined entity labels <ref type="bibr" target="#b19">(Li et al., 2022)</ref>, which serves as the foundation of complicated applications such as machine translation. The BiLSTM architecture is the most commonly used architecture for solving NER tasks in the early days of deep learning <ref type="bibr" target="#b19">(Li et al., 2022)</ref>. In the bi-directional LSTM, each word's representation can be derived from the contextual representation that connects its left and right directions, which is advantageous for many tagging applications <ref type="bibr" target="#b14">(Lample et al., 2016)</ref>. Later, with the development of pre-trained language models represented by BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, transformer-based pre-trained models quickly became the preferred model for the NLP area <ref type="bibr">(Lin et al., 2021)</ref>. The embeddings learned by these transformer-based pre-trained language models are contextual and trained on a large corpus. As a result, for NER tasks that value input representation, pre-trained language models rapidly become a new paradigm. In recent years, introducing external data to PLMs becomes dominant and shows powerful contextual representations, such as NER-BERT <ref type="bibr" target="#b24">(Liu et al., 2021)</ref>, LUKE <ref type="bibr" target="#b50">(Yamada et al., 2020)</ref> and CL-KL <ref type="bibr">(Wang et al., 2021b)</ref>. LUKE <ref type="bibr" target="#b50">(Yamada et al., 2020)</ref> proposes an entity-aware selfattention mechanism and a pre-training task to predict masked words and entities in a sizeable entityannotated corpus. CL-KL <ref type="bibr">(Wang et al., 2021b)</ref> finds the external contexts of query sentences by employing a search engine and then processes them by a cooperative learning method. However, largescale external datasets consume considerable collection time and even labor costs. Therefore, we explore an extra-data-free framework by MRC strategies after transferring NER tasks to MRC problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Enhancing NLP Tasks via MRC perspective</head><p>Treating NLP tasks other than MRC as MRC problems strengthens neural networks' reasoning processes, including event extraction <ref type="bibr" target="#b10">(Du and Cardie, 2020;</ref><ref type="bibr" target="#b23">Liu et al., 2020)</ref>, relation extraction <ref type="bibr" target="#b21">(Li et al., 2019;</ref><ref type="bibr" target="#b17">Levy et al., 2017)</ref>, and named-entity recognition <ref type="bibr" target="#b20">(Li et al., 2020)</ref>. The current mainstream approaches <ref type="bibr" target="#b20">(Li et al., 2020;</ref><ref type="bibr" target="#b3">Banerjee et al., 2021;</ref><ref type="bibr" target="#b40">Sun et al., 2021;</ref><ref type="bibr" target="#b10">Du and Cardie, 2020;</ref><ref type="bibr" target="#b21">Li et al., 2019;</ref><ref type="bibr" target="#b17">Levy et al., 2017;</ref><ref type="bibr" target="#b23">Liu et al., 2020;</ref><ref type="bibr" target="#b48">Xue et al., 2020;</ref><ref type="bibr" target="#b38">Shrimal et al., 2022)</ref> utilizes data reconstruc-tion to address NER tasks using MRC methodology. Specifically, those approach involves restructuring the input data into MRC format by incorporating MRC-style question prompts. These prompts encompass the direct utilization of entities as questions, human-designed question templates and unsupervised generated questions. However, those methods have missed important label semantics, which describe the entity labels from annotation guidelines in each NER dataset. On the other hand, even though all of these approaches explore the utilization of MRC for solving the NER problem, they merely employ the MRC scheme during the data reconstruction phase, rather than effectively leveraging the most critical reasoning strategy of MRC for the NER task. To address the problems above, our method inferences the possible entity type through: i) introducing label information; ii) applying MRC reasoning strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Complete NER-to-MRC Conversion</head><p>The current field of NER faces the following challenges: i) hand-crafted design and inadequate information; ii) failure to integrate crucial MRC reasoning strategies throughout the entire inference process. In this section, we outline the proposed NER-to-MRC framework, including input reconstruction (Sec. 3.1) to address the hand-crafted design challenge, MRC reasoning network (Sec. 3.2) to introduce essential MRC processes into NER tasks. We demonstrate the fine-tuning of our framework in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Reconstruction</head><p>Input reconstruction is a key topic for facilitating the solution of NER tasks from a MRC perspective. We introduce a simple and instructional reconstruction rule (Details in Appendix A.2). Alternative to the question-and-answer format in previous work <ref type="bibr" target="#b20">(Li et al., 2020)</ref>, we consider a multiplechoice (MC) format that incorporates label information, as shown in Fig. <ref type="figure" target="#fig_2">2</ref> (a). Given a common NER dataset, a sample includes a text sequence X = {x 1 , x 2 , ..., x n } with n words and the corresponded entity label Y = {y 1 , y 2 , ..., y n } for each word. Then, the transformed MC format ((passage, question, options) triplets) is constructed by: Passage: We obtain the passage part effortlessly, which comes from the text sequence X in the original dataset. Options Label 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, PER ORG LOC MISC Text Label "EU" "B-ORG", "rejects" "O", "German" "B-MISC", "call" "O", "to" "O", "boycott" "O", "British" "B-MISC", "lamb" "O", "."</p><p>"O" E(Qn)</p><formula xml:id="formula_1">E(Qn) E(Qn) ? = E(Qn) E(On 3 ) E(Qn) ? = E(Pn) E(Pn) ? = E(Pn) E(On 3 ) E(On 3 ) E(On 3 ) E(P n ) E(P n ) H(O n 3 ) P n , Q n , O n 4 P n , Q n , O n 3</formula><p>"EU rejects German call to boycott British lamb. "</p><p>Passage "What kind of entity is this?"</p><p>Question "Person ?", "Organization ?", "Location ?", "Miscellaneous?"</p><formula xml:id="formula_2">Options P n , Q n , O n 2 E(P n ) E(P n ) H(O n 2 ) H(P n ) H(Q n ) H(O n 1 ) E(Qn) E(Qn) E(Qn) ? = E(Qn) E(On 2 ) E(Qn) ? = E(Pn) E(Pn) ? = E(Pn) E(On 2 ) E(On 2 ) E(On 2 ) H(Qn) H(Qn) H(Qn) ? H(Qn) H(On 1 ) H(Qn) ? H(Pn) ? H(Pn) H(On 1 ) H(On 1 )</formula><p>?? ? HRCA layers</p><formula xml:id="formula_3">H(Qn) H(On 1 ) Network P n , Q n , O n 1</formula><p>Encoder (PLMs)</p><formula xml:id="formula_4">H(P n ) H(Q n ) H(O n 1 ) H(Qn) H(Qn) H(Qn) ? H(Qn) H(On 1 ) H(Qn) ? H(Pn) ? H(Pn) H(On 1 ) H(On 1 )</formula><p>Step 1:</p><p>Step 2:</p><p>Step 3: Question: For less human-crafted processing, we only provide a universal question ("What kind of entity is this?") for all types of entities. It is noteworthy that, due to the fixed inputs, our model produces stable results that are reproducible, in contrast to hand-crafted questions that can vary from individual to individual.</p><p>Options: We treat an entity type as significant input with semantic information rather than just a label. Specifically, we borrow the description of each entity label from the annotation guidelines.</p><p>Finally, an input sample consists of a passage P = {p 1 , p 2 , . . . , p k }, a question Q = {q 1 , q 2 , . . . , q m }, and</p><formula xml:id="formula_5">N O options O = o i 1 , o i 2 , . . . , o i n N O</formula><p>i=1 with an entity description, where k, m, n are the corresponding sequence lengths for passage, question, and options. Regarding labels, we remove the "B-" and "I-" tagging and only keep the entity type itself. Then, based on the sequence length of the passage and the number of entity types, the label is processed as a binary matrix M label ? B k?N O with assigning 1 to each corresponding entity type on the labels, 0 to the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MRC Reasoning Network</head><p>Our framework employs PLMs with MRC strategies as the MRC reasoning network. Remarkable advances <ref type="bibr" target="#b2">(Banditvilai, 2020;</ref><ref type="bibr" target="#b1">Baier, 2005)</ref> suggest that reading methods improve participants to learn a deep understanding of reading comprehension. Note that the success of HRCA <ref type="bibr" target="#b52">(Zhang and Yamana, 2022)</ref> on MRC tasks presents a humanlike reasoning flow as a MRC strategy. However, HRCA only focuses on MRC tasks and does not support other NLP tasks. Therefore, we apply it to our NER-to-MRC framework to enhance reasoning ability. In particular, we firstly generate N O MRC triplets input by contacting each option and P and Q (P ? Q ? O). Then, encoded by PLMs (Encoder), the hidden state H is obtained as:</p><formula xml:id="formula_6">H(P ? Q ? O) = Encoder(P ? Q ? O).</formula><p>After that, the H(P ? Q ? O) are separated into three parts H(P ), H(Q), H(O) corresponding to the MRC triplet and fed into HRCA layers. We process those three hidden states in the following 3 steps: i) "reviewing" H(Q) (question) by applying self-attention mechanism. ii) "reading" H(O) (options) with H(Q) (question) by computing cross attention weights. iii) "finding" final results in H(P ) (passages) with H(O) (options). Additionally, we apply multi-head operation <ref type="bibr" target="#b52">(Zhang and Yamana, 2022)</ref> in all attention computation in the HRCA layer. After that, rich semantics from options and questions are embedded into the hidden states of the passage part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-tuning</head><p>For model multiple downstream datasets, we fine-tune the NER-to-MRC framework, as shown in Fig. <ref type="figure">3</ref>. In detail, we pass the encoded hidden states of the passage to a MLP layer, which includes N O sub MLP layers. Those sub MLP layers reduce the original dimensions to 2, which indicates select it or not. Therefore, after the MLP layer, we can obtain a prediction matrix M pred ? R k?N O ?2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Training</head><p>Considering the reconstructed golden label matrix M label and the prediction matrix M pred , we apply Label 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, PER ORG LOC MISC ? ???? PRED "B-ORG", "B-LOC", "B-MISC", "I-MISC", "I-MISC", "O", "O", "B-ORG", "O" the categorical cross-entropy (CCE) loss as training loss. The CCE loss can be calculated as follows:</p><formula xml:id="formula_7">MLP 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, [0.</formula><formula xml:id="formula_8">LCCE = - 1 k k i=1 2 c=1 (m label i ? log m c pred i + (1 -m label i ) ? log 1 -m c pred i )<label>(1)</label></formula><p>where k represents the number of span indexes, as well as the sequence length of the passage part; m label ? B k is a subset of M label , representing the label matrix corresponding to each class of entities;</p><formula xml:id="formula_9">m c pred ? R k?2 (c ? {1, 2}</formula><p>) is a subset of M pred , representing the c-th value along the last dimension of the prediction matrix corresponding to each class of entities.</p><p>Then, we calculate the overall training loss by summing the CCE loss for all entity types:</p><formula xml:id="formula_10">L overall = N O i=1 L CCE i</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Inference</head><p>Since the output is a matrix M pred ? R k?N O ?2 , we design a simple recovering rule to generate the entity labels, as described in Algorithm 1 and Fig. <ref type="figure">3</ref>.</p><p>Recall that we remove IOB-format tagging such as"B-" and "I-" from the label and convert it using one-hot encoding in Sec. 3.1. Therefore, we employ a simple argmax calculation on the last dimension of M pred . After the calculation, M pred ? B k?N O becomes a binary matrix, where 1 indicates that the related class is selected, and 0 implies that it is not selected. The following two scenarios can be considered based on the selected conditions:</p><p>Case A: The case A has multiple categories with a predicted result of 1, indicating that the model 1:</p><formula xml:id="formula_11">a pred ? argmax M pred a pred ? B k?N O 2: for m = 1 ? k do 3: if N O n=1 a pred [m] [n] &gt;= 1 then Case A 4:</formula><p>predict max (M pred [m]) along the second value of the last dimension as the label on position m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>else Case B 6:</p><p>predict "O" as the label on position m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>end if 8: end for 9: for e = 1 ? k do 10:</p><p>if pentity [e] is the first entity for current entity type then 11:</p><p>Add "B-" before pentity [e]. 12:</p><p>else if pentity [e] is not the first entity for current entity type then 13:</p><p>Add "I-" before pentity [e]. 14: else 15:</p><p>Keep pentity [e] unchanged. 16:</p><p>end if 17: end for 18: return pentity considers multiple entity types as possible labels. We choose the entity type with the highest probability as the label for the current position based on M pred before the argmax calculation. When the predicted result only has one category with a value of 1, we take the category with a result of 1 as the entity type of the current position. Case B: All categories predict 0s. In this case, we predict the label of the current position to be "O".</p><p>After assigning the possible entity types for each word, we add IOB-format tagging "B-" and "I-" to the entity labels. Specifically, we allocate 'B-" to the first of each entity label and "I-" to the subsequent ones if they are consecutive identical labels. For example, given a predicted sequence of entity types ("LOC", "LOC", "PER"), the final prediction will be "B-LOC", "I-LOC", "B-PER".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We evaluate our NER-to-MRC framework on several NER benchmarks that are widely used in the three domains:</p><p>? ing datasets in the NER field, and most SoTA methods still struggle with around 60% on F1 scores. Two factors make these datasets challenging, the first of which is that they contain a wide variety of entity types. For example, the WNUT-16 dataset has 10 types of entities, while the WNUT-17 dataset has 6 types of entities. In addition, they have a wide range of different entities, therefore remembering one particular entity will only be helpful for part of the task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Setting</head><p>In our framework, we employ the DeBERTa-v3large <ref type="bibr" target="#b12">(He et al., 2021)</ref> as backbone models in all scenarios. Unless otherwise specified, the learning rate is 8e -6, warming up first and then decaying linearly, and the training epoch number is 10. For the HRCA layers, we consider different settings of the multi-head attention to deal with different datasets. For the challenging datasets, WNUT-16 and WNUT-17, we apply 16 self-attention heads with 32 dimensions in attention hidden states. For other datasets, we set 8 self-attention heads with 64 dimensions in attention hidden states. In our experiments, we run three times and take average scores as the final score, and they are done on a single A100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>We examine the effectiveness of our framework in the axes of i) various domains and ii) models with different designing purposes, as shown in Table 1. In summary, our NER-to-MRC framework achieves the best performance across all datasets with improvement ranging from 0.33% to 11.24%.</p><p>Regarding domains, our framework only learns from the datasets without any extra information. Specifically, biomedical benchmarks require expert knowledge of specific vocabulary. Therefore, indomain models such as Bio-BERT address this issue by introducing biomedical corpus. Interestingly, our general-purpose framework only employs a general-purpose PLM to solve specific problems. Moreover, our data processing workflow does not need to change for different domains.</p><p>Considering various NER methods, the most successful ones rely on additional knowledge from pretraining data (LUKE, Bio-BERT) or the Internet (CL-KL). In contrast, our method overcomes them with what is on hand. There are at least two potential explanations for our improvements. One is that our inputs include almost all information from the dataset, such as overlooked label information. Another explanation is that the MRC reasoning strategy helps learn powerful representations and then prompts networks to generate proper choices.</p><p>BERT-MRC is a well-known model that transforms NER tasks into MRC tasks in the data reconstruction stage. The comparison shows that our framework outperforms BERT-MRC on CoNLL-03. Additionally, their input reconstruction requires hand-crafted question prompts, resulting in unstable predictions and complex extensions on other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">The effectiveness on NER-to-MRC across different backbones</head><p>As presented in Table <ref type="table" target="#tab_6">2</ref>, we explore the mainstream PLMs as our backbone models across various domains. Specifically, we consider DeBERTa <ref type="bibr" target="#b12">(He et al., 2021)</ref>, XLM-RoBERTa <ref type="bibr" target="#b4">(Conneau et al., 2020)</ref> in the general vocabulary (social media and news) and BioBERT <ref type="bibr" target="#b16">(Lee et al., 2020)</ref> in the special vocabulary (biomedical). The detailed model architecture are shown in Appendix B.2. We design the "vanilla" framework as the typical tokenlevel classification tasks by the similar setting in BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. In detail, we incorporate PLMs with an additional classification layer for the hidden state corresponding to each word to generate the answer.</p><p>The results show the effectiveness of our framework, which improves all backbone models on all benchmarks. For example, DeBERTa-v3-base earns 24.72% improvement gains on WNUT-16 by introducing the NER-to-MRC framework. In particular, the results on BC5CDR and NCBI imply that our framework also works well on domainspecific pre-training models, which verifies its cross-domain generalization capability. Specifically, in biomedical domain, the NER-to-MRC improves general-propose DeBERTa-v3-base more than specific-propose BioBERT-base. A possible reason is that our framework introduces the option information to assist PLMs in learning domainspecific information.</p><p>Another point worth noting is that our approach showed a relatively large improvement on the WNUT-16 dataset compared to other datasets, achieving an average improvement of 20.58% across different backbones. WNUT-16 dataset has two distinctive characteristics: i) increased difficulty: compared to general datasets like CoNLL-2003, WNUT-16 includes some entity types that are very "challenging" for models, such as Sports team, TV show, Other entity, etc. These entities often have word combinations that are difficult for models to imagine, requiring higher contextual reasoning ability; ii) increased noise: the annotation guidelines for WNUT-16 have a lot of noise (as mentioned in Appendix B.4), which contaminates the labels to a certain extent. This requires models to have denoising and understanding capabilities, otherwise they cannot handle this dataset. Our framwork has stronger reasoning and understand-   ing capabilities, which is why it achieved significant improvement on the WNUT-16 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Impact of the main modules</head><p>For a comprehensive understanding of our purposed NER-to-MRC, we consider two main designed modules: 1) MRC reconstruction and 2) MRC reasoning strategy (detailed setting in Appendix B.3). Table <ref type="table" target="#tab_7">3</ref> reports the joint effect of the aforementioned modules. In a nutshell, both MRC reconstruction and MRC reasoning strategy cause positive effects on NER performance. Specifically, for a challenging dataset with massive entity classes such as WNUT-17, MRC reasoning strategy provides more improvements than MRC reconstruction. A possible reason is that our reasoning method includes the option content to enhance the passage tokens for final predictions. Considering a dataset with few entity classes (4 types), the vanilla case has achieved a high micro-averaged F1 score (94.39%). Furthermore, there are two potential explanations for MRC reconstruction to help more than the MRC reasoning strategy. One is that the PLMs can easily solve the issue with a small search space of entity labels. Therefore, the label information is good enough for PLMs to learn. Another is the small promotion space to limit the potentiality of the MRC reasoning strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Influence of option content</head><p>Introducing label information as options improves our MRC framework on NER datasets. Therefore, Table <ref type="table" target="#tab_8">4</ref> ablates the effect of input reconstruction with different option contents. Here, we study three option constructions with different sources: i) annotation guidelines, ii) definitions from the Internet (Def. from the Int.), and iii) Entity names only. Specifically, annotation guidelines are the annotation descriptions of entities appended in the dataset. For Def. from the Int., we collect the retrieval text from the Google search engine where the queries are the entity names. The specific compositions are given in Appendix B.5. Table <ref type="table" target="#tab_8">4</ref> shows that providing the descriptions of entities prompts PLMs to learn better than just the entity names. Providing more instructions to models will enable them to gain a deeper understanding of the text. Moreover, the annotation guidelines work better because the annotators understand the task and construct the dataset based on this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Convergence speed</head><p>We explore the detailed comparisons of the performance v.s. training epochs trade-off as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. Our framework only passes 3 epochs to converge around 60.00 percent of the microaveraged F1 score. In contrast, CL-KL * requires at least 20 epochs to converge and presents an un-  <ref type="bibr">(Wang et al., 2021b)</ref>. For a stable result, we take the average score of 3 runs. Under a fair environment, we align the batch size as 2 for them.</p><p>stable disturbance on early 5 epochs. The outcome verifies that our method learns better while viewing fewer samples, which implies better usability for further deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have proposed a complete NERto-MRC conversion by considering NER problems via a MRC perspective. Our approach first reconstructs the NER data into MRC inputs, and then applies the MRC reasoning strategy to predict a rational choice. Furthermore, it shows the state-ofthe-art performance on 6 benchmarks across three domains. Note that this success is based on a single general-purpose PLM without external data. Moreover, our experimental results demonstrate that the NER-to-MRC framework is compatible with a set of different PLMs and that our design is efficient in terms of performance enhancement and convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Most existing NER datasets do not feature nested tags <ref type="bibr" target="#b49">(Yadav and Bethard, 2018)</ref>. In a nested NER dataset, multiple entities can be found inside an entity. Though we provide a NER-to-MRC framework with generalization ability in the NER task, we only evaluate our method with flat NER datasets, which might lead to a less comprehensive scope of our benchmarks. Our solution can be naturally and conveniently applied to handle nested NER tasks. Specifically, we only need to set a threshold so that each word outputs a label for each entity category, allowing a word to have multiple labels to handle nested NER problems. In the future, we will provide the related instructions and further evaluate more NER datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>Our NER-to-MRC framework brings a powerful tool to the real-world across multiple domains such as social media, news, and biomedical. Therefore, the ethical influence of this work might spread to many applications. The ethical implications involve two main points: i) the bias from the backbone networks and ii) training datasets. For the networks, several analyses point out that ethnic biases are included in the PLMs such as BERT <ref type="bibr" target="#b27">(Milios and BehnamGhader, 2022)</ref> and GPT3 <ref type="bibr" target="#b25">(Lucy and Bamman, 2021)</ref>. The potential risks are unpredictable after deployments with those PLMs. Fortunately, our backbones are replaceable as described in Sec. 3.2. Therefore, we encourage the users to install unbiased language models and provide model cards for the details. Beyond the backbone PLMs, it is necessary to pay attention to our downstream tasks, such as gender, race, and sexual orientation.</p><p>In real-world deployments, we suggest it is necessary to design a slew of cleaning procedures such as SampleClean and CPClean <ref type="bibr" target="#b15">(Lee et al., 2021)</ref>.</p><p>After that, we encourage open discussions about its utilization, hoping to reduce potential malicious behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Details</head><p>A.1 Dataset statistics</p><p>We summarize the statistics of the datasets used in this paper in Table <ref type="table">6</ref>. Specifically, "Avg. Length" implies the average of the lengths across all dataset splits with including train set, development set, and test set. Since our method create options with label information as described in Sec. 3.1, we collect their average lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Examples of input reconstruction</head><p>As presented in Table <ref type="table">7</ref>, we outline a sample reconstructed MRC triplet of passage, question, and options from the WNUT-17 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Evaluation metric library</head><p>In Sec. 4.1, we explained how we selected the evaluation metric. To enable easy and comprehensive comparison of both past and future schemes, we computed all of the span-based micro F1 score results presented in this paper using seqeval<ref type="foot" target="#foot_0">1</ref> , an open-source Python framework for sequence labeling evaluation. + MRC reconstruction: we reconstruct the NER dataset into the MRC reconstruction. However, this setting plugs a MLP layer to do multiple choice without MRC reasoning strategy. + MRC reconstruction &amp; MRC reasoning strategy: it follows the same instructions as the NERto-MRC desgin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Sources in our inputs</head><p>As discussion in Sec. 3.1, we take the same descriptions in dataset papers or their released homepages. We apply this setting on <ref type="bibr">WNUT-17, CoNLL-2003, CoNLL++, BC5CDR and NCBI.</ref> For all the datasets we used in this paper, we follow the license terms of the corresponding papers. In particular, WNUT-16 dataset only provide a google document<ref type="foot" target="#foot_1">2</ref> as the annotation guidelines. Unfortunately, this document is inadequate in entity types and contains a lot of noisy text, resulting not applicable label information. Therefore, we collect the definitions of each entity from the Internet, which is the same approaches in Sec. 4.4.3. In detail, we put the full information in Table <ref type="table">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Details of different option types</head><p>In Sec. 4.4.3, we ablate the impact of different sources of option type composition on the model performance. The annotation guidelines can be easily found and utilized in the paper or homepage corresponding to the dataset. Consequently, we do not repeat the examples of annotation guidelines compositions. Instead, we only demonstrate examples of compositions defined from the Internet and examples of compositions using only entity names for the WNUT-17 dataset in Table <ref type="table">9</ref> and the CoNLL++ dataset in Table <ref type="table" target="#tab_3">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Inference speed</head><p>In order to simultaneously address the flat NER and nested NER tasks, our proposed approach predicts the probability of each entity type separately.</p><p>Considering that inference time is an important consideration in NER task, our strategy may exhibit less-than-optimal performance in terms of inference speed compared to those approaches that handle all entity types together. Therefore, we compared several baseline approaches, including traditional BiLSTM <ref type="bibr" target="#b14">(Lample et al., 2016)</ref>, PLM +</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>HPFigure 2 :</head><label>2</label><figDesc>Figure 2: The proposed NER-to-MRC framework. A data sample from the NER tasks will first go through step (a) reconstruction, and then into step (b) network for learning a powerful representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance v.s. training epochs tradeoff based on different methods. The results come from WNUT-17 test set. CL-KL * indicates our reproduction of CL-KL w/o CONTEXT with open-source codes(Wang et al., 2021b). For a stable result, we take the average score of 3 runs. Under a fair environment, we align the batch size as 2 for them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>B. 2</head><label>2</label><figDesc>Backbone model architecture We test the performance of our NER-to-MRC framework using different PLMs as the backbone model in Sec. 4.4.1, including DeBERTa-V3base, DeBERTa-V3-large, XLM-RoBERTa-large and BioBERT-base. We give the detailed model architectures as follows: DeBERTa-V3-base: Number of Layers = 12, Hidden size = 768, Attention heads = 12, Total Parameters = 86M. DeBERTa-V3-large: Number of Layers = 24, Hidden size = 1024, Attention heads = 12, Total Parameters = 304M. XLM-RoBERTa-large: Number of Layers = 24, Hidden size = 1024, Attention heads = 16, Total Parameters = 355M. BioBERT-base: Number of Layers = 12, Hidden size = 768, Attention heads = 12, Total Parameters = 110M.B.3 Ablation experiment setupIn Sec. 4.4.2, we perform ablation experiments for the main modules of our proposed NER-to-MRC. The detailed settings are: Vanilla: we fine-tune the hidden state of the PLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1 NER-to-MRC Inference Input: Sequence length: k; Number of the options: NO; Predicted matrix: M pred ? R k?N O ?2 ; Reconstructed label matrix: M label ? B k?N O ; Output: Predicted entity label pentity with IOB-format tagging.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of our proposed model with previous SoTA results on various NER datasets. Each result is reported as a percentage of the micro-average F1 score. The best scores are in bold, and the second best scores are underlined.</figDesc><table><row><cell>Social Media</cell><cell>News</cell><cell>Biomedical</cell></row><row><cell cols="3">WNUT-16 WNUT-17 CoNLL-03 CoNLL++ BC5CDR NCBI</cell></row></table><note><p>Social Media: WNUT-16 (WNUT 2016 Twitter Named Entity Recognition (Strauss et al., 2016)) dataset and WNUT-17 (WNUT 2017 Emerging and Rare Entity Recognition (Derczynski et al., 2017)) dataset are two challeng-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>The results by deploying NER-to-MRC framework across different backbones. Each result is reported as a percentage of the micro-averaged F1 score. The best performance for each dataset is shown in bold.</figDesc><table><row><cell></cell><cell>WNUT-17</cell><cell>CoNLL++</cell></row><row><cell>Vanilla</cell><cell>57.78</cell><cell>94.39</cell></row><row><cell>+ MRC reconstruction</cell><cell>58.65 (+1.51%)</cell><cell>95.01 (+0.66%)</cell></row><row><cell>+ MRC reconstruction &amp; MRC reasoning strategy</cell><cell cols="2">60.91 (+5.42%) 95.31 +(0.97%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation over the main modules of our purposed NER-to-MRC on WNUT-17 and CoNLL++ test set. Each result is reported as a percentage of the microaveraged F1 score.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results on WNUT-17 and CoNLL++ test set using different option content for the input reconstruction. Each result is reported as a percentage of the micro-averaged F1 score.</figDesc><table><row><cell cols="3">Option content source WNUT-17 CoNLL++</cell></row><row><cell>Entity names only</cell><cell>58.43</cell><cell>94.98</cell></row><row><cell>Def. from the Int.</cell><cell>60.09</cell><cell>95.22</cell></row><row><cell>Annotation guidelines</cell><cell>60.91</cell><cell>95.31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/chakki-works/seqeval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://docs.google.com/document/d/12hI-2A3vATMWRdsKkzDPHu5oT74_tG0-PPQ7VN0IRaw</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BiLSTM scheme <ref type="bibr" target="#b5">(Dai et al., 2019)</ref>, and the current state-of-the-art approach CL-KL, in terms of their inference speed and accuracy on the WNUT-17 dataset in Table <ref type="table">5</ref>.</p><p>We conducted tests on all of the aforementioned results using our own implementation. Based on the results, our approach has a similar inference speed to XLM-RoBERTa-large+BiLSTM, and the model's inference speed is faster than CL-KL. It is worth mentioning that compared to CL-KL, our training cost (our: 3 epoch v.s CL-KL 20 epoch) is relatively better. Our method achieves a better computational resources v.s. performance trade-off. What kind of entity is this?</p><p>Options:</p><p>-Names of corporations (e.g. Google). Don't mark locations that don't have their own name. Include punctuation in the middle of names.</p><p>-Names of creative works (e.g. Bohemian Rhapsody). Include punctuation in the middle of names. The work should be created by a human, and referred to by its specific name.</p><p>-Names of groups (e.g. Nirvana, San Diego Padres). Don't mark groups that don't have a specific, unique name, or companies (which should be marked corporation).</p><p>-Names that are locations (e.g. France). Don't mark locations that don't have their own name. Include punctuation in the middle of names. Fictional locations can be included, as long as they're referred to by name (e.g. Hogwarts).</p><p>-Names of people (e.g. Virginia Wade). Don't mark people that don't have their own name. Include punctuation in the middle of names. Fictional people can be included, as long as they're referred to by name (e.g. Harry Potter).</p><p>-Name of products (e.g. iPhone). Don't mark products that don't have their own name. Include punctuation in the middle of names. Fictional products can be included, as long as they're referred to by name (e.g. Everlasting Gobstopper). It's got to be something you can touch, and it's got to be the official name. Table <ref type="table">7</ref>: An example of the inputs on WNUT-17 dataset.</p><p>Options composition for the WNUT-16 dataset -Compnay entities, or business entities, describes any organization formed to conduct business.</p><p>-Facility, or facilities are places, buildings, or equipments used for a particular purpose or activity.</p><p>-Geolocation refers to the use of location technologies such as GPS or IP addresses to identify and track the whereabouts of connected electronic devices.</p><p>-Musicartist is One who composes, conducts, or performs music, especially instrumental music.</p><p>-Other entities are entities other than company, facility, geolocation, music artist, person, product, sports team and tv show.</p><p>-Person entities are named persons or family.</p><p>-Product entities are name of products (e.g. iPhone**) which you can touch it, buy it and it's the technical or manufacturer name for it. Not inclduing products that don't have their own name. Include punctuation in the middle of names., -Sports team is a group of individuals who play sports (sports player).</p><p>-Tv show is any content produced for viewing on a television set which can be broadcast via over-the-air, satellite, or cable, excluding breaking news, advertisements, or trailers that are typically placed between shows.</p><p>Table <ref type="table">8</ref>: Specific composition of the options part on WNUT-16 dataset.</p><p>Option type: Def. from the Int.</p><p>-Corporate entities are business structures formed specifically to perform activities, such as running an enterprise or holding assets. Although it may be comprised of individual directors, officers, and shareholders, a corporation is a legal entity in and of itself.</p><p>-Creative work entities are performance, musical composition, exhibition, writing (poetry, fiction, script or other written literary forms), design, film, video, multimedia or other new media technologies and modes of presentation.</p><p>-Group entities are specific, unique names, or companies.</p><p>-Location entities are the name of politically or geographically defined locations such as cities, provinces, countries, international regions, bodies of water, mountains, etc.</p><p>-Person entities are named persons or family.</p><p>-Product entities are name of products (e.g. iPhone**) which you can touch it, buy it and it's the technical or manufacturer name for it. Not inclduing products that don't have their own name. Include punctuation in the middle of names.</p><p>Option type: Entity name only</p><p>Table <ref type="table">9</ref>: Specific composition of the three different option types on WNUT-17 dataset.</p><p>Option type: Def. from the Int.</p><p>-Person entities are named persons or family.</p><p>-Organization entities are limited to named corporate, governmental, or other organizational entities.</p><p>-Location entities are the name of politically or geographically defined locations such as cities, provinces, countries, international regions, bodies of water, mountains, etc.</p><p>-Miscellaneous entities include events, nationalities, products and works of art.</p><p>Option type: Entity name only -Person -Organization -Location -Miscellaneous</p><p>Table <ref type="table">10</ref>: Specific composition of the three different option types on CoNLL++ dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Content-based information retrieval by named entity recognition and verb semantic role labelling</title>
		<author>
			<persName><forename type="first">Betina</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mahalakshmi</surname></persName>
		</author>
		<author>
			<persName><surname>Suryanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Univers. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1830" to="1848" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading comprehension and reading strategies</title>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Baier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="323" to="335" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The effectiveness of reading strategies on reading comprehension</title>
		<author>
			<persName><forename type="first">Choosri</forename><surname>Banditvilai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Science and Humanity</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="46" to="50" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biomedical named entity recognition via knowledge guidance and question answering</title>
		<author>
			<persName><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuntal</forename><surname>Kumar Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murthy</forename><forename type="middle">V</forename><surname>Devarakonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Heal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Named entity recognition using BERT bilstm CRF for chinese electronic health records</title>
		<author>
			<persName><forename type="first">Zhenjin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xutao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CISP-BMEI</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fundamental factors of comprehension in reading</title>
		<author>
			<persName><forename type="first">Frederick</forename><forename type="middle">B</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf02288722</idno>
		<imprint>
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Results of the WNUT2017 shared task on novel and emerging entity recognition</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marieke</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NUT@EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NCBI disease corpus: A resource for disease name recognition and concept normalization</title>
		<author>
			<persName><forename type="first">Rezarta</forename><surname>Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Event extraction by answering (almost) natural questions</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="671" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spanner: Named entity re-/recognition as span prediction</title>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7183" to="7195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deberta: decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>ICLR. OpenReview.net</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07249</idno>
		<title level="m">Regularization for long named entity recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A survey on data cleaning methods for improved machine learning model performance</title>
		<author>
			<persName><forename type="first">Young</forename><surname>Ga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubna</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bakhtiyar</forename><surname>Alzamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Doskenov</surname></persName>
		</author>
		<author>
			<persName><surname>Termehchy</surname></persName>
		</author>
		<idno>CoRR, abs/2109.07127</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Biocreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database J. Biol. Databases Curation</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on deep learning for named entity recognition</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianglei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="70" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Xiangyang Liu, and Xipeng Qiu. 2021. A survey of transformers</title>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2106.04554</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Event extraction as machine reading comprehension</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1641" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">NER-BERT: A pre-trained model for lowresource entity tagging</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feijun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno>CoRR, abs/2112.00405</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gender and representation bias in GPT-3 generated stories</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Lucy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.nuse-1.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Narrative Understanding</title>
		<meeting>the Third Workshop on Narrative Understanding</meeting>
		<imprint>
			<publisher>Virtual. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Don&apos;t miss the labels: Label-semantic augmented meta-learner for few-shot text classification</title>
		<author>
			<persName><forename type="first">Qiaoyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.245</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2773" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An analysis of social biases present in BERT variants across multiple languages</title>
		<author>
			<persName><forename type="first">Aristides</forename><surname>Milios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parishad</forename><surname>Behnamghader</surname></persName>
		</author>
		<idno>CoRR, abs/2211.14402</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Label semantic aware pre-training for few-shot text classification</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8318" to="8334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bertweet: A pre-trained language model for english tweets</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (Demos)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Named entity recognition for social media texts with semantic augmentation</title>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhe</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1383" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-adaptive named entity recognition by retrieving unstructured knowledge</title>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<idno>CoRR, abs/2210.07523</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reinforcement-based denoising of distantly supervised NER with partial annotation</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Nooralahzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Tore</forename><surname>L?nning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilja</forename><surname>?vrelid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DeepLo@EMNLP-IJCNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLC@ACL</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<editor>Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F?vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Stella Biderman, Leo Gao, Thomas Wolf</pubPlace>
		</imprint>
	</monogr>
	<note>In ICLR. Open-Review. net</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inferner: an attentive model leveraging the sentence-level information for named entity recognition in microblogs</title>
		<author>
			<persName><forename type="first">Moemmur</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayesha</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel-Cyrille</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS</title>
		<imprint>
			<date type="published" when="2021">Ngonga Ngomo. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Bioflair: Pretrained pooled contextualized embeddings for biomedical sequence labeling tasks</title>
		<author>
			<persName><forename type="first">Shreyas</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Daniel</surname><genName>Jr</genName></persName>
		</author>
		<idno>CoRR, abs/1908.05760</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NER-MQMRC: formulating named entity recognition as multi question machine reading comprehension</title>
		<author>
			<persName><forename type="first">Anubhav</forename><surname>Shrimal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Promod</forename><surname>Yenigalla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (Industry Papers)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Results of the WNUT16 named entity recognition shared task</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bethany</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2016 Organizing Committee</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="138" to="144" />
		</imprint>
	</monogr>
	<note>NUT@COLING</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Biomedical named entity recognition using BERT in the machine reading comprehension framework</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Informatics</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">103799</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Reading comprehension, learning strategies and verbal reasoning: Possible relationships</title>
		<author>
			<persName><forename type="first">Ang?lica</forename><surname>Polvani Trassi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Lays Monteiro In?cio</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Katya Luciane De</forename><surname>Oliveira</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Lays Monteiro In?cio</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1590/1413-82712019240401</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Named entity and relation extraction with multi-modal retrieval</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (Findings)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5925" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">2021a. Automated concatenation of embeddings for structured prediction</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2643" to="2660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">2021b. Improving named entity recognition by external context retrieving and cooperative learning</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP (1)</title>
		<imprint>
			<biblScope unit="page" from="1800" to="1812" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Crossweigh: Training named entity tagger from imperfect annotations</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5153" to="5162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Crossweigh: Training named entity tagger from imperfect annotations</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5153" to="5162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Coarse-to-fine pre-training for named entity recognition</title>
		<author>
			<persName><forename type="first">Mengge</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6345" to="6354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A survey on recent advances in named entity recognition from deep learning models</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2145" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">LUKE: deep contextualized entity representations with entity-aware self-attention</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Zero-shot learners for natural language understanding via a unified multiple choice perspective</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruyi</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<idno>CoRR, abs/2210.08590</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">HRCA+: advanced multiple-choice machine reading comprehension method</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayato</forename><surname>Yamana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6059" to="6068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dual adversarial neural transfer for low-resource named entity recognition</title>
		<author>
			<persName><forename type="first">Joey Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick Siow Mong</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3461" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
