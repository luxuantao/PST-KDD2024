<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Captioning with Deep Bidirectional LSTMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Wang</surname></persName>
							<email>cheng.wang@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Hasso Plattner Institute</orgName>
								<orgName type="institution" key="instit2">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert-Str. 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haojin</forename><surname>Yang</surname></persName>
							<email>haojin.yang@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Hasso Plattner Institute</orgName>
								<orgName type="institution" key="instit2">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert-Str. 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Bartz</surname></persName>
							<email>christian.bartz@student.hpi.uni-potsdam.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Hasso Plattner Institute</orgName>
								<orgName type="institution" key="instit2">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert-Str. 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
							<email>christoph.meinel@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Hasso Plattner Institute</orgName>
								<orgName type="institution" key="instit2">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert-Str. 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Captioning with Deep Bidirectional LSTMs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6514F1009A0455749666196C0A28293A</idno>
					<idno type="DOI">10.1145/2964284.2964299</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts</term>
					<term>Computing methodologies → Natural language generation</term>
					<term>Neural networks</term>
					<term>Computer vision representations</term>
					<term>deep learning, LSTM, image captioning, visual-language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our model builds on a deep convolutional neural network (CNN) and two separate LSTM networks. It is capable of learning long term visual-language interactions by making use of history and future context information at high level semantic space. Two novel deep bidirectional variant models, in which we increase the depth of nonlinearity transition in different way, are proposed to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale and vertical mirror are proposed to prevent overfitting in training deep models. We visualize the evolution of bidirectional LSTM internal states over time and qualitatively analyze how our models "translate" image to sentence. Our proposed models are evaluated on caption generation and image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art results on caption generation even without integrating additional mechanism (e.g. object detection, attention model etc.) and significantly outperform recent methods on retrieval task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatically describe an image using sentence-level captions has been receiving much attention recent years <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b39">39]</ref>. It is a challenging task integrating Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.</p><p>MM <ref type="bibr">'16, October 15-19, 2016</ref>, Amsterdam, Netherlands c 2016 ACM. ISBN 978-1-4503-3603-1/16/10. . . $15.00 DOI: http://dx.doi.org/10.1145/2964284.2964299 visual and language understanding. It requires not only the recognition of visual objects in an image and the semantic interactions between objects, but the ability to capture visual-language interactions and learn how to "translate" the visual understanding to sensible sentence descriptions. The most important part of this visual-language modeling is to capture the semantic correlations across image and sentence by learning a multimodal joint model. While some previous models <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b26">26]</ref> have been proposed to address the problem of image captioning, they rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption. Those approaches usually suffer difficulty in generating variablelength and novel sentences. Recent work <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b39">39]</ref> has indicated that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) can yield promising results.</p><p>In this work, we propose novel architectures to the problem of image captioning. Different to previous models, we learn a visual-language space where sentence embeddings are encoded using bidirectional Long-Short Term Memory (Bi-LSTM) and visual embeddings are encoded with CNN. Bi-LSTM is able to summarize long range visual-language interactions from forward and backward directions. Inspired by the architectural depth of human brain, we also explore the deep bidirectional LSTM architectures to learn higher level visual-language embeddings. All proposed models can be trained in end-to-end by optimizing a joint loss.</p><p>Why bidirectional LSTMs? In unidirectional sentence generation, one general way of predicting next word wt with visual context I and history textual context w1:t-1 is maximize log P (wt|I, w1:t-1). While unidirectional model includes past context, it is still limited to retain future context wt+1:T that can be used for reasoning previous word wt by maximizing log P (wt|I, wt+1:T ). Bidirectional model tries to overcome the shortcomings that each unidirectional (forward and backward direction) model suffers on its own and exploits the past and future dependence to give a prediction. As shown in Figure <ref type="figure">1</ref>, two example images with bidirectionally generated sentences intuitively support our assumption that bidirectional captions are complementary, combining them can generate more sensible captions.</p><p>Why deeper LSTMs? The recent success of deep CNN in image classification and object detection <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b33">33]</ref> demonstrates that deep, hierarchical models can be more efficient at learning representation than shallower ones. This motivated our work to explore deeper LSTM architectures in the context of learning bidirectional visual-language embed-Figure <ref type="figure">1</ref>: Illustration of generated captions. Two example images from Flickr8K dataset and their best matching captions that generated in forward order (blue) and backward order (red). Bidirectional models capture different levels of visual-language interactions (more evidence see Sec.4.4). The final caption is the sentence with higher probabilities (histogram under sentence). In both examples, backward caption is selected as final caption for corresponding image.</p><p>dings. As claimed in <ref type="bibr" target="#b29">[29]</ref>, if we consider LSTM as a composition of multiple hidden layers that unfolded in time, LSTM is already deep network. But this is the way of increasing "horizontal depth" in which network weights W are reused at each time step and limited to learn more representative features as increasing the "vertical depth" of network. To design deep LSTM, one straightforward way is to stack multiple L-STM layers as hidden to hidden transition. Alternatively, instead of stacking multiple LSTM layers, we propose to add multilayer perception (MLP) as intermediate transition between LSTM layers. This can not only increase LSTM network depth, but can also prevent the parameter size from growing dramatically.</p><p>The core contributions of this work are threefold:</p><p>• We propose an end-to-end trainable multimodal bidirectional LSTM (see Sec.3.2) and its deeper variant models (see Sec.3.3) that embed image and sentence into a high level semantic space by exploiting both long term history and future context.</p><p>• We visualize the evolution of hidden states of bidirectional LSTM units to qualitatively analyze and understand how to generate sentence that conditioned by visual context information over time (see Sec.4.4).</p><p>• We demonstrate the effectiveness of proposed models on three benchmark datasets: Flickr8K, Flickr30K and MSCOCO. Our experimental results show that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art on caption generation (see Sec.4.5) and perform significantly better than recent methods on retrieval task (see Sec.4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Multimodal representation learning <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b35">35]</ref> has significant value in multimedia understanding and retrieval. The shared concept across modalities plays an important role in bridging the "semantic gap" of multimodal data. Image captioning falls into this general category of learning multimodal representations.</p><p>Recently, several approaches have been proposed for image captioning. We can roughly classify those methods into three categories. The first category is template based approaches that generate caption templates based on detecting objects and discovering attributes within image. For example, the work <ref type="bibr" target="#b20">[20]</ref> was proposed to parse a whole sentence into several phrases, and learn the relationships between phrases and objects within an image. In <ref type="bibr" target="#b15">[15]</ref>, conditional random field (CRF) was used to correspond objects, attributes and prepositions of image content and predict the best label. Other similar methods were presented in <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b16">16]</ref>. These methods are typically hard-designed and rely on fixed template, which mostly lead to poor performance in generating variable-length sentences. The second category is retrieval based approach, this sort of methods treat image captioning as retrieval task. By leveraging distance metric to retrieve similar captioned images, then modify and combine retrieved captions to generate caption <ref type="bibr" target="#b17">[17]</ref>. But these approaches generally need additional procedures such as modification and generalization process to fit image query.</p><p>Inspired by the success use of CNN <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b45">45]</ref> and Recurrent Neural Network <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>. The third category is emerged as neural network based methods <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42]</ref>. Our work also belongs to this category. Among those work, Kiro et al. <ref type="bibr" target="#b12">[12]</ref> can been as pioneer work to use neural network for image captioning with multimodal neural language model. In their follow up work <ref type="bibr" target="#b13">[13]</ref>, Kiro et al. introduced an encoder-decoder pipeline where sentence was encoded by LSTM and decoded with structure-content neural language model (SC-NLM). Socher et al. <ref type="bibr" target="#b34">[34]</ref> presented a DT-RNN (Dependency Tree-Recursive Neural Network) to embed sentence into a vector space in order to retrieve images. Later on, Mao et al. <ref type="bibr" target="#b23">[23]</ref> proposed m-RNN which replaces feed-forward neural language model in <ref type="bibr" target="#b13">[13]</ref>. Similar architectures were introduced in NIC <ref type="bibr" target="#b39">[39]</ref> and LRCN <ref type="bibr" target="#b4">[4]</ref>, both approaches use LSTM to learn text context. But NIC only feed visual information at first time step while Mao et al. <ref type="bibr" target="#b23">[23]</ref> and LRCN <ref type="bibr" target="#b4">[4]</ref>'s model consider image context at each time step. Another group of neural network based approaches has been introduced in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref> where image captions generated by integrating object detection with R-CNN (region-CNN) and inferring the alignment between image regions and descriptions.</p><p>Most recently, Fang et al. <ref type="bibr" target="#b5">[5]</ref> used multi-instance learning and traditional maximum-entropy language model for description generation. Chen et al. <ref type="bibr" target="#b2">[2]</ref> proposed to learn visual representation with RNN for generating image caption. In <ref type="bibr" target="#b42">[42]</ref>, Xu et al. introduced attention mechanism of human visual system into encoder-decoder framework. It is shown that attention model can visualize what the model "see" and yields significant improvements on image caption generation. Unlike those models, our deep LSTM model directly assumes the mapping relationship between visual-language is antisymmetric and dynamically learns long term bidirectional and hierarchical visual-language interactions. This is proved to be very effective in generation and retrieval tasks as we demonstrate in Sec.4.5 and Sec.4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL</head><p>In this section, we describe our multimodal bidirectional LSTM model (Bi-LSTM for short) and explore its deeper variants. We first briefly introduce LSTM which is at the center of model. The LSTM we used is described in <ref type="bibr" target="#b44">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Long Short Term Memory</head><p>Our model builds on LSTM cell, which is a particular form of traditional recurrent neural network (RNN). It has been successfully applied to machine translation <ref type="bibr" target="#b3">[3]</ref>, speech recognition <ref type="bibr" target="#b8">[8]</ref> and sequence learning <ref type="bibr" target="#b36">[36]</ref>. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, the reading and writing memory cell c is controlled by a group of sigmoid gates. At given time step t, LSTM receives inputs from different sources: current input xt, the previous hidden state of all LSTM units ht-1 as well as previous memory cell state ct-1. The updating of those gates at time step t for given inputs xt, ht-1 and ct-1 as follows.</p><formula xml:id="formula_0">it = σ(Wxixt + W hi ht-1 + bi) (1) ft = σ(W xf xt + W hf ht-1 + b f ) (2) ot = σ(Wxoxt + W ho ht-1 + bo) (3) gt = φ(Wxcxt + W hc ht-1 + bc) (4) ct = ft ct-1 + it gt (5) ht = ot φ(ct)<label>(6)</label></formula><p>where W are the weight matrices learned from the network and b are bias vectors. σ is the sigmoid activation function</p><formula xml:id="formula_1">σ(x) = 1/(1 + exp(-x)) and φ presents hyperbolic tangent φ(x) = (exp(x) -exp(-x))/(exp(x) + exp(-x))</formula><p>. denotes the products with a gate value. The LSTM hidden output ht={h tk } K k=0 , ht ∈ R K will be used to predict the next word by Softmax function with parameters Ws and bs:</p><formula xml:id="formula_2">F(pti; Ws, bs) = exp(Wshti + bs) K j=1 exp(Wshtj + bs)<label>(7)</label></formula><p>where pti is the probability distribution for predicted word.</p><p>Our key motivation of chosen LSTM is that it can learn long-term temporal activities and avoid quick exploding and vanishing problems that traditional RNN suffers from during back propagation optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bidirectional LSTM</head><p>In order to make use of both the past and future context information of a sentence in predicting word, we propose a Our model is end-to-end trainable by minimize a joint loss. bidirectional model by feeding sentence to LSTM from forward and backward order. Figure <ref type="figure" target="#fig_1">3</ref> presents the overview of our model, it is comprised of three modules: a CNN for encoding image inputs, a Text-LSTM (T-LSTM) for encoding sentence inputs, a Multimodal LSTM (M-LSTM) for embedding visual and textual vectors to a common semantic space and decoding to sentence. The bidirectional LSTM is implemented with two separate LSTM layers for computing forward hidden sequences -→ h and backward hidden sequences ←h . The forward LSTM starts at time t = 1 and the backward LSTM starts at time t = T . Formally, our model works as follows, for raw image input I, forward order sentence -→ S and backward order sentence ← -S , the encoding performs as</p><formula xml:id="formula_3">It = C( I; Θv) -→ h 1 t = T ( -→ E -→ S ; -→ Θ l ) ← - h 1 t = T ( ← - E ← - S ; ← - Θ l ) (8)</formula><p>where C, T represent CNN, T-LSTM respectively and Θv, Θ l are their corresponding weights.</p><p>-→ E and ← -E are bidirectional embedding matrices learned from network. Encoded visual and textual representations are then embedded to multimodal LSTM by: -→</p><formula xml:id="formula_4">h 2 t = M( -→ h 1 t , It; -→ Θm) ← - h 2 t = M( ← - h 1 t , It; ← - Θm) (9)</formula><p>where M presents M-LSTM and its weight Θm. M aims to capture the correlation of visual context and words at different time steps. We feed visual vector I to model at each time step for capturing strong visual-word correlation.</p><p>On the top of M-LSTM are Softmax layers which compute the probability distribution of next predicted word by</p><formula xml:id="formula_5">-→ p t+1 = F( -→ h 2 t ; Ws, bs) ← - p t+1 = F( ← - h 2 t ; Ws, bs)<label>(10)</label></formula><p>where p ∈ R K and K is the vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deeper LSTM architecture</head><p>To design deeper LSTM architectures, in addition to directly stack multiple LSTMs on each other that we named as Bi-S-LSTM (Figure <ref type="figure" target="#fig_2">4(c</ref>)), we propose to use a fully connected layer as intermediate transition layer. Our motivation comes from the finding of <ref type="bibr" target="#b29">[29]</ref>, in which DT(S)-RNN (deep transition RNN with shortcut) is designed by adding hidden to hidden multilayer perception (MLP) transition. It  </p><formula xml:id="formula_6">h l+1 t = F h (h l-1 t , h l t-1 ) = Uh l-1 t + Vh l t-1<label>(11)</label></formula><p>where h l t presents the hidden states of l-th layer at time t, U and V are matrices connect to transition layer (also see Figure <ref type="figure" target="#fig_3">5(L)</ref>). For readability, we consider one direction training and suppress bias terms. Similarly, in Bi-F-LSTM, to learn a hidden transition function F h by</p><formula xml:id="formula_7">h l+1 t = F h (h l-1 t ) = φr(Wh l-1 t ⊕ (V(Uh l-1 t )) (<label>12</label></formula><formula xml:id="formula_8">)</formula><p>where ⊕ is the operator that concatenates h l-1 t and its abstractions to a long hidden states (also see Figure <ref type="figure" target="#fig_3">5(R)</ref>). φr presents rectified linear unit (ReLu) activation function for transition layer, which performs φr(x) = max(0, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Augmentation</head><p>One of the most challenging aspects of training deep bidirectional LSTM models is preventing overfitting. Since our largest dataset has only 80K images <ref type="bibr" target="#b21">[21]</ref> which might cause overfitting easily, we adopted several techniques such as finetuning on pre-trained visual model, weight decay, dropout and early stopping that commonly used in the literature. Additionally, it has been proved that data augmentation such as randomly cropping and horizontal mirror <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b32">32]</ref>, adding noise, blur and rotation <ref type="bibr" target="#b40">[40]</ref> can effectively alleviate over-fitting and other. Inspired by this, we designed new data augmentation techniques to increase the number of image-sentence pairs. Our implementation performs on visual model, as follows:</p><p>• Multi-Corp: Instead of randomly cropping on input image, we crop at the four corners and center region.</p><p>Because we found random cropping is more tend to select center region and cause overfitting easily. By cropping four corners and center, the variations of network input can be increased to alleviate overfitting.</p><p>• • Vertical Mirror: Motivated by the effectiveness of widely used horizontal mirror, it is natural to also consider the vertical mirror of image for same purpose.</p><p>Those augmentation techniques are implemented in realtime fashion. Each input image is randomly transformed using one of augmentations to network input for training. In principle, our data augmentation can increase imagesentence training pairs by roughly 40 times (5×4×2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Inference</head><p>Our model is end-to-end trainable by using Stochastic Gradient Descent (SGD). The joint loss function L = -→ L + ← -L is computed by accumulating the Softmax losses of forward and backward directions. Our objective is to minimize L, which is equivalent to maximize the probabilities of correctly generated sentences. We compute the gradient L with Back-Propagation Through Time (BPTT) algorithm.</p><p>The trained model is used to predict a word wt with given image context I and previous word context w1:t-1 by P (wt|w1:t-1, I) in forward order, or by P (wt|wt+1:T , I) in backward order. We set w1=wT =0 at start point respectively for forward and backward directions. Ultimately, with generated sentences from two directions, we decide the final sentence for given image p(w1:T |I) according to the summation of word probability within sentence p(w1:T |I) = max( </p><p>Follow previous work, we adopted beam search to consider the best k candidate sentences at time t to infer the sentence at next time step. In our work, we fix k = 1 in all experiments although the average of 2 BLEU <ref type="bibr" target="#b28">[28]</ref> points better results can be achieved with k = 20 compare to k = 1 as reported in <ref type="bibr" target="#b39">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we design several groups of experiments to accomplish following objectives:</p><p>• Qualitatively analyze and understand how bidirectional multimodal LSTM learns to generate sentence conditioned by visual context information over time.</p><p>• Measure the benefits and performance of proposed bidirectional model and its deeper variant models that we increase their nonlinearity depth from different ways.</p><p>• Compare our approach with state-of-the-art methods in terms of sentence generation and image-sentence retrieval tasks on popular benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To validate the effectiveness, generality and robustness of our models, we conduct experiments on three benchmark datasets: Flickr8K <ref type="bibr" target="#b31">[31]</ref>, Flickr30K <ref type="bibr" target="#b43">[43]</ref> and MSCOCO <ref type="bibr" target="#b21">[21]</ref>.</p><p>Flickr8K. It consists of 8,000 images and each of them has 5 sentence-level captions. We follow the standard dataset divisions provided by authors, 6,000/1,000/1,000 images for training/validation/testing respectively.</p><p>Flickr30K. An extension version of Flickr8K. It has 31,783 images and each of them has 5 captions. We follow the public accessible 1 dataset divisions by Karpathy et al. <ref type="bibr" target="#b11">[11]</ref>. In this dataset splits, 29,000/1,000/1,000 images are used for training/validation/testing respectively.</p><p>MSCOCO. This is a recent released dataset that covers 82,783 images for training and 40,504 images for validation. Each of images has 5 sentence annotations. Since there is lack of standard splits, we also follow the splits provided by Karpathy et al. <ref type="bibr" target="#b11">[11]</ref>. Namely, 80,000 training images and 5,000 images for both validation and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Visual feature. We use two visual models for encoding image: Caffe <ref type="bibr" target="#b9">[9]</ref> reference model which is pre-trained with AlexNet <ref type="bibr" target="#b14">[14]</ref> and 16-layer VggNet model <ref type="bibr" target="#b33">[33]</ref>. We extract features from last fully connected layer and feed to train visual-language model with LSTM. Previous work <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b39">39]</ref> have demonstrated that with more powerful image models such as GoogleNet <ref type="bibr" target="#b37">[37]</ref> and VggNet <ref type="bibr" target="#b33">[33]</ref> can achieve promising improvements. To make a fair comparison with recent works, we select the widely used two models for experiments.</p><p>Textual feature. We first represent each word w within sentence as one-hot vector, w ∈ R K , K is vocabulary size built on training sentences and different for different datasets. By performing basic tokenization and removing the words that occurs less than 5 times in the training set, we have 2028, 7400 and 8801 words for Flickr8K, Flickr30K and MSCOCO dataset vocabularies respectively.</p><p>Our work uses the LSTM implementation of <ref type="bibr" target="#b4">[4]</ref> on Caffe framework. All of our experiments were conducted on Ubuntu 14.04, 16G RAM and single Titan X GPU with 12G 1 http://cs.stanford.edu/people/karpathy/deepimagesent/ memory. Our LSTMs use 1000 hidden units and weights initialized uniformly from [-0.08, 0.08]. The batch sizes are 150, 100, 100, 32 for Bi-LSTM, Bi-S-LSTM, Bi-F-LSTM and Bi-LSTM (VGG) models respectively. Models were trained with learning rate η = 0.01 (except η = 0.005 for Bi-LSTM (VGG)), weight decay λ is 0.0005 and we used momentum 0.9. Each model is trained for 18∼35 epochs with early stopping. The code for this work can be found at https:// github.com/ deepsemantic/ image captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We evaluate our models on two tasks: caption generation and image-sentence retrieval. In caption generation, we follow previous work to use BLEU-N (N=1,2,3,4) scores <ref type="bibr" target="#b28">[28]</ref>:</p><formula xml:id="formula_10">BN = min(1, e 1-r c ) • e 1 N N n=1 log pn<label>(16)</label></formula><p>where r, c are the length of reference sentence and generated sentence, pn is the modified n-gram precisions. We also report METETOR <ref type="bibr" target="#b18">[18]</ref> and CIDEr <ref type="bibr" target="#b38">[38]</ref> scores for further comparison. In image-sentence retrieval (image query sentence and vice versa), we adopt R@K (K=1,5,10) and Med r as evaluation metrics. R@K is the recall rate R at top K candidates and Med r is the median rank of the first retrieved ground-truth image and sentence. All mentioned metric scores are computed by MSCOCO caption evaluation server<ref type="foot" target="#foot_0">2</ref> , which is commonly used for image captioning challenge<ref type="foot" target="#foot_1">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization and Qualitative Analysis</head><p>The aim of this set experiment is to visualize the properties of proposed bidirectional LSTM model and explain how it works in generating sentence word by word over time.</p><p>First, we examine the temporal evolution of internal gate states and understand how bidirectional LSTM units retain valuable context information and attenuate unimportant information. Figure <ref type="figure" target="#fig_7">6</ref> shows input and output data, the pattern of three sigmoid gates (input, forget and output) as well as cell states. We can clearly see that dynamic states are periodically distilled to units from time step t = 0 to t = 11. At t = 0, the input data are sigmoid modulated to input gate i(t) where values lie within in [0,1]. At this step, the values of forget gates f (t) of different LSTM units are zeros. Along with the increasing of time step, forget gate starts to decide which unimportant information should be forgotten, meanwhile, to retain those useful information. Then the memory cell states c(t) and output gate o(t) gradually absorb the valuable context information over time and make a rich representation h(t) of the output data.</p><p>Next, we examine how visual and textual features are embedded to common semantic space and used to predict word over time. Figure <ref type="figure">7</ref> shows the evolution of hidden units at different layers. For T-LSTM layer, units are conditioned by textual context from the past and future. It performs as the encoder of forward and backward sentences. At M-LSTM layer, LSTM units are conditioned by both visual and textual context. It learns the correlations between input word sequence and visual information that encoded by CNN. At given time step, by removing unimportant information that make less contribution to correlate input word and visual context, the units tend to appear sparsity pattern    <ref type="figure">7</ref>: Pattern of the first 96 hidden units chosen at each layer of Bi-LSTM in both forward and backward directions. The vertical axis presents time steps. The horizontal axis corresponds to different LSTM units. In this example, we visualize the T-LSTM layer for text only, the M-LSTM layer for both text and image and Softmax layer for computing word probability distribution. The model was trained on Flickr 30K dataset for generating sentence word by word at each time step. In (g), we provide the predicted words at different time steps and their corresponding index in vocabulary where we can also read from (e) and (f) (the highlight point at each row). Word with highest probability is selected as the predicted word. and learn more discriminative representations from inputs. At higher layer, embedded multimodal representations are used to compute the probability distribution of next predict word with Softmax. It should be noted, for given image, the number of words in generated sentence from forward and backward direction can be different. Figure <ref type="figure" target="#fig_10">8</ref> presents some example images with generated captions. From it we found some interesting patterns of bidirectional captions: (1) Cover different semantics, for example, in (b) forward sentence captures "couch" and "table" while backward one describes "chairs" and "table". (2) Describe static scenario and infer dynamics, in (a) and (d), one caption describes the static scene, and the other one presents the potential action or motion that possibly happen in the next time step. (3) Generate novel sentences, from generated captions, we found that a significant proportion (88% by randomly select 1000 images on MSCOCO validation set) of generated sentences are novel (not appear in training set). But generated sentences are highly similar to ground-truth captions, for example in (d), forward caption is similar to one of ground-truth captions ("A passenger train that is pulling into a station") and backward caption is similar to ground-truth caption ("a train is in a tunnel by a station"). It illustrates that our model has strong capability in learning visual-language correlation and generating novel sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on Caption Generation</head><p>Now, we compare with state-of-the-art methods. Table <ref type="table">1</ref> presents the comparison results in terms of BLEU-N. Our A woman in a tennis court holding a tennis racket.</p><p>A woman getting ready to hit a tennis ball.</p><p>A living room with a couch and a table.</p><p>Two chairs and a table in a living room.</p><p>A giraffe standing in a zoo enclosure with a baby in the background.</p><p>A couple of giraffes are standing at a zoo.</p><p>A train is pulling into a train station.</p><p>A train on the tracks at a train station.  <ref type="formula">13</ref>) which selects the sentence with the higher probability. The final captions are marked in bold.</p><p>Table <ref type="table">1</ref>: Performance comparison on BLEU-N(high score is good). The superscript "A" means the visual model is AlexNet (or similar network), "V" is VggNet, "G" is GoogleNet, "-" indicates unknown value, " ‡" means different data splits 4 . The best results are marked in bold and the second best results with underline. The superscripts are also applicable to Table <ref type="table">2</ref>. approach achieves very competitive performance on evaluated datasets although with less powerful AlexNet visual model. We can see that increase the depth of LSTM is beneficial on generation task. Deeper variant models mostly obtain better performance compare to Bi-LSTM, but they are inferior to latter one in B-3 and B-4 on Flickr8K. We conjecture it should be the reason that Flick8K is a relatively small dataset which suffers difficulty in training deep models with limited data. One of interesting facts we found is that by stacking multiple LSTM layers is generally superior to LSTM with fully connected transition layer although Bi-S-LSTM needs more training time. By replacing AlexNet with VggNet brings significant improvements on all BLEU evaluation metrics. We should be aware of that a recent interesting work <ref type="bibr" target="#b42">[42]</ref> achieves the best results by integrating attention mechanism <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b42">42]</ref> on this task. Although we believe incorporating such powerful mechanism into our framework can make further improvements, note that our current model Bi-LSTM V achieves the best or second best results on most of metrics while the small gap in performance between our model and Hard-Attention V <ref type="bibr" target="#b42">[42]</ref> is existed. The further comparison on METEOR and CIDEr scores is plotted in Figure <ref type="figure" target="#fig_11">9</ref>. Without integrating object detection and more powerful vision model, our model (Bi-LSTM A ) 4 On MSCOCO dataset, NIC uses 4K images for validation and test. LRCN randomly selects 5K images from MSCO-CO validation set for validation and test. m-RNN uses 4K images for validation and 1K as test. </p><formula xml:id="formula_11">Flickr8K Flickr30K MSCOCO Models B-1 B-2 B-3 B-4 B-1 B-2 B-3 B-4 B-1 B-2 B-3 B-4 NIC[39] G, ‡<label>63</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results on Image-Sentence Retrieval</head><p>For retrieval evaluation, we focus on image to sentence retrieval and vice versa. This is an instance of cross-modal retrieval <ref type="bibr">[6,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b41">41]</ref> which has been a hot research subject in multimedia field. Table <ref type="table">2</ref> illustrates our results on different datasets. The performance of our models exceeds those compared methods on most of metrics or matching existing results. In a few metrics, our model didn't show better result than Mind's Eye <ref type="bibr" target="#b2">[2]</ref> which combined image and text features in ranking (it makes this task more like multimodal retrieval) and NIC <ref type="bibr" target="#b39">[39]</ref> which employed more powerful vision Table <ref type="table">2</ref>: Comparison with state-of-the-art methods on R@K (high is good) and Med r (low is good). All scores are computed by averaging the results of forward and backward results. "+O" means the approach with additional object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image to Sentence</head><p>Sentence to Image Datasets Methods R@1 R@5 R@10 Med r R@1 R@5 R@10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Discussion</head><p>Efficiency. In addition to showing superior performance, our models also possess high computational efficiency. Table <ref type="table" target="#tab_2">3</ref> presents the computational costs of proposed models. We randomly select 10 images from Flickr8K validation set, and perform caption generation and image to sentence retrieval  <ref type="table">1</ref>, 2 and 3, deep models have only slightly higher time consumption but yield significant improvements and our proposed Bi-F-LSTM can strike the balance between performance and efficiency.</p><p>Challenges in exact comparison. It is challenging to make a direct, extract comparison with related methods due to the differences in dataset division on MSCOCO. In principle, testing on smaller validation set can lead to better results, particularly in retrieval task. Since we strictly follow dataset splits as in <ref type="bibr" target="#b11">[11]</ref>, we compare to it in most cases. Another challenge is the visual model that utilized for encoding image inputs. Different models are employed in different works, to make a fair and comprehensive comparison, we select commonly used AlexNet and VggNet in our work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We proposed a bidirectional LSTM model that generates descriptive sentence for image by taking both history and future context into account. We further designed deep bidirectional LSTM architectures to embed image and sentence at high semantic space for learning visual-language models. We also qualitatively visualized internal states of proposed model to understand how multimodal LSTM generates word at consecutive time steps. The effectiveness, generality and robustness of proposed models were evaluated on numerous datasets. Our models achieve highly completive or stateof-the-art results on both generation and retrieval tasks. Our future work will focus on exploring more sophisticated language representation (e.g. word2vec) and incorporating multitask learning and attention mechanism into our model. We also plan to apply our model to other sequence learning tasks such as text recognition and video captioning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Long Short Term Memory (LSTM) cell. It is consist of an input gate i, a forget gate f , a memory cell c and an output gate o. The input gate decides let incoming signal go through to memory cell or block it. The output gate can allow new output or prevent it. The forget gate decides to remember or forget cell's previous state. Updating cell states is performed by feeding previous cell output to itself by recurrent connections in two consecutive time steps.</figDesc><graphic coords="3,118.12,53.80,110.46,115.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Multimodal Bidirectional LSTM. L1: sentence embedding layer. L2: T-LSTM layer. L3: M-LSTM layer. L4: Softmax layer. We feed sentence in both forward (blue arrows) and backward (red arrows) order which allows our model summarizes context information from both left and right side for generating sentence word by word over time. Our model is end-to-end trainable by minimize a joint loss.</figDesc><graphic coords="3,320.88,53.80,230.98,129.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustrations of proposed deep architectures for image captioning. The network in (a) is commonly used in previous work, e.g. [4, 23]. (b) Our proposed Bidirectional LSTM (Bi-LSTM). (c) Our proposed Bidirectional Stacked LSTM (Bi-S-LSTM). (d) Our proposed Bidirectional LSTM with fully connected (FC) transition layer (Bi-F-LSTM).</figDesc><graphic coords="4,53.80,53.80,502.12,99.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Transition for Bi-S-LSTM(L) and Bi-F-LSTM(R)</figDesc><graphic coords="4,85.48,212.94,175.74,97.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Multi-Scale: To further increase the number of imagesentence pairs, we rescale input image to multiple scales. For each input image I with size H × W , it is resized to 256 × 256, then we randomly select a region with size of s * H×s * W , where s ∈ [1, 0.925, 0.875, 0.85] is scale ratio. s = 1 means we do not multi-scale operation on given image. Finally we resize it to AlexNet input size 227 ×227 or VggNet input size 224 × 224.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>-→ p (wt|I)), T t=1 ( ←p (wt|I))) (13) -→ p (wt|I) = T t=1 p(wt|w1, w2, ..., wt-1, I) (14) ←p (wt|I) = T t=1 p(wt|wt+1, wt+2, ..., wT , I)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) input (b) input gate (c) forget gate (d) cell state (e) output gate (f) output</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of LSTM cell. The horizontal axis corresponds to time steps. The vertical axis is cell index. Here we visualize the gates and cell states of the first 32 Bi-LSTM units of T-LSTM in forward direction over 11 time steps.</figDesc><graphic coords="6,78.47,53.80,70.86,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure</head><label></label><figDesc>Figure7: Pattern of the first 96 hidden units chosen at each layer of Bi-LSTM in both forward and backward directions. The vertical axis presents time steps. The horizontal axis corresponds to different LSTM units. In this example, we visualize the T-LSTM layer for text only, the M-LSTM layer for both text and image and Softmax layer for computing word probability distribution. The model was trained on Flickr 30K dataset for generating sentence word by word at each time step. In (g), we provide the predicted words at different time steps and their corresponding index in vocabulary where we can also read from (e) and (f) (the highlight point at each row). Word with highest probability is selected as the predicted word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Examples of generated captions for given query image on MSCOCO validation set. Blue-colored captions are generated in forward direction and red-colored captions are generated in backward direction. The final caption is selected according to equation (13) which selects the sentence with the higher probability. The final captions are marked in bold.</figDesc><graphic coords="7,450.95,67.95,79.37,70.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: METEOR/CIDEr scores on different datasets.</figDesc><graphic coords="7,320.32,477.22,112.98,87.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>preparing a football for a field goal kick, while his teammates can coach watch him Black dog jumping out of the water with a stick in his mouth A black dog jumps in a body of water with a stick in his mouth A black dog is swimming through water A black dog swimming in the water with a tennis ball in his mouth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Examples of image retrieval (top) and caption retrieval (bottom) with Bi-S-LSTM on Flickr30K validation set. Queries are marked with red color and top-4 retrieved results are marked with green color.</figDesc><graphic coords="9,65.43,271.02,85.04,56.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Time costs for testing 10 images on Flickr8K The table shows the averaged time costs across 5 test results. The time cost of network initialization is excluded. The costs of caption generation includes: computing image feature, sampling bidirectional captions, computing the final caption. The time costs for retrieval considers: computing image-sentence pair scores (totally 10 × 50 pairs), ranking sentences for each image query. As can be seen from Table</figDesc><table><row><cell></cell><cell cols="3">Bi-LSTM Bi-S-LSTM Bi-F-LSTM</cell></row><row><cell>Generation</cell><cell>0.93s</cell><cell>1.1s</cell><cell>0.97s</cell></row><row><cell>Retrieval</cell><cell>5.62s</cell><cell>7.46s</cell><cell>5.69s</cell></row><row><cell cols="2">test for 5 times respectively.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/tylin/coco-caption</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://mscoco.org/home/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2422" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval with correspondence autoencoder</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6645" to="6649" />
			<date type="published" when="2013">2013</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collective generation of natural image descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Treetalk: Composition and compression of trees for image descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. of the Association for Computational Linguistics(TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="351" to="362" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meteor universal: language specific translation evaluation for any target language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page">376</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Composing simple image descriptions using web-scale n-grams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="220" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rapid: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In INTERSPEECH</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generating image descriptions from computer vision detections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Midge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="747" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6026</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the role of correlation and abstraction in cross-modal multimedia retrieval</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collecting image annotations using amazon&apos;s mechanical turk</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT Workshop</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. of the Association for Computational Linguistics(TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepfont: Identify your font from an image</title>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep compositional cross-modal learning to rank via local-global alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. of the Association for Computational Linguistics(TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
