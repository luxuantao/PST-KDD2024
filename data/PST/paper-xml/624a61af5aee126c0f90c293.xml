<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-01">1 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Welker</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aveek</forename><surname>Purohit</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Ryoo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Johnny</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pete</forename><forename type="middle">Florence</forename><surname>Google</surname></persName>
						</author>
						<title level="a" type="main">Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-01">1 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.00598v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large foundation models can exhibit unique capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internetscale text with no images (e.g. from spreadsheets, to SAT questions). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this model diversity is symbiotic, and can be leveraged to build AI systems with structured Socratic dialogue -in which new multimodal tasks are formulated as a guided languagebased exchange between different pre-existing foundation models, without additional finetuning. In the context of egocentric perception, we present a case study of Socratic Models (SMs) that can provide meaningful results for complex tasks such as generating freeform answers to contextual questions about egocentric video, by formulating video Q&amp;A as short story Q&amp;A, i.e. summarizing the video into a short story, then answering questions about it. Additionally, SMs can generate captions for Internet images, and are competitive with state-of-the-art on zero-shot video-to-text retrieval with 42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models zeroshot to capture new multimodal functionalities, without domain-specific data collection. Prototypes are available at socraticmodels.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Foundation models <ref type="bibr" target="#b8">(Bommasani et al. 2021</ref>) (e.g., BERT, GPT-3, CLIP) have enabled impressive capabilities in recent years: from zero-shot image classification <ref type="bibr" target="#b65">(Radford et al. 2021;</ref><ref type="bibr">Li et al. 2021a)</ref>, to high-level planning <ref type="bibr" target="#b32">(Huang et al. 2022;</ref><ref type="bibr" target="#b2">Ahn et al. 2022</ref>). These capabilities depend on their training data distribution -and while they may be generic or indiscriminately crawled from the web, their distributions remain distinct across domains. For example, in terms of linguistic data, visual-language models (VLMs) <ref type="bibr" target="#b86">(Wang et al. 2021;</ref><ref type="bibr" target="#b32">Jain et al. 2021</ref>) are trained on image and video captions, but large language models (LMs) <ref type="bibr" target="#b19">(Devlin et al. 2018;</ref><ref type="bibr" target="#b84">Thoppilan et al. 2022;</ref><ref type="bibr" target="#b11">Chen et al. 2021)</ref> are additionally trained on a large corpora of other data such as spreadsheets, fictional novels, and standardized test questions. These different domains offer distinct commonsense knowledge: VLMs can ground text to visual content, but LMs can perform a variety of other linguistic tasks (e.g., answer-Fig. <ref type="figure">1</ref>: Language-based foundation models trained across different domains learn complementary forms of commonsense. Language is an intermediate representation by which these models can communicate with each other to generate joint predictions for new multimodal tasks, without finetuning. New applications (e.g., augmented reality (AR), human feedback, robotics) can be viewed as adding participants to the multi-model discussion. In this paper, we study inter-model dialogue in the context of multimodal reasoning.</p><p>ing reading comprehension questions <ref type="bibr" target="#b66">(Rajpurkar, Jia, and Liang 2018)</ref>) that to date have not been demonstrated with VLMs alone. In this work, we propose that these model differences are complementary, and can be jointly leveraged to build AI systems with structured Socratic dialogue -in which new multimodal tasks are formulated as a guided exchange between different pre-existing language-based foundation models, without additional finetuning. Rather than scaling training data in the areas of overlap (e.g., alt-text captions <ref type="bibr" target="#b32">(Jia et al. 2021</ref>)), or unifying model architectures <ref type="bibr" target="#b31">(Hu and Singh 2021)</ref>, Socratic Models 1 (SMs) are a class of systems that embrace the zero-shot capabilities of foundation models by engineering guided discussions between the independent models to reach a shared consensus on a taskspecific output. SMs use language as the representation by which inter-domain foundation models can jointly be used for inference. Fig. <ref type="figure">2</ref>: In this work we propose Socratic Models (SMs), a framework that uses structured dialogue between pre-existing foundation models, each of which can exhibit unique (but complementary) capabilities depending on the distributions of data on which they are trained. On various perceptual tasks (shown), this work presents a case study of SMs with visual language models (VLMs, e.g., CLIP), large language models (LMs, e.g., <ref type="bibr">RoBERTa)</ref>, and audio language models (ALMs, e.g., Wav2CLIP, Speech2Text). From video search, to image captioning; from generating free-form answers to contextual reasoning questions, to forecasting future activities -SMs can provide meaningful results for complex tasks across classically challenging computer vision domains, without any model finetuning.</p><p>Across a number of tasks spanning vision, language, and audio modalities, we find that specific instantiations of SMs, using LMs together with VLMs and audio-language models (ALMs), can generate results on challenging perceptual tasks (examples in Fig. <ref type="figure">2</ref>) that are often coherent and correct. We present results on Internet image captioning (Sec. 4) and the common video understanding task of videoto-text retrieval (Sec. 5), but our highlighted application is open-ended reasoning in the context of egocentric perception (Fig. <ref type="figure" target="#fig_1">4</ref>) -from answering free-form contextual reasoning questions about first-person videos (e.g. "why did I go to the front porch today?"), to forecasting events into the future with commonsense (e.g. "what will I do 3 hours from now?"). Our egocentric SM system consists of two primary components, each of which benefits from multimodal multimodel discussions: (i) assembling video into a languagebased world-state history, i.e. a story or event log, then (ii) performing various types of open-ended text-prompted tasks based on that world-state history. We find that simple scripted policies to guide a closed-loop exchange between pre-trained LM, VLM, and ALM models can (a) generate meaningful captions that respond to questions like "what am I doing?" with answers like "receiving a package" that span beyond the label set of standard vision datasets <ref type="bibr" target="#b77">(Sigurdsson et al. 2018;</ref><ref type="bibr" target="#b54">Smaira et al. 2020), and (b)</ref> exhibit open-ended contextual Q&amp;A capabilities previously thought to be out-of-reach for egocentric perception without domain-specific data collection <ref type="bibr" target="#b26">(Grauman et al. 2021;</ref><ref type="bibr" target="#b16">Damen et al. 2020)</ref>.</p><p>The goal of this paper is (1) to discuss new perspectives on building AI systems that embrace the heterogeneity of language-based foundation models through structured Socratic dialogue, and (2) give example demonstrations of what is already possible today with SMs on challenging perceptual tasks. Specifically, our contributions include (i) the Socratic Models framework, (ii) demonstration of an egocentric perception system using Socratic Models, (iii) qualitative results on video understanding (synthesizing video snippets from a full day of activity) that is not covered by existing benchmark datasets, (iv) qualitative comparisons to a state-of-the-art model <ref type="bibr" target="#b56">(Mokady, Hertz, and Bermano 2021)</ref> on the task of single-image captioning in egocentric and Internet image domains, (v) quantitative comparisons to state-of-the-art video understanding models on the popular MSR-VTT <ref type="bibr" target="#b92">(Xu et al. 2016;</ref><ref type="bibr" target="#b95">Yu, Kim, and Kim 2018)</ref> dataset for video-to-text retrieval, and (vi) a framework for unsupervised quantitative model selection of Socratic Models through sub-model ablations.</p><p>Overall, these ideas shed light on promising new opportunities to build simple systems for general applications that compose foundation models out-of-the-box. By construction, multimodal foundation models are likely to be trained on different distributions of Internet data, resulting in different test time capabilities. These capabilities can be improved for a given target distribution through finetuning, but at the cost of generality and robustness to distribution shifts <ref type="bibr" target="#b89">(Wortsman et al. 2021)</ref>. SMs offer an alternative approach in which these capabilities can be integrated cooperatively, and in which we can make use of concepts in domain A that are more easily obtained from domain B without complex alignment of the representations, i.e., through additional large large-scale training across multiple domains. Instead, the common representation is language, and it may be used to compose existing models, in a zero-shot manner.</p><p>Of course, our demonstrated SM systems are not without their limitations. We discuss these limitations, such as unreliability inherited from the foundation models on which they are constructed, together with other potential broader impacts (Sec. 8.1). Socratic Models (SMs) is a framework (Fig. <ref type="figure">2</ref>) in which multiple pretrained language-interactable foundation models are composed (Fig. <ref type="figure" target="#fig_0">3</ref>) zero-shot to perform new downstream tasks e.g., egocentric perception (our primary highlight in this work), as well as Internet data image captioning and video understanding (i.e., video-to-text retrieval). While a common trend in multimodal learning has been to seek embedding spaces in which multiple modalities may co-exist, we instead compose multiple models with language. While we may leverage submodels built with shared embedding spaces, language is the representation itself upon which multiple of these models, as well as other language-interactable models, may interact. In closed-loop, multi-model interactions can perform joint inference. These guided multi-model exchanges are best described through examples, as provided in Sections 3. <ref type="bibr">3, 3.4, 4, and 5.</ref> This work explores the use of several classes of models, each capturing a different domain or functionality. For visualization purposes, outputs from LMs are blue, VLMs green, ALMs purple, other prompt text is gray, LM completions on a shown prompt are bolded in blue, and user inputs are magenta.</p><p>Large Language Models (LMs) at the scale of 100B or more parameters trained on Internet-scale data e.g., <ref type="bibr">GPT-3 (Brown et al. 2020</ref>) have shown to achieve strong zero-shot performance on many NLP tasks including dialogue <ref type="bibr" target="#b84">(Thoppilan et al. 2022)</ref>, coding <ref type="bibr" target="#b11">(Chen et al. 2021)</ref>, and reasoning <ref type="bibr" target="#b88">(Wei et al. 2022)</ref>. LMs can generate language sequences autoregressively, or compute similarity measures between sentence embeddings <ref type="bibr" target="#b19">(Devlin et al. 2018</ref>). In the SMs framework, LMs (and the knowledge that they store) are used to perform contextual reasoning and Q&amp;A, conditioned on the information provided by other models. This reasoning may in turn be subsequently checked by other models as well.</p><p>Visual-Language Models (VLMs) such <ref type="bibr" target="#b65">(Radford et al. 2021;</ref><ref type="bibr" target="#b32">Jia et al. 2021;</ref><ref type="bibr">Li et al. 2021a;</ref><ref type="bibr" target="#b32">Jain et al. 2021;</ref><ref type="bibr" target="#b97">Zhai et al. 2021</ref>) learn models that map images and text into a shared feature embedding space. VLMs can serve as image-text critics that can rank multiple text suggestions (either generic visual entity categories from existing datasets, or outputs from the LM) against a given image using cosine similarity between features. Functionally, this operates as an open-set vocabulary image classifier, which can be replaced with domain-specific alternatives e.g., <ref type="bibr" target="#b29">(He et al. 2017)</ref>. VLMs are often trained on filtered image alt-text HTML pairs crawled from the Internet <ref type="bibr" target="#b74">(Sharma et al. 2018</ref>). In the SMs framework, VLMs are used to detect visual entities, across various categories, from large pre-existing dictionaries, and these detections can both inform and check the work of LMs used for reasoning. (Note: we indicate a VLM checking an LM's work, for example through ranking, with green-underlined blue text.)</p><p>Audio-Language Models (ALMs) ground audio information into language, and can take the form of speech-to-text models, or audio-text critics <ref type="bibr">(Wu et al. 2021a</ref>; Zhao et al. 2021) that learn a semantic similarity measure between audio and text. We use both types of models to capture information from the audio domain (e.g., to rank generic sound categories such as "running water" against audio snippets, or to directly transcribe speech from audio inputs). (Note: we indicate a VLM checking an LM's work, for example through ranking, with purple-underlined blue text.)</p><p>Composing Multimodal Models. There are many options of foundation models to choose from, but the examples we showcase in the following sections use models that are publicly available, so that our SM systems can be made accessible to the community. In particular, we use CLIP <ref type="bibr" target="#b65">(Radford et al. 2021)</ref> as the VLM (for egocentric perception, ViT-L/14 with 428M params); Wav2CLIP <ref type="bibr">(Wu et al. 2021a</ref>) as the sound-critic ALM and Google Cloud Speech-to-text API as the speech-to-text ALM; and GPT-3 with 175B params <ref type="bibr" target="#b9">(Brown et al. 2020;</ref><ref type="bibr" target="#b59">Ouyang et al. 2022)</ref> and RoBERTa <ref type="bibr">(Liu et al. 2019b</ref>) with 355M params as the LMs. All pretrained models are used off-the-shelf with no additional finetuning. In all our example systems shown, the composed foundation models are used zero-shot, and the interactions between these models are scripted. While in future work we are excited to explore the possibility of learning the interactions themselves, we also have found that there are practical benefits of harnessing a framework in which zero applicationspecific learning is performed. In particular, new applications can be quickly retargeted by a small amount of creative programming, with no training time or compute resources required. Additionally, the zero-shot capabilities may especially benefit domains in which training data is expensive to acquire (e.g., egocentric perception for AR, or robotics). The goal of this work is to demonstrate the capabilities available today in multimodal domains with Socratic Models.</p><p>In the following sections, we showcase example SM systems and results for egocentric perception (Sec. 3), generic Internet data image captioning (Sec. 4), and Internet data video-to-text retrieval (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Socratic Models for Egocentric Perception</head><p>Here we describe an SM system for egocentric perception. We first provide a brief system overview (Section 3.1) and address "why egocentric perception?" (Section 3.2), then describe and show results for two primary system components: (i) compiling language-based world-state history (Section 3.3), and (ii) performing open-ended reasoning and video search on egocentric video (Section 3.4). We also explore methods to scale up video search capabilities (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview: Socratic Egocentric Perception</head><p>Overall, our system formulates egocentric video understanding as SM-guided discussions between different foundation models spanning multiple modalities. In general, our specific system can be thought of as a case study in the ways in which Socratic Models can be composed. Further, a goal of our egocentric perception system is to demonstrate progress towards performing unconstrained video understanding of egocentric videos. The input to this system is a video and an interactive sequence of natural language prompts (dialogue) from a user. For each prompt, depending on its type, the output is either a textual answer, and/or relevant frame(s) or video snippets from the video.</p><p>To this end, a key component of our system is using Socratic dialogue (Fig. <ref type="figure">5</ref>) to turn key moments from an egocentric video into a language-based summary, which we term a language-based world-state history (Fig. <ref type="figure" target="#fig_1">4</ref>, middle). This world-state history can then aid in prompting an LM to perform various reasoning tasks (Fig. <ref type="figure" target="#fig_1">4</ref>, bottom) via text completion. In contrast to common paradigms for video Q&amp;A, which may involve supervising video-to-text models on labeled datasets or with adversarial training (we refer to (Patel, Parikh, and Shastri 2021) for a recent survey), our approach simply maintains a language-based reconstruction of the video, generated by independently captioning key video frames, then recursively summarizing <ref type="bibr">(Wu et al. 2021b)</ref> them to fit as input context to an LM (example in Fig. <ref type="figure">5</ref>).</p><p>For open-ended reasoning, a key aspect of our approach is formulating video understanding as reading comprehension, i.e. re-frame "video Q&amp;A" as a "short story Q&amp;A" problem, where the language-based world-state history can be viewed as an interpretable state representation in the form of short stories, which can be used for reading comprehension Q&amp;A, for which LMs have demonstrated strong zero-shot performance <ref type="bibr" target="#b9">(Brown et al. 2020)</ref>. Drawing analogies to 3D vision and robotics, this can be thought of as building an on-thefly reconstruction of the sequence of events in the observable world with language, rather than other representations, such as dynamically-updated 3D meshes <ref type="bibr" target="#b32">(Izadi et al. 2011)</ref> or neural fields <ref type="bibr" target="#b83">(Tancik et al. 2022).</ref> In addition to openended question-answering in the form of text, we can also provide video search capabilities (in the form of image or audio retrieval, Fig <ref type="figure" target="#fig_3">7</ref>) from natural language questions through zero-shot composition of SMs as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why Egocentric Perception?</head><p>We highlight SMs on egocentric perception because it is an important yet challenging computer vision domain <ref type="bibr" target="#b26">(Grauman et al. 2021;</ref><ref type="bibr" target="#b16">Damen et al. 2020;</ref><ref type="bibr" target="#b77">Sigurdsson et al. 2018)</ref> with downstream applications in augmented reality (AR) and robotics <ref type="bibr" target="#b2">(Ahn et al. 2022)</ref>. From unusual viewpoints to the lack of temporal curation -the characteristics of firstperson videos are unique and not often found in existing datasets, which focus more on generic Internet content captured from third-person spectator views <ref type="bibr" target="#b18">(Deng et al. 2009;</ref><ref type="bibr" target="#b47">Lin et al. 2014;</ref><ref type="bibr" target="#b74">Sharma et al. 2018)</ref>. Notably, this domain shift makes it difficult for data-driven egocentric models to benefit from the standard paradigm of pretraining on third person Internet data <ref type="bibr">(Li et al. 2021b;</ref><ref type="bibr" target="#b77">Sigurdsson et al. 2018)</ref>. See Related Work (Sec. 7) for a more detailed discussion on prior work in egocentric perception -overall, the key challenges for the field have included how to acquire sufficient egocentric data, and/or how to make sufficient use of this data (either with dense labels, or otherwise).</p><p>Despite the challenges of egocentric perception, we find that SMs can reconcile the complementary strengths of pretrained foundation models to address these difficulties through contextual reasoning. For example, while modern activity recognition models trained on third person data might over-index to the motion of the primary person in video (making the models difficult to be adapted to firstperson videos), we find that LMs like GPT-3 can suggest equally plausible activities (e.g., "receiving a package") that may be occurring given only a brief description of the scene (e.g., "front porch") and the objects detected in the image ("package, driveway, door") by a VLM. These activity suggestions are often more expressive than the class categories that can be found in typical activity recognition datasets (e.g., Charades <ref type="bibr" target="#b77">(Sigurdsson et al. 2018)</ref>, Kinetics <ref type="bibr" target="#b78">(Smaira et al. 2020)</ref>), and reflect the information already stored in the model, agnostic to the point of view. Our SM system for egocentric perception leverages these advantages, and also suggests future research directions in contextual reasoning that leverage existing language-based models without having to curate large annotated datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Language-Based World-State History from Video</head><p>In order to provide language-based reasoning capabilities for open-ended question-answering, a key aspect of our system is to describe the observed states of the world in language, with the goal of creating a language-based world-state history (Fig. <ref type="figure" target="#fig_1">4</ref>) that can be used as context to an LM. To this end, a component of our method generates Socratic image summaries of individual video frames (Sec. 3.3-A), that can then be concatenated (along with timestamps) to form an event log (illustrated at the top and middle of Fig. <ref type="figure" target="#fig_1">4</ref>).</p><p>3.3-A. Socratic Egocentric Image Summaries. Given an image frame as input, this component generates a natural language summary (e.g., caption) of what is occurring in the image. Our system uses a Socratic approach with guided multimodal multi-model discussion to provide answers to 3 questions that describe the visual scene: "where am I?", "what do I see?", and "what am I doing?", which are then summarized into a single caption per image frame.</p><p>? Where am I? For place recognition, we use a VLM to  <ref type="bibr" target="#b56">(Mokady, Hertz, and Bermano 2021)</ref>. Key moments from egocentric video are summarized with SMs into a language-based world-state history (middle), which can be provided as context to an LM for open-ended question answering. Results (bottom) for generated answers (blue) and model explanations (blue) suggest SMs are fairly capable of performing a variety of reasoning tasks including answering binary yes or no questions, contextual and temporal reasoning questions, as well as subjective questions.</p><p>Fig. <ref type="figure">5</ref>: Examples of guided multi-model exchanges (Socratic Models) for an egocentric perception system: (i, left) parsing a natural language question into search entities (with LM) to be used to find the most relevant key moments in the video (with VLM); (ii, middle) describing each key frame by detecting places and objects (VLM), suggesting commonsense activities (LM), pruning the most likely activity (VLM), then generating a natural language summary (LM) of the SM interaction; (iii, right) concatenating key frame summaries into a language-based world-state history that an LM can use as context to answer the original question.</p><p>rank Places365 <ref type="bibr" target="#b98">(Zhou et al. 2016</ref>) scene categories against the image, with the top n candidates (out of 365) inserted into a prefix: "Places: {place1}, {place2}, {place3}.".</p><p>? What do I see? For object and people recognition, we use a VLM to rank OpenImages object categories <ref type="bibr" target="#b37">(Kuznetsova et al. 2020</ref>) against the image, with the top m categories (out of 600) inserted into a second prefix: "Objects: {object1}, {object2}, {object3}."</p><p>? What am I doing? For activity recognition, we use a back-and-forth interaction between an LM and VLM: we first use an LM to infer the activities most related to the places and objects previously listed by the VLM (green):</p><p>Places: {place1}, {place2}, {place3}. Objects: {object1}, {object2}, {object3}. Activities: activity a, activity b, activity c.</p><p>We find that generating candidate activities using an LM yields more suitable descriptions of egocentric activities and interactions with first-person video, than using standard activity recognition dataset categories (e.g., from Charades or Kinetics). Activity recognition datasets are often tailored to third person videos, and can only cover a partial subset of human activities, which instead can be more holistically captured through LM reasoning <ref type="bibr" target="#b62">(Petroni et al. 2019</ref>) over the objects and places that the VLM perceives. For example, "receiving a package" is a common household activity not found in most datasets. After the LM generates candidate activities, these candidates are then fed back to the VLM and re-ranked to sort out the top k activities by relevance to the key image frame: "Activities: {activity1}, {activity2}, {activity3}."</p><p>This process of generating candidate activities from places and objects is one way of extracting commonsense from LMs as knowledge bases <ref type="bibr" target="#b62">(Petroni et al. 2019)</ref>. Continuing the Socratic dialogue further, this can be repeated likewise to generate new relevant objects (conditioned on activities and places), as well as new places (conditioned on objects and activities). One can iterate the procedure (LM generate, VLM re-rank, repeat) to populate the set of places, objects, and activities until equilibrium (i.e., no more new entities), which generally helps to cover a broader set of places and objects that expand beyond the initial seed categories from Places365 and OpenImages. For example:</p><p>If I am making making pancakes, objects that I am likely to see include: a frying pan, a spatula, a bowl, milk, eggs, flour, sugar, baking powder, butter, a plate, syrup.</p><p>Given the final set of places, objects, and activities, we use the LM to generate an overall first-person summary of what is happening in the image. Specifically, the prompt is:</p><p>I am in a place1, place2, place3. I see a object1, object2, object3. I am activity1. Question: What am I doing? Answer: I am most likely</p><p>The summarization process in general can capture more rich descriptions conditioned on the places, objects, and activities, and qualitatively seem to do well at ignoring irrelevant categories (i.e., denoising). For example:</p><p>I am in a nursing home, landfill, living room. I see a wine, wine glass, woman. I am drinking wine. Question: What am I doing? Answer: I am most likely enjoying a glass of wine with a friend or loved one.</p><p>However, while the LM's denoising capabilities can compensate for the shortcomings of the VLM, it is important to note that this may also cause unwanted ignoring of notable, but rare events (e.g., such as witnessing a purple unicorn, which may be ignored, but potentially it is Halloween).</p><p>Finding new ways in which such events can be indexed appropriately may be useful for downstream applications.</p><p>Egocentric Image Summary Results. On egocentric images, we show several qualitative examples of summaries generated by our system in Fig. <ref type="figure" target="#fig_1">4</ref>, and compare them to results from a state-of-the-art image captioning model, Clip-Cap <ref type="bibr" target="#b56">(Mokady, Hertz, and Bermano 2021)</ref>. While state-ofthe-art captioning models can perform reasonably over several of the images, we find that our system generally produces more relevant captions for a larger portion of the egocentric examples. Image captioning models are biased based on the datasets they are trained on, and have shown to perform poorly on egocentric images <ref type="bibr" target="#b1">(Agarwal et al. 2020</ref>), which aligns with our observations. Relatively less research has been carried out specifically on egocentric image captioning <ref type="bibr" target="#b21">(Fan, Zhang, and Crandall 2018)</ref>. SMs can nevertheless produce reasonable captions without additional training on domain-specific data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3-B. Adding Audio into</head><p>Single-moment Summaries. In addition to using visual perceptual inputs, we may use a Socratic approach which engages perceptual inputs from audio as well, via an ALM (audio language model). Our example egocentric perception system uses Wav2CLIP <ref type="bibr">(Wu et al. 2021a</ref>) as the ALM. Wav2CLIP is trained on 5-second audio clips from the VGGSound dataset <ref type="bibr" target="#b10">(Chen et al. 2020)</ref>, and is trained in a contrastive manner by aligning its audio encoder to the visual CLIP embeddings from video.</p><p>Incorporating an ALM like Wav2CLIP into our Socratic framework can provide an additional modality with which to perform zero-shot cross-modal reasoning, and this may help further improve inference beyond the vision-language-only case. Fig. <ref type="figure" target="#fig_2">6</ref> displays a driving example for which a visualonly summarization produced the less-than-desirable summary: "I am climbing a staircase, and I may see a hamster or human leg" with the incorrect propogation of the false detection of a hamster and human leg.</p><p>To perform audio-aided single-moment summarization, we first run image-based summarization as described previously, but we then prompt the LM to suggest sounds that it may hear, given the visual context, via " visual single-image summary . 5 Possible Sounds:". For the example in Fig. <ref type="figure" target="#fig_2">6</ref> an example prompt, which has already gone through multiple rounds of Socratic dialogue to be generated, together with completion by the LM is:</p><p>These auditory entities expressed in language can then be ranked by the ALM. In this moment of the video, the sound of footsteps can be faintly heard in the background, and in this case the ALM provides a correct detection of ranking footsteps as the most likely sound. This ranking can then be incorporated into a prompt for the LM to provide the singleimage summary, for example: As above, incorporating "I think I hear footsteps" into the summary and prompting this to the LM provides the completion: "climbing a staircase, and I may hear footsteps."</p><p>In this case, this summary result is preferable to the mentioned single-image caption without sound.</p><p>While this example demonstrates in a certain case the utility of audio-informed summaries, overall in egocentric video, with a variety of background noise, we find that Wav2CLIP can provide reasonable detections for certain languagerepresented auditory entities such as 'baby babbling' and entities to do with running water, but do not provide as robust detections as CLIP. Also, while there are many advantages to the specific Wav2CLIP approach, including its use of the CLIP embedding space, a major downside is that the training process is "blind" to hearing things that cannot be seen. Accordingly, for the rest of demonstrations shown, we simply build world-state history from VLM-LM interactions alone. We expect however that with further attention to model approaches, and scaling of audio-language datasets, approaches like Wav2CLIP will increase in robustness. We also show an additional application (Sec. 3.4) of audio, for audio retrieval. In that case, only a single auditory search entity is required in order to enable a useful application, and so it can be easier to verify that it is a sufficiently robustlydetected entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3-C. Compiling a Language-Based World-State History</head><p>Our system compiles the image summaries from each key video frame into a language-based world-state history. Since the total number of frames in the video may be large, compiling a summary for every individual frame would create text that is too large (too many tokens) to be processed directly by an LM as context for Q&amp;A. Accordingly in this work, we propose solutions that sparsify and/or condense languagebased world-state histories (e.g., via search-based methods) into practically usable context sizes for reasoning. In particular, we explore two methods of identifying "key moments" in videos for summarization: (i) uniform sampling over time, and (ii) video search (image or audio retrieval) for on-the-fly compilation of context.</p><p>The first method, uniform sampling, is straightforward and compiles a world-state history from Socratic summaries of video frames sampled at fixed time intervals. This can also be condensed hierarchically using recursive linguistic summarization <ref type="bibr">(Wu et al. 2021b)</ref>, to fit even dense sampling into usable LM-context sizes. However, while broadly indiscriminate, uniform sampling may not have sufficient temporal resolution to capture important spontaneous events in the video (such as adding salt to the pot while cooking soup in the kitchen).</p><p>Hence the second method, identifying key moments with video search, uses a VLM or ALM to search for entities most relevant to the question, which can more precisely index the frames in which the subject appears. Specifically, our instantiation of SMs for this component parses a natural language question with an LM into several search entities to be used to find key frames in the video. For example, the question "did I drink coffee today?" yields a search entity "drink coffee" that is then used with language-conditioned video search to index the most relevant n key frames of "drink coffee" in the video. The LM categorizes the search, which can be image-based (VLMs) or audio-based (ALMs), e.g., for language-conditioned auditory recall questions ( <ref type="bibr" target="#b58">(Oncescu et al. 2021</ref>)) like "why was my wife laughing today?" . While search-based indexing of key moments can be useful for finding spontaneous events, this method for generating context can also provide disadvantages for downstream Q&amp;A if the answer to the question depends on events that are not directly related to the search subject. For example, "why was I chopping wood today?" returns key frames related to "chopping wood", but does not return the key frames after the event related to making a campfire. On the other hand, if uniform sampling is employed and the campfire events are captured by the summary, then the LM can successfully return the answer "I was making a campfire." Choosing which method to use for compiling the language-based world-state history may depend on the application.</p><p>Language-based World-state History Results. Fig. <ref type="figure" target="#fig_1">4</ref>, middle, shows results generated by our system. The specific event log shown in Fig. <ref type="figure" target="#fig_1">4</ref> has been trimmed down for space considerations, but is representative of the type of event logs that may be generated without manual curation. These event logs are used as context to enable LM open-ended reasoning on video, as demonstrated in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Open-Ended Reasoning on Egocentric Video</head><p>In this section we describe a few examples of how the Socratic Models framework can be used to perform open-ended multimodal-informed completion of text prompts, conditioned on egocentric video (examples in Fig. <ref type="figure">2</ref>). There are of course limitations to they can provide, but our demonstrated examples suggest that we can already today often generate compelling answers to open-ended reasoning tasks, at a scope that is beyond what we are aware is possible today with available methods. Of course, the answers may also inherit undesirable characteristics from the component models, such as an LM that is overconfident even when wrong. It is our hope that our results may help inspire work on preparing even more comprehensive video understanding datasets for the community, to assist further assessment.</p><p>Our example system uses a language-based world-state history generated through Socratic multi-model discussion (Sec. 3.3), and provides this as context to an LM to enable open-ended reasoning on egocentric videos. Open-ended text prompts from a user, conditioned on an egocentric video, can yields three types of responses: a text-based response, a visual result, and/or an audio clip. These latter two provide examples that open up the capabilities of the system to respond not only with text-based responses, but also respond with video snippets themselves, which may be a higher-bandwidth way to respond to user requests ("a picture is worth a thousand words"). The specific composition of our system is of course just one example -overall, the modularity of the Socratic approach makes it easy to compose together foundation models, zero-shot, in a variety of ways to provide a spectrum of multimodal reasoning capabilities.</p><p>The demonstrated tasks include (i) summarization, (ii) openended Q&amp;A, (iii) forecasting, (iv) corrections, and (v) video search for either visual or audio cues. These tasks have predominantly been studied in isolation in the research community -but our example results with SMs suggest they can be subsumed under the same unified language-based system for multimodal reasoning. Descriptions and results for each of (i)-(v) are shown below.</p><p>(i) Summarization can be implemented by prompting an LM to complete the excerpt "{world-state history} Summary of my day:" to which it can respond with outputs like "I slept in a bed, made coffee, watched TV, did laundry, received a package, bench pressed, showered, ate a sandwich, worked on a computer, and drank wine." Since the language-based world-state history is constructed with summaries of visual content, it carries contextual information that can be complementary to what is found in closed captions (e.g., speech and dialogue, explored in Sec. 5). Summarizing egocentric videos enables a number of applications, including augmenting human memory to recall events, or life-logging of daily activities for caregiver assistance. Our system draws similarity to early work in the area involving text-based summarization and identifying key frames (see <ref type="bibr" target="#b7">(Barbieri, Agnihotri, and Dimitrova 2003)</ref> for an early survey and <ref type="bibr" target="#b17">(Del Molino et al. 2016;</ref><ref type="bibr" target="#b3">Apostolidis et al. 2021</ref>) for more recent surveys).</p><p>(ii) Open-ended Q&amp;A can be implemented by prompting the LM to complete the template: "{world-state history} Q: {question} A:". We find that LMs such as GPT-3 can generate surprisingly meaningful results to binary yes or no questions, contextual reasoning questions, as well as temporal reasoning questions. As in <ref type="bibr" target="#b94">(Yang et al. 2021)</ref> we can further prompt the LM to explain the answer by adding "This is because:". We find that the accuracy of the answers and explanations remain largely conditioned on whether the necessary information can be found within the world-state history. This suggests that the quality of the language-based reconstructions of the videos (e.g., via key frame sampling and captioning in this work) is central to the approach.</p><p>We show several qualitative examples of free-form question answering using our SM system on egocentric video in Fig. <ref type="figure" target="#fig_1">4</ref>, bottom, Fig. <ref type="figure">5</ref>, and Fig. <ref type="figure" target="#fig_3">7</ref> generated using a firstperson POV video<ref type="foot" target="#foot_2">2</ref> as input.</p><p>Recall Questions. SMs can perform simple retrieval of events. For example, "did I eat dinner today?", yields a response "yes I ate dinner today." along with an explanation "I was seen eating a sandwich in a kitchen at 5:27 PM." which points to the key frame that was captioned with the sandwich in hand. Another example that involves contextual reasoning to recall events is "what was I doing outdoors?" to which the system responds "I was chopping wood in a yard." Likewise, if the entities described in the question do not appear in the world-state history, such as "did I drive today?" the system can respond with a negative answer: "no, I did not drive today." with an explanation "I was at home all day." This capability expands beyond standard video search, which might only return nearest neighbor video frames, without a natural language response (or a negative response).</p><p>The performance of recalling events largely depends on the relevance of the language-based world-state history to the question. We find that recall-type questions work best with world-state history logs that are compiled by using searchbased key frame indexing (see Sec. 3.3-B). The system can still return negative responses, since the captioning of the key frames are not influenced by the question.</p><p>Temporal Reasoning. SMs can answer questions related to time by appending timestamps to each key moment in the world-state history. By associating image summaries to times of the day, this allows answering questions that time index various activities. For example "when did I last drink coffee?" can return the last time drinking coffee was mentioned in the log, with a full response "I last drank coffee at 10:17 AM" and an explanation "I was making coffee in the kitchen." The system can also count events, for example when asked "how many times did I receive a package today?", the system will respond appropriately "I received a package once today." with an explanation "I was receiving a package at 3:24 PM". We find that a common failure mode for these types of questions is that the system tends to over-count, especially as a reaction to false positive VLM detection results that get surfaced into the world-state history. For example, asking "who did I interact with?" would yield "woman, hamster" where hamster was a false positive prediction from CLIP. These issues become more prominent with search-based key frame sampling, as a byproduct of an inability to distinguish neighboring local argmaxes of the same event from each other.</p><p>Cause and Effect Reasoning. SMs can answer questions about cause and effect relationships between events, conditioned on that all the events appear in the world-state history. For example, when asked "why did I go to the front porch today?" the system would respond "I went to the front porch today to receive a package." and an explanation "I saw on the porch a package and knew that I was expecting it." These types of questions are exciting because they suggest opportunities for prompting logical deduction of events. However, since information about both the cause and the effect needs to be in the world-state history, the quality of results remains highly dependent on the key frame sampling strategy used to compile it (Sec. 3.3-B). Uniform gives an unbiased account of events, and is currently the best variant for this form of reasoning. More targeted construction of the world-state history with search based key frames can sometimes miss frames that capture the answer to the question.</p><p>Subjective Reasoning. SMs can also answer more subjective questions, such as "was I happy today?" or "what was my favorite drink today?". Without additional context, these questions rely on biases from the LM's dataset -which could have negative consequences, and should be managed carefully with additional mechanisms for safety and groundedness <ref type="bibr" target="#b84">(Thoppilan et al. 2022)</ref>. The full personalization of these subjective questions are likely to be conditioned on whether a better context can be constructed of prior user behaviors related to the question.</p><p>(iii) Forecasting of future events can be formulated as language-based world-state completion. Our system prompts the LM to complete the rest of an input event log. Timestamps of the predictions can be preemptively specified depending on the application needs. The completion results are generative, and are more broad than binary event classification (e.g., <ref type="bibr" target="#b41">(Lei et al. 2020)</ref>). Example completion results (also shown in Fig. <ref type="figure">2</ref>): Few-shot prompting the LM with additional examples of prior event logs most similar to the current one is likely to improve the accuracy of the completion results. Without additional context, these results are again biased towards typical schedules seen by the LM across Internet-scale data.</p><p>To a certain extent, this forecasting capability extends and generalizes the traditional topic of activity forecasting in computer vision. In the research community, activity forecasting has been often formulated as an extension of action classification, tracking, or feature generation: Given a sequence of image frames, they directly predict a few categorized actions (Ryoo 2011; Hoai and De la Torre 2014; Rhinehart and Kitani 2017), human locations <ref type="bibr" target="#b36">(Kitani et al. 2012)</ref>, or image features <ref type="bibr" target="#b85">(Vondrick, Pirsiavash, and Torralba 2016)</ref> to be observed in the future frames. In contrast, Socratic Models with LMs enables generating more semantically interpretable descriptions of future events, conditioned on multimodal information.</p><p>(iv) Corrections. SMs can be prompted to incorporate human feedback in the loop as well, which could be useful for interactive language-based systems. For example, given image captions generated from an VLM and LM:</p><p>Fig. <ref type="figure">8</ref>: Example zero-shot language-prompted auditory retrieval (shown: top 2 results) in response to "what did my daughter's laugh sound like today?", for which an LM identifies the audio search query of "daughter's laugh", and an ALM (Wav2CLIP) is used for audio retrieval. The top (left) retrieval is only partially correct, returning a video clip involving the daughter but not laughter. The second (right) retrieval is correct, from a moment of playing (getting tossed into the air). Faces obscured for privacy. (v) Video Search: Image or Audio Retrieval. Our SM system can also return additional modalities (images, audio) as answers to questions, by simply few-shot prompting the LM to classify a target modality based on the input question. For example, "where did I leave my remote control" can map to image search using VLM features for "remote control" while "what did my daughter's laugh sound like today?" can map to natural-langauge-queried audio search ( <ref type="bibr" target="#b58">(Oncescu et al. 2021</ref>)) using ALM features for "daughter's laugh" (Fig. <ref type="figure">8</ref>). This can be useful for some applications (e.g., AR) in which the user may find the retrieved modality to be more useful than a natural language response. Our approach for this uses an LM to parse a search entity from the question to index key video frames. This is done with several fewshot examples provided as context. For example, the question "when did I last wash my hands?" yields a search entity "wash my hands" that is then used with video search to index the most relevant n key frames of "wash my hands" in the video. Specifically, our system runs video search by ranking matching CLIP or Wav2CLIP features of the entity text against all video frames, and returning the top n local maximums. For each frame, the features can either be image features or audio features (e.g., from the surrounding 5 seconds with Wav2CLIP) -where the LM few-shot categorizes which domain to use for any given question. This can be thought of as calling different subprograms for hierarchical search.</p><p>Limitations. Overall, our results suggest that SMs are capable of generating meaningful outputs for various egocentric perception tasks via visual contextual reasoning -but its limitations also suggest areas for future work. For example, a primary bottleneck in the Q&amp;A system is that it relies on the richness (i.e., recall) and quality (i.e., precision) of the event log. This likely could be improved with better image and audio detectors or captioning systems <ref type="bibr" target="#b27">(Gu et al. 2021)</ref>. Also, we find that the used Wav2CLIP may provide satisfactory results for certain categories in audio retrieval, but we currently do not involve it in generating the event log, since its robustness and range of open-language detection is not at the same level as CLIP. This seems addressable with further approaches and scaling of datasets in the audio-language domain.</p><p>Additionally, accurate response to cause and effect reasoning questions also require relevant key moments to be reflected in the event log -which points to open ended questions on how to achieve better key frame sampling (beyond the simple baselines that we have demonstrated). Finally, the dialogue between the different models are fairly structured with manually engineered prompts. It may be interesting to investigate more autonomous means of achieving languagebased closed loop discussions between the models until a commonsense consensus is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Scaling Up Socratic Video Search</head><p>The search algorithms of the SMs, which may be used both for compiling world-state history (Sec. 3.3-C) and for video search retrieval (Sec. 3.4) rely on the matching procedure conducted in the corresponding latent space (e.g. VLM features of the text snippet against these of the video frames). This can be abstracted as dot-product-maximization key search in the given key-dataset. In practice, if the keydataset is large (e.g. long videos) a naive linear search is prohibitively expensive. We propose several solutions to this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIP-Search.</head><p>The first observation is that several data preprocessing techniques applied in the so-called maximum inner product (MIP) search can be directly used to reorganize the keys (e.g. latent representations of video frames) to provide sub-linear querying mechanism for the incoming text snippet (see: <ref type="bibr" target="#b0">(Abuzaid et al. 2019)</ref>). Those include pruning and various indexing techniques, such as LSH-hashing <ref type="bibr" target="#b76">(Shrivastava and Li 2014</ref>). In the hashing approach, a collection of hash-tables, indexed by the binarized representations of the hashes is stored with different entries of the hash table corresponding to the subsets of keys producing a particular hash. There are several cheap ways of computing such hashes, e.g. signed random projection (those in principle linearize the angular distance, but every MIP task can be translated to the minimum angular distance search problem).</p><p>The querying is then conducted by searching for the most similar hash-entries in the hash-tables and then performing linear search only on the subsets of keys corresponding to these entries to obtain final ranking.</p><p>Associative Memories. The above approach provides sub-linear querying mechanism, but does not address the space complexity problem. In the scenario of strict mem-ory requirements, we propose to leverage recently introduced techniques on linear attention <ref type="bibr">(Choromanski et al. 2021b</ref>) combined with modern continuous associative memory (MCAM) models <ref type="bibr" target="#b67">(Ramsauer et al. 2021)</ref>. MCAM models are de facto differentiable dictionaries (with provable few-shot retrieval) that can be thought of as energy-based models using negated exponentiated latent-representationsdot-product energy for the exponential storage capacity. A naive computation of such an energy still requires explicitly keeping all the patterns (which is exactly what we want to avoid), but this can be bypassed by applying the linearization of that energy (which effectively is just the negated sum of the softmax kernel values) with the FAVOR+ mechanism used in linear-attention Transformers, called Performers <ref type="bibr">(Choromanski et al. 2021b)</ref>. This modification has several advantages: (1) it makes the size of the dictionary completely independent from the number of the implicitly stored patterns; the size now scales linearly with the number of random features used for energy linearization, ( <ref type="formula">2</ref>) it provides a constant-time querying mechanism at the price of compressing all the patterns (and thus losing some information).</p><p>Random Feature Trees. The other approach, that combined the ideas from both MIP-search and linear attention systems, leverages the so-called random feature tree (RFT) data structure <ref type="bibr" target="#b68">(Rawat et al. 2019)</ref>. This approach relaxes the MIP-search to sampling from the linearized softmax distribution via FAVOR+ <ref type="bibr">(Choromanski et al. 2021a)</ref>. Sampling from such a linearized distribution can be done in time logarithmic in the number of samples via RFT which is a balanced tree with leaves corresponding to latent representations of video frames and nodes encoding representations of the subsets of keys (e.g. the video frames) defined as sums of the random feature transforms of the keys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Socratic Internet Data Image Captioning</head><p>The SMs framework can also be used to generate text captions for generic Internet images with a guided multi-model exchange between a VLM and LM. We describe an example system in Sec. 4.1 and demonstrate results in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System: Image Captioning on Internet Data</head><p>Overall, our example SMs system for Internet image captioning is extremely similar to how we perform single-image captioning in our egocentric system, but (i) adapted for Internet images rather than ego-centric images, and (ii) adapted such that the "final task" is the generation of a single image caption, rather than open-ended tasks based on textprompted completion.</p><p>First, similar to the process of generating egocentric image captions, we may prompt the VLM to zero-shot detect visual entities across different categories of language. As with the egocentric system, we return top matching place categories and object categories. For Internet data, we use Tencent ML-Images <ref type="bibr">(Wu et al. 2019)</ref> for object entities. We also choose to detect the image type from the set {photo, cartoon, sketch, painting} and the amount of people from the set {are no people, is one person, are two people, are three people, are several people, are many people}. For generic Internet images, which are not necessarily real photos, and very often are taken by people of people, we find that these additional contextual pieces of information help generate better captions. The various VLM detections give: "Places: {place1}, {place2}, {place3}. Objects: {object1}, {object2}, {object3}. Image type: {image type}. People result: {people result}."</p><p>Next, given the VLM detections of various visual entities, we can then prompt the LM to generate several (n) candidate captions. For this step, we employ a non-zero sampling temperature (we find 0.9 gives good results) in order to give sufficient variety of results across the n options, but still generate reasonably likely options.</p><p>I am an intelligent image captioning bot. This image is a {img type}. There {num people}. I think this photo was taken at a {place1}, {place2}, or {place3}. I think there might be a {object1}, {object2}, {object3},... in this {img type}. A creative short caption I can generate to describe this image is:</p><p>Examples of an actual prompt and generated sample captions, for example for the bottom-left photo of Fig. <ref type="figure" target="#fig_4">9</ref> is:</p><p>I am an intelligent image captioning bot. This image is a photo. There are no people. I think this photo was taken at a indoor bow window, dining room, or interior balcony. I think there might be a double-hung window, casement window, sliding window, pivoting window, breakfast area, breakfast nook, dining area, storm window, storm sash, dining room, bay window, bow window, lancet window in this photo. A creative short caption I can generate to describe this image is: ... (run n times, examples shown below)</p><p>? Looking through the window to a beautiful view.</p><p>? This serene and stately room is centered around a beautiful window, which floods the space with natural light. The double-hung window allows for ventilation, while the storm sash protects against inclement weather. The perfect spot for a lazy Sunday brunch or an intimate dinner party.</p><p>? . . .</p><p>Lastly, after the LLM has generated several captions, they are subsequently ranked by the VLM, and the highest scor-ing caption is returned -in the example above, this is: "This image shows an inviting dining space with plenty of natural light."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results: Image Captioning on Internet Data</head><p>Fig. <ref type="figure" target="#fig_4">9</ref> shows several qualitative comparisons with Clip-Cap <ref type="bibr" target="#b56">(Mokady, Hertz, and Bermano 2021)</ref>, a state-of-the-art method for image captioning specifically trained via finetuning on image captioning. We show zero-shot generation of captions on the set of images displayed in ClipCap's code release. <ref type="foot" target="#foot_3">3</ref> We make a colab available to generate these results.<ref type="foot" target="#foot_4">4</ref> </p><p>Overall, our results (Fig. <ref type="figure" target="#fig_4">9</ref>) show that the Socratic Model framework can be adopted to provide often convincing results for image captioning via the creativity of the LM combined with the visual grounding of the VLM. While these results are promising, the degree to which visual details are provided in the captions is largely limited by the capabilities of the VLM. For example, attributes (e.g., color of a shirt, a person's facial expression, or the spatial relationships between objects) are details not often captured in our particular system, which relies more on the contextual image classification capabilities of the VLM. Future work may explore open-vocabulary object detectors <ref type="bibr" target="#b27">(Gu et al. 2021</ref>) as a means to recover salient details for more rich captions. It is also important to note that the system may generate captions that reflect unwanted biases found in the Internet-scale data that it is trained on, and should be used with caution (and checked for correctness) in downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Socratic Video-to-Text Retrieval</head><p>We also adapt the Socratic Models framework to the task of video-to-text retrieval, a common video understanding task with clear and quantitative metrics. We describe an example SM-based system (Sec. 5.1) which uses a guided multimodel exchange between a VLM, a causal LM, a speech-totext (ALM) model, and a masked LM, that achieves state-ofthe-art performance for zero-shot video-to-text (i.e., caption) retrieval on the popular MSR-VTT dataset (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Overview: Socratic Video-to-Text Retrieval</head><p>Our approach uses SMs to augment Portillo-Quintero et al.</p><p>(2021) with commonsense information from the audio and language domains. Portillo-Quintero et al. ( <ref type="formula">2021</ref>) computes a similarity measure between the average VLM (i.e., CLIP) features of the image frames from a video, and the CLIP text features of a caption. This zero-shot method can be used directly for MSR-VTT video-to-text retrieval via one-to-many nearest neighbor matching.</p><p>We can improve upon their method through a combination of speech-to-text ALMs together with LM-based commonsense reasoning. First, we transcribe the audio from all videos with speech-to-text ALMs <ref type="bibr" target="#b6">(Bapna et al. 2022</ref>) (also called ASR, or automatic speech recognition), using the publicly-available Google Cloud speech-to-text API. <ref type="foot" target="#foot_5">5</ref> Although raw transcripts may be challenging to incorporate into meaningful improvements for video/caption retrieval, we may leverage reasoning capabilities from large LMs in order to usefully harness the transcripts. For videos with sufficiently long transcripts, we summarize the content with an LM (e.g., GPT-3) using the following prompt:</p><p>I am an intelligent video captioning bot.' I hear a person saying: "{transcript}". Q: What's a short video caption for this video? A: In this video,"</p><p>We then compute the similarity scores of the generated summary to the set of captions with a masked LM (e.g., similarity between sentence embeddings from RoBERTa <ref type="bibr">(Liu et al. 2019b</ref>)), and use those scores to re-weight the CLIP-based ranking from Portillo-Quintero et al. Specifically, for videos with sufficiently-long transcripts (we use ?100-character transcripts), the matching score is:</p><formula xml:id="formula_0">CLIP (caption) ? CLIP (video ) ? RoBERTa (caption) ? RoBERTa (GPT-3 with prompt (Speech2Text (audio )))</formula><p>, where ? represents normalized dot product of embeddings, and ? represents scalar multiplication. For a given video, if there is no audio or the transcript is too short, we resort to exactly the method of Portillo-Quintero et al., which computes matching scores with CLIP(caption) ? CLIP(video ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSR-VTT 1k-A MSR-VTT Full</head><p>Category Method R@1? R@5? R@10? MdR? R@1? R@5? R@10? MdR? Audio CLIP enc. Here, the Socratic interaction lies mainly between the ALM (speech-to-text) to the commonsense LM (GPT-3 to summarize the transcriptions), and between the commonsense LM to the ranking based system that is a combination of the VLM (CLIP) and the masked LM (RoBERTa). Note that we may also prompt LMs (in this case, via multiple-choice) to determine if one caption is a better fit than another for a given video. However, for this specific task and dataset, with thousands of possible answers to choose from, the numerical ranking provided by embedding similarity scores provides a practical solution rather than relying on thousandway multiple-choice commonsense reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finetuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results: Socratic Video-to-Text Retrieval</head><p>Long-transcript subset of . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSR-VTT 1k-A</head><p>Method R@1? R@5? R@10? MdR? For video-to-text evaluation we use the MSR-VTT dataset <ref type="bibr" target="#b92">(Xu et al. 2016)</ref>, which as noted in other recent works <ref type="bibr" target="#b25">(Gao et al. 2021;</ref><ref type="bibr" target="#b12">Cheng et al. 2021)</ref> is the most popular benchmark dataset for the task of video-to-text retrieval. Like other recent works <ref type="bibr" target="#b25">(Gao et al. 2021)</ref>, we focus our results on this dataset. One of the reasons this is a good task and dataset for generally testing the value of the SMs approach is that there is already a strong zero-shot baseline, provided by <ref type="bibr" target="#b64">Portillo-Quintero et al. (2021)</ref>, which uses CLIP by itself, but does not use the Socratic method: there is no multimodel exchange, and no LMs are used. Additionally, this task provides a great opportunity to incorporate another type of modality -speech-to-text from audio data. We compare our method both with zero-shot methods, and with finetuned methods specifically trained on MSR-VTT.</p><p>Results show that our method sets a new zero-shot stateof-the-art for video-to-text retrieval on MSR-VTT (Tab.1), both on the "1k-A" and "full" test sets. Since our demonstrated system uses exactly the method of <ref type="bibr" target="#b64">Portillo-Quintero et al. (2021)</ref> for its processing of CLIP features but additionally incorporates LLM reasoning on speech-to-text transcripts, the increased measured performance of our method (i.e. 27.2 ? 42.8 R@1 on MSR-VTT 1k-A) directly reflects the additional benefit of incorporating language-based multimodal reasoning. Additionally, to keep the comparison between our method and Portillo-Quintero et al. ( <ref type="formula">2021</ref>) as direct as possible, we maintain the usage of their precomputed CLIP features<ref type="foot" target="#foot_6">6</ref> from ViT-B/32. Given results from other recent methods (Tab. 1) it seems likely we may be able to improve our performance by switching to ViT-B/16, or other recent more-performant VLM models <ref type="bibr" target="#b97">(Zhai et al. 2021)</ref>.</p><p>As shown in Table <ref type="table">2</ref>, if we look at only the long-transcript videos, i.e. the videos for which our method used a transcript, then we especially see an increase in performance -on MSR-VTT 1k-A, R@1 almost doubles, from 28.2 to 55.0, for our method compared to <ref type="bibr" target="#b64">Portillo-Quintero et al. (2021)</ref>. Further, although it is on only a subset of the test set, note that this R@1 metric achieved of 55.0 is roughly comparable to the R@1 of the best finetuned-SOTA method, DRL <ref type="bibr" target="#b87">(Wang et al. 2022</ref>) on the entire 1k-A dataset, with 56.2 R@1 (Tab. 1). If we assume that, for visual-only methods, the videos with-or-without transcripts are of roughly equal difficulty from a visual-only retrieval perspective, this suggests that on internet videos with sufficient spoken language present, our method for zero-shot video-to-text retrieval can nearly match the finetuned-SOTA method for video-to-text retrieval.</p><p>Note that instead of video-to-text retrieval, but rather on textto-video retrieval, a recent method <ref type="bibr" target="#b45">(Li et al. 2022</ref>) has shown strong zero-shot results. Other methods have also attempted zero-shot on MSR-VTT text-to-video retrieval <ref type="bibr" target="#b93">(Xu et al. 2021;</ref><ref type="bibr" target="#b54">Miech et al. 2020;</ref><ref type="bibr" target="#b4">Bain et al. 2021</ref>), but these have all been outperformed by <ref type="bibr" target="#b64">Portillo-Quintero et al. (2021)</ref>. Our method may be adapted as well to text-to-video, but due to our use of transcripts on only a subset of the videos, unlike in video-to-text, this creates an asymmetry which may require an unwieldly relative weighting for ranking videos with or without transcripts.</p><p>Also note that (Tab. 1) prior to the CLIP revolution in videoto-text retrieval, using the audio modality was not uncommon amongst competitive video-to-text retrieval methods <ref type="bibr" target="#b55">(Mithun et al. 2018;</ref><ref type="bibr">Liu et al. 2019a</ref>). The trend over the past year, however, has been to instead focus on using only visual features, with all recent competitive methods being based off of CLIP, and not using audio data. Our approach, through leveraging commonsense reasoning stored in the LMs, is able to once again allow audio data to enable progress in this common video understanding task, beyond what CLIP alone can provide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Unsupervised Socratic Model Selection</head><p>The combination of complementary models, in which one may compensate for the weaknesses of the other, opens an interesting avenue for unsupervised evaluation of model performance. Since our metric of interest is the combined performance of e.g., a VLM and a LM -rather than asking the question: '(A): how well does this VLM perform in absolute?', we can instead ask: '(B): how well does this VLM compensate for the weakness of the LM?'. <ref type="bibr" target="#b82">(Strope et al. 2011)</ref> proposes a scheme which does so without requiring any evaluation ground truth. They also find that asking question (B) correlates well with answers to question (A), and is useful e.g., for model selection. The method assumes you have access to a weak (wLM) and a strong (sLM) LM (respectively VLM if evaluating the LM's performance).</p><p>Asking "how well does this VLM compensate for the weaknesses of the LM" is equivalent to asking: "if we have a collection of VLMs, and we combine them with a weak LM, which model is going to perform the closest to the combination of the VLM with a strong LM?" If a VLM combined with a weak LM, instead of a strong one, makes up for the LM's shortcomings and still performs well in combination, then it may serve as a better component in the context of this combined system.</p><p>The benefit of this approach, while not entirely making up for doing absolute evaluations against a ground truth, is that because it only measures relative distance between model outputs, it can be performed unsupervised without annotated data: the distance between the output of the weak and strong combination can be measured using measures of semantic distance, for instance here by scoring them against a distinct, held-out language model.</p><p>As an example of using this approach, we extend the method in <ref type="bibr" target="#b82">(Strope et al. 2011)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>Socratic Models are a class of systems that leverage structured dialogue between multiple language-based foundation models to make joint predictions for new multimodal tasks. SMs leverage the commonsense knowledge already stored within foundation models pretrained on different domains of data (e.g., text-to-text, text-to-images, text-to-audio), which may include for example Internet-scale data. Our shown systems for egocentric perception, image captioning, and videoto-text retrieval are just examples of the SMs framework, and may shed light on new opportunities to build simple systems that adapt foundation models to (i) capture new multimodal functionalities zero-shot without having to rely on additional domain-specific data collection or model finetuning, and (ii) do so while retaining their robustness to distribution shifts (which is known to deteriorate after finetuning) <ref type="bibr" target="#b89">(Wortsman et al. 2021)</ref>.</p><p>SMs present a language-based approach to combining the outputs of multiple foundation models, which differs from a classical Bayesian approach where one model is used as a prior and the other as evidence. Relying on language-only multi-model discussion carries both pros and cons. For example, the intermediate outputs of the models may be more interpretable, but are treated as "truth" between modelsi.e., not weighing them against the other's priors or evidence, which can lead to more divergent model interactions.</p><p>In the context of egocentric perception, we find that formulating video Q&amp;A as reading comprehension in SMs directly leverages the extent to which large LMs are capable of logical reasoning by connecting commonsense relationships with knowledge learned from Internet-scale data. For example, the system returns the following answer when presented with the world-state history log:</p><p>8:00 AM: went to grocery store to buy orange juice, chocolate, and bread. 8:15 AM: I went to gas station to fill up the vehicle tank. 8:30 AM: drove back home and left the groceries in the kitchen. 8:45 AM: started cooking eggs in the pan. 9:00 AM: the dog went into the kitchen. 9:15 AM: took the dog out for a walk. 9:30 AM: the dog is sick. Q: Why is the dog sick? A: The dog may have eaten something it was not supposed to, such as chocolate.</p><p>Arriving at the answer requires bridging multiple connections between observations e.g., that the dog went into the kitchen, that the groceries are still in the kitchen, and that the groceries contain chocolate. Such results offer a glimpse of what might be possible using SMs for deductive reasoning across multiple domains of information, and raises interesting research questions on (i) how to better assemble language-based world-state histories (beyond what is presented in this work) that capture relevant evidence to im-prove the accuracy of conclusions, and (ii) how to elicit chain of thought prompting <ref type="bibr" target="#b88">(Wei et al. 2022)</ref> to decompose multi-step problems into intermediate ones. For example, one promising extension could be prompting the LM with chain of thought sequences to expand on hypotheses:</p><p>Q: What are reasons for why I might be chopping wood? A: Reasons might include: needing firewood, wanting to make a statement, or needing the exercise.</p><p>to which each hypothesis can be progressively explored by downstream subprograms called at recursively higher resolutions until a conclusion is reached. These directions suggest pathways towards achieving increasingly meaningful utility and analysis by digital multimodal assistants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Broader Impacts</head><p>Socratic Models offer a new perspective that encourages building AI systems using off-the-shelf languageinteractable foundation models without additional data collection or model finetuning. This leads to several practical benefits, new applications, and risks as well. For one, SMs provide an interpretable window, through language, into the behavior of the systems (even for non-experts). Further, the barrier to entry for this technology is small: SMs can be engineered to capture new functionalities with minimal compute resources. No model training was used to create any demonstrated results. This can be enabling, but also raises potential risks, since it increases the flexibility of unintended end use applications, and should be carefully monitored over time. We welcome broad discussion on how to maximize the potential positive impacts (enabling broad, new multimodal applications, with minimal new resources) while minimizing the capabilities of bad actors.</p><p>Regarding the impact on energy and other resource consumption for machine learning, this work may help pave a path for new, capable machine learning models to be composed with minimal training resource consumption, provided that large foundational pretrained models are available. This may help provide an answer for how large pretrained models may be retargeted to a wide variety of multimodal applications, without additional considerable compute resources required. Since SMs help demonstrate how a wide variety of applications may be addressed with fixed (pretrained) models zero-shot, this may also help foster adoption of new machine learning accelerators (e.g., fixed analog circuity <ref type="bibr" target="#b69">(Reuther et al. 2020</ref>), optical diffraction <ref type="bibr" target="#b48">(Lin et al. 2018</ref>)) for inference with substantially lower power consumption and more compact form factors.</p><p>We are excited about opportunities as well in downstream applications. For example, SMs suggest promising research directions for data-driven learning in robotics, where the various modules within a robot system (e.g., planning <ref type="bibr" target="#b2">(Ahn et al. 2022;</ref><ref type="bibr" target="#b32">Huang et al. 2022)</ref>, perception <ref type="bibr" target="#b75">(Shridhar, Manuelli, and Fox 2022)</ref>) can be replaced with zero-shot foundation models imbued with commonsense priors across domains. These ideas may give rise to a new class of robot systems where by grounding affordances <ref type="bibr" target="#b97">(Zeng 2019</ref>) on language, control algorithms can begin to tap into the capabilities of models trained on Internet-scale data, and to tackle applications that have traditionally been data-scarce.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Socratic Models are composed via guided multi-model exchanges. In this example, LMs, VLMs, and ALMs are composed closed-loop via language. Examples of guided multi-model discussions are provided in Sec. 3.3, Sec. 3.4, Sec. 4, and Sec. 5 on various multimodal applications.</figDesc><graphic url="image-3.png" coords="3,319.50,54.00,238.49,106.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: An instantiation of the SMs framework for open-ended reasoning with egocentric perception. SMs can generate meaningful structured captions (top) for egocentric images through Socratic dialogue between VLMs (green) and LMs (blue), and qualitatively perform well versus state-of-the-art captioning models such as ClipCap (Mokady, Hertz, and Bermano 2021). Key moments from egocentric video are summarized with SMs into a language-based world-state history (middle), which can be provided as context to an LM for open-ended question answering. Results (bottom) for generated answers (blue) and model explanations (blue) suggest SMs are fairly capable of performing a variety of reasoning tasks including answering binary yes or no questions, contextual and temporal reasoning questions, as well as subjective questions.</figDesc><graphic url="image-4.png" coords="5,54.00,39.06,483.82,651.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: Example frame and corresponding (centered) 5-second audio clip which provide the driving example for Sec. 3.3-B, i.e., adding in ALMs into Socratic dialogue to improve single-moment summarization. Note that this waveform mostly represents the background piano music, but the system is still able to rank correctly that footsteps as the highest sounds relative to others in the LM-suggested candidate set.</figDesc><graphic url="image-6.png" coords="7,367.20,54.00,143.09,113.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: SMs can interface with the user through dialogue and perform a variety of tasks (formulated as Q&amp;A) with egocentric video: sorting reasoning questions by their output modalities e.g., text-base responses, images from visual search, video snippets from audio search. Depending on the modality, each question can pass through a different sequence of Socratic interactions between the LM, VLM, and ALM.</figDesc><graphic url="image-7.png" coords="9,56.52,58.98,498.97,176.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :</head><label>9</label><figDesc>Fig.9: The SMs framework with a VLM and LM can be used to zero-shot generate captions for generic Internet images, and can be as expressive as finetuned-for-captioning state-of-the-art models e.g., ClipCap<ref type="bibr" target="#b56">(Mokady, Hertz, and Bermano 2021)</ref>.</figDesc><graphic url="image-9.png" coords="12,56.52,58.98,498.94,332.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Evolution over time of state-of-the-art (SOTA) results for video-to-text retrieval, with R@1 metric, on the popular MSR-VTT (Xu et al. 2016) 1k-A (Yu, Kim, and Kim 2018) dataset. See Tab. 1 for additional information on these methods.</figDesc><graphic url="image-10.png" coords="13,319.50,54.00,238.50,225.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="2,56.52,58.98,498.96,161.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="6,56.52,58.98,498.95,175.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>am camping with my family and enjoying the company of them around the fire.</head><label></label><figDesc></figDesc><table><row><cell>Context: Where am I? outdoor cabin, campsite, outdoor</cell></row><row><cell>inn. What do I see? fire, marshmallow, fire iron,</cell></row><row><cell>hearth, fireside, camp chair. What am I doing?</cell></row><row><cell>Commonsense suggests: roasting marshmallows, sitting</cell></row><row><cell>around the fire, chatting. Most likely: sitting</cell></row><row><cell>around the fire.</cell></row><row><cell>Original Summary: I am camping and enjoying the</cell></row><row><cell>company of my friends around the fire.</cell></row><row><cell>Corrections: It was actually my family, not friends,</cell></row><row><cell>sitting around the fire.</cell></row><row><cell>Corrected Summary: I</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Video-to-text retrieval results on MSR-VTT<ref type="bibr" target="#b92">(Xu et al. 2016</ref>) dataset, both on the popular 1k-A<ref type="bibr" target="#b95">(Yu, Kim, and Kim 2018)</ref> subset and the original 'full' test set. Differentiated are methods which train on the MSR-VTT dataset (finetuning), compared with zero-shot methods, which do not. Also noted: whether the methods use audio channels, and if CLIP<ref type="bibr" target="#b65">(Radford et al. 2021</ref>) is used, which CLIP encoder is used. See Fig.10for the chronology of the SOTA across each category.</figDesc><table><row><cell></cell><cell>JEMC (Mithun et al. 2018)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12.5 32.1</cell><cell>42.4</cell><cell>16.0 yes</cell></row><row><cell></cell><cell>Collaborative Experts (Liu et al. 2019a)</cell><cell cols="2">20.6 50.3</cell><cell>64.0</cell><cell>5.3</cell><cell>15.6 40.9</cell><cell>55.2</cell><cell>8.3 yes</cell></row><row><cell></cell><cell>SSB (Patrick et al. 2020)</cell><cell cols="2">28.5 58.6</cell><cell>71.6</cell><cell>3.0</cell><cell></cell><cell></cell><cell>no</cell></row><row><cell></cell><cell>CLIP4Clip (Luo et al. 2021)</cell><cell cols="2">43.1 70.5</cell><cell>81.2</cell><cell>2.0</cell><cell></cell><cell></cell><cell>no</cell><cell>ViT-V/32</cell></row><row><cell></cell><cell>CLIP2Video (Fang et al. 2021)</cell><cell cols="2">43.5 72.3</cell><cell>82.1</cell><cell>2.0</cell><cell>54.6 82.1</cell><cell>90.8</cell><cell>1.0 no</cell><cell>ViT-V/32</cell></row><row><cell></cell><cell>DRL (Wang et al. 2022), ViT-B/32</cell><cell cols="2">45.3 73.9</cell><cell>83.3</cell><cell>2.0</cell><cell></cell><cell></cell><cell>no</cell><cell>ViT-V/32</cell></row><row><cell></cell><cell>CAMoE (Cheng et al. 2021)</cell><cell cols="2">49.1 74.3</cell><cell>84.3</cell><cell>2.0</cell><cell></cell><cell></cell><cell>no</cell><cell>ViT-B/32</cell></row><row><cell></cell><cell>CLIP2TV (Gao et al. 2021)</cell><cell cols="2">54.1 77.4</cell><cell>85.7</cell><cell>1.0</cell><cell></cell><cell></cell><cell>no</cell><cell>ViT-B/16</cell></row><row><cell></cell><cell cols="3">DRL (Wang et al. 2022), ViT-B/16 + QB-n 56.2 79.9</cell><cell>87.4</cell><cell>1.0</cell><cell></cell><cell></cell><cell>no</cell><cell>ViT-B/16</cell></row><row><cell></cell><cell>SSB (Patrick et al. 2020), zero-shot</cell><cell>8.7</cell><cell>23.0</cell><cell>31.1</cell><cell>31.0</cell><cell></cell><cell></cell><cell>no</cell></row><row><cell>Zero-shot</cell><cell>CLIP via Portillo-Quintero (2021)</cell><cell cols="2">27.2 51.7</cell><cell>62.6</cell><cell>5.0</cell><cell>40.3 69.7</cell><cell>79.2</cell><cell>2.0 no</cell><cell>ViT-B/32</cell></row><row><cell></cell><cell>SMs (ours)</cell><cell cols="2">42.8 62.6</cell><cell>70.6</cell><cell>2.0</cell><cell>44.7 71.2</cell><cell>80.0</cell><cell>2.0 yes</cell><cell>ViT-B/32</cell></row><row><cell>Tab. 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>to Socratic Models on egocentric perception, where we show it is possible to quantify the mutual dependence between foundation models without ground truth data. Specifically, to evaluate a new VLM (VLM') for generating language-based world-state history, we first use a baseline VLM VLM paired with the strong LM (sLM) to generate pseudo ground truth predictions VLM?sLM. We then take both the baseline VLM VLM and new VLM VLM', and pair them with a weak LM wLM to generate predictions VLM? wLM and VLM'?wLM respectively. We score these predictions (per image summary) against the pseudo ground truth VLM?sLM. Since the outputs are linguistic, we can measure the similarity of a given prediction to the ground truth, by comparing their sentence embeddings produced by another language model e.g., RoBERTa(Liu et al. 2019b). It is important to use a distinct LM for scoring to avoid spurious correlations with the models under evaluation. This is because the SM combinations that use the same VLM as the one that generates ground truth are biased to produce similar visual grounding results and can exhibit an unfair advantage during the comparisons. Those numbers in our tests have been grayed out in Tab. 3.Liu  et al. 2019a) have been studied -note that results fromLiu  et al. (2019a)  are included in Table1. Investigating further these techniques in the context of recent pretrained foundation models may be a promising direction for future work. Our work may be interpreted as a particular extension of Mixture-of-Experts in which experts may be composed to provide feedback to each other, closed-loop, via the common representation of language.<ref type="bibr" target="#b46">Liu, and Rehg 2018)</ref>.(Kazakos et al. 2019) learns multimodal embeddings (i.e., video + audio), and<ref type="bibr" target="#b24">(Furnari and Farinella 2019)</ref> studies future action anticipation from egocentric videos. Lack of sufficient data however, consistently remains a bottleneck -motivating researchers to construct new larger-scale egocentric video datasets including EPIC-Kitchens<ref type="bibr" target="#b15">(Damen et al. 2018)</ref>, Charades-Ego<ref type="bibr" target="#b77">(Sigurdsson et al. 2018), and</ref> Ego4D (Grauman et al. 2021).</figDesc><table><row><cell>VLM (CLIP) Variants + Weak LM RN50x16 ViT-B/32 ViT-B/16 0.646 0.686 0.861 0.851 0.689 0.655 70.5 63.2 68.6 291M 151M 150M Tab. 3: Unsupervised evaluation (higher is better) of various VLMs Truth Models RN50x4 ViT-L/14 GPT-3 + ViT-B/16 0.628 0.704 GPT-3 + RN50x16 0.667 0.704 ImageNet Accuracy 65.8 76.2 Size (# params) 178M 427M by pairing them with a weak LM and comparing outputs to a VLM paired with a strong LM, which provides relative 'truth gradients' that inform how well the VLMs can compensate for the weak LM. These results suggest that better VLMs (measured by zero-shot Im-ageNet classification accuracies) can improve Socratic synergies. Tab. 3 shows example results of this analysis with GPT-3 "Davinci" as the sLM, and GPT-3 "Curie" as the wLM, to compare VLM (i.e., CLIP) variants with different back-bones: vision transformers (ViT) (Dosovitskiy et al. 2020) and ResNets (RN50) (He et al. 2016) with different model sizes. We find that this method can capture a correlation of ascending performance curve with increasingly better VLMs (e.g., better variants of CLIP) (Radford et al. 2021), as measured by zero-shot image classification accuracy on ImageNet (Deng et al. 2009) -with correlation coefficients of 0.41 and 0.46 between ImageNet accuracies and mean similarity to truth models via ViT-B/16 and RN50x16 re-spectively. We find that with our SM system for egocentric perception (and in contrast to the original setting in (Strope et al. 2011)), it is necessary to use a third baseline VLM bVLM?sLM to generate the pseudo ground truth, instead of VLM?sLM. Multi-model multimodal reasoning. In the context of transfer learning (e.g., via fine-tuning), pre-trained founda-tion models have achieved strong results when combined and trained together for a number of downstream multi-modal (Ngiam et al. 2011) applications including VLMs with LMs for image captioning (e.g., CLIP with GPT-2) (Mokady, Hertz, and Bermano 2021), video understanding (e.g., CLIP with BERT (Gao et al. 2021)), visual ques-tion answering e.g., (Song et al. 2022a) and ALMs and LMs for speech and text modeling e.g., (Song et al. 2022b; Bapna et al. 2022). These systems are often finetuned on task-specific data, and while this paradigm is likely to be preferred in domains for which data is abundant, our initial results suggest that SMs can be a strong zero-shot alterna-tive for applications in which data is less available or more expensive to obtain, e.g., egocentric perception and robotics. The notion as well of "Mixture-of-Experts" ((Jordan and Jacobs 1994), see (Masoudnia and Ebrahimpour 2014) for a review) is a common paradigm for combining the out-puts of multiple models, and specifically mixtures of experts across multimodal domains including vision and audio (Egocentric perception continues to be an important prob-lem in computer vision. Early work in the area explores hand-designed first-person visual features for egocentric ac-tion recognition, object understanding, and video summa-rization. This includes ego-motion (e.g., optical flows) (Ki-tani et al. 2011; Ryoo and Matthies 2013) as well as features from human gaze, hands, and objects (Spriggs, De La Torre, and Hebert 2009; Lee, Ghosh, and Grauman 2012; Fathi, Farhadi, and Rehg 2011; Pirsiavash and Ramanan 2012; Li and Kitani 2013; Lee and Grauman 2015). Focusing on hand-designed features was common in early egocentric vi-sion research, as the availability of data (or videos in gen-eral) was very limited. More recent approaches in egocentric perception leverage learned feature representations, utilizing pre-trained convolutional network features (Ryoo, Rothrock, and Matthies 2015), finetuning them (Ma, Fan, and Kitani 2016; Zellers et al. 2022), or training them from scratch (Bambach et al. 2015) with first-person videos. Similar to the topics explored in early work, learning of visual rep-resentations capturing human hands, objects, and eye gaze has been extensively studied (Garcia-Hernando et al. 2018; Li,</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The name Socratic Models draws inspiration from an analogy to the Socratic Method, but with language-interactable models that may produce or interpret language.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Places: staircase. Objects: stairs, animal, mammal, hamster, human leg. Activities: climbing. 5 Possible Sounds: footsteps, creaking stairs, someone calling your name, a dog barking, a centipede crawling.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Examples on https://youtu.be/-UXKmqBPk1w used with permission from Cody Wanner.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>https://github.com/rmokady/CLIP prefix caption</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Note that due to the non-zero temperature used for sampling from the generative language model, results from this approach are stochastic, but comparable results are producible.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>https://cloud.google.com/speech-to-text. Key used parameters include 'model=video' and 'use enhanced=True'. At 0.006 cents per 15 seconds, this represents an estimated speech-to-text processing cost of under 25 cents (USD) for all MSR-VTT test data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>https://github.com/Deferf/CLIP Video Representation</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9">Acknowledgements</head><p>We thank <rs type="person">Debidatta Dwibedi</rs> and <rs type="person">Matthew O'Kelly</rs> for excellent feedback on improving this manuscript, <rs type="person">Anelia Angelova</rs>, <rs type="person">Jean-Jacques Slotine</rs>, <rs type="person">Jonathan Tompson</rs>, <rs type="person">Maria Attarian</rs>, <rs type="person">Shuran Song</rs>, for fruitful technical discussions, <rs type="person">Kan Huang</rs> for applications support, <rs type="person">Ahmed Omran</rs>, <rs type="person">Aren Jensen</rs>, <rs type="person">Malcolm Slaney</rs>, <rs type="person">Karolis Misiunas</rs> for advice on audio models, and <rs type="person">Cody Wanner</rs> for YouTube videos.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">To index or not to index: Optimizing exact maximum inner product search</title>
		<author>
			<persName><forename type="first">F</forename><surname>Abuzaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th IEEE International Conference on Data Engineering</title>
		<meeting><address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-04-08">2019. 2019. April 8-11, 2019</date>
			<biblScope unit="page" from="1250" to="1261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panagiotou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>D?az-Rodr?guez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11743</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Ruano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jesmonth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quiambao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rettinghouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sievers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2022.00000</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video summarization using deep neural networks: A survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Apostolidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adamantidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Metsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1838" to="1863" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bambach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1949" to="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khanuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01374</idno>
		<title level="m">mslam: Massively multilingual joint pre-training for speech and text</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video summarization: methods and landscape</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agnihotri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitrova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internet Multimedia Management Systems IV</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">5242</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vggsound: A large-scale audio-visual dataset</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="721" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04290</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sehanobish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
		<idno>ICLR 2022 abs/2110.04367</idno>
		<imprint/>
	</monogr>
	<note>Hybrid random features. to appear in</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13256</idno>
		<title level="m">Rescaling egocentric vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Summarization of egocentric videos: A comprehensive survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Del Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepdiary: Lifelogging image captioning and summarization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="40" to="55" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11097</idno>
		<title level="m">Clip2video: Mastering video-text retrieval via image clip</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6252" to="6261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clip2tv: An empirical study on transformer-based methods for video-text retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05610</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2021. 2018</date>
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>First-person hand action benchmark with rgb-d videos and 3d hand pose annotations</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hamburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07058</idno>
		<title level="m">Ego4d: Around the world in 3,000 hours of egocentric video</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13921</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transformer is all you need: Multimodal multitask learning with a unified transformer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints arXiv-2102</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07207</idno>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual ACM symposium on User interface software and technology</title>
		<meeting>the 24th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2011">2022. 2011. 2021. 2021</date>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the em algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5492" to="5501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast unsupervised ego-action learning for first-person sports videos</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3241" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting important objects for egocentric video summarization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="55" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">What is more likely to happen next? video-and-language future event prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07999</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pixel-level hand detection in egocentric videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">H. 2021a. Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">2021b. Egoexo: Transferring visual representations from third-person to firstperson videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="6943" to="6953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12086</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">In the eye of beholder: Joint learning of gaze and actions in first person video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="619" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">All-optical machine learning using diffractive deep neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rivenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Yardimci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jarrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="issue">6406</biblScope>
			<biblScope unit="page" from="1004" to="1008" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<title level="m">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Going deeper into firstperson activity recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1894" to="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mixture of experts: a literature survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Masoudnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09734</idno>
		<title level="m">Clipcap: Clip prefix for image captioning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Oncescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02192</idno>
		<title level="m">Audio retrieval with natural language queries</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Recent advances in video question answering: A review of datasets and methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shastri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="339" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<title level="m">Language models as knowledge bases? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A straightforward framework for video retrieval using clip</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<title level="m">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hopfield networks is all you need</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch?fl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sampled softmax with random fourier features</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>; D'alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="13834" to="13844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Survey of machine learning accelerators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reuther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michaleas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gadepally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kepner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>In 2020 IEEE high performance extreme computing conference (HPEC</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">First-person activity forecasting with online inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3696" to="3705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">First-person activity recognition: What are they doing to me?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2730" to="2737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="896" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1036" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cliport: What and where pathways for robotic manipulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Manuelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="894" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. 2014. December 8-13 2014</date>
			<biblScope unit="page" from="2321" to="2329" />
		</imprint>
	</monogr>
	<note>In Ghahramani</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09626</idno>
		<title level="m">Charades-ego: A large-scale dataset of paired third and first person videos</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10864</idno>
		<title level="m">A short note on the kinetics-700-2020 human action dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07190</idno>
		<title level="m">Clip models are few-shot learners: Empirical studies on vqa and visual entailment</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Large pretrained models on multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence in China</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="506" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Temporal segmentation and activity classification from first-person sensing</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Spriggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beeferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<title level="m">Unsupervised testing strategies for asr</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05263</idno>
		<title level="m">Blocknerf: Scalable large scene neural view synthesis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Lamda: Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<title level="m">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Disentangled representation learning for text-video retrieval</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07111</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Tencent ml-images: A large-scale multi-label image database for visual representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01903</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="172683" to="172693" />
			<date type="published" when="2019">2021. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Robust finetuning of zero-shot models</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">2021a. Wav2clip: Learning robust audio representations from clip</title>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11499</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10862</idno>
		<title level="m">Recursively summarizing books with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14084</idno>
		<title level="m">Videoclip: Contrastive pre-training for zero-shot video-text understanding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">An empirical study of gpt-3 for few-shot knowledge-based vqa</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05014</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02639</idno>
		<title level="m">Merlot reserve: Neural script knowledge through vision and language and sound</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Lit: Zero-shot transfer with locked-image text tuning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07991</idno>
		<idno>arXiv:2112.08995</idno>
	</analytic>
	<monogr>
		<title level="m">Connecting the dots between audio and text without parallel data through visual knowledge transfer</title>
		<imprint>
			<date type="published" when="2019">2019. 2021. 2021</date>
		</imprint>
		<respStmt>
			<orgName>Ph.D. Dissertation, Princeton University.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning visual affordances for robotic manipulation</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Places: An image database for deep scene understanding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02055</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
