<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combinatorial Optimization with Physics-Inspired Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-05">July 5, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Martin</forename><forename type="middle">J A</forename><surname>Schuetz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Quantum Solutions Lab</orgName>
								<address>
									<postCode>98170</postCode>
									<settlement>Seattle</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">AWS Intelligent and Advanced Compute Technologies</orgName>
								<address>
									<postCode>98170</postCode>
									<settlement>Professional Services, Seattle</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">AWS Center for Quantum Computing</orgName>
								<address>
									<postCode>91125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Kyle Brubaker</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AWS Intelligent and Advanced Compute Technologies</orgName>
								<address>
									<postCode>98170</postCode>
									<settlement>Professional Services, Seattle</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Helmut</forename><forename type="middle">G</forename><surname>Katzgraber</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Quantum Solutions Lab</orgName>
								<address>
									<postCode>98170</postCode>
									<settlement>Seattle</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">AWS Intelligent and Advanced Compute Technologies</orgName>
								<address>
									<postCode>98170</postCode>
									<settlement>Professional Services, Seattle</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">AWS Center for Quantum Computing</orgName>
								<address>
									<postCode>91125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combinatorial Optimization with Physics-Inspired Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-05">July 5, 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2107.01188v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We demonstrate how graph neural networks can be used to solve combinatorial optimization problems. Our approach is broadly applicable to canonical NP-hard problems in the form of quadratic unconstrained binary optimization problems, such as maximum cut, minimum vertex cover, maximum independent set, as well as Ising spin glasses and higher-order generalizations thereof in the form of polynomial unconstrained binary optimization problems. We apply a relaxation strategy to the problem Hamiltonian to generate a differentiable loss function with which we train the graph neural network and apply a simple projection to integer variables once the unsupervised training process has completed. We showcase our approach with numerical results for the canonical maximum cut and maximum independent set problems. We find that the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Optimization is ubiquitous across science and industry. Specifically, the field of combinatorial optimization-the search for the minimum of an objective function within a finite but often large set of candidate solutions-is one of the most important areas in the field of optimization, with practical (yet notoriously challenging) applications found in virtually every industry, including both the private and public sectors, as well as in areas such as transportation and logistics, telecommunications, and finance <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. While efficient specialized algorithms exist for specific use cases, most optimization problems remain intractable, especially in real-world applications where problems are more structured and thus require additional steps to make them amenable to traditional optimization techniques. Despite remarkable advances in both algorithms and computing power, significant yet generic improvements have remained elusive, generating an increased interest in new optimization approaches that are broadly applicable and radically different from traditional operations research tools.</p><p>In the broader physics community, the advent of quantum annealing devices such as the D-Wave Systems Inc. quantum annealers <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> has spawned a renewed interest in the development of heuristic approaches to solve discrete optimization problems. On the one hand, recent advances in quantum science and technology have inspired the development of novel classical algorithms, sometimes dubbed nature-inspired or physics-inspired algorithms (e.g., simulated quantum annealing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> running on conventional CMOS hardware) that have raised the bar for emerging quantum annealing hardware; see, for example, Refs. <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. On the other hand, in parallel to these algorithmic developments, substantial progress has been made in recent years on the development of programmable special-purpose devices based on alternative technologies, such as the coherent Ising machine based on optical parametric oscillators <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, digital MemComputing machines based on self-organizing logic gates <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, and the ASIC-based Fujitsu Digital Annealer <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. Some of these approaches face severe scalability limitations. For example, in the coherent Ising machine there is a trade off between precision and the number of variables and the Fujitsu Digital Annealer -baked into an ASIC -can currently handle at most 8192 variables. Thus, it is of much interest to find new alternate approaches to tackle large-scale combinatorial optimization problems, going far beyond what is currently accessible with quantum and nature-inspired approaches alike.</p><p>In the deep learning community, graph neural networks (GNNs) have seen a burst in popularity over the last few years <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. In essence, GNNs are deep neural network architectures specifically designed for graph structure data, with the ability to learn effective </p><formula xml:id="formula_0">C R Z D E j 2 O R S X 1 o 0 q F T 9 m j 8 H W i V B Q a p Q o D m o f P W H C b G C S k M 4 1 r o X + K k J M 6 w M I 5 z O y n 2 r a Y r J B I 9 o z 1 G J B d V h N r 9 1 h s 6 d M k R x o l x J g + b q 7 4 k M C 6 2 n I n K d A p u x X v Z y 8 T + v Z 0 1 8 E 2 Z M p t Z Q S R a L Y s u R S V D + O B o y R Y n h U 0 c w U c z d i s g Y K 0 y M i 6 f s Q g i W X 1</formula><p>4 l 7 X o t u K r V H y 6 r j b s i j h K c w h l c Q A D X 0 I B 7 a E I L C I z h G V 7 h z R P e i / f u f S x a 1 7 x i 5 g T + w P v 8 A b h u j g o = &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q V G o j J e J e i V P m B h t d U h 9 D r d M O J 0 = " &gt; A A A B / X i c b V D L S s N A F L 3 x W e s r P n Z u g k V w V Z I i 6 r K g C 5 c V 7 A O a G C b T S T t 0 M g k z E 6 G G 4 K + 4 c a G I W / / D n X / j p M 1 C W w 8 M H M 6 5 l 3 v m B A m j U t n 2 t 7 G 0 v L K 6 t l 7 Z q G 5 u b e / s m n v 7 H R m n A p M 2 j l k s e g G S h</p><formula xml:id="formula_1">F F O 2 o o q R n q J I C g K G O k G 4 6 v C 7 z 4 Q I W n M 7 9 Q k I V 6 E h p y G F C O l J d 8 8 d C O k R k G Y j X I / c</formula><p>3 m a 3 2 f j 3 D d r d t 2 e w l o k T k l q U K L l m 1 / u I M Z p R L j C D E n Z d + x E e R k S i m J G 8 q q b S p I g P E Z D 0 t e U o 4 h I L 5 u m z 6 0 T r Q y s M B b 6 c W V N 1 d 8 b G Y q k n E S B n i y y y n m v E P / z + q k K L 7 2 M 8 i R V h O P Z o T B l l o q t o g p r Q A X B i k 0 0 Q V h Q n d X C I y Q Q V r q w q i 7 B m f / y I u k 0 6 s 5 5 v X F 7 V m t e l 3 V U 4 A i O 4 R Q c u I A m 3 E A L 2 o D h E Z 7 h F d 6 M J + P F e D c + Z q N L R r l z A H 9 g f P 4 A o 0 G W A Q = = &lt; / l a t e x i t &gt; h k ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V v D P S x 1 g 0 6 + m a N E Z j d X Q Y Z e H d D I = " &gt; A A A B / X i c b V D L S s N A F L 2 p r 1 p f 8 b F z E y y C G 0 t S R F 0 W d O G y g n 1 A G 8 N k O m m H T i Z h Z i L U E P w V N y 4 U c e t / u P N v n L R Z a O u B g c M 5 9 3 L P H D 9 m V C r b / j Z K S 8 s r q 2 v l 9 c r G 5 t b 2 j r m 7 1 5 Z R I j B p 4 Y h F o u s j S R j l p K W o Y q Q b C 4 J C n 5 G O P 7 7 K / c 4 D E Z J G / E 5 N Y u K G a M h p Q D F S W v L M g 3 6 I 1 M g P 0 l H m p U l 2 n 4 5 P n c w z q 3 b N n s J a J E 5 B q l C g 6 Z l f / U G E k 5 B w h R m S s u f Y s X J T J B T F j G S V f i J J j P A Y D U l P U 4 5 C I t 1 0 m j 6 z j r U y s I J I 6 M e V N V V / b 6 Q o l H I S + n o y z y r n v V z 8 z + s l K r h 0 U 8 r j R B G O Z 4 e C h F k q s v I q r A E V B C s 2 0 Q R h Q X V W C 4 + Q Q F j p w i q 6 B G f + y 4 u k X a 8 5 5 7 X 6 7 V m 1 c V 3 U U Y Z D O I I T c O A C G n A D T W g B h k d 4 h l d 4 M 5 6 M F + P d + J i N l o x i Z x / + w P j 8 A f 9 h l Z U = &lt; / l a t e x i t &gt; h k 1 u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i g Q 6 z N 7 I s p B h D 8 A t F C x o 5 X Y o p f w = " &gt; A A A B / 3 i c b V D L S s N A F L 2 p r 1 p f U c G N m 2 A R 3 F i S I u q y o A u X F e w D m l g m 0 0 k 7 d D I J M x O h x C z 8 F T c u F H H r b 7 j z b 5 y 0 W W j r g Y H D O f d y z x w / Z l Q q 2 / 4 2 S k v L K 6 t r 5 f X K x u b W 9 o 6 5 u 9 e W U S I w a e G I R a L r I 0 k Y 5 a S l q G K k G w u C Q p + R j j + + y v 3 O A x G S R v x O T W L i h W j I a U A x U l r q m w d u i N T I D 9 J R 1 k 9 d n m T 3 6 f j U y f p m 1 a 7 Z U 1 i L x C l I F Q o 0 + + a X O 4 h w E h K u M E N S 9 h w 7 V l 6 K h K K Y k a z i J p L E C I / R k P Q 0 5 S g k 0 k u n + T P r W C s D K 4 i E f l x Z U / X 3 R o p C K S e h r y f z t H L e y 8 X / v F 6 i g k s Following a recursive neighborhood aggregation scheme, the graph neural network is iteratively trained against a custom loss function that encodes the specific optimization problem (e.g., Maximum Cut). At training completion, we project the final values for the soft node assignments at the final graph neural network layer back to binary variables xi = 0, 1, providing the solution bit string x = (x1, x2, . . . ). Further details are given in the text.</p><formula xml:id="formula_2">v p T x O F O F 4 d i h I m K U i K y / D G l B B s G I T T R A W V G e 1 8 A g J h J W u r K J L c O a / v E j a 9 Z p z X q v f n l U b 1 0 U d Z T i E I z g B B y 6 g A T f Q h B Z g e I R</formula><p>feature representations of nodes, edges, or even entire graphs. Prime examples of GNN applications include classification of users in social networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, the prediction of future interactions in recommender systems <ref type="bibr" target="#b31">[32]</ref>, and the prediction of certain properties of molecular graphs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. As a convenient and general framework to model a variety of real-world complex structural data, GNNs have successfully been applied to a broad set of problems, including recommender systems in social media and e-commerce <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, the detection of misinformation (fake news) in social media <ref type="bibr" target="#b36">[37]</ref>, and various domains of natural sciences including event classification in particle physics <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, to name a few. While several specific implementations of GNNs exist <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, at their core typically GNNs iteratively update the features of the nodes of a graph by aggregating the information from their neighbors (often referred to as message passing <ref type="bibr" target="#b41">[42]</ref>) thereby iteratively making local updates to the graph structure as the training of the network progresses. Because of their scalability and inherent graph-based design, GNNs present an alternate platform to build large-scale combinatorial heuristics.</p><p>In this work we present a highly-scalable GNNbased solver to (approximately) solve combinatorial optimization problems with up to millions of variables. The approach is schematically depicted in Fig. <ref type="figure">1</ref>, and works as follows: First, we identify the Hamiltonian (cost function) H that encodes the optimization problem in terms of binary decision variables x ? ? {0, 1} and we associate this variable with a vertex ? ? V for an undirected graph G = (V, E) with vertex set V = {1, 2, . . . , n} and the edge set E = {(i, j) : i, j ? V} capturing interactions between the decision variables. We then apply a relaxation strategy to the problem Hamiltonian to generate a differentiable loss function with which we perform unsupervised training on the node representations of the GNN. The GNN follows a standard recursive neighborhood aggregation scheme <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, where each node ? = 1, 2, . . . , n collects information (encoded as feature vectors) of its neighbors to compute its new feature vector h k ? at layer k = 0, 1, . . . , K. After k iterations of aggregation, a node is represented by its transformed feature vector h k ? , which captures the structural information within the node's khop neighborhood <ref type="bibr" target="#b26">[27]</ref>. For binary classification tasks we typically use convolutional aggregation steps, followed by the application of a nonlinear softmax activation function to shrink down the final embeddings h K ? to one-dimensional soft (probabilistic) node assignments</p><formula xml:id="formula_3">p ? = h K ? ? [0, 1].</formula><p>Finally, once the unsupervised training process has completed, we apply a projection heuristic to map these soft assignments p ? back to integer variables x ? ? {0, 1} using, for example, x ? = int(p ? ). We numerically showcase our approach with results for canonical NP-hard optimization problems such as Maximum Cut (MaxCut) and Maximum Independent Set (MIS), showing that our GNN-based approach can perform on par or even better than existing well-established solvers, while being broadly applicable to a large class of optimization problems. Further, the scalability of our approach opens up the possibility of studying unprecedented problem sizes with hundreds of millions of nodes when leveraging distributed training in a mini-batch fashion on a cluster of machines as demonstrated recently in Ref. <ref type="bibr" target="#b43">[44]</ref>.</p><p>The paper is structured as follows. In Sec. II we provide some context for our work, discussing recent developments at the cross-section between machine learning and combinatorial optimization. Section III summarizes the basic concepts underlying our approach, as well as information on the class of problems that this approach can solve.</p><p>Section IV outlines the implementation of the proposed GNN-based optimizer, followed by numerical experiments in Sec. V. In Sec. VI we draw conclusions and give an outlook on future directions of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this Section we briefly review relevant existing literature, with the goal to provide a detailed context for our work.</p><p>Broadly speaking, our work makes a physics-inspired contribution to the emerging crossfertilization between combinatorial optimization and machine learning, where the development of novel deep learning architectures has sparked a renewed interest in heuristics for solving NP-hard combinatorial optimization problems using neural networks, as extensively reviewed in e.g., Refs. <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Leaving alternative, non-graph-based approaches as presented for example in Ref. <ref type="bibr" target="#b46">[47]</ref> aside, in the following short survey we focus on graph-based optimization problems-where modern deep learning architectures such as sequence models, attention mechanisms, and GNNs provide a natural tool set <ref type="bibr" target="#b44">[45]</ref>-and we primarily distinguish between approaches based on supervised learning, reinforcement learning, or unsupervised learning. This categorization can be refined further with respect to the typical size of a problem solved by a specific approach and the scope of the solver (special-purpose vs generalpurpose).</p><p>Supervised Learning. The majority of neuralnetwork-based approaches to combinatorial optimization are based on supervised learning, with the goal to approximate some (typically complex, non-linear) mapping from an input representation of the problem to the target solution, based on the minimization of some empirical, handcrafted loss function. Early work was based on pointer networks which leverage sequence-to-sequence models to produce permutations over inputs of variable size, as, for example, relevant for the canonical traveling salesman problem (TSP) <ref type="bibr" target="#b47">[48]</ref>. Since then, numerous studies have fused GNNs with various heuristics and search procedures to solve specific combinatorial optimization problems, such as quadratic assignment <ref type="bibr" target="#b48">[49]</ref>, graph matching <ref type="bibr" target="#b49">[50]</ref>, graph coloring <ref type="bibr" target="#b50">[51]</ref>, and the TSP <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>. As pointed out in Ref. <ref type="bibr" target="#b53">[54]</ref>, however, the viability and performance of supervised approaches critically depends on the existence of large, labelled training data sets with previously optimized hard problem instances, resulting in a problematic chicken-and-egg scenario, that is further amplified by the fact that it is hard to efficiently sample unbiased and representative labeled instances of NP-hard problems <ref type="bibr" target="#b54">[55]</ref>.</p><p>Reinforcement Learning. The critical need for training labels can be circumvented with Reinforcement Learning (RL) techniques that aim to learn a policy with the goal of maximizing some expected reward function. Specifically, optimization problems can typically be described with a native objective function that can then serve as a reward function in an RL approach <ref type="bibr" target="#b44">[45]</ref>. Motivated by the challenges associated with the need for optimal target solutions, Bello et al. extended the pointer network architecture <ref type="bibr" target="#b47">[48]</ref> to an actor-critic RL framework to train an approximate TSP solver, using a recurrent neural network encoder scheme and the expected tour length as a reward signal <ref type="bibr" target="#b55">[56]</ref>.</p><p>Using a general RL framework based on a graph attention network architecture <ref type="bibr" target="#b40">[41]</ref>, significant improvements in accuracy on two-dimensional euclidean TSP have subsequently been presented in Ref. <ref type="bibr" target="#b56">[57]</ref>, getting close to optimal results for problems up to 100 nodes. Moreover, TSP variants with hard constraints have been analyzed in Ref. <ref type="bibr" target="#b57">[58]</ref>, with the help of a multi-level RL framework in which each layer of a hierarchy learns a different policy, and from which actions can then be sampled. Finally, while the majority of the RL-based approaches have focused on the TSP or variants thereof, Dai et al. proposed a combination of RL and graph embedding to learn efficient greedy meta-heuristics to incrementally construct a solution, and showcased their approach with numerical results for Minimum Vertex Cover, MaxCut, and TSP as test problems, for graphs with up to ? 1000 -1200 nodes <ref type="bibr" target="#b58">[59]</ref>.</p><p>Unsupervised Learning. Conceptually, our work is most similar to those that aim to train neural networks in an unsupervised, end-to-end fashion, without the need for labelled training sets <ref type="bibr" target="#b53">[54]</ref>. Specifically, Toenshoff et al. have recently used a recurrent GNN architecturedubbed RUN-CSP-to solve optimization problems that can be framed as maximum constraint satisfaction problems <ref type="bibr" target="#b59">[60]</ref>. For other types of problems, such as the Maximum Independent Set problem, the model relies on empirically-selected hand-crafted loss functions. Using the language of constraint satisfaction problems, where the system size is expressed in terms of both the number of variables and the number of constraints, the authors solve problem instances of Maximum 2satisfiability, 3-colorability, MaxCut and Maximum Independent Set with up to 5000 nodes, showing that RUN-CSP can compete with traditional approaches like greedy heuristics or semi-definite programming. Finally, by either optimizing a smooth relaxation of the cut objective or applying a policy gradient, Yao et al. trained a GNN to specifically solve the Maximum Cut problem, albeit at relatively small system sizes with up to 500 nodes <ref type="bibr" target="#b60">[61]</ref>, and without any details on runtime.</p><p>Here, we present a highly-scalable, physics-inspired framework that uses deep-learning tools in the form of GNNs to approximate solutions to hard combinatorial optimization problems with up to millions of variables.</p><p>Our GNN optimizer is based on a direct mathematical relation between prototypical Ising spin Hamiltonians <ref type="bibr" target="#b61">[62]</ref>, the Quadratic Binary Unconstrained Optimization (QUBO) and Polynomial Binary Unconstrained Optimization (PUBO) formalism and the differentiable loss function with which we train the GNN, thereby providing one unifying framework for a broad class of combinatorial optimization problems, and opening up the powerful toolbox of statistical physics to modern deep-learning approaches. Fusing concepts from statistical physics with modern machine learning tooling, we propose a simple, generic, and robust solver that does not rely on hand-crafted loss functions. Specifically, we show that the same GNN optimizer can solve different QUBO problems, without any need to change the architecture or loss function, while scaling to problem instances orders of magnitude larger than what many traditional QUBO solvers can handle <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head><p>To set up our notation and terminology we start out with a brief review of both combinatorial optimization, and graph neural networks.</p><p>Combinatorial Optimization.</p><p>The field of combinatorial optimization is concerned with settings where a large number of yes/no decisions must be made and each set of decisions yields a corresponding objective function value, like a cost or profit value, that is to be optimized <ref type="bibr" target="#b0">[1]</ref>. Canonical combinatorial optimization problems include, among others, the maximum cut problem (MaxCut), the maximum independent set problem (MIS), the minimum vertex cover problem, the maximum clique problem and the set cover problem. In all cases exact solutions are not feasible for sufficiently-large systems due to the exponential growth of the solution space as the number of variables n increases. Bespoke (approximate) algorithms to solve these problems can typically be identified, at the cost of limited scope and generalizability. Conversely, in recent years the QUBO framework has resulted in a powerful approach that unifies a rich variety of these NPhard combinatorial optimization problems <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b64">65]</ref>. The cost function for a QUBO problem can be expressed in compact form with the following Hamiltonian </p><formula xml:id="formula_4">H QUBO = x Qx = i,j x i Q ij x j ,<label>(1)</label></formula><formula xml:id="formula_5">O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U d A e l s l t x F y D r x M t J G X I 0 B q W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m x n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k 0 i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D W z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D</formula><p>M 2 m 6 I N w V t 9 e Z 2 0 q x X v u l J t 1 s r 1 W h 5 H A c 7 h A q 7 A g x u o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 b T j 5 z B n / g f P 4 A d y 2 M r Q = = &lt; / l a t e x i t &gt; 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X G t B g T L q j 9 8 u z        Training strategy</p><formula xml:id="formula_6">L v X Q d O a 1 V o w 8 9 U = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U 9 A a l s l t x F y D r x M t J G X I 0 B q W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m x n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k 0 i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D W z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 6 I N w V t 9 e Z 2 0 q x X v u l J t 1 s r 1 W h 5 H A c 7 h A q 7 A g x u o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 b T j 5 z B n / g f P 4 A e L G M r g = = &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R p k y d w Y u L 2 K K g B B f p Q 5 m q v p I Z q U = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U r A 5 K Z b f i L k D W i Z e T M u R o D E p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N i P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I p V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N k U b g r f 6 8 j p p V y v e d a X a r J X r t T y O A p z D B V y B B z d Q h 3 t o Q A s Y I D z D K</formula><formula xml:id="formula_7">Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S 4 6 p f L L l l d w G y T r y M l C B D v V / 8 6 g 1 i l k Y o D R N U 6 6 7 n J s a f U m U 4 E z g r 9 F K N C W V j O s S u p Z J G q P 3 p 4 t A Z u b D K g I S x s i U N W a i / J 6 Y 0 0 n o S B b Y z o m a k V 7 2 5 + J / X T U 1 4 6 0 + 5 T F K D k i 0 X h a k g J i b z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u C D c F b f X m d t C p l 7 7 p c a V R L t W o W R x 7 O 4 B w u w Y M b q M E 9 1 K E J D B C e 4 R</formula><formula xml:id="formula_8">G Q 6 c S R X p M Z 6 z p l x O U w k = " &gt; A A A B 5 H i c b V B N S 8 N A E J 3 U r x q / q l c v i 0 X w V J J S 1 G P B i 8 c K 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h S v / i Z v / h u 3 b Q 7 a + m D g 8 d 4 M M / P C V H B t P O / b K W 1 t 7 + z u l f f d g 8 O j 4 5 O K e 9 r R S a Y Y t l k i E t U L q U b B J b Y N N w J 7 q U I a h w K 7 4 f R u 4 X e f U W m e y E c z S z G I 6 V j y i D N q r P T Q G F a q X s 1 b g m w S v y B V K N A a V r 4 G o 4 R l M U r D B N W 6 7 3 u p C X K q D G c C 5 + 4 g 0 5 h S N q V j 7 F s q a Y w 6 y J e H z s m l V U Y k S p Q t a c h S / T 2 R 0 1 j r W R z a z p i a i V 7 3 F u J / X j 8 z 0 W 2 Q c 5 l m B i V b L Y o y Q U x C F l + T E V f I j J h Z Q p n i 9 l b C J l R R Z m w 2 r g 3 B X 3 9 5 k 3 T q N f + 6 V q 8 2 G 0 U Y Z T i H C 7 g C H 2 6 g C f f Q g j Y w Q H i B N 3 h 3 n p x X 5 2 P V W H K K i T P 4 A + f z B x L n i 4 c = &lt; / l a t e x i t &gt; 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X G t B g T L q j 9 8 u z L v X Q d O a 1 V o w 8 9 U = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U 9 A a l s l t x F y D r x M t J G X I 0 B q W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m x n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k 0 i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D W z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 6 I N w V t 9 e Z 2 0 q x X v u l J t 1 s r 1 W h 5 H A c 7 h A q 7 A g x u o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 b T j 5 z B n / g f P 4 A e L G M r g = = &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R p k y d w Y u L 2 K K g B B f p Q 5 m q v p I Z q U = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U r A 5 K Z b f i L k D W i Z e T M u R o D E p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N i P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I p V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N k U b g r f 6 8 j p p V y v e d a X a r J X r t T y O A p z D B V y B B z d Q h 3 t o Q A s Y I D z D K</formula><formula xml:id="formula_9">Q = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U d A e l s l t x F y D r x M t J G X I 0 B q W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m x n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k 0 i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D W z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 6 I N w V t 9 e Z 2 0 q x X v u l J t 1 s r 1 W h 5 H A c 7 h A q 7 A g x u o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 b T j 5 z B n / g f P 4 A d y 2 M r Q = = &lt; / l a t e x i t &gt; 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X G t B g T L q j 9 8 u z L v X Q d O a 1 V o w 8 9 U = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U 9 A a l s l t x F y D r x M t J G X I 0 B q W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m x n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k 0 i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D W z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 6 I N w V t 9 e Z 2 0 q x X v u l J t 1 s r 1 W h 5 H A c 7 h A q 7 A g x u o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c</formula><formula xml:id="formula_10">O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U r A 5 K Z b f i L k D W i Z e T M u R o D E p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N i P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I p V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N k</formula><formula xml:id="formula_11">k i E t U L q U b B J b Y N N w J 7 q U I a h w K 7 4 f R u 4 X e f U W m e y E c z S z G I 6 V j y i D N q r P T Q G F a q X s 1 b g m w S v y B V K N A a V r 4 G o 4 R l M U r D B N W 6 7 3 u p C X K q D G c C 5 + 4 g 0 5 h S N q V j 7 F s q a Y w 6 y J e H z s m l V U Y k S p Q t a c h S / T 2 R 0 1 j r W R z a z p i a i V 7 3 F u J / X j 8 z 0 W 2 Q c 5 l m B i V b L Y o y Q U x C F l + T E V f I j J h Z Q p n i 9 l b C J l R R Z m w 2 r g 3 B X 3 9 5 k 3 T q N f + 6 V q 8 2 G 0 U Y Z T i H C 7 g C H 2 6 g C f f Q g j Y w Q H i B N 3 h 3 n p x X 5 2 P V W H K K i T P 4 A + f z B x L n i 4 c = &lt; / l a t e x i t &gt;</formula><formula xml:id="formula_12">O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U d A e l s l t x F y D r x M t J G X I 0 B q W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m x n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k 0 i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D W z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 6 I N w V t 9 e Z 2 0 q x X v u l J t 1 s r 1 W h 5 H A c 7 h A q 7 A g x u o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c</formula><formula xml:id="formula_13">l u v 1 B C C G S k 3 U M O Q q u N W q g 2 I b G e B E l i h N S y + D 0 Y R S U I q N O F Y q T 5 y Y u 2 l W G p G O J 0 X B o m i M S Z T P K Z 9 Q w U O q f L S x a l z e G a U E Q w i a U p o u F C / T 6 Q 4 V G o W + q Y z x H q i f n u Z + J f X T 3 R Q 9 1 I m 4 k R T Q Z a L g o R D H c H s b z h i k h L N Z 4 Z g I p m 5 F Z I J l p h o k 0 7 B h P D 1 K f y f d M o 2 q t n l G 7 f U d F d x 5 M E J O A X n A I E L 0 A T X o A X</formula><p>? GNN ansatz (GCN, GAT, ?)</p><p>? Hyperparameters</p><p>? ML optimizer (Adam, SGD, ?)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Projection scheme d</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X G t B g T L q j 9 8  </p><formula xml:id="formula_14">u z L v X Q d O a 1 V o w 8 9 U = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U 9 A a l s l t x F y D r x M t J G X I 0 B q W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m x n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k 0 i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D W z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 6 I N w V t 9 e Z 2 0 q x X v u l J t 1 s r 1 W h 5 H A c 7 h A q 7 A g x u o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c</formula><formula xml:id="formula_15">W A h V E 6 6 Z j x 9 B K i Q J O B c u K X q J Z T G i f X L O m g S G R T L f S 3 H m G N w 3 T x p 1 I m R M C z t n v E y m R W g 9 k Y D q H 3 v R v b U j + p z U T 6 B y 2 U h 7 G C b C Q j h Z 1 E o E h w s M Y c Z s r R k E M D C B U c f N X T L t E E Q o m</formula><formula xml:id="formula_16">H QUBO = x | Qx = X i,j x i Q ij x j &lt; l a t e x i t s h a _ b a s e = " a U / m J S p Q O G p m S C u d Z t N P B = " &gt; A A A C c i c b V F N S N A E N E r x q / o o I X D y W x Y s l K W K C B U v H h W s C k p m + L t s w u G L K F / w J / n z X / h x b u b N n H R h + Z n Y m T D h T v P e L X t u f m F x q b T s r K y u r W + m v K k l h Q a N e S w f Q K A M w E N z T S H x Q C i U I O D H / K t c f n k E q F o s P U i g F Z G e Y F G i T Z U x B f Y C U L o M Z E l E d G S v Q w d D x h v h v l E H g / D + h R m k n O T h / y v + A E I D q / T d t u a t o D T w C A G R V x b f g k M w i E p p w o f S R L c y I j W j H I Z O k C p I C O T H j Q N F C Q C c p G O x v i Q N c D e W J o X G I / a v I y O R U o M o N J V m w C c q e X k L K Z u K M i S T U I O m U T T n W M c P g D t M A t V Y A C h k p l Z M X i k l B t z p Q v w Z / j S r b s r r R c r x X r K K F d t I + O k Y q q I u Q q I I o + r B r z L W p r s H L b K j z b F / Y J / c K h C &lt; / l a t e x i t &gt; A = 0 B B B B @ 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 C C C C A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 3 S m m 8 o C + G F l E J O o 3 R B 3 P 4 0 1 y A k = " &gt; A A A C e X i c b Z F N S 8 N A E I Y 3 8 a v G r 6 p H P a y W S h E s S S v q R S h 4 8 a h g V W h K 2 W y n d e l m E 3 Y 3 Y g n 9 D / 4 2 b / 4 R L 1 7 c p q n U 1 o G B l + e d 2 Y + Z I O Z M a d f 9 t O y l 5 Z X V t c K 6 s 7 G 5 t b 1 T 3 N 1 7 V F E i K T R p x C P 5 H B A F n A l o a q Y 5 P M c S S B h w e A o G N 2 P / 6 R W k Y p F 4 0 M M Y 2 i H p C 9 Z j l G i D O s X 3 e 3 y N H T + A P h N p H B I t 2 d v I O a v h E z x N N 0 v f d 8 b i b E p q s z R z 6 r 8 9 M 3 T G m a P 5 a Y b 6 I L q / d 3 e K J b f q Z o E X h Z e L E s r j r l P 8 8 L s R T U I Q m n K i V M t z Y 9 1 O i d S M c h g 5 f q I g J n R A + t A y U p A Q V D v N J j f C Z U O 6 u B d J k 0 L j j M 5 2 p C R U a h g G p t K 8 7 0 X N e 2 P 4 n 9 d K d O + q n T I R J x o E n V z U S z j W E R 6 v A X e Z B K r 5 0 A h C J T N v x f S F S E K 1 W Z Z j h u D N f 3 l R P N a q 3 k W 1 d n 9 e a l T y c R T Q A T p G F e S h S 9 R A t + g O N R F F X 9 a h V b Z O r G / 7 y K 7 Y p 5 N S 2 8 p 7 9 t G f s O s / D p + q G g = = &lt; / l a t e x i t &gt; Q = 0 B B B B @ 2 2 2 0 0 0 2 0 2 0 0 0 3 2 2 0 0 0 3 2 0 0 0 0 2 1 C C C C A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y x 9 a O S k T y 7 H f j U c R i S X j Y B L e V W Q = " &gt; A A A C d 3 i c b Z F N S 8 N A E I Y 3 8 a v G r 6 o 3 P b h Y L b 1 Y k i L q R S h 4 8 d i C r U J T y m Y 7 r U s 3 m 7 C 7 E U v o X / D H e f N / e P H m N g 2 l t g 4 M v L z P D j M 7 E 8 S c K e 2 6 X 5 a 9 t r 6 x u V X Y d n Z 2 9 / Y P i o d H b R U l k k K L R j y S L w F R w J m A l m a a w 0 s s g Y Q B h + d g 9 D D l z 2 8 g F Y v E k x 7 H 0 A 3 J U L A B o 0 Q b q 1 f 8 a O J 7 7 P g B D J l I 4 5 B o y d 4 n z p W H y 7 i R p 5 u l 7 z t T k R F 3 T n J 3 T m Y 1 C + 4 C W X J z 4 v g g + v P O v W L J r b p Z 4 F X h 5 a K E 8 m j 0 i p 9 + P 6 J J C E J T T p T q e G 6 s u y m R m l E O E 8 d P F M S E j s g Q O k Y K E o L q p t n e J v j S O H 0 8 i K R J o X H m L l a k J F R q H A b m p Z n v V S 2 z q f k f 6 y R 6 c N d N m Y g T D Y L O G g 0 S j n W E p 0 f A f S a B a j 4 2 g l D J z K y Y v h J J q D a n c s w S v O U v r 4 p 2 r e r d V G v N 6 1 K 9 k q + j g E 7 R O a o g D 9 2 i O n p E D d R C F H 1 b J 1 b J u r B + 7 D O 7 b O d v b S u v O U Z / w v Z + A a u D q f s = &lt; / l a t e x i t &gt; Q = 0 B B B B @ 1 P P 0 0 0 1 0 P 0 0 0 1 P P 0 0 0 1 P 0 0 0 0 1 1 C C C C A</formula><p>a c e b &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 1  where x = (x 1 , x 2 , . . . ) is a vector of binary decision variables and the QUBO matrix Q is a square matrix of constant numbers that encodes the actual problem to solve. Without loss of generality, the Q-matrix can be assumed to be symmetric or in upper triangular form <ref type="bibr" target="#b0">[1]</ref>. We have omitted any irrelevant constant terms, as well as any linear terms as these can always be absorbed into the Q-matrix because x 2 i = x i for binary variables x i ? {0, 1}. Problem constraints, as relevant for many real-world optimization problems, can be accounted for with the help of penalty terms entering the objective function (rather than being explicitly imposed), as detailed in Ref. <ref type="bibr" target="#b0">[1]</ref>. The significance of QUBO problems is further illustrated by the close relation to the famous Ising model, which is known to provide mathematical formulations for many NPcomplete and NP-hard problems, including all of Karp's 21 NP-complete problems <ref type="bibr" target="#b64">[65]</ref>. As opposed to QUBO problems, Ising problems are described in terms of binary spin variables z i ? {-1, 1}, that can be mapped straightforwardly to their equivalent QUBO form, and vice versa, using z i = 2x i -1. By definition, both the QUBO and the Ising models are quadratic, but can be naturally generalized to higher order PUBO problems, as described by the N -local Hamiltonian</p><formula xml:id="formula_17">+ P q I K 7 e V M s R R R q H x k d / V 5 j G t E = " &gt; A A A B 6 H i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L b s x G H M L e P G Y g H l A s o T Z S S c Z M / t g Z l Y I S 7 7 A i w d F v P p J 3 v w b J 8 k K U b S g o a j q p r v L j w V X 2 n E + r b X 1 j c 2 t 7 d x O f n d v / + C w c H T c U l E i G T Z Z J C L Z 8 a l C w U N s a q 4 F d m K J N P A F t v 3 J z d x v P 6 B U P A r v 9 D R G L 6 C j k A 8 5 o 9 p I j c t + o e j a z g L E s a s G l X J G q i 7 5 t o q Q o d 4 v f P Q G E U s C D D U T V K m u 6 8 T a S 6 n U n A m c 5 X u J w p i y C R 1 h 1 9 C Q B q i 8 d H H o j J w b Z U C G k T Q V a r J Q V y d S G i g 1 D X z T G V A 9 V r + 9 u f i X 1 0 3 0 8 N p L e R g n G k O 2 X D R M B N E R m X 9 N B l w i 0 2 J q C G W S m 1 s J G 1 N J m T b Z 5 F d D + J + 0 S r Z 7 Z Z c a 5 W K t n M W R</formula><formula xml:id="formula_18">f c m Q 9 X m / O F k 7 Z N 4 6 K v H z 1 j m U C T E = " &gt; A A A B 5 H i c d V D L S g N B E O y N r 7 i + o l c v g 0 H w t O y G Y M w t 4 M V j B P O A Z A m z k 9 5 k z O y D m V k h L P k C L x 4 U r 3 6 T N / / G S b K C i h Y 0 F F X d d H c F q e B K u + 6 H V d r Y 3 N r e K e / a e / s H h 0 c V + 7 i r k k w y 7 L B E J L I f U I W C x 9 j R X A v s p x J p F A j s B b P r p d 9 7 Q K l 4 E t / p e Y p + R C c x D z m j 2 k i 3 9 V G l 6 j n u C s R 1 m g a N e k G a H v m y q l C g P a q 8 D 8 c J y y K M N R N U q Y H n p t r P q d S c C V z Y w 0 x h S t m M T n B g a E w j V H 6 + O n R B z o 0 y J m E i T c W a r N T v E z m N l J p H g e m M q J 6 q 3 9 5 S / M s b Z D q 8 8 n M e p 5 n G m K 0 X h Z k g O i H L r 8 m Y S 2 R a z A 2 h T H J z K 2 F T K i n T J h v</formula><formula xml:id="formula_19">H PUBO = N k=0 i1,i2,...,i k Q i1i2???i k x i1 x i2 ? ? ? x i k ,<label>(2)</label></formula><p>with real-numbered coefficients Q i1i2???i k , for some N ? 3, and i 1 , i 2 , . . . , i k indicating a group of k binary variables (or spins in the Ising formulation). Terms containing a product of k variables, of the form</p><formula xml:id="formula_20">Q i1i2???i k x i1 x i2 ? ? ? x i k , are commonly referred to as k-local interactions with Q i1i2???i k being the coupling constant.</formula><p>As we exemplify below for some canonical problems, graph (hypergraph) problems can be naturally framed as QUBO (PUBO) problems. To this end, given an undirected graph G = (V, E), we simply associate a binary variable x i with every vertex i ? V , and then express the (node classification) objective as a QUBO problem, where the specific assignment x can be visualized as a specific twotone (e.g., light and dark) coloring of the graph <ref type="bibr" target="#b65">[66]</ref>; see Fig. <ref type="figure">1</ref>. Graph Neural Networks. On a high level, GNNs are a family of neural networks capable of learning how to aggregate information in graphs for the purpose of representation learning. Typically, a GNN layer is comprised of three functions <ref type="bibr" target="#b33">[34]</ref>: (i) a message passing function that permits information exchange between nodes over edges, (ii) an aggregation function that combines the collection of received messages into a single, fixed-length representation, and (iii) a (typically nonlinear) update activation function that produces node-level representations given the previous layer representation and the aggregated information. While a single-layer GNN encapsulates a node's features based on its immediate or one-hop neighborhood, by stacking multiple layers, the model can propagate each node's features through intermediate nodes, analogous to the broadening the receptive field in downstream layers of convolutional neural networks. Formally, at layer k = 0, each node ? ? V is represented by some initial representation h 0 ? ? R d0 , usually derived from the node's label or given input features of dimensionality d 0 <ref type="bibr" target="#b66">[67]</ref>. Following a recursive neighborhood aggregation scheme, the GNN then iteratively updates each node's representation, in general described by some parametric function f k ? , resulting in</p><formula xml:id="formula_21">h k ? = f k ? h k-1 ? , {h k-1 u |u ? N ? } ,<label>(3)</label></formula><p>for the layers k = 1, . . . , K, with N ? = {u ? V|(u, ?) ? E} referring to the local neighborhood of node ?, i.e., the set of nodes that share edges with node ?. The total number of layers K is usually determined empirically as a hyperparameter, as are the intermediate representation dimensionality d k . Both can be optimized in an outer loop. While a growing number of possible implementations for GNN architectures <ref type="bibr" target="#b28">[29]</ref> exists, here we use a graph convolutional network (GCN) <ref type="bibr" target="#b27">[28]</ref> for which Eq. ( <ref type="formula" target="#formula_21">3</ref>) reads explicitly as</p><formula xml:id="formula_22">h k ? = ? ? ? W k u?N (?) h k-1 u |N (?)| + B k h k-1 ? ? ? ,<label>(4)</label></formula><p>with W k and B k being (shared) trainable weight matrices, the denominator |N (?)| serving as normalization factor (with other choices available as well) and ?(?) being some (component-wise) nonlinear activation function such as sigmoid or ReLU. While GNNs can be used for various prediction tasks (including node classification, link prediction, community detection, network similarity, or graph classification), here we focus on node classification, where usually the last (K-th) layer's output is used to predict a label y ? for every node ? ? V. To this end, we feed the (parametrized) final node embeddings z ? = h K ? (?) into a problem-specific loss function and run stochastic gradient descent to train the weight parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMBINATORIAL OPTIMIZATION WITH GRAPH NEURAL NETWORKS</head><p>We now detail how to use GNNs to solve combinatorial optimization problems, as schematically outlined in FIG. <ref type="figure" target="#fig_4">3</ref>: Example solution to MaxCut for a random 3-regular graph with n = 100 nodes. After training completion, the GNN provides a binary bit string x that assigns one of two possible colors (e.g., black or white) to each vertex. An edge is said to be cut when it connects two vertices of different colors. For a given graph, the optimization problem is to assign the colors in a way that as many edges as possible can be cut at the same time (corresponding to the antiferromagnetic ground-state of the system).</p><p>Fig. <ref type="figure" target="#fig_3">2</ref>. To this end, we frame combinatorial optimization problems as unsupervised node classification tasks, without the need for any labelled data. Because the nodes do not carry any inherent features, in our setup the node embeddings h 0 ? are initialized randomly. Warmstarting the training process with pre-training (transfer learning) will be left for future research. The class of Hamiltonians described above are not differentiable and cannot be used straightforwardly within the GNN training process.</p><p>Therefore, for a given problem Hamiltonian H and graph G, we generate a differentiable loss function L(?), as required for standard backpropagation, by promoting the binary decision variables x i ? {0, 1} to continuous (parametrized) probability parameters p i (?) with the following (heuristic) relaxation approach</p><formula xml:id="formula_23">x i -? p i (?) ? [0, 1].<label>(5)</label></formula><p>The soft assignments p i can be viewed as class probabilities. They are generated by our GNN Ansatz as final node embeddings p i = h K i ? [0, 1] at layer K, after the application of a non-linear softmax activation function. Then, they are used as input for the loss function L(?). In particular, for QUBO-type problems:</p><formula xml:id="formula_24">H QUBO -? L QUBO (?) = i,j p i (?)Q ij p j (?),<label>(6)</label></formula><p>which is differentiable with respect to the parameters of the GNN model ?, and similarly for PUBO problems on hyper-graphs with higher-order terms of the form p i p j p k etc., thereby establishing a straightforward, general connection between combinatorial optimization problems, Ising Hamiltonians and GNNs. For training with gradient descent, standard ML optimizers such as ADAM can be used. Once the (unsupervised) training process has completed, we apply projection heuristics to map these soft assignments p i back to integer variables x i = 0, 1, using for example simply x i = int(p i ).</p><p>The application of other, more sophisticated projection schemes will be left for future research. Note that any projection heuristics can be applied throughout training after every epoch, thereby increasing the pool of solution candidates, at no additional computational cost. With the GNN guiding the search through the solution space, one can then book keep all solution candidates identified throughout training and simply pick the best solution found. Our general GNN approach features several hyperparameters, including the number of layers K, the dimensionality of the embedding vectors h k i , and the learning rate ?, with details depending on the specific architecture and optimizer used. These can be fine-tuned and optimized in an outer loop, using, e.g., standard techniques such as grid search or more advanced Bayesian optimization methods.</p><p>Listing 1: Core code block of example script based on the DGL library. The first block defines a two-layer GCN architecture Ansatz; the second code block defines the loss function as described by Eq. ( <ref type="formula" target="#formula_24">6</ref>). Further details can be found in the main text.</p><p># Import r e q u i r e d p a c k a g e s import d g l import t o r c h import t o r c h . nn a s nn from d g l . nn . p y t o r c h import GraphConv # D e f i n e two-l a y e r GCN c l a s s GCN( nn . Module ) : def __init__ ( s e l f , i n _ f e a t s , hidden , c l a s s e s ) : super (GCN, s e l f ) . __init__ ( ) s e l f . conv1 = GraphConv ( i n _ f e a t s , hi dd en ) s e l f . conv2 = GraphConv ( hidden , c l a s s e s ) def f o r w a r d ( s e l f , g , i n p u t s ) : h = s e l f . conv1 ( g , i n p u t s ) h = t o r c h . r e l u ( h ) h = s e l f . conv2 ( g , h )</p><formula xml:id="formula_25"># b i n a r y c l a s s i f i c a t i o n h = t o r c h . s i g m o i d ( h ) return h # D e f i n e custom l o s s f u n c t i o n f o r QUBOs def l o s s _ f u n c ( probs_ , Q_mat ) :</formula><p>""" f u n c t i o n t o compute c o s t v a l u e f o r g i v e n s o f t a s s i g n m e n t s and p r e d e f i n e d QUBO m a t r i x """ # m i n i m i z e c o s t = x . T * Q * x c o s t = ( probs_ . T @ Q_mat @ probs_ ) . s q u e e z e ( ) return c o s t</p><p>Our GNN-based approach can be readily implemented with open-source libraries such as PyTorch Geometric <ref type="bibr" target="#b67">[68]</ref> or the Deep Graph Library <ref type="bibr" target="#b68">[69]</ref>. The core of the corresponding code is displayed in Listing 1 for a GCN with two layers and a loss function for any QUBO problem. For illustration, an example solution to the archetypal MaxCut problem (as implemented with this Ansatz) for a 3-regular graph with n = 100 vertices is shown in Fig. <ref type="figure" target="#fig_4">3</ref>. Here, the cut size achieved with our GNN method amounts to 132. Further details are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. NUMERICAL EXPERIMENTS</head><p>We perform numerical experiments using MaxCut and MIS benchmark problems. Before providing details on these numerical experiments, we first describe our GNN model architecture as it is consistent across the d-regular MaxCut and MIS problem instances described below. It is certainly possible that better solutions can be found by fine-tuning the hyper-parameters for every given problem instance. However, one of our goals is to design a robust and scalable solver that is able to solve a large sample of instances efficiently without the need of hand-tuning the parameters on an instance-by-instance base.</p><p>GNN Architecture. We use a simple two-layer GCN architecture based on PyTorch GraphConv units. The first convolutional layer is fed the node embeddings of dimension d 0 and outputs a representation of size d 1 . Next, we apply a component-wise, non-linear ReLU transformation. The second convolutional layer is then fed this intermediate representation and outputs the output layer of size d 2 , which is then fed through the component-wise sigmoid transformation to provide a soft probability p i ? [0, 1] for every node i ? V. We find that the following simple heuristic for determining the hyper-parameters d 0 and d 1 works well: if the number of nodes is large (n ? 10 5 ), then we set</p><formula xml:id="formula_26">d 0 = int( ? n),</formula><p>else we set d 0 = int( 3 ? n), and we take d 1 = int(d 0 /2).</p><p>Because we solve for binary classification tasks, we set the final output dimension as d 2 = 1. However, for multi-color problems this could be extended to C &gt; 2 classes by passing the output layer through a softmax transformation (instead of a sigmoid) and taking the argmax. Note that as the graph size scales beyond ? 10 5 nodes, memory becomes a concern, and so we further reduce the representations to allow the GNN to be trained on a single GPU. Distributed training leveraging a whole cluster of machines will be discussed below in Sec. VI. With the GNN's output depending on the random initialization of the hidden feature vectors there is a risk of becoming stuck in a local optimum where the GNN stops learning. To counter this issue, one can take multiple shots (i.e., run the GNN training multiple times for different random seeds and choose the best solution), thereby boosting the performance at the cost of extended runtime. In our numerical experiments we limited the number of shots per instance to five, only re-running the training when an obviously sub-optimal solution was detected. Finally, we set the learning rate to ? = 10 -4 and allow the model to train for up to ? 10 5 epochs, with a simple early stopping rule set to an absolute tolerance of 10 -4 and a patience of 10 3 . Maximum Cut.</p><p>MaxCut is an NP-hard combinatorial optimization problem with practical applications in machine scheduling <ref type="bibr" target="#b69">[70]</ref>, image recognition <ref type="bibr" target="#b70">[71]</ref>, and electronic circuit layout design <ref type="bibr" target="#b71">[72]</ref>. In the current era of noisy intermediate-scale quantum devices, with the advent of novel hybrid quantum-classical algorithms such as the Quantum Approximate Optimization Algorithm (QAOA) <ref type="bibr" target="#b72">[73]</ref>, the MaxCut problem has recently attracted considerable attention as a potential use case of pre-error-corrected quantum devices, see Refs. <ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref>. MaxCut is a graph partitioning problem defined as follows: given a graph with vertex set V and edge set E, we seek a partition of V into two subsets with maximum cut, where a cut refers to edges connecting two nodes from different vertex sets. Intuitively, that means we score a point whenever an edge connects two nodes of different colors. To formulate MaxCut mathematically, we introduce binary variables satisfying x i = 1 if vertex i is in one set and x i = 0 if it is in the other set. It is then easy to verify that the quantity x i + x j -2x i x j = 1 if the edge (i, j) has been cut, and 0 otherwise. With the help of the adjacency matrix A ij with A ij = 0 if edge (i, j) does not exist and A ij &gt; 0 if a (possibly weighted) edge connects node i with j, the MaxCut problem is described by the following quadratic Hamiltonian</p><formula xml:id="formula_27">H MaxCut = i&lt;j A ij (2x i x j -x i -x j )<label>(7)</label></formula><p>that falls into the broader class of QUBO problems described by Eq. (1); we provide the explicit Qmatrix for a sample MaxCut problem in Fig. <ref type="figure" target="#fig_3">2</ref>.</p><p>Up to an irrelevant constant, the MaxCut problem can equivalently by described by the compact Ising Hamiltonian H MaxCut = i&lt;j J ij z i z j with J ij = A ij /2, favoring antiferromagnetic ordering of the spins for J ij &gt; 0, as expected intuitively based on the problem definition.</p><p>As our figure of merit, we denote the largest cut found as cut = -H MaxCut (x ), with x referring to the corresponding bit string.</p><p>The complexity of MaxCut depends on the regularity and connectivity of the underlying graph. Following an existing trend in the community <ref type="bibr" target="#b74">[75]</ref>, we first consider the MaxCut problem on random (unweighted) d-regular graphs, where every vertex is connected to exactly d other vertices. We perform the benchmarks as follows. For graphs with up to a few hundred nodes, we compare our GNN-based solver to the (approximate) polynomialtime Goeman-Williamson (GW) algorithm <ref type="bibr" target="#b77">[78]</ref>, which provides the current record for an approximate answer within some fixed multiplicative factor of the optimum (referred to as approximation ratio ?), using semidefinite programming and randomized rounding. Specifically, the GW algorithm achieves a guaranteed approximation ratio of ? ? 0.878 for generic graphs. This lower bound can be raised for specific graphs such as unweighted 3-regular graphs where ? ? 0.9326 <ref type="bibr" target="#b78">[79]</ref>.</p><p>Our implementation of the GW algorithm is based on the open-source CVXOPT solver, with CVXPY as modeling interface. For very large graphs with up to a million nodes, numerical benchmarks are not available, but we can compare our best solution cut to an analytical result derived in Ref. <ref type="bibr" target="#b79">[80]</ref>, where it was shown that with high probability (in the limit n ? ?) the size of the maximum cut for random d-regular graphs with n nodes is given by cut = (d/4 + P</p><formula xml:id="formula_28">* d/4 + O( ? d))n + O(n).</formula><p>Here, P * ? 0.7632 refers to an universal constant related to the ground-state energy of the Sherrington-Kirkpatrick model <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref> that can be expressed analytically via Parisi's formula <ref type="bibr" target="#b79">[80]</ref>. We thus take cut ub = (d/4 + P * d/4)n as an upper-bound estimate for the maximum cut size in the large-n limit. We complement this upper bound with a lower bound as achieved by a simple, randomized 0.5-approximation algorithm that (on average) cuts half of the edges, yielding a cut size of cut rnd ? (d/4)n for a d-regular graph with |E| = (d/2)n. Our results for the achieved cut size as a function of the number of vertices n are shown in Fig. <ref type="figure" target="#fig_8">4</ref>. All results are bootstrapped estimates of the mean, with error bars denoting twice the bootstrapped standard deviations, sampled across 20 random d-regular graphs for every data point. For graphs with up to a few hundred nodes, we find that a simple two-layer GCN architecture can perform on par with the GW algorithm, while showing a runtime advantage compared to GW starting at around n ? 100 nodes. For large graphs with n ? 10 4 to 10 6 nodes, we find that our approach consistently achieves high-quality solutions with cut 0.9 ? cut ub for both d = 3 and d = 5, respectively (i.e., much better than any naive randomized algorithm). As expected for dregular graphs, we find cut to scale linearly with the number of nodes n, i.e., cut ? ? d n, with ? 3 ? 1.28 and ? 5 ? 1.93 for d = 3 and d = 5, respectively. Moreover, utilizing modern GPU hardware, we observe a favorable runtime scaling at intermediate and large system sizes that allows us to solve instances with n = 10 6 nodes in approximately 10 minutes (which includes both GNN model training and post-processing steps). Specifically, as shown in Fig. <ref type="figure" target="#fig_8">4</ref>, we observe an approximately linear scaling of total runtime with ? n, for large d-regular graphs with 10 5 ? n ? 10 6 ; contrasted with the observed GW algorithm scaling as ? n 3.5 for problem sizes in the range n 250, thereby showing the (expected) time complexity ?(n 3.5 ) of the interior-point method (as commonly used for solving the semidefinite program underlying the GW algorithm) that dominates the GW algorithm runtime <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b83">84]</ref>.</p><p>To complement our work on random d-regular graphs, we have performed additional experiments on standard Max-Cut benchmark instances, with published results, based on the publicly available Gset data set <ref type="bibr" target="#b88">[89]</ref> commonly used for testing Max-Cut algorithms. We the Goeman-Williamson (GW) algorithm. On each graph instance, the GNN solver is allowed up to five shots, and the GW algorithm takes 100 shots. Solid lines for n ? 10 3 represent theoretical upper bounds, as described in the main text. Inset: The estimated relative approximation ratio defined as cut /cut ub shows that our approach consistently achieves high-quality solutions. Right panel: Algorithm runtime in seconds for both the GNN solver and the GW algorithm. Error bars refer to twice the bootstrapped standard deviations, sampled across 20 random graph instances for every data point.  provide benchmark results for seven different graphs, with thousands of nodes, including (i) two Erd?s-Renyi graphs with uniform edge probability, (ii) two graphs where the connectivity gradually decays from node 1 to n, (iii) two 4-regular toroidal graphs, and (iv) one of the largest Gset instances with n = 10 4 . The results are displayed in Tab. I. Here, we report cut sizes achieved with our physics-inspired GNN solver (PI-GNN), together with results sourced from Refs. <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref>; the latter include an SDP solver using dual scaling (DSDP) <ref type="bibr" target="#b86">[87]</ref>, a combination of local search and adaptive perturbation referred to as Breakout Local Search (BLS) <ref type="bibr" target="#b85">[86]</ref> (providing the best known solutions for the Gset data set), a Tabu Search metaheuristic (KHLWG) <ref type="bibr" target="#b84">[85]</ref>, and a recurrent GNN architecture for maximum constraint satisfaction problems (RUN-CSP) <ref type="bibr" target="#b59">[60]</ref>. We assess the solution quality achieved with PI-GNN with the relative error = (cut bestcut )/|E| quantifying the gap to the best known solution, normalized by the number of edges |E|, thereby giving the fraction of uncut edges as compared to the best known solution. We find that our general-purpose approach is competitive with other solvers and typically within ? 1% of the best published results.</p><p>Maximum Independent Set. The MIS problem is a prominent combinatorial optimization problem with practical applications in network design <ref type="bibr" target="#b90">[91]</ref> and finance <ref type="bibr" target="#b91">[92]</ref>, and is closely related to the maximum clique, minimum vertex cover, and set packing problems. In the quantum community, the MIS problem has recently attracted significant interest <ref type="bibr" target="#b92">[93]</ref> as a potential target use case for novel experimental platforms based on neutral atom arrays <ref type="bibr" target="#b93">[94]</ref>. The MIS problem reads as follows. Given an undirected graph G = (V, E), an independent set is a subset of vertices that are not connected with each other. The MIS problem is then the task to find the largest independent set, with its (maximum) cardinality typically denoted as the independence number ?. To formulate the MIS problem mathematically, for a given graph G = (V, E), one first  <ref type="bibr" target="#b89">[90]</ref>. Solid lines for n ? 10 3 refer to theoretical upper bounds as described in the main text. Inset: The estimated relative approximation ratio comparing the achieved independence number ? against known theoretical upper bounds shows that our approach consistently achieves high-quality solutions. Right panel: Algorithm runtime in seconds for both the GNN solver and the Boppana-Halldorsson algorithm. Error bars refer to twice the bootstrapped standard deviations, sampled across 20 random graph instances for every data point. associates a binary variable x i ? {0, 1} with every vertex i ? V , with x i = 1 if vertex i belongs to the independent set, and x i = 0 otherwise. The MIS can then be formulated in terms of a Hamiltonian that counts the number of marked (colored) vertices and adds a penalty to nonindependent configurations (when two vertices in this set are connected by an edge). It is given by</p><formula xml:id="formula_29">H MIS = - i?V x i + P (i,j)?E x i x j ,<label>(8)</label></formula><p>with a negative pre-factor to the first term (because we solve for the largest independent set within a minimization problem), and the penalty parameter P &gt; 0 enforcing the constraints. Note that the numerical value for P is typically set as P = 2 <ref type="bibr" target="#b94">[95]</ref>, but can be further optimized in an outer loop. Energetically, the Hamiltonian H MIS favors each variable to be in the state x i = 1 unless a pair of these are connected by an edge. Again, the Hamiltonian H MIS is quadratic and falls into the broader class of QUBO problems described by Eq. ( <ref type="formula" target="#formula_4">1</ref>); again we provide the explicit Q-matrix for a sample MIS problem in Fig. <ref type="figure" target="#fig_3">2</ref>.</p><p>The MIS problem is known to be strongly NPhard, making the existence of an efficient algorithm for finding the maximum independent set on generic graphs unlikely. In addition, the MIS problem is even hard to approximate. In general, the MIS problem cannot be approximated to a constant factor in polynomial time (unless P = NP). Again we study the MIS problem on random unweighted d-regular graphs. Because in our approach the independence constraint is enforced with soft penalty terms ? P -just like in any QUBO-based model-the predicted set may violate the independence condition (i.e., the set may contain nodes connected by an edge). Setting P = 2, we have observed these violations only in very few cases. If present, as part of our postprocessing, we have enforced the independence constraint by greedily removing one of the nodes of each induced edge from the set, and only reporting results after this correction. For small graphs with up to a few hundred nodes, we compare the GNN-based results to results obtained with the Boppana-Halldorsson algorithm built into the Python NetworkX library <ref type="bibr" target="#b89">[90]</ref>. For very large graphs with up to a million nodes (where benchmarks are not available) we resort to analytical upper bounds for random d-regular graphs as presented in Ref. <ref type="bibr" target="#b95">[96]</ref>. Here, the best known bounds on the ratio ? d /n are reported as ? 3 /n = 0.45537 and ? 5 /n = 0.38443 for d = 3 and d = 5, respectively, as derived using refined versions of Markov's inequality <ref type="bibr" target="#b96">[97]</ref>. Our results for the achieved independence number as a function of the number of vertices n are shown in Fig. <ref type="figure" target="#fig_16">5</ref>. All results are bootstrapped estimates of the mean, with error bars denoting twice the bootstrapped standard deviations, sampled across 20 random d-regular graphs for every data point. Our numerical results for MIS are similar to the observations we have made for MaxCut: for graphs with up to a few hundred nodes, we find that a simple two-layer GCN architecture can perform on par with (or better than) the traditional solver, with the GNN solver showing a favorable runtime scaling. For large graphs with n ? 10 4 to 10 6 nodes we find that our approach consistently achieves high-quality solutions with ? 3 /n ? 0.416 and ? 5 /n ? 0.338 for d = 3 and d = 5, respectively, resulting in estimated numerical approximation ratios of 0.416/0.45537 ? 0.92 and 0.338/0.38443 ? 0.88, respectively. Finally, as shown in Fig. <ref type="figure" target="#fig_16">5</ref>, we observe a moderate, super-linear scaling of the total runtime as ? n 1.7 for large d-regular graphs with n 10 5 , as opposed to the Boppana-Halldorsson solver with a runtime scaling of ? n 2.9 in the range n 500. Note that the GNN model training alone displays sub-linear runtime scaling as ? n 0.8 , in line with our MaxCut results, while the aggregate runtime (including post-processing to enforce the independence condition) scales as ? n 1.7 in the regime n ? 10 5 -10 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND OUTLOOK</head><p>In summary, we have proposed and analyzed a versatile and scalable general-purpose solver that is powered by graph neural networks and draws from concepts in statistical physics.</p><p>Our approach is applicable to any k-local Ising model, including canonical NPhard combinatorial optimization problems such as the maximum cut, maximum clique, minimum vertex cover or maximum independent set problems, among others <ref type="bibr" target="#b64">[65]</ref>. Starting from a problem formulation in Ising form, we apply a relaxation strategy to the problem Hamiltonian by dropping integrality constraints on the decision variables in order to generate a differentiable loss function with which we perform unsupervised training on the node representations of the GNN. The GNN is then trained to generate soft assignments to predict the likelihood of belonging in one of two classes, for each vertex in the graph. To find a binary (twocolor) labelling consistent with the original problem formulation, simple projection heuristics are applied. Overall, we find that this approach can compete with existing special-purpose solvers, such as the Goemans-Williamson algorithm designed to solve the maximum cut problem, with the potential to tap into the rich toolbox of statistical physics, including, for example, the study of phase transitions. In the current noisy intermediate scale quantum era, our approach could be used as a broadly applicable, scalable benchmark for emerging quantum technologies, including special-purpose quantum <ref type="bibr" target="#b5">[6]</ref> and quantum-inspired annealers <ref type="bibr" target="#b18">[19]</ref>, while not being resource constrained nor being limited to problem instances in QUBO form, as is also the case for coherent Ising machines <ref type="bibr" target="#b97">[98]</ref>.</p><p>Finally, we highlight possible extensions of research going beyond our present work.</p><p>First, to better understand the limitations of GNNs in the context of combinatorial optimization, further studies are in order, systematically benchmarking GNNs against state-of-theart solvers for a large class of optimization problems while leveraging the entire zoo of GNN implementations including, for example, GraphSAGE <ref type="bibr" target="#b25">[26]</ref> or Graph Attention Networks (GATs) <ref type="bibr" target="#b40">[41]</ref> to potentially boost the GNN Ansatz with an attention mechanism enabling vertices to weigh neighbor representations during the aggregation steps. Second, the presented GNN approach should be able to accommodate problems sizes with hundreds of millions of nodes when leveraging distributed training in a mini-batch fashion on a cluster of machines <ref type="bibr" target="#b43">[44]</ref>, thereby challenging the capabilities of several existing solvers. While we have solved individual problem instances from scratch, using a random initialization process for the initial node embeddings, in the future warm-starting the training process with pre-trained weights (transfer learning) could boost the time to solution.</p><p>Moreover, one could potentially boost the performance of our optimizer by implementing randomized projection schemes (as opposed to the simple deterministic approach used here), or augment these strategies with simple greedy post-processing routines that check for local optimality with a sequence of local bit flips. Finally, as discussed in the main text, our approach can be generalized to PUBO problems on hyper-graphs where so-called hyper-edges may contain more than just two nodes, with no need for (typically) resource-intensive degree reduction schemes, as opposed to resourceconstrained QUBO solvers. Potential applications cover many real-world optimization problems involving multibody interactions, as found in scheduling problems <ref type="bibr" target="#b98">[99]</ref> or chemistry <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b100">101]</ref>.</p><p>In conclusion, the proposed cross-fertilization between machine learning, operations research and physics opens up a number of interesting research directions, with the ultimate goal to further advance our ability to solve hard combinatorial optimization problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&lt; l a t</head><label></label><figDesc>e x i t s h a 1 _ b a s e 6 4 = " S U z a G Y 2 q S T 2 B M 8 S r I 8 7 / h c 0 TM N I = " &gt; A A A B 6 3 i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h d 0 g 6 j G g B 4 8 R z A O S J c x O Z p M h M 7 P L P I S w 5 B e 8 e F D E q z / k z b 9 x N t m D J h Y 0 F F X d d H d F K W f a + P 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b O r G K 0 B Z J e K K 6 E d a U M 0 l b h h l O u 6 m i W E S c d q L Jb e 5 3 n q j S L J G P Z p r S U O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 ?FIG. 1 :</head><label>11</label><figDesc>FIG.1: Schematic illustration of the graph neural network approach for combinatorial optimization presented in this work.Following a recursive neighborhood aggregation scheme, the graph neural network is iteratively trained against a custom loss function that encodes the specific optimization problem (e.g., Maximum Cut). At training completion, we project the final values for the soft node assignments at the final graph neural network layer back to binary variables xi = 0, 1, providing the solution bit string x = (x1, x2, . . . ). Further details are given in the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 6 5 r Y t A E v Z 8 2 3 O 8 1 D j 8 l o z e i P K K Q = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 &lt;</head><label>2</label><figDesc>7 w 5 j 8 6 L 8 + 5 8 L F s 3 n H z m D P 7 A + f w B e j W M r w = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 6 r 6 I H o M L P U 5 o D l Y O O L 8 B E Q 4 b 2 s = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H a R q E c S L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 &lt;</head><label>3</label><figDesc>X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A H u 5 j L A = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " R e j r j R u 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>7 w 5 j 8 6 L 8 + 5 8 L F s 3 n H z m D P 7 A + f w B e j W M r w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " 6 5 r Y t A E v Z 8 2 3 O 8 1 D j 8 l o z e i P K K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>+ l q 0 b T j 5 z B n / g f P 4 A e L G M r g = = &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R p k y d w Y u L 2 K K g B B f p Q 5 m q v p I Z q U = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>U b g r f 6 8 j p p V y v e d a X a r J X r t T y O A p z D B V y B B z d Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s 3 n H z m D P 7 A + f w B e j W M r w = = &lt; / l a t e x i t &gt; 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 6 r 6 I H o M L P U 5 o D l Y O O L 8 B E Q 4 b 2 s = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H a R q E c S L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S 4 6 p f L L l l d w G y T r y M l C B D v V / 8 6 g 1 i l k Y o D R N U 6 6 7 n J s a f U m U 4 E z g r 9 F K N C W V j O s S u p Z J G q P 3 p 4 t A Z u b D K g I S x s i U N W a i / J 6 Y 0 0 n o S B b Y z o m a k V 7 2 5 + J / X T U 1 4 6 0 + 5 T F K D k i 0 X h a k g J i b z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u C D c F b f X m d t C p l 7 7 p c a V R L t W o W R x 7 O 4 B w u w Y M b q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A H u 5 j L A = &lt; / l a t e x i t &gt; 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R e j r j R u 7 G Q 6 c S R X p M Z 6 z p l x O U w k = " &gt; A A A B 5 H i c b V B N S 8 N A E J 3 U r x q / q l c v i 0 X w V J J S 1 G P B i 8 c K 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h S v / i Z v / h u 3 b Q 7 a + m D g 8 d 4 M M / P C V H B t P O / b K W 1 t 7 + z u l f f d g 8 O j 4 5 O K e 9 r R S a Y Y t l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 &lt;</head><label>4</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " 6 5 r Y t A E v Z 8 2 3 O 8 1 D j 8 l o z e i P K K Q = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>+ l q 0 b T j 5 z B n / g f P 4 A d y 2 M r Q = = &lt; / l a t e x i t &gt; 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c 7 W 4 e j + F 5 g o B q K a n C / 5 D S a g + W i 8 = " &gt; A A A B 6 n i c d V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 G i b t t L Y L o e D G Z U X 7 g H Y o m T T T h m Y y Q 5 I R y t B P c O N C E b d + k T v / x k x b Q U U P X D i c c y / 3 3 u P H n C n t O B 9 W b m 1 9 Y 3 M r v 1 3 Y 2 d 3 b P y g e H n V U l E h C 2 y T i k e z 5 W F H O B G 1 r p j n t x Z L i 0 O e 0 6 0 + v M r 9 7 T 6 V i k b j T s 5 h 6 I R 4 L F j C C t Z F u p 5 d o W C w 5 t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>a g I A x e A B P 4 N n i 1 q P 1 Y r 0 u W 3 P W a u Y Y / I D 1 9 g k 4 2 I 2 8 &lt; / l a t e x i t &gt; k = 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i1 i E Z Y 7 G o v i Y M 9 C e Z h M W 9 s o Q i G U = " &gt; A A A B 6 n i c d V D L S g N B E O y N r x h f U Y 9 e B o P g a d l N N j E 5 C A E v H i O a B y R L m J 3 M J k N m H 8 z M C m H J J 3 j x o I h X v 8 i b f + N s E k F F C x q K q m 66 u 7 y Y M 6 k s 6 8 P I r a 1 v b G 7 l t w s 7 u 3 v 7 B 8 X D o 4 6 M E k F o m 0 Q 8 E j 0 P S 8 p Z S N u K K U 5 7 s a A 4 8 D j t e t O r z O / e U y F Z F N 6 p W U z d A I 9 D 5 j O C l Z Z u p 5 f l Y b F k m U 6 9 U r N t l J F y w 2 5 o U r W c S r W B b N N a o A Q r t I b F 9 8 E o I k l A Q 0 U 4 l r J v W 7 F y U y w U I 5 z O C 4 N E 0 h i T K R 7 T v q Y h D q h 0 0 8 W p c 3 S m l R H y I 6 E r V G i h f p 9 I c S D l L P B 0 Z 4 D V R P 7 2 M v E v r 5 8 o v + 6 m L I w T R U O y X O Q n H K k I Z X + j E R O U K D 7 T B B P B 9 K 2 I T L D A R O l 0 C j q E r 0 / R / 6 R T N u 2 a W b 5 x S k 1 n F U c e T u A U z s G G C 2 j C N b S g D Q T G 8 A B P 8 G x w 4 9 F 4 M V 6 X r T l j N X M M P 2 C 8 f Q I 6 X I 2 9 &lt; / l a t e x i t &gt; k = 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 &lt; 2</head><label>12</label><figDesc>+ l q 0 b T j 5 z B n / g f P 4 A e L G M r g = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " R p k y d w Y u L 2 K K g B B f p Q 5 m q v p I Z q U = " &gt;A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k p 6 r H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U r A 5 K Z b f i L k D W i Z e T M u R o D E p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N i P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I p V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N kU b g r f 6 8 j p p V y v e d a X a r J X r t T y O A p z D B V y B B z d Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s 3 n H z m D P 7 A + f w B e j W M r w = = &lt; / l a t e x i t &gt; Final evaluation &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 5 B / K i N 6 z l 0 h 4 Z J r y o O T e 7 V c W c w = " &gt; A A A C O X i c b V D L S g M x F M 3 4 r P V V d e k m W A Q F K T M i 6 k Y o u n E h 2 I J T h U 4 Z M m l q 0 y Y z Q 3 J H K M P 8 l h v / w p 3 g x o U i b v 0 B 0 2 k V X x d C T s 6 5 l 5 t z g l h w D b b 9 Y E 1 M T k 3 P z B b m i v M L i 0 v L p Z X V h o 4 S R Z l L I x G p q 4 B o J n j I X O A g 2 F W s G J G B Y J d B / 2 S o X 9 4 w p X k U X s A g Z i 1 J r k P e 4 Z S A o f x S z Z M E u p S I 9 C z z c 6 x k W n e P z z O 8 5 U G X A d n G R 9 j T i f R T v t P L c G z u 7 F O q m 0 c v M 1 T v i / J L Z b t i 5 4 X / A m c M y m h c N b 9 0 7 7 U j m k g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>7 K I J w f l t + S 9 o 7 F a c / c p u f a 9 c P R j H U U D r a A N t I Q c d o C o 6 R T X k I o p u 0 S N 6 R i / W n f V k v V p v o 9 Y J a z y z h n 6 U 9 f 4 B A u u t n Q = = &lt; / l a t e x i t &gt; L QUBO (?) = X i,j p i (?)Q ij p j (?) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t / r G m E R w s D X f I F F y f z / 5 z r D J f l E = " &gt; A A A C O n i c b V D L S g M x F M 3 4 t r 6 q L t 0 E i + B C y o y I d S M U 3 b i z A 7 Y K b R 0 y a a a m T T J D k p G W I d / l x q 9 w 5 8 K N C 0 X c + g F m a s F H v Z D c k 3 P u J f e e M G F U a d d 9 d K a m Z 2 b n 5 h c W C 0 v L K 6 t r x f W N h o p T i U k d x y y W V y F S h F F B 6 p p q R q 4 S S R A P G b k M + 6 e 5 f n l L p K K x u N D D h L Q 5 6 g o a U Y y 0 p Y K i f x a 0 O N I 3 k m d + / e T c w G M 4 e o d R N j D X W Y s K T S R G z P j f d F 6 j U h 5 k d K 9 n 4 M B m 4 9 u r Z y z s m a B Y c s v u K O A k 8 M a g B M Z R C 4 o P r U 6 M U 0 6 E x g w p 1 f T c R L c z J D X F j J h C K 1 U k Q b i P u q R p o U C c q H Y 2 W t 3 A H c t 0 Y B R L e 4 S G I / Z n R 4 a 4 U k M e 2 s p 8 f v V X y 8 n / t G a q o 6 N 2 R k W S a i L w 1 0 d R y q C O Y e 4 j 7 F B J s G Z D C x C W 1 M 4 K 8 Q 2 S C F u / V M G a 4 P 1 d e R I 0 9 s v e Y X n f P y h V K 2 M 7 F s A W 2 A a 7 w A M V U A V n o A b q A I M 7 8 A R e w K t z 7 z w 7 b 8 7 7 V + m U M + 7 Z B L / C + f g E C + W v a g = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>3 &lt;</head><label>3</label><figDesc>g 1 M 4 g w t w o Q I 1 u I U 6 N I E B w i M 8 w 4 t 1 b z 1 Z r 9 b b s n X N y m Z O 4 A e s 9 y 8 C Z o 0 O &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = "Y n G J E d m w / K 3 G v 3 f u b 1 E T 6 B 7 0 r z Y = " &gt; A A A B 6 H i c d V D L S g N B E O y N r x h f U Y 9 e B o P g a d k N w Z h b w I v H B M w D k i X M T n q T M b M P Z m a F E P I F X j w o 4 t V P 8 u b f O E l W i K I F D U V V N 9 1 d f i K 4 0 o 7 z a e U 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J W 8 W p Z N h i s Y h l 1 6 c K B Y + w p b k W 2 E 0 k 0 t A X 2 P E n N w u / 8 4 B S 8 T i 6 0 9 M E v Z C O I h 5 w R r W R m s 6 g W H J t Z w n i 2 D W D a i U j N Z d 8 W y X I 0 B g U P / r D m K U h R p o J q l T P d R L t z a j U n A m c F / q p w o S y C R 1 h z 9 C I h q i 8 2 f L Q O b k w y p A E s T Q V a b J U 1 y d m N F R q G v q m M 6 R 6 r H 5 7 C / E v r 5 f q 4 N q b 8 S h J N U Z s t S h I B d E x W X x N h l w i 0 2 J q C G W S m 1 s J G 1 N J mT b Z F N Z D + J + 0 y 7 Z 7 Z Z e b l V K 9 k s W R h z M 4 h 0 t w o Q p 1 u I U G t I A B w i M 8 w 4 t 1 b z 1 Z r 9 b b q j V n Z T O n 8 A P W + x f 9 y 4 0 L &lt; / l a t e x i t &gt; 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>4 &lt; 3 &lt; 4 &lt;FIG. 2 :</head><label>4342</label><figDesc>FIG.2: Flow chart illustrating the end-to-end workflow for the proposed physics-inspired GNN optimizer. (a), The problem is specified by a graph G with associated adjacency matrix A, and a cost function as described (for example) by the QUBO Hamiltonian HQUBO. Within the QUBO framework the cost function is fully captured by the QUBO matrix Q, as illustrated for both MaxCut and MIS for a sample (undirected) graph with five vertices and six edges. (b), The problem setup is complemented by a training strategy that specifies the GNN Ansatz, a choice of hyperparameters and a specific ML optimizer. (c), The GNN is iteratively trained against a custom loss function LQUBO(?) that encodes a relaxed version of the underlying optimization problem as specified by the cost function HQUBO. Typically, a GNN layer operates by aggregating information within the local one-hop neighbourhood (as illustrated by the k = 1 circle for the top node with label 0). By stacking layers one can extend the receptive field of each node, thereby allowing distant propagation of information (as illustrated by the k = 2 circle for the top node with label 0). (d)-(e), The GNN generates soft node assignments which can be viewed as class probabilities. Using some projection scheme, we then project the soft node assignments back to (hard) binary variables xi = 0, 1, providing the final solution bit string x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>FIG. 4 :</head><label>4</label><figDesc>FIG.4: Numerical results for MaxCut. Left panel: Average cut size for d-regular graphs with d = 3 and d = 5 as a function of the number of vertices n, bootstrap-averaged over 20 random graph instances, for both the GNN-based method and the Goeman-Williamson (GW) algorithm. On each graph instance, the GNN solver is allowed up to five shots, and the GW algorithm takes 100 shots. Solid lines for n ? 10 3 represent theoretical upper bounds, as described in the main text. Inset: The estimated relative approximation ratio defined as cut /cut ub shows that our approach consistently achieves high-quality solutions. Right panel: Algorithm runtime in seconds for both the GNN solver and the GW algorithm. Error bars refer to twice the bootstrapped standard deviations, sampled across 20 random graph instances for every data point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>FIG. 5 :</head><label>5</label><figDesc>FIG.5: Numerical results for the MIS problem. Left panel: Average independence number ? for d-regular graphs with d = 3 and d = 5 as a function of the number of vertices n, (bootstrap-)averaged over 20 random graph instances, for both the GNN-based method and a traditional MIS algorithm<ref type="bibr" target="#b89">[90]</ref>. Solid lines for n ? 10 3 refer to theoretical upper bounds as described in the main text. Inset: The estimated relative approximation ratio comparing the achieved independence number ? against known theoretical upper bounds shows that our approach consistently achieves high-quality solutions. Right panel: Algorithm runtime in seconds for both the GNN solver and the Boppana-Halldorsson algorithm. Error bars refer to twice the bootstrapped standard deviations, sampled across 20 random graph instances for every data point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Numerical results for MaxCut on Gset instances. We report cut sizes achieved with our physics-inspired GNN solver (PI-GNN), together with results sourced from Refs.<ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref>. Best known results are marked in bold. The last column specifies the relative error comparing PI-GNN to the best known cut size. Further details are provided in the main text. GNN model configurations are detailed in<ref type="bibr" target="#b87">[88]</ref>.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Fernando Brandao</rs>, <rs type="person">George Karypis</rs>, <rs type="person">Michael Kastoryano</rs>, <rs type="person">Eric Kessler</rs>, <rs type="person">Tyler Mullenbach</rs>, <rs type="person">Nicola Pancotti</rs>, <rs type="person">Mauricio Resende</rs>, <rs type="person">Shantu Roy</rs>, <rs type="person">Grant Salton</rs>, <rs type="person">Simone Severini</rs> and <rs type="person">Jason Zhu</rs> for fruitful discussions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supplemental Material for: Combinatorial Optimization with Physics-Inspired Graph Neural Networks Martin J. A. Schuetz, 1,2,3 J. Kyle Brubaker, 2 and Helmut G. Katzgraber 1,2,3</p><p>1 Amazon Quantum Solutions Lab, Seattle, Washington 98170, USA 2 AWS Intelligent and Advanced Compute Technologies, Professional Services, Seattle, Washington 98170, USA 3 AWS Center for Quantum Computing, Pasadena, CA 91125, USA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. HYPERPARAMETERS FOR G-SET EXPERIMENTS</head><p>In this section, we provide details for the specific model configurations (hyperparameters) as used to solve the Gset instances with our physics-inspired GNN solver (PI-GNN). The results achieved with these model configurations are displayed in Tab. I; the corresponding hyperparameters are given in Tab. II. Our base GCN architecture with tunable number of layers K is specified in Listing 2.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kochenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quantum Bridge Analytics I: A Tutorial on Formulating and Using QUBO Models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Unconstrained Binary Quadratic Programming Problem: A Survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kochenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Optimization</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quadratic reformulations of nonlinear binary optimization problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Crama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Steiglitz</surname></persName>
		</author>
		<title level="m">Combinatorial Optimization: Algorithms and Complexity</title>
		<meeting><address><addrLine>North Chelmsford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Courier Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Korte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vygen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorial Optimization</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quantum annealing with manufactured spins</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gildert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lanting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hamze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Berkley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bunyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">473</biblScope>
			<biblScope unit="page">194</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Architectural Considerations in the Design of a Superconducting Quantum Annealing Processor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bunyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoskinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tolkacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altomare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Berkley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lanting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whittaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Appl. Supercond</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Viewing vanilla quantum annealing through spin glasses</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Katzgraber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum Science and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">30505</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perspectives of quantum annealing: methods and implementations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hauke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Katzgraber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nishimori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rep. Prog. Phys</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">54401</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quantum annealing in the transverse Ising model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kadowaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nishimori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">5355</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A quantum adiabatic evolution algorithm applied to random instances of an NP-complete problem</title>
		<author>
			<persName><forename type="first">E</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lapan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lundgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Preda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page">472</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Strengths and weaknesses of weakstrong cluster problems: A detailed overview of state-ofthe-art classical heuristics versus quantum approaches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mandr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perdomo-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Katzgraber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">22337</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A deceptive step towards quantum speedup detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mandr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Katzgraber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum Sci. Technol</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>3, 04LT01</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimization of population annealing Monte Carlo for large-scale spin-glass simulations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barzegar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pattison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Katzgraber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">53308</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coherent Ising machine based on degenerate optical parametric oscillators</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Byer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">63853</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaling advantages of all-to-all connectivity in physical annealers: The Coherent Ising Machine vs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inagaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Venturelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onodera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Langrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Honjo</surname></persName>
		</author>
		<idno>arXiv:quant- phys/1805.05217</idno>
	</analytic>
	<monogr>
		<title level="j">D-Wave</title>
		<imprint>
			<date type="published" when="2000">2000. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perspective: Memcomputing: Leveraging memory and physics to compute efficiently</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Ventra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Traversa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Phys</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">180901</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memcomputing NP-complete problems in polynomial time using polynomial resources and collective states</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Traversa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ramella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bonani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Di</forename><surname>Ventra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Adv</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vatankhahghadim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsukamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takemoto</surname></persName>
		</author>
		<title level="m">Complex, Intelligent, and Software Intensive Systems -Proceedings of the 11th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS-2017)</title>
		<meeting><address><addrLine>Torino, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">July 10-12, 2017. 2017</date>
			<biblScope unit="page">432</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Accelerator Architecture for Combinatorial Optimization Problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tsukamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FUJITSU Sci. Tech. J</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Physics-inspired optimization for constraint-satisfaction problems using a digital annealer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aramon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Katzgraber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Phys</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<title level="m">2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting><address><addrLine>Palm Springs CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">729</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural network for graphs: A contextual constructive approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">498</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">2224</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">1024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weihua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
	</analytic>
	<monogr>
		<title level="j">A Comprehensive Survey on Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">701</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01973</idno>
		<title level="m">Graph Convolutional Neural Networks for Web-Scale Recommender Systems (2018)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast and flexible protein design using deep graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strokach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Becerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Corbi-Verge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perez-Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">402</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Utilising Graph Machine Learning within Drug Discovery and Development</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gaudelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Day</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05716</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest (Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">2311</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Temporal graph networks for deep learning on dynamic graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10637</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fake news detection on social media using geometric deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06673</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Choma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gerhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palczewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ronaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bhimji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMLA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph neural networks in particle physics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Vlimant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">21001</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning-Volume</title>
		<meeting>the 34th International Conference on Machine Learning-Volume</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">1263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<title level="m">International Conference on Machine Learning (ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">5453</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05337</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kotary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fioretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Hentenryck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16378</idno>
		<title level="m">End-to-End Constrained Optimization Learning: A Survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Cappart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chetelat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09544</idno>
		<title level="m">Combinatorial optimization and reasoning with graph neural networks (2021)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Finding the ground state of spin Hamiltonians with reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ronagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tamblyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">509</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">2692</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Revised Note on Learning Algorithms for Quadratic Assignment with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07450</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05689</idno>
		<title level="m">Graph edit distance computation via graph neural networks (2018)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Graph Colouring Meets Deep Learning: Effective Graph Neural Network Models for Combinatorial Problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lemos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Avelar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04598</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10659</idno>
	</analytic>
	<monogr>
		<title level="j">Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">An efficient graph convolutional network technique for the travelling salesman problem</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01227</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Erdoes Goes Neural: an Unsupervised Learning Framework for Combinatorial Optimization on Graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Karalias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10643</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">It&apos;s not what machines can learn, it&apos;s what we cannot teach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yehuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schuster</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09398</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09940</idno>
		<title level="m">Neural combinatorial optimization with reinforcement learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Attention, learn to solve routing problems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08475</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Combinatorial optimization by graph pointer networks and hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04936</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01665</idno>
		<title level="m">Learning combinatorial optimization algorithms over graphs (2018)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Run-csp: Unsupervised learning of message passing networks for binary constraint satisfaction problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Toenshoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08387</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Experimental performance of graph neural networks on random instances of max-cut</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05767</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Beitrag zur Theorie des Ferromagnetismus</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ising</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Z. Phys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">253</biblScope>
			<date type="published" when="1925">1925</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirotaka</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motomu</forename><surname>Takatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behraz</forename><surname>Vatankhahghadim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hironobu</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanroku</forename><surname>Tsukamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhiro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Takemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sheikholeslami</surname></persName>
		</author>
		<title level="m">Ising-Model Optimizer with Parallel-Trial Bit-Sieve Engine</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">432</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Experimental investigation of performance differences between coherent Ising machines and a quantum annealer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inagaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Venturelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onodera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Langrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Honjo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Adv</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">823</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ising formulations of many NP problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Physics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">With coloring of the graph we refer to a specific node classification as given by the assignment vector x, taking for example xi = 0 as red node coloring and xi = 1 as blue node coloring. We do not refer to the well-known vertex coloring problem which seeks to color the vertices of a graph such that no two adjacent vertices are of the same color</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">0-1 quadratic programming approach for optimum solutions of two scheduling problems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alidaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kochenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmadian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Systems Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">401</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Image recognition with an adiabatic quantum computer I. Mapping to quadratic unconstrained binary optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0804.4457</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Applications of cut polyhedra</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laurent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">191</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Gutmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4028</idno>
		<title level="m">A quantum approximate optimization algorithm</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Quantum Approximate Optimization Algorithm: Performance, Mechanism, and Implementation on Near-Term Devices</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lukin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. X</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21067</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">QAOA for Max-Cut requires hundreds of qubits for quantum speed-up</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Guerreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Matsuura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">6903</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Performance of the Quantum Approximate Optimization Algorithm on the Maximum Cut Problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Crooks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08419</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Empirical performance bounds for quantum approximate optimization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Lotshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Humble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ostrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Siopsis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06813</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Goemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">1115</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">MAX CUT in Cubic Graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Livnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Zwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Extremal cuts of sparse random graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dembo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">1190</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Solvable model of a spin glass</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sherrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">1792</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Spin Glasses: Experimental Facts, Theoretical Concepts and Open Questions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">801</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Interior point methods in semidefinite programming with applications to combinatorial optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A coherent Ising machine for MAX-CUT problems: Performance evaluation against semidefinite programming and simulated annealing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Haribara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Utsunomiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Physics</title>
		<imprint>
			<biblScope unit="volume">911</biblScope>
			<biblScope unit="page">251</biblScope>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<pubPlace>Tokyo</pubPlace>
		</imprint>
	</monogr>
	<note>Principles and methods of quantum information technologies</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Solving large scale Max Cut problems via tabu search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kochenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Glover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Heuristics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">565</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Breakout Local Search for the Max-Cut problem</title>
		<author>
			<persName><forename type="first">U</forename><surname>Benlic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1162</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">52242</biblScope>
			<pubPlace>Iowa City, IA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Management Sciences, University of Iowa</orgName>
		</respStmt>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">See supplemental material for additional information</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The Gset Dataset</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<ptr target="https://web.stanford.edu/yyye/yyye/Gset/" />
	</analytic>
	<monogr>
		<title level="j">URL</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Approximating maximum independent sets by excluding subgraphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Boppana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Halldorsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">180</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Frequency assignment: Theory and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">1497</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Statistical analysis of financial networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Boginski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Butenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Pardalos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">431</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Quantum Algorithm for Approximating Maximum Independent Sets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wilczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13089</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Pichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lukin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10816</idno>
		<title level="m">Quantum Optimization for Maximum Independent Set Using Rydberg Atom Arrays (2018)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Djidjev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chapuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rizk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08653</idno>
		<title level="m">Efficient Combinatorial Optimization Using Quantum Annealing (2018)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Large independent sets in random regular graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="page">5236</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Independent sets in regular graphs of high girth, Ars Combinatoria 23A</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">179</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A fully programmable 100-spin coherent Ising machine with all-to-all connections</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L T</forename><surname>Mcmahon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">354</biblScope>
			<biblScope unit="page">614</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Inapproximability of hypergraph vertex cover and applications to scheduling problems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">A novel graph-based approach for determining molecular similarity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaribafiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aramon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naghibi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06693</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Akrobotu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Negre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mniszewski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00542</idno>
		<title level="m">Quantum isomer search</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
