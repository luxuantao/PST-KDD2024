<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-06">6 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
							<email>xiaojiang_liu@apple.com</email>
						</author>
						<author>
							<persName><forename type="first">Jianhao</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong 3 Apple</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huayang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
							<email>lijian83@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-06">6 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.02369v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (e.g., greedy search). This phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02% in Wikitext-103). To investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. Through our quantitative experiments, we find that 1) Language models have a preference to repeat the previous sentence;</p><p>2) The sentence-level repetitions have a self-reinforcement effect: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) The sentences with higher initial probabilities usually have a stronger self-reinforcement effect. Motivated by our findings, we propose a simple and effective training method DITTO (PseuDo-RepetITion PenalizaTiOn), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. Although our method is motivated by mitigating repetitions, experiments show that DITTO not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. Extensive experiments on open-ended text generation (Wikitext-103) and text summarization (CNN/DailyMail) demonstrate the generality and effectiveness of our method. * The work was conducted in Apple.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, large-scale Transformer-based <ref type="bibr" target="#b30">[31]</ref> neural language models (e.g., GPT2 <ref type="bibr" target="#b25">[26]</ref>, BART <ref type="bibr" target="#b14">[15]</ref> and OPT <ref type="bibr" target="#b34">[35]</ref>) have shown remarkable capability to generate text and achieved better performance than before, such as open-ended generation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4]</ref> and summarization tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>. However, models with standard maximization-based decoding are known to get stuck in redundant consecutive repetitions <ref type="bibr" target="#b10">[11]</ref>. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the model has a stronger preference for consecutive sentencelevel repetitions than the word-or phrase-level while human language has fewer consecutive sentencelevel repetitions, which shows a discrepancy between human language and the generated texts. Existing approaches to mitigate repetitions can be categorized into decoding-based and training-based methods. Decoding-based methods rectify these issues by soft or hard n-gram blocking <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref> and stochastic sampling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11]</ref>. Training-based methods minimize the probabilities of tokens that already are generated in the previous context <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref>. Despite their effectiveness, the reasons why the model prefers repetitions and how repetition occurs during decoding are still unclear. We train a Transformer model (750M parameters, similar to GPT-2 Large) on large-scale human corpus Wikitext-103 (over 100 million words). Left: Greedy decoding gets stuck in consecutive sentence-level repetition. Right: The percent of consecutive repetition of the word-, phrase (# number of words)-and sentence-level (see Appendix A for formulations of consecutive repetition). The model results are the average of generated texts given different prefixes from the Wikitext-103 dev set. Specifically, given 50 tokens as the prefix, the model greedily generates the next 200 tokens. Compared to human language, the model has substantially more consecutive sentence-level repetition.</p><p>Fu et al. <ref type="bibr" target="#b7">[8]</ref> is the first to analyze the repetition problems from a theoretical perspective by assuming that the language models can be approximated by short-sighted first-order Markov models. However, Holtzman et al. <ref type="bibr" target="#b10">[11]</ref> observe the cases of a positive feedback loop of repetitions, which indicates that language models do look at the long-distance context and may not be simply viewed as firstorder Markov models. The cases also reveal that the probabilities of repetitive tokens have certain relationships with previous repetitions in context. However, they do not analyze why the model prefers consecutive repetitions. In this paper, we further dig into the problem and conduct quantitative experiments to analyze the underlying issue of the repetition.</p><p>For a quantitative investigation on consecutive repetitions, we compare the probabilities of the same tokens in repetitive sentences. For example, given a sequence, 'I love oranges . I love oranges .', we compare the probability P ? ('oranges'|'I love oranges . I love') with P ? ('oranges'|'I love'). The difference between them is that, for the second 'oranges', there is already a token 'oranges' that shares the same sentence-level context 'I love'. We manually repeat the sentence n times as the context so that the next 'oranges' has n repetitions in context. In this way, we can investigate the relationship between the probability of the token and the number of repetitions in context. For example, we can first construct the context by repeating the sentence 'I love oranges .' n times plus 'I love', and then obtain the probability that the model outputs 'orange' at the current step.</p><p>Through our quantitative investigation across the different corpus, we find that 1) The model tends to raise the probability of repeating the previous sentence. Specifically, even if there is only one sentence-level context repetition, the probability of repetition at the current step increases in most cases. The cause of the phenomenon may be that the model is so confident with its context, when there is a previous token (i.e., 'oranges') sharing the same sentence-level context (i.e., 'I love'), that the model learns a shortcut to directly copy the token; 2) Self-reinforcement effect: the probability of repetition increases almost monotonically with the number of historical repetitions. Finally, the probability of repetition stabilizes around a certain ceiling value. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, as the number of repetitions increases, the probability of the word 'rounds' and 'general' both increase almost monotonically and finally stabilize; 3) Sentences with higher initial probabilities usually have a stronger self-reinforcement effect. For example, in Figure <ref type="figure" target="#fig_1">2</ref>, we can find that the sentence with a higher initial probability (i.e., the red bar at '0' of x-axis) grows faster and can reach an extremely high value with a few repetitions. Furthermore, the sentences with a higher initial likelihood (e.g., sentences generated by the model itself with maximization-based decoding algorithms) may have a stronger self-reinforcement effect.</p><p>According to our findings, the reasons why the model tends to repeat themselves are as follows: The sentence repetition occurs since the previous sentence generated by the maximization-based decoding algorithms has a relatively high probability, and the model tends to further increase the probability of repeating that sentence. Once the model generates one repetitive sentence, the probability of the repetitive sentence would be further enhanced since there are more repetitions sharing the same sentence-level context to support the choice of copying. As a result, the model gets stuck in the sentence-level loops due to the self-reinforcement effect.</p><p>To mitigate the tendency to repeat previous sentences and overcome the self-reinforcement effect, we aim to make the model aware that the probabilities of generating redundant sentences should be reduced. In addition, the more sentence-level repetitions exist, the lower the probability of the redundant sentence should be. In this paper, we propose a simple and effective training-based method, PseuDo-RepetITion PenalizaTiOn (DITTO). We first manually construct pseudo data by repeating a sentence sampled from the training corpus. Then, we design a repetition penalization loss function on these pseudo data for the model to learn to exponentially decay the repetition probability as the number of sentence repetitions increases. The experimental results show that DITTO not only significantly reduces repetitions without sacrificing language model capability in terms of perplexity, but also achieves better generation quality. In the Wikitext-103 <ref type="bibr" target="#b18">[19]</ref> open-ended generation task, our methods can achieve lower perplexity and higher MAUVE scores than the strong competitors. With stochastic decoding strategies such as top-k <ref type="bibr" target="#b5">[6]</ref> and nucleus sampling <ref type="bibr" target="#b10">[11]</ref>, generated texts by our methods are the closest to human-written texts, as measured by MAUVE score and human evaluation. In the commonly-used CNN/DailyMail summarization benchmark <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>, our methods also achieve the best performance among the strong competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Analyzing Repetition</head><p>In this section, we would like to statistically investigate the relationship between the probabilities of repetitive sentences and previous repetitions in context. We first define several metrics to measure how the probability changes as the number of repetitions increases. Then, we summarize our findings from the experimental results, and discuss how sentence-level repetitions occur during decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experiment Design</head><p>Formally, we have a sentence s from a corpus D and repeat it N times to construct a sample sequence</p><formula xml:id="formula_0">x = (s 0 , s 1 , s 2 , ? ? ? , s N ) where s n = (x n,1 , ? ? ? , x n,Ls ).</formula><p>x n,l is the l-th token in the n-th repetition of the sentence s and L s is the number of tokens in sentence s. Denote the sentence-level context of token x n,l as x n,&lt;l = (x n,1 , ? ? ? , x n,l-1 ). We define that there is a sentence-level context repetition for the token x n,l if and only if there is another same token x i,l in context such that x i,&lt;l = x n,&lt;l and i &lt; n. For example, for the sequence 'I love oranges . I love oranges .' aforementioned, the second token 'oranges' has a sentence-level context repetition. By feeding the sequence x to a pre-trained model P ? , we can obtain the probability distribution P ? (x n,l |x &lt;n,l ) where x &lt;n,l = (s 0 , ? ? ? , s n-1 , x n,&lt;l ). To study whether P ? (x n,l |x &lt;n,l ) increases as n increases, we define several metrics as follows: ? Average Token Probability: TP(s n ) = 1 Ls Ls l=1 P ? (x n,l |x &lt;n,l ), which is the average token probability of n-th repetitive sentence s n . TP(s 0 ) is the initial probability of tokens in the first sentence.</p><p>? Rate of Increased Token Probability:</p><formula xml:id="formula_1">IP(s n ) = 1 Ls Ls l=1 1(P ? (x n,l |x &lt;n,l ) &gt; P ? (x 0,l |x &lt;0,l ))</formula><p>where 1 is the indicator function. We use IP(s n ) to calculate that, how many probabilities of tokens increase in s n compared to the initial ones in s 0 . ? Winner Rate: We say x n,l is a winner if P ? (x n,l |x &lt;n,l ) &gt; P ? (x 0,l |x &lt;0,l ) and x n,l = arg max P(?|x &lt;n,l ). Then, we define the winner rate as WR(s n ) = 1 Ls Ls l=1 1(x n,l is a winner).</p><p>A higher winner rate means that the repetitions are more likely to be generated by a maximizationbased decoding algorithm such as greedy decoding.</p><p>Following the previous work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref>, the pre-trained model is a 16-layer Transformer decoder trained on Wikitext-103 <ref type="bibr" target="#b18">[19]</ref>. The details are introduced in Sec. 4.1. Given the corpus D, we can calculate the average values of TP, IP and WR of the n-th repetitive sentence s n as</p><formula xml:id="formula_2">TP n = 1 |D| s?D TP(s n ), IP n = 1 |D| s?D IP(s n ), WR n = 1 |D| s?D WR(s n )</formula><p>by enumerating all sentences s in D. To further study the effect of different corpus, we construct three corpus 1) D random : the tokens of sentences are randomly sampled from the vocabulary of the model; 2) D book : the sentences are randomly sampled from BookCorpus <ref type="bibr" target="#b35">[36]</ref>, and 3) D wiki : the sentences are randomly sampled from the dev set of Wikitext-103. The size of all different corpus is 1,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Results and Analyses</head><p>In the analyses, we attempt to answer: 1) Why sentence repetitions occur? 2) Why the model gets stuck into the repetition loop? 3) What kinds of sentences are more likely to be repeated?</p><p>Why sentence repetitions occur? To our surprise, as shown in Figure <ref type="figure" target="#fig_2">3</ref> (b), IP 1 is higher than 90% across the various corpus, meaning that even if there is only one sentence-level context repetition, the probability of repetition increases in most cases. That indicates the model has a strong preference to repeat the previous sentence. Note that the token repetition has not occurred at the current prediction step, and there is only the same sentence-level context. For example, model assigns a higher probability to P ? ('oranges'|'I love oranges . I love') than P ? ('oranges'|'I love') since the model has seen the pattern 'I love oranges' in the previous context. Thus, the model may be too confident with its previous context repetition and learn a 'cheap' shortcut where it directly copies the next token, 'oranges'. It is quite different from human language because human would try to avoid full sentence repetition, as the repeated sentence has no new information in most cases.</p><p>Why the model gets stuck into the repetition loop? Furthermore, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, as the number of repetitions increases, TP, IP and WR increase monotonically. That means, the sentencelevel repetition has a self-reinforcement effect: the more times a sentence has been repeated in the context, the higher the probability of continuing to generate that sentence. Finally, IP, WR, TP converge around certain ceiling values.</p><p>What kinds of sentences are more likely to be repeated? We can find that the sentences with high initial sentence probability TP 0 , such as sentences sampled from D wiki and D book , have a stronger self-reinforcement effect: TP and WR grows faster and reach the high ceiling values with a few repetitions. The higher TP and WR, the more likely the sentence to be repetitively generated by maximization-based decoding algorithms. Note that sentences generated by a maximization-based decoding algorithm have higher initial likelihoods. Thus, the model prefers to repeat itself.</p><p>Analyses During decoding with maximization-based algorithms, the sentence-level repetition occurs because 1) Previous generated sentences have high likelihoods and thus have more potential to be repeated; 2) Given previously generated sentences, the model is more likely to generate the repetitive sentence since the model is confident on the historically generated context and tries to find hints in context for the current generation; 3) Once the model repeats the sentence for several times, it would get stuck in the sentence loop due to self-reinforcement effect. The effectiveness of stochastic sampling approaches such as top-k <ref type="bibr" target="#b5">[6]</ref> and nucleus sampling <ref type="bibr" target="#b10">[11]</ref> may rely on 1) compared to the maximization-based decoding algorithms, the likelihood of previously generated sentences is lower and thus the sentences have less risk of being repeated; 2) current tokens are generated stochastically rather than chosen with maximum probability which can avoid the self-reinforcement effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pseudo-repetition Penalization Training</head><p>According to our analyses in Sec. 2, it can be clearly seen that the core issue of sentence-level repetition for the model is the tendency to repeat previous sentences and the self-reinforcement effect.</p><p>In this section, we propose a simple and effective method, named PseuDo-RepetITion PenalizaTiOn (DITTO). It first manually feeds repetitive sentences to the model and then explicitly teaches the model to be averse to such repetitions.</p><p>Pseudo Repetitive Data Construction. To construct a pseudo repetitive sample x, we first randomly pick a sentence s from the training corpus. Then, the pseudo data Sentence-level Repetition Penalization. To define the per-step penalization loss for token x ? {x 1,0 , ? ? ? , x N,Ls }, we define the training objective for the l-th token in the n-th repetition of the sentence s as</p><formula xml:id="formula_3">x = (s 0 , ? ? ? , s N ) = (x 0,0 , ? ? ? , x 1,0 , ? ? ? , x N,0 , ? ? ? , x N,</formula><formula xml:id="formula_4">L n,l DITTO (P ? (x n,l |x &lt;n,l )) = -log(1 -P ? (x n,l |x &lt;n,l ) -? ? P * ? (x n-1,l |x &lt;n-1,l ) ),<label>(1)</label></formula><p>where P * ? (?) means that the value is excluded for gradient backpropgation, which can implemented by tensor.detach in pytorch <ref type="bibr" target="#b21">[22]</ref>. ? is the penalization factor. When ? = 1, the loss function is minimized when the probability of token x n,l in n-th repetition is same as that in the (n-1)-th repetition to avoid the self-reinforcement effect; when ? &lt; 1, the probability of token x n,l in n-th repetition should be smaller than that in the (n -1)-th repetition to make model averse to sentence-level repetition. In other words, the probability of tokens in repetitive sentence should decay exponentially with a factor of ?, where ? is a hyper-parameter.</p><p>In our experiments, we apply the sentence-level repetition penalization by fine-tuning a standard MLE baseline. Fine-tuning is done by equally mixing the sentence-level repetition penalization update and normal MLE loss update. We find that, appending another sentence sampled from the training corpus randomly as the prefix to the pseudo repetitive data x can achieve better and more stable performance. We have tried other alternative loss functions, but they do not perform better. See Appendix C for the details.</p><p>Discussion In human language, there are necessary token-level and phrase-level repetitions that naturally occurs. For example, names of people, city and lemma, set phrases and proverbs appear many times in long documents such as Wikipedia. Thus, given the prefix, the model should have the ability to copy these words or phrases from the prefix and increase their repetition probabilities. These useful repetitions can be viewed as positive samples, and our pseudo repetitive data can be viewed as negative samples. Combining them for training, the model should learn to distinguish between useful and useless contexts. Thus, although our method is motivated by overcoming sentence-level repetition issues, it may improve the learning ability and generalizability of the model. Unlike previous methods (e.g., unlikehood <ref type="bibr" target="#b31">[32]</ref>, n-gram blocking <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref> and contrastive search <ref type="bibr" target="#b29">[30]</ref>) that put hard constraints to penalize any repetitions (regardless of the necessity of repetitions), our training objective explicitly encodes the intuition that the model should be inclined to avoid over-repetitions. Therefore, it may alleviate the drawbacks of over-penalization.  <ref type="foot" target="#foot_0">2</ref> . Specifically, we use a 16-layer Transformer with 8 attention heads, hidden size 1024 and fully-connected dimension 4096. There are a total of 750 million parameters, similar to GPT-2 Large <ref type="bibr" target="#b25">[26]</ref>. We first train the baseline model with standard maximum likelihood (MLE) for a maximum of 150k updates with a batch size of 3 samples. The maximum length of each training sample is 1,536 tokens. Then, we fine-tune the model that has the best validation perplexity with DITTO training for 10k steps. Unless otherwise mentioned, ? is set as 0.5. Baseline models are trained following their official implementations. All models are trained on 8 NVIDIA Tesla V100 GPUs. The experiments are implemented based on fairseq codebase <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Evaluation Following common practices <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref>, we evaluate the quality of open-ended sequence completions on the test set of Wikitext-103, where the prefix is 50 tokens, and models autoregressively generate the next 100 tokens. Models including various baselines are selected with the best perplexity on the validation set for a fair comparison. The evaluation metrics are listed as follows:</p><p>? MAUVE: MAUVE <ref type="bibr" target="#b23">[24]</ref> is a metric to measure how close model-generated text is to human language by approximately calculating the KL divergence between the distribution of model generated sequences and human language. The range of MAUVE is from 0 to 1. A higher MAUVE score indicates better generated sequences.</p><p>? Perplexity and Accuracy: Given the prefix and true next token, we use perplexity, and next-token prediction accuracy to measure the language modeling ability.</p><p>? Repetition: Following previous work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref>, we calculate the portion of duplicate 4grams (Repetition-4) in a generated sequence to measure phrase-level repetition, defined as 1.0 -|unique 4-grams|/|4-grams|, and average over completions. Similarly, we use portion of duplicate sentences (Repetition-Sen) to measure sentence-level repetition, defined as 1.0 -|unique sentences|/|sentences|. Since there are natural repetitions in human language, the optimal model should produce text whose repetition metrics are close to that of the gold text. Compatibility with Stochastic Decoding Strategies Sampling-based decoding algorithms such as top-k <ref type="bibr" target="#b5">[6]</ref> and top-p (nucleus sampling) <ref type="bibr" target="#b10">[11]</ref> are widely used in the various applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref> and large models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4]</ref>. We also confirm that our DITTO is compatible with these popular stochastic decoding strategies. The results are presented in Table <ref type="table" target="#tab_2">2</ref>. It can be seen that models trained with DITTO can reduce repetitions compared to the MLE baseline and achieve the closest results to human in repetition metrics. For the quality of generated sequences, DITTO achieves the highest MAUVE of 0.96 with top-k or nucleus sampling. Human Evaluation We conduct a pairwise crowdworker evaluation to judge the quality of the generations of DITTO compared to other baselines. For each pair of methods, the model generates texts based on the same 100 random prefixes from the test set of Wikitext-103. Evaluators are asked to independently judge which generation is better in terms of their grammaticality, relevance, coherence and informativeness. The evaluation interface and more details are in Appendix D. As shown in Table <ref type="table" target="#tab_3">3</ref>, DITTO consistently and significantly outperforms other competitors across different decoding strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results of</head><p>Self-reinforcement Effect As mentioned in Sec. 2, the sentence-level repetition has a self-reinforcement effect, which is the core issue of sentence-level repetition. We further study it when the model is trained with different methods. The settings of experiments follow those in Sec. 2. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, we can find 1) TP, IP and WR increase almost monotonically in UL and SG, which shows that the self-reinforcement effect has not been solved in these training-based methods; 2) When the model is trained with DITTO, these metrics drop rapidly as the number of repetitions grows, showing the effectiveness of DITTO in overcoming the self-reinforcement effect.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of Directed Generation</head><p>Setup We further conduct experiments on the directed abstractive summarization task CNN/DailyMail <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>: given an input document, the model generates several sentences as summarization. We adopt the state-of-the-art model BART-large <ref type="bibr" target="#b14">[15]</ref> as our baseline, which is large-scale encoder-decoder Transformer architecture trained on 160Gb data. For DITTO training, given a document and its summarization, we construct the pseudo data by first random sampling a sentence from the summarization and then repeating it until reaching the maximum summarization length of the decoder model while leaving the document unchanged. We follow the official implementations <ref type="bibr" target="#b14">[15]</ref> to train the BART-large model on the CNN/DailyMail and then fine-tune the model with the best validation perplexity using DITTO training. The ? is set as 0.9. During inference, tri-gram blocking <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20]</ref> and beam search (beam size = 5) are used as in Lewis et al. <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We evaluate the performance of the model with the standard F1-based ROUGE <ref type="bibr" target="#b27">[28]</ref> scores (ROUGE-1, ROUGE-2, ROUGE-L). As shown in Figure <ref type="figure" target="#fig_5">4</ref>, our training method consistently outperforms UL and SG with a large margin on ROUGE scores and outperforms other competitive baselines. The results also show that DITTO is compatible with the n-gram blocking, a decodingbased method for mitigating repetitions, demonstrating the generality of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analyses</head><p>Auto-completion with different decoding lengths From a practice point of view, we analyze DITTO in different decoding lengths. As shown in Figure <ref type="figure" target="#fig_6">5</ref>(a) and (b), the models trained with DITTO consistently and significantly outperform those trained with MLE with the constraints of different decoding lengths, which shows the effectiveness and robustness of our method.</p><p>Hyper-parameter Study Towards better usage and understanding of DITTO, we study the two hyper-parameters: 1) mix ratio of pseudo data and actual data, denoted as ?, and 2) penalization factor ?. As is shown in Figure <ref type="figure" target="#fig_6">5</ref>(c), models achieve the best performance with ? = 0.5 on both datasets. Thus, equally mixing the DITTO update and the MLE training loss update is recommended in practice. As for ?, we can observe that the optimal value varies in different tasks. In open-ended generation task Wikitext-103, a stronger penalization with ? = 0.5 is preferred. However, a mild penalization with ? = 0.9 is better in the summarization task. We conjecture that the repetition issue is more severe in generation tasks with more freedom, such as the open-ended generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Repetition in Neural Text Generation. Repetition has been a key problem in neural text generation for various tasks including open-ended generation (e.g., text continuation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b16">17]</ref>) and direct generation (e.g., summarization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b17">18]</ref>). Previous work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13]</ref> observes that, with maximization-based decoding strategies such as greedy search, language models prefer to generate bland and consecutive repetitions at the word-level, phrase-level, and sentence-level. Consecutive word-level and phrase-level repetition are lexical repetitions <ref type="bibr" target="#b8">[9]</ref> where the sentence structure is incorrect, while sentence-level repetition is semantic repetition that indicates the expression is not informative and has a discrepancy with human language <ref type="bibr" target="#b8">[9]</ref>. Recently, large-scale pre-training with Transformer architecture <ref type="bibr" target="#b30">[31]</ref> such as GPT-2 <ref type="bibr" target="#b25">[26]</ref> and BART <ref type="bibr" target="#b14">[15]</ref>, have greatly improved the ability of language modeling and can generate fluent sentences similar to human language. However, the generations still have a large number of unexpected consecutive sentence-level repetitions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref> in practice, which remains a severe problem in neural text generation.</p><p>Approaches for Mitigating Repetition. Approaches for mitigating repetition in neural text generation can be categorized into training-based <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref> and decoding-based <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref> approaches. The representative training-based approaches are unlikelihood training (UL) <ref type="bibr" target="#b31">[32]</ref> and straight to gradient (SG) <ref type="bibr" target="#b16">[17]</ref>. UL reduces the probability of negative candidate tokens, and SG improves the probability of tokens that do not belong to negative tokens, where the negative tokens are a set of tokens that appear in the previous context. However, our experiments in Sec. 4.2 show that they cannot solve self-reinforcement issue. In contrast, our work analyzes why models prefer to repeat themselves and quickly get stuck into the sentence-level loop, and mitigate the self-reinforcement issue. Many decoding-based methods have been proposed to rectify these issues in various tasks.</p><p>In summarization, n-gram blocking <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15]</ref> is often used to block repetitive n-grams from subsequent generations. In open-ended tasks, top-k <ref type="bibr" target="#b5">[6]</ref> and nucleus sampling <ref type="bibr" target="#b10">[11]</ref> truncate unreliable tail and employ sampling according to token distribution for generating fluent and informative texts.</p><p>Analyses of Repetition Problem. Although previous work has noticed the repetition issue and many approaches have been proposed to mitigate it at the model training or decoding stages, little has been discussed about the causes of the repetition. Fu et al. <ref type="bibr" target="#b7">[8]</ref> theoretically analyzes the problem by assuming the language models can be approximated to first-order Markov models. However, the cases of positive feedback loop of repetitions presented by Holtzman et al. <ref type="bibr" target="#b10">[11]</ref> indicate the repetition probability has complex relationships with a quite long-range context and language models may not be simplified as first-order Markov models. However, there is a lack of detailed numeric and statistical results across various sentences and analyses in their work. In contrast, we further dig into the problem and conduct quantitative experiments to analyze underlying issues of repetition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We quantitatively investigate why language models with maximization-based decoding prefer consecutive sentence-level repetitions. Our observations and analyses show that the issue of the repetition is the tendency to repeat previous sentences and the self-reinforcement effect. Guided by our analyses, we propose a simple and effective method named DITTO by constructing pseudo data and teaching model to learn to gradually decay the repetition probability as the number of sentence repetitions grows. The experiments on the Wikitext-103 open-ended generation task and the CNN/DailyMail summarization task demonstrate the superiority of our methods.</p><p>This work focuses on the sentence structure repetition issue. However, how to overcome semantic repetition while not hurting the modeling ability of the language model is still under exploration, which we leave for future work. We hope our quantitative analyses and approaches to mitigate sentence-level repetition can help the NLP community better understand, resolve the repetition issue and improve the fundamental language modeling ability of neural language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Consecutive Repetitions and Statistics of Beam Search Results</head><p>Previous work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b10">11]</ref> has observed that standard training and greedy decoding usually cause models to generate consecutive repetitive texts. These consecutive repetitive texts are redundant and do not convey new information, which is avoided in human language. There are three types of consecutive repetitions: word-level, phrase-level and sentence-level. The phrase-level means that a phrase consisting of several words is repeated consecutively. The sentence in our paper refers to a sequence split by '.!?' is repeated consecutively <ref type="foot" target="#foot_1">3</ref> . We calculate the ratio of consecutive repetition in a sequence x as follows.</p><p>Consecutive word-and phrase-level repetition Denote a sequence as</p><formula xml:id="formula_5">x = (x 1 , ? ? ? , x |x| ).</formula><p>The word-level repetition is calculated by</p><formula xml:id="formula_6">1 |x|-1 |x| i=2 1(x i = x i-1</formula><p>) where 1 refers to indicator function. The phrase-level repetition where the phrase has k words is calculated by</p><formula xml:id="formula_7">1 |x|-2k+1 |x| i=2k 1((x i-k+1 , ? ? ? , x i ) = (x i-2k+1 , ? ? ? , x i-k )).</formula><p>We calculate them for each se- quence x and average over the whole corpus.</p><p>Consecutive sentence-level repetition Denote a sample sequence as x = (s 0 , ? ? ? , s N ) that contains (N + 1) sentences. The sentence-level repetition is calculated by</p><formula xml:id="formula_8">1 N N i=1 1(s i = s i-1</formula><p>). We calculate it for each sequence x and average over the whole corpus.</p><p>As discussed in Section 1, compared to human language, the model with greedy decoding has substantially more sentence-level repetition. This phenomenon holds for other maximization-based decoding methods, such as beam search shown in Figure <ref type="figure" target="#fig_7">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Self-reinforcement in Model Generated Texts</head><p>When decoding auto-regressively, the probabilities of the repetitive sentence loops also have a selfreinforcement effect. As shown in Figure <ref type="figure">7</ref>, the probability of the token 'located' increases almost monotonically with the number of historical repetitions, which shows the same trend as in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Why manually repeat sentences to observe the probabilities rather than directly observe those of repetitive texts generated by the model auto-regressively Since our target is to study the relationships between the probabilities of repetitive tokens with their repetitions in context, we can manually repeat the sentence and observe the probabilities of the next repetition to simulate the situation when decoding auto-regressively. We do not directly observe the probabilities of repetitions across various generations and calculate metrics TP n , IP n and WR n , as described in Section 2, because 1) The model does not always get stuck into the repetitive sentence; 2) There are usually some 'dirty' tokens such as '&lt;/s&gt;' inserted into the consecutive sentences (e.g., the sentence in Table <ref type="table">6</ref>) or some minor changes (e.g., the sentence in Table <ref type="table" target="#tab_6">7</ref>)) in consecutive sentences repetitions so that it is not feasible to calculate those metrics for every token in sentences; 3) Most importantly,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Human Evaluation Details</head><p>We conduct a pairwise crowdworker evaluation to judge the quality of the generations of DITTO compared to other baselines. Models generate continuations based on the same 100 random prefixes from the test set of Wikitext-103. For each comparison of two continuations, three fluent English speakers are asked to independently judge which continuation is better. The overall quality is judged from four aspects: 1) grammar correctness; 2) relevance; 3) coherence; and 4) informativeness. The win rate is calculated as the total number of times that DITTO beats the other model divided by the total number of comparisons in the evaluation. The interface for the human evaluation is shown in Figure <ref type="figure" target="#fig_10">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Examples</head><p>In  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UL-token</head><p>John Russell , who had been appointed High Commissioner for Ireland , was also present at the hearing . &lt;/s&gt; &lt;/s&gt; = = = = Public response = = = = &lt;/s&gt; &lt;/s&gt; The public response to the report was mixed . The Irish Times called it " a most damning and comprehensive report on the Irish Question " . The Irish Times called it " a book of great importance " and " a work of great importance " . The Irish Times called it " a work of great importance " and " a work of great importance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UL-token+seq</head><p>Geoffrey Howe , the Prime Minister , and his wife were among the signatories to the London Necropolis Act , which was passed on 30 June 1921 . &lt;/s&gt; &lt;/s&gt; = = = = The London Necropolis Railway Act 1921 = 6 1 ? 2-mile ( 8.0 km ) long railway from Brookwood to Brookwood was the first railway in the world to be built in the United States outside the British Empire . The Act allowed the LSWR to carry out the work by rail from its own station at Brookwood , and the LSWR SG Edward Cave read the report and stated that he had been " deeply disappointed " that the government had not acted on the grounds that it had not acted on the grounds that it had not acted on the grounds that it had not acted on the grounds that it had not acted on the grounds that it had not acted on the grounds that it had not acted on the grounds that it had not acted on the grounds that it had not acted on the grounds that it had not acted on the grounds that it had not DITTO Edward Cave , the Director of Public Prosecutions , was quoted as saying that " the Government has not been able to make a decision on the issue of the bodies of the dead . " &lt;/s&gt; &lt;/s&gt; = = = Public reaction = = = &lt;/s&gt; &lt;/s&gt; The public reaction to the killings was generally positive . The Times called the actions " a most appalling and appalling act " and the Daily Mail called for the police to be withdrawn . The Daily Mail called for the police to be withdrawn , but the Daily Mail said that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UL-token</head><p>critics praised his performance . The New York Times ' Janet Maslin wrote , " Fast Eddie Felson is a real joy to watch , and he is a real joy to watch . " The New York Times ' Janet Maslin wrote , " Fast Eddie Felson is a real joy to watch , and he is a real joy to watch . " The New York Times ' Janet Maslin wrote , " Fast Eddie Felson is a real joy to watch , and he is a real joy to watch . " The New York Times UL-token+seq critics have praised Fast Eddie Felson as the film 's most memorable moment . In his review for the Chicago Reader , critic Richard Schickel wrote that " Fast Eddie Felson is the first of the great actors to come across as a master of the art of acting . " In his review for the New York Times , critic Anthony Boucher wrote that " Fast Eddie Felson is the first actor who can make a living through the motions of his actors , and the film is a triumph of invention . " In his review for the SG critics have praised Fast Eddie 's performance , including Roger Ebert , who wrote that " Fast Eddie is a great comic actor , and he has a great comic timing . " &lt;/s&gt; &lt;/s&gt; = = Personal life = = &lt;/s&gt; &lt;/s&gt; Fast Eddie was married twice , first to Patricia ( n?e &lt;unk&gt; ) and then to Patricia ( n?e &lt;unk&gt; ) . They divorced in 1977 . Their son , Eddie Jr . , is a retired basketball player who played for the Denver Nuggets , New York Knicks , Boston Celtics , and Dallas Mavericks .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DITTO</head><p>critics have praised Fast Eddie 's performance , including the New York Times critic Bosley Crowther , who wrote that " Fast Eddie is a fine actor , and he is a fine actor . " &lt;/s&gt; &lt;/s&gt; = = = Accolades = = = &lt;/s&gt; &lt;/s&gt; Fast Eddie was nominated for the Academy Award for Best Actor in a Leading Role for his role as the title character in the 1986 film Fast Eddie . He was also nominated for the Golden Globe Award for Best Actor -Motion Picture Musical or Comedy for his role as the title Table <ref type="table">8</ref>: Example 1 of generated texts by models with nucleus sampling. Prefix 's screenplay was selected by the Writers Guild of America in 2006 as the 96th best motion picture screenplay of all time . In June 2008 , AFI released its " Ten top Ten " -the best ten films in ten " classic " American film genres -after </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Statistics of human sentences versus model generation on the dev set of Wikitext-103. We train a Transformer model (750M parameters, similar to GPT-2 Large) on large-scale human corpus Wikitext-103 (over 100 million words). Left: Greedy decoding gets stuck in consecutive sentence-level repetition. Right: The percent of consecutive repetition of the word-, phrase (# number of words)-and sentence-level (see Appendix A for formulations of consecutive repetition). The model results are the average of generated texts given different prefixes from the Wikitext-103 dev set. Specifically, given 50 tokens as the prefix, the model greedily generates the next 200 tokens. Compared to human language, the model has substantially more consecutive sentence-level repetition.</figDesc><graphic url="image-1.png" coords="2,303.64,72.62,200.20,122.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Manually repeat a given sentence several times, feed to the model and observe the same token's probability p(x t |x &lt;t ) (in red) and maximum probability max p(?|x &lt;t ) (in blue) as sentence repetition times increase. Left: Repeat a normal sentence ('She is named a proxy general under Gaara .') and present the probability of the token 'general'. Right: Repeat a random sentence ('fr?a backed rounds Manganiello Benzedrine Magruder Crego Stansel Zemin compressus .') where tokens are randomly sampled from the vocabulary, and present the probability of the token 'rounds'. The probability of repetition (in red) has a self-reinforcement effect: the probability of repetition (y-axis) increases almost monotonically with the number of historical repetitions (x-axis). Best viewed in color and zoomed in a desktop monitor.</figDesc><graphic url="image-2.png" coords="3,108.00,66.33,190.08,126.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Construct a sample by repeating a sentence N = 100 times, manually feed it to the model and calculate TP n (average token probability), IP n (rate of increased probability) and WR n (winner rate) for n = 1, ? ? ? , N . The results average on sentences from different corpus D random , D book and D wiki respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Ls ) is constructed by repeating the sentence s by N + 1 times. The sentences are repeated until they reaches the maximum input sequence length of the model (e.g., 1,536 tokens in the Transformer for open-end generation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 1</head><label>1</label><figDesc>SetupDataset and Model Training We train models on the benchmark dataset Wikitext-103<ref type="bibr" target="#b18">[19]</ref> to evaluate the performance of open-ended generation. The dataset contains over 100 million words. The experiments are conducted at the word level. The model architecture and training hyper-parameters exactly follow the implementations of Welleck et al.<ref type="bibr" target="#b31">[32]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of different training-based methods by feeding repetitive sentences as described in Sec. 2]. We average the results on sentences from D wiki .Table4: Abstractive summarization results on CNN/DailyMail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of DITTO in different decoding lengths and hyper-parameters. ? is the penalization factor and ? is the mix ratio of pseudo data and true data. Results are average of three runs with different random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Statistics of human language versus model generation. The model generates the next 200 tokens with beam search (b = 10) given 50 tokens as the prefix. The results are the average of the Wikitext-103 dev set. Compared to human language, the model has substantially more consecutive sentence-level repetition.</figDesc><graphic url="image-9.png" coords="13,203.40,337.81,198.00,121.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison of sentence probability. The model trained with L DITTO-margin quickly reduces SP to close to 0 even if there is only one sentence repetition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>this section, we show the examples of generated texts of different training methods in the openended generation task. The examples are presented in the following tables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Screenshot of the user interface in human evaluation experiments.</figDesc><graphic url="image-11.png" coords="16,108.00,285.60,395.98,202.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>MLE a deal with Chase &amp; Caldera Entertainment was reached . Ten countries also released 100 top ten films in multiple-day international markets . &lt;/s&gt; &lt;/s&gt; = = = 2010s = = = &lt;/s&gt; &lt;/s&gt; &lt;/s&gt; = = = = 2010s = = = = &lt;/s&gt; &lt;/s&gt; &lt;/s&gt; = = = = = 2010s = = = = = &lt;/s&gt; &lt;/s&gt; &lt;/s&gt; = = = = = 2010s = = = = = &lt;/s&gt; &lt;/s&gt; &lt;/s&gt; = = = = = = 3D movies = = = = = = &lt;/s&gt; &lt;/s&gt; &lt;/s&gt; = = = = 3D movies UL-token adjusting their publication guidelines for rated films and having it between 25th and 26th , with cumulative scores of 70.2 and 90.5 . In December 2011 , The Hollywood Reporter said that with those achieved in reverse , characters such as Hopper could become " more powerful than previously unseen " . It also said that women would have to look after the characters in pictures like Barbie and Paris Hilton for inspiration . &lt;/s&gt; In April 2014 , Warner Bros. was sued by Roger Hathaway , David Fincher , Arnold Schwarzenegger , Darren Aronofsky , UL-token+seq polling for AFI 's 10 Top 10 , and also including WALL-E , WALL Scattered , WALL Building 3 , and WALL Jack . &lt;/s&gt; In October 2013 , Netflix announced the long-running website Expedition to Antarctica , celebrating the new Space Travel , and the search for and rescue mission planners had been in the middle of the year . A 2012 study by video game journalist Toby Philpott found a large majority of the space explorers who have survived on the ship as well as exploring and exploring locations . The Discovery and Curiosity SG polling over 300 people from 108 countries worldwide . In 2012 , AFI ranked 65th on Bravo 's 100 Years ... 100 Movies list . &lt;/s&gt; &lt;/s&gt; = = Awards = = &lt;/s&gt; &lt;/s&gt; Writers Guild of America 's 50 Years ... 100 Heroes &amp; Villains : &lt;/s&gt; * AFI 's 100 Years ... 100 Movie Quotes : " Run to your sister , son ... If you hate her [ ... ] you wouldn 't believe that you 're too stupid ? " -Nominated &lt;/s&gt; AFI 's 100 Years ... 100 Thrills -Nominated &lt;/s&gt; AFI 's DITTO polling more than 130,000 people at the screening and success . &lt;/s&gt; In October 2009 , AFI 's 10 Top 10 ranked " The 100 Scariest Movie Trailer " , the first-ever list of AFI 's 100 Years ... 100 Thrills -while the film was also ranked among the top ten all-time greatest movie villains in the last five years -and in December 2012 , Time magazine listed it as the sixth-greatest film villain in history . In August 2012 , Total Film ranked " The 100 Scariest Movie Characters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Open-ended GenerationMethod Comparison We compare with training-based methods since decoding-based methods can be readily applied to models with our training method. Following<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref>, we first compare different training-based algorithms with greedy decoding. The results are presented in Table1. Our DITTO can significantly reduce the phrase-level and sentence-level repetitions and achieve the best MAUVE score of 0.77. Although UL-token+seq can generate fewer repetitions, its improvement in the general quality of the model measured by MAUVE is limited since its generations are usually not relevant to the prefix (See Appendix E for examples). Compared to the standard baseline MLE, other competitors reduce repetitions by sacrificing perplexity while DITTO achieves lower perplexity and higher accuracy. These results show that DITTO can not only mitigate repetitions but also help the model improve the language modeling and generation quality.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of different training-based methods on the test set of Wikitext-103 for open-ended generation. The results are reported based on three runs with different random seeds. The best value is bolded and the second best is underlined.</figDesc><table><row><cell>Model</cell><cell cols="5">MAUVE Perplexity Accuracy Repetition-4 Repetition-Sen</cell></row><row><cell>MLE [26]</cell><cell cols="2">0.34 ?0.02 25.68 ?0.04</cell><cell>0.39 ?0.00</cell><cell>44.20 ?1.43 %</cell><cell>14.50 ?1.59 %</cell></row><row><cell>UL-token [32]</cell><cell cols="2">0.57 ?0.01 26.98 ?0.12</cell><cell>0.39 ?0.00</cell><cell>28.30 ?0.78 %</cell><cell>7.40 ?0.83 %</cell></row><row><cell cols="3">UL-token+seq [32] 0.48 ?0.03 25.95 ?0.08</cell><cell>0.40 ?0.00</cell><cell>7.60 ?0.46 %</cell><cell>0.05 ?0.03 %</cell></row><row><cell>SG [17]</cell><cell cols="2">0.74 ?0.01 25.84 ?0.06</cell><cell>0.40 ?0.00</cell><cell>23.00 ?0.28 %</cell><cell>5.24 ?0.75 %</cell></row><row><cell>DITTO (ours)</cell><cell cols="2">0.77 ?0.01 24.33 ?0.04</cell><cell>0.42 ?0.00</cell><cell>22.00 ?0.31 %</cell><cell>2.85 ?0.74 %</cell></row><row><cell>Human</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.10%</cell><cell>0.01%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of different training-based methods on the test set of Wikitext-103 under different stochastic decoding algorithms. k = 50 and top-p (p = 0.9) for nucleus sampling. The numbers closest to human scores are in bold except for MAUVE<ref type="bibr" target="#b23">[24]</ref>.</figDesc><table><row><cell>Search</cell><cell>Model</cell><cell cols="3">MAUVE Repetition-4 Repetition-Sen</cell></row><row><cell></cell><cell>MLE [26]</cell><cell>0.94 ?0.00</cell><cell>1.60 ?0.09 %</cell><cell>0.25 ?0.06 ?</cell></row><row><cell></cell><cell>UL-token [32]</cell><cell>0.95 ?0.00</cell><cell>0.70 ?0.13 %</cell><cell>0.00 ?0.00 ?</cell></row><row><cell>Top-k</cell><cell cols="2">UL-token+seq [32] 0.93 ?0.01</cell><cell>0.09 ?0.11 %</cell><cell>0.06 ?0.02 ?</cell></row><row><cell></cell><cell>SG [17]</cell><cell>0.93 ?0.01</cell><cell>0.50 ?0.19 %</cell><cell>0.00 ?0.00 ?</cell></row><row><cell></cell><cell>DITTO</cell><cell>0.96 ?0.00</cell><cell>1.00 ?0.10 %</cell><cell>0.09 ?0.01 ?</cell></row><row><cell></cell><cell>MLE [26]</cell><cell>0.94 ?0.00</cell><cell>1.40 ?0.08 %</cell><cell>0.08 ?0.01 ?</cell></row><row><cell></cell><cell>UL-token [32]</cell><cell>0.94 ?0.00</cell><cell>0.47 ?0.08 %</cell><cell>0.00 ?0.00 ?</cell></row><row><cell>Nucleus</cell><cell cols="2">UL-token+seq [32] 0.94 ?0.01</cell><cell>0.08 ?0.05 %</cell><cell>0.02 ?0.02 ?</cell></row><row><cell></cell><cell>SG [17]</cell><cell>0.93 ?0.01</cell><cell>0.40 ?0.19 %</cell><cell>0.06 ?0.01 ?</cell></row><row><cell></cell><cell>DITTO</cell><cell>0.96 ?0.00</cell><cell>0.98 ?0.09 %</cell><cell>0.08 ?0.01 ?</cell></row><row><cell></cell><cell>Human</cell><cell>-</cell><cell>1.10%</cell><cell>0.10?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Human Evaluation Results.</figDesc><table><row><cell></cell><cell>Win Rate</cell></row><row><cell>Greedy Search</cell><cell></cell></row><row><cell>DITTO vs MLE</cell><cell>*84%</cell></row><row><cell>DITTO vs UL-token</cell><cell>*80%</cell></row><row><cell>DITTO vs UL-token+seq</cell><cell>*65%</cell></row><row><cell>DITTO vs SG</cell><cell>*68%</cell></row><row><cell>Nucleus Sampling p = 0.9</cell><cell></cell></row><row><cell>DITTO vs MLE</cell><cell>*65%</cell></row><row><cell>DITTO vs UL-token</cell><cell>*71%</cell></row><row><cell>DITTO vs UL-token+seq</cell><cell>*62%</cell></row><row><cell>DITTO vs SG</cell><cell>*63%</cell></row></table><note><p>* means the results are statistically significant (2-sided binomial test, p&lt;.05).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Abstractive summarization results on CNN/DailyMail.</figDesc><table><row><cell>Model</cell><cell>ROUGE-1</cell><cell cols="2">ROUGE-2 ROUGE-L</cell></row><row><cell>Pointer-generator + Coverage [29]</cell><cell>39.53</cell><cell>17.28</cell><cell>36.38</cell></row><row><cell>Mask Attention Network [7]</cell><cell>40.98</cell><cell>18.29</cell><cell>37.88</cell></row><row><cell>BertSum [18]</cell><cell>42.13</cell><cell>19.60</cell><cell>39.18</cell></row><row><cell>UniLM [5]</cell><cell>43.08</cell><cell>20.43</cell><cell>40.34</cell></row><row><cell>UniLM V2 [2]</cell><cell>43.16</cell><cell>20.42</cell><cell>40.14</cell></row><row><cell>ERNIE-GEN-large [33]</cell><cell>44.02</cell><cell>21.17</cell><cell>41.26</cell></row><row><cell>PEGASUS [34]</cell><cell>44.17</cell><cell>21.47</cell><cell>41.11</cell></row><row><cell>ProphetNet [25]</cell><cell>44.20</cell><cell>21.17</cell><cell>41.30</cell></row><row><cell>PALM [3]</cell><cell>44.30</cell><cell>21.12</cell><cell>41.14</cell></row><row><cell>BART-large w.t. MLE [15]</cell><cell cols="3">44.11?0.03 21.21?0.01 40.83?0.02</cell></row><row><cell>BART-large w.t. UL-token [32]</cell><cell cols="3">44.17?0.04 21.20?0.02 40.83?0.03</cell></row><row><cell cols="4">BART-large w.t. UL-token+seq [32] 44.13?0.07 21.15?0.11 40.71?0.09</cell></row><row><cell>BART-large w.t. SG [17]</cell><cell cols="3">44.18?0.06 21.17?0.07 40.89?0.05</cell></row><row><cell>BART-large w.t. DITTO</cell><cell cols="3">44.41?0.03 21.45?0.01 41.16?0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of different loss functions on the test subset of Wikitext-103 for the open-ended generation task.</figDesc><table><row><cell>Objective</cell><cell cols="5">MAUVE Perplexity Accuracy Repetition-4 Repetition-Sen</cell></row><row><cell>L DITTO-mse</cell><cell cols="2">0.73 ?0.01 24.34 ?0.04</cell><cell>0.42 ?0.00</cell><cell>22.70 ?0.34 %</cell><cell>2.98 ?0.77 %</cell></row><row><cell cols="3">L DITTO-margin 0.66 ?0.04 24.38 ?0.04</cell><cell>0.41 ?0.00</cell><cell>19.31 ?0.44 %</cell><cell>2.17 ?0.77 %</cell></row><row><cell>L DITTO</cell><cell cols="2">0.77 ?0.01 24.33 ?0.04</cell><cell>0.42 ?0.00</cell><cell>22.00 ?0.31 %</cell><cell>2.85 ?0.74 %</cell></row><row><cell>Human</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.10%</cell><cell>0.01%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Example 2 of generated texts by models with greedy decoding. Prefix is summed up as " not to be missed . " &lt;/s&gt; Paul Newman reprised his role as Fast Eddie Felson in the 1986 film The Color of Money , for which he won the Academy Award for Best Actor in a Leading Role . A number of observers and MLE critics have praised Fast Eddie Felson as one of the greatest actors of all time . In 2007 , Fast Eddie Felson was ranked # 1 on Empire magazine 's list of the 500 Greatest Movie Characters of All Time . In 2010 , Fast Eddie 's popularity was listed at # 3 on Empire 's list of the 500 Greatest Movie Characters of All Time . In 2012 , Fast Eddie 's popularity was ranked # 1 on Empire 's list of the 500 Greatest Movie Characters of All Time . In 2013 , Fast Eddie 's popularity was</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/facebookresearch/unlikelihood_training</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The strict definition of the sentence can be found at https://en.wikipedia.org/wiki/Sentence_ (linguistics). Here we use the end token to split sentences for ease of experiments.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>The rim is slightly raised about the surrounding surface and it has an outer rampart that has been only lightly impacted . No significant craters intersect the rim , and it is sloped about 1 @.@ 5 ?toward the direction 50 -90 ?from the Earth .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greedy Decoding:</head><p>The age of the crater is about 3 @.@ 6 billion years and it has been in the proximity of the south lunar pole for at least 10 @,@ 000 years . The South Crater is located on the southern edge of the northern highlands . The South Crater is located on the southern edge of the northern highlands . The South Crater is located on the southern edge of the northern highlands . The South Crater is located on the southern edge of the northern highlands . The South Crater is located on the southern edge of the northern highlands . The South Crater is located on the southern edge of the northern highlands . The South Crater is located on the southern edge of the northern highlands . The South Crater is located on the southern edge of the northern highlands . ? Figure <ref type="figure">7</ref>: As mentioned in Figure <ref type="figure">1</ref>, given the prefix, the model gets stuck inthe sentence-level loop (i.e., repeat the sentence 'The South Crater is located on the southern edge of the northern highlands .'). We present the probability of the token 'located' (y-axis) as the number of historical repetitions (x-axis) grows. For texts generated by the model autoregressively, the probability of repetition (in red) also has a self-reinforcement effect. Best viewed in color and zoomed in a desktop monitor.</p><p>we want to study whether the self-reinforcement effect is caused by some problems in the language modeling ability itself so that the effects exists for not only the sentences generated by the model but also any sentences no matter whether the sentences repeated in contexts are grammatically correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Loss Function of DITTO</head><p>The training loss function of DITTO is</p><p>where P * ? (?) means that the value is excluded for gradient backpropgation and ? is the penalization factor. The loss function is minimized when</p><p>where ? is a key hyper-parameter. When ? = 1, the loss function requires the probability of token x n,l in n-th repetition is same as that in the (n -1)-th repetition to avoid the self-reinforcement effect; when ? &lt; 1, the probability of token x n,l in n-th repetition should be smaller than that in the (n -1)-th repetition to make model averse to sentence-level repetition. To achieve this goal, we can use the following loss function:</p><p>where MSE refers to mean square error. However, in practice, we find that the MSE loss achieves inferior performance. The results are shown in Table <ref type="table">5</ref>.</p><p>Rather than requiring the probability of token x n,l in n-th repetition is the same as that in the last repetition multiplying ?, we can relax the constraint and enable the probability of token x n,l to be no more than that in the (n -1)-th repetition multiplying ? as</p><p>In practice, as shown in Table <ref type="table">5</ref>, L DITTO-margin achieve worse results. We further analyze its performance on the self-reinforcement effect. As shown in Figure <ref type="figure">8</ref>, the model trained with L DITTO-margin quickly reduces TP to close to 0 even if there is only one sentence repetition. It indicates that the model may have learned a 'cheap' solution to optimize the loss function L DITTO-margin : Regardless of the probability of the previous sentence P * ? (x n-1,l |x &lt;n-1,l ), the probability of repeating the previous sentence is directly reduced to 0. Thus, the model may over-penalize repetitions so that all repetitions are forbidden in generations. However, there are some necessary repetitions that naturally occur, as discussed in Section 4. over-penalizing repetitions may hurt the language modeling ability and thus lead to inferior performance. Table <ref type="table">9</ref>: Example 2 of generated texts by models with nucleus sampling. Prefix business activities upon his return . He invested in a wide variety of businesses and infrastructure , spurring economic activity in the state . His investments ranged widely , including maritime insurance ( where is father-in-law had made his fortune ) , bridges , locks , MLE power plants and homes . He and his wife died from injuries sustained in labor ventures when they were old . He was buried in P?re Lachaise Cemetery in 1919 . He left a lasting legacy to his family . &lt;/s&gt; After leaving politics , Holloway became close friends with Josiah Quincy III . &lt;/s&gt; &lt;/s&gt; = = Early life and family = = &lt;/s&gt; &lt;/s&gt; Holloway was born on 12 November 1827 in Saint Joseph , Missouri , the only child of Elias and Emily Holloway . Most of Holloway 's family was derived from him , though they UL-token dams , gambling equipment , and positions . &lt;/s&gt; Henderson lost heavily during his sojourn to Mississippi . He attempted to build a canal , which he provided on several occasions with investment in agriculture and other pursuits . His wife Grace later wrote that Henderson " loathed working men so long that it nearly destroyed the whole business " . She only learned to gamble at all levels of the United States in the late 1840s . When he had returned to Columbus in the mid-1830s , all those who had survived the Ohio River had little UL-token+seq a gas leak , and the offices of the administration , engineers , and messengers for scientific and industrial interests . He also invested in William &lt;unk&gt; Ingram who purchased two berths from the Sons of the American Revolution , a natural gas ship that had been fired in 1807 in retaliation for the order to resupply the British Army troops at Fredericksburg . He did not work on the reorganization of the army , but supported the need for self-government-managers in a transition known as Federal service . &lt;/s&gt; Small farmers raised cattle in their SG tunnels , pumps , fire stations , telephones , showers and ships . &lt;/s&gt; Retaining custody after the September 11 attacks , Romney returned to family life by appointing first-class city councilman Carl McCall as U.S. Attorney General . Later that year , he was recognized by the Sons of Prosperity organization , where he served as chair of the Drafting Committee on Government Reform . While in the mid-1980s , Romney unsuccessfully ran for re-election as Secretary of Commerce under Governor Lawrence B. Jennings , losing to Richards . While in office , DITTO hotels , art galleries , and utilities . He formed new , larger enterprises ( offering bonds and furniture ) , offering services to smaller companies , attracted much investment in newly discovered companies . He also served as the state 's commissioner of the Board of Higher Education , providing education and higher education . &lt;/s&gt; &lt;/s&gt; = = Personal life = = &lt;/s&gt; &lt;/s&gt; John Smith was born in 1830 in Salt Lake City . His father , also named John , was an army officer stationed in a successful military career in the early 19th century .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards a human-like open-domain chatbot</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Palm: Pre-training an autoencoding&amp;autoregressive language model for context-conditioned generation</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8681" to="8691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask attention networks: Rethinking and strengthen transformer</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1692" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A theoretical analysis of the repetition problem in text generation</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Man-Cho</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14660</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long text generation by modeling sentence-level and discourse-level coherence</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6379" to="6393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to write with cooperative discriminators</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08562</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Straight to the gradient: Learning to use novel tokens for neural text generation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6642" to="6653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C?cero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?aglar</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<editor>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</editor>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016-08-11">2016. August 11-12, 2016. 2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<publisher>Demonstrations</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mauve: Measuring the gap between neural text and human text using divergence frontiers</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Pillutla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prophetnet: Predicting future n-gram for sequence-to-sequencepre-training</title>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2401" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prompt programming for large language models: Beyond the few-shot paradigm</title>
		<author>
			<persName><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Rouge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Text Summarization of ACL</title>
		<meeting>Workshop on Text Summarization of ACL<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A contrastive framework for neural text generation</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06417</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural text generation with unlikelihood training</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Erniegen: an enhanced multi-flow pre-training and fine-tuning framework for natural language generation</title>
		<author>
			<persName><forename type="first">Dongling</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3997" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">OPT: open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
