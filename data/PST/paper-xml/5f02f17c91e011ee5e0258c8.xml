<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Graph Neural Networks with Approximate PageRank</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-03">3 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
							<email>a.bojchevski@in.tum.de</email>
						</author>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
							<email>klicpera@in.tum.de</email>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@acm.org</email>
						</author>
						<author>
							<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benedek</forename><surname>Rózemberczki</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
							<email>guennemann@in.tum.de</email>
						</author>
						<author>
							<persName><forename type="first">Stephan</forename><surname>Günne</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling Graph Neural Networks with Approximate PageRank</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-03">3 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403296</idno>
					<idno type="arXiv">arXiv:2007.01570v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph neural networks</term>
					<term>personalized pagerank</term>
					<term>scalability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge -many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings.</p><p>We demonstrate that PPRGo outperforms baselines in both distributed and single-machine training environments on a number of commonly used academic graphs. To better analyze the scalability of large-scale graph learning methods, we introduce a novel benchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million node features. We show that training PPRGo from scratch and predicting labels for all nodes in this graph takes under 2 minutes on a single machine, far outpacing other baselines on the same graph. We discuss the practical application of PPRGo to solve large-scale node classification problems at Google. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) excel on a wide variety of network mining tasks from semi-supervised node classification and link prediction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b54">55]</ref> to community detection and graph classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref>. The success of GNNs on academic datasets has generated significant interest in scaling these methods to larger graphs for use in real-world problems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54]</ref>. Unfortunately, there are few large graph baseline datasets available; apart from a handful of exceptions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54]</ref>, the scalability of most GNN methods has been demonstrated on graphs with fewer than 250K nodes. Moreover, the majority of existing work focuses on improving scalability on a single machine. Many interesting network mining problems involve graphs with billions of nodes and edges that require distributed computation across many machines. As a result, we believe most of the current literature does not accurately reflect the major challenges of large scale GNN computing.</p><p>The main scalability bottleneck of most GNNs stems from the recursive message-passing procedure that propagates information through the graph. Computing the hidden representation for a given node requires joining information from its neighbors, and the neighbors in turn have to consider their own neighbors, and so on. This process leads to an expensive neighborhood expansion, growing exponentially with each additional layer.</p><p>In many proposed GNN pipelines, the exponential growth of neighborhood size corresponds to an exponential IO overhead. A common strategy for scaling GNNs is to sample the graph structure during training, e.g. sample a fixed number of nodes from the k-hop neighborhood of a given node to generate its prediction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b53">54]</ref>. The key differences between many scalable techniques lies in the design of the sampling scheme. For example, Chen et al. <ref type="bibr" target="#b12">[13]</ref> directly sample the receptive field for each layer using importance sampling, while Chen et al. <ref type="bibr" target="#b13">[14]</ref> use the historical activations of the nodes as a control variate. Huang et al. <ref type="bibr" target="#b26">[27]</ref> propose an adaptive sampling strategy with a trainable sampler per layer, and Chiang et al. <ref type="bibr" target="#b15">[16]</ref> sample a block of nodes corresponding to a dense subgraph identified by the clustering algorithm METIS <ref type="bibr" target="#b29">[30]</ref>. Because these approaches still rely on a multi-hop message passing procedure, there is an extremely steep trade-off between runtime and accuracy. Unfortunately, for many of the proposed methods sampling does not directly reduce the number of nodes that need to be retrieved, since e.g. we have first have to compute the importance scores <ref type="bibr" target="#b12">[13]</ref>.</p><p>Recent work shows that personalized PageRank <ref type="bibr" target="#b27">[28]</ref> can be used to directly incorporate multi-hop neighborhood information of a node without explicit message-passing <ref type="bibr" target="#b32">[33]</ref>. Intuitively, propagation based on personalized PageRank corresponds to infinitely many neighborhood aggregation layers where the node influence decays exponentially with each layer. However, as proposed, Klicpera et al. <ref type="bibr" target="#b32">[33]</ref>'s approach does not easily scale to large graphs since it performs an expensive variant of power iteration during training.</p><p>In this work, we present PPRGo, a GNN model that scales to large graphs in both single and multi-machine (distributed) environments by using an adapted propagation scheme based on approximate personalized PageRank. Our approach removes the need for performing expensive power iteration during each training step by utilizing the (strong) localization properties <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref> of personalized PageRank vectors for real-world graphs. These vectors can be readily approximated with sparse vectors and efficiently pre-computed in a distributed manner <ref type="bibr" target="#b4">[5]</ref>. Using the sparse pre-computed approximations we can maintain the influence of relevant nodes located multiple hops away without prohibitive message-passing or power iteration costs. We make the following contributions:</p><p>• We introduce the PPRGo model based on approximate personalized PageRank. On a graph of over 12 million nodes, PPRGo runs in under 2 minutes on a single machine, including pre-processing, training and inference time.</p><p>• We show that PPRGo scales better than message-passing GNNs, especially with distributed training in a real-world setting.</p><p>• We introduce the MAG-Scholar dataset (12.4M nodes, 173M edges, 2.8M node features), a version of the Microsoft Academic Graph that we augment with "ground-truth" node labels. The dataset is orders of magnitude larger than many commonly used benchmark graphs.</p><p>• Most previous work exclusively focuses on training time. We also show a significantly reduced inference time and furthermore propose sparse inference to achieve an additional 2x speed-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 GNNs and Message-Passing</head><p>Many proposed GNN models can be analyzed using the messagepassing framework proposed by Gilmer et al. <ref type="bibr" target="#b21">[22]</ref> or other similar frameworks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b49">50]</ref>. Typically, the computation is carried out in two phases: (i) messages are propagated along the neighbors; and (ii) the messages are aggregated to obtain the updated representations. At each layer, transformation of the input (e.g. linear projection plus a non-linearity) is coupled with aggregation/propagation among the neighbors (e.g. averaging). Increasing the number of layers is desirable since: (i) it allows the model to incorporate information from more distant neighbors; and (ii) it enables hierarchical feature extraction and thus the learning of richer node representations. However, this has both computational and modelling consequences. First, the recursive neighborhood expansion at each layer implies an exponential increase in the overall number of nodes we need to aggregate to produce the output at the final layer which is computationally prohibitive for large graphs. <ref type="foot" target="#foot_0">2</ref> Second, it has been shown <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52]</ref> that naively stacking multiple layers may suffer from over-smoothing that can reduce predictive performance.</p><p>To tackle both of these challenges Klicpera et al. <ref type="bibr" target="#b32">[33]</ref> suggest decoupling the feature transformation from the propagation. In their PPNP model, predictions are first generated (e.g. with a neural network) for each node utilizing only that node's own features, and then propagated using an adaptation of personalized PageRank. Specifically, PPNP is defined as:</p><formula xml:id="formula_0">Z = softmax Π sym H , H i,: = f θ (x i )<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">Π sym = α(I n −(1−α) Ã) −1 is a symmetric propagation matrix, Ã = D −1/2 AD −1/2</formula><p>is the normalized adjacency matrix with added self-loops, α is a teleport (restart) probability, H is a matrix where each row is a vector representation for a specific node, and Z is a matrix where each row is a prediction vector for each node, after propagation. The local per-node representations H i,: are generated by a neural network f θ that processes the features x i of every node i independently. The responsibility for learning good representations is delegated to f θ , while Π sym ensures that the representations are smoothly changing w.r.t. the graph.</p><p>Because directly calculating the dense propagation matrix Π sym in Eq. 1 is inefficient, the authors propose a variant of power iteration to compute the final predictions instead. Unfortunately, even a moderate number of power iteration evaluations (e.g. Klicpera et al. <ref type="bibr" target="#b32">[33]</ref> used K = 10 to achieve a good approximation) is prohibitively expensive for large graphs since they need to be computed during each gradient-update step. Moreover, despite the fact that Ã is sparse, graphs beyond a certain size cannot be stored in memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Personalized PageRank and Localization</head><p>Since it is more amenable to efficient approximation we analyze the personalized PageRank matrix</p><formula xml:id="formula_2">Π ppr = α(I n − (1 −α)D −1 A) −1 . Each row π (i) := Π ppr i,:</formula><p>is equal to the personalized (seeded) PageRank vector of node i. PageRank and its many variants <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref> have been extensively studied in the literature. Here we are interested in efficient and scalable algorithms for computing (an approximation) of personalized PageRank. Luckily, given the broad applicability of PageRank, many such algorithms have been developed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Random walk sampling <ref type="bibr" target="#b18">[19]</ref> is one such approximation technique. While simple to implement, in order to guarantee at most ϵ absolute error with probability of 1 − 1/n we need O( log n ϵ 2 ) random walks. Forward search <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> and backward search <ref type="bibr" target="#b3">[4]</ref> can be viewed as deterministic variants of the random walk sampling method. Given a starting configuration, the PageRank scores are updated by traversing the out-links (respect., in-links) of the nodes.</p><p>For this work we adapt the approach by Andersen et al. <ref type="bibr" target="#b4">[5]</ref> since it offers a good balance of scalability, approximation guarantees, and ease of distributed implementation. They show that π (i) can be weakly approximated with a low number of non-zero entries using a scalable algorithm that applies a series of push operations which can be executed in a distributed manner.</p><p>When the graph is strongly connected π (i) is non-zero for all nodes. Nevertheless, we can obtain a good approximation by truncating small elements to zero since most of the probability mass in the personalized PageRank vectors π (i) is localized on a small number of nodes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>. Thus, we can approximate π (i) with a sparse vector and in turn approximate Π ppr with a sparse matrix.</p><p>Once we obtain an approximation Π (ϵ ) of Π ppr we can either use it directly to propagate information, or we can renormalize it via D 1/2 Π (ϵ ) D −1/2 to obtain an approximation of the matrix Π sym .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Related work</head><p>Scalability. GNNs were first proposed in Gori et al. <ref type="bibr" target="#b23">[24]</ref> and in Scarselli et al. <ref type="bibr" target="#b41">[42]</ref> and have since emerged as a powerful approach for solving many network mining tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>. Most GNNs do not scale to large graphs since they typically need to perform a recursive neighborhood expansion to compute the hidden representations of a given node. While several approaches have been proposed to improve the efficiency of graph neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref>, the scalability of GNNs to massive (web-scale) graphs is still under-studied. As we discussed in § 1 the most prevalent approach to scalability is to sample a subset of the graph, e.g. based on different importance scores for the nodes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54]</ref>. <ref type="foot" target="#foot_1">3</ref> Beyond sampling, Gao et al. <ref type="bibr" target="#b20">[21]</ref> collect the representations from a node's neighborhood into a matrix, sort independently along each column/feature, and use the k largest entries as input to a 1-dimensional CNN. These techniques all focus on single-machine environments with limited (GPU) memory.</p><p>Buchnik and Cohen <ref type="bibr" target="#b9">[10]</ref> propose feature propagation which can be viewed as a simplified linearized GNN. They perform graphbased smoothing as a preprocessing step (before learning) to obtain diffused node features which are then used to train a logistic regression classifier to predict the node labels. Wu et al. <ref type="bibr" target="#b48">[49]</ref> propose an equivalent simple graph convolution (SGC) model and diffuse the features by multiplication with the k-th power of the normalized adjacency matrix. However, node features are often high dimensional, which can make the preprocessing step computationally expensive. More importantly, while node features are typically sparse, the obtained diffused features become denser, which significantly reduces the efficiency of the subsequent learning step. Both of these approaches are a special case of the PPNP model <ref type="bibr" target="#b32">[33]</ref> which experimentally shows higher classification performance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Approximating PageRank. Recent approaches combine basic techniques to create algorithms with enhanced guarantees <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. For example Wei et al. <ref type="bibr" target="#b47">[48]</ref> propose the TopPPR algorithm combining the strengths of random walks and forward/backward search simultaneously. They can compute the top k entries of a personalized PageRank vector up to a given precision using a filterand-refine paradigm. Another family of approaches <ref type="bibr" target="#b19">[20]</ref> are based on the idea of maintaining upper and lower bounds on the PageRank scores which are then used for early termination with certain guarantees. For our purpose the basic techniques are sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PPRGO MODEL</head><p>The design of our model is motivated by: (i) the insights from § 2.1, namely that we can decouple the feature transformation from the information propagation, and (ii) the insights from § 2.2, namely that we can approximate Π ppr with a sparse matrix. Analogous to Eq. 1 we define the final predictions of our model (see Fig. <ref type="figure" target="#fig_1">1</ref>):</p><formula xml:id="formula_3">Z = softmax Π (ϵ ) H , H i,: = f θ (x i )<label>(2)</label></formula><p>where Π (ϵ ) is a sparse approximation of Π ppr . To obtain each row of Π (ϵ ) we adapt the push-flow algorithm described in Andersen et al. <ref type="bibr" target="#b4">[5]</ref>. We additionally truncate Π (ϵ ) to contain only the top k largest entries for each row. That is, for each node i we only  consider the set of nodes with top k largest scores according to π (i).</p><p>Combined, the predictions for a given node i are:</p><formula xml:id="formula_4">z i = softmax j ∈N k (i) π (ϵ ) (i) j H j<label>(3)</label></formula><p>where N k (i) enumerates the indices of the top k largest non-zero entries in π (ϵ ) (i). Eq. 3 highlights that we only have to consider a small number of other nodes to compute the final prediction for a given node. Furthermore, this definition allows us to explicitly trade-off scalability and performance by increasing/decreasing the number of neighbors k we take into account. We can achieve a similar trade-off by changing the threshold ϵ which effectively controls the norm of the residual. We show the pseudo-code for computing π (ϵ ) in Algorithm 1. For further details see § A.4.</p><p>Algorithm 1 Approximate personalized PageRank (G, α, t, ϵ) <ref type="bibr" target="#b4">[5]</ref> Inputs: Graph G, teleport prob. α, target node t, max. residual ϵ 1: Initialize the (sparse) estimate-vector π (ϵ ) = 0 and the (sparse) residual-vector r = α • e t (i.e. e t = 1,</p><formula xml:id="formula_5">e v = 0, v t) 2: while ∃v s.t . r v &gt; α • ϵ • d v do # d v is the out-degree 3: π (ϵ ) v += r v 4: r v = 0 5: m = (1 − α) • r v /d v 6: for u ∈ N out G (v) do # v's outgoing neighbors 7: r u += m 8:</formula><p>end for 9: end while 10: return π (ϵ )   In contrast to the PPNP model, a big advantage of PPRGo is that we can pre-compute the sparse matrix Π (ϵ ) before we start training. Pre-computation allows PPRGo to calculate the training and inference predictions in O(k) time, where k ≪ N , and N is number of nodes. Better still, for training we only require the rows of Π (ϵ ) corresponding to the training nodes and the representations f θ (x i ) of their top-k neighbors. Furthermore, our model lends itself nicely to batched computation. For example, for a batch of nodes of size b we have to load in memory the features of at most b • k nodes. In practice, this number is smaller than b • k since the nodes that appear in N k (i) often overlap for the different nodes in the batch. We discuss the applicability and limitations of PPRGo in § A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effective Neighborhood, α and k</head><p>From the definition of personalized PageRank we can conclude that the hyper-parameter α controls the amount of information we are incorporating from the neighborhood of a node. Namely, for values of α close to 1 the random walks return (teleport) to the node i more often and we are therefore placing more importance on the immediate neighborhood of the node. As the value of α decreases to 0 we instead give more and more importance to the extended (multi-hop) neighborhood of the node. Intuitively, the importance of the k-hop neighborhood is proportional to (1 − α) k . Note that the importance that each node assigns to itself (i.e. the value of π (i) i ) is typically higher than the importance it assigns to the rest of the nodes. In conjunction with α, we can modify the number of k largest entries we consider to increase or decrease the size of the effective neighborhood. This stands in stark contrast to message-passing frameworks, where incorporating information from the extended neighborhood requires additional layers, thereby significantly increasing the computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SCALABILITY</head><p>Here we discuss the properties of PPRGo which make it suitable for large-scale classification problems occurring in industry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Node Classification in the Real World</head><p>The web is an incredibly rich data source and many different large graphs (potentially with hundreds of billions of nodes and edges) can be derived from it. Many web graphs have interesting node classification problems that can be addressed via semi-supervised learning.</p><p>Their applications occur across all media types and power many different Google products <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. In web-scale datasets, the node sets are large, the graphs commonly have power-law degrees, the datasets change frequently, and labels can quickly become stale. Therefore, having a model that trains as fast as possible is desirable to reduce the latency. Arguably even more important is having a model for which inference is as fast as possible, since inference is typically performed much more frequently than training in realworld settings. A low enough inference time may even open the door to using the model for online tasks, an impactful domain of problems where these models have limited penetration. Our proposed model, PPRGo, ameliorates many of the difficulties associated with scaling these learning systems. We have successfully tested it on internal graphs with billions of nodes and edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distributed Training</head><p>In contrast to most previously proposed methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref> we utilize distributed computing techniques which significantly reduce the overall runtime of our method. Our model is trained in two stages. First, we pre-compute the approximated personalized PageRank vectors using the distributed version of Algorithm 1 (see § A.4). Second, we train the model parameters with stochastic gradient descent. Both stages are implemented in a distributed fashion.</p><p>For the first stage we use an efficient batch data processing pipeline <ref type="bibr" target="#b10">[11]</ref> similar to MapReduce. Since we can compute the PageRank vectors for every node in parallel our implementation easily scales to graphs with billions of nodes. Moreover, we can a priori determine the number of iterations we need for achieving a desired approximation accuracy <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> which in turn means we can reliably estimate the runtime beforehand.</p><p>We implement PPRGo in Tensorflow and optimize the parameters with asynchronous distributed stochastic gradient descent. We store the model parameters on a parameter server (or several parameter servers depending on the model size) and multiple workers process the data in parallel. We use asynchronous training to avoid the communication overhead between many workers. Each worker fetches the most up-to-date parameters and computes the gradients for a mini-batch of data independently of the other workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficient Inference</head><p>As discussed in § 3 we only need to compute the approximate personalized PageRank vectors for the nodes in the training/validation set in order to train the model. In the semi-supervised classification setting these typically comprise only a small subset of all nodes (a few 100s or 1000s). However, during inference we still need to compute the PPR vector for every test node (see Eq. 3). Specifically, to predict the class label for m &lt; n test nodes we have to compute Z = softmax ΠH where Π is a m × n matrix such that each row contains the personalized PageRank vector for a given test node, and H is a n × c matrix of logits. Even though the computation of each of these m PPR vectors can be trivially parallelized, when m is extremely large the overall runtime can still be considerable. However, during inference we only use the PPR vectors a single time. In this case it is more efficient to circumvent this calculation and fall back to power iteration, i.e.</p><formula xml:id="formula_6">Q (0) = H , Q (p+1) = (1 − α)D −1 AQ (p) + αH . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>We furthermore found that, as opposed to training, during inference only very few (i.e. 1-3) steps of power iteration are necessary until accuracy improvements level off (see § 5.5). Hence we only need very few sparse matrix-matrix multiplications for inference, which can be implemented very efficiently. Since this truncated power iteration is very fast to compute, the neural network f θ quickly becomes the limiting factor for inference time, especially if it is computationally expensive (e.g. a deep ResNet architecture <ref type="bibr" target="#b25">[26]</ref> or recurrent neural network (RNN)). With PPRGo, we can leverage the graph's homophily to reduce the number of nodes that need to be analyzed. Since nearby nodes are likely to be similar we only need to calculate predictions H for a small, randomly chosen fraction of nodes. Setting the remaining entries to zero we can smooth out these sparse labels over the rest via Eq. 4.</p><p>In the very sparse case, using homophily to limit the number of needed predictions can be viewed as a label propagation problem with labels given by logits H . In the context of label propagation, the power iteration in Eq. 4 is a common algorithm known as "label propagation with return probability". This algorithm is known to perform well; we find that we can almost match the performance of full prediction with only a small fraction (e.g. 10 % or 1 %) of logits (see § 5.5). Overall, this approach allows us to reduce the runtime even below a model that ignores the graph and instead considers each node independently, without sacrificing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Setup. We focus on semi-supervised node classification on attributed graphs and demonstrate the strengths and scalability of PPRGo in both distributed and single-machine environments. To best align with real use cases we only use 20 • number of classes uniformly sampled (non-stratified) training nodes. We fix the value of the teleport parameter to a common α = 0.25 for all experiments except the unusually dense Reddit dataset, where α = 0.5. For details regarding training, hyperparameters, and metrics see § A.3 in the appendix. We answer the following research questions:</p><p>• What kind of trade-offs between scalability and accuracy can we achieve with PPRGo? ( § 5.2)</p><p>• How effectively can we leverage distributed training? ( § 5.3)</p><p>• How much resources (memory, compute) does PPRGo need compared to other scalable GNNs? ( § 5.4)</p><p>• How efficient is the proposed sparse inference scheme? ( § 5.5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Large-Scale Datasets</head><p>The majority of previous approaches are evaluated on a small set of publicly available benchmark datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>. The size of these datasets is relatively small, with the Reddit graph (233K nodes, 11.6M edges, 602 node features) <ref type="bibr" target="#b24">[25]</ref> typically being the largest graph used for evaluation. <ref type="foot" target="#foot_2">4</ref> Chiang et al. <ref type="bibr" target="#b15">[16]</ref> recently introduced the Amazon2M graph (2.5M nodes, 61M edges, 100 node features) which is large in terms of number of nodes, but tiny in terms of node feature size. <ref type="foot" target="#foot_3">5</ref>MAG-Scholar. To facilitate the development of scalable GNNs we create a new benchmark dataset based on the Microsoft Academic Graph (MAG) <ref type="bibr" target="#b42">[43]</ref>. Nodes represent papers, edges denote citations, and node features correspond to a bag-of-words representation of paper abstracts. We augmented the graph with "groundtruth" node labels corresponding to the papers' field of study.</p><p>We extract the node labels semi-automatically by mapping the publishing venues (conferences and journals) to a field of study using metadata on the top venues from Google Scholar. We create two sets of labels for the same graph. Coarse-grained labels correspond to the following 8 coarse-grained fields of study: biology, engineering, humanities, medicine, physics, sociology, business, and other. We refer to this graph as MAG-Scholar-C. Fine-grained labels correspond to 253 fine-grained fields of study such as: architecture, epidemiology, geology, ethics, anthropology, linguistics, etc. The fine-grained labels make the classification problem more difficult. We refer to this graph as MAG-Scholar-F.</p><p>The resulting MAG-Scholar graph is a few orders of magnitude larger then the commonly used benchmark graphs (12.4M nodes, 173M edges, 2.8M node features). The graphs and the code to generate them will be made publicly available. See § A.2 for a detailed description of the graph construction and node labelling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Scalability vs. Accuracy Trade-off</head><p>The approximation parameter ϵ and the number of top-k nodes are important hyper-parameters that modulate scalability and accuracy (see Eq. 3). We note that α and k play similar roles, so we choose to analyze k for a fixed α. To examine their effect on the performance of PPRGo we train our model on the MAG-Scholar-C graph for different values of k and ϵ. We repeat the experiment five times and report the mean performance. We investigate two cases: a sparsely labeled scenario similar to industry settings (160 nodes), and an "academic" setting with many more labeled nodes (105415 nodes).</p><p>As expected, we can see in Fig. <ref type="figure" target="#fig_2">2</ref> that the performance consistently increases if we either use a more accurate approximation of the PageRank vectors (smaller ϵ) or a larger number of top-k neighbors. This also shows that we can smoothly trade-off performance for scalability since models with higher value of k and lower value of ϵ are computationally more expensive. For example, in the academic setting (Fig. <ref type="figure" target="#fig_2">2b</ref>) a model with ϵ = 0.1, k = 2 had an overall (preprocessing + training + inference) runtime of 6 minutes, while a model with ϵ = 0.001, k = 256 had an overall runtime of 12 minutes. Since many nodes are labeled (1 %) the difference between the highest accuracy (top right corner) and lowest accuracy (bottom left corner) is under 2 % and the model is not sensitive to the hyperparameters. In the sparsely labeled setting (Fig. <ref type="figure" target="#fig_2">2a</ref>) the choice of hyperparameters is more important and depends on the desired trade-off level (slowest overall runtime was &lt;2 minutes).</p><p>Interestingly, we can see on Fig. <ref type="figure" target="#fig_2">2</ref> that for any value of ϵ the performance starts to plateau at around top-k = 32. The reason for this behavior becomes more clear by examining Fig. <ref type="figure" target="#fig_4">3</ref>. Here, for each node i we calculate the sum of the top-k largest scores in π (ϵ ) (i) and we plot the average across all nodes. We see that by looking at a very few nodes -e.g. 32 out of 12.4 million -we are able to capture the majority of the PageRank scores on average (recall that j π (ϵ ) (i) j ≤ 1). Therefore, the curves in both Fig. <ref type="figure" target="#fig_2">2</ref>     The 95% confidence intervals around the mean (estimated with bootstrapping) are too small to be visible.</p><p>Fig. <ref type="figure" target="#fig_4">3</ref> plateau around the same value of k. These figures validate our approach of approximating the dense (but localized) personalized PageRank vectors with their respective sparse top-k versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Distributed Training</head><p>In this section we aim to compare the performance of one-hop propagation using personalized PageRank and traditional multihop message passing propagation in a real distributed environment at Google. To make sure that the differences we observe are only due to the used model and not other factors, we implement simple 2-hop and 3-hop GNN models <ref type="bibr" target="#b31">[32]</ref>, which are also trained in a distributed manner using the same infrastructure as PPRGo. Specifically, we make sure that both the multi-hop models and PPRGo consider the same number of neighbors, e.g. if PPRGo uses k = 64 then the 2-hop model uses information from 8 × 8 = 64 nodes from its first and second hop respectively. To select these neighborhoods we use a weighted sampling scheme similar to previous work <ref type="bibr" target="#b53">[54]</ref>.</p><p>Additionally, we implement a distributed version of FastGCN <ref type="bibr" target="#b12">[13]</ref> to evaluate the effect of different sampling schemes. Our first observation is that there is no significant difference in terms of predictive performance between the different models (around 61% accuracy). However, there is a significant difference in terms of runtime. On Fig. <ref type="figure" target="#fig_6">4</ref> we show the speedup in terms of number of gradient-update steps per second on the MAG-Scholar-F graph as we increase the number of worker machines used for distributed training. Specifically, we show the relative speedup compared to the baseline method -2-hop GCN on a single worker. We see that PPRGo is considerably faster than the baseline (note that both axes are on a log-scale). PPRGo also requires fewer steps in total to converge. Moreover, the speedup gap between the 2 hop model and PPRGo increases with the number of workers. Crucially, since we have to fetch all neighbors to calculate their importance scores and since the runtime in the distributed setting is dominated by IO we see that FastGCN does not offer any significant scalability advantage over the baseline GNN 2-hop model. In § A.1 we additionally analyze parallel efficiency, i.e. how well the different models utilize the additional workers.</p><p>PPRGo is able to process all top-k neighbors at once, compared to the multi-hop models which have to recursively update the hidden representations. Therefore, while increasing the number of top-k neighbors makes all model computationally more expensive, we expect the runtime of PPRGo to increase the least. To validate this claim, we analyze the relative speed (number of gradient updates  Both axes are on a log scale. PPRGo is consistently the fastest method and can best utilize additional workers.  per second) compared to the slowest method at different values of k. The results in Fig. <ref type="figure" target="#fig_7">5</ref> exactly match our intuition, and indeed the curve of PPRGo has the smallest slope as we increase k, while the relative speed of the 2-and 3-hop GNNs deteriorate faster. FastGCN again matches GNN 2-hop, like it does in Fig. <ref type="figure" target="#fig_6">4</ref> (not shown here).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Runtime and Memory on a Single Machine</head><p>Setup. To highlight the benefits of PPRGo we compare the runtime, memory, and predictive performance with SGC <ref type="bibr" target="#b48">[49]</ref> and Cluster-GCN <ref type="bibr" target="#b15">[16]</ref>, two strong baselines that represent the current state-ofthe-art scalable GNNs. Since SGC and Cluster-GCN report significant speedup over FastGCN <ref type="bibr" target="#b12">[13]</ref> and VRGCN <ref type="bibr" target="#b13">[14]</ref> we omit these models from our comparison. We run the experiments on Nvidia 1080Ti GPUs and on Intel CPUs (5 cores), using CUDA and TensorFlow. We run each experiment on five different random splits and report mean values and standard deviation. For SGC we use the second power of the graph Laplacian as suggested by the authors (i.e. we effectively have a 2-hop model). For PPRGo we set ϵ = 10 −4 and k = 32 following the discussion in § 5.2. We compute the overall runtime including the preprocessing time, the time to train the models, and the time to perform inference for all test nodes. This is in contrast to previous work which rarely report preprocessing time and almost never report inference time. For training, we report both the overall training time, as well as the time per epoch. Preprocessing time. For each model, during preprocessing we perform the computation only for the training nodes. For SGC, the preprocessing step involves computing the diffused features using the second power of the graph Laplacian. We significantly optimized preprocessing for Cluster-GCN, resulting in node cluster computation with METIS <ref type="bibr" target="#b29">[30]</ref> becoming its main bottleneck. For PPRGo, the preprocessing step involves computing the approximate personalized PageRank vectors using Algorithm 1 and selecting the top k neighbors. Inference time. For SGC, during inference we have to compute the diffused features for the test nodes (again using the second power of the graph Laplacian). Following the implementation by the original authors of Cluster-GCN, we do not cluster the test nodes, but rather perform "standard" message-passing inference on the full graph. For PPRGo, as discussed in § 4.3, we run power iteration rather than computing the approximate PPR vectors for the test nodes. Two iteration steps were already sufficient to obtain good accuracy. We analyze the inference step in more detail in § 5.5.</p><p>The results when training a model on the Reddit dataset (233K nodes, 11.6M edges, 602 node features) are summarized in Table <ref type="table" target="#tab_1">1</ref>. Both SGC and Cluster-GCN are several orders of magnitude slower than PPRGo . Interestingly, SGC is significantly slower w.r.t. inference time (since we have to compute the diffused features for all test nodes) while Cluster-GCN is significantly slower w.r.t. preprocessing and training time. The overall runtime of Cluster-GCN (2310 s) and SGC (7470 s) is in stark contrast to our proposed approach: under 20 s. Moreover, we see that the amount of memory used by PPRGo is 4 times smaller compared to Cluster-GCN and 2 times smaller compared to SGC. Given that Cluster-GCN and SGC achieve significantly worse accuracy, the benefits of our proposed approach in terms of scalability are apparent.</p><p>We extend the above analysis to several other datasets. We chose two comparatively small academic graphs that are commonly used as benchmark datasets -Cora-Full <ref type="bibr" target="#b6">[7]</ref> (18.7K nodes, 62.4K edges, 8.7K node features) and PubMed <ref type="bibr" target="#b52">[53]</ref> (19.7K nodes, 44.3K edges, 0.5K node features) -as well as our newly introduced MAG-Scholar-C dataset (10.5M nodes, 133M edges, 2.8M node features). In addition to the two scalable baselines, we also evaluate how PPRGo compares to the APPNP model <ref type="bibr" target="#b32">[33]</ref> which we build upon. The results are summarized in Table <ref type="table" target="#tab_2">2</ref>. We can see that the performance of most models is comparable in terms of accuracy. In most cases our proposed model PPRGo has the smallest overall runtime and it always uses the least amount of memory. PPRGo's comparatively long runtime on Cora-Full can be explained by its training set size:</p><p>The training set is so large that PPRGo accesses more neighbors per batch than there are nodes in this graph, not leveraging the duplicate information. This can only happen with small graphs, for which runtime is not an issue. We see that the APPNP model runs out of memory for even the moderately sized Reddit graph, highlighting the necessity of our approach. More importantly, on the largest graph MAG-Scholar-C, we successfully trained PPRGo from scratch and obtained the predictions for all test nodes in under 2 minutes, while Cluster-GCN and SGC were not able to finish in over 24 hours, with Cluster-GCN still stuck in preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Efficient Inference</head><p>Inference time is crucial for real-world applications since a machine learning model needs to be trained only once, while inference is run continuously when the model is put into production. We found that PPRGo can achieve an accuracy of 68.7 % with a single power iteration step, i.e. without even calculating the PPR vectors. At this point, the neural network f θ and not the propagation becomes the limiting factor. However, as described in § 4.3, we can reduce the neural network cost by only computing logits for a small, random subset of nodes. Fig. <ref type="figure" target="#fig_8">6</ref> shows that the accuracy only reduces by around 0.6 percentage points when reducing the number of inferred nodes by a factor of 10. We can therefore trade in a small amount of accuracy to significantly reduce inference time, in this case by 50 %. With this approximation, PPRGo has a shorter inference time than the forward pass of a simple neural network executed on each node independently. Furthermore, note that we use a rather simple feed-forward neural network in our experiments. This reduction will become even more dramatic in cases that leverage more computationally expensive neural networks for f θ . Fig. <ref type="figure" target="#fig_10">7</ref> shows that when reducing the fraction of inferred nodes, the corresponding accuracy drops off earlier if we perform fewer power iteration steps p. Therefore, we need to increase the number of power iteration steps when we calculate fewer logits. This furthermore shows that subsampling logits would not be possible with methods that use locally sampled subgraphs (e.g. FastGCN). Note that we do not use this additional improvement in Table <ref type="table" target="#tab_2">1 and 2</ref></p><formula xml:id="formula_8">.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose a GNN for semi-supervised node classification that scales easily to graphs with millions of nodes. In comparison to previous work our model does not rely on expensive message-passing, making it well suited for use in large-scale distributed environments. We can trade scalability and performance via a few intuitive hyperparameters. To stimulate the development of scalable GNNs we   present MAG-Scholar -a new large-scale graph (12.4M nodes, 173M edges, and 2.8M node features) with coarse/fine-grained "groundtruth" node labels. On this web-scale dataset PPRGo achieves high performance in under 2 minutes (preprocessing + training + inference time) on a single machine. Beyond the single machine scenario, we demonstrate the scalability of PPRGo in a distributed setting and show that it is more efficient compared to multi-hop models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An illustration of PPRGo. For each node i we precompute an approximation of its personalized PageRank vector π (ϵ ) (i). The approximation is computed efficiently and in parallel using a distributed batch data processing pipeline. The final prediction z i is then generated as a weighted average of the local (per-node) representations H j,: = f θ (x j ) for the top k nodes ordered by largest personalized PageRank score π (i) j . To train the model f θ (•) that maps node attributes x i to local representations H i , we only need the personalized PageRank vectors of the training nodes and attributes of the their respective top k neighbors. The model is trained in a distributed manner on multiple batches of data in parallel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mean accuracy (%) over 5 runs on MAG-Scholar-C as we vary the number of neighbors and the approx. parameter ϵ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: For each node in MAG-Scholar-C we calculate the sum of the top-k largest scores in π (ϵ ) (i) and we plot the average across all nodes for different values of ϵ. The dashed line indicates k = n, i.e. the entire sum of π (ϵ ) (i) averaged across nodes. The 95% confidence intervals around the mean (estimated with bootstrapping) are too small to be visible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Relative speed in terms of number of gradientupdate steps per second on the MAG-Scholar-F graph compared to the baseline method (GNN 2-hop, single worker).Both axes are on a log scale. PPRGo is consistently the fastest method and can best utilize additional workers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Relative speed comparison (num. gradient updates per second) between PPRGo and multi-hop models for different values of k on MAG-Scholar-F. Distributed training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Accuracy and corresponding inference time (NN inference (dark blue) + propagation (light blue)) on MAG-Scholar-C w.r.t. the fraction of nodes for which local logits H are inferred by the NN.PPRGo performs very well even if the NN is evaluated on very few nodes. We need more power iteration steps p if we do fewer forward passes (see Fig.7), increasing the propagation time. Note the logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Accuracy on MAG-Scholar-C w.r.t. the fraction of nodes for which local logits H are inferred and number of power iteration steps p. The fewer logits we calculate, the more power iteration steps we need for stabilizing the prediction. Note the logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Breakdown of the runtime, memory, and predictive performance on a single machine for different models on the Reddit dataset. We use 820 (20 • #classes) nodes for training. We see that PPRGo has a total runtime of less than 20 s and is two orders of magnitude faster than SGC and Cluster-GCN. PPRGo also requires less memory overall.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Runtime (s)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Memory (GB)</cell><cell>Accuracy (%)</cell></row><row><cell></cell><cell cols="2">Preprocessing</cell><cell cols="2">Training</cell><cell></cell><cell>Inference</cell><cell></cell><cell>Total</cell><cell>RAM</cell><cell>GPU</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Per Epoch Overall Forward Propagation</cell><cell>Overall</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cluster-GCN</cell><cell></cell><cell>1175(25)</cell><cell>4.77(12)</cell><cell>953(24)</cell><cell>-</cell><cell>-</cell><cell>186(21)</cell><cell cols="3">2310(40) 20.97(15) 0.071(6)</cell><cell>17.1(8)</cell></row><row><cell>SGC</cell><cell></cell><cell>313(9)</cell><cell>0.0026(2)</cell><cell>0.53(3)</cell><cell>-</cell><cell>-</cell><cell cols="3">7470(150) 7780(150) 10.12(3)</cell><cell>0.027</cell><cell>12.1(1)</cell></row><row><cell cols="2">PPRGo (1 PI step)</cell><cell>2.26(4)</cell><cell cols="3">0.0233(5) 4.67(10) 0.341(9)</cell><cell>5.85(3)</cell><cell>6.19(4)</cell><cell cols="2">13.10(7) 5.560(19)</cell><cell>0.073</cell><cell>26.5(19)</cell></row><row><cell cols="2">PPRGo (2 PI steps)</cell><cell>2.22(12)</cell><cell>0.021(3)</cell><cell>4.1(7)</cell><cell>0.43(8)</cell><cell>10.1(14)</cell><cell>10.5(15)</cell><cell>16.8(17)</cell><cell>5.42(18)</cell><cell>0.073</cell><cell>26.6(18)</cell></row><row><cell></cell><cell>10 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relative Speed</cell><cell>10 0 10 1</cell><cell></cell><cell cols="2">Model PPRGo GNN 2-hop GNN 3-hop</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Number of neighbors k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Single machine runtime (s), memory (GB), and accuracy (%) for different models and datasets using 20 • #classes training nodes. PPRGo shows comparable accuracy and scales much better to large datasets than its competitors.</figDesc><table><row><cell></cell><cell></cell><cell>Cora-Full</cell><cell></cell><cell></cell><cell>PubMed</cell><cell></cell><cell></cell><cell>Reddit</cell><cell></cell><cell cols="2">MAG-Scholar-C</cell></row><row><cell></cell><cell>Time</cell><cell>Mem.</cell><cell>Acc.</cell><cell>Time</cell><cell>Mem.</cell><cell>Acc.</cell><cell>Time</cell><cell>Mem.</cell><cell>Acc.</cell><cell>Time</cell><cell>Mem.</cell><cell>Acc.</cell></row><row><cell>Cluster-GCN</cell><cell cols="4">84(4) 2.435(18) 58.0(7) 54.3(27)</cell><cell>1.90(3)</cell><cell cols="4">74.7(30) 2310(50) 21.04(15) 17.1(8)</cell><cell>&gt;24h</cell><cell>-</cell><cell>-</cell></row><row><cell>SGC</cell><cell>92(3)</cell><cell>3.95(3)</cell><cell>58.0(8)</cell><cell>5.3(3)</cell><cell cols="4">2.172(4) 75.7(23) 7780(140) 10.15(3)</cell><cell>12.1(1)</cell><cell>&gt;24h</cell><cell>-</cell><cell>-</cell></row><row><cell>APPNP</cell><cell cols="3">10.7(5) 2.150(19) 62.8(11)</cell><cell>6.5(4)</cell><cell cols="2">1.977(4) 76.9(26)</cell><cell>-</cell><cell>OOM</cell><cell>-</cell><cell>-</cell><cell>OOM</cell><cell>-</cell></row><row><cell cols="2">PPRGo (ϵ = 10 −4 , k = 32) 25(3)</cell><cell>1.73(3)</cell><cell>61.0(7)</cell><cell>3.8(9)</cell><cell cols="3">1.626(25) 75.2(33) 16.8(17)</cell><cell cols="5">5.49(18) 26.6(18) 98.6(17) 24.51(4) 69.3(31)</cell></row><row><cell cols="4">PPRGo (ϵ = 10 −2 , k = 32) 6.6(5) 1.644(13) 58.1(6)</cell><cell>2.9(5)</cell><cell cols="3">1.623(17) 73.7(39) 16.3(17)</cell><cell>5.61(6)</cell><cell>26.2(18)</cell><cell>89(5)</cell><cell cols="2">24.49(5) 63.4(29)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">For large graphs on distributed storage, just gathering the required neighborhood data requires many expensive remote procedure calls that greatly increase run time.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">The importance sampling score by Ying et al.<ref type="bibr" target="#b53">[54]</ref> can be seen as an approximation of the non-personalized PageRank, however the number of random walks required to achieve a good approximation is relatively high<ref type="bibr" target="#b18">[19]</ref> making it a suboptimal choice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">The Twitter geo-location datasets used in previous work<ref type="bibr" target="#b48">[49]</ref> have limited usefulness for evaluating GNNs since they have no meaningful graph structure, e.g. 70% of the nodes in the Twitter-World dataset only have a self-loop and no other edges.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">While larger benchmark graphs can be found in the literature, they either do not have node features or they do not have "ground-truth" node labels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENTS</head><p>We would like to thank Chandan Yeshwanth for his assistance with conducting the experiments. This research was supported by the Deutsche Forschungsgemeinschaft (DFG) through the Emmy Noether grant GU 1409/2-1 and the TUM International Graduate School of Science and Engineering (IGSSE), GSC 81.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To further investigate the performance of different models in the distributed training setting we also evaluate parallel efficiency. Intuitively, this efficiency measures how well we can utilize additional workers. Let m t be the number of steps per second using t workers, then the parallel efficiency of a model is defined as m t m 1 •t . In Fig. <ref type="figure">8</ref> we see that PPRGo achieves the best parallel efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MAG-Scholar Graph Construction</head><p>First, we obtain the "raw" data from the Microsoft Academic Graph (MAG) <ref type="bibr" target="#b42">[43]</ref> repository, specifically we downloaded a snapshot of the data on 01.25.2019. We construct a graph where each node is a paper and the edges indicate citations/references between the papers. The node features are a bag-of-words representation of the paper's abstract. We preprocess the feature matrix by keeping only those words that appear in at least 5 abstracts. We preprocess the graph by keeping only the nodes that belong to the largest connected component. The resulting MAG-Scholar-F graph has 12.40393 million nodes, 2.78424 million features, and 173.050172 million edges. The MAG-Scholar-C graph has 10.54156 million nodes, 2.78424 million features, and 132.817644 million edges. To obtain the fields of study for each paper, we first create a mapping between a venue (i.e. conference or journal) and its respective field of study. Specifically, we consider the top-20 venues in each field of study according to Google Scholar 6 . We manually match the same venues that have different tittles (e.g. because of abbreviations) in the MAG data compared to the Google Scholar data. These venues are categorized in 8 different coarse-grained categories (e.g. engeneering 7 ) and 253 different fine-grained categories (e.g. biophysics 8 ) and we use them to define coarse/fine-grained "ground-truth" labels for the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Experimental Details</head><p>We keep all PPRGo hyperparameters constant across all datasets, except the value of the teleport parameter α = 0.25, which we set to α = 0.5 for reddit. The feed-forward neural network has two layers, 6 https://scholar.google.com/citations?view_op=top_venues&amp;hl=en 7 https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng 8 https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=phy_ biophysics i.e. a single hidden layer of size 32. We use a dropout of 0.1 and set the weight decay to 10 −4 . We train for 200 epochs using a learning rate of 0.005 and the Adam optimizer <ref type="bibr" target="#b30">[31]</ref> with a batch size of 512. To achieve a consistent setup across all models and datasets we always use the same number of epochs, use no early stopping and only evaluate validation accuracy after training. For the validation set we randomly sample 10 times the number of training nodes.</p><p>We standardize the graphs as a preprocessing step, i.e. we choose only the subset of nodes belonging to the largest connected component and make the graph undirected and unweighted. We do not include dataset loading time in the overall runtime since it is the same for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Further Implementational Details</head><p>The pseudo code in Algorithm 1 shows how we compute the approximate personalized PageRank based on <ref type="bibr" target="#b4">[5]</ref>. For single-machine experiments we implement the algorithm as described in Python using Numba for acceleration (not parallelized). In the distributed setting instead of carrying out push-flow iterations until convergence, we perform a fixed number of iterations (i.e. we replace the while with a for loop), and drop nodes whose residual score is below a specified threshold in each iteration. Additionally, we truncate nodes with a very large degree (≥ 10000) by randomly sampling their neighbors. The above modifications proved to be just as effective as Andersen et al. <ref type="bibr" target="#b4">[5]</ref>'s method while being significantly faster in terms of runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Applicability and Limitations</head><p>When using PPRGo for your own purposes you should first be aware that this model assumes a homophilic graph, which is mostly, but not always the case. Furthermore, it cannot perform arbitrary message passing schemes like GNNs do, since we essentially compress the message passing into a single step. It therefore has less theoretical expressiveness than GNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b50">51]</ref>, even if it practically shows the same or better accuracy. However, note that PPRGo allows arbitrary realizations of f θ and can therefore be used with more complex data and models such as images and CNNs, audio and LSTMs, or text and Transformers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">N-gcn: Multi-scale graph convolution for semi-supervised node classification</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08888</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICML. 21-29</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ddgk: Learning graph representations for deep divergence graph kernels</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Local Computation of PageRank Contributions</title>
		<author>
			<persName><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">T</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Mirrokni</surname></persName>
		</author>
		<author>
			<persName><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="23" to="45" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph fibrations, graph isomorphism, and PageRank</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Violetta</forename><surname>Lonati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastiano</forename><surname>Vigna</surname></persName>
		</author>
		<idno type="DOI">10.1051/ita:2006004</idno>
		<ptr target="https://doi.org/10.1051/ita:2006004" />
	</analytic>
	<monogr>
		<title level="j">RAIRO Theor. Informatics Appl</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="253" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrapped graph diffusions: Exposing the power of nonlinearity</title>
		<author>
			<persName><forename type="first">Eliav</forename><surname>Buchnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edith</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FlumeJava: easy, efficient data-parallel pipelines</title>
		<author>
			<persName><forename type="first">Craig</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Raniwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frances</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Weizenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="363" to="375" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine Learning on Graphs: A Model and Comprehensive Taxonomy</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">FastGCN: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic Training of Graph Convolutional Networks with Variance Reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Supervised Community Detection with Line Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<ptr target="http://arxiv.org/abs/1903.02428" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Towards Scaling Fully Personalized PageRank</title>
		<author>
			<persName><forename type="first">Dániel</forename><surname>Fogaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Rácz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In WAW</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and Exact Top-k Algorithm for PageRank</title>
		<author>
			<persName><forename type="first">Yasuhiro</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Nakatsuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Shiokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Mishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Onizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Kyle</forename><surname>David F Gleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huda</forename><surname>Kloster</surname></persName>
		</author>
		<author>
			<persName><surname>Nassar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00016</idno>
		<title level="m">Localization in Seeded PageRank</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<idno>IJCNN. 729-734</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive Sampling Towards Fast Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4563" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Scaling personalized web search</title>
		<author>
			<persName><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Smart reply: Automated response suggestion for email</title>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balint</forename><surname>Miklos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laszlo</forename><surname>Lukacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="955" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gãĳnnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gL-2A9Ym" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07606</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Personalized PageRank Estimation and Search: A Bidirectional Approach</title>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Peter Lofgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Strong localization in personalized PageRank vectors</title>
		<author>
			<persName><forename type="first">Huda</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kloster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Algorithms and Models for the Web-Graph</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="190" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2016. 2014-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">When Recommendation Goes Wrong: Anomalous Link Discovery in Recommendation Networks</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schueppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Saalweachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayur</forename><surname>Thakur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="569" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<ptr target="https://ai.googleblog.com/2016/10/graph-powered-machine-learning-at-google.html" />
		<title level="m">Graph-powered Machine Learning at Google</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07868</idno>
		<title level="m">Constant Time Graph Neural Networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">An Overview of Microsoft Academic Service (MAS) and Applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">HubPPR: Effective Indexing for Approximate Personalized PageRank</title>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youze</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengxiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="205" to="216" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">FORA: Simple and Effective Approximate Single-Source Personalized PageRank</title>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renchi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dirichlet PageRank</title>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azadeh</forename><surname>Shakery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">TopPPR: Top-k Personalized PageRank Queries with Precision Guarantees on Large Graphs</title>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying Graph Convolutional Networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How Powerful are Graph Neural Networks?</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation Learning on Graphs with Jumping Knowledge Networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Defining and evaluating network communities based on ground-truth</title>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="181" to="213" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01973</idno>
		<title level="m">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09691</idno>
		<title level="m">Link Prediction Based on Graph Neural Networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
