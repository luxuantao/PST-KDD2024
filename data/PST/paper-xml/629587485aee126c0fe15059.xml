<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinya</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
							<email>zhouhang@link.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
							<email>kaisiyuan.wang@sydney.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
							<email>qianyi.wu@monash.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
							<email>caoxun@nju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Sydney Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Monash University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">SenseTime Research Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">BNRist and school of software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<address>
									<addrLine>10 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3528233.3530745</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Facial Animation, Neural Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although significant progress has been made to audio-driven talking face generation, existing methods either neglect facial emotion or cannot be applied to arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model (EAMM) to generate oneshot emotional talking faces by involving an emotion source video. Specifically, we first propose an Audio2Facial-Dynamics module, which renders talking faces from audio-driven unsupervised zeroand first-order key-points motion. Then through exploring the motion model's properties, we further propose an Implicit Emotion Displacement Learner to represent emotion-related facial dynamics as linearly additive displacements to the previously acquired</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The task of audio-driven talking face animation enables various applications ranging from visual dubbing, digital avatars, teleconferencing to mixed reality. While extensive progress has been made in this area <ref type="bibr" target="#b17">[Fried et al. 2019;</ref><ref type="bibr" target="#b46">Thies et al. 2020;</ref><ref type="bibr">Zhou et al. 2021</ref><ref type="bibr" target="#b65">Zhou et al. , 2020]]</ref>, many of them rely on long video recordings of a source portrait <ref type="bibr" target="#b16">[Edwards et al. 2016;</ref><ref type="bibr" target="#b26">Karras et al. 2017;</ref><ref type="bibr" target="#b59">Yao et al. 2021</ref>] to generate facial expressions, which are not available in most scenarios. On the other hand, methods driving only one frame <ref type="bibr" target="#b12">[Chung et al. 2017;</ref><ref type="bibr" target="#b38">Mittal and Wang 2020;</ref><ref type="bibr" target="#b63">Zhou et al. 2019</ref>] merely focus on synthesizing audio-synchronized mouth shapes without considering emotion, the key factor for realistic animation. Thus how to enable expressive emotional editing under the one-shot talking face setting remains an open problem.</p><p>Previous methods either identify emotion from a fixed number of labels <ref type="bibr" target="#b0">[Abdrashitov et al. 2020;</ref><ref type="bibr" target="#b31">Li et al. 2021;</ref><ref type="bibr" target="#b51">Wang et al. 2020b]</ref> or only a small range of labeled audio data <ref type="bibr" target="#b24">[Ji et al. 2021</ref>]. However, fixed labels can only represent limited emotions in a coarse-grained discrete manner, making it hard to achieve natural emotion transitions. Additionally, determining emotions from audio only may lead to ambiguities. People sometimes fail to perceive the varying emotions hidden in the speech and the performance of emotion recognition models is not satisfying for general speech. Thus both of them limit the applicability of an emotional talking face model. Differently, we argue that the dynamic emotion can be formulated into a transferable motion pattern extracted from an additional emotional video.</p><p>Therefore, our goal is to devise a one-shot talking-face system that takes four kinds of inputs, including an identity source image with neutral expression, a speech source audio, a pre-defined pose and an emotion source video. However, achieving such a system is not trivial. 1) Generating emotional information requires deforming the non-rigid facial structures, which are implicitly but strongly coupled with identity and mouth movements. Previous methods usually adopt strong prior of human faces, such as landmarks <ref type="bibr" target="#b27">[Kim et al. 2019;</ref><ref type="bibr" target="#b51">Wang et al. 2020b</ref>] and 3D models <ref type="bibr" target="#b1">[Anderson et al. 2013;</ref><ref type="bibr" target="#b41">Richard et al. 2021</ref>]. Nevertheless, these methods suffer from error 1 All materials are available at https://jixinya.github.io/projects/EAMM/. accumulation caused by model inaccuracy.</p><p>2) The extraction of emotion patterns is also challenging due to their entanglement with other factors.</p><p>To cope with such issues, in this paper, we present a novel approach named Emotion-Aware Motion Model (EAMM). Our intuition is that unsupervised zero-and first-order motion representations <ref type="bibr">[Siarohin et al. 2019a,b;</ref><ref type="bibr" target="#b55">Wang et al. 2021b</ref>] are capable of modeling local flow fields on faces, which is suitable for manipulating emotion deformations. The key is to transfer the local emotional deformations to an audio-driven talking face with selflearned key-points and local affine transformations. Specifically, we firstly achieve talking face generation from a single image through a simple Audio2Facial-Dynamics (A2FD) module. It maps audio representations and extracted poses to unsupervised key points and their first-order dynamics. An additional flow estimator and a generator then cope with the representations for image reconstruction.</p><p>In order to further decompose the local emotion dynamics from appearances, we perform empirical explorations for the motion model's intrinsic working mechanisms. Two interesting properties are identified. 1) The dynamic movements in the facial region are only affected by specific key-points and affine transforms, which are denoted as face-related representations. 2) The relative displacements of the face-related representations are generally linear-additive. However, the face-related displacements also contain undesirable mouth movements and structural deformation, making them not directly applicable to our current model.</p><p>To this end, we design an Implicit Emotion Displacement Learner to learn only emotion-related displacements on the A2FD module's face-related representations. Particularly, we leverage an effective augmentation strategy on emotion sources to alleviate the influence of undesired factors. Then we derive an emotion-feature conditioned implicit function that maps the whole set of motion representations in the A2FD module to the expected face-related representations' displacements. By linearly combining all motion representations from the two modules, our model complementarily covers both the mouth shapes and emotional dynamics. Extensive experiments demonstrate that our method can generate satisfactory talking face results on arbitrary subjects with realistic emotion patterns.</p><p>Our contributions are summarized as follows: 1) We propose the Audio2Facial-Dynamics module, which generates neutral audiodriven talking faces by predicting unsupervised motion representations in a simple manner. 2) Based on two empirical observations, we propose the Implicit Emotion Displacement Learner that can extract the face-related representations' displacements from emotion sources. 3) Our proposed Emotion-Aware Motion Model (EAMM) manages to generate one-shot talking head animations with emotion control. To the best of our knowledge, it is one of the earliest attempts in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Audio-Driven Talking Face Generation. It is a task which aims to generate talking-face videos from audio clips <ref type="bibr" target="#b3">[Brand 1999;</ref><ref type="bibr">Bregler et al. 1997a,b;</ref><ref type="bibr" target="#b34">Lu et al. 2021;</ref><ref type="bibr" target="#b52">Wang et al. 2012;</ref><ref type="bibr" target="#b67">Zhou et al. 2018</ref>]. These methods can be mainly classified into person-specific or person-agnostic methods. Though person-specific methods produce better animation results, their application scenarios are limited. Their training time required for modeling one person may cost several hours <ref type="bibr" target="#b45">[Suwajanakorn et al. 2017]</ref> or few minutes <ref type="bibr" target="#b34">[Lu et al. 2021;</ref><ref type="bibr" target="#b46">Thies et al. 2020</ref>]. <ref type="bibr" target="#b45">Suwajanakorn et al. [2017]</ref> synthesize highquality Obama talking faces from his voice track by using 17-hour videos for training. <ref type="bibr" target="#b46">Thies et al. [2020]</ref> and <ref type="bibr" target="#b34">Lu et al. [2021]</ref> propose to generate photo-realistic talking videos with about a 3-minute length person-specific video for training. But they cannot be applied to one image. On the other hand, <ref type="bibr" target="#b12">Chung et al. [2017]</ref> generate talking faces in a one-shot manner for the first time. Later, <ref type="bibr" target="#b9">Chen et al. [2019a]</ref> and <ref type="bibr" target="#b65">Zhou et al. [2020]</ref> improve the schedule by leveraging facial landmarks as intermediate representations. <ref type="bibr">Zhou et al. [2021]</ref> further involve pose control into the one-shot setting, but none of these works achieves emotional control.</p><p>Emotional Talking Face generation. Emotion <ref type="bibr" target="#b15">[Cole et al. 2017</ref>] is a factor that plays a strong role in realistic animation. Only a few works consider it in talking face generation due to the difficulty of producing emotion dynamics. <ref type="bibr">Sadoughi et al. [2019]</ref> learn the relationship between emotion and lip movements from a designed conditional generative adversarial network. <ref type="bibr" target="#b50">Vougioukas et al. [2020]</ref> introduce three discriminators for a temporal GAN. However, both of them fail to generate semantic expressions and achieve emotion manipulation. Recently, <ref type="bibr" target="#b51">Wang et al. [2020b]</ref> collect the MEAD dataset and set emotions as one-hot vectors to achieve emotion control. While <ref type="bibr" target="#b24">Ji et al. [2021]</ref> propose to decompose speech into decoupled content and emotion spaces, and then synthesize emotion dynamics from audio. Nevertheless, their methods cannot be applied to unseen characters and audios. Different from them, we resort to a source video and disentangle the emotion information to achieve emotion control in the one-shot setting.</p><p>Video-driven Facial Animation. Video-driven animation leverages a video to reenact facial motion, which is highly related to audio-driven talking-face generation. Traditional approaches demand prior knowledge or manual labels of the animated target such as 3D morphable model <ref type="bibr" target="#b28">[Kim et al. 2018;</ref><ref type="bibr" target="#b47">Thies et al. 2016;</ref><ref type="bibr" target="#b68">ZollhÃ¶fer et al. 2018</ref>] or 2D landmark <ref type="bibr" target="#b6">[Burkov et al. 2020;</ref><ref type="bibr" target="#b11">Chen et al. 2020;</ref><ref type="bibr" target="#b21">Huang et al. 2020;</ref><ref type="bibr" target="#b23">Isola et al. 2017;</ref><ref type="bibr" target="#b48">Tripathy et al. 2021;</ref><ref type="bibr" target="#b57">Wu et al. 2018;</ref><ref type="bibr" target="#b58">Yao et al. 2020;</ref><ref type="bibr" target="#b61">Zakharov et al. 2020;</ref><ref type="bibr" target="#b62">Zhang et al. 2020]</ref>. Recently, a few methods <ref type="bibr">[Siarohin et al. 2019a,b]</ref> that do not require priors have been explored. They employ a self-supervised framework and model the motion in a dense field, in which case appearance and motion are decoupled. Our model is built upon a similar idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>The overview of our Emotion-Aware Motion Model (EAMM) is shown in Figure <ref type="figure" target="#fig_0">2</ref>, where different kinds of signals are taken as the inputs to generate emotional talking faces. Our EAMM mainly consists of two parts, an Audio2Facial-Dynamics module that achieves audio-driven talking face generation with neutral expressions from one neutral frame (Section 3.1) and an Implicit Emotion Displacement Learner that involves emotional dynamics (Section 3.2). In the following sections, we introduce each part in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Audio2Facial-Dynamics Module</head><p>The first step toward audio-driven emotional talking face is to build a one-shot system that is plausible for integrating expression dynamics. To this end, we design the Audio2Facial-Dynamics (A2FD) module, which firstly models facial movements with neutral expressions. The motion is represented as a set of unsupervised key-points and their first order dynamics inspired by <ref type="bibr" target="#b44">[Siarohin et al. 2019b;</ref><ref type="bibr" target="#b53">Wang et al. 2021a</ref>]. Based on this motion representation, warping fields can be calculated to account for the local facial motions, and thus facilities further the generation of emotional talking faces.</p><p>Training Formulation. Since direct supervision is not available due to the lack of paired data, we adopt the self-supervised training strategy <ref type="bibr" target="#b10">[Chen et al. 2019b;</ref><ref type="bibr">Zhou et al. 2021]</ref>. For each training video clip V = {ğ‘° 1 , ...ğ‘° ğ‘¡ , ...ğ‘° ğ‘‡ }, we randomly select one frame ğ‘° as the identity source image, and take Mel Frequency Cepstral Coefficients (MFCC) <ref type="bibr" target="#b33">[Logan 2000</ref>] ğ’” 1:ğ‘‡ of the corresponding speech audio ğ’‚ as the speech source audio representations. Considering that head pose is also a key component, which can hardly be inferred from audios, we assign the pose sequence ğ’‘ 1:ğ‘‡ estimated from the training video clip by an off-the-shelf tool <ref type="bibr" target="#b19">[Guo et al. 2020</ref>] as additional inputs. A 6-dim vector (i.e., 3 for rotation, 2 for translation and 1 for scale) is used to represent head pose for each frame ğ’‘ ğ‘¡ . Note that in the testing stage, the identity image ğ‘° , the speech source audio clip ğ’‚ and the pose sequence ğ’‘ 1:ğ‘‡ can come from different sources.</p><p>Pipeline of A2FD. As illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, we first use three encoders (i.e., ğ‘¬ ğ¼ , ğ‘¬ ğ‘ and ğ‘¬ ğ‘ ) to extract the corresponding information from the three inputs, which are denoted as the identity feature f ğ¼ , the audio feature f ğ‘ and the pose feature f ğ‘ . Then we combine the three extracted features and feed them into a LSTM-based [Hochreiter and Schmidhuber 1997] decoder ğ‘« to recurrently predict the unsupervised motion representations for the whole sequence. The motion representations at each time step ğ‘¡ are composed of ğ‘ implicitly learned key-points ğ’™ ğ‘ ğ‘¡ âˆˆ R ğ‘ Ã—2 and their first order motion dynamics, i.e., jacobians ğ‘± ğ‘ ğ‘¡ âˆˆ R ğ‘ Ã—2Ã—2 , where each jacobian denotes the local affine transformation for the neighbourhood area at each key-point (zero-order representation) position. We set ğ‘ = 10 by default throughout the paper.</p><p>In order to derive the warping fields correlated with local dynamics, the standard-positioned zero-and first-order representations of the initial frame ğ‘° should be provided. Instead of learning all representations from scratch, we argue that our A2FD module would be easier to learn if we share the audio-involved key-point distribution with a pretrained video-driven first-order motion model's <ref type="bibr" target="#b44">[Siarohin et al. 2019b</ref>].</p><p>Thus we employ a pretrained key-point detector ğ‘¬ ğ‘˜ from [Siarohin et al. 2019b] to predict the initial motion representations ğ’™ ğ‘  and ğ‘± ğ‘  from the source image ğ‘° . Then we adopt a flow estimator ğ‘­ to generate a dense warping field that describes the non-linear transformation from the source image to the target video frame. Specifically, at each time step ğ‘¡, we first calculate ğ‘ warping flows as well as a set of masks M based on the predicted key-points ğ’™ ğ‘ ğ‘¡ , ğ’™ ğ‘  and the jacobians ğ‘± ğ‘ ğ‘¡ , ğ‘± ğ‘  . Then by weighted combining the masks M to the warping flows, we obtain the final dense warping field. Finally, we feed the dense warping field together with the source image ğ‘° into an image generator ğ‘® to produce the final output frame at each time step Ãğ‘¡ . Please refer to <ref type="bibr" target="#b44">[Siarohin et al. 2019b</ref>] for more details.</p><p>Training Objectives. As stated before, we would like to share the motion representation's distribution with the visual-based model, we leverage ğ‘¬ ğ‘˜ as a specific teacher network for our audio-based model learning. Specifically, the key-points ğ’™ ğ‘£ ğ‘¡ , and their jacobian ğ‘± ğ‘£ ğ‘¡ extracted by ğ‘¬ ğ‘˜ from the training video clip V are served as intermediate supervisions. Then we formulate a key-point loss term ğ¿ ğ‘˜ğ‘ defined below to train our A2FD module:</p><formula xml:id="formula_0">ğ¿ ğ‘˜ğ‘ = 1 ğ‘‡ ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 (âˆ¥ğ’™ ğ‘ ğ‘¡ âˆ’ ğ’™ ğ‘£ ğ‘¡ âˆ¥ 1 + âˆ¥ ğ‘± ğ‘ ğ‘¡ âˆ’ ğ‘± ğ‘£ ğ‘¡ âˆ¥ 1 ).<label>(1)</label></formula><p>In the second stage, we use a perceptual loss term ğ¿ ğ‘ğ‘’ğ‘Ÿ to fine-tune the model by minimizing the difference between the reconstructed frame Ãğ’• and the target frame ğ‘° ğ‘¡ :</p><formula xml:id="formula_1">ğ¿ ğ‘ğ‘’ğ‘Ÿ = ğ‘™ âˆ‘ï¸ ğ‘–=1 âˆ¥VGG ğ‘– ( Ãğ‘¡ âˆ’ VGG ğ‘– (ğ‘° ğ‘¡ ))âˆ¥ 1 ,<label>(2)</label></formula><p>where VGG ğ‘– (â€¢) is the ğ‘– ğ‘¡â„ channel feature of a pretrained VGG network <ref type="bibr" target="#b25">[Johnson et al. 2016</ref>] with ğ‘™ channels. The total loss function is defined as:</p><formula xml:id="formula_2">ğ¿ ğ‘šğ‘œ = ğ¿ ğ‘˜ğ‘ + ğœ† ğ‘ğ‘’ğ‘Ÿ ğ¿ ğ‘ğ‘’ğ‘Ÿ ,<label>(3)</label></formula><p>where ğœ† ğ‘ğ‘’ğ‘Ÿ represents the weight for ğ¿ ğ‘ğ‘’ğ‘Ÿ .</p><p>Discussion. After the generation of neutral talking faces with audio inputs, one straightforward idea is to directly incorporate the emotional source into this pipeline. However, an emotional source naturally contains all facial information including the mouth, identity and pose, leading to undesirable results. Thus this brings the need to decouple emotional information within our motion representations and warping field.</p><p>We start by exploring how the warping field transforms the source image ğ‘° based on the key-points ğ’™. We visualize the composition masks M shown in Figure <ref type="figure" target="#fig_1">3</ref> and observe that the face region is only affected by three face-related key-points. The set of representations with only three key-points are denoted as (ğ’™ â€² , ğ‘± â€² ).</p><p>Table <ref type="table">1</ref>: Quantitative comparisons with state-of-the-art methods. We show quantitative results on LRW <ref type="bibr" target="#b13">[Chung and Zisserman 2016a]</ref> and MEAD <ref type="bibr" target="#b51">[Wang et al. 2020b]</ref> datasets. The results of LRW are only generated by the Audio2Facial-Dynamics Module, as there is no emotion annotation in LRW. The metrics related to video quality and landmarks are calculated by comparing the generated results with the ground truth. Here M-denotes mouth and F-denotes face region. The signages "â†‘" and "â†“" indicate higher and lower metric values for better results, respectively. Inspired by this observation, we perform a simple experiment to validate whether we can transfer the emotion pattern from an emotion source video to our A2FD module by merely editing the three face-related key-points and their jacobians. One simple idea is to find whether the deviation between emotional and neutral motion representations of the same person can be linearly additive, i.e., to impose emotion by adding the displacements on other faces' motion representations. To alleviate the influence of the mouth, we leverage both the pretrained model that extracts full facial dynamics and our A2FD model that generates neutral talking faces. Ideally, their mouth shapes should be aligned within the representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><formula xml:id="formula_3">LRW [2016a] MEAD [2020b] SSIM â†‘ PSNR â†‘ SyncNet â†‘ M-LMD â†“ F-LMD â†“ SSIM â†‘ PSNR â†‘ SyncNet â†‘ M-LMD â†“ F-LMD â†“ ATVG [2019b] 0.</formula><p>Concretely, we first detect key-points ğ’™ ğ‘’ â€² and jacobians ğ‘± ğ‘’ â€² from an emotion source video with ğ‘¬ ğ‘˜ . Then we feed its audio and a neutral status image of this person into our A2FD module to generate ğ’™ ğ‘› â€² and ğ‘± ğ‘› â€² . We calculate the deviation (ğ’™ ğ‘’ â€² âˆ’ ğ’™ ğ‘› â€² , ğ‘± ğ‘’ â€² âˆ’ ğ‘± ğ‘› â€² ), which is assumed to include emotion information. By simply adding this deviation as displacements onto the motion representations of an arbitrary person, we observe that the motion dynamics can be successfully transferred on the generated results. Thus we can regard these representations as roughly linearly additive.</p><p>However, while the emotion information can be preserved, we observe that there are many undesirable artifacts around the face boundary and the mouth. A possible explanation is the calculated displacements include not only emotion information but also other factors, such as identity, pose and speech content, which results in inaccurate guidance for the subsequent generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implicit Emotion Displacement Learner</head><p>According to the observation above, we can basically formulate the emotion pattern as the complementary displacements to the facerelated key-points and jacobians. Therefore, we design an Implicit Emotion Displacement Learner to extract emotion information from the emotional video V ğ‘’ = {ğ‘¸ 1 , ...ğ‘¸ ğ‘¡ , ...ğ‘¸ ğ‘‡ } and then encode them as the displacements (Î”ğ’™ â€² , Î”ğ‘± â€² ) to the three face-related key-points and jacobians (ğ’™ â€² , ğ‘± â€² ) from the A2FD module.</p><p>Data Processing. To disentangle the emotion from other factors, we design a special data augmentation strategy. Specifically, to block the speech content information, we occlude the lip and jaw movements using a mask filled with random noise. In addition, to eliminate the effects of pose and natural movement like blinking, we introduce a temporal perturbation technique. For each time step ğ‘¡, instead of using the frame ğ‘¸ ğ‘¡ for emotion extraction, we select a frame from different time steps perturbed around the current time ğ‘¡. Moreover, to further alleviate the influence of facial structure information, we apply the perspective transformation and random horizontal flip <ref type="bibr">[Zhou et al. 2021</ref>]. This data augmentation strategy is also demonstrated in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>Learning Emotion Displacements. To incorporate the emotion pattern into our A2FD module, we first employ an emotion extractor ğ‘¬ ğ‘’ to extract the emotion feature f ğ‘’ from the processed video frames. For generating emotion dynamics synchronized with the input audio, we take the key-points ğ’™ ğ‘ 1:ğ‘‡ and their jacobians ğ‘± ğ‘ 1:ğ‘‡ predicted from the A2FD module together with f e as the inputs to our displacement predictor ğ‘· ğ‘‘ . It employs a 4-layer multiple layer perceptrons (MLP) to predict the displacements referred to as Î”ğ’™ ğ‘ â€² 1:ğ‘‡ and Î”ğ‘± ğ‘ â€² 1:ğ‘‡ . Note that a positional encoding operation <ref type="bibr" target="#b37">[Mildenhall et al. 2020</ref>] is performed to project the key-points into a high dimensional space, which enables the model to capture higher frequency details. Finally, we produce the ğ‘ emotional audio-learned keypoints ğ’™ ğ‘’ğ‘ 1:ğ‘‡ and jacobians ğ‘± ğ‘’ğ‘ 1:ğ‘‡ by linearly adding Î”ğ’™ ğ‘ â€² 1:ğ‘‡ and Î”ğ‘± ğ‘ â€² 1:ğ‘‡ onto audio-learned representations ğ’™ ğ‘ â€² 1:ğ‘‡ , ğ‘± ğ‘ â€² 1:ğ‘‡ .</p><p>Training Objectives. During training, we follow the self-supervised training strategy in Sec. 3.1. Specifically, for each emotion source video V ğ‘’ , we use the pretrained detector ğ‘¬ ğ‘˜ to extract the ğ‘ keypoints ğ’™ ğ‘’ 1:ğ‘‡ and jacobians ğ‘± ğ‘’ 1:ğ‘‡ as the ground-truth, and then we minimize the difference between the emotional audio-learned keypoints ğ’™ ğ‘’ğ‘ 1:ğ‘‡ , jacobians ğ‘± ğ‘’ğ‘ 1:ğ‘‡ and the ground-truth by reformulating the loss term ğ¿ ğ‘˜ğ‘ in Eq. 1 as:</p><formula xml:id="formula_4">ğ¿ ğ‘˜ğ‘ = 1 ğ‘‡ ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 (âˆ¥ğ’™ ğ‘’ğ‘ ğ‘¡ âˆ’ ğ’™ ğ‘’ ğ‘¡ âˆ¥ 1 + âˆ¥ ğ‘± ğ‘’ğ‘ ğ‘¡ âˆ’ ğ‘± ğ‘’ ğ‘¡ âˆ¥ 1 ). (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Note that we also use the loss ğ¿ ğ‘ğ‘’ğ‘Ÿ in Eq. 2 to finetune the A2FD module when training our implicit emotion displacement learner.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>All videos are aligned via centering (crop &amp; resize) the location of the first frame's face and resized to 256 Ã— 256. The videos are sampled at the rate of 25 FPS and the audios are pre-processed to 16kHz. For audio features, we compute 28-dim MFCC with the window size of 10 ms to produce a 28 Ã— 12 feature for each frame.</p><p>Dataset. We use the LRW <ref type="bibr" target="#b13">[Chung and Zisserman 2016a</ref>] dataset which has no emotion annotation to train our A2FD module. LRW is an in-the-wild audio-visual dataset collected from BBC news, including 1000 utterances of 500 different words, each of which lasts for about 1 second. Featuring a variety of speakers and head motions, it is well-suited for our training objectives. We split the train/test corpus following the setting of LRW.</p><p>Emotional dataset MEAD <ref type="bibr" target="#b51">[Wang et al. 2020b</ref>] is used to train our implicit emotion displacement learner. MEAD is a high-quality emotional talking-face dataset, including recorded videos of different actors speaking with 8 different emotions. Here, we select 34 actors for training and 6 actors for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>In the following, we present the comparison results with other state-of-the-art methods, the results of a user study and the design evaluation of our approach. Please see the supplementary for more details of the experiment settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation</head><p>We perform comparisons with state-of-the-art methods (i.e., ATVG <ref type="bibr" target="#b10">[Chen et al. 2019b</ref>], Speech-driven-animation <ref type="bibr" target="#b49">[Vougioukas et al. 2018]</ref>, <ref type="bibr">Wav2Lip [Prajwal et al. 2020]</ref>, MakeItTalk <ref type="bibr" target="#b65">[Zhou et al. 2020</ref>], PC-AVS <ref type="bibr">[Zhou et al. 2021]</ref>) on the test set of LRW and MEAD.</p><p>Evaluation Metrics. To evaluate the synchronization between the generated mouth shapes and the input audio, we adopt the metric landmarks distances on the mouth (M-LMD) <ref type="bibr" target="#b10">[Chen et al. 2019b</ref>] and the confidence score of SyncNet <ref type="bibr" target="#b14">[Chung and Zisserman 2016b</ref>]. Then we use the LMD on the whole face (F-LMD) to measure the accuracy of facial expression and poses. To evaluate the quality of the generated videos, we also introduce SSIM <ref type="bibr" target="#b56">[Wang et al. 2004</ref>] and PSNR as additional metrics.</p><p>Quantitative Results. The experiments are conducted in a selfdriving setting, in which we use the audio and detected pose sequence of each test video as the audio and pose source. Note that for the LRW dataset without emotion, we only use the A2FD Module to generate the results, where we randomly select a frame from each video in LRW as the source image. While for the MEAD dataset with emotions, the source image is randomly selected from a neutral video of the same speaker as in the test video. Moreover, instead of directly using the test video as the emotion source, we adopt a fair setting as in <ref type="bibr">[Zhou et al. 2021]</ref> for emotion source acquisition. We first align all the generated and real frames to the same size and then detect their facial landmarks for comparison. The comparison results are reported in Table <ref type="table">1</ref>. Our method achieves the highest score among all metrics on MEAD and most metrics on LRW. It's worth noting that Wav2Lip is trained with a SyncNet discriminator, thus it is natural to get the highest confidence score of SyncNet on LRW. Our results are comparable with the ground truth, which means the achievement of satisfactory audio-visual synchronization. As for the F-LMD that accounts for both pose and expressions, our method achieves comparable results with PC-AVS on LRW. The reason is that there are fewer emotional expression changes on LRW compared with MEAD, on which we achieve better results.</p><p>Qualitative Results. We also provide a qualitative comparison between our method and state-of-the-art methods in Figure <ref type="figure" target="#fig_3">4</ref>. Here we randomly select an emotional video in MEAD as the emotion source for our method. Our method can generate vivid emotional animation with natural head movements and accurate mouth shapes, while other methods cannot generate obvious emotional dynamics (see the red arrows). Concretely, only Wav2Lip and PC-AVS can generate mouth motions competitive with ours. However, Wav2Lip merely focuses on the synchronization between the speech audio and lip movements without considering the facial expression and head pose. Though PC-AVS is able to control the head poses, it neglects emotion dynamics for generating realistic animation. SDA can produce results with changing facial expressions, however, the generated expressions are always unstable which affects the identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">User Study</head><p>We conduct a user study to compare our method with real data and other state-of-the-art methods mentioned before. We recruit 20 participants with computer science background, in which 14 are male and 6 are female. The ages of the participants range from 21 to 25. We select 5 videos for each emotion category on the test set of MEAD as the emotion source videos. For each emotion source video, we randomly select the image and audio source from the test set of LRW and MEAD to generate 40 videos (5 Ã— 8 emotions) for each method. We also randomly select 40 real videos with corresponding emotions. Thus, each participant engages in 280 (7 Ã— 40 videos) trials and the videos are shown in a random order to reduce fatigue. We first show real labeled videos with eight different emotion categories to the participants for reference. Then, for each presented video clip, we design a two-stage procedure. In the first stage, the participants are required to evaluate the given video from three aspects (i.e., "lip synchronization", "naturalness of facial expression", and "video quality") and give a score from 1 (worst) to 5 (best) for each aspect.</p><p>Moreover, since there are specific emotion labels for source videos in MEAD, we conduct an emotion classification task to evaluate the generated emotion on our method in the second stage. Specifically, we show the same muted video and ask the participants to choose the emotion type of the video from eight categories. The videos shown in the second stage are muted so that the participants can only focus on the facial expressions. The generated video and emotion can be well evaluated in this way. Basically, it takes about 90 minutes for each participant to finish the experiment.</p><p>The results are shown in Table <ref type="table" target="#tab_1">2</ref>. Our work achieves the highest scores over the three aspects apart from the real data, which indicates the effectiveness of our method. What's more, we obtain 58% accuracy in emotion classification, while the accuracy scores of other methods are much lower than ours, since they cannot generate realistic emotion dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct ablation studies on the MEAD dataset to demonstrate the effectiveness of our Implicit Emotion Displacement Learner (Section 3.2) and verify the contributions of the three important components in it (i.e., data augmentation, conditional audio-driven keypoints and jacobians input and learning displacements for three face-related representations). Specifically, we design five variants in total, and the first two of them are designed to evaluate our motion model design: (1) A2FD (Baseline) : our EAMM without using Implicit Emotion Displacement Learner; (2) Feature-based: representing emotion dynamics at the feature space. The other three are designed to verify the components in Implicit Emotion Displacement Learner: (3) w/o augmentation: without using data augmentation; (4) w/o condition: without using the conditional audio-driven keypoints and jacobians input; (5) displacement for all points: learning emotion displacements for all the key-points and their jacobians. Note that the variant Feature-based is designed to explore whether the emotion pattern can be represented as a feature instead of the displacement manner. Specifically, we first use two separated encoders to extract the audio feature f ğ‘ and the emotion feature f ğ‘’ , respectively. Then we introduce a commonly used operation in the works of style-transfer, AdaIN <ref type="bibr" target="#b22">[Huang and Belongie 2017]</ref>, to transfer the emotion style from the emotion feature f ğ‘’ to the audio feature f ğ‘ . Lastly, we use a decoder <ref type="bibr" target="#b44">[Siarohin et al. 2019b</ref>] to predict the final key-points and jacobians.</p><p>Apart from the metrics mentioned in Section 4.1, we additionally use an off-the-shelf emotion classification network <ref type="bibr" target="#b36">[Meng et al. 2019]</ref> to evaluate the accuracy of the generated emotion. The classification network is trained on the MEAD dataset and achieves a 90% accuracy rate on the test set, which ensures the confidentiality of the evaluation results. The quantitative results shown in Table <ref type="table" target="#tab_2">3</ref> and the visualization shown in Figure <ref type="figure" target="#fig_4">5</ref>, both demonstrate that Implicit Emotion Displacement Learner and its three components are effective designs for emotion generation. Among the three components, the data augmentation strategy is especially critical for our model, as it contributes to transferring accurate emotional dynamics without sacrificing the identity (see the red arrows). Moreover, we observe that the face shape produced by the feature-based model is unstable and the emotion is not obvious either, which indicates that emotion cannot be well disentangled at the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Limitations</head><p>Despite the success of our approach, we also recognize some limitations during the exploration. Firstly, the emotion dynamics generated in the mouth region are not obvious in our work due to the mouth occlusion operation in our data augmentation strategy. Secondly, since emotion is a personalized factor, the emotion pattern extracted from a character sometimes seems to be unnatural after being transferred to another one. Moreover, our method neglects the correlation between the audio and emotion by involving an emotion source video which may lead to incongruent animation results. These will be part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we present the Emotion-Aware Motion Model to generate one-shot emotional talking faces by incorporating an additional emotion source video. We extract the emotion dynamics which is formulated as a transferable motion pattern in the emotion source video and apply it to arbitrary audio-driven talking face. This allows us to synthesize more realistic talking faces which has great potential in applications, e.g., video conferencing, digital avatars. Qualitative and quantitative experiments show that our method can generate more expressive animation results compared with state-of-the-art methods. We hope our work can inspire future research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ETHICAL CONSIDERATIONS</head><p>Our method focuses on synthesizing emotional talking face animation, which is intended for developing digital entertainment and an advanced teleconferencing system. However, it may also be misused for some malicious proposes on social media, which leads to negative impacts on the whole society. To alleviate the concerns above, significant progress has been made in the Deepfake detection area. Some works <ref type="bibr" target="#b8">[Chai et al. 2020;</ref><ref type="bibr" target="#b18">GÃ¼era and Delp 2018;</ref><ref type="bibr" target="#b30">Li et al. 2020;</ref><ref type="bibr" target="#b42">Rossler et al. 2019;</ref><ref type="bibr" target="#b54">Wang et al. 2020a;</ref><ref type="bibr" target="#b60">Yu et al. 2019]</ref> focus on identifying visual deepfakes by detecting texture artifacts or inconsistency. Recent studies <ref type="bibr" target="#b2">[Arandjelovic and Zisserman 2017;</ref><ref type="bibr" target="#b29">Korbar et al. 2018;</ref><ref type="bibr" target="#b39">Owens et al. 2016;</ref><ref type="bibr" target="#b66">Zhou and Lim 2021]</ref> also consider the relationship between the video and audio and use the synchronization of these two modalities to benefit the detection. However, the lack of large realistic and emotional portrait data limits their performance and generalizability. Therefore, we are also committed to supporting the Deepfake detection community by sharing our generated emotional talking face results, which can improve the detection algorithms to handle more complex scenarios. We believe that the proper use of this technique will enhance positive societal development in both machine learning research and daily life.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our Emotion-Aware Motion Model. Our framework includes two modules: Audio2Facial-Dynamics module for one-shot audio-driven talking head generation and Implicit Emotion Displacement Learner for extracting emotional patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of the masks for face related keypoints. The bottom-left image shows ten learned key-points and the bottom-right image indicates the composition masks. We also visualize the masks of the three face-related keypoints separately in the top row. Natural face from CFD dataset Â©The University of Chicago.</figDesc><graphic url="image-259.png" coords="4,326.93,163.93,109.96,109.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative comparisons with state-of-the-art methods on two cases. The top row shows the identity, speech content audio, and emotion source clips. The emotion categories of the videos are surprised (left) and angry (right). The second row shows the corresponding frames of the audio source, i.e., ground truth for mouth shapes. Please zoom in to see the expressions at the red arrows. Natural videos (second row) from LRW dataset Â©BBC. Natural face (left) from CFD dataset Â©The University of Chicago. Natural face (right) from CREMA-D dataset (ODbL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative results of ablation study. We use the same source image for all the variants and the pose is set to be static for better visualization. Natural face from CREMA-D dataset (ODbL).</figDesc><graphic url="image-483.png" coords="8,385.21,152.99,54.49,54.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>User study results evaluated on three different aspects (the maximum score value is 5) and the emotion classification accuracy.</figDesc><table><row><cell>Score/Method</cell><cell cols="6">ATVG [2019b] SDA [2018] Wav2Lip [2020] MakeItTalk [2020] PC-AVS [2021] Real Ours</cell></row><row><cell>Lip Synchronization</cell><cell>3.01</cell><cell>2.30</cell><cell>3.54</cell><cell>3.48</cell><cell>3.79</cell><cell>4.41 3.81</cell></row><row><cell>Facial Expression Naturalness</cell><cell>2.52</cell><cell>2.25</cell><cell>2.65</cell><cell>3.34</cell><cell>3.10</cell><cell>4.03 3.47</cell></row><row><cell>Video Quality</cell><cell>2.54</cell><cell>2.17</cell><cell>3.83</cell><cell>3.79</cell><cell>3.26</cell><cell>4.42 3.89</cell></row><row><cell>Emotion Accuracy</cell><cell>10%</cell><cell>11%</cell><cell>13%</cell><cell>12%</cell><cell>15%</cell><cell>71% 58%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative ablation study. We provide quantitative results of five variants and our EAMM on emotion accuracy, M-LMD, F-LMD and SSIM.</figDesc><table><row><cell>Method/Score</cell><cell cols="4">Acc ğ‘’ğ‘šğ‘œ â†‘ M-LMDâ†“ F-LMDâ†“ SSIM â†‘</cell></row><row><cell>A2FD (Baseline)</cell><cell>14.07</cell><cell>2.78</cell><cell>2.69</cell><cell>0.62</cell></row><row><cell>Feature-based</cell><cell>48.26</cell><cell>2.62</cell><cell>2.61</cell><cell>0.64</cell></row><row><cell>w/o augmentation</cell><cell>49.37</cell><cell>2.60</cell><cell>2.56</cell><cell>0.63</cell></row><row><cell>w/o condition</cell><cell>52.54</cell><cell>2.58</cell><cell>2.60</cell><cell>0.65</cell></row><row><cell>All points</cell><cell>57.26</cell><cell>2.49</cell><cell>2.62</cell><cell>0.66</cell></row><row><cell>Ours</cell><cell>68.41</cell><cell>2.41</cell><cell>2.55</cell><cell>0.66</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the NSFC (No.62025108, 62021002, 61727808), the NSFJS (BK20192003), the Beijing Natural Science Foundation (JQ19015), the National Key R&amp;D Program of China 2018YFA0704000. This work was supported by the Institute for Brain and Cognitive Science, Tsinghua University (THUIBCS) and Beijing Laboratory of Brain and Cognitive Intelligence, Beijing Municipal Education Commission (BLBCI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive Exploration and Refinement of Facial Expression Using Manifold Learning</title>
		<author>
			<persName><forename type="first">Rinat</forename><surname>Abdrashitov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanny</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 33rd Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="778" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An expressive text-driven 3D talking head</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BjÃ¶rn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2013 Posters</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Voice puppetry</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
				<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video rewrite: Driving visual speech with audio</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual conference on Computer graphics and interactive techniques</title>
				<meeting>the 24th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1997">1997a</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video Rewrite: Driving Visual Speech with Audio</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
		<idno type="DOI">10.1145/258734.258880</idno>
		<ptr target="https://doi.org/10.1145/258734.258880" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;97)</title>
				<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;97)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1997">1997b</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural head reenactment with latent pose descriptors</title>
		<author>
			<persName><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Pasechnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13786" to="13795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crema-d: Crowd-sourced emotional multimodal actors dataset</title>
		<author>
			<persName><forename type="first">Houwei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><forename type="middle">C</forename><surname>Michael K Keutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ragini</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on affective computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="377" to="390" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What makes fake images detectable? understanding properties that generalize</title>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="103" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Puppeteergan: Arbitrary portrait animation with semantic-aware appearance transformation</title>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13518" to="13527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02966</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">You said that? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading, ACCV</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synthesizing normalized faces from facial identity features</title>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3703" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">JALI: an animatorcentric viseme model for expressive lip synchronization</title>
		<author>
			<persName><forename type="first">Pif</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text-based editing of talking-head video</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>ZollhÃ¶fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepfake video detection using recurrent neural networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>GÃ¼era</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE international conference on advanced video and signal based surveillance (AVSS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3d dense face alignment</title>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
				<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23">2020. August 23-28, 2020</date>
			<biblScope unit="page" from="152" to="168" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIX 16</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Identity-Invariant Motion Representations for Cross-ID Face Reenactment</title>
		<author>
			<persName><forename type="first">Po-Hsiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-En</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7084" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Audio-driven emotional video portraits</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14080" to="14089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Audiodriven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural style-preserving visual dubbing</title>
		<author>
			<persName><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>ZollhÃ¶fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>ZollhÃ¶fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face x-ray for more general face forgery detection</title>
		<author>
			<persName><forename type="first">Lingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5001" to="5010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Write-a-speaker: Text-based Emotional and Rhythmic Talkinghead Generation</title>
		<author>
			<persName><forename type="first">Lincheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1911">2021. 1911-1920</date>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English</title>
		<author>
			<persName><forename type="first">R</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">A</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">e0196391</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mel frequency cepstral coefficients for music modeling</title>
		<author>
			<persName><forename type="first">Beth</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Music Information Retrieval</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation</title>
		<author>
			<persName><forename type="first">Yuanxun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3478513.3480484</idno>
		<ptr target="https://doi.org/10.1145/3478513.3480484" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Chicago face database: A free stimulus set of faces and norming data</title>
		<author>
			<persName><forename type="first">Debbie</forename><forename type="middle">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Wittenbrink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1122" to="1135" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Frame attention networks for facial expression recognition in videos</title>
		<author>
			<persName><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3866" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Pratul P Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Animating face using disentangled audio representations</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
				<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3290" to="3298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="801" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName><forename type="first">Rudrabha</forename><surname>Kr Prajwal</surname></persName>
		</author>
		<author>
			<persName><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><surname>Vinay P Namboodiri</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Audio-and gaze-driven facial animation of codec avatars</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
				<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Speech-driven expressive talking lips with conditional sequential generative adversarial networks</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rossler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>NieÃŸner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision. 1-11. Najmeh Sadoughi and Carlos Busso</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision. 1-11. Najmeh Sadoughi and Carlos Busso</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>Faceforensics++: Learning to detect manipulated facial images</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Animating arbitrary objects via deep motion transfer</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">StÃ©phane</forename><surname>LathuiliÃ¨re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="2377" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">First order motion model for image animation</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">StÃ©phane</forename><surname>LathuiliÃ¨re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7137" to="7147" />
			<date type="published" when="2019">2019b. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ayush Tewari, Christian Theobalt, and Matthias NieÃŸner</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="716" to="731" />
		</imprint>
	</monogr>
	<note>Neural voice puppetry: Audio-driven facial reenactment</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>NieÃŸner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">FACEGAN: Facial Attribute Controllable rEenactment GAN</title>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
				<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">End-to-end speechdriven facial animation with temporal gans</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09313</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Realistic speechdriven facial animation with gans</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="1398" to="1413" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mead: A large-scale audio-visual dataset for emotional talking-face generation</title>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="700" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">High quality lip-sync animation for 3D photo-realistic talking head</title>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4529" to="4532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion</title>
		<author>
			<persName><forename type="first">Suzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lincheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.09293</idno>
		<imprint>
			<date type="published" when="2021">2021a. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cnn-generated images are surprisingly easy to spot... for now</title>
		<author>
			<persName><forename type="first">Sheng-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="8695" to="8704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">One-shot free-view neural talking-head synthesis for video conferencing</title>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="page" from="10039" to="10049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Reenactgan: Learning to reenact faces via boundary transfer</title>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mesh guided one-shot face reenactment using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Guangming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1773" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Iterative text-based editing of talking-heads using neural retargeting</title>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayvon</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attributing fake images to gans: Learning and analyzing gan fingerprints</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
				<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7556" to="7566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast bi-layer neural synthesis of one-shot realistic head avatars</title>
		<author>
			<persName><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksei</forename><surname>Ivakhnenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandra</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Freenet: Multi-identity face reenactment</title>
		<author>
			<persName><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5326" to="5335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pose-controllable talking face generation by implicitly modularized audio-visual representation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4176" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">MakeltTalk: speaker-aware talking-head animation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Echevarria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Joint Audio-Visual Deepfake Detection</title>
		<author>
			<persName><forename type="first">Yipin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14800" to="14809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Visemenet: Audio-driven animator-centric speech animation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">State of the art on monocular 3D face reconstruction, tracking, and applications</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>ZollhÃ¶fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>NieÃŸner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="523" to="550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
