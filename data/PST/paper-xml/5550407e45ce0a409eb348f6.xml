<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A branch-and-cut decomposition algorithm for solving chance-constrained mathematical programs with finite support</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">James</forename><surname>Luedtke</surname></persName>
							<email>jrluedt1@wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial and Systems Engineering</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial and Systems Engineering</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A branch-and-cut decomposition algorithm for solving chance-constrained mathematical programs with finite support</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FC0562F76DFA0D6046CEFA2E59B4C53C</idno>
					<idno type="DOI">10.1007/s10107-013-0684-6</idno>
					<note type="submission">Received: 13 May 2011 / Accepted: 28 April 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Stochastic programming</term>
					<term>Integer programming</term>
					<term>Chance constraints</term>
					<term>Probabilistic constraints</term>
					<term>Decomposition Mathematics Subject Classification (2010) 90C11</term>
					<term>90C15</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new approach for exactly solving chance-constrained mathematical programs having discrete distributions with finite support and random polyhedral constraints. Such problems have been notoriously difficult to solve due to nonconvexity of the feasible region, and most available methods are only able to find provably good solutions in certain very special cases. Our approach uses both decomposition, to enable processing subproblems corresponding to one possible outcome at a time, and integer programming techniques, to combine the results of these subproblems to yield strong valid inequalities. Computational results on a chance-constrained formulation of a resource planning problem inspired by a call center staffing application indicate the approach works significantly better than both an existing mixedinteger programming formulation and a simple decomposition approach that does not use strong valid inequalities. We also demonstrate how the approach can be used to efficiently solve for a sequence of risk levels, as would be done when solving for the efficient frontier of risk and cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We introduce a new approach for exactly solving chance-constrained mathematical programs (CCMPs) having discrete distributions with finite support. A chance constraint (also known as a probabilistic constraint) states that the chosen decision vector should, with high probability, lie within a region that depends on a set of random variables. A generic CCMP can be stated as</p><formula xml:id="formula_0">min { f (x) | P{x ∈ P(ω)} ≥ 1 -, x ∈ X } ,<label>(1)</label></formula><p>where x ∈ R n is the vector of decision variables to be chosen to minimize f : R n → R, ω is a random vector, P(ω) ⊆ R n is a region parameterized by ω, and X ⊆ R n represents a set of deterministic constraints on x. The interpretation is that the region P(ω) is defined such that the event x / ∈ P(ω) is an undesirable outcome. The likelihood of such an outcome is restricted to be less than the given risk tolerance ∈ (0, 1), which is typically small. We don't make explicit assumptions on the form of the objective function f (•), or the deterministic constraint set X , but our algorithm will require solving subproblems at least as hard as minimizing f (x) over x ∈ X , so f and X should be sufficiently "nice" so that these subproblems can be solved (e.g., one could assume f and X are convex).</p><p>Our algorithm solves CCMPs of the form (1) satisfying the following assumptions:</p><p>(A1) The random vector ω has discrete and finite support: specifically P{ω = ω k } = 1/N for k ∈ N := {1, . . . , N }.<ref type="foot" target="#foot_0">1</ref> (A2) Each P(ω k ), k ∈ N is a non-empty polyhedron. (A3) P(ω k ) have the same recession cone for all k ∈ N , i.e., there exists C ⊆ R n such that C = {r ∈ R n | x + λr ∈ P(ω k ) ∀x ∈ P(ω k ), λ ≥ 0} for all k ∈ N .</p><p>We refer to the possible outcomes of ω, ω k for k ∈ N , as scenarios. Also, to simplify notation, we let P k = P(ω k ) for k ∈ N . While assumption A1 is certainly a restriction, recent results on using sample-average approximation on CCMPs with general distributions <ref type="bibr" target="#b0">[1]</ref> demonstrate that such finite support approximations, when obtained from a Monte Carlo sample of the original distribution, can be used to find good feasible solutions to the original problem and statistical bounds on solution quality. See also <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> for related results on sampling approaches to CCMPs. Assumption A2 includes the case where P k , k ∈ N are defined by the set of vectors for which a feasible recourse action exists, i.e.,</p><formula xml:id="formula_1">P k = x ∈ R n + | ∃y ∈ R d + with T k x + W k y ≥ b k , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where b k ∈ R m and T k and W k are appropriately sized matrices. The special case with d = 0 yields a mathematical program with chance-constrained linear constraints having random coefficients: P{T (ω)x ≥ b(ω)} ≥ 1 -. The assumption that the polyhedra P k are non-empty is without loss of generality, since we can discard any scenario with P k = ∅ and consider a problem with risk tolerance = -1/N . Assumption A3 can be motivated by the result of Jeroslow <ref type="bibr" target="#b5">[6]</ref> that, a general set S ⊆ R n is representable using a binary mixed-integer program if and only if S is the union of finitely many polyhedra having the same recession cone. A simple case in which assumption A3 holds is if each P k is bounded, so that C = {0} for all k ∈ N . The problem we use for our computational study in Sect. 3 provides an example where assumption A3 holds with C = R n + . Although the CCMP (1) can implicitly model recourse actions, e.g., when the polyhedra P k are described as in <ref type="bibr" target="#b1">(2)</ref>, this model doesn't consider costs associated with these recourse actions, making this essentially a static model. Of course, a limit on the cost of the recourse actions can be included in the definition of P k , thus requiring that a recourse action with cost not exceeding a (possibly random) threshold exist with probability at least 1 -. On the other hand, many CCMPs fit exactly the structure of (1). In Sect. 3.1 we describe a flexible resource planning application that fits this model. In this application, one wishes to determine the quantity of different resources to have available, while requiring that the resources available are sufficient to meet the random demands with high probability. In this context, the costs are incurred when resources are acquired, but the allocation of resources to demands incurs no additional cost. A small list of example applications of CCMPs that fit the structure of (1) includes multicommodity flow <ref type="bibr" target="#b6">[7]</ref>, optimal vaccination planning <ref type="bibr" target="#b7">[8]</ref>, air quality management <ref type="bibr" target="#b8">[9]</ref>, aquifer remediation <ref type="bibr" target="#b9">[10]</ref>, and reliable network design <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>CCMPs have a long history dating back to Charnes, Cooper and Symonds <ref type="bibr" target="#b12">[13]</ref>. The version considered here, which requires a system of constraints to be enforced with high probability, was introduced by Prékopa <ref type="bibr" target="#b13">[14]</ref>. However, CCMPs have remained computationally challenging for two reasons: the feasible region is generally not convex, and evaluating solution feasibility requires multi-dimensional integration. Because of these difficulties, methods for obtaining provably good solutions for CCMPs have been successful in only a few very special cases. If the chance constraint consists of a single row and all random coefficients are normally distributed <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, then a deterministic (nonlinear and convex) reformulation is possible if &lt; 1/2. If the randomness appears only in the right-hand side (i.e., P(ω) = {x | T x ≥ b(ω)}) and the distribution of b(ω) is discrete, then approaches based on certain "efficient points" of the random vector <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> or on strong integer programming formulations <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> have been successful.</p><p>While the algorithm proposed in this paper cannot be directly applied to a CCMP that violates assumption A1, it can be used to solve a sample-average approximation (SAA) of such a problem. We believe that this approach is complementary to approaches that attempt to directly solve a CCMP without using SAA (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>). The advantage of the SAA approach is that it requires relatively few assumptions on the structure of the CCMP or the distribution of ω. On the other hand, the SAA will only yield statistical bounds on solution feasibility and optimality, and requires replication to do so. In addition, the SAA problem with &gt; 0 is always non-convex, whereas, in some special cases the original chance constraint defines a convex feasible region. For example, if the randomness appears only in the right-hand side of the chance constraints and the random vector b(ω) has a log-concave distribution, then the resulting feasible region is convex and so nonlinear programming techniques can be used <ref type="bibr" target="#b13">[14]</ref>.</p><p>Very few methods are available for finding provably good solutions for CCMPs with the structure we consider here, i.e., for problems having linear constraints with random coefficients or recourse actions as in <ref type="bibr" target="#b1">(2)</ref>. In <ref type="bibr" target="#b6">[7]</ref>, an approach based on an integer programming formulation (which we give in Sect. 2.1), strengthened with precedence constraints is presented. In more recent work, <ref type="bibr" target="#b26">[27]</ref> presents a specialized branch-and-cut algorithm based on identification of irreducible infeasible sets of certain linear inequality systems, and a specialized branch-and-bound algorithm applicable for CCMPs with random constraint matrix is given in <ref type="bibr" target="#b27">[28]</ref>. While these are important contributions, the size of instances that are demonstrated to be solvable with these approaches is very limited, in particular, because these approaches do not enable decomposition. In another recent important stream of research, a number of conservative approximations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> have been studied that solve tractable (convex) approximations to yield feasible solutions to many classes of CCMPs. However, these approaches do not say anything about the cost of the resulting solutions relative to the optimal, and tend to yield highly conservative solutions.</p><p>The key contribution of this paper is to present an exact approach for solving CCMPs that requires relatively few assumptions about the problem structure, and, as we show in Sect. 3, has the potential to solve problems with high-dimensional random parameters and a large number of scenarios. Our algorithm builds on the ideas of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref> that were successful for solving chance-constrained problems with random right-hand side by developing a method to apply the same types of valid inequalities used there to the more general case considered here. The other important aspect of our algorithm is that it enables decomposition of the problem into single-scenario optimization and separation subproblems. This is important for solving CCMPs with discrete distributions because the problem size grows as the size of the support increases. Another advantage is that the decomposed single-scenario optimization and separation subproblems can be solved using specialized algorithms for the deterministic counterpart, if available. The decomposition approach also enables a straightforward parallel implementation.</p><p>When using a chance-constrained formulation, the risk tolerance is an important user input, as it controls the balance between solution cost and risk of violating the uncertain constraints. Consequently, it is important in practice to perform a sensitivity analysis on this parameter. For example, this can done by solving the CCMP with varying values of risk level , and then constructing an efficient frontier displaying the non-dominated solutions in terms of the two objectives of cost and risk. While any algorithm that can solve a CCMP for a given risk level can be used to construct such an efficient frontier by simply applying the algorithm at each risk level, we show that our algorithm can effectively take advantage of information obtained in the solution at one risk level to more efficiently solve problems at other risk levels.</p><p>Decomposition has long been used for solving traditional two-stage stochastic programming problems, where the objective is to minimize the sum of costs of the first stage decisions and the expected costs of second-stage recourse decisions (see, e.g., <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>). For CCMPs, the only existing paper we are aware of that considers a decomposition approach is <ref type="bibr" target="#b37">[38]</ref>. The decomposition idea is similar to what we present here, but the mechanism for generating cuts is significantly different: they use a convex hull reformulation (based on the Reformulation-Linearization Technique <ref type="bibr" target="#b38">[39]</ref>) which involves "big-M" constants, likely leading to weak inequalities. In contrast, we com-bine the valid inequalities we obtain from different subproblems in a way that avoids the need for "big-M" constants and hence yields strong valid inequalities. As we see in the computational results in Sect. 3, the use of these stronger inequalities makes a very significant difference beyond the benefits obtained from decomposition.</p><p>An extended abstract of this paper appeared in <ref type="bibr" target="#b39">[40]</ref>. This full version includes a convergence proof of the algorithm, more details on how to effectively implement the algorithm, and the approach for solving for multiple risk levels to obtain an efficient frontier of risk and cost. This paper also introduces and conducts extensive computational tests on the chance-constrained resource planning application, which generalizes the model used in <ref type="bibr" target="#b39">[40]</ref> because it allows the recourse constraints to include random coefficients.</p><p>The remainder of this paper is organized as follows. The algorithm is described in Sect. 2. In Sect. 3, we describe the chance-constrained resource planning application, show how the algorithm can be specialized for this application, and present computational results demonstrating the effectiveness of the algorithm for this application. In Sect. 4, we describe how the algorithm can be used to solve for multiple risk levels in sequence and present a numerical illustration. We close in Sect. 5 with some comments on possible modifications of the algorithm that may help when solving mixed-integer CCMPs, in which some of the decision variables have integer restrictions, and nonlinear CCMPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The branch-and-cut decomposition algorithm</head><p>We begin this section with an overview of the approach in Sect. 2.1. We describe the primary subproblems our algorithm is required to solve in Sect. 2.2. In Sect. 2.3 we describe how we obtain strong valid inequalities. Section 2.4 describes the algorithm details and proves its correctness, and computational enhancements are discussed in Sect. 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>We first state a mixed-integer programming formulation for (1) that uses logical constraints. Introduce a binary variable z k for each k ∈ N , where z k = 0 implies x ∈ P k . Then (1) is formulated as:</p><formula xml:id="formula_3">min f (x) (3a) s.t. z k = 0 ⇒ x ∈ P k , k ∈ N , (<label>3b</label></formula><formula xml:id="formula_4">) N k=1 z k ≤ p , (3c) x ∈ X, z ∈ {0, 1} N , (<label>3d</label></formula><formula xml:id="formula_5">)</formula><p>where p := N . Inequality (3c) is a rewritten and strengthened version of the constraint 1 N N k=1 (1-z k ) ≥ 1-, and so models the constraint P{x ∈ P(ω)} ≥ 1-. Let F = {(x, z) |(3b)-(3d)} be the feasible region of (3).</p><p>A natural approach to solve (3) is to use "big-M" constraints to reformulate the conditions (3b). For example, if P k , k ∈ N are explicitly given by (2) and are compact, (3b) can be formulated using additional variables as</p><formula xml:id="formula_6">T k x + W k y k + z k M k ≥ b k , k ∈ N (4a) y k ∈ R d + , k ∈ N . (<label>4b</label></formula><formula xml:id="formula_7">)</formula><p>Here M k ∈ R m + , k ∈ N are sufficiently large to ensure that when z k = 1, constraints (4a) are not active. On the other hand, when z k = 0, constraints (4a) enforce x ∈ P k . Our goal is to avoid the use of big-M constants as in (4a), which are likely to lead to weak lower bounds when solving a continuous relaxation, and to use decomposition to avoid explicit introduction of the constraints (4a) and recourse variables y k that make a formulation based on (4) a very large mixed-integer program when N is large.</p><p>The goal of our approach is similar in spirit to the goal of combinatorial Benders cuts introduced by Codato and Fischetti <ref type="bibr" target="#b40">[41]</ref>. However, we are able to take advantage of the structural properties of the CCMP to obtain stronger valid inequalities. In particular, the valid inequalities we use include both the "logical" z variables and the variables x, in contrast to the combinatorial Benders cuts that are based only on the logical variables. The approach in <ref type="bibr" target="#b26">[27]</ref> has a closer connection to combinatorial Benders cuts.</p><p>Our decomposition algorithm is based on a master problem that includes the original variables x, and the binary variables z. The constraints (3b) are enforced implicitly with cutting planes, as in a Benders decomposition approach. The key difference, however, is that given the mixed-integer nature of our master problem, we seek to add cutting planes that are strong. Specifically, we are interested in strong valid inequalities for the feasible region F, of (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Required subproblems</head><p>Our algorithm assumes we have available algorithms (i.e., oracles) to solve three primary subproblems. Specialized algorithms that take advantage of the structure of the problem (e.g., the constraint set of a shortest path problem) may be used to solve any of these, if available.</p><p>The first subproblem is optimization of a linear function over P k ∩ X , where X ⊆ R n is a fixed closed set containing X , i.e., X ⊇ X , chosen such that P k ∩ X = ∅. Any X ⊇ X can be used for the algorithm to be correct (e.g., one can take X = R n ). In Sect. 2.3, we discuss the trade-offs involved in choosing X . The single scenario optimization subproblem for scenario k ∈ N is then:</p><formula xml:id="formula_8">h k (α) := min αx | x ∈ P k ∩ X<label>(5)</label></formula><p>where α ∈ R n . Problem ( <ref type="formula" target="#formula_8">5</ref>) is always feasible because we require X ∩ P k = ∅. In addition, if α is chosen to be a vector in the dual cone of C, C * := {α ∈ R N | αr ≥ 0, ∀r ∈ C}, then ( <ref type="formula" target="#formula_8">5</ref>) is bounded, and hence the optimal value h k (α) exists and is finite.</p><p>The second subproblem is the single scenario separation problem over the sets</p><formula xml:id="formula_9">P k , k ∈ N : Procedure Sep(k, x) Input: k ∈ N , x ∈ R n Output: (viol, α, β): If x ∈ P k , return viol = FALSE,α = 0, β = 0. Otherwise, return viol = TRUE, and (α, β) ∈ such that α x &lt; β and αx ≥ β for all x ∈ P, where ⊆ R n × R is a finite set.</formula><p>Because each P k is defined by finitely many linear inequalities, the assumption that the separation problem returns a separating hyperplane from a finite set is not restrictive. For example, if P k is given explicitly as {x | T k x ≥ b k }, then this separation routine can be implemented by returning viol = FALSE if T k x ≥ b k , and otherwise returning viol = TRUE and (T k i , b k i ) for some i such that T k i x &lt; b i , where T k i denotes the ith row of T k . As a less trivial example, if P k is defined as in ( <ref type="formula" target="#formula_1">2</ref>), the oracle Sep(k, x) can be implemented by obtaining an extreme point optimal solution to the following linear program:</p><formula xml:id="formula_10">v( x) = max π π T (b k -T k x) s.t. π T W k ≤ 0, π T e = 1, π ∈ R m + .<label>(6)</label></formula><p>If v( x) &gt; 0 and π is an optimal extreme point solution, then the oracle returns TRUE with α = π T T k and β = π T b k , otherwise the oracle returns FALSE. Finally, the algorithm requires solving a master problem of the form:</p><formula xml:id="formula_11">RP(N 0 , N 1 , R) := min f (x) (7a) s.t. N k=1 z k ≤ p, (x, z) ∈ R, x ∈ X, z ∈ [0, 1] N , (<label>7b</label></formula><formula xml:id="formula_12">)</formula><formula xml:id="formula_13">z k = 0, k ∈ N 0 , z k = 1, k ∈ N 1 ,<label>(7c)</label></formula><p>where R ⊆ R n+N is a polyhedron that contains F, and</p><formula xml:id="formula_14">N 0 , N 1 ⊆ N are such that N 0 ∩ N 1 = ∅.</formula><p>To ensure ( <ref type="formula" target="#formula_11">7</ref>) is well-defined, we make the following assumption: <ref type="formula" target="#formula_11">7</ref>) is either infeasible, or has an optimal solution.</p><formula xml:id="formula_15">(A4) For any polyhedron R ⊆ R n+N such that F ⊆ R, and any N 0 , N 1 ⊆ N such that N 0 ∩ N 1 = ∅ with |N 1 | ≤ p, problem (</formula><p>This assumption is satisfied, for example, if f is continuous and X is compact, or if f is linear and X is a polyhedron. We adopt the convention that if ( <ref type="formula" target="#formula_11">7</ref>) is infeasible, then RP(N 0 , N 1 , R) = +∞. In our algorithm, the restrictions on the binary variables z given by N 0 and N 1 are obtained by branching, and the set R is defined by cuts, valid for F, that are added in the algorithm to enforce (3b). If f is linear and X is a polyhedron, ( <ref type="formula" target="#formula_11">7</ref>) is a linear program. If f is convex, and X is a convex set, then ( <ref type="formula" target="#formula_11">7</ref>) is a convex program. If f is linear, and X is a set defined by linear inequalities and integer restrictions on some of the variables, then ( <ref type="formula" target="#formula_11">7</ref>) is a mixed-integer program, so this subproblem will not be efficiently solvable in general. In Sect. 2.5, we discuss a modification to the algorithm that avoids solving mixed-integer programs for this case.</p><p>Finally, if we don't assume f is convex, or that X is a convex set, then again ( <ref type="formula" target="#formula_11">7</ref>) is not efficiently solvable in general, so the requirement to solve this problem would be a severe limitation of our algorithm. Of course, in this case the problem ( <ref type="formula" target="#formula_0">1</ref>) is generally intractable even without the chance constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generating strong valid inequalities</head><p>We now describe our procedure for generating valid inequalities of the form</p><formula xml:id="formula_16">αx + π z ≥ β (8)</formula><p>for the set F, where α ∈ R n , π ∈ R N , and β ∈ R. We assume here that the coefficients α ∈ C * are given, so our task is find π and β that make (8) valid for F. In addition, given a point ( x, ẑ) our separation task is to find, if possible, π and β such that ( x, ẑ) violate the resulting inequality.</p><p>The approach is very similar to that used in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>, which applies only to chanceconstrained problems with random right-hand side. However, by exploiting the fact that we have assumed α to be fixed, we are able to reduce our more general problem to the structure studied in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref> and ultimately apply the same types of valid inequalities.</p><p>The first step is to solve the single scenario optimization problems (5):</p><formula xml:id="formula_17">h k (α) := min αx | x ∈ P k ∩ X , k ∈ N .</formula><p>We have assumed that X is chosen so that P k ∩ X = ∅, and hence these problems are feasible. In addition, α ∈ C * and so αr ≥ 0 for all r ∈ C, the common recession cone of each P k . Thus, each of these problems is bounded and so the values h k (α), k ∈ N are well-defined. The choice of X represents a trade-off in time to compute the values h k (α) and strength of the resulting valid inequalities. Choosing X = R n leads to a problem for calculating h k (α) that has the fewest number of constraints (and presumably the shortest computation time), but choosing X = X yields larger values for h k (α) and consequently stronger inequalities. For example, if X is described as a polyhedron with additional integer restrictions on some of the variables, problem (5) would become a mixed-integer program and hence could be computationally demanding to solve, although doing so may yield significantly better valid inequalities.</p><p>Having obtained the values h k (α) for k ∈ N , we then sort them to obtain a permutation σ of N such that:</p><formula xml:id="formula_18">h σ 1 (α) ≥ h σ 2 (α) ≥ • • • ≥ h σ N (α) .</formula><p>Although the permutation depends on α, we suppress this dependence to simplify notation. Our first lemma uses these values to establish a set of "base" inequalities that are valid for F, which we ultimately combine to obtain stronger valid inequalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1</head><p>The following inequalities are valid for F:</p><formula xml:id="formula_19">αx + (h σ i (α) -h σ p+1 (α))z σ i ≥ h σ i (α), i = 1, . . . , p . (<label>9</label></formula><formula xml:id="formula_20">)</formula><p>The proof of this result is almost identical to an argument in <ref type="bibr" target="#b33">[34]</ref> and follows from the observation that z k = 0 implies αx ≥ h k (α), whereas (3c) implies that z k = 0 for at least one of the p + 1 largest values of h k (α), and hence αx ≥ h σ p+1 (α) is always valid. Now, as was done in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>, we can apply the star inequalities of <ref type="bibr" target="#b41">[42]</ref>, or equivalently in this case, the mixing inequalities of <ref type="bibr" target="#b42">[43]</ref> to "mix" the inequalities ( <ref type="formula" target="#formula_19">9</ref>) to obtain additional strong valid inequalities.</p><p>Theorem 1 <ref type="bibr">([42,43]</ref>)</p><formula xml:id="formula_21">Let T = {t 1 , t 2 , . . . , t } ⊆ {σ 1 , . . . , σ p } be such that h t i (α) ≥ h t i+1 (α) for i = 1, . . . , , where h t +1 (α) := h σ p+1 (α). Then the inequality αx + i=1 (h t i (α) -h t i+1 (α))z t i ≥ h t 1 (α) (<label>10</label></formula><formula xml:id="formula_22">)</formula><p>is valid for F.</p><p>These inequalities are strong in the sense that, if we consider the set</p><formula xml:id="formula_23">Y = (y, z) ∈ R × {0, 1} p | y +(h σ i (α)-h σ p+1 (α))z σ i ≥ h σ i (α), i = 1, . . . , p ,</formula><p>then the inequalities <ref type="bibr" target="#b9">(10)</ref>, with αx replaced by y, define the convex hull of Y <ref type="bibr" target="#b41">[42]</ref>. Furthermore, the inequalities of Theorem 1 are facet-defining for the convex hull of Y (again with y = αx) if and only if h t 1 (α) = h σ 1 (α), which suggests that when searching for a valid inequality of the form <ref type="bibr" target="#b9">(10)</ref>, one should always include σ 1 ∈ T . In particular, for any fixed i ∈ {2, . . . , p}, using T = {σ 1 , σ p } in (10) yields the inequality</p><formula xml:id="formula_24">αx + (h σ 1 (α) -h σ i (α))z σ 1 + (h σ i (α) -h σ p+1 (α))z σ i ≥ h σ 1 (α) , (<label>11</label></formula><formula xml:id="formula_25">)</formula><p>which dominates the inequality (9) for this i. Theorem 1 presents an exponential family of valid inequalities, but given a point ( x, ẑ) separation of these inequalities can be accomplished very efficiently. In <ref type="bibr" target="#b41">[42]</ref> an algorithm based on finding a longest path in an acyclic graph is presented that has complexity O( p 2 ), and <ref type="bibr" target="#b42">[43]</ref> gives an O( p log p) algorithm. We use the algorithm of <ref type="bibr" target="#b42">[43]</ref>.</p><p>Additional classes of inequalities have been derived in <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b18">[19]</ref> for the set defined by <ref type="bibr" target="#b8">(9)</ref> and k z k ≤ p, and these could be applied in this algorithm. However, as the results in <ref type="bibr" target="#b19">[20]</ref> indicate that the mixing inequalities <ref type="bibr" target="#b9">(10)</ref> provide a substantial portion of the strength of this relaxation, we do not pursue this option in the current work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Algorithm details</head><p>A basic version of our proposed algorithm is given in Algorithm 1. The algorithm is a basic branch-and-bound algorithm, with branching being done on the binary variables z k . A node in the search tree is defined by the sets N 0 ( ) and N 1 ( ), representing the sets of variables z k fixed to zero and to one, respectively. The algorithm also uses a polyhedral set R, defined by the cuts added throughout the algorithm. The algorithm is initialized with R = R n×N and a root node 0 having N 0 (0) = N 1 (0) = ∅. The only important difference between this algorithm and a standard branch-and-bound algorithm is how nodes are processed (Step 2 in the algorithm). In this step, the current node relaxation ( <ref type="formula" target="#formula_11">7</ref>) is solved repeatedly until no cuts have been added to the description of R or the lower bound exceeds the incumbent objective value U . Whenever an integer feasible solution ẑ is found, and optionally otherwise, the cut separation routine SepCuts is called. The SepCuts routine must be called when ẑ is integer feasible to check whether the solution ( x, ẑ) is in the set F (i.e., is truly a feasible solution). The routine is optionally called otherwise to possibly improve the lower bound.</p><p>Algorithm 1: Branch-and-cut decomposition algorithm. Let ( x, ẑ) be an optimal solution to <ref type="bibr" target="#b6">(7)</ref>, and lb ← RP(N 0 ( ), N 1 ( ), R); Choose k ∈ N such that ẑk ∈ (0, 1); The SepCuts routine, described in Algorithm 2, attempts to find strong violated inequalities using the approach described in Sect. 2.3. The key here is the method for selecting the coefficients α that are taken as given in Sect. 2.3. The idea is to test whether the conditions that define F,</p><formula xml:id="formula_26">t ← 0, N 0 (0) ← ∅, N 1 (0) ← ∅, R ← R n×N , OPEN ← {0}, U ← +∞;</formula><formula xml:id="formula_27">10 if ẑ ∈ {0, 1} N then 11 CUTFOUND ← SepCuts( x, ẑ, R); 12 if CUTFOUND = FALSE then U ← lb;</formula><formula xml:id="formula_28">22 N 0 (t + 1) ← N 0 ( ) ∪ {k}, N 1 (t + 1) ← N 1 ( ); 23 N 0 (t + 2) ← N 0 ( ), N 1 (t + 2) ← N 1 ( ) ∪ {k}; 24 t ← t + 2; 25 OPEN ← OPEN {t + 1, t + 2};</formula><formula xml:id="formula_29">z k = 0 ⇒ x ∈ P k , ∀k ∈ N , (<label>12</label></formula><formula xml:id="formula_30">)</formula><p>are satisfied. If so, the solution is feasible, otherwise, we identify a scenario k such that ẑk = 0 and x / ∈ P k . We then find an inequality, say αx ≥ β, that is valid for P k and that separates x from P k .</p><p>Algorithm 2: Cut separation routine SepCuts( x, ẑ, R).</p><p>Data: x, ẑ, R Result: If valid inequalities for F are found that are violated by ( x, ẑ), adds these to description of R and returns TRUE, else returns FALSE.</p><p>CUTFOUND ← FALSE;</p><formula xml:id="formula_31">1 for k ∈ N such that ẑk &lt; 1 do 2</formula><p>Call the Sep(k, x) oracle to obtain (viol, α, β);</p><formula xml:id="formula_32">3 if viol = TRUE then 4</formula><p>Using the coefficients α, exactly solve the separation problem for inequalities of the form 5 <ref type="bibr" target="#b9">(10)</ref>. If an inequality violated by ( x, ẑ) exists, add a non-empty set of violated inequalities to the description of R; CUTFOUND ← TRUE; In line 2 of Algorithm 2, we test whether x ∈ P k for any k such that ẑk &lt; 1. To obtain a convergent algorithm, it is sufficient to check only those k such that ẑk = 0; we also optionally check k such that ẑk ∈ (0, 1) in order to possibly generate additional violated inequalities. We now establish that Algorithm 1 solves (3).</p><p>Theorem 2 Assume A1-A4, and that we have algorithms available to solve subproblems (5), Sep(k, x) for any x ∈ X and k ∈ N , and <ref type="bibr" target="#b6">(7)</ref>. Then, algorithm 1 terminates finitely, and at termination if U = +∞, problem (3) is infeasible, otherwise U is the optimal value of (3).</p><p>Proof First, we verify that the values h k (α), k ∈ N , obtained in solving (5) are well defined (these are used in line 5 of the SepCuts subroutine, when separating inequalities of the form (10)). The coefficient vector α ∈ R n , obtained from the procedure Sep(k, x), defines a valid inequality of the form αx ≥ β for P k . Thus, αr ≥ 0 for any r ∈ C, since otherwise there would not exist a β such that αx ≥ β for all x ∈ P k . Thus, α ∈ C * , and so h k (α), k ∈ N are well-defined by the arguments in Sect. 2.3.</p><p>We next argue that the algorithm terminates finitely. Indeed, the algorithm trivially processes a finite number of nodes as it is based on branching on a finite number of binary variables. In addition, the set of possible inequalities that can be produced by the procedure SepCuts procedure is finite because for any coefficient vector α there are finitely many mixing inequalities of the form <ref type="bibr" target="#b9">(10)</ref>, and furthermore there are finitely many coefficient vectors α since the SepCuts procedure is assumed to return inequalities from a finite set. Thus, the terminating condition for processing a node (line 19) must be satisfied after finitely many iterations.</p><p>Next, the algorithm never cuts off an optimal solution because the branching never excludes part of the feasible region and only valid inequalities for the set F are added. This proves that, at termination, if U = +∞, then (3) is infeasible, and otherwise that U is an upper bound on the optimal value of (3). The final point we must argue is that U is only updated (in line 13) if the solution ( x, ẑ) ∈ F (i.e., it is feasible), and hence U is also a lower bound on the optimal value of (3). U is updated only if ẑ ∈ {0, 1} N and SepCuts( x, ẑ, R) returns FALSE. We therefore must argue that if ẑ ∈ {0, 1} N but ẑ / ∈ F, then SepCuts( x, ẑ, R) must return TRUE. If ẑ / ∈ F, then there must exist a scenario k such that ẑk = 0 but x / ∈ P k . For the first such scenario k , the separation procedure Sep(k , x) returns viol = TRUE and (α, β) such that αx ≥ β is valid for P k and α x &lt; β. The vector α is then used to calculate h k (α) for all k ∈ N . Because αx ≥ β holds for any x ∈ P k ,</p><formula xml:id="formula_33">h k (α) = min αx | x ∈ P k ∩ X ≥ β.</formula><p>We then consider two cases. First, if h σ p+1 (α) ≥ h k (α) then any inequality of the form <ref type="bibr" target="#b9">(10)</ref> is violated by ( x, ẑ) because</p><formula xml:id="formula_34">h t 1 (α) - i=1 (h t i (α) -h t i+1 (α))ẑ t i ≥ h σ p+1 (α) ≥ h k (α) ≥ β &gt; α x.</formula><p>Otherwise, if h σ p+1 (α) &lt; h k (α) then k = σ i for some i = 1, . . . , p. Then, the inequality <ref type="bibr" target="#b9">(10)</ref> defined by taking T = {k } reduces to:</p><formula xml:id="formula_35">αx + (h k (α) -h σ p+1 (α))z k ≥ h k (α)</formula><p>which cuts off ( x, ẑ) since ẑk = 0. Because separation of the inequalities ( <ref type="formula" target="#formula_21">10</ref>) is done exactly, this implies that in either case a violated inequality is added to R and SepCuts( x, ẑ, R) returns TRUE.</p><p>An advantage of this algorithm is that most of the subproblems are decomposed and so can be solved one scenario at a time and can be implemented in parallel. In particular, the subproblems Sep(k, x) for any k such that ẑk &lt; 1 can be solved in parallel. The subsequent work of generating a strong valid inequality is dominated by calculation of the values h k (α) as in <ref type="bibr" target="#b4">(5)</ref>, which can also be done in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Computational enhancements</head><p>We have stated our approach in relatively simple form in Algorithm 1. However, as this approach is a variant of branch-and-cut for solving a particularly structured mixed-integer programming problem, we can and should also use all the computational enhancements commonly used in such algorithms. In particular, it is important to use heuristics to find good feasible solutions early and use some sort of pseudocost branching <ref type="bibr" target="#b43">[44]</ref>, strong branching <ref type="bibr" target="#b44">[45]</ref>, or reliability branching <ref type="bibr" target="#b45">[46]</ref> approach for choosing which variable to branch. These enhancements are easily achieved if the algorithm is implemented within a commercial integer programming solver such as IBM Ilog CPLEX, which already has these and many other useful techniques implemented.</p><p>In our experience, we found that a potential bottleneck in the algorithm is solving the separation subproblem Sep(k, x), which we implemented using the linear program <ref type="bibr" target="#b5">(6)</ref>, within the SepCuts routine. In the worst case, this problem may be solved for all scenarios k with ẑk &lt; 1. If the solution ( x, ẑ) is feasible, this effort is necessary to verify that it is feasible. However, it is often the case that it is eventually found that ( x, ẑ) is not feasible, and hence the time spent solving Sep(k, x) for scenarios in which x ∈ P k is unproductive as these separations problems fail to yield an inequality that cuts off ( x, ẑ). Stated another way, if a violated inequality exists, we would prefer to find it at one of the first scenarios we check. This potential for significant unproductive calls to Sep(k, x) is the reason we terminate the SepCuts routine after finding the first scenario that yields one or more violated inequalities, as opposed to continuing through all scenarios. In addition, two other strategies helped to minimize unproductive calls to Sep(k, x).</p><p>The first and most beneficial strategy is to save a list of all the α vectors generated in the SepCuts routine, along with the corresponding calculated values h k (α), throughout the algorithm in a coefficient pool. The coefficient pool is similar to the standard strategy in mixed-integer programming solvers of maintaining a "cut pool" that stores valid inequalities that may be later added to the linear programming relaxation; the difference is that the coefficient pool does not store valid inequalities, but instead stores information useful for generating valid inequalities. When the SepCuts routine is called we first solve the separation problem to search for violated mixing inequalities <ref type="bibr" target="#b9">(10)</ref> for each of the coefficient vectors in the coefficient pool. If we find any violated inequalities by searching through the coefficient pool, we add these and avoid solving Sep(k, x) altogether. While there is some computational expense in solving the separation problem for the inequalities <ref type="bibr" target="#b9">(10)</ref> for all the vectors in the coefficient pool, the separation of ( <ref type="formula" target="#formula_21">10</ref>) is very efficient and hence this time was significantly outweighed by the time saved by avoiding solving Sep(k, x). Although we did not pursue this option, if the coefficient pool becomes too large, a strategy could be implemented to "prune" the list, e.g., based on the frequency in which each vector in the pool yielded a violated inequality.</p><p>The second strategy for limiting unproductive calls to Sep(k, x) is to heuristically choose the sequence of scenarios in a way that finds a scenario that yields a violated inequality, if there is one, earlier. First, observe that when ẑk &gt; 0, it is possible to find x / ∈ P k , and yet not find a violated mixing inequality <ref type="bibr" target="#b9">(10)</ref>. This motivates first checking scenarios with ẑk = 0, and more generally, checking scenarios in increasing order of ẑk . In addition, it seems intuitive that scenarios that have yielded inequalities previously are the ones that are more likely to yield inequalities in future calls to SepCuts. Thus, for each scenario k, we also keep a count s k , of the total number of times that scenario k has yielded a violated inequality <ref type="bibr" target="#b9">(10)</ref>. We heuristically combine these observations by searching scenarios in decreasing order of the value (1 -ẑk )s k .</p><p>3 Application and computational results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A probabilistic resource planning problem</head><p>We tested our approach on a probabilistic resource planning problem. This problem consists of a set of resources (e.g., server types), denoted by i ∈ I := {1, . . . , n}, which can be used to meet demands for a set of customer types, denoted by j ∈ J := {1, . . . , m}. The objective is to choose the quantity of each resource to have on hand to meet customer demands.</p><p>A deterministic version of this problem can be stated as:</p><formula xml:id="formula_36">min x∈R n + ,y∈R nm + ⎧ ⎨ ⎩ cx | j∈J y i j ≤ ρ i x i , ∀i ∈ I, i∈I μ i j y i j ≥ λ j , ∀ j ∈ J ⎫ ⎬ ⎭ .</formula><p>Here c i represents the unit cost of resource i, ρ i ∈ (0, 1] represents the yield of resource i, i.e., the fraction of what is planned to be available that actually can be used, λ j ≥ 0 represents the demand of customer type j, and μ i j ≥ 0 represents the service rate of resource i for customer type j, i.e., how many units of demand of customer type j can be met with a unit of resource i. The variables x i determine the quantity of resource i to have on hand, and the variables y i j represent the amount of resource i to allocate to customer type j. Thus, the problem is to choose resource levels and allocations of these resources to customer types to minimize the total cost of resources, while requiring that the allocation does not exceed the available resource levels and is sufficient to meet customer demands.</p><p>In the probabilistic resource planning problem, the customer demands, resource yields, and service rates are nonnegative random vectors of appropriate size denoted by λ, ρ, and μ. The resource decisions x i must be made before these random quantities are observed, but the allocation decisions can adapt to these realizations. We require that all customer demands should be met with high probability, leading to the chance-constrained model min</p><formula xml:id="formula_37">x∈R n + cx P x ∈ P( λ, ρ, μ) ≥ 1 - , where P(λ, ρ, μ) = ⎧ ⎨ ⎩ x ∈ R n + | ∃y ∈ R nm + s.t. m j=1 y i j ≤ ρ i x i , ∀i ∈ I, n i=1 μ i j y i j ≥ λ j , ∀ j ∈ J . (<label>13</label></formula><formula xml:id="formula_38">)</formula><p>We test our algorithm on three versions of this problem, varying which components are random. In the first version, only the arrival rates λ are random; this is the model that was used in the call center staffing problem studied in <ref type="bibr" target="#b46">[47]</ref> and was used as the test case in <ref type="bibr" target="#b39">[40]</ref>. In the second version, both the arrival rates and the yields are random, and all are random in the final version.</p><p>When we use a finite scenario approximation of the random vectors λ, ρ, and μ, assumption A1 of Sect. 2.1 is satisfied. Assumption A2 is satisfied because P(λ, ρ, μ) is a non-empty polyhedron for any nonnegative vectors λ, ρ, and μ. It is easy to see that the recession cone of P(λ, ρ, μ) is C = R n + for any nonnegative λ, ρ, μ and hence Assumption A3 is satisfied. Assumption A4 is satisfied because the master problem ( <ref type="formula" target="#formula_11">7</ref>) is always a feasible linear program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation details</head><p>A key advantage of the proposed algorithm is the ability to use problem-specific structure to efficiently solve the separation and optimization problems over P k for all scenarios k. As an illustration, we describe here how the optimization problems can be solved efficiently for this application.</p><p>Given a coefficient vector α ∈ R n + , the following optimization problem is to be solved for each scenario k:</p><formula xml:id="formula_39">h k (α) = min x,y i∈I α i x i s.t. j∈J y i j -ρ k i x i ≤ 0, ∀i ∈ I, (<label>14a</label></formula><formula xml:id="formula_40">) i∈I μ k i j y i j ≥ λ k j , ∀ j ∈ J, x ∈ R n + , y ∈ R nm + . (<label>14b</label></formula><formula xml:id="formula_41">)</formula><p>Because α ≥ 0 and ρ k i &gt; 0 for all i, there exists an optimal solution in which inequalities (14a) are tight, and hence x can be eliminated from the problem.</p><p>After doing so, the problem can be decomposed by customer type yielding</p><formula xml:id="formula_42">h k (α) = j∈J min y • j i∈I α i ρ k i y i j i∈I μ k i j y i j ≥ λ k j , y • j ∈ R m + 123 = j∈J min i∈I α i λ k j ρ k i μ k i j .</formula><p>Thus, given a coefficient vector α, optimization over all scenario sets P k can be accomplished with O(N nm) calculations using the above closed-form expression.</p><p>When the yields and service rates are not random, optimization over all scenario sets can be done even more efficiently. Specifically, in this case the expression for h k (α) reduces to:</p><formula xml:id="formula_43">h k (α) = j∈J min i∈I α i λ k j ρ i μ i j = j∈J λ k j min i∈I α i ρ i μ i j .</formula><p>Thus, the minimization over i ∈ I is independent of scenario, and hence can be done just once for each customer type j, allowing optimization over all scenarios to be accomplished in O(N n + mn).</p><p>In this application, the set of deterministic constraints is simply X = R n + . Thus, the only choice for X is to use X = X = R n + . We implemented our approach within the commercial integer programming solver CPLEX 12.2. The main component of the approach, separation of valid inequalities of the form <ref type="bibr" target="#b9">(10)</ref>, was implemented within a cut callback that CPLEX calls whenever it has finished solving a node (whether the solution is integer feasible or not) and also after it has found a heuristic solution. In the feasibility checking phase of the SepCuts routine (line 2) we searched for k with ẑk &lt; 1 and x / ∈ P k in decreasing order of (1 -ẑk )s k . The separation problem Sep(k, x) was implemented by solving the linear program (6), also using CPLEX. For the first k we find in which ẑk &lt; 1 and x / ∈ P k (and only the first) we add all the violated valid inequalities of the form <ref type="bibr" target="#b10">(11)</ref> as well as the single most violated inequality of the form <ref type="bibr" target="#b9">(10)</ref>. Our motivation for adding the inequalities <ref type="bibr" target="#b10">(11)</ref> is that they are sparse and this is a simple way to add additional valid inequalities in one round; we found that doing this yielded somewhat faster convergence. As required in the algorithm, the separation of valid inequalities is always attempted if the relaxation solution ẑ is integer feasible. When ẑ is not integer feasible, at the root node we continued generating valid inequalities until no more were found, or until the relative improvement in relaxation objective value from one round of cuts to the next was less than 0.01 %. Throughout the branch-and-bound tree, we attempt to generate cuts if ẑ is fractional only every 20 nodes, and for such nodes we add only one round of cuts. This strategy appeared to offer a reasonable balance between time spent generating valid inequalities and the corresponding improvement in relaxation objective values, but it is certainly possible that an improved strategy could be found.</p><p>All computational tests were conducted on a Mac Mini, running OSX 10.6.6, with a two-core 2.66 GHz (only a single core was used) having 8GB memory. A time limit of 1 h was enforced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Test instances</head><p>We randomly generated test instances. For a given problem size (number of resources and customer types) we first generated a single "base instance" consisting of the unit costs of the resources and a set of base service rates. A significant fraction of the service rates μ i j were randomly set to zero, signifying that resource i cannot be used to meet customer type j. The costs were generated in such a way that "more efficient" resources were generally more expensive, in order to make the solutions nontrivial. Customer demands λ are assumed to be multivariate normally distributed. When the yields ρ are random, they are assumed to take the form ρ = max{ ρ, e} where e is a vector of all ones and ρ is a vector of independent normal random variables. For the test instances in which μ is random, it is modeled as μi j = e -Z i j μ i j where μ i j are the base service rates and Z i j are independent normal random variables with mean zero and standard deviation 0.05. For each base instance, we generated five different base distributions of the arrival rate and yield random vectors and for each of these we generated a sample of 3,000 realizations for instances in which only λ is random, and 1,500 realizations for instances in which ρ and/or μ are random. For each base instance we also generated 5 independent samples of 1,500 realizations of μi j from the same base distribution (i.e., in contrast to λ and ρ, the base distribution of μ is the same in all instances; only the random sample varies). Instances with N &lt; 3,000 (or N &lt; 1,500 when ρ or μ are random) are obtained by using only the first N scenarios in the sample. For the version of the problem in which only the arrival rates are random, we used ρ i = 1 for all resources i, and used the base service rates without modification. In addition to varying the sample size N , we also considered two risk levels, = 0.05 and = 0.1. Complete details of the instance generation, and the actual instances used, are available from the author <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>We compared our algorithm against the Big-M formulation that uses (4) and also against a basic decomposition algorithm that does not use the strong valid inequalities of Sect. 2.3 or the computational enhancements discussed in Sect. 2.5. We compare against this simple decomposition approach to understand whether the success of our algorithm is due solely to decomposition, or whether the strong inequalities are also important. The difference between the basic decomposition algorithm and the strengthened version is in the type of cuts that are added in the SepCuts routine. Specifically, given a solution ( x, ẑ) such that there exists scenario k with ẑk = 0 and x / ∈ P k , and a valid inequality αx ≥ β for the set P k that is violated by x, the basic decomposition algorithm simply adds the inequality</p><formula xml:id="formula_44">αx ≥ β(1 -z k ) .</formula><p>When the sets P k have the form <ref type="bibr" target="#b12">(13)</ref>, this inequality is valid for F because x ≥ 0 and any valid inequality for P k has α ≥ 0. Furthermore, this inequality successfully cuts off the infeasible solution ( x, ẑ). The main results are presented in Tables 1, 2 and 3. These tables compare the results for solving these instances with three methods: directly solving the big-M formulation based on (4), the simple Benders decomposition algorithm (labeled Basic Decomp), and the algorithm proposed in this paper (labeled Strong Decomp). Each row in these tables presents summary results for 5 instances with the same characteristics: a base instance with size (n, m), risk level , and N scenarios. In most cases, three entries are reported for each method: the # column reports how many of the five instances were solved to optimality within the time limit, the Time column reports the average solution time of the instances that were solved to optimality, in seconds rounded to the nearest integer, and the Gap column reports the average optimality gap at the time limit for the instances that were not solved to optimality. Optimality gap is calculated as (U B -L B)/U B where U B and L B are the values of the best feasible solution and best lower bound, respectively, found by that method. A '-' in a Time or Gap entry indicates there were no instances on which to calculate this average (because either none or all of them were solved to optimality, respectively). A '*' in the Gap column indicates that for at least one of the instances no feasible solution was found within the time limit, and hence such instances were not included in the average gap calculation. If a '*' appears with no number, no feasible solution was found for any of the instances. Table <ref type="table" target="#tab_0">1</ref> gives the results for instances in which only the demands ( λ) are random. The big-M formulation based on (4) is not able to solve any of these instances within the time limit (and hence, only a "Gap" column is reported). This formulation only successfully solves the LP relaxation and finds a feasible solution for the smallest instance sizes (and not even for all of these). The basic decomposition approach significantly improves over the big-M formulation in that it is able to find feasible solutions for most of the instances. However, only some of the smallest of the instances could be solved to optimality, and the larger instances had very large optimality gaps after the limit. The branch-and-cut decomposition algorithm is able to solve all these instances to optimality in an average of less than two minutes. Table <ref type="table" target="#tab_0">1</ref> also shows that the total number of nodes required to solve these instances with this method is very small on average (0 nodes indicates the instances were solved at the root node), which occurs because for this problem class the lower bounds produced by the strong valid inequalities are almost identical to the true optimal values. Table <ref type="table">2</ref> gives the results for instances with random demands and yields and Table <ref type="table" target="#tab_1">3</ref> gives the results for the case where demands, yields, and service rates are random. These instances are significantly more challenging than those with only random demands, and hence we report results for smaller instances. However, these instances are still large enough to prevent solution using the big-M formulation based on (4), as the linear programming relaxation again is often not solved within the time limit, and when it does the optimality gap is very large. We see that again the basic decomposition algorithm has more success solving the smallest instances, but leaves large optimality gaps for the larger instances. While the branch-and-cut decomposition algorithm solves many more of the instances to optimality, a significant portion of the larger instances are not solved to optimality within the time limit, in contrast to the random demands only case. However, the optimality gap achieved within the time limit is still quite small, usually less than 1 %, and almost always less than 2 %. Table <ref type="table" target="#tab_2">4</ref> presents results comparing the optimality gap and solution times of the two decomposition approaches at the root node for some of the instances with random yields and service rates. Specifically, for the largest instances in this test set, we report the average quality of the lower bound obtained at the root node (compared against the optimal solution, or best solution found by any method if optimal is unknown) and the average time to process the root node. These results indicate that the strong valid inequalities close substantially more of the optimality gap at the root node than the basic Benders inequalities, and do so in a comparable amount of time. In addition, the computation times suggest that although many of these instances are not solved to optimality in the 1 h time limit, it is possible to obtain strong bounds on solution quality relatively quickly. Finally, these gaps also suggest that, in addition to obtaining additional classes of strong valid inequalities to close the gap further, investigating problem-specific branching strategies may also be beneficial in helping to finish solving these instances to optimality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Solving for the efficient frontier</head><p>We now describe how the proposed algorithm can be used to efficiently solve for multiple risk levels in sequence, as would be done when constructing an efficient frontier between cost and risk of constraint violation. We assume we have a fixed set of N scenarios, and we wish to solve the CCMP for a set of risk levels</p><formula xml:id="formula_45">1 &lt; 2 &lt; • • • &lt; r .</formula><p>Of course, one option is simply to solve these r instances independently. Alternatively, we demonstrate how, by solving the instances in a particular sequence, the branch-andcut decomposition algorithm can use information obtained from solving one instance to "warm-start" the solution of later instances. This is not straightforward because these are mixed-integer programming instances with different feasible regions. First, observe that if x * t is an optimal solution to the instance with risk level t , then x * t is a feasible solution to the instance with risk level t+1 , since t &lt; t+1 . This motivates solving the instances in order of increasing risk level, so that the optimal solution of one instance can be used as a starting incumbent solution for the next.</p><p>Another strategy for using information from one solution of a mixed-integer program to the next is to use valid inequalities derived from one for the next. Unfortunately, if the instances are being solved in increasing order of risk level, a valid inequality for the instance with risk level t may not be valid for the instance with risk level t+1 &gt; t , since this instance has a larger feasible region. However, the information used to generate the mixing inequalities (10)-the coefficient vectors α and the corresponding scenario objective values h k (α)-is independent of the risk level. Thus, we can save all the information in the coefficient pool (described in Sect. 2.5) from one instance to the next, and continue to use this information to generate the mixing inequalities <ref type="bibr" target="#b9">(10)</ref>. As the primary work in generating these inequalities is the derivation of the coefficient vector α by solving a separation problem and the calculation of the scenario objective values h k (α), this can save a significant amount of time. When saving the coefficient pool from one instance to the next, its size can grow very significantly. To prevent the time spent checking the coefficient pool from becoming a bottleneck of the algorithm we keep only a subset of the coefficient vectors in the pool from one instance to the next. To choose which to keep, we maintain a count of how many times a violated inequality was found using each coefficient vector throughout the solution of an instance, and keep 20 % of the coefficient vectors that have the highest counts (we also keep any that are tied with the smallest count in the top 20 %, so the number kept may exceed 20 %). In addition to limiting the size of the coefficient pool, this strategy has the potential benefit of identifying and focusing attention on the coefficient vectors that are most effective at generating valid inequalities.</p><p>We conducted a computational experiment to test this approach for computation of an efficient frontier. For this test we chose three base instances; one with only demands ( λ) random, one with random demands and yields ( λ, ρ), and one with random demands, yields and service rates ( λ, ρ, μ). We also chose a single sample size N for each. These instances were chosen to be the largest in our test set that our algorithm can solve to optimality within the time limit at the largest risk level we solve for. For each of these base instances, we solved for 16 risk levels ranging from 0.0 to 0.15 in increments of 0.01, with and without using warm-start information. We repeated this exercise for the five different random instances of each base instance.</p><p>Table <ref type="table" target="#tab_3">5</ref> presents the average total solution time and average total number of nodes to solve all 16 risk levels using the two approaches. The results indicate that these warm-start strategies can indeed effectively reduce the total solution time for computing an efficient frontier. The ideas were particularly effective for the instances in which only the demands are random, reducing the total solution time by about 75 % on average. When the yields and/or service rates are random the warm-start strategies were relatively less helpful, but still reduced total solution time by about 50 % on average. The reduced impact can be explained by the total number of nodes processed. When only demands are random, very few branch-and-bound nodes are processed, so a significant portion of the total time is spent finding a good feasible solution and generating valid inequalities to solve the initial LP relaxation, and this work is aided significantly by the warm-start techniques. In contrast, when the yields and/or service rates are random, relatively more branch-and-bound nodes are processed, and the number processed is not significantly affected by the warm-start techniques. As a result, in these instances, the proportion of the time spent solving the node linear programming relaxations is higher, and this time is not helped by the warm-start techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We close by discussing adaptations and extensions of the algorithm. In our definition of the master relaxation <ref type="bibr" target="#b6">(7)</ref>, we have enforced the constraints x ∈ X . If X is a polyhedron and f (x) is linear, ( <ref type="formula" target="#formula_11">7</ref>) is a linear program. However, if X is not a polyhedron, suitable modifications to the algorithm could be made to ensure that the relaxations solved remain linear programming problems. For example, if X is defined by a polyhedron Q with integrality constraints on some of the variables, then we could instead define the master relaxation to enforce x ∈ Q, and then perform branching both on the integerconstrained x variables and on the z variables. Such a modification is straightforward to implement within existing integer programming solvers. In this case, Q would be a natural choice for the relaxation X of X used in Sect. 2.3 when obtaining the h k (α) values as in <ref type="bibr" target="#b4">(5)</ref>. In addition, Q could be augmented with inequalities that are valid for X ∩ P k to obtain a linear relaxation that is a closer approximation to conv(X ∩ P k ).</p><p>Although these modifications are likely to help our algorithm when X contains integrality restrictions, we should also point out that the algorithm may face some difficulties with such problems due to these integrality restrictions. For example, suppose that X = Q ∩ Z n where Q is a polyhedron, and consider the (trivial) case with = 0, so that the feasible set becomes Q ∩ P ∩Z n where P = ∩ N k=1 P k . Because our approach does not make use of integrality of the x variables, except possibly when solving single scenario problems to obtain the h k (α) values, the absolute best relaxation our valid inequalities could produce would be N k=1 conv(Q ∩ P k ∩ Z n ), which could certainly be a poor approximation to conv Q ∩ P ∩ Z n . As a result, for chance-constrained mixed-integer programs, we expect that further research will be needed to find strong valid inequalities that yield tighter relaxations of the latter set.</p><p>If X is defined by convex nonlinear inequalities of the form g(x) ≤ 0, then the master relaxation problem (7) could be made a linear program by using an outer approximation of X , as in the LP/NLP branch-and-bound algorithm for solving mixedinteger nonlinear programs (MINLPs) <ref type="bibr" target="#b48">[49]</ref>. In this case, when a solution ( x, ẑ) is found in which ẑ is integer feasible, in addition to being required to check feasibility of the logical conditions (3b), a nonlinear programming problem min{ f (x) | g(x) ≤ 0, x ∈ R} would also be solved, and the gradient inequalities at the optimal solution x, g( x) + ∇g( x) T (x -x) ≤ 0, would be added to the current outer approximation linear programming relaxation for each nonlinear constraint. (A similar inequality, with an auxiliary variable, would be added if the objective is a convex nonlinear function.) The details are beyond the scope of this paper, but we conjecture that this algorithm could be shown to converge finitely provided that a constraint qualification holds at every nonlinear programming problem solved in the algorithm, an assumption that is standard for convergence of the LP/NLP algorithm.</p><p>Many special cases of (1) are known to be N P-hard <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50]</ref>. Therefore, we cannot expect a polynomial-time algorithm for <ref type="bibr" target="#b0">(1)</ref>, which is why we propose a branchand-cut algorithm. On the other hand, in our tests, the proposed algorithm performed remarkably well on the instances in which randomness appeared only in the right-hand side of the constraints, most notably requiring a very small number of branch-andbound nodes to be explored. We therefore think it would be an interesting direction for future research to investigate whether a polynomial-time algorithm, or approximation algorithm with a priori guarantee on optimality or feasibility violation, may exist for this special case, possibly with additional assumptions on the data (e.g., that it was a obtained as a sample approximation from a log-concave distribution).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 while OPEN = ∅ do 2 Step 1 : 3 Step 2 :</head><label>12132</label><figDesc>Choose ∈ OPEN and let OPEN ← OPEN \ { }; Process node ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>18 until 19 Step 3 :</head><label>18193</label><figDesc>← SepCuts( x, ẑ, R); CUTFOUND = TRUE or lb ≥ U ; Branch if necessary; 20 if lb &lt; U then 21</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Results for instances with random demands only</figDesc><table><row><cell>(n, m)</cell><cell></cell><cell>N</cell><cell>Big-M (4)</cell><cell cols="2">Basic Decomp</cell><cell></cell><cell cols="2">Strong Decomp</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gap (%)</cell><cell>#</cell><cell>Time</cell><cell>Gap (%)</cell><cell>Time</cell><cell>Nodes</cell></row><row><cell>(20, 30)</cell><cell>0.05</cell><cell>1,000</cell><cell>*23.7</cell><cell>3</cell><cell>3,033</cell><cell>0.1</cell><cell>3</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell>2,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>3.1</cell><cell>9</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell>3,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>6.8</cell><cell>16</cell><cell>1.6</cell></row><row><cell></cell><cell>0.1</cell><cell>1,000</cell><cell>*32.1</cell><cell>0</cell><cell>-</cell><cell>3.9</cell><cell>5</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell>2,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>12.9</cell><cell>12</cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell>3,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>23.0</cell><cell>22</cell><cell>16.4</cell></row><row><cell>(40, 50)</cell><cell>0.05</cell><cell>1,000</cell><cell>*</cell><cell>2</cell><cell>2,382</cell><cell>1.7</cell><cell>6</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell>2,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>11.0</cell><cell>15</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell>3,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>16.6</cell><cell>25</cell><cell>0.0</cell></row><row><cell></cell><cell>0.1</cell><cell>1,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>10.6</cell><cell>7</cell><cell>2.0</cell></row><row><cell></cell><cell></cell><cell>2,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>19.5</cell><cell>23</cell><cell>18.2</cell></row><row><cell></cell><cell></cell><cell>3,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>25.4</cell><cell>26</cell><cell>16.0</cell></row><row><cell>(50, 100)</cell><cell>0.05</cell><cell>1,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>10.4</cell><cell>21</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell>2,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>*22.8</cell><cell>39</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell>3,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>*</cell><cell>70</cell><cell>0.8</cell></row><row><cell></cell><cell>0.1</cell><cell>1,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>21.7</cell><cell>20</cell><cell>2.2</cell></row><row><cell></cell><cell></cell><cell>2,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>*31.3</cell><cell>63</cell><cell>1.4</cell></row><row><cell></cell><cell></cell><cell>3,000</cell><cell>*</cell><cell>0</cell><cell>-</cell><cell>*</cell><cell>106</cell><cell>0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Results with random demands, yields, and service rates</figDesc><table><row><cell>(n, m)</cell><cell>N</cell><cell></cell><cell cols="2">Big-M (4)</cell><cell></cell><cell cols="2">Basic Decomp</cell><cell></cell><cell cols="2">Strong Decomp</cell></row><row><cell></cell><cell></cell><cell></cell><cell>#</cell><cell cols="3">Time Gap (%) #</cell><cell cols="3">Time Gap (%) #</cell><cell cols="2">Time Gap (%)</cell></row><row><cell>(5, 10)</cell><cell>0.05</cell><cell>500</cell><cell cols="2">4 1,524</cell><cell>1.6</cell><cell>5</cell><cell>163</cell><cell cols="2">-5</cell><cell>5</cell><cell>-</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>0</cell><cell>-</cell><cell>2.9</cell><cell cols="2">2 2,158</cell><cell>0.5</cell><cell>5</cell><cell>16</cell><cell>-</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>0</cell><cell>-</cell><cell>6.4</cell><cell>0</cell><cell>-</cell><cell>4.3</cell><cell>5</cell><cell>140</cell><cell>-</cell></row><row><cell></cell><cell>0.1</cell><cell>500</cell><cell>0</cell><cell>-</cell><cell>2.5</cell><cell cols="2">5 1,151</cell><cell cols="2">-5</cell><cell>7</cell><cell>-</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>0</cell><cell>-</cell><cell>9.5</cell><cell>0</cell><cell>-</cell><cell>6.5</cell><cell>5</cell><cell>362</cell><cell>-</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>0</cell><cell>-</cell><cell>16.8</cell><cell>0</cell><cell>-</cell><cell>12.3</cell><cell cols="2">3 2,182</cell><cell>0.1</cell></row><row><cell cols="2">(10, 20) 0.05</cell><cell>500</cell><cell>4</cell><cell>943</cell><cell>0.6</cell><cell>5</cell><cell>674</cell><cell cols="2">-5</cell><cell>19</cell><cell>-</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>0</cell><cell>-</cell><cell>16.5</cell><cell>0</cell><cell>-</cell><cell>3.5</cell><cell>5</cell><cell>235</cell><cell>-</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>0</cell><cell>-</cell><cell>20.1</cell><cell>0</cell><cell>-</cell><cell>9.0</cell><cell cols="2">2 1,436</cell><cell>0.3</cell></row><row><cell></cell><cell>0.1</cell><cell>500</cell><cell>0</cell><cell>-</cell><cell>1.3</cell><cell>0</cell><cell>-</cell><cell>2.1</cell><cell>5</cell><cell>92</cell><cell>-</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>0</cell><cell>-</cell><cell>23.6</cell><cell>0</cell><cell>-</cell><cell>13.8</cell><cell>1</cell><cell>489</cell><cell>0.4</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>0</cell><cell>-</cell><cell>31.7</cell><cell>0</cell><cell>-</cell><cell>15.7</cell><cell>0</cell><cell>-</cell><cell>0.7</cell></row><row><cell cols="2">(20, 30) 0.05</cell><cell>500</cell><cell>0</cell><cell>-</cell><cell>20.7</cell><cell cols="2">4 1,828</cell><cell>0.6</cell><cell>5</cell><cell>314</cell><cell>-</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>0</cell><cell>-</cell><cell>*26.7</cell><cell>0</cell><cell>-</cell><cell>7.2</cell><cell>0</cell><cell>-</cell><cell>0.6</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>0</cell><cell>-</cell><cell cols="2">* 0</cell><cell>-</cell><cell>14.5</cell><cell>0</cell><cell>-</cell><cell>1.4</cell></row><row><cell></cell><cell>0.1</cell><cell>500</cell><cell>0</cell><cell>-</cell><cell>*26.5</cell><cell>0</cell><cell>-</cell><cell>7.0</cell><cell cols="2">3 1,245</cell><cell>0.3</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>0</cell><cell>-</cell><cell cols="2">* 0</cell><cell>-</cell><cell>16.9</cell><cell>0</cell><cell>-</cell><cell>1.7</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>0</cell><cell>-</cell><cell cols="2">* 0</cell><cell>-</cell><cell cols="2">20.9 0</cell><cell>-</cell><cell>1.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Root gaps and times for decomposition approaches for instances with n = 20, m = 30</figDesc><table><row><cell>Random</cell><cell>N</cell><cell></cell><cell>Basic Decomp</cell><cell></cell><cell>Strong Decomp</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Root gap (%)</cell><cell>Root time</cell><cell>Root gap (%)</cell><cell>Root time</cell></row><row><cell>λ, ρ</cell><cell>0.05</cell><cell>500</cell><cell>16.5</cell><cell>33.4</cell><cell>1 . 4</cell><cell>5 3 .3</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>16.6</cell><cell>92.4</cell><cell>1.6</cell><cell>168.9</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>16.7</cell><cell>126.3</cell><cell>2.1</cell><cell>231.0</cell></row><row><cell></cell><cell>0.1</cell><cell>500</cell><cell>20.0</cell><cell>45.1</cell><cell>1 . 4</cell><cell>7 3 .8</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>20.1</cell><cell>88.5</cell><cell>2.5</cell><cell>165.2</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>20.1</cell><cell>125.2</cell><cell>2.8</cell><cell>228.9</cell></row><row><cell>λ, ρ, μ</cell><cell>0.05</cell><cell>500</cell><cell>16.2</cell><cell>79.4</cell><cell>1.6</cell><cell>166.8</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>16.5</cell><cell>177.4</cell><cell>2.2</cell><cell>254.1</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>16.5</cell><cell>265.0</cell><cell>2.6</cell><cell>285.0</cell></row><row><cell></cell><cell>0.1</cell><cell>500</cell><cell>19.8</cell><cell>96.0</cell><cell>2.0</cell><cell>140.8</cell></row><row><cell></cell><cell cols="2">1,000</cell><cell>20.1</cell><cell>198.7</cell><cell>3.1</cell><cell>231.0</cell></row><row><cell></cell><cell cols="2">1,500</cell><cell>19.8</cell><cell>261.9</cell><cell>3.1</cell><cell>246.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Impact of warm-start strategies for computing an efficient frontier</figDesc><table><row><cell>Random</cell><cell>(n, m)</cell><cell>N</cell><cell cols="2">Without warm-start</cell><cell cols="2">With warm-start</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Time</cell><cell>Nodes</cell><cell>Time</cell><cell>Nodes</cell></row><row><cell>λ</cell><cell>(40, 50)</cell><cell>3,000</cell><cell>223.6</cell><cell>180.0</cell><cell>7 1 .3</cell><cell>4 .0</cell></row><row><cell></cell><cell>(50, 100)</cell><cell>3,000</cell><cell>1,279.8</cell><cell>129.8</cell><cell>291.6</cell><cell>8 .0</cell></row><row><cell>λ, ρ</cell><cell>(5, 10)</cell><cell>500</cell><cell>83.6</cell><cell>6 ,297.2</cell><cell>4 7 .3</cell><cell>6 ,059.0</cell></row><row><cell></cell><cell>(10, 20)</cell><cell>500</cell><cell>510.2</cell><cell>10,843.6</cell><cell>329.6</cell><cell>1 0 ,953.0</cell></row><row><cell>λ, ρ, μ</cell><cell>(5, 10)</cell><cell>500</cell><cell>160.3</cell><cell>12,563.6</cell><cell>8 7 .9</cell><cell>1 0 ,708.0</cell></row><row><cell></cell><cell>(10, 20)</cell><cell>500</cell><cell>2,635.4</cell><cell>27,805.6</cell><cell>1 ,133.6</cell><cell>2 1 ,553.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The extension to more general discrete distributions of the form P{ω = ω k } = p k , where p k ≥ 0 and k p k = 1, is straightforward and is omitted to simplify exposition.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author thanks Shabbir Ahmed for the suggestion to compare the presented approach with a basic decomposition algorithm. The author also thanks the anonymous referees for helpful comments.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research has been supported by NSF under grant CMMI-0952907.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A sample approximation approach for optimization with probabilistic constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luedtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="674" to="699" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Uncertain convex programs: randomized solutions and confidence levels</title>
		<author>
			<persName><forename type="first">G</forename><surname>Calafiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="25" to="46" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The scenario approach to robust control design</title>
		<author>
			<persName><forename type="first">G</forename><surname>Calafiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="742" to="753" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scenario approximation of chance constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic and Randomized Methods for Design Under Uncertainty</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Calafiore</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Dabbene</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="3" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A sampling-and-discarding approach to chance-constrained optimization: feasibility and optimality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Campi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="257" to="280" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representability in mixed integer programming, I: characterization results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jeroslow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discr. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="223" to="243" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic programming with discrete distributions and precedence constrained knapsack polyhedra</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruszczyński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="195" to="215" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding optimal vaccination strategies under parameter uncertainty using stochastic programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sattenspiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ntaimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Biosci</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page" from="144" to="151" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic programming models for air quality management</title>
		<author>
			<persName><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="651" to="663" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aquifer remediation design under uncertainty using a new chance constrained programming technique</title>
		<author>
			<persName><forename type="first">D</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eheart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valocchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water Resour. Res</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="551" to="561" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mathematical programming algorithms for two-path routing problems with reliability considerations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS J. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="553" to="564" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Branch-and-cut algorithms for chance-constrained formulations of reliable network design problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luedtke</surname></persName>
		</author>
		<ptr target="http://www.optimization-online.org" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cost horizons and certainty equivalents: an approach to stochastic programming of heating oil</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Symonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manag. Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="235" to="263" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On probabilistic constrained programmming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prékopa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Princeton Symposium on Mathematical Programming</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</editor>
		<meeting>the Princeton Symposium on Mathematical Programming<address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970">1970</date>
			<biblScope unit="page" from="113" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On distributionally robust chance-constrained linear programs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Calafiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deterministic equivalents for optimizing and satisficing under chance constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="18" to="39" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The probabilistic set-covering problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Beraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruszczyński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="956" to="967" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Concavity and efficient points of discrete distributions in probabilistic programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dentcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prékopa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruszczyński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="55" to="77" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On mixing sets arising in chance-constrained programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Küçükyavuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="31" to="56" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An integer programming approach for linear programs with probabilistic constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luedtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="247" to="272" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MIP reformulations of the probabilistic set covering problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lejeune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Regularization methods for optimization problems with probabilistic constraints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dentcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-012-0539-6</idno>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="223" to="251" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A gradient formula for linear chance constraints under gaussian distribution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Henrion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="475" to="488" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequential convex approximations to joint chance constrained programs: A Monte Carlo approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="617" to="630" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Programming under probabilistic constraints with a random technology matrix</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prékopa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Operationsforschung Statistik</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="109" to="116" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On probabilistic constraints induced by rectangular sets and multivariate normal distributions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Van Ackooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henrion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zorgati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Methods Oper. Res</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="535" to="549" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">IIS branch-and-cut for joint chance-constrained programs and application to optimal vaccine allocation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ntaimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="290" to="296" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An exact approach for solving integer problems under probabilistic constraints with random technology matrix</title>
		<author>
			<persName><forename type="first">P</forename><surname>Beraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bruni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="127" to="137" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust solutions of linear programming problems contaminated with uncertain data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="411" to="424" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The price of robustness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="35" to="53" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convex approximations of chance constrained programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="969" to="996" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ambiguous chance constrained problems and robust optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="37" to="61" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On two-stage convex chance constrained problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Meth. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="115" to="140" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An integer programming approach for linear programs with probabilistic constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luedtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nemhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPCO 2007</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Fischetti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Williamson</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="410" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to Stochastic Programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Birge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Louveaux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">L-shaped linear programs with applications to optimal control and stochastic programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Slyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Wets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="638" to="663" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stochastic decomposition: an algorithm for two-stage stochastic linear programs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Higle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="650" to="669" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Expectation and chance-constrained models and algorithms for insuring critical paths</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manage. Sci</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1794" to="1814" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A hierarcy of relaxations between the continuous and convex hull representations for zero-one programming problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sherali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Discrete Math</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An integer programming and decomposition approach for general chance-constrained mathematical programs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luedtke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Eisenbrand</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Shepherd</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6080</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combinatorial Benders&apos; cuts for mixed-integer linear programming</title>
		<author>
			<persName><forename type="first">G</forename><surname>Codato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fischetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="756" to="766" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The mixed vertex packing problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atamtürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W P</forename><surname>Savelsbergh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="35" to="53" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mixing mixed-integer inequalities</title>
		<author>
			<persName><forename type="first">O</forename><surname>Günlük</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pochet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="429" to="457" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A computational study of search strategies for mixed integer programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Linderoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savelsbergh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS J. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="173" to="187" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Finding cuts in the TSP</title>
		<author>
			<persName><forename type="first">D</forename><surname>Applegate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bixby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chvátal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cook</surname></persName>
		</author>
		<idno>95-05</idno>
	</analytic>
	<monogr>
		<title level="j">DIMACS</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Branching rules revisited</title>
		<author>
			<persName><forename type="first">T</forename><surname>Achterberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res. Lett</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="42" to="54" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Call center staffing with uncertain arrival rates: a chance-constrained optimization approach</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gurvich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luedtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tezcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manag. Sci</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1093" to="1115" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Online supplement to: a branch-and-cut decomposition algorithm for solving chanceconstrained mathematical programs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luedtke</surname></persName>
		</author>
		<ptr target="http://www.cae.wisc.edu/~luedtkej" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An LP/NLP based branch-and-bound algorithm for convex MINLP optimization problems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Quesada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Grossmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Chem. Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="937" to="947" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Covering linear programming with violations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolsey</surname></persName>
		</author>
		<ptr target="http://www.optimization-online.org" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
