<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SQL Database Primitives for Decision Tree Classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai-Uwe</forename><surname>Sattler</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Oliver</forename><surname>Dunemann</surname></persName>
							<email>kus¢dunemann£@iti.cs.uni-magdeburg.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Magdeburg</orgName>
								<address>
									<postBox>P.O.Box 4120</postBox>
									<postCode>39016</postCode>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CIKM&apos;01</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SQL Database Primitives for Decision Tree Classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">44FABFDF93FFCEA1D7EC109D6B6EB3D3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scalable data mining in large databases is one of today's challenges to database technologies. Thus, substantial effort is dedicated to a tight coupling of database and data mining systems leading to database primitives supporting data mining tasks. In order to support a wide range of tasks and to be of general usage these primitives should be rather building blocks than implementations of specific algorithms. In this paper, we describe primitives for building and applying decision tree classifiers. Based on the analysis of available algorithms and previous work in this area we have identified operations which are useful for a number of classification algorithms. We discuss the implementation of these primitives on top of a commercial DBMS and present experimental results demonstrating the performance benefit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The integration of data mining with database systems is an emergent trend in database research and development. A tight coupling between data mining and database systems is motivated by several observations. Today, most data mining systems process data in main memory. Though this results in high performance as long as enough memory is available, it ignores the fact that most data subject of analysis has been already stored in database systems and that database systems provide powerful mechanisms for accessing, filtering and indexing data. In addition, the main-memory or noncoupling approach suffers from the drawback of limited scalability. If the data set does not fit into the available memory the performance decreases dramatically. This problem has been addressed previously by data reduction techniques like sampling, discretization and dimension reduction. In contrast, SQL-aware data mining techniques could utilize sophisticated features available in modern DBMS like management of GB data sets, parallelization, filtering and aggregation and in this way improve the scalability. Another reason for building SQL-aware data mining systems is ad-hoc mining <ref type="bibr" target="#b3">[3]</ref>, i.e., allowing to mine arbitrary query results and not only base data. So it would not be necessary to preprocess data just for applying mining operations. Instead the data set is created "on the fly".</p><p>However, a major drawback of SQL-aware data mining in today's DBMS is often poor performance. This is mainly due to the facts, that the rather simple SQL operations like join, grouping and aggregation are not sufficient for data mining. Therefore, data mining operations have to be implemented as series of SQL queries, which are treated by the DBMS normally isolated and independent from each other. In order to achieve a more efficient implementation, functionality of the data mining system should be pushed into the DBMS, allowing to utilize knowledge about the operations and their access patterns and paths. But because there are various data mining algorithms for different problems it is difficult to decide, which functionality should be integrated into the DBMS.</p><p>Based on this observation the main challenge is at first to identify data mining primitives and secondly to implement them using DBMS extension facilities, e.g. cartridges, extenders or datablades.</p><p>We can distinguish at least three levels of data mining support in DBMS:</p><p>(1) A first idea is adding new language constructs to SQL as proposed in <ref type="bibr" target="#b16">[16]</ref> for association rules.</p><p>(2) A second approach is to exploit data mining functionality implemented internally using a special API like OLE DB for Data Mining <ref type="bibr" target="#b18">[17]</ref> or user-defined types and methods as proposed for SQL/MM Part 6.</p><p>(3) Finally, a DBMS could provide special operators or primitives, which are generally useful for data mining but not implementing a particular data mining task, e.g. the AVC sets described in <ref type="bibr" target="#b9">[9]</ref>.</p><p>The advantage of approach (3) is the usefulness for a broader range of data mining functions and obviously, both the language and the API approaches could benefit from such primitives. Moreover, if we consider the complexity of the SQL-99 standard and the extent of the features currently implemented in commercial systems it should become clear, that the implementation of primitives based on available DBMS extension technologies seems to be the most promising approach.</p><p>In this paper we present results of our work on database primitives for decision tree classifiers. Classification is an important problem in data mining and well studied in the literature. Furthermore, there are proposals for classifier operations, e.g. <ref type="bibr" target="#b9">[9]</ref>, which form the basis for our work. We extend the idea of computing AVC groups or CC tables respectively to implement a SQL operator for a commercial DBMS. We evaluate the benefit of multi-dimensional hashing for speeding up partial-match queries, which are typical queries in decision tree construction. Finally, we discuss the imple-mentation of prediction joins -operators for applying an induced classification model on new data.</p><p>The remainder of this paper is organized as follows: In section 2 we review the problem of decision tree classification, describe previous work and identify potential functions for database primitives. Section 3 describes these primitives in detail and discusses their implementation using the Oracle DBMS. In section 4 we report results of the evaluation of this implementation. Finally, we present related work in section 5 and conclude in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DECISION TREE CLASSIFICATION</head><p>Classification is an important data mining problem that has been studied extensively over the years. So, several classification models have been proposed, e.g. bayesian classification, neural networks, regression and decision trees. Decision tree classification is probably the most popular model, because it is simple and easy to understand.</p><p>A number of algorithms for constructing decision trees are available including ID3, C4.5, SPRINT, SLIQ, and PUBLIC. Most decision tree algorithms follow a greedy approach, that can be described as follows <ref type="bibr" target="#b2">[2]</ref> (Fig. <ref type="figure" target="#fig_0">1</ref>). In the tree-growing phase the algorithm starts with the whole data set at the root node. The data set is partitioned according to a splitting criterion into subsets. This procedure is repeated recursively for each subset until each subset contains only members belonging to the same class or is sufficiently small. In the second phase -the tree-pruning phase -the full grown tree is cut back to prevent over-fitting and to improve the accuracy of the tree. An important approach to pruning is based on the minimum description length (MDL) principle <ref type="bibr" target="#b15">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>procedure BUILDTREE (data set )</head><p>if all records in belong to the same class return foreach attribute ¡ £¢ evaluate splits on attribute ¡ ¤¢ use best split found to partition into ¦¥ and During the tree-growing phase the splitting criterion is determined by choosing the attribute that will best separate the remaining samples of the nodes partition into individual classes. This attribute becomes the decision attribute at the node. Using this attribute ¡ a splitting criterion for partitioning the data is defined, which is either of the form ¡ VU XW Y CW acb d$ H¡ ¤ ) for numeric attributes or ¡ e8f (f hg ib d$ H¡ p ) for categorical attributes. For selecting the best split point several measures were proposed, e.g. ID3 and C4.5 select the split that minimizes the information entropy of the partitions, while SLIQ and SPRINT use the gini index.</p><formula xml:id="formula_0">¨ § BUILDTREE ( ¥ ) BUILDTREE ( ¨ § ) procedure PRUNETREE (node © ) if © is leaf return "! # £$ &amp;% (' )¥ := PRUNETREE (© 0¥ ) "! # £$ &amp;% (' § := PRUNETREE (© § ) "! # £$ &amp;% (' 21 := 3 04 65 7 7 8 &amp;9 A@ CB ED ¢ GF H© I P Q! H# £$ R% (' )¥ S Q! H# £$ R% (' 2 § T return Q! H# £$ R% (' 21</formula><p>For a data set containing # records the information entropy q r is defined as q 0 s ut wv yx ¢ &amp; 6 &amp; § ¢ where ¢ is the relative fre- quency of class ! . For a split dividing into the subsets ¥ and 7 § the entropy is q r ¥ (9 7 § T t d q 0 ¦¥ ) w &amp; q 0 7 § E . The gini index for a data set is defined as ! # ! st uv x § ¢ and for a split ! H# 7! @ CB ED ¢ F ) ¦t w R ! H# ! ¦¥ R ! H# ! ¨ § T . Once an attribute is associated with a node, it needs not be considered in the node's children.</p><p>The most time-consuming part of decision tree construction is obviously the splitting point selection. For each active node the subset of data (a partition) fulfilling the conjunction of the splitting conditions of the node and its predecessors has to be constructed and for each remaining attribute the possible splits have to be evaluated. Though selecting the best split point based on the measures described above requires no access to the data itself, but only to statistics about the number of records where a combination of attribute value and class label occurs. This information can be obtained from a simple table consisting of the columns attrib-name, attrib-value, class-label and count. This structure is described in <ref type="bibr" target="#b4">[4]</ref> as CC table and in a similar form as AVC group (Attribute-Value-Class) in <ref type="bibr" target="#b9">[9]</ref>. It could be created using a SQL query of the following kind <ref type="bibr" target="#b4">[4]</ref>: The optimizers of most database systems are usually not able to construct a plan consisting of only a single scan: typically at least for each grouping a separate scan is required. Thus computing the statistics table in a single scan would be a good candidate for a classification primitive as already observed in <ref type="bibr" target="#b4">[4]</ref>.</p><formula xml:id="formula_1">select 'A1'</formula><p>An alternative approach would be a primitive that returns directly the split criterion. However, as already mentioned the individual classification algorithms differ in the measure used for choosing the split. Thus, a primitive which computes only the necessary statistics supports a wider range of algorithms than a specialized primitive.</p><p>Considering the queries for selecting the examples belonging to a partition of a node another observation holds. These queries are typically partial-match queries with a condition of the form: P¥ d § d fe Ee (e Rd g h where ¢ is a predicate ¡ Ai Tj W k9 lj a`¡ U p9 m p9 t n9 Ro t £ or ¡ i pf and rq # , where # is the number of attributes. For large data sets where a full scan is too expensive an appropriate access path (index) is required for an efficient evaluation. However, simple one-dimensional indexes are not very useful, because apart from the root node and its direct children all other nodes require multi-dimensional selections. Thus, a second potential primitive for classification is a filtering operation for obtaining the partition of a node by implementing a partial-match query, possibly based on a special index structure. The node statistics primitive could benefit from such a operation because the statistics is computed only for the active partition (Fig. <ref type="figure" target="#fig_1">2</ref>). In order to prevent over-fitting of the training data, the tree from the growing phase is pruned by applying the minimum description length (MDL) principle. The basic ideas is that the tree is the best one, which can be encoded using the smallest number of bits. The cost of encoding a tree -called MDL-cost -is computed from ( <ref type="bibr" target="#b15">[15]</ref>) s the cost of encoding the structure of the tree, e.g. for a binary </p><formula xml:id="formula_2">¡ £¢ ¡ £¤ ¡ ¦¥ ¡ ¨ § ¡ ©¢ ¡ £¢ ¡ ¨ § ¡ £¢ ¡ ¦¢ ¡ ¨ § ¡ ¤ ¡ ¨ §</formula><formula xml:id="formula_3">t &amp; ¢ # ¢ 6 § # # ¢ (' v p ! 6 &amp; § # ! 6 &amp; § 0) 21 3 54 §</formula><p>where is the set of # records belonging to one of ' classes and # ¢ is the number of records with class label ! .</p><p>The MDL-pruning is performed by traversing the tree bottom-up and pruning the children of a node © if the cost of minimum-cost subtree rooted at © is greater than or equal to the cost of encod- ing the records directly at © . The cost of a subtree can be recursively computed. Thus, the most expensive operation during the pruning phase is to compute the cost s , which requires infor- mation about the number of records belonging to the individual classes in the active partition.</p><p>A further task in classification addressed in our work is prediction -applying the induced mining model on new data. This operation is sometimes called prediction join because the attribute values of the new data (the source) are matched with the possible cases from the model. However, this is not the standard relational join for the following reasons:</p><p>s The assignment of class labels to leaf nodes is based on sta- tistical estimations obtained from the training set. Thus, the predicted classes for a given case are annotated by additional statistics, like the probability, which is derived from the training data. In most cases, the prediction is not a single value, but rather a nested table containing classes and probabilities.</p><p>s In case of numeric attributes a split is performed by defin- ing a condition of the form ¡ U W for binary splits or W ¥ q ¡ q W &amp; § for n-ary splits. During the prediction join the corresponding bucket for the attribute value of the source data has to be found.</p><p>s In order to treat missing values in the source data correctly, aggregating statistics like occurrence frequencies are necessary.</p><p>Finally, the implementation of the prediction join depends heavily on the model representation. For example, the decision tree could be represented by its nodes and edges as well as the associated conditions and classes or by materializing the individual combinations of attribute values in the nodes (or more precisely the splitting points) together with the predicted class label. Given a particular model presentation a prediction join operator is a further important classification primitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRIMITIVES</head><p>In the previous section we have identified several candidates of primitives for building and applying decision tree classifiers. In this section we describe now these primitives in more detail and present our implementation based on Oracle8i.</p><p>Let us first consider the filtering primitive supporting partial match queries. Let © ¢ 9 76 q ! Iq h# be nodes of a decision tree 8 with © @9 as the root node. With each node © ¢ 29 ! BA a split con- dition is associated in form of a predicate 1 DC either as ¡ ¢ t eW or as ¡ £¢ 0f . In addition, we assign to each node a class label ' 1 EC that is determined by selecting the most frequent class in the partition of the node. We denote a sequence © F9 E© ¥ EG HG HG © @I 9 QP q # a decision path, if the following holds for every ! 9 ! BA SR d© ¢ is a di- rect descendant of © ¢ UT ¥ in tree 8 . Each decision path to a node © B implies a conjunctive condition Cond 1 WV t 1 d e Ee (e Gd P 1 XV , where each attribute ¡ £¢ in the predicates occurs at most once. With these definitions we are able to specify the purpose of the filter primitive:</p><formula xml:id="formula_4">FILTERPARTITION INPUT: a condition Cond 1 WV a data set OUTPUT: a partition B g B t `Y Cond a 1 XV $b</formula><p>In principle, several approaches are possible for implementing this filter primitive: multi-dimensional indexes like KdB-tree and others, grid files or bitmap indexes. After some experiments with bitmap indexes supported by Oracle (see section 4 for results), we decided to implement a different approach: multi-dimensional hashing (MDH) <ref type="bibr" target="#b6">[6]</ref>. MDH is based on linear hashing, where hash values are bit vectors computed for each attribute. These bit vectors are composed to a single value using bit-interleaving (Fig. <ref type="figure" target="#fig_2">3</ref>). Let q gt rq ¥ 9 G sG HG 9 q `t ht b d$ H¡ ¥ vu e (e Ee Xu gb d$ H¡ a multi-dimensional value, and w t x ¢ yx X9 ¢ b d$ H¡ £¢ ¢ the number of bits required for encoding the final vector, the composition function rq is defined as follows:</p><formula xml:id="formula_5">rq ¦t &amp; ¢ x X9 q a ¢ r I b y ¥ 3 ! ¢ y I ¥ v q a ¢ r I b y ¥ 3 ! ¢ y I ! ¢ y I ! ¢</formula><p>As for other hashing schemes the complexity for exact-match queries using MDH is 2 T . For partial-match queries it depends on the number of unknown dimensions (attribute values). Let b be the dimensionality and the number of unknown attribute values, then the complexity is # ¥ T d . Partial-match queries are imple- mented by setting the corresponding bits in the composed bit vector. For unknown attribute values all combinations of the possible values have to be considered. Therefore, in this case the hash function produces actually a set of hash values.</p><p>MDH can be implemented on top of a commercial DBMS in different ways. Probably the most efficient implementation would base on an extensible indexing API as available in Oracle8 or IBM DB2. Here, several routines for index maintenance (insert, delete, update tuples) as well as scan routines (start, fetch, close) are provided by the implementor. In addition, an operator (a SQL function) utilizing the index is defined. This operator could now be used in a query like this: select * from data where mdh match (a1, 1, a2, 2, a3, 0, a4, -1, a5, -1) = 1</p><p>In this example the arguments for the function mdh match are the attribute values, where v p denotes an unknown attribute. Thus, the above query is in fact a rewrite of:</p><p>select * from data where a1 = 1 and a2 = 2 and a3 = 0</p><p>However, due to the complexity of the extensible indexing API we have chosen a different implementation strategy for first experiments, which is -to some extent -more portable to other DBMS. The hash function is implemented as a table function mdh partial match returning a table of hash values for a partial match query. For the data table an additional column hid for storing the hash value derived from the attributes is required. In this way, a join on the values returned by the hash function and the hash values stored in the data table can be performed in order to select the affected tuples. Obviously, an index is required on column hid for fast access. Thus, the above query is now formulated as follows: s For each tuple the hash value has to be computed in advance, e.g. this could be easily done using a trigger. s In the above described form only conditions of kind ¡ t W k9 W rQb d$ H¡ ¤ are supported.</p><formula xml:id="formula_6">select * from table (mdh partial match (1, 2, 0, -1, -<label>1</label></formula><p>Whereas the first limitation is inherent to the approach, the third point can be handled by the following extension. For all values of the domain of an attribute an ordering is defined in order to be able to encode each value by a single bit position in a bit vector. Thus, for a condition like ¡ rVf 9 f g b d$ H¡ p the set f of values are encoded by bit-wise disjunction of all values W c8f as given by: w gt x # ¡ ¢ ! # . If now the individual values w ¤£ G HG HG w ¥£ §¦ are used as arguments for the function mdh partial match, the condition is supported as well. We evaluate the performance gain of the MDH-based filter operation in section 4.</p><p>Filtering the partition associated with a node of the decision tree is only the first part of determining splitting attribute and condition. Both the information entropy and the gini index can be computed from a table summarizing the number of tuples for each combination of an attribute value and a class label. Here, all attributes of the data set not already used as splitting attributes are examined. Therefore, for the partially grown tree shown in Fig. <ref type="figure" target="#fig_3">4</ref>(a) and a data set of schema ¨ H¡ ¥ 9 G HG G 9 l¡ 9 l at node © © the attributes ¡ © G HG HG ¡ have to be considered. Assuming a remaining partition of the data set at © © as given in Fig. <ref type="figure" target="#fig_3">4</ref>(b) the resulting table contains the infor- mation shown in Fig. <ref type="figure" target="#fig_3">4(c)</ref>. </p><formula xml:id="formula_7">PSfrag replacements ¡ ¦¢ ¡ § ¡ ¦¢ ¡ ¥ ¡ ¦¢ ¡ § (a) Decision tree © 0 1 1 ¥ 0 1 1 ¥ 1 1 2 ¥ 2 0 1 § (b) Partition for © © aname avalue class count © 0 ¥ 2 © 1 ¥ 1 © 2 § 1 1 ¥ 3 0 § 1 1 ¥ 2 1 § 1 2 ¥ 1 (c) Node statistics for © ©</formula><formula xml:id="formula_8">H¡ £¢ 29 W k9 ' 9 ' $ # ' l PÌ Stat 4 65 ¡ ¢ 7" 8d W r`f 8d ' ` V d ' $ # ' t ¢ ¡ ' ¢ ' Aỳ B d "' H¡ ¢ t 8W £d "' H¡ % t ' £ ¢</formula><p>One efficient approach for building this table is to use the super group features introduced in SQL-99 and for example supported by DB2 Version 7. As part of an extended group by clause a list of attribute combinations for creating groups is provided. Thus, the query select case when grouping(A3) then 'A3' end when grouping(A4) then 'A4' end when grouping(A5) then 'A5' end else NULL end as aname, case when grouping(A3) then A3 end when grouping(A4) then A4 end when grouping(A5) then A5 end else NULL end as value, C, count(*) from R where condition group by grouping sets ((A3, C), (A4, C), (A5, C), (C))</p><p>returns the result table in Fig. <ref type="figure" target="#fig_3">4(c</ref>). An alternative approach is necessary if the advanced grouping features is not supported as in case of Oracle8. We have implemented this feature for Oracle as a table <ref type="table">function</ref>  In the function avc group the given query is evaluated and the result is processed. The attribute-class-count combinations are collected in a three-dimensional array indexed by attribute name, value and class label. The count values are stored in the fields itself. The cardinalities of the three dimensions could be estimated in advance from the table statistics. In very most cases, this array is small enough to fit into main memory, e.g. for 20 attributes with at most 10 distinct values and 10 classes the size is ! $6 6 H6 ¡ Bytes ¢ £ KBytes. After processing the whole result set and collecting the counts the array is used to build the resulting node statistics table, which is finally returned to the caller (Fig. <ref type="figure" target="#fig_4">5</ref>).</p><p>We will show results of the performance evaluation of the implementation of this primitive in section 4.</p><p>As mentioned in section 2 computing the MDL-cost of a subtree is a further candidate for a primitive. In particular, cost of encoding records involves the number of classes, the number of records belonging to an individual class and the total number of records associated with the active node. Obviously, this can be easily combined with the COMPUTENODESTATISTICS, so that both statistics are obtained in one scan. We omit the details here, because of the straight-forward implementation. The cost value computed for a node is stored along with the node in the tree. Based on this, the procedure COMPUTENODESTATISTICS (query ¤ , attribute set " , class attribute ) initialize array ' $ # ' execute given query ¤ foreach tuple ' ¦t rq Y¥ E9 G HG HG 9 q 9 ' foreach attribute ¡ ¥ 9 G HG HG 9 ¡ `"</p><formula xml:id="formula_9">count[¡ £¢ ][q ¢ ][ ' ] += 1 foreach attribute ¡ £¢ foreach value W 0ỳb d$ H¡ ¢ foreach class label ' Qb d$ if ' $ $ # ' ¦¥ ¡ ¢ ¨ § ¥ W § ¥ ' § m 6</formula><p>produce tuple H¡ £¢ 29 W k9 ' 9 ' $ # ' ©¥ ¡ £¢ § ¥ W § ¥ ' § total MDL cost for each subtree are easily computable during the pruning phase and the pruning can be performed.</p><p>After building the decision tree, the induced model can be used to predict the class of new data records, where the class attribute is missing. For this operation -called prediction join -the model has to be interpreted, i.e. by following a path of the tree where for each node the associated split condition is evaluated. We can define the semantics of this operation as follows:</p><formula xml:id="formula_10">PREDICTIONJOIN INPUT:</formula><p>a decision tree 8 of nodes © 9 G sG HG © a source relation ¨ H¡ ¥ 9 G sG HG 9 l¡ h OUTPUT: a relation of predictions</p><formula xml:id="formula_11">H¡ ¥ 9 G HG G ¡ h 9 l¡ % It holds: Y' s`¨ ' `¨ R there is a path © 9 © 0¥ G HG HG © I 9 P q d £P is maximal d ! 9 ! t G HG HG P ©R &amp;' ) H¡ £¢ ¦t 8' B H¡ £¢ d £$ # b 1 WV k C' ¦t ' $ Ad ' t ' 1 C</formula><p>Because the implementation of the prediction join depends on the model representation, an appropriate structure is required. There is a standard proposal for representing classification tree defined as part of the Predictive Model Markup Language (PMML) <ref type="bibr" target="#b10">[10]</ref>. However, for storing the tree in a RDBMS a flat table structure is necessary. In Fig. <ref type="figure">6</ref> a possible structure is shown together with the corresponding tree. Each tuple in the table represents an edge of the tree from node parent to node node. Each edge is associated with a condition, where the attribute name is stored in the field attrib and the domain for the split is represented by the values of the fields minval and maxval. The field class holds the label of the most frequent class in the partition associated with the node node of the edge together with the probability prob of the class occurrence. Like the other primitives the prediction join is implemented as a table function. Probably, the best solution would be a function which accepts tables as parameters. But due to the lack of support in current DBMS we have chosen the same approach as already presented for the avc group function: the source and model tables are passed as query strings. A second restriction is the strong typing of the function result. It is not possible to construct a table with a schema that depends on the schemas of the input tables. So, a simple solution is to return a table of a fixed type, e.g. consisting of the primary key of the source table -which is specified as an additional parameter of the function -and the predicted class label. The following example shows the usage of the prediction opera-replacements The pseudo-code for the prediction join is given in Fig. <ref type="figure" target="#fig_6">7</ref>. We assume a model representation as described above (Fig. <ref type="figure">6</ref>). For each tuple ' t h rq ¥ 9 G sG G 9 q h p of the source relation the nodes are se- lected, whose condition is fulfilled by the attribute values of the given tuple. This is performed by the following query ¤ C' Y : These candidate nodes are ordered by their node-id. Next, the candidate nodes are processed in this order as follows: Starting with the root node the next node with a parent-id equal to current nodeid is obtained until no further node can be found. In this case, the class and probability values are assigned to the active source tuple. This approach reduces the total number of nodes, which have to be examined.</p><formula xml:id="formula_12">¡ ¢ ¢ ¡£ ¢ ¢ ¡ ¦¢ ¡ § ¥¤ ¦ § ¦ § ¦ § (a) Decision tree node parent attrib minval maxval class prob ¨9 © © © © ¥ 0.70 ¨¥ ¨9 ¥ MIN 50 ¥ 0.86 ¨ § ¨9 ¥ 50 MAX § 0.66 ¨© ¨¥ § -1 0 ¥ 0.98 ¨ ¨¥ § 0 1 § 0.60 (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PERFORMANCE EVALUATION</head><p>In order to evaluate the performance of our primitives compared to pure SQL queries we performed several experiments. For all tests we used an Oracle8i DBMS Rel. 8.1.6 running on a Pentiu-mIII/500MHz Linux machine with 512 MB of RAM. The primitives were implemented in C as user-defined table functions using the Oracle Call Interface (OCI). The synthetic test data sets were constructed as tables with 10 and 20 attributes and different numbers of tuples up to 100,000. Each attribute contained only discrete values of the interval 6 G HG HG . For the tests of the MDH-based imple- mentation a further attribute containing the hash values was added to the tables. The hash values were computed in advance and an index was created on this attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>procedure PREDICTIONJOIN (source table , model table )</head><p>foreach tuple ' rt X rq ¥ 9 G HG sG 9 q h p sÌ</p><formula xml:id="formula_13">execute query ¤ C' fetch tuple ' Vt X # 9 C S9 ' 9 C $ $w E # $ b SR t # ' q % T% SR t ' ! # ! % Eb R t false do do</formula><p>fetch tuple ' t X # 9 H ¨9 ' q % T% &amp;9 C $ w T if tuple not found produce result tuple rq ¥ 9 G HG HG 9 q h 9 ' ! H# ! % Eb R t true while fo t # $ b # $ b @R t # ' q % T% SR t ' while ! H# 7! % Eb In the first experiment we studied the performance of different strategies for partial-match queries: a simple full table scan, the usage of bitmap indexes and the MDH-based primitive as described in Section 3.</p><p>Fig. <ref type="figure" target="#fig_7">8</ref> shows the elapsed times (for 100 queries) of these strategies for a table of 100,000 tuples with different numbers of undefined attributes. Here, the value 0 at the x axis corresponds to an exact-match query, the value 10 means a full scan in the case of a table consisting of 10 attributes.</p><p>If all or nearly all attributes are given in the query, the MDH approach needs approximately 1.2 seconds for 100 queries, while the scan needs 17 and the access via bitmap indexes needs 19.5 seconds. As expected, the elapsed times for the table scans are nearly constant with the growing number of undefined attributes. The MDH algorithm produces better times for up to 6 undefined attributes. This may be caused by our implementation, because we rely on a traditional B-tree index to find the tuples belonging to the calculated hash values. A second reason could be the inefficient way of returning the table of hash values from the table function, because the table is constructed first and then returned instead of using a iterator-like interface. The difference between the table scan and the bitmap index supported search was of almost no importance in these cases. One conclusion is that the influence of the total number of attributes on the behavior of MDH is not significant. The main factor is the number of unspecified attributes. All test cases produced an excellent performance as long as there are not more than 5 undefined attributes, which corresponds to a selectivity of approximately 50%. However, we believe that a more efficient implementation, e.g. based on the extensible indexing API, could improve this factor.</p><p>In the second experiment we evaluated two strategies for computing node statistics: The UNIONALL approach (see Section 2) and the AVCGROUP primitive. We used the same data sets as described above and compared the running times of queries with different number of grouped attributes. In each query the attributes not involved in grouping were used in the selection condition. As shown in Fig. <ref type="figure" target="#fig_8">9</ref> the running times (again for 100 queries) increased nearly linear with the number of dimensions to be grouped in the case of using UNIONALL. For all possible numbers of groups the AVCGROUP strategy led to faster execution. In the third experiment we studied the effects of the two primitives on a complete decision tree algorithm. For this purpose, we implemented the ID3 algorithm in C++ using SQL queries for filtering the partitions of each node and computing the node statistics. So, no training data has to be held in memory, only the tree is constructed as main-memory structure. Furthermore, no pruning was performed in our implementation and we considered only categorical attributes. We generated synthetic data sets using the data generator from the IBM Quest project 1 and discretized the numeric values into 4 distinct categorical values. The experiments were performed with data sets of 3,000 up to 50,000 tuples and 9 attributes. We evaluated 4 different strategies of the classifier: SCAN is implemented using pure SQL where the entropy for each attribute is computed by a separate query. For MDH the entropy is computed in the same way (by a separate query), but the filtering of partitions is performed by the MDH primitive if the number of unspecified attributes as less than 5. The UNION strategy is based on the COMPUTENODESTATISTICS primitive, but uses the UNIONALL approach. Finally, the AVC strategy computes the node statistics using the avc group function. Fig. <ref type="figure" target="#fig_9">10</ref> shows the elapsed times for inducing a decision tree from a training set of 50,000 tuples. Both implementations based on the primitives provide a significant performance improvement compared with their pure SQL counterparts. Because in both strategies UNION and AVC a scan-based filtering is performed a further improvement could be achieved by using MDH for filtering the partitions in AVC. However, we were not able to evaluate this promising strategy due to the limitations of the current release of the Oracle </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>Many approaches have been proposed to constructing decision tree classifiers in general and particularly to improving scalability of classification. Most well-known algorithm like CART <ref type="bibr" target="#b2">[2]</ref>, ID3 <ref type="bibr" target="#b20">[19]</ref>, C4.5 <ref type="bibr" target="#b21">[20]</ref> assume the data to be in memory and therefore are able to work only with relative small data sets efficiently. In the database research scalability is addressed by developing algorithms based on special data structures, e.g. SPRINT <ref type="bibr" target="#b24">[23]</ref>, SLIQ <ref type="bibr" target="#b14">[14]</ref> or optimistic tree construction <ref type="bibr" target="#b8">[8]</ref>. In <ref type="bibr" target="#b9">[9]</ref> the RainForest framework is described that introduces an AVC-group data structure providing sufficient statistics for determining the split and algorithms for constructing this structure. <ref type="bibr" target="#b4">[4]</ref> describes a similar data structure called CC table and a middleware based on a scheduler ensuring optimized scans and staging. Whereas the RainForest framework does not address SQL databases, the middleware is implemented on a commercial DBMS. Our COMPUTENODESTATICTICS is derived directly from this both approaches. Other approaches consider approximation techniques for scaling up the classification, e.g. sampling <ref type="bibr" target="#b1">[1]</ref> and discretization, as well as permitting the user to specify constraints on tree size <ref type="bibr">[7]</ref>. Particularly, approximation techniques could be supported by the database systems very well and thus could lead to further primitives.</p><p>The integration of data mining and database systems resulting in SQL-aware data mining systems is discussed in <ref type="bibr" target="#b3">[3]</ref>. This paper argues for identifying and unbundling a set of new SQL operators or primitives from data mining procedures implementing individual algorithms. Our approach of primitives follows this idea.</p><p>Some further examples of tightly coupled data mining techniques are presented in <ref type="bibr" target="#b22">[21]</ref> for association rules and in <ref type="bibr" target="#b19">[18]</ref> for clustering. Particularly, for the problem of finding association rules several primitives has been identified and implemented as table functions in DB2. Because of this similar approach we beware that supporting table functions and/or table operators <ref type="bibr" target="#b13">[13]</ref> is an important issue for implementing data mining primitives efficiently. An ideal supplement to this kind of extension mechanism are userdefined aggregates. An example of a powerful framework for building aggregates and the application in data mining is presented in <ref type="bibr" target="#b26">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>Tight coupling of data mining and database systems is -beside improving data mining algorithms -a key issue for efficient and scalable data mining in large databases. Tight coupling means not only to link specific data mining algorithms to the database system <ref type="bibr" target="#b11">[11]</ref>, e.g. as stored procedures, but rather that essential data mining primitives supporting several classes of algorithms are provided by the DBMS. Two important tasks convey the development of such kind of primitives: (1) analyzing data mining functions and identifying common primitives and (2) providing extension mechanisms for an efficient implementation of these primitives as part of the database system API.</p><p>In this paper we have presented first results of our work on primitives for decision tree classification. These primitives implement special database operations, which support the SQL-aware implementation of a wide range of classification algorithms. Based on the primitives additional operations are possible. An example are operations for computing the splitting measures (gini index, information entropy), which could be implemented as user-defined aggregation functions for the node statistics table. Moreover, we are convinced that other data mining techniques could benefit from the described primitives as well. For example, partitioning of data sets is a common task and statistics information like node statistics are needed in various mining algorithms. Finally, data preparation as an important preprocessing step of data mining is a further application for databases primitives <ref type="bibr" target="#b23">[22]</ref>.</p><p>The experimental results have demonstrated the benefit of the primitives, but also the need for an implementation tighter integrated with the database system. Modern object-relational systems provide already some extension facilities supporting this task (userdefined table functions, user-defined aggregates, extensible indexing). However, our experiences have shown that still more advanced extension APIs are required, e.g. for implementing user-defined table operators which are able to process tables or tuple streams as input parameters. Furthermore, optimizing queries containing this kind of operators is an important but still open issue. Here, techniques considering foreign functions <ref type="bibr" target="#b5">[5]</ref> or expensive predicates <ref type="bibr" target="#b12">[12]</ref> during optimization have to be extended.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Algorithms for decision trees</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Computing node statistics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Bit interleaving in MDH</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example for node statisticsLet B g be a partition of the data set and ! 5V the set of class labels occurring in B . Furthermore, we define a set of attributes " Vt ¡ ¡ ¥ 9 G HG sG 9 l¡ £h £ , a set of values f t $# ¢ b d$ H¡ ¢ and a record ' g B , where ' H¡ ¢ denotes the value of record ' for attribute ¡ £¢ . We specify the COMPUTENODESTATISTICS primitive as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Algorithm for computing the node statistics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>¥</head><label></label><figDesc>and minval &lt; ¥ and maxval &gt;= ¥ ) or (aname= § and minval &lt; § and maxval &gt;= § ) or (aname= h and minval &lt; h and maxval &gt;= h )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Algorithm for prediction join</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: FILTERPARTITION evaluation results for 100,000 rows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: COMPUTENODESTATISTICS evaluation results for 100,000 rows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>¥Figure 10 :</head><label>10</label><figDesc>Figure 10: ID3 evaluation results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>of int; create type itable t is table of int;</head><label></label><figDesc>For Oracle, an additional cast around the function call and a named table type as result type are required. Moreover, because Oracle supports a collection type array a more generic function expecting an array of values as argument can be implemented: It works only for categorical attributes. Numeric attributes have to be discretized or excluded from indexing.</figDesc><table><row><cell>select *</cell></row><row><cell>from table (cast (mdh partial match (</cell></row><row><cell>iarray t (1, 2, 0, -1, -1)))</cell></row><row><cell>as itable t) h, data d</cell></row><row><cell>where h.id = d.hid</cell></row><row><cell>Our approach has several consequences:</cell></row></table><note><p>)) h, data d where h.id = d.hid create type iarray t as varray(30) s</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>returning a table of objects of type avc group, which is defined as follows:The function avc group requires as arguments a list of attributes which have to be examined, the attribute representing the class label and an input table. This table could be a temporary view as in the following query But because Oracle does not support temporary views and tables as function parameters a complete query string has to be given for the input table in our implementation. Thus, the query looks as follows:</figDesc><table><row><cell>create type avc group t as object (</cell></row><row><cell>aname varchar(30), avalue int,</cell></row><row><cell>class int, cnt int);</cell></row><row><cell>create type avc table t is table of avc group t;</cell></row><row><cell>with pdata as (select * from data</cell></row><row><cell>where condition)</cell></row><row><cell>select *</cell></row><row><cell>from table (avc group ((a1, a2, a3), class, pdata))</cell></row><row><cell>select *</cell></row><row><cell>from table (cast (</cell></row><row><cell>avc group (sarray t ('a1', 'a2', 'a3'), 'class',</cell></row><row><cell>'select * from data where condition')</cell></row><row><cell>as avc table t)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table representation Figure 6</head><label>representation6</label><figDesc></figDesc><table><row><cell>: Representation of a decision tree</cell></row><row><cell>tion.</cell></row><row><cell>select id, clabel</cell></row><row><cell>from table (cast (prediction join ('model',</cell></row><row><cell>'select id, a1, a2, a3 from source', 'id'))</cell></row><row><cell>as pred table t)</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was partially supported by the DFG (FOR 345/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CLOUDS: A Decision Tree Classifier for Large Datasets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Alsabti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD-98</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Stolorz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</editor>
		<meeting>KDD-98<address><addrLine>New York City; New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Chapman &amp; Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data Mining and Database Systems: Where is the Intersection?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="8" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable Classification over SQL Databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE-99</title>
		<meeting>ICDE-99<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="470" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Query Optimization in the Presence of Foreign Functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB&apos;93</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Bell</surname></persName>
		</editor>
		<meeting>VLDB&apos;93<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="529" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multidimensional access methods</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gaede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Günther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="231" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient Algorithms for Constructing Decision Trees with Constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garofalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BOAT -Optimistic Decision Tree Construction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD&apos;99</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Delis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Ghandeharizadeh</surname></persName>
		</editor>
		<meeting>ACM SIGMOD&apos;99<address><addrLine>Philadephia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RainForest -A Framework for Fast Decision Tree Construction of Large Datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB&apos;98</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Shmueli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</editor>
		<meeting>VLDB&apos;98<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="416" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Group</surname></persName>
		</author>
		<ptr target="http://www.dmg.org/html/pmmlv11.html" />
		<title level="m">Predictive Model Markup Language (PMML) 1.1, 2000</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Data Mining: Concepts and Techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Morgan Kaufman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimization Techniques for Queries with Expensive Methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TODS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="157" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">User-Defined Table Operators: Enhancing Extensibility for ORDBMS</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaedicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitschang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB&apos;99</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Atkinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Orlowska</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Brodie</surname></persName>
		</editor>
		<meeting>VLDB&apos;99<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="494" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SLIQ: A Fast Scalable Classifier for Data Mining</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EDBT&apos;96</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Apers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Bouzeghoub</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Gardarin</surname></persName>
		</editor>
		<meeting>EDBT&apos;96<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1057</biblScope>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MDL-based Decision Tree Pruning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD-95</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Uthurusamy</surname></persName>
		</editor>
		<meeting>KDD-95<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A New SQL-like Operator for Mining Association Rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Meo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Psaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ceri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB&apos;96</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Vijayaraman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Buchmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Mohan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sarda</surname></persName>
		</editor>
		<meeting>VLDB&apos;96<address><addrLine>Mumbai (Bombay), India</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="122" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integration of Data Mining with Database Technology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Netz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB 2000</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Abbadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Brodie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Chakravarthy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Dayal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Kamel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Schlageter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-Y</forename><surname>Whang</surname></persName>
		</editor>
		<meeting>VLDB 2000<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="719" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SQLEM: Fast Clustering in SQL using the EM Algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cereghini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD 2000</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Naughton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bernstein</surname></persName>
		</editor>
		<meeting>ACM SIGMOD 2000<address><addrLine>Dallas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integrating Mining with Relational Database Systems: Alternatives and Implications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD&apos;98</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Haas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Tiwary</surname></persName>
		</editor>
		<meeting>ACM SIGMOD&apos;98<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="343" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Data Preparation Framework based on a Multidatabase Language</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schallehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Database Engineering and Applications Symposium (IDEAS 2001)</title>
		<meeting>of Int. Database Engineering and Applications Symposium (IDEAS 2001)<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SPRINT: A Scalable Parallel Classifier for Data Mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB&apos;96</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Vijayaraman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Buchmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Mohan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Sarda</surname></persName>
		</editor>
		<meeting>VLDB&apos;96<address><addrLine>Mumbai (Bombay), India</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="544" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using SQL to Build New Aggregates and Extenders for Object-Relational Systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB 2000</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Abbadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Brodie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Chakravarthy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Dayal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Kamel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Schlageter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-Y</forename><surname>Whang</surname></persName>
		</editor>
		<meeting>VLDB 2000<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="166" to="175" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
