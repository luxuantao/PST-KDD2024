<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Advanced recurrent network-based hybrid acoustic models for low resource speech recognition</title>
				<funder ref="#_jeHc2Xb #_PxrZyrP">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jian</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic, Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Electronic, Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wei-Qiang</forename><surname>Zhang</surname></persName>
							<email>wqzhang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic, Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Electronic, Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei-Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Chinese People&apos;s Liberation Army</orgName>
								<address>
									<postCode>62315, 100842</postCode>
									<settlement>Unit, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic, Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Electronic, Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Johnson</surname></persName>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">College of Engineering</orgName>
								<orgName type="institution">University of Kentucky</orgName>
								<address>
									<settlement>Lexington</settlement>
									<region>KY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Advanced recurrent network-based hybrid acoustic models for low resource speech recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1186/s13636-018-0128-6</idno>
					<note type="submission">Received: 25 October 2017 Accepted: 19 June 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gated recurrent units</term>
					<term>Recurrent architectures</term>
					<term>Low resource speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) have shown an ability to model temporal dependencies. However, the problem of exploding or vanishing gradients has limited their application. In recent years, long short-term memory RNNs (LSTM RNNs) have been proposed to solve this problem and have achieved excellent results. Bidirectional LSTM (BLSTM), which uses both preceding and following context, has shown particularly good performance. However, the computational requirements of BLSTM approaches are quite heavy, even when implemented efficiently with GPU-based high performance computers. In addition, because the output of LSTM units is bounded, there is often still a vanishing gradient issue over multiple layers. The large size of LSTM networks makes them susceptible to overfitting problems. In this work, we combine local bidirectional architecture, a new recurrent unit, gated recurrent units (GRU), and residual architectures to address the above problems. Experiments are conducted on the benchmark datasets released under the IARPA Babel Program. The proposed models achieve 3 to 10% relative improvements over their corresponding DNN or LSTM baselines across seven language collections. In addition, the new models accelerate learning speed by a factor of more than 1.6 compared to conventional BLSTM models. By using these approaches, we achieve good results in the IARPA Babel Program.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic speech recognition (ASR) has undergone rapid change in recent years. Deep neural networks (DNN) combined with hidden Markov models (HMM) have become the dominant approach for acoustic modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, replacing the traditional Gaussian mixture modelhidden Markov models (GMM-HMMs) approach. Utilizing increased availability of both computational power and training data, error rates have been reduced significantly across many speech recognition tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. A wide variety of NN architectures have been introduced, each having associated advantages and disadvantages. Of all these architectures, recurrent neural networks have shown strong comparative performance. speech applications, such as voice search tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, with good performance.</p><p>Although LSTM models have achieved excellent results for large vocabulary continuous speech recognition, they still struggle when applied to certain tasks, such as training for low-resource languages.</p><p>Conventional LSTM and bidirectional LSTM (BLSTM) require complex training mechanisms which make them difficult to implement. Some of these mechanisms, such as clipping of cell activations and peephole connections, require careful tuning to a particular training set. The vanishing gradient problem across multiple layers is also a potential problem. We hope to solve these shortcomings using additional gating mechanisms that further constrain temporal dependencies and focus the training process.</p><p>In this paper, we aim at building advanced RNN structures with a hybrid acoustic model, to maximize use of prior knowledge in the speech signals. The solutions we propose include local window BLSTM, gated recurrent units, and residual architecture-based models. This work expands on our previous work <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. Experiments are carried out on the Babel benchmark datasets, for low resource keyword search evaluations. By using these techniques, we achieve 3 to 10% relative improvements over the corresponding DNN or LSTM baselines. In addition, the new models improve training time by a factor of more than 1.6 compared to conventional BLSTM models.</p><p>The remainder of the paper is organized as follows. Section 2 briefly introduces the baseline LSTM model. Section 3 describes the proposed approaches. We report our experimental results in detail in Section 4, including experimental setup, hyperparameters evaluation, and comparisons between selected datasets. Finally, conclusions and future work are outlined in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline LSTM System</head><p>We first give a brief introduction of the DNN and LSTM network structures used as a baseline. A DNN contains a series of hidden layers, which for speech applications is most commonly fully connected with sigmoid activation functions.</p><p>RNNs are a neural network framework with selfconnections from the previous time step used as inputs. This structure allows the network to capture a dynamic history of information about input feature sequences and is less affected by temporal distortion. Due to these properties, RNN have performed better than traditional DNNs in large vocabulary speech recognition tasks. Although conventional RNNs have feedback connections in the hidden layers to model temporal correlations, this structure captures short-term dependencies much better than long-term dependencies due to the vanishing and exploding gradients in the Stochastic Gradient Descent (SGD) training process <ref type="bibr" target="#b10">[11]</ref>.</p><p>The LSTM RNN topology is an advanced network structure designed to model long-term dependencies while limiting the rate of gradient decay through a gating mechanism.</p><p>LSTM units were first introduced in <ref type="bibr" target="#b11">[12]</ref>. A popular LSTM structure is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The forward pass from x t to h t follows the equations:</p><formula xml:id="formula_0">g t = ? W gx x t + W gh h t-1 + b g (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">i t = ? (W ix x t + W ih h t-1 + W ic c t-1 + b i ) (<label>2</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">f t = ? W fx x t + W fh h t-1 + W fc c t-1 + b f (<label>3</label></formula><formula xml:id="formula_5">)</formula><formula xml:id="formula_6">c t = f t ? c t-1 + i t ? g t (<label>4</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">o t = ? (W ox x t + W oh h t-1 + W oc c t + b o ) (<label>5</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">h t = o t ? ?(c t )<label>(6)</label></formula><p>Here, x t is the input, and h t-1 is the previous output of the LSTM. W * x , W * h , W * c , and b * are forward matrices, recurrent matrices, diagonal peephole connections, and biases for all gates, respectively. ? is the sigmoid function, ? is the hyperbolic tangent function, and ? denotes element-wise multiplication. For convenience, we denote the above calculations as h t = LSTM(x t , h t-1 ).</p><p>The LSTM uses gates to control information flow and effectively creates shortcut paths across multiple temporal steps. The key ideas behind the LSTM unit are the addition of a memory cell block to maintain temporal information and the use of non-linear activation gates to control both the information flow into the memory cell and the output of the unit. Each LSTM unit consists of one cell unit and four control gates. These gates control the input and output as well as the temporal extent of the memory cell through a forget gate. The memory cell itself can also directly control the gates. The LSTM units implement this by peephole connections from the memory cell to the gates to learn precise timing information.</p><p>As a hybrid acoustic model, the network is trained to predict HMM states using a forced alignment. For both networks, a softmax layer is added at the top of the recurrent layers to generate posterior possibilities. The output of the softmax layer provides an estimate of the posterior probabilities P(s|o) for states s, with given features o. The output in the softmax layer is computed by</p><formula xml:id="formula_11">P(s|o) = softmax(W s h out + b s ),<label>( 7 )</label></formula><p>where (W s , b s ) is the connection weight matrix and bias vector for the softmax layer, and h out is the output of the top recurrent layer. Inspired by DNNs, multiple LSTM layers can be stacked to build deep LSTM RNNs <ref type="bibr" target="#b8">[9]</ref>. When input features propagate through the recurrent layer, the output features at each time step incorporate the history of temporal features from previous time steps. Compared to a shallow LSTM, the features generated by a deep LSTM are more generalizable and suitable for prediction. Thus, a deep LSTM RNN takes advantage of the merits of both DNNs and conventional LSTM.</p><p>Recently, linear recurrent projection layers have been proposed for reducing the number of parameters at no loss of accuracy <ref type="bibr" target="#b15">[16]</ref>. Following this work, we use the term LSTM to denote such a deep LSTM-projected architecture and use this approach as our baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Advanced recurrent architectures and training algorithms</head><p>Although LSTM models have achieved excellent performance in speech recognition tasks, they still have some shortcomings. For example, conventional unidirectional LSTMs only use preceding context information, which limits its ability to generate future context information. Furthermore, the architectures of traditional LSTM units are complex. This large size restricts their abilities to generalize and leads to a vanishing gradient problem across multiple layers. Because of the use of temporal context, training on entire sentence-length utterances requires extremely large training times, and the decoding real time factor is also poor. The proposed architectures and algorithms are designed to substantially reduce the above shortcomings.</p><p>In this section, we extend conventional LSTM and investigate in more depth the gated recurrent unit (GRU) element introduced in our previous work <ref type="bibr" target="#b17">[18]</ref>. We also propose a new element called a residual GRU (rGRU) to alleviate the vanishing gradient problem across multiple layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bidirectional LSTM</head><p>As discussed previously, the BLSTM <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> is able to make use of both the preceding and following contexts within an utterance. The BLSTM does this by processing the data in both directions with two separate parameter sets, forward parameters and backward parameters.</p><p>The propagating output is calculated as follows:</p><formula xml:id="formula_12">-? h t = ---? LSTM(x t , h t-1 ),<label>( 8 )</label></formula><formula xml:id="formula_13">? - h t = ? --- LSTM(x t , h t+1 ), (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>where h t = -? h t , ?h t , t = 1 : L, and L is the length of training sentences. (8) uses the forward parameters, while (9) uses the backward parameters.</p><p>After propagating in both forward and backward directions, the individual directional outputs are concatenated and fed forward to the next hidden layer. This bidirectional approach enables the system to more fully take advantage of the time dependencies of the input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local window BLSTM</head><p>Although BLSTM networks often achieve better performance in speech recognition than LSTM, the latency during training and decoding is significant because it is necessary to wait until seeing a whole sentence. This makes BLSTM inappropriate for real-time online speech recognition.</p><p>However, in traditional backpropagation through time algorithm (BPTT) <ref type="bibr" target="#b23">[24]</ref>, the error signals are truncated based on the BPTT batch size parameter. This results in temporal isolation between blocks of the signal. The impact of the gradient becomes attenuated through these time steps, a process which is impacted by the batch size as well. For frames which are too far away from the current time, the impact of their gradient will be lower.</p><p>As we know, when considering the mechanism of pronunciation for human, the history phoneme information, including the shape changes of vocal cords, mouths, and noses, occupy the most impacts of sequential information. The future information play an auxiliary role, such as continuous changes of vocal organs. In addition, one of the successful key points behind the RNN is that they model the temporal relationships over phonemes. In general, the relationships between phonemes are constrained within word boundaries. The future information within word boundaries may be most useful to help to learn the current phoneme information.</p><p>Inspired by above thoughts, we consider the time dependencies within a local window. Based on this concept, in this subsection, we introduce the local window BLSTM (LW-BLSTM) approach.</p><p>Figure <ref type="figure">2</ref> shows the illustration of LW-BLSTM. All time dependencies are considered within a fixed local window N. The length of local window N is Len(N). These local windows are non-overlapping chunks. Training LW-BLSTM is same as for the BLSTM. When using a new chunk during training, the initial -? h 0 directly uses the final -? h Len(N) from the previous chunk of the same utterance. In contrast to the traditional BLSTM, we do not need to wait until seeing a whole sentence, so both training and decoding speed are accelerated significantly. Moreover, the computational resources used for training are reduced sharply. In particular, the GPU memory requirements are reduced by a factor of about 10. This enables us to train a larger number of chunks in parallel, which accelerates the training speed further.</p><p>Some prior work has investigated methods to reduce latency and speed up the training process of BLSTM. This includes context-sensitive-chunk BLSTM (CSC-BLSTM) <ref type="bibr" target="#b24">[25]</ref> and latency-controlled BLSTM (LC-BLSTM) <ref type="bibr" target="#b25">[26]</ref>. Figure <ref type="figure">2</ref> shows the differences among these approaches. Comparing to the CSC-BLSTM approach, the LW-BLSTM approach introduced here incorporates the entire past history by using the final hidden states as the initial condition for the next block. This may lead to a more accurate approximation and be one of the key points behind the LW-BLSTM. Compared to LC-BLSTM, we do not distinguish truncated future context and preceding context, relative to a fixed local window. This may lead to fewer backward frames but avoids the potential issue of having the appended frames generate no output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gated recurrent units and LW-BGRU</head><p>Although LSTM RNNs have achieved excellent results, this architecture has some weaknesses. The architecture has a large number of parameters and can overfit relatively easily, especially for low resource tasks. In addition, training requires several complex mechanisms such as nonlinear clipping operations on cell activations and peephole connections <ref type="bibr" target="#b15">[16]</ref> which may make it difficult to tune the parameters. To address these problems, we will adopt GRUs, another type of recurrent unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2 Illustration of local window BLSTM (LW-BLSTM)</head><p>The GRU was recently proposed by Cho et al. <ref type="bibr" target="#b26">[27]</ref>. Like LSTM, it was designed to adaptively reset or update memory content. As is shown in Fig. <ref type="figure" target="#fig_1">3</ref>, each GRU has a reset gate and an update gate, which control the memory flow. The GRU fully exposes its memory content at each time step and balances output between the previous memory state and the new candidate memory state.</p><p>The GRU reset gate r t is computed by</p><formula xml:id="formula_15">r t = ? (W rx x t + U rh h t-1 + b r ), (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where ? is the sigmoid function, and x t and h t-1 are the input to the GRU and the previous output of the GRU. W rx , U rh , and b r are forward matrices, recurrent matrices, and biases for reset gate, respectively. Similarly, the update gate z t is computed by</p><formula xml:id="formula_17">z t = ? (W zx x t + U zh h t-1 + b z ), (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where the parameters are as above.</p><p>Next, the candidate memory state m t is calculated by</p><formula xml:id="formula_19">m t = ?(Wx t + U(r t * h t-1 ) + b), (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>where ? is the hyperbolic tangent function and * denotes element-wise multiplication.</p><p>Lastly, the output of the GRU is calculated by</p><formula xml:id="formula_21">h t = z t * h t-1 + (1 -z t ) * m t . (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>From the above propagating procedures, we can see that both GRU and LSTM use gates to control information flow and effectively create shortcut paths across multiple temporal steps. These gates and shortcuts help to detect and obtain the existence of an important feature in the input sequence. In addition, they allow the error to be backpropagated easily, thus reducing the difficulty due to vanishing or exploding gradients with respect to time <ref type="bibr" target="#b10">[11]</ref>.</p><p>The update gate helps the GRU to capture long term dependencies and plays a role like that of the forget gate in LSTM. The reset gate helps the GRU to reset whenever the detected feature is not necessary anymore. So when the GRU tries to learn temporally changed features, these gates activate differently.</p><p>The main difference between LSTM units and GRUs is that there is no output activation function or output gate to control the output in a GRU. Intuitively, because the output may be unbounded, this could hurt performance significantly. However, experimental results show that this is not true for GRUs, perhaps because coupling the reset gate and update gate avoids this problem and makes the use of an output gate or activation function less valuable <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Further, because an output gate is not used in GRUs, the total size of GRU layers is smaller than that of LSTM layers, which helps the GRU network avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Residual BLSTM and residual BGRU</head><p>In LSTM or GRU architectures, a sigmoid function or hyperbolic tangent function is chosen as the nonlinear activation. These bounded functions may accentuate the vanishing gradient problem because it is easy for the gradients to become small when the error signal passes through multiple layers. This issue has attracted attentions of researchers in the machine learning community.</p><p>In recent years, some novel architectures, like residual net <ref type="bibr" target="#b30">[31]</ref> and highway networks <ref type="bibr" target="#b31">[32]</ref> have introduced an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple layers.</p><p>The residual network approach <ref type="bibr" target="#b30">[31]</ref> was successfully applied to train more than 100 convolutional layers for image classification and detection. The key insight in the residual network is the inclusion of a shortcut path between layers that can be used for an additional gradient path. The highway network <ref type="bibr" target="#b31">[32]</ref> is an another way of implementing a shortcut path in a feed-forward neural network. Highway LSTM <ref type="bibr" target="#b32">[33]</ref> is a recurrent version of highway network. This approach reuses shortcut gradient paths in the temporal direction for a highway shortcut in the spatial domain. Highway connections are used between internal memory cells instead of output layers. A new gate network was also introduced to control highway paths from the prior layer memory cells.</p><p>Inspired by these approaches, we propose the bidirectional residual LSTM and GRU (BrLSTM and BrGRU). These new architectures combine the merits of both bidirectional RNN and residual networks. In detail, (1) ? ( <ref type="formula" target="#formula_8">5</ref>) and ( <ref type="formula" target="#formula_15">10</ref>) ? (12) do not change, while ( <ref type="formula" target="#formula_10">6</ref>) is updated as follows:</p><formula xml:id="formula_23">h t = o t ? ?(c t ) + W hx x t (14)</formula><p>Similarly, ( <ref type="formula" target="#formula_21">13</ref>) is updated as follows:</p><formula xml:id="formula_24">h t = z t * h t-1 + (1 -z t ) * m t + +W hx x t (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>There has been some similar recent work in this area, notably residual LSTM <ref type="bibr" target="#b33">[34]</ref> and highway LSTM <ref type="bibr" target="#b32">[33]</ref>. In contrast to residual LSTM, we add the residual directly to the output of the LSTM units, while <ref type="bibr" target="#b33">[34]</ref> add the residual part to the hidden states before the projection functions. In addition, we apply the residual part to the new local window BLSTM and BGRU. In contrast to the highway LSTM, our methods use an output layer for the spatial shortcut connection instead of an internal memory cell, which reduces interference with a temporal gradient flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data corpus</head><p>In order to evaluate our model, we implement experiments on a series of low resource speech recognition task, OpenKWS.</p><p>Since 2013, the National Institute of Standards and Technology (NIST) has conducted a series of keyword search evaluations called OpenKWS <ref type="bibr" target="#b34">[35]</ref>. It is a part of the IARPA Babel program. These evaluations try to build a high-performance automatic speech recognition system for keyword search tasks. The data for the IARPA Babel program consists of conversational telephone speech from 25 languages. During the evaluations every year, an unknown and resource-limited surprise language is released, and participating teams are given only a short period of time to finish the task.</p><p>For each surprise language, the amount of training data is about 40 h. In addition to the training set, there are also tuning, development, and evaluation sets for each surprise language. The duration of the tuning set is about 5 h, while the development set contains about 10 h of transcribed data. The development set is for evaluation by the participants themselves. The evaluation set contains about 90 h of conversational speech. There is no pronunciation lexicon released. However, a language-specific peculiarities (LSP) document is available, which can help participating teams build the grapheme-to-phoneme lexicon.</p><p>We included seven different languages in our experimental works. These languages include Cantonese, Pashto, Vietnamese, Tamil, Swahili, Kazakh, and Georgian. Figure <ref type="figure">4</ref> shows all Babel languages and highlights our target languages, represented by white points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline setup</head><p>For these target languages, the pronunciation lexicons are processed based on LSP document information. The language model used for each language is a trigram, trained using just the transcript of the training data for each language and modified by Kneser-Ney smoothing.</p><p>We select two kinds of input features to train the individual models. The first is a vector of 40-dimensional Mel filterbank features concatenated with first and second order derivatives. For this inputs, a GMM-HMM is trained to generate targets using the Kaldi toolkit <ref type="bibr" target="#b35">[36]</ref>. The The second feature type is a vector of 128-dim multilingual bottleneck features <ref type="bibr" target="#b36">[37]</ref>. Figure <ref type="figure">5</ref> shows the configuration of the bottleneck feature extractor. We use a six-layer TDNN <ref type="bibr" target="#b37">[38]</ref> as the feature extractor. The splicing   <ref type="bibr" target="#b38">[39]</ref> to train the bottleneck feature extractor. For each language in the 24 language set, a separate GMM-HMM is trained to generate the frame-level senone alignments as the targets of the feature extractor using the above methods. The details regarding corpus size and the number of senones for all 24 languages are listed in Table <ref type="table" target="#tab_0">1</ref>. The total duration of training data used to extract multilingual features is about 1400 h. For this data, a separate GMM-HMM system is trained using the above multilingual bottleneck features of each target language.</p><p>A summary of baseline configurations, including the vocabulary size, the number of phonemes, and the language model perplexity, is given in Table <ref type="table" target="#tab_1">2</ref>.</p><p>We then build DNN and LSTM as the baseline acoustic models. The DNN consists of seven hidden layers with 1024 units per layer. The input feature is the multilingual bottleneck features or fbank features concatenated within a long context window of 11. We choose a sigmoid function for nonlinear activation. The parameters are randomly initialized with a normalized uniform distribution, and the DNN is pre-trained using DBN-based layer-wise pre-training. The DBN pre-training is performed with three epochs for each layer. The parameters are optimized using the CE criterion SGD algorithm. The batch size is 128 with an initial learning rate of 0.008. The initial learning rates are empirically tuned on the development set. At the end of every epoch, the learning rate is reduced by a factor of 2 if the frame accuracy on the development set drops.</p><p>The LSTM model contains three LSTM layers with 800 memory cells per layer, and each LSTM layer is followed by a low-rank linear recurrent projection layer of 512 units. The input features are the same as for the DNN. The target label is delayed for five frames. Training is performed using the truncated BPTT algorithm. Twenty frames are used in the BPTT training, and 20 sentences are trained in parallel. The initial learning rate is 0.0001, and the momentum is 0.9. The learning rate is controlled the same as DNN.</p><p>Discriminative learning methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> can help to improve the recognition performance. In order to achieve better performance, we select models using multilingual bottleneck features as inputs, then train them using the SMBR sequence training algorithm <ref type="bibr" target="#b16">[17]</ref>. For fair comparison, we only show the cross-entropy WER for fbank input models, as in <ref type="bibr" target="#b20">[21]</ref>.</p><p>We use the Kaldi decoder with the beam set to 11.0, the lattice beam set to 8.0, and the acoustic weight set to 0.083333 in all of our evaluation experiments. For Cantonese, the evaluation metric is character error rate (CER), while for other target languages, the metric is word error rate (WER). The results of the baseline models are given in Table <ref type="table" target="#tab_2">3</ref>. These results are comparable with some previous published researches <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BLSTM and LW-BLSTM</head><p>The results of the BLSTM and LW-BLSTM acoustic models are presented in this subsection. We first evaluate the impact of hyperparameters in BLSTM and LW-BLSTM  for Pashto. Following this, we give the results using the optimal strategy for each language.</p><p>The input features for these individual acoustic models are as described above, namely 128-dimensional multilingual bottleneck features and 40-dimensional Mel filter banks with delta and delta-delta derivatives. The target labels are also as mentioned above. All parameters are randomly initialized. The training algorithm is the same as that for baseline LSTM. We use an initial learning rate of 0.00005 and the momentum of 0.9.</p><p>For BLSTM, we focus on two parameters. The first is the hidden layer size. The second is the number of hidden layer. For LW-BLSTM, we evaluate the impact of the length of local window N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">The hyperparameters in BLSTM</head><p>For testing the hidden layer size and the number of hidden layer, we fix the number of parallel sentences as 20. The results are shown in Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">5</ref>. For testing the first parameter, we fix the hidden layer size as 500, i.e., 500 units per direction. For testing the second parameter, we fix the number of hidden layers as 3.</p><p>From the results, it can be seen that when the number of BLSTM layer is larger than 3 and the number of hidden units per direction is larger than 600, the results become worse. This may be because when BLSTMs are applied to low resource tasks, a large model size has negative impact on generative ability.</p><p>In addition, results show that the BLSTM with three hidden layers and 500 units per direction has the best performance, achieving a WER of 43.8 and 48.3%, which are better than the LSTMs. The strong baseline results show that the WER of the LSTMs are 44.9 and 50.5% for different input features. This represents a relative decrease in WER of about 3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">LW-BLSTM hyperparameters</head><p>For testing the length of the local window N, we use the best network structure, i.e., containing three bidirectional  recurrent hidden layers and 500 units per direction. The results are shown in Tables <ref type="table" target="#tab_5">6</ref> and<ref type="table" target="#tab_6">7</ref>.</p><p>From the results, we can conclude that when we apply all time dependencies calculations within a fixed local window whose length is larger than 20-30, the WER does not change significantly. This may be because the effective time dependencies last more than 20-30 frames but do not affect frames too far away. The farther the distance between two frames, the less they influence each other. So the time dependencies LW-BLSTM learn are not much less than that of the traditional BLSTM.</p><p>Apart from the WER, another matter to take into consideration is the training time. Compared to BLSTM, one potential benefit of LW-BLSTM is that it may be able to reduce the training and decoding time. To compare the training time, all experiments have been implemented on a single NVIDIA Tesla K80 using CUDA <ref type="bibr" target="#b47">[48]</ref>.</p><p>From the training time data presented in Table <ref type="table" target="#tab_6">7</ref>, we can see that the LW-BLSTM approach improves the training time by a factor of more than 1.6 compared to the baseline BLSTMs.</p><p>Another interesting note is when the length of local window is larger than 20-30, the WER results become stable. In order to explore the reasons, we use above word boundary thoughts, analysing the average duration of phonemes, the average number of phonemes in words for all seven languages. For agglutinating languages, we also analyse the average number of phonemes in morphemes.</p><p>By using the linguistic information in Table <ref type="table" target="#tab_7">8</ref>, for a local window containing 20-30 frames, considering context impact, it contains about three to four phonemes, which are almost equal to the the average number of phonemes in words or morphemes. We think a local window containing 20-30 frames has a complete word boundary, so it can learn the temporal dependencies within a whole word. It may be the reason why when the length of the local  The number in the () indicates the average number of phonemes in morphemes for agglutinating languages window is 20-30, the LW-BLSTM can achieve nearly the same performance as the traditional BLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Results for all languages</head><p>Although the BLSTM and LW-BLSTM models give good results for Pashto, it is important to see how these models perform for other languages as well. We conduct experiments for all seven languages using the best network structure as described previously.</p><p>The results in Tables <ref type="table" target="#tab_8">9</ref> and<ref type="table" target="#tab_9">10</ref> show the BLSTM and LW-BLSTM model results across a variety of different languages. Compared with the baseline LSTM models, the relative improvements of the BLSTMs are about 3.0% for both fbank input features and bottleneck input features. The results also show that the BLSTMs with a local window have nearly the same performance as BLSTM, while reducing the training time significantly, confirming the discoveries in the previous experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">LW-BGRU and LW-BrGRU</head><p>In these experiments, we briefly evaluate the impact of hyperparameters in LW-BGRU and apply these two architectures to all seven languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">LW-BGRU and LW-BrGRU hyperparameters</head><p>We now investigate the optimal network structure, number of hidden layers, and hidden layer size, fixing the local window length as 20. The results are shown in Table <ref type="table" target="#tab_10">11</ref>.</p><p>The results show that the average performance of LW-BGRU models containing two GRU layers are better than those containing three GRU layers. The results become worse when the layer size per direction is larger than 700. These phenomena are similar to the LW-BLSTM, i.e., the large models may hurt the generation properties for lowresource languages. In addition, we find that the average performance is better when LW-BrGRU models contain more layers. This may be because the residual architectures learn more information from the original signals and help to avoid the gradient vanishing problems across multiple layers.</p><p>In addition, the best result of LW-BrLSTM is 43.5 on Pashto and is achieved when model contains three hidden layers and 500 units per direction. As the phenomena are coincident with LW-BrGRU, we do not compare the parameters duplicately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Results for all languages</head><p>The results are shown in Table <ref type="table" target="#tab_11">12</ref>. From the results, we can confirm that the GRU-based systems outperform the LSTM-based systems. The LW-BGRU models decrease the WER by about 7% relative to the baseline LSTM model, 5% relative to the baseline LSTM model, and about 3% relative to the LW-BLSTM. For the LW-BrGRU, these are 8, 6, and 4%, respectively. The reasons why GRUbased models are better than LSTM-based models for low resource tasks may be because the GRU-based systems can more easily avoid overfitting, and the more compact architectures are easier to tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental results for different models</head><p>From the above results, all bidirectional recurrent systems achieve better performance than traditional feedforward systems and unidirectional recurrent systems, because they yield more diversity of time dependencies and more fully take advantage of the sequential features. In addition, the GRU architecture and residual architecture learn more precise information. Due to these merits, our new models achieve excellent performance in a series of low resource tasks, OpenKWS. There has been some previous research focusing on low-resource speech recognition tasks. The architectures in this work make use of the prior knowledge of different aspects of speech signals. These improvements over the traditional CNN and RNN can help to learn more precise time domain or frequency domain information. For example, in <ref type="bibr" target="#b20">[21]</ref>,  the convolutional maxout neural networks (CMNN) and recurrent maxout neural networks (RMNN) use local spectral properties within frames and long-term dependencies among frames. In our experiments, we compare the results achieved by our models to these already strong baselines of these other models.</p><p>All WER results of all acoustic models for all languages are listed in Table <ref type="table" target="#tab_12">13</ref>. Except for the conclusions mentioned above, we find that our new models achieve better performance than CNN, CMNN, and RMNN approaches. The WER of the LW-BLSTM are 5.2 to 12.5% lower relative to the CNN and 2 to 8% relatively lower than the CMNN. For LW-BGRU, the WER decrease is about 10.6, 5.9, and 1.3% relative to CNN, CMNN, and RMNN, respectively. For LW-BrGRU, these three values are 11.7, 7.0, and 2.1%. This indicates that our advanced recurrent network-based models achieve excellent performances compared to the traditional DNN, RNN, and CNN as well as other advanced models like CMNN and RMNN. Compared to RMNN, our advanced recurrent networkbased models are benefit from local window architectures, advanced recurrent units, while RMNN are benefit from maxout no-bounded nonlinear activation, so we think our advanced recurrent networks are better than RMNN can be due to the impact of advanced recurrent units. Compared to CNN and CMNN, our networks are based on recurrent architectures, while CNN and CMNN are Here, "A", "B", "C," and "D" represent "W-BGRU-fbank," "LW-BrGRU-fbank," "LW-BGRU-MBN," and "LW-BrGRU-MBN," respectively based on convolution architectures. Although in recent years, CNN-based models achieve excellent performances <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref>, the input to these models must be spectral features, such as filerbank features and spectrogram features. While for low resource speech recognition tasks, one of the most effective methods is using multilingual bottleneck features. Obviously, multilingual bottleneck features are not suitable for CNN models. Moreover, compared models that all use fbank features, our advanced recurrent network-based models are better than CNN and CMNN models. We think it is because RNNbased models are more suitable for low-resource speech recognition tasks.</p><p>In addition, another interesting note is that the improvements of systems using MBN features are smaller than those of systems using fbank features. For example, the average WER of the LW-BGRU are 10.5% lower relative to the DNN when using fbank features, while only 3.8% lower when using MBN features. We believe that the reason behind this is because the MBN features use linguistic information of all 24 Babel languages and thus the features contain more information. This characteristic may not be noticeable when there are sufficient data resources; however, it has significant impact in low resource conditions. As we can see from the results, when MBN features are used, the performance of the DNN is better than that of the LSTM, which is contrary to what has been found for traditional speech recognition tasks <ref type="bibr" target="#b15">[16]</ref>. When using MBN features, the merits of advanced acoustic models become smaller. Despite this, our new models still achieve more than 4% relative reduction in WER, which confirms that our new methods are well suited to low resource speech recognition tasks.</p><p>A further note is that the improvements among languages vary. This is likely due to different properties of the languages. For example, the vocabulary size of Tamil is much larger than that of other languages, which makes the speech recognition tasks for Tamil harder. Also, the amount of training data for Cantonese and Vietnamese is larger, so the improvements between models become smaller. Furthermore, the discriminative information between Georgian words is large, so it is easier to capture the ability of models. Other metrics we are interested in include the duration time of training and the decoding real-time factor (RTF). To measure these, we use the same configuration described above. These results are shown in Table <ref type="table" target="#tab_13">14</ref>. The training time per epoch for the LSTMs is about two or three times slower than the DNNs. This is because the non-linearity computation and recurrent calculation procedures in the recurrent structures are less efficient for GPUs. In addition, the training time for BLSTM is much slower than LSTM. This is because each sentence needs to be fully loaded during BLSTM training, which occupies more GPU cache and leads to much time delay. The high GPU cache occupation leads to fewer utterances being able to be trained in parallel, which further increases the training time. However, LW-BLSTM and LW-BGRU alleviate this problem. These approaches accelerate the learning speed by a factor of more than 1.6 compared to the conventional BLSTMs. The result is that the training time per epoch for the LW-BLSTMs or LW-BGRUs is about 1.5 times lower than the LSTMs and nearly equal to that of CNNs.</p><p>Finally, we combine the results of different systems. Since different models have different architectures and corresponding merits, they are adept in modeling different aspects of speech signals. When we combine these strong individual models, we may benefit from their complementary information.  We use the lattice-based system combination method proposed in <ref type="bibr" target="#b51">[52]</ref>, implemented in the Kaldi toolkit. We combine the top best individual models. The final WER results become stable when the number of individual models is above five. From the final results, the relative improvements of the combined results are relatively 1.7 to 3.5% better relative to the best LW-BrGRU models. This system achieved third place in OpenSAT 2017 Pilot Evaluation based on Pashto [53].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization for different models</head><p>The final experiment, inspired by <ref type="bibr" target="#b52">[54]</ref>, investigates the evolution of different models when performing recognition. Following this approach, we draw the output of the last hidden layer using the t-SNE tool <ref type="bibr" target="#b53">[55]</ref> when decoding an utterance. For these temporal traces, if they are smoother, the discriminative distribution of corresponding high dimension output data is stronger and the output confidence is higher. In addition, the amount of information contained in the output data depends on the number of traces. The results are shown in Fig. <ref type="figure" target="#fig_3">6</ref>. The types of acoustic models are presented at the upper left corner of each subpicture.</p><p>Some interesting observations can be found from the figure. The temporal traces of LW-BGRU and LW-BLSTM are more concise and smooth than those of DNN and LSTM. This indicates that the LW-BGRU tends to remember more than DNN and LSTM. The novelty within a long span window leads to a smooth temporal trace. The output decision has higher confidence, so the temporal trace is more concise. In addition, the temporal traces of LW-BrGRU are more complex. We believe this indicates that LW-BrGRU retains more original feature information from lower layers.</p><p>In order to evaluate completely, we show the decoding results and reference of this utterance. From Table <ref type="table" target="#tab_14">15</ref>, we can find that the decoding result of LW-BrGRU is the most accurate. For low-resource speech recognition tasks,  The id of the utterance is 81424-B-20141123-000421-030633 for Georgian the main part of errors is deleting error. It means that the real words are deleted because of the low acoustic and language scores. Deleting errors will become less if the output contains more helpful sequential information. So the LW-BrGRU and LW-BGRU learn more discriminative knowledge and achieve better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we have proposed a local bidirectional RNN architecture, a new recurrent unit, gated recurrent units (GRU) as well as residual architectures, and combine these into a final system. Experiments are conducted on the benchmark datasets released under the IARPA Babel Program. Results show that the local window bidirectional RNN models decrease the WER about 3 to 8% relative to the baseline LSTM models and about 4 to 10% relative to the baseline DNN models across the selected seven target languages. In addition, the local window bidirectional RNN models decrease the learning time by a factor of more than 1.6 compared to conventional BLSTMs. In the future, we will extend these architectures with other useful components, such as convolutional architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Long short-term memory unit. "T" denotes a delay of one time step, ? denotes the sigmoid function, and "tanh" denotes hyperbolic tangent function</figDesc><graphic url="image-2.png" coords="3,156.46,93.66,283.72,234.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Gated recurrent unit. ? denotes the sigmoid function, "tanh" denotes hyperbolic tangent function</figDesc><graphic url="image-3.png" coords="5,67.03,466.08,214.84,237.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 Fig. 5</head><label>45</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref> The distribution of Babel languages. The white points represent the selected languages</figDesc><graphic url="image-4.png" coords="6,63.82,486.93,467.08,226.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Fig.<ref type="bibr" target="#b5">6</ref> The temporal trace of different acoustic models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="7,156.46,107.82,268.36,349.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The corpus size and number of senones of the Babel languages</figDesc><table><row><cell>Language</cell><cell>Training hours</cell><cell cols="2"># senone Language</cell><cell>Training hours</cell><cell cols="2"># senone Language</cell><cell>Training hours</cell><cell># senone</cell></row><row><cell cols="2">Cantonese(101) 140.7</cell><cell>4687</cell><cell cols="2">Assamese(102) 60.3</cell><cell>4707</cell><cell>Bengali(103)</cell><cell>61.1</cell><cell>4929</cell></row><row><cell>Pashto(104)</cell><cell>77.3</cell><cell>4823</cell><cell>Turkish(105)</cell><cell>76.4</cell><cell>4791</cell><cell>Tagalog(106)</cell><cell>83.8</cell><cell>4814</cell></row><row><cell cols="2">Vietnamese(107) 87.1</cell><cell>4692</cell><cell>Haitian(201)</cell><cell>66.5</cell><cell>4875</cell><cell>Swahili(202)</cell><cell>44.0</cell><cell>4638</cell></row><row><cell>Lao(203)</cell><cell>65.2</cell><cell>4667</cell><cell>Tamil(204)</cell><cell>64.1</cell><cell>4560</cell><cell>Kurdish (205)</cell><cell>41.7</cell><cell>4412</cell></row><row><cell>Zulu(206)</cell><cell>61.3</cell><cell>4464</cell><cell>Tok Pisin(207)</cell><cell>39.0</cell><cell>4565</cell><cell>Cebuano(301)</cell><cell>41.0</cell><cell>4603</cell></row><row><cell>Kazakh(302)</cell><cell>39.6</cell><cell>4714</cell><cell>Telugu(303)</cell><cell>41.8</cell><cell>4515</cell><cell cols="2">Lithuanian(304) 42.1</cell><cell>4755</cell></row><row><cell>Guarani(305)</cell><cell>42.6</cell><cell>4504</cell><cell>Igbo(306)</cell><cell>43.7</cell><cell>4659</cell><cell>Amharic(307)</cell><cell>43.2</cell><cell>4685</cell></row><row><cell cols="2">Mongolian(401) 45.7</cell><cell>4537</cell><cell cols="2">Javanese(402) 45.1</cell><cell>4763</cell><cell>Dholuo(403)</cell><cell>41.3</cell><cell>4571</cell></row></table><note><p>The number in the () indicates the language id per the Babel program</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>The baseline configuration of all languages, including vocabulary size, number of phonemes, and language model perplexity</figDesc><table><row><cell>Language</cell><cell># words</cell><cell># phonemes</cell><cell>LM perplexity</cell></row><row><cell>Cantonese</cell><cell>18512</cell><cell>216</cell><cell>124.87</cell></row><row><cell>Pashto</cell><cell>17646</cell><cell>126</cell><cell>217.55</cell></row><row><cell>Vietnamese</cell><cell>6210</cell><cell>375</cell><cell>176.99</cell></row><row><cell>Swahili</cell><cell>21890</cell><cell>75</cell><cell>435.74</cell></row><row><cell>Tamil</cell><cell>52369</cell><cell>34</cell><cell>656.51</cell></row><row><cell>Kazakh</cell><cell>19587</cell><cell>133</cell><cell>476.04</cell></row><row><cell>Georgian</cell><cell>34946</cell><cell>35</cell><cell>471.03</cell></row></table><note><p>indexes used are {-2, -1, 0, 1, 2} {-1, 2} {-3, 3} {-7, 2} {0}( * ) {0}. The splicing indexes of {-2, -1, 0, 1, 2} indicate that the first layer sees five consecutive frames of input, and the {-1, 2} indicate that the second hidden layers see two frames of the previous layer, separated by three frames. All layers except the bottleneck layer contain 1024 neurons. The bottleneck layer is located at the fifth layer, denoted as ( * ). The dimension of the bottleneck layer 128. The original input to the TDNN feature extractor is the 40-dimensional Mel-filter bank features concatenated with 3-dimensional pitch features. To fully take advantage of available low resource languages, we include 24 languages from the IARPA Babel program dataset and use multitask learning methods</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>WER (%) of the baseline models for all languages</figDesc><table><row><cell>Model</cell><cell>WER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>101 Cantonese</cell><cell>104 Pashto</cell><cell>107 Vietnamese</cell><cell>202 Swahili</cell><cell>204 Tamil</cell><cell>302 Kazakh</cell><cell>404 Georgian</cell></row><row><cell>DNN-fbank</cell><cell>44.8</cell><cell>51.2</cell><cell>53.1</cell><cell>46.2</cell><cell>66.7</cell><cell>54.1</cell><cell>50.5</cell></row><row><cell>LSTM-fbank</cell><cell>40.7</cell><cell>50.5</cell><cell>47.8</cell><cell>42.5</cell><cell>65</cell><cell>52.9</cell><cell>48.9</cell></row><row><cell>DNN-MBN</cell><cell>36.1</cell><cell>44.2</cell><cell>44.7</cell><cell>38.9</cell><cell>61.3</cell><cell>48.8</cell><cell>45</cell></row><row><cell>LSTM-MBN</cell><cell>35.7</cell><cell>44.9</cell><cell>45</cell><cell>39.6</cell><cell>61.3</cell><cell>49</cell><cell>45.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>BLSTM accuracy as a function of the number of layers</figDesc><table><row><cell>Model</cell><cell># layers</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>BLSTM-fbank</cell><cell>48.9</cell><cell>48.3</cell><cell>48.5</cell></row><row><cell>BLSTM-MBN</cell><cell>43.9</cell><cell>43.8</cell><cell>44.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>BLSTM accuracy as a function of the hidden layer size</figDesc><table><row><cell>Model</cell><cell cols="2"># hidden layer size</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>400</cell><cell>500</cell><cell>600</cell><cell>700</cell><cell>800</cell></row><row><cell>BLSTM-fbank</cell><cell>48.6</cell><cell>48.3</cell><cell>48.7</cell><cell>49.2</cell><cell>49.5</cell></row><row><cell>BLSTM-MBN</cell><cell>44</cell><cell>43.8</cell><cell>43.9</cell><cell>44.5</cell><cell>44.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>WER results vs. the length of the local window</figDesc><table><row><cell>Model</cell><cell cols="3">the length of local window</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>LW-BLSTM-fbank</cell><cell>48.8</cell><cell>48.3</cell><cell>48.3</cell><cell>48.3</cell><cell>48.3</cell></row><row><cell>LW-BLSTM-MBN</cell><cell>44.5</cell><cell>43.9</cell><cell>43.9</cell><cell>43.9</cell><cell>43.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Training time per epoch (hours) vs. the length of local window</figDesc><table><row><cell>Model</cell><cell cols="3">The length of local window</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>LW-BLSTM-fbank</cell><cell>3.92</cell><cell>4.71</cell><cell>5.03</cell><cell>5.34</cell><cell>5.68</cell></row><row><cell>BLSTM-fbank</cell><cell></cell><cell></cell><cell>8.40</cell><cell></cell><cell></cell></row><row><cell>LW-BLSTM-MBN</cell><cell>3.92</cell><cell>4.71</cell><cell>5.01</cell><cell>5.34</cell><cell>5.70</cell></row><row><cell>BLSTM-MBN</cell><cell></cell><cell></cell><cell>8.40</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>The linguistic information of all languages, including the average duration of phonemes and the average number of phonemes in words</figDesc><table><row><cell>Language</cell><cell>The average duration of</cell><cell>The average number of</cell></row><row><cell></cell><cell>phonemes</cell><cell>phonemes in words</cell></row><row><cell>Cantonese</cell><cell>14.01</cell><cell>3.92</cell></row><row><cell>Pashto</cell><cell>9.23</cell><cell>6.39(3.08)</cell></row><row><cell>Vietnamese</cell><cell>13.27</cell><cell>2.83</cell></row><row><cell>Swahili</cell><cell>12.77</cell><cell>7.54(3.44)</cell></row><row><cell>Tamil</cell><cell>11.02</cell><cell>9.30(3.87)</cell></row><row><cell>Kazakh</cell><cell>9.65</cell><cell>7.12(3.63)</cell></row><row><cell>Georgian</cell><cell>12.41</cell><cell>8.47(6.39)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc>WER (%) of the BLSTMs and LW-BLSTMs for all languages</figDesc><table><row><cell>Model</cell><cell>WER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>101 Cantonese</cell><cell>104 Pashto</cell><cell>107 Vietnamese</cell><cell>202 Swahili</cell><cell>204 Tamil</cell><cell>302 Kazakh</cell><cell>404 Georgian</cell></row><row><cell>BLSTM-fbank</cell><cell>39.5</cell><cell>48.3</cell><cell>45.8</cell><cell>41</cell><cell>63.7</cell><cell>50.3</cell><cell>46.6</cell></row><row><cell>LW-BLSTM-fbank</cell><cell>39.6</cell><cell>48.3</cell><cell>45.9</cell><cell>41.1</cell><cell>63.7</cell><cell>50.2</cell><cell>46.7</cell></row><row><cell>BLSTM-MBN</cell><cell>34.7</cell><cell>43.8</cell><cell>43.6</cell><cell>38</cell><cell>60.6</cell><cell>47.8</cell><cell>44</cell></row><row><cell>LW-BLSTM-MBN</cell><cell>34.7</cell><cell>43.9</cell><cell>43.7</cell><cell>38</cell><cell>60.6</cell><cell>47.7</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10</head><label>10</label><figDesc>Training time per epoch (hours) of BLSTMs and LW-BLSTMs for all languages</figDesc><table><row><cell>Model</cell><cell cols="2">Training time per epoch (h)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>101 Cantonese</cell><cell>104 Pashto</cell><cell>107 Vietnamese</cell><cell>202 Swahili</cell><cell>204 Tamil</cell><cell>302 Kazakh</cell><cell>404 Georgian</cell></row><row><cell>BLSTM-fbank</cell><cell>12.79</cell><cell>8.40</cell><cell>7.69</cell><cell>4.92</cell><cell>7.00</cell><cell>4.85</cell><cell>4.88</cell></row><row><cell>LW-BLSTM-fbank</cell><cell>8.01</cell><cell>4.71</cell><cell>4.83</cell><cell>3.06</cell><cell>4.37</cell><cell>3.03</cell><cell>3.04</cell></row><row><cell>BLSTM-MBN</cell><cell>12.78</cell><cell>8.40</cell><cell>7.67</cell><cell>4.93</cell><cell>7.01</cell><cell>4.85</cell><cell>4.87</cell></row><row><cell>LW-BLSTM-MBN</cell><cell>8.01</cell><cell>4.71</cell><cell>4.83</cell><cell>3.06</cell><cell>4.37</cell><cell>3.03</cell><cell>3.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11</head><label>11</label><figDesc>LW-BGRU and LW-BrGRU accuracy as a function of the number of layer and hidden layer size</figDesc><table><row><cell># layers</cell><cell>Model</cell><cell cols="2"># hidden layer size</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>300</cell><cell>500</cell><cell>700</cell><cell>800</cell><cell>900</cell></row><row><cell>2</cell><cell>A</cell><cell>48.4</cell><cell>47.6</cell><cell>47.4</cell><cell>47.4</cell><cell>48</cell></row><row><cell></cell><cell>B</cell><cell>48.1</cell><cell>47.3</cell><cell>47</cell><cell>47.1</cell><cell>47.8</cell></row><row><cell></cell><cell>C</cell><cell>43.9</cell><cell>43.3</cell><cell>43.1</cell><cell>43.2</cell><cell>44.1</cell></row><row><cell></cell><cell>D</cell><cell>43.7</cell><cell>43.1</cell><cell>42.7</cell><cell>42.9</cell><cell>43.5</cell></row><row><cell>3</cell><cell>A</cell><cell>48.2</cell><cell>47.7</cell><cell>47.9</cell><cell>48.5</cell><cell>48.9</cell></row><row><cell></cell><cell>B</cell><cell>47.5</cell><cell>46.9</cell><cell>46.9</cell><cell>47.1</cell><cell>47.6</cell></row><row><cell></cell><cell>C</cell><cell>43.8</cell><cell>43.5</cell><cell>43.7</cell><cell>44.3</cell><cell>44.9</cell></row><row><cell></cell><cell>D</cell><cell>42.9</cell><cell>42.6</cell><cell>42.6</cell><cell>43.1</cell><cell>43.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12</head><label>12</label><figDesc>WER (%) of the LW-BrLSTMs, LW-BGRUs and LW-BrGRUs for all languages</figDesc><table><row><cell>Model</cell><cell>WER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>101 Cantonese</cell><cell>104 Pashto</cell><cell>107 Vietnamese</cell><cell>202 Swahili</cell><cell>204 Tamil</cell><cell>302 Kazakh</cell><cell>404 Georgian</cell></row><row><cell>LW-BrLSTM-fbank</cell><cell>39.2</cell><cell>47.9</cell><cell>45.3</cell><cell>40.6</cell><cell>62.9</cell><cell>49.5</cell><cell>45.7</cell></row><row><cell>LW-BGRU-fbank</cell><cell>38.7</cell><cell>47.4</cell><cell>44.8</cell><cell>40</cell><cell>62.8</cell><cell>49</cell><cell>45.4</cell></row><row><cell>LW-BrGRU-fbank</cell><cell>38.5</cell><cell>47</cell><cell>44.3</cell><cell>39.5</cell><cell>62.2</cell><cell>48.5</cell><cell>44.1</cell></row><row><cell>LW-BrLSTM-MBN</cell><cell>34.4</cell><cell>43.5</cell><cell>43.1</cell><cell>37.4</cell><cell>60.1</cell><cell>47.2</cell><cell>43.3</cell></row><row><cell>LW-BGRU-MBN</cell><cell>34.1</cell><cell>43.1</cell><cell>42.7</cell><cell>37</cell><cell>59.7</cell><cell>46.7</cell><cell>42.9</cell></row><row><cell>LW-BrGRU-MBN</cell><cell>34.1</cell><cell>42.7</cell><cell>42.7</cell><cell>36.8</cell><cell>59.2</cell><cell>46.2</cell><cell>41.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13</head><label>13</label><figDesc>WER (%) across all languages</figDesc><table><row><cell>Model</cell><cell>WER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>101 Cantonese</cell><cell>104</cell><cell>107 Vietnamese</cell><cell>202</cell><cell>204</cell><cell>302</cell><cell>404 Georgian</cell></row><row><cell></cell><cell></cell><cell>Pashto</cell><cell></cell><cell>Swahili</cell><cell>Tamil</cell><cell>Kazakh</cell><cell></cell></row><row><cell>DNN-MBN</cell><cell>36.1</cell><cell>44.2</cell><cell>44.7</cell><cell>38.9</cell><cell>61.3</cell><cell>48.8</cell><cell>45</cell></row><row><cell>LSTM-MBN</cell><cell>35.7</cell><cell>44.9</cell><cell>45</cell><cell>39.6</cell><cell>61.3</cell><cell>49</cell><cell>45.2</cell></row><row><cell>BLSTM-MBN</cell><cell>34.7</cell><cell>43.8</cell><cell>43.6</cell><cell>38</cell><cell>60.6</cell><cell>47.8</cell><cell>44</cell></row><row><cell>LW-BLSTM-MBN</cell><cell>34.7</cell><cell>43.9</cell><cell>43.7</cell><cell>38</cell><cell>60.6</cell><cell>47.7</cell><cell>44</cell></row><row><cell>LW-BrLSTM-MBN</cell><cell>34.4</cell><cell>43.5</cell><cell>43.1</cell><cell>37.4</cell><cell>60.1</cell><cell>47.2</cell><cell>43.3</cell></row><row><cell>LW-BGRU-MBN</cell><cell>34.1</cell><cell>43.1</cell><cell>42.7</cell><cell>37</cell><cell>59.7</cell><cell>46.7</cell><cell>42.9</cell></row><row><cell>LW-BrGRU-MBN</cell><cell>34.1</cell><cell>42.7</cell><cell>42.7</cell><cell>36.8</cell><cell>59.2</cell><cell>46.2</cell><cell>41.7</cell></row><row><cell>DNN-fbank</cell><cell>44.8</cell><cell>51.2</cell><cell>53.1</cell><cell>46.2</cell><cell>66.7</cell><cell>54.1</cell><cell>50.5</cell></row><row><cell>LSTM-fbank</cell><cell>40.7</cell><cell>50.5</cell><cell>47.8</cell><cell>42.5</cell><cell>65</cell><cell>52.9</cell><cell>48.9</cell></row><row><cell>BLSTM-fbank</cell><cell>39.5</cell><cell>48.3</cell><cell>45.8</cell><cell>41</cell><cell>63.7</cell><cell>50.3</cell><cell>46.6</cell></row><row><cell>LW-BLSTM-fbank</cell><cell>39.6</cell><cell>48.3</cell><cell>45.9</cell><cell>41.1</cell><cell>63.7</cell><cell>50.2</cell><cell>46.7</cell></row><row><cell>LW-BrLSTM-fbank</cell><cell>39.2</cell><cell>47.9</cell><cell>45.3</cell><cell>40.6</cell><cell>62.9</cell><cell>49.5</cell><cell>45.7</cell></row><row><cell>LW-BGRU-fbank</cell><cell>38.7</cell><cell>47.4</cell><cell>44.8</cell><cell>40</cell><cell>62.8</cell><cell>49</cell><cell>45.4</cell></row><row><cell>LW-BrGRU-fbank</cell><cell>38.5</cell><cell>47</cell><cell>44.3</cell><cell>39.5</cell><cell>62.2</cell><cell>48.5</cell><cell>44.1</cell></row><row><cell>CNN-fbank [21]</cell><cell>43.6</cell><cell>51.5</cell><cell>52.5</cell><cell>-</cell><cell>67.2</cell><cell>-</cell><cell>-</cell></row><row><cell>CMNN-fbank [21]</cell><cell>41.7</cell><cell>49.3</cell><cell>49.9</cell><cell>-</cell><cell>64.2</cell><cell>-</cell><cell>-</cell></row><row><cell>RMNN-fbank [21]</cell><cell>39</cell><cell>48.1</cell><cell>45.7</cell><cell>-</cell><cell>63.4</cell><cell>-</cell><cell>-</cell></row><row><cell>DNN + LW-BLSTM LW-BGRU + LW-BrGRU</cell><cell>33</cell><cell>41.5</cell><cell>41.3</cell><cell>35.7</cell><cell>58.2</cell><cell>44.6</cell><cell>40.6</cell></row><row><cell>DNN + LW-BLSTM LW-BrLSTM + LW-BGRU +</cell><cell>32.8</cell><cell>41.2</cell><cell>41</cell><cell>35.5</cell><cell>57.9</cell><cell>44.3</cell><cell>40.2</cell></row><row><cell>LW-BrGRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14</head><label>14</label><figDesc>Training time per epoch (hours) and decoding real time factor (RTF) of all models for all languages</figDesc><table><row><cell>Model</cell><cell>Training time per epoch (hours)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15</head><label>15</label><figDesc>The decoding results and word error rates of different models</figDesc><table><row><cell>Model</cell><cell cols="3">Text (ins/del/sub percentage)</cell></row><row><cell>Reference</cell><cell cols="3">aa ilap'arak'e ravi biC'o sosom</cell></row><row><cell></cell><cell cols="3">rom mankana daamt'vria da</cell></row><row><cell></cell><cell cols="3">gamopKizlda aKla ra Kasiatzea</cell></row><row><cell>DNN</cell><cell cols="3">aa ilap'arak'e Kasiatzea (0/77/0)</cell></row><row><cell>LSTM</cell><cell cols="3">aa ilap'arak'e mankana aKla ra</cell></row><row><cell></cell><cell cols="2">Kasiatzea (0/54/0)</cell><cell></cell></row><row><cell>LW-BLSTM</cell><cell>aa</cell><cell>ilap'arak'e</cell><cell>davT'ert</cell></row><row><cell></cell><cell cols="3">masesKeo mankana daadgeba</cell></row><row><cell></cell><cell cols="3">gamopKizlda aKla ra Kasiatzea</cell></row><row><cell></cell><cell cols="2">(0/23/23)</cell><cell></cell></row><row><cell>LW-BGRU</cell><cell>aa</cell><cell>ilap'arak'e</cell><cell>mankana</cell></row><row><cell></cell><cell cols="3">daamt'vria da gamopKizlda</cell></row><row><cell></cell><cell cols="3">aKla ra Kasiatzea (0/31/0)</cell></row><row><cell>LW-BrGRU</cell><cell cols="3">aa ilap'arak'e ravi mankana</cell></row><row><cell></cell><cell cols="3">daamt'vria da gamopKizlda</cell></row><row><cell></cell><cell cols="3">aKla ra Kasiatzea (0/23/0)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by <rs type="funder">National Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">61370034</rs> and No. <rs type="grantNumber">61403224</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jeHc2Xb">
					<idno type="grant-number">61370034</idno>
				</org>
				<org type="funding" xml:id="_PxrZyrP">
					<idno type="grant-number">61403224</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>The datasets used or analysed during this paper are available from Babel program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>ASR: Automatic speech recognition; BLSTM: Bidirectional long short-term memory; BNF: Bottleneck feature; GRU: Gated recurrent unit; LSTM: Long short-term memory; LW-BLSTM: Local-window bidirectional long short-term memory; LW-BGRU: Local-window bidirectional gated recurrent unit; RNN: Recurrent neural network; WER: Word error rate Authors' contributions JK carried out these approaches, implemented the experiments, and finished this article. WQZ conceived of the study and participated in its design and coordination. WWL participated in implementation of the experiments and performed the statistical analysis. JL participated in the design of the study. MJ conceived of the study and helped to draft the manuscript. All authors read and approved the final manuscript. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors' information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare that they have no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's Note</head><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A-R</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature engineering in context-dependent deep neural networks for conversational speech transcription</title>
		<author>
			<persName><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><surname>Gang</surname></persName>
		</author>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ASRU</title>
		<meeting>ASRU<address><addrLine>Waikoloa</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automatic speech and speaker recognition. The use of recurrent neural networks in continuous speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><surname>Renals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="233" to="258" />
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Mikolov</surname></persName>
		</author>
		<title level="m">Proc. Interspeech. Recurrent neural network based language model</title>
		<meeting>Interspeech. Recurrent neural network based language model</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Watanabe</surname></persName>
		</author>
		<title level="m">Proc ICASSP. Recurrent deep neural networks for robust speech recognition</title>
		<meeting>ICASSP. Recurrent deep neural networks for robust speech recognition<address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5569" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phone sequence modeling with recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP</title>
		<meeting>ICASSP<address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5417" to="5421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP</title>
		<meeting>ICASSP<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for noise reduction in robust ASR</title>
		<author>
			<persName><surname>Al Maas</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc INTERSPEECH: ISCA</title>
		<meeting>INTERSPEECH: ISCA</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><surname>Tn Sainath</surname></persName>
		</author>
		<title level="m">Proc ICASSP. Convolutional, long short-term memory, fully connected deep neural networks</title>
		<meeting>ICASSP. Convolutional, long short-term memory, fully connected deep neural networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4580" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling</title>
		<author>
			<persName><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="631" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context dependent phone models for LSTM RNN acoustic modelling</title>
		<author>
			<persName><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4585" to="4589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Beaufays</surname></persName>
		</author>
		<title level="m">INTERSPEECH. Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence discriminative distributed training of long short-term memory recurrent neural networks</title>
		<author>
			<persName><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="17" to="18" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gated recurrent units based hybrid acoustic models for robust speech recognition</title>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ISCSLP</title>
		<meeting>ISCSLP<address><addrLine>Tianjin</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neuron sparseness versus connection sparseness in deep neural network for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brisbane</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4954" to="4958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-performance Swahili keyword search with very limited language pack: the THUEE system for the OpenKWS15 evaluation</title>
		<author>
			<persName><forename type="first">Lv</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhiqiang</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ASRU</title>
		<meeting>ASRU<address><addrLine>Scottsdale</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="215" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maxout neurons for deep convolutional and LSTM neural networks in speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Comm</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sig. Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mohamed A-r</title>
		<author>
			<persName><forename type="first">N</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ASRU. Hybrid speech recognition with deep bidirectional LSTM</title>
		<meeting>ASRU. Hybrid speech recognition with deep bidirectional LSTM<address><addrLine>Olomouc</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training deep bidirectional LSTMm acoustic model for lvcsr by a context-sensitive-chunk bptt approach</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech Lang. Process. (TASLP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1185" to="1193" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Highway long short-term memory rnns for distant speech recognition</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP</title>
		<meeting>ICASSP<address><addrLine>Shanghai</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5755" to="5759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: encoder-decoder approaches</title>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICML</title>
		<meeting>ICML<address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LSTM: a search space odyssey</title>
		<author>
			<persName><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Br Steunebrink</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><surname>Kaiming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Greff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recurrent highway networks</title>
		<author>
			<persName><surname>Jg Zilly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Residual LSTM: Design of a deep recurrent architecture for distant speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaeyoung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03360</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Kws16 keyword search evaluation plan</title>
		<author>
			<persName><surname>Nist</surname></persName>
		</author>
		<ptr target="https://www.nist.gov/itl/iad/mig/openkws16-evaluation" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><surname>Schwarz</surname></persName>
		</author>
		<idno>EPFL-CONF-.192584</idno>
		<title level="m">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. The Kaldi speech recognition toolkit</title>
		<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multilingual representations for low resource speech recognition and keyword search</title>
		<author>
			<persName><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ASRU</title>
		<meeting>ASRU<address><addrLine>Scottsdale</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="259" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><surname>Khudanpur</surname></persName>
		</author>
		<title level="m">Proc INTERSPEECH. A time delay neural network architecture for efficient modeling of long temporal contexts (ISCA</title>
		<meeting>INTERSPEECH. A time delay neural network architecture for efficient modeling of long temporal contexts (ISCA<address><addrLine>Dresden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2440" to="2444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jinyu</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP</title>
		<meeting>ICASSP<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7304" to="7308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Minimum phone error and I-smoothing for improved discriminative training</title>
		<author>
			<persName><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP</title>
		<meeting>ICASSP<address><addrLine>Orlando</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling</title>
		<author>
			<persName><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP</title>
		<meeting>ICASSP<address><addrLine>Taipei</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3761" to="3764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><surname>Hannemann</surname></persName>
		</author>
		<title level="m">Proc INTERSPEECH. BUT BABEL system for spontaneous Cantonese (ISCA</title>
		<meeting>INTERSPEECH. BUT BABEL system for spontaneous Cantonese (ISCA<address><addrLine>Lyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2589" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">BUT 2014 Babel system: Analysis of adaptation in NN based systems</title>
		<author>
			<persName><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Nf Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Sivadas</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
		<title level="m">Proc ICASSP. Strategies for Vietnamese keyword search</title>
		<meeting>ICASSP. Strategies for Vietnamese keyword search<address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4149" to="4153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><surname>Tanel</surname></persName>
		</author>
		<title level="m">Proc ICASSP. The 2016 BBN Georgian telephone speech keyword spotting system</title>
		<meeting>ICASSP. The 2016 BBN Georgian telephone speech keyword spotting system<address><addrLine>New Orleans, IEEE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5755" to="5759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient methods to train multilingual bottleneck feature extractors for low resource keyword search</title>
		<author>
			<persName><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP</title>
		<meeting>ICASSP<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5650" to="5654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><surname>Hartmann</surname></persName>
		</author>
		<title level="m">Proc ICASSP. Analysis of keyword spotting performance across IARPA babel languages</title>
		<meeting>ICASSP. Analysis of keyword spotting performance across IARPA babel languages<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5765" to="5769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">CUDA Nvidia, Programming guide</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName><forename type="first">A-R</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><surname>Lecun</surname></persName>
		</author>
		<title level="m">2016 IEEE International Conference on. Very deep multilingual convolutional neural networks for LVCSR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4955" to="4959" />
		</imprint>
	</monogr>
	<note>Acoustics, Speech and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
		<title level="m">Proc CVPR. Going deeper with convolutions</title>
		<meeting>CVPR. Going deeper with convolutions</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Minimum Bayes Risk decoding and system combination based on a recursion for edit distance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mangu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="802" to="828" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Memory visualization for gated recurrent neural networks in speech recognition</title>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2736" to="2740" />
		</imprint>
	</monogr>
	<note>2017 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visualizing highdimensional data using t-SNE</title>
		<author>
			<persName><surname>Lvd Maaten</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
