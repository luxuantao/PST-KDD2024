<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HoloNet: Towards Robust Emotion Recognition in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
							<email>anbang.yao@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs China</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shandong</forename><surname>Wang</surname></persName>
							<email>shandong.wang@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs China</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongqi</forename><surname>Cai</surname></persName>
							<email>dongqi.cai@intel.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs China</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sha</forename><surname>Liang</surname></persName>
							<email>liang.sha@buaa.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Hu</surname></persName>
							<email>ping1.hu@intel.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Intel Labs China</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
							<email>yurong.chen@intel.com</email>
							<affiliation key="aff5">
								<orgName type="institution">Intel Labs China</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HoloNet: Towards Robust Emotion Recognition in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D591AEA50C6D0F5BE6A8AB988BF5E46</idno>
					<idno type="DOI">10.1145/2993148.2997639</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computing methodologies ~ Appearance and texture representations Computing methodologies ~ Neural networks Emotion Recognition</term>
					<term>EmotiW 2016 Challenge</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present HoloNet, a well-designed Convolutional Neural Network (CNN) architecture regarding our submissions to the video based sub-challenge of the Emotion Recognition in the Wild (EmotiW) 2016 challenge. In contrast to previous related methods that usually adopt relatively simple and shallow neural network architectures to address emotion recognition task, our HoloNet has three critical considerations in network design. (1) To reduce redundant filters and enhance the non-saturated nonlinearity in the lower convolutional layers, we use a modified Concatenated Rectified Linear Unit (CReLU) instead of ReLU. (2) To enjoy the accuracy gain from considerably increased network depth and maintain efficiency, we combine residual structure and CReLU to construct the middle layers. (3) To broaden network width and introduce multi-scale feature extraction property, the topper layers are designed as a variant of inception-residual structure. The main benefit of grouping these modules into the HoloNet is that both negative and positive phase information implicitly contained in the input data can flow over it in multiple paths, thus deep multi-scale features explicitly capturing emotion variation can be well extracted from multi-path sibling layers, and then can be further concatenated for robust recognition. We obtain competitive results in this year s video based emotion recognition sub-challenge using an ensemble of two HoloNet models trained with given data only. Specifically, we obtain a mean recognition rate of 57.84%, outperforming the baseline accuracy with an absolute margin of 17.37%, and yielding 4.04% absolute accuracy gain compared to the result of last year s winner team. Meanwhile, our method runs with a speed of several thousands of frames per second on a GPU, thus it is well applicable to real-time scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p><ref type="foot" target="#foot_0">1</ref> The video based Emotion Recognition in the Wild (EmotiW) challenge has been continuously held for three years <ref type="bibr">[5 7</ref>]. In the task, competitors are asked to use their methods to automatically assign one label from seven candidate emotion categories (i.e., Angry, Disgust, Fear, Happy, Neutral, Sad and Surprise) to each of test video clips collected from real Hollywood movies. In contrast to the previous further introduces video clips extracted from real TV programs for testing only <ref type="bibr" target="#b6">[8]</ref>. Generally, this challenge provides a comprehensive evaluation dataset, covering a broad range of difficulties such as diverse illumination, viewpoint variation in face, partial occlusion, cluttered background and context. Since emotion recognition in unconstrained conditions has great potential for many applications, such as intelligent human-machine interaction, smart robotics and online advertising, the EmotiW challenge series has attracted ever growing interest both in academia and industry.</p><p>According to the report <ref type="bibr" target="#b5">[7]</ref> summarized by the organizers, deep learning based methods, especially popular Convolutional Neural Networks (CNNs), have been adopted by most of the teams in the past challenges. This is mainly due to the fact that CNNs have made performance breakthroughs on many computer vision tasks such as image classification <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23]</ref> and face recognition <ref type="bibr" target="#b24">[26]</ref>. One important fact we noticed is that the network architectures of former winning teams are both simple and shallow. The EmoNet proposed by the 2013 winner team <ref type="bibr" target="#b10">[12]</ref> is a four-stage architecture which has three convolutional layers and one fully connected layer with 7 softmax units. The winner team of 2014 <ref type="bibr" target="#b14">[16]</ref> presents a multiple-kernel learning method to study the potential of combining hand-crafted features and CNN features. However, the architectures of two CNNs they adopted are also shallow and simple. 2015 winner team <ref type="bibr" target="#b29">[31]</ref> also uses a five-layer shallow CNN model to enhance the performance of their proposed relation based features. In this paper, we explore the method on how to design a deep yet computationally efficient CNN architecture for advancing emotion recognition in the wild. Our method is inspired by three important facts. First, the latest study <ref type="bibr" target="#b18">[20]</ref> shows that the filters in the lower convolutional layers of a deep CNN form pairs in phase. This means the filters in the lower convolutional layers contain considerable redundancy, thus the number of filters can be largely reduced while the accuracy of the whole network will not decrease through a simple yet effective activation scheme. Second, recent progress in deep learning sufficiently demonstrates that residual structure <ref type="bibr" target="#b9">[11]</ref> can resolve the dilemma between training considerably deeper network and gaining accuracy from largely increased depth. Finally, it has been well validated in the previous works [1, <ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b17">19]</ref> that multi-scale feature extraction scheme can bring performance improvement in many face related tasks. We conjecture that a more powerful CNN architecture could be introduced for emotion recognition in unconstrained conditions if above three facts can be jointly considered. To test our hypothesis, we present HoloNet, a deep yet computationally efficient CNN architecture, coupling all above facts naturally: (1) To reduce redundant filters and improve the non-saturated non-linearity in the lower convolutional layers, we apply a modified Concatenated Rectified Linear Unit (CReLU) <ref type="bibr" target="#b18">[20]</ref> instead of basic ReLU <ref type="bibr" target="#b11">[13]</ref>.</p><p>(2) To reap the accuracy gain from considerably increased depth and maintain efficiency, we combine powerful residual structure and CReLU to construct the middle layers. (3) To broaden network width and introduce multi-scale feature extraction property, the topper layers are designed as a variant of inceptionresidual structure <ref type="bibr" target="#b22">[24]</ref>. Besides, features from different modalities can be well combined for improved accuracy. Extensive experiments on EmotiW 2016 challenge well validate the efficacy of our method.</p><p>We will detail HoloNet architecture and show its performance on the video based emotion recognition sub-challenge of EmtoiW 2016 in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE PROPOSED METHOD</head><p>Figure <ref type="figure" target="#fig_1">1</ref> shows the flowchart of our HoloNet framework. It can be seen that the core modules (parts within the rectangle with red dashed border) of HoloNet are four stacks of convolutional layers abbreviated as conv_1, conv_2, conv_3 and conv_4. As shown in Figure <ref type="figure" target="#fig_1">1</ref>, we present three different building blocks namely Phase-Convolution Block, Phase-Residual Block and Inception-Residual Block, to construct these stacked convolutional layers. To make the whole paper self-contained, in what follows, we will start the description with data pre-processing and network input. The data pre-processing module includes four steps. We use a self-developed Adaboost based multi-view face detector <ref type="bibr" target="#b25">[27]</ref> to locate target face in the first frame of each video clip. After obtaining a face region in the first frame, Supervised Descent Method (SDM) <ref type="bibr" target="#b28">[30]</ref> is applied to track facial features over time. Based on the tracked facial features, related face image regions are cropped out and scaled to a standard size of 156×156 pixels. Subsequently, face frontalization is performed by back-projecting each scaled face image to an estimated 3D face coordinate system <ref type="bibr" target="#b8">[10]</ref>. The frontalized face images are further scaled to a size of 136×136 pixels. The last pre-processing step is to compensate for lighting effect by a popular Discrete Cosine Transform (DCT) based method <ref type="bibr">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Pre-processing and Network Input</head><p>After data pre-processing, the gray-scale face image together with its corresponding basic Local Binary Patterns (LBP) and mean LBP feature images <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b30">32]</ref> are feed-forwarded through our HoloNet, as a three-channel input image. It is worth noting that other types of feature images may also be fed to HoloNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Phase-Convolution Block</head><p>During training or feed-forwarding, the input of our HoloNet is a fixed-size image (128×128×3 pixels). The image is first passed through conv_1 which is a stack of composite layers formed by one Phase-Convolution Block. The structure of our Phase-Convolution Block is shown in Figure <ref type="figure" target="#fig_0">2</ref>. Unlike basic ReLU <ref type="bibr" target="#b11">[13]</ref>, Phase-Convolution Block utilizes a different feature activation scheme. Basic ReLU popularly used in deep CNNs retains the phase information but discards the modulus information when the phase of a filter response is negative. As validated in <ref type="bibr" target="#b18">[20]</ref>, the filters in the lower convolutional layers of a deep CNN form pairs in phase. This means the filters in the lower convolutional layers contain considerable redundancy. To reduce redundant filters and enhance the non-saturated non-linearity in the lower convolutional layers, we apply a modified Concatenated Rectified Linear Unit (CReLU) instead of basic ReLU. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, our CReLU makes an identical copy of the linear responses right after convolution, first negates responses, then concatenates copied responses and negated responses along channels, and finally applies ReLU altogether to obtain activation maps. In this way, both the positive and the negative phase information are well preserved with no additional hyper-parameters. This is the reason why such kind of building blocks is named as Phase-Convolution Block. Compared with the basic ReLU, the other merit of CReLU is that the number of convolutional filters can be reduced to half under the same number of response maps. In this paper, we apply batch normalization right after each convolution and before activation, following <ref type="bibr" target="#b9">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Phase-Residual Block</head><p>Following conv_1, there are two deep stacks of convolutional layers in HoloNet, namely conv_2 and conv_3 which are built by two Phase-Residual Blocks.</p><p>Recently, residual networks with largely increased depth have shown leading performance in the famous ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2015 <ref type="bibr" target="#b9">[11]</ref>. However, the network used in the ILSVRC 2015 is substantially deep (hundreds of layers), which means heavy computational cost both in training and feed-forwarding. Furthermore, such kind of deep residual networks is prone to incur over-fitting problem, thus increasing difficulties on training significantly. To overcome these problems, especially in emotion recognition task, our Phase-Residual Block combines aforementioned CReLU with residual structure.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the structures of our two Phase-Residual Blocks used for constructing conv_2 and conv_3. It can be seen that each Phase-Residual Block is composed of two basic bottleneck building blocks sharing similar layer structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Bottleneck Building Blocks</head><p>In <ref type="bibr" target="#b9">[11]</ref>, the authors have shown that bottleneck building block is more economical and powerful than non-bottleneck ones when constructing deep residual networks. For HoloNet, a bottleneck building block variant is designed, in which we replace originally basic ReLU with our CReLU and present a new residual variant, just as the blue dashed rectangle shown in Figure <ref type="figure" target="#fig_2">3</ref>. Specifically, our bottleneck building block is composed of three stacked convolutional layers with 1×1, 3×3 and 1×1 filters, where the first 1×1 convolution is responsible for dimensionality reduction and the other is used to increase dimensionality, leaving the middle 3×3 convolutional layer as a bottleneck . On top of the 3×3 convolution, CReLU operation is added. Our bottleneck building block is ended with a residual operation, performing element-wise addition channel by channel. Mathematically, the operations in a bottleneck building block can be formulated as</p><formula xml:id="formula_0">x W W x F y s i</formula><p>, where is the input, y is the output, and is the residual mapping in which i W denotes weight parameters to be learnt, denotes CReLU, and s W denotes the identity mapping or a linear projection matrix. If the dimensions of and are the same, identity mapping is used to form short connections, otherwise a linear projection by the shortcut connections is used to match dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Phase-Residual Block</head><p>In HoloNet, conv_2 and conv_3 are constructed by two similar Phase-Residual Blocks, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. It is clear that each Phase-Residual Block includes two bottleneck building blocks that have the same filter banks in respective stacked convolutional layers (1×1, 3×3 and 1×1 filters). In each Phase-Residual Block, the first bottleneck building block makes short connections by the identity mapping and the other makes short connections by a linear projection. For each Phase-Residual Block, two bottleneck building blocks compose a couple, increasing network depth while enhancing feature representation. In the first Phase-Residual Block (i.e. conv_2), a couple of two bottleneck building blocks extracts feature map with the same spatial size to the input. And in the second Phase-Residual Block (i.e. conv_3), the couple starts with a 1×1 convolution with stride 2 to down-sample feature map, and then performs subsequent operations. In this way, conv_3 can mine face characteristic through a larger scale compared with conv_2. With above Phase-Residual Blocks, our HoloNet can well enjoy accuracy gain from considerably increased depth and maintain efficiency both in training and feed-forwarding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Inception-Residual Block</head><p>The last stack of convolutional layers of HoloNet is conv_4 which is formed as a variant of Inception-Residual Block. Our Inception-Residual Block is inspired by two crucial factors. First, it has been well proved that high dimensional feature extracted in a multi-scale manner is necessary to achieve competitive results in many face related tasks. As well validated in the work of [1], with multi-scale facial feature extraction strategy, increasing feature dimensionality from 1K to over 100K can bring over 6% accuracy improvement in face recognition for five popular local descriptors. However, making a high dimensional feature will incur high cost on training, testing and storage. Second, the inception architecture proposed by Szegedy et al. <ref type="bibr" target="#b23">[25]</ref> provide an easy and safe way of incorporating multi-scale feature extractation into the network by increasing both the depth and the width, yielding better accuracy. However, increasing network depth and width means a larger number of learnable parameters, which will result in dramatically increased use of computational resources, and the network may also be more prone to overfitting. Recently, Szegedy et al. <ref type="bibr" target="#b22">[24]</ref> further give clear empirical evidence that the combinations of inception and residual connections can well accelerate the training.</p><p>In their paper, there is also experimental evidence to show that several variants of such inception residual networks outperform similarly expensive pure inception networks by a thin accuracy margin. Although inception residual networks have been proved to be useful in image classification task, how to design such a hybrid network which is suitable for emotion recognition is still an open problem. Here, we present an Inception-Residual Block variant, a novel and efficient module which elaborately embeds multi-scale hierarchical sparse feature extraction into the last stack of convolutional layers of HoloNet.</p><p>The detailed structure of our Inception-Residual Block is shown in Figure <ref type="figure">4</ref>. Note that there are four sibling branches whose inputs are the output of conv_3, acting as a multi-scale sparse feature re- presentation. From left to right, the first sibling branch in Figure <ref type="figure">4</ref> is a 1×1 convolution, akin to a 1×1 width filter illustrated in Figure <ref type="figure">5</ref> (a). The second sibling branch is a stack of 1×1 and 3×3 convolutions, akin to a 3×3 width filter illustrated in Figure <ref type="figure">5 (c</ref>).</p><p>The third and fourth sibling branches have their stacked layer configurations as shown in Figure <ref type="figure">4</ref>, akin to a 5×5 width filter and a 2×2 width filter illustrated in Figure <ref type="figure">5</ref> (d) and (b), respectively. It can be seen that all sibling branches except the first have filter expansion layers (1×1 or 3×3 convolution) which scale up the dimensions of the filter banks to compensate for reduction in spatial feature channels. The sibling branches are followed by a grouping layer in which the output feature vectors are sequentially concatenated into a single output feature vector. At the next stage, the concatenated feature vector is further compressed by a 1×1 convolution. Subsequently, there is a residual operation layer where the final multi-scale feature vector is added to the conv_3 s output with a linear projection of shortcut connections. Finally, we perform a max pooling operation for down-sampling directly after residual layer. HoloNet is ended with two fully connected layers: the first has 1024 channels, and the second performs 7-way (one for each emotion category) classification with a softmax.</p><p>It should be emphasized here that our Inception-Residual Block is uniquely designed for emotion recognition, in contrast to any existing inception-residual variant <ref type="bibr" target="#b22">[24]</ref>. Our design has two main merits. On the one hand, informative facial features, which are useful for discriminating target emotion categories, can be well extracted from micro scale to macro scale through inception unit. On the other hand, residual unit strengthens fast convergence and computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Model Efficiency</head><p>Besides the novel architecture of our HoloNet, it is also worth emphasizing its computational merit. Compared with popular classification CNN models such as AlexNet <ref type="bibr" target="#b11">[13]</ref>, VGG16 <ref type="bibr" target="#b20">[22]</ref> and 18-layer ResNet <ref type="bibr" target="#b9">[11]</ref>, our HoloNet model has much fewer convolutional filters and thus has much lower computational cost, although they have similar depths. Generally, our HoloNet model has 75 million FLOPs (multiply-add), which is only 6.6% of AlexNet (1.14 billion FLOPs), 4.17% of 18-layer ResNet (1.8 billion FLOPs) and 0.48% of VGG16 (15.5 billion FLOPs). Considering that NVIDIA GPU such as K40 can process with a speed of over 250 image crops per second when using AlexNet model, thus our HoloNet model can be well run on any mobile platform with real-time processing requirement. The summary of FLOPs of our HoloNet model is given in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PERFORMANCE EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parameter Settings</head><p>Challenge Data. We evaluate the performance of our HoloNet based method on the video based task of 4 th Emotion Recognition in the Wild (EmotiW) 2016 challenge. The video base emotion recognition task includes audio-video clips containing seven basic emotion categories, namely Angry, Disgust, Fear, Happy, Neutral, Sad and Surprise. In the previous challenges, all video clips are collected from Hollywood real movie records, thus numerous variations in viewpoint, lighting, background, occlusion, context and etc. are introduced. The task is to assign one unique emotion label from seven candidate categories to each test video clip. The major change in this year s challenge as compared to the earlier challenges is the introduction of reality TV data into the test set while not into the training and validation sets. Another minor change is that the volume of training video clips is also increased. Similarly, this year s data, i.e., AFEW 6.0 is also split into three parts: training set (773 video clips), validation set (383 video clips) and test set (593 video clips, and 54 out of them are real TV clips). It should be highlighted that all our models described below are trained on the given data only. That is, in the challenge, we do not apply any outside data for augmenting the performance of our models.</p><p>Implementation. Our visual models are mainly based on HoloNet. We train two HoloNet models (denoted as A and B for simplicity), whose network architectures are exactly the same, for evaluation. The popular Caffe tool <ref type="bibr" target="#b19">[21]</ref> is used to train our HoloNet models. In the implementation, we use the pre-processing steps described in Section 2.1 to normalize face image. The face image is finally scaled to a standard size of 136×136 pixels. A crop of 128×128 pixels is randomly sampled from the normalized face image or its horizontal flip for data augmentation. We initialize the weights as in <ref type="bibr" target="#b9">[11]</ref> and train two HoloNet models from scratch. The standard SGD with a batch size of 128 is set. The learning rate starts from 0.01 and is divided by 10 per 20000/40000 iterations or the error plateaus. Totally, the models are trained for up to 70000/120000 iterations. We use a weight decay of 0.0002 and a momentum of 0.9. Considering that spatio-temporal hand-crafted features have shown to be useful for improving the accuracy of CNN models when handling video based emotion recognition task <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b29">31]</ref>, we also extract improved Dense Trajectories (iDT) <ref type="bibr" target="#b27">[29]</ref> over each video clip to characterize the dynamics of visual facial motions. Given a face video clip, three popular hand-crafted descriptors, namely Histogram of Oriented Gradients (HoG) <ref type="bibr" target="#b2">[4]</ref>, Histogram of Optical Flow (HoF) <ref type="bibr" target="#b1">[3]</ref> and Motion Boundary Histogram (MBH) <ref type="bibr" target="#b26">[28]</ref>, are extracted. Fisher vectors for each of three descriptors are calculated first, and then are concatenated together as the final iDT descriptor. We use a linear Support Vector Machine (SVM) classifier to train our iDT model. Besides above visual models, similar to all previous participants, we also train an audio model to describe acoustic context cues. Our audio model is an SVM classifier with polynomial kernel (we set c=2.87 and g=0.0025). We use popular Opensmile tool <ref type="bibr" target="#b7">[9]</ref> to extract 1582-D acoustic feature over every labeled video clip for training an audio model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on AFEW 6.0</head><p>Results on the Validation Set. We first use validation set to explore the performance of different fusion strategies. Note we have four basic models: two HoloNet model A and B, an audio model and an iDT model. For a test video clip, both the audio model and the iDT model can predict the emotion scores of seven categories directly. However, the emotion scores of the HoloNet model are obtained in two steps. It sequentially operates on every frame first, and then the average of the predicted emotion scores over all frames is calculated and used as the final emotion score for the given test video clip. Following this evaluation procedure, on the validation set, the recognition rate of each of above four models is 42.82%, 44.57%, 35.77% and 32.11%, respectively. Since our visual method is mainly based on HoloNet, so we first test the fusion strategy using each HoloNet model and the audio model. As a result, we get the recognition rate of 47.78% and 48.83% when fusing HoloNet model A and B separately with the audio model, obtaining 4.96 and 4.26 percent accuracy gain to that of respective individual HoloNet model. Such improvements are on par with those reported from the previous winner teams <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b29">31]</ref>. Another common fact is that the ensemble of several same typed CNN models usually performs better than single one <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b20">22]</ref>. Inspired by this, we further test the performance of fusing our two HoloNet models with the audio model. Specifically, we get the recognition rate of 50.03% and 50.91% in another two fusions, yielding higher accuracy gains against former two fusions. The main difference of two new fusions is only in the contribution portions of different models. As demonstrated in <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b14">16]</ref>, deep hierarchical CNN features and conventional hand-crafted features are complementary in nature. Therefore, in the last fusion, we use all our four models and achieve an impressive recognition rate of 51.96% which is the best among our 5 fusion strategies. Detailed fusion results are summarized in Table <ref type="table">2</ref>.</p><p>Results on the Test Set. Our 5 submissions to the challenge are based on the aforementioned 5 fusion strategies evaluated on the validation set, respectively. The summary of the mean recognition rate obtained from each of our 5 submissions in the challenge is shown in Table <ref type="table">2</ref>. On the test set, our best recognition rate is 57.84%, outperforming the baseline of 40.47% with an absolute margin of 17.37%, and yielding 4.04% absolute accuracy gain compared to the result of last year s winner team <ref type="bibr" target="#b29">[31]</ref>. For each of 5 fusion strategies, it can be concluded from Table <ref type="table">2</ref> that larger accuracy improvement is consistently obtained on the test set than on the validation set. This may be partially attributed to training data augmentation by introducing the validation set. Generally, these results well demonstrate the efficacy of our HoloNet method. The confusion matrices regarding the fusion method of our best submission on the validation and the test sets are shown in Figure <ref type="figure">6</ref> (a) and (b), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussions</head><p>Result Analysis. Although the proposed HoloNet based method gets competitive results in the challenge, we find its capability in classifying predefined seven basic emotion categories is not well balanced. Table <ref type="table">3</ref> gives the per-class recognition rate concerning the method of our best submission. It can be seen that our method can well recognize Angry, Happy and Neutral, usually with above 70% recognition rate both on the test and the validation sets, yet it shows much lower accuracy in classifying the other four emotions. We can also see that our method gets an accuracy rate of 15% in recognizing Disgust on the validation set, but on the test set it produces a zero recognition rate. Similar phenomena can be also found in previous top-performing methods <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b29">31]</ref>. We think this may be attributed to two main factors regardless of common difficulties like different viewpoints, poses, lighting conditions, etc. First, in the dataset, there exists non-negligible ambiguity among different emotion categories (as can be seen from Figure <ref type="figure">6</ref> to some extent). Second, unbalanced training data distribution is another critical factor that may make the training procedure bias towards emotion categories with more training data. Future Improvement. According to above analysis, we believe that the performance can be further improved in two ways. One is to develop a tree based hierarchical method. In the method, seven emotion categories can be divided into several predefined nonoverlapping sub-groups. Emotion categories sharing sufficient ambiguity will be merged into a sub-group. In this way, a tree based model can be trained to recognize target emotions in a coarse-to-fine manner <ref type="bibr" target="#b12">[14]</ref>. The other one is to develop a robust learning strategy that can well handle unbalanced training data. Through bridging the performance gap from unbalanced training data, better recognition accuracy can be expected. We will explore them in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>This paper presents HoloNet, a deep yet computational efficient convolutional neural network for emotion recognition in the wild. Our framework elaborately combines several state-of-the-art CNN backbones, including CReLU, residual structure and inceptionresidual structure, which are by design naturally complementary to each other when handling tasks like emotion recognition in unconstrained conditions. It achieves considerably better accuracy in the video based task of EmotiW 2016 challenge compared with the baseline and previous winner counterparts. Besides, it is also computationally efficient during feed-forward inference, enabling the possible use in real-time applications. We believe that our framework naturally enjoys the benefits from the latest progress in the CNN field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Structure of the Phase-Convolution Block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The flowchart of our HoloNet based emotion recognition framework.</figDesc><graphic coords="2,317.88,221.76,239.16,135.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Structure of two paired Phase-Residual Blocks.</figDesc><graphic coords="3,317.88,55.32,239.16,229.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Structure of the Inception-Residual Block.</figDesc><graphic coords="4,55.20,338.88,58.20,58.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Summary of FLOPs in our HoloNet model.</head><label>1</label><figDesc></figDesc><table><row><cell>Layer name</cell><cell>Output size</cell><cell>FLOPs (million)</cell></row><row><cell>conv_1</cell><cell>128×128</cell><cell>19.27</cell></row><row><cell>conv_2</cell><cell>64×64</cell><cell>22.22</cell></row><row><cell>conv_3</cell><cell>32×32</cell><cell>10.85</cell></row><row><cell>conv_4</cell><cell>16×16</cell><cell>5.86</cell></row><row><cell>fc5</cell><cell>1024</cell><cell>16.79</cell></row><row><cell>fc6</cell><cell>7</cell><cell>0.01</cell></row><row><cell>Total</cell><cell></cell><cell>75.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This work was done by Dongqi Cai, Ping Hu, Shandong Wang and Liang Sha with equal contribution, led by Anbang Yao.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">REFERENCES</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Man</forename><surname>Systems</surname></persName>
		</author>
		<author>
			<persName><surname>Cybernetics</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Part B (Cybernetics)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="458" to="466" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Conference on Computer Vision</title>
		<meeting>the 9th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">886</biblScope>
			<biblScope unit="page">893</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild challenge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 15th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013. 2013</date>
			<biblScope unit="volume">509</biblScope>
			<biblScope unit="page">516</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild challenge 2014: Baseline, data and protocol</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction</title>
		<meeting>the 16th International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">461</biblScope>
			<biblScope unit="page">466</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video and image based emotion recognition challenges in the wild: Emotiw</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the17th ACM on International Conference on Multimodal Interaction</title>
		<meeting>the17th ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2015</date>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page">426</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emotiw 2016: Video and group-level emotion recognition challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the18th ACM on International Conference on Multimodal Interaction</title>
		<meeting>the18th ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">4295</biblScope>
			<biblScope unit="page">4304</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 15th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">543</biblScope>
			<biblScope unit="page">550</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotion recognition using a hierarchical binary decision tree approach</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1162" to="1171" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning multi-scale block local binary patterns for face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics</title>
		<meeting>the International Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="volume">828</biblScope>
			<biblScope unit="page">837</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 16th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">494</biblScope>
			<biblScope unit="page">501</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A mixed bag of emotions: Model, predict, and transfer emotion distributions</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">860</biblScope>
			<biblScope unit="page">868</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">816</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DIY deep learning for vision: A hands-on tutorial with caffe</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Vision</title>
		<meeting>the 13th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">511</biblScope>
			<biblScope unit="page">518</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">3551</biblScope>
			<biblScope unit="page">3558</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Capturing au-aware facial features and their latent relations for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 17th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">451</biblScope>
			<biblScope unit="page">458</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Mmachine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
