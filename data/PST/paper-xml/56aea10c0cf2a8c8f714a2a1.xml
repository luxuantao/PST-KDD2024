<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leaf segmentation in plant phenotyping: a collation study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hanno</forename><surname>Scharr</surname></persName>
							<email>h.scharr@fz-juelich.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Bio-and Geosciences: Plant Sciences (IBG</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Massimo</forename><surname>Minervini</surname></persName>
							<email>m.minervini@imtlucca.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Forschungszentrum Jülich GmbH</orgName>
								<address>
									<settlement>Jülich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">IMT Institute for Advanced Studies</orgName>
								<address>
									<settlement>Lucca</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>French</surname></persName>
							<email>andrew.p.french@nottingham.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Schools of Biosciences and Computer Science</orgName>
								<orgName type="department" key="dep2">Centre for Plant Integrative Biology</orgName>
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<settlement>Nottingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Klukas</surname></persName>
							<email>christian.klukas@lemnatec.de</email>
							<affiliation key="aff9">
								<orgName type="department">LemnaTec GmbH</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Kramer</surname></persName>
							<email>kramerd8@cns.msu.edu</email>
							<affiliation key="aff5">
								<orgName type="department">Departments of Energy Plant Research Lab, and Biochemistry and Molecular Biology</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<email>liux@cse.msu.edu</email>
							<affiliation key="aff6">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Imanol</forename><surname>Luengo</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Schools of Biosciences and Computer Science</orgName>
								<orgName type="department" key="dep2">Centre for Plant Integrative Biology</orgName>
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<settlement>Nottingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Michel</forename><surname>Pape</surname></persName>
							<email>pape@ipk-gatersleben.de</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Molecular Genetics</orgName>
								<orgName type="department" key="dep2">Leibniz Institute of Plant Genetics and Crop Plant Research (IPK)</orgName>
								<address>
									<settlement>Gatersleben</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gerrit</forename><surname>Polder</surname></persName>
							<email>gerrit.polder@wur.nl</email>
							<affiliation key="aff7">
								<orgName type="department">Greenhouse Horticulture</orgName>
								<orgName type="institution">Wageningen University and Research Centre</orgName>
								<address>
									<settlement>Wageningen</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danijela</forename><surname>Vukadinovic</surname></persName>
							<email>danijela.vukadinovic@wur.nl</email>
							<affiliation key="aff7">
								<orgName type="department">Greenhouse Horticulture</orgName>
								<orgName type="institution">Wageningen University and Research Centre</orgName>
								<address>
									<settlement>Wageningen</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
							<email>yinxi1@cse.msu.edu</email>
							<affiliation key="aff6">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sotirios</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Forschungszentrum Jülich GmbH</orgName>
								<address>
									<settlement>Jülich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">IMT Institute for Advanced Studies</orgName>
								<address>
									<settlement>Lucca</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leaf segmentation in plant phenotyping: a collation study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8FC05B805B24D783F4E0A5DDB71293FB</idno>
					<idno type="DOI">10.1007/s00138-015-0737-3</idno>
					<note type="submission">Received: 19 April 2015 / Revised: 11 September 2015 / Accepted: 27 October 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant phenotyping</term>
					<term>Leaf</term>
					<term>Multi-instance segmentation</term>
					<term>Machine learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image-based plant phenotyping is a growing application area of computer vision in agriculture. A key task is the segmentation of all individual leaves in images.</p><p>Here we focus on the most common rosette model plants, Arabidopsis and young tobacco. Although leaves do share MM and SAT acknowledge a Marie Curie Action: "Reintegration</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The study of a plant's phenotype, i.e. its performance and appearance, in relation to different environmental conditions is central to understanding plant function. Identifying and evaluating phenotypes of different cultivars (or mutants) of the same plant species, are relevant to, e.g. seed production and plant breeders. One of the most sought-after traits is plant growth, i.e. a change in mass, which directly relates to yield. Biologists grow model plants, such as Arabidopsis (Arabidopsis thaliana) and tobacco (Nicotiana tabacum), in controlled environments and monitor and record their phenotype to investigate general plant performance. While previously such phenotypes were annotated manually by experts, recently image-based nondestructive approaches are gaining attention among plant researchers to measure and study visual phenotypes of plants <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">54]</ref>.</p><p>In fact, most experts now agree that lack of reliable and automated algorithms to extract fine-grained information from these vast datasets forms a new bottleneck in our understanding of plant biology and function <ref type="bibr" target="#b36">[37]</ref>. We must accelerate the development and deployment of such computer vision algorithms, since according to the Food and Agriculture Organization of the United Nations (FAO), large-scale experiments in plant phenotyping are a key factor in meeting agricultural needs of the future, one of which is increasing crop yield for feeding 11 billion people by 2050.</p><p>Yield is related to plant mass and the current gold standard for measuring mass is weighing the plant; however, this is invasive and destructive. Several specialized algorithms have been developed to measure whole-plant properties and particularly plant size <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b60">60]</ref>. Nondestructive measurement of a plant's projected leaf area (PLA), i.e. the counting of plant pixels from top-view images, is considered a good approximation of plant size for rosette plants and is currently used. However, when studying growth, PLA reacts relatively weakly, as it includes growing and non-growing leaves, but the per-leaf-derived growth (implying a per-leaf segmentation), has a faster and clearer response. Thus, for example, growth regulation <ref type="bibr" target="#b4">[5]</ref> and stress situations <ref type="bibr" target="#b25">[26]</ref> can be evaluated in more detail. Additionally, since growth stages of a plant are usually based on the number of leaves <ref type="bibr" target="#b14">[15]</ref>, an estimate of leaf count as provided by leaf segmentation is beneficial.</p><p>However, obtaining such refined information at the individual leaf level (as for example in <ref type="bibr" target="#b55">[55]</ref>) which could help us identify even more important plant traits is, from a computer vision perspective, particularly challenging. Plants are not static, but changing organisms with complexity in shape and appearance increasing over time. Over a period of hours, leaves move and grow, with the whole plant changing over days or even months, in which the surrounding environmental (as well as measurement) conditions may also vary.</p><p>Considering also the presence of occlusions, it is not surprising that the segmentation of leaves from single view images (a multi-instance image segmentation problem) remains a challenging problem even in the controlled imaging of model plants. Motivated by this, we organized the Leaf Segmentation Challenge (LSC) of the Computer Vision Problems in Plant Phenotyping (CVPPP 2014) workshop, <ref type="foot" target="#foot_0">1</ref>held in conjunction with the 13th European Conference on Computer Vision (ECCV), to assess the current state of the art.</p><p>This paper offers a collation study and analysis of several methods from the LSC challenge, and also from the literature. We briefly describe the annotated dataset, the first of its kind, that was used to test and evaluate the methods for the segmentation of individual leaves in image-based plant phenotyping experiments (see Fig. <ref type="figure">1</ref> and also <ref type="bibr" target="#b46">[46]</ref>). Colour images in the dataset show top-down views on rosette plants. Two datasets show different cultivars of Arabidopsis (A. thaliana), while another one shows tobacco (N. tabacum) under different treatments. We manually annotated leaves in these images to provide ground truth segmentation and defined appropriate evaluation criteria. Several methods are briefly presented, and in the results section, we discuss and evaluate each method.</p><p>The remainder of this article is organized as follows: Sect. 2 offers a short literature review, while Sect. 3 defines the adopted evaluation criteria. Section 4 presents the datasets and annotations used to support the LSC challenge, which is described in Sect. 5. Section 6 describes the methods compared in this study, with their performance and results Fig. <ref type="figure">1</ref> Example images of Arabidopsis (A1, A2) and tobacco (A3) from the datasets used in this study <ref type="bibr" target="#b46">[46]</ref> discussed in Sect. 7. Finally, Sect. 8 offers conclusions and outlook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>At first glance, the problem of leaf segmentation appears similar to leaf identification and isolated leaf segmentation (see e.g. <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b58">58]</ref>), although as we will see later it is not. Research on these areas has been motivated by several datasets showing leaves in isolation cut from plants and imaged individually, or showing leaves on the plant but with a leaf encompassing a large field of view (e.g. by imaging via a smart phone application). This problem has been addressed in an unsupervised <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b58">58]</ref>, shape-based <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref>, and interactive <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> fashion.</p><p>However, the problem at hand is radically different. The goal, as the illustrative example of Fig. <ref type="figure">1</ref> shows, is not to identify the plant species (usually known in this context) but to segment accurately each leaf in an image showing a whole plant. This multi-instance segmentation problem is excep-tionally complex in the context of this application. This is not only due to the variability in shape, pose, and appearance of leaves, but also due to lack of clearly discernible boundaries among overlapping leaves with typical imaging conditions where a top-view fixed camera is used. Several authors have dealt with the segmentation of a live plant from background to measure growth using unsupervised <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref> and semisupervised methods <ref type="bibr" target="#b33">[34]</ref>, but not of individual leaves. The use of colour in combination with depth images or multiple images for supervised or unsupervised plant segmentation is also common practice <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b56">56]</ref>.</p><p>Several authors have considered leaf segmentation in a tracking context, where temporal information is available. For example, Yin et al. <ref type="bibr" target="#b60">[60]</ref> segment and track the leaves of Arabidopsis in fluorescence images using a Chamferderived energy functional to match available segmented leaf templates to unseen data. Dellen et al. <ref type="bibr" target="#b17">[18]</ref> use temporal information in a graph-based formulation to segment and track leaves in a high spatial and temporal resolution sequence of tobacco plants. <ref type="bibr">Aksoy et al. [3]</ref> track leaves over time, merging segments derived by superparametric clustering by exploiting angular stability of leaves. De Vylder et al. <ref type="bibr" target="#b15">[16]</ref> use an active contour formulation to segment and track Arabidopsis leaves in time-lapse fluorescence images.</p><p>Even in the general computer vision literature, this type of similar-appearance, multi-instance problem is not well explored. Although several interactive approaches exist <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40]</ref>, user interaction inherently limits throughput. Therefore, here we discuss automated learning-based object segmentation approaches, which might be adaptable to leaf segmentation. Wu and Nevatia <ref type="bibr" target="#b57">[57]</ref> present an approach that detects and segments multiple, partially occluded objects in images, relying on a learned, boosted whole-object segmentor and several part detectors. Given a new image, pixels showing part responses are extracted and a joint likelihood estimation inclusive of inter-object occlusion reasoning is maximized to obtain final segmentations. Notably, they test their approach on classical pedestrian datasets, where appearance and size variation does exist, so in leaf segmentation where neighbouring leaves are somewhat similar, this approach might yield less appealing results. Another interesting work <ref type="bibr" target="#b45">[45]</ref> relies on Hough voting to jointly detect and segment objects. Interestingly, beyond pedestrian datasets they also use a dataset of house windows where appearance and scale variation is high (as is common also in leaves), but they do not overlap. Finally, graphical methods have also been applied to resolve and segment overlapping objects <ref type="bibr" target="#b24">[25]</ref>, and were tested also on datasets showing multiple horses.</p><p>Evidently, until now, the evaluation and development of leaf segmentation algorithms using a common reference dataset of individual images without temporal information is lacking, and therefore is the main focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation measures</head><p>Measuring multi-object segmentation accuracy is an active topic of research with several metrics previously proposed <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b42">42]</ref>. For the challenge and this study, we adopted several evaluation criteria and devised Matlab implementations. Some of these metrics are based on the Dice score of binary segmentations:</p><formula xml:id="formula_0">Dice (%) = 2|P gt ∩ P ar | |P gt | + |P ar | , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>measuring the degree of overlap among ground truth P gt and algorithmic result P ar binary segmentation masks.</p><p>Overall, two groups of criteria were used. To evaluate segmentation accuracy we used:</p><p>-Symmetric Best Dice (SBD), the symmetric average Dice among all objects (leaves), where for each input label the ground truth label yielding maximum Dice is used for averaging, to estimate average leaf segmentation accuracy. Best Dice (BD) is defined as</p><formula xml:id="formula_2">BD(L a , L b ) = 1 M M i=1 max 1≤ j≤N 2|L a i ∩ L b j | |L a i | + |L b j | , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where | • | denotes leaf area (number of pixels), and L a i for 1 ≤ i ≤ M and L b j for 1 ≤ j ≤ N are sets of leaf object segments belonging to leaf segmentations L a and L b , respectively. SBD between L gt , the ground truth, and L ar , the algorithmic result, is defined as SBD(L ar , L gt ) = min BD(L ar , L gt ), BD(L gt , L ar ) .</p><p>(3) -Foreground-Background Dice (FBD), the Dice score of the foreground mask (i.e. the whole plant assuming the union of all leaf labels), to evaluate a delineation of a plant from the background obtained algorithmically with respect to the ground truth.</p><p>We should note that we also considered the Global Consistency Error (GCE) <ref type="bibr" target="#b31">[32]</ref> and Object-level Consistency Error (OCE) <ref type="bibr" target="#b42">[42]</ref> metrics, which are suited for evaluating segmentation of multiple objects. However, we found that they are harder to interpret and that the SBD is capable of capturing relevant leaf segmentation errors.</p><p>To evaluate how good an algorithm is in identifying the correct number of leaves present, we relied on:</p><p>-Difference in Count (DiC), the number of leaves in algorithm's result minus the ground truth:</p><formula xml:id="formula_4">DiC = #L ar -#L gt . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>-|DiC|, the absolute value of DiC.</p><p>In addition to the metrics adopted for the challenge, in Sect. 7.5, we augment our evaluation here with metrics that prioritize leaf shape and boundary accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Leaf data and manual annotations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Imaging setup and protocols</head><p>We devised imaging apparatus, setups, and experiments emulating typical phenotyping experiments, to acquire three imaging datasets, summarized in Table <ref type="table" target="#tab_0">1</ref>, to support this study. They were acquired in two different labs with highly diverse equipment. Detailed information is available in <ref type="bibr" target="#b46">[46]</ref>, but brief descriptions are given below for completeness.</p><p>Arabidopsis data: Arabidopsis images were acquired in two data collections, in June 2012 and in September-October 2013, hereafter named Ara2012 and Ara2013, respectively, both consisting of top-view time-lapse images of several Arabidopsis thaliana rosettes arranged in trays. Arabidopsis images were acquired using a setup for investigating affordable hardware for plant phenotyping. <ref type="foot" target="#foot_1">2</ref> Images were captured with a 7 megapixel consumer-grade camera (Canon Power-Shot SD1000) during day time only, every 6 h over a period of 3 weeks for Ara2012, and every 20 min over a period of 7 weeks for Ara2013.</p><p>Acquired raw images (3108 × 2324 pixels, pixel resolution of ∼0.167 mm) were first saved as uncompressed (TIFF) files, and subsequently encoded using the lossless compression standard available in the PNG file format <ref type="bibr" target="#b53">[53]</ref>.</p><p>Tobacco data: Tobacco images were acquired using a robotic imaging system for the investigation of automated plant treatment optimization by an artificial cognitive system. <ref type="foot" target="#foot_2">3</ref> The robot head consisted of two stereo camera systems, black-and-white and colour, each consisting of 2 Point-Grey Grashopper cameras with 5 megapixel (2448 × 2048, pixel size 3.45 µm) resolution and high-quality lenses (Schneider Kreuznach Xenoplan 1.4/17-0903). We added lightweight white and NIR LED light sources to the camera head.</p><p>Using this setup, each plant was imaged separately from different but fixed poses. In addition, for each pose, small baseline stereo image pairs were captured using each single camera, respectively, by a suitable robot movement, allowing for 3D reconstruction of the plant. For the top-view pose, distance between camera centre and top edge of pot varied between 15 and 20 cm for different plants, but being fixed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Selection of data for training and testing</head><p>As part of the benchmark data for the LSC, we released three datasets, named respectively 'A1', 'A2', and 'A3', consisting of single-plant images of Arabidopsis and tobacco, each accompanied by manual annotation (discussed in the next section) of plant and leaf pixels. Examples are shown in Fig. <ref type="figure">1</ref>.</p><p>From the Ara2012 dataset, to form the 'A1' dataset, we extracted cropped regions of 161 individual plants (500 × 530 pixels) from tray images, spanning a period of 12 days. An additional 40 images (530 × 565 pixels) form 'A2', which were extracted from the Ara2013 dataset spanning a period of 26 days. From the tobacco dataset, to form the 'A3' dataset, we extracted 83 images (2448×2048 pixels). Each dataset was split into training and testing sets for the challenge (cmp. Sect. 5).</p><p>The data differ in resolution, fidelity, and scene complexity, with plants appearing in isolation or together with other plants (in trays), with plants belonging to different cultivars (or mutants), and subjected to different treatments.</p><p>Due to the complexity of the scene and of the plant objects, the datasets present a variety of challenges with respect to analysis. Since our goal was to produce a wide variety of images, images corresponding to several challenging situations were included.</p><p>Specifically, in 'A1' and 'A2', on occasion, a layer of water from irrigation of the tray causes reflections. As the plants grow, leaves tend to overlap, resulting in severe leaf occlusions. Nastic movements (a movement of a leaf usually on the vertical axis) make leaves appear of different shapes and sizes from one time instant to another. In 'A3' beneath shape changes due to nastic movements, also different leaf shapes appear due to different treatments. Under high illumination conditions (one of the treatment options in the Tobacco experiments), plants stay more compact with partly wrinkled, severely overlapping leaves. Under lower light conditions, leaves are more rounded and larger.</p><p>Furthermore, the 'A1' images of Arabidopsis present a complex and changing background, which could complicate plant segmentation. A portion of the scene is slightly out of focus (due to the large field of view) and appears blurred, and some images include external objects such as tape or other fiducial markers. In some images, certain pots have moss on the soil, or have dry soil and appear yellowish (due to increased ambient temperature for a few days). 'A2' presents a simpler scene (e.g. more uniform background, sharper focus, without moss); however, it includes mutants of Arabidopsis with different phenotypes related to rosette size (some genotypes produce very small plants) and leaf appearance with major differences in both shape and size.</p><p>The images of tobacco in 'A3' have considerably higher resolution, making computational complexity more relevant. Additionally, in 'A3', plants undergo a wide range of treatments changing their appearance dramatically, while Arabidopsis is known to have different leaf shape among mutants. Finally, other factors such as self-occlusion, shadows, leaf hairs, and leaf colour variation make the scene even more complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Annotation strategy</head><p>Figure <ref type="figure">2</ref> depicts the procedure we followed to annotate the image data. In the first place, we obtained a binary segmentation of the plant objects in the scene in a computer-aided fashion. For Arabidopsis, we used the approach based on Fig. <ref type="figure">2</ref> Schematic of the workflow to annotate the images. Plants were first delineated in the original image and then individual leaves were labelled active contours described in <ref type="bibr" target="#b33">[34]</ref>, while for tobacco, a simple colour-based approach for plant segmentation was used. The result of this segmentation was manually refined using raster graphics editing software. Next, within the binary mask of each plant, we delineated individual leaves, following an approach completely based on manual annotation. A pixel with black colour denotes background, while all other colours are used to uniquely identify the leaves of the plants in the scene. Across the frames of the time-lapse sequence, we consistently used the same colour code to label the occurrences of the same leaf. The labelling procedure involved always two annotators to reduce observer variability, one annotating the dataset and one inspecting the other.</p><p>Note that LSC did not involve leaf tracking over time, therefore all individual plant images were considered separately, ignoring any temporal correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">File types and naming conventions</head><p>Plant images were encoded using the lossless PNG <ref type="bibr" target="#b53">[53]</ref> format and their dimensions varied. Plant objects appeared centred in the (cropped) images. Segmentation masks were image files encoded as indexed PNG, where each segmented leaf was identified with a unique (per image) integer value, starting from '1', whereas '0' denotes background. The union of all pixel labels greater than zero provides the ground truth plant segmentation mask. A colour index palette was included within the file for visualization reasons. The filenames have the form:</p><p>-plantXXX_rgb: the original RGB colour image; -plantXXX_label: the labelled image;</p><p>where XXX is an integer number. Note that plants were not numbered sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CVPPP 2014 LSC challenge outline</head><p>The LSC challenge was organized by two of the authors (HS and SAT), as part of the CVPPP workshop, which was held in conjunction with the European Conference on Computer Vision (ECCV), in Zürich, Switzerland, in September 2014. Electronic invitations for participation were communicated to a large number of researchers working on computer vision solutions for plant phenotyping and via computer vision and pattern recognition mailing lists and several phenotyping consortia and networks such as DPPN, 4 IPPN, 5 EPPN, 6   4 http://www.dppn.de/. 5 http://www.plant-phenotyping.org/. 6 http://www.plant-phenotyping-network.eu/. iPlant. <ref type="foot" target="#foot_3">7</ref> Interested parties were asked to visit the website and register for the challenge after agreeing to rules of participation and providing contact info via an online form.</p><p>Overall, 25 teams registered for the study and downloaded training data, 7 downloaded testing data, and finally 4 submitted manuscript and testing results for review at the workshop. For this study, we invited several of the participants (see Sect. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training phase</head><p>An example preview of the training set (i.e. one example image from each of the three datasets as shown in Fig. <ref type="figure">1</ref>) was released in March 2014 on the CVPPP 2014 website. The full training set, consisting of colour images of plants and annotations, was released in April 2014.</p><p>A total of 372 PNG images were available in 186 pairs of raw RGB colour images and corresponding annotations in the form of indexed images, namely 128, 31, and 27 images for 'A1', 'A2', and 'A3', respectively. Images of many different plants were included at different time points (growth stages). Participants were unaware of any temporal relationships among images, and were expected to treat each image in an individual fashion. Participants were allowed to tailor pipelines to each dataset and could choose supervised or unsupervised methods. Matlab evaluation functions were also provided to help participants assess performance on the training set using the criteria discussed in Sect. 3. The data and evaluation script are in the public domain. <ref type="foot" target="#foot_4">8</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Testing phase</head><p>We released 98 colour images for testing (i.e. 33, 9, and 56 images from 'A1', 'A2', and 'A3', respectively) and kept the respective label images hidden. Images here corresponded to plants at different growing stages (with respect to those included in the training set) or completely new and unseen plants. Again this was unknown to the participants. A short testing period was allowed: the testing set was released on June 9, 2014, and authors were asked to submit their results by June 17, 2014, and accompanying papers by <ref type="bibr">June 22, 2014.</ref> In order to assess the performance of their algorithm on the test set, participants were asked to email to the organizers a ZIP archive following a predefined folder/file structure that enabled automated processing of the results. Within 24 h, all participants who submitted testing results received their evaluation using the same evaluation criteria as for training, as well as summary tables in L A T E X and also individual perimage results in a CSV format. Algorithms and the papers were subject to peer review and the leading algorithm <ref type="bibr" target="#b40">[41]</ref> presented at the CVPPP workshop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Methods</head><p>We briefly present the leaf segmentation methods used in this collation study. We include methods not only from challenge participants but also others for completeness and for offering a larger view of the state of the art. Overall, three methods rely on post-processing of distance maps to segment leaves, while one uses a database of templates which are matched using a distance metric. Each method's description aims to provide an understanding of the algorithms, and wherever appropriate, we offer relevant citations for readers seeking additional information.</p><p>Please note that participating methods were given access to the training set (including ground truth) and testing set but without ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">IPK Gatersleben: segmentation via 3D histograms</head><p>The IPK pipeline relies on unsupervised clustering and distance maps to segment leaves. Details can be found in <ref type="bibr" target="#b40">[41]</ref>. The overall workflow is depicted in Fig. <ref type="figure" target="#fig_0">3</ref> and summarized in the following.</p><p>1. Supervised foreground/background segmentation utilizing 3D histogram cubes, which encode the probability for any observed pixel colour in the given training set of belonging to the foreground or background; and 2. Unsupervised feature extraction of leaf centre points and leaf split point detection for individual leaf segmentation by using a distance map, skeleton, and the corresponding graph representation (cmp. Fig. <ref type="figure" target="#fig_0">3</ref>).</p><p>To avoid the partitioning of the 3D histogram in rectangular regions <ref type="bibr" target="#b29">[30]</ref>, here a direct look-up in the 3D histogram cubes instead of (multiple) one-dimensional colour component thresholds is used. For this approach, two 3D histogram cubes for foreground and background are accumulated using the provided training data. To improve the performance against illumination variability, input images are converted into the Lab colour space <ref type="bibr" target="#b6">[7]</ref>. Entries which are not represented in the training data are estimated by using an interpolation of the surrounding values of a histogram cell. The segmentation results are further processed by morphological operations and cluster-analysis to suppress noise and artefacts. The outcome of this operation serves as input for the feature extraction phase to detect leaf centre points and optimal split points of corresponding leaf segments.</p><p>For this approach, the leaves of Arabidopsis plants in 'A1' and 'A2' are considered as compact objects which only partly overlap. In the corresponding Euclidean distance map (EDM), the leaf centre points appear as peaks, which are detected by a maximum search. At the next step, a skeleton image is calculated. To resolve leaf overlaps, split points at the thinnest connection points are detected. Values of the EDM are mapped on the skeleton image. The resulting image is used for creating a skeleton graph, where leaf centre points, skeleton end-points, and skeleton branch-points are represented as nodes in the graph. Edges are created if the corresponding image points are connected by a skeleton line. Additionally, a list of the positions and minimal distances of each particular edge segment are saved as an edge-attribute. This list is used to detect the exact positions of the leaf split points. In order to find the split point(s) between two leaf centre points, all nodes and edges of the graph structure connecting these two points are traversed and the position with the minimal EDM values is determined. This process is repeated, if there are still connections between the two leaves which need to be separated. For calculating the split line belonging to a particular minimal EDM point, two coordinates on the plant leaf border are calculated. The nearest background pixel is searched (first point), and also the nearest background pixel at the opposite position relative to the split point (second point) is located. The connection line is used as border during the segmentation of overlapping leaves. In a final step, the separated leaves are labelled by a region growing algorithm.</p><p>Our approach was implemented in Java, and tested on a desktop PC with 3.4 GHz processor and 32 GB memory. Java was configured to use a maximum of 4 GB RAM. On average each image takes 1.6, 1.2, and 9 seconds for 'A1', 'A2', and 'A3', respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Nottingham: segmentation with SLIC superpixels</head><p>A superpixel-based method that does not require any training is used. The training dataset has been used for parameter tuning only. The processing steps visualized in Fig. <ref type="figure">5</ref> can be summarized as follows:</p><p>1. Superpixel over-segmentation in Lab colour space using SLIC <ref type="bibr" target="#b0">[1]</ref>; 2. Foreground (plant) extraction using simple seeded region growing in the superpixel space; 3. Distance map calculation on extracted foreground; 4. Individual leaf seed matching by identifying the superpixels whose centroid lays in the local maxima of the distance map; and 5. Individual leaf segmentation by applying watershed transform with the extracted seeds.</p><p>Steps (1) and ( <ref type="formula" target="#formula_2">2</ref>) are used to extract the whole plant while (3), ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_10">5</ref>) for extracting the individual leaves. We now present a detailed explanation of each of the steps, with Figs. 4 and 5 summarizing the process.</p><p>Preparation. Given an RGB image, it is first converted to the Lab colour space <ref type="bibr" target="#b6">[7]</ref>, to enhance discrimination between Foreground extraction. Having the mean colour of each superpixel for channel 'a', a simple region growing approach <ref type="bibr" target="#b1">[2]</ref> in superpixel space allows the complete plant to be extracted. The superpixel with the lowest mean colour (the most bright green superpixel) defined in Lab space is used as the initial seed. However, for 'A1' and 'A2', since they do not contain shadows, an even simpler thresholding of the mean colour of each superpixel allows faster yet still accurate segmentation of the plant. Thresholds for the 'A1' and 'A2' are set to -25 and -15 respectively.</p><p>Leaf identification. Once the plant is extracted from the background, superpixels not belonging to the plant are removed. A distance map is computed (first removing strong edges using the Canny detector <ref type="bibr" target="#b10">[11]</ref>) and the centroids are calculated for all superpixels. A local maxima filter is applied to extract the superpixels that lay in the centre of the leaves. A superpixel is selected as a seed only if it is the most central in the leaf compared to its neighbours within a radius. This is implemented by considering the superpixel centroid value in the distance map, and filtering the superpixels that do not have the maximum value within its neighbours.</p><p>Leaf segmentation. Finally, watershed segmentation [51] is applied with the obtained initial seeds over the image space, yielding the individual leaf segmentation.</p><p>Using a Python implementation running on a i3 quad-core desktop with i3-4130 (3.4 GHz) processor and 8 GB memory, on average, each image takes &lt; 1 second for dataset 'A1' and 'A2', and 1-5 seconds for 'A3'.</p><p>Overall, it is a fast method with no training required. It also could be tuned to get a much higher accuracy on a perimage basis. The parameters that can be tuned are as follows:</p><p>(1) number of superpixels, (2) compactness of superpixels, (3) foreground extractor (threshold or region growing), ( <ref type="formula" target="#formula_4">4</ref>) parameters of the canny edge detector, and (5) colour space for SLIC, foreground extractor and canny edge detector. All those parameters were tuned in a per-dataset basis using the training set in order to maximize the mean Symmetric Best Dice score for each dataset. However, they can be easily tuned manually on a per-image basis if required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">MSU: leaf segmentation with Chamfer matching</head><p>The MSU approach extends a multi-leaf alignment and tracking framework <ref type="bibr" target="#b59">[59]</ref><ref type="bibr" target="#b60">[60]</ref><ref type="bibr" target="#b61">[61]</ref> to the LSC. As discussed in Sect. 2, this framework was originally designed for segmenting and tracking leaves in plant fluorescence videos where plant segmentation is straightforward due to the clean background. For the LSC, a more advanced background segmentation process was adopted.</p><p>The framework is motivated by the well-known Chamfer Matching (CM) <ref type="bibr" target="#b7">[8]</ref>, which aligns one object instance in an image with a given template. However, since there are large variations of leaves in plant images, it is infeasible to match leaves with only one template. Therefore, we generate a set of templates with different shapes, scales, and orientations. Specifically, H leaves with representative shapes (e.g. different aspect ratios) are selected from H images of the training set. Each leaf shape is scaled to S different sizes, and each size is rotated to R different orientations. This leads to a set of H × S × R leaf templates (5 × 9 × 24 for 'A1' and 'A2', 8 × 10 × 24 for 'A3') with labelled tip locations, which will be used in the segmentation process.</p><p>An accurate plant segmentation and edge map are critical to obtain reliable CM results. To this end, all RGB images are converted into the Lab colour space, and a threshold τ is applied to channel 'a' for estimating a foreground mask Morphological operations are applied to remove small edges (noise) and lines (leaf veins). A mask (Fig. <ref type="figure" target="#fig_2">6a</ref>) and edge map (Fig. <ref type="figure" target="#fig_2">6b</ref>) are cropped from the RGB image.</p><p>For each template, we search all possible locations on the edge map and find one location with the minimum CM distance. Doing so for all templates generates an overcomplete set of leaf candidates (Fig. <ref type="figure" target="#fig_2">6c</ref>). For each leaf candidate, we compute the CM score, its overlap with foreground mask, and the angle difference, which measures how well the leaf points to the centre of the plant. Our goal is to select a subset of leaf candidates as the segmentation result. First, we delete candidates with large CM scores, small overlap with the foreground mask, or a large angle difference. Second, we develop an optimization process <ref type="bibr" target="#b61">[61]</ref> to select an optimal set of leaf candidates by optimizing the minimal number of candidates with smaller CM distances and leaf angle differences to cover the foreground mask as much as possible. Third, all leaf candidates are selected as an initialization and gradient descent is applied to iteratively delete redundant leaf candidates, which leads to a small set of final leaf candidates.</p><p>As shown in Fig. <ref type="figure" target="#fig_2">6d</ref>, a finite number of templates cannot perfectly match all edges. We apply a multi-leaf tracking procedure <ref type="bibr" target="#b60">[60]</ref> to transform each template, i.e. rotation, scaling, and translation, to obtain an optimal match with the edge map. This is done by minimizing the summation of three terms: the average CM score, the difference between the synthesized mask of all candidates and the test image mask, and the average angle difference. The leaf alignment result provides initialization of the transformation parameters and gradient descent is used to update these parameters. When a leaf candidate becomes smaller than a threshold, we will remove it. After this optimization, the leaf candidates will match the edge map much more accurately (Fig. <ref type="figure" target="#fig_2">6e</ref>), which remedies the limitation of a finite set of leaf templates. Finally, we use the tracking result and foreground mask to generate a label image so that all foreground pixels, and only foreground pixels, have labels.</p><p>Only one leaf out of each of the H training images is used for template generation. The same pre-processing and segmentation procedures are conducted independently for each image of the training and testing set.</p><p>Using our Matlab implementation running on a quad-core desktop with 3.40 GHz processor and 32 GB memory, on average each image takes 63, 49, and 472 seconds for 'A1', 'A2', and 'A3' respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Wageningen: leaf segmentation with watersheds</head><p>The method consists of two steps: plant segmentation and separate leaf segmentation, illustrated in Fig. <ref type="figure" target="#fig_3">7</ref>. Plant segmentation from the background uses supervised classification with a neural network. Since the nature of the three datasets ('A1', 'A2', and 'A3') is different, a separate classifier and post-processing steps are applied to each individual set. The ground truth images are used to mask plant and background pixels. For all images, 3000 pixels of each class are randomly selected for training. When the plant is smaller than 3000 pixels, all plant pixels are used. To separate the plants from the background, four colour and two texture features are used for each pixel. The colour features used in the classification are red, green, and blue pixel values (R, G, B) and the excessive Green value (2G-R-B) which highlights green pixels. For texture features, the pixel values of the variance filtered green channel <ref type="bibr" target="#b62">[62]</ref>, and the pixel values of the gradient magnitude filtered green channel are used. The latter two highlight edges and rough parts in the image.</p><p>A large range of linear and nonlinear classifiers have been tested on each dataset, with a feed-forward (MLP) neural network with one hidden layer of 10 U giving the best results. Morphological operations are applied on the binary image obtained after plant classification, resulting in the plant masks (i.e. a foreground-background segmentation). For 'A1' and 'A2', the morphological operations consist of an erosion followed by a propagation using the original results as mask. Small blobs mainly from moss are removed this way. For 'A3', all blobs in the image are removed, except for the largest one.</p><p>In order to remove moss that occurs in 'A2' and 'A3' and in order to emphasize spaces between stems and leaves (cf. Fig. <ref type="figure">8</ref>) to which the watershed algorithm is highly sensitive, additional colour transformation, shape and spatial filtering, and morphological operations are applied. For 'A2', all components of the foreground segmentation are filtered out that are further away from the centre of gravity of the foreground mask than 1.5 times estimated radius of the foreground mask. The radius r is estimated from mask area A as r = (A/π )</p><formula xml:id="formula_6">1 2 .</formula><p>Next, the Y-component image of the YUV colour transformation, giving the luminance, is thresholded with a threshold optimized on the training set (th = 85). For 'A3', there are cases of large moss areas attached to the foreground segmentation mask. To remove them, first the compactness C of the foreground mask is calculated as C = L 2 /(4π A), where L is the foreground mask contour length. C &gt; 20 indicates the presence of a large moss area segmented as foreground. There, the X-component of the XYZ colour transformation yielding chromatic information is thresholded (th = 55), and the pixels that are smaller than the threshold are filtered out. In this way, the moss pixels which have a slightly different colour than the plants are removed from the foreground image. Next, in order to emphasize spaces between the leaves and the stems, all foreground masks are corrected with the Fig. <ref type="figure">8</ref> Wageningen: accentuating holes thresholded Y-component of the YUV colour-transformed as described for 'A2'. The second step, i.e. separate leaf segmentation, is performed using a watershed method <ref type="bibr" target="#b8">[9]</ref> applied on the Euclidean distance map of the resulting plant mask image from the first step of the method. Initially, the watershed transformation is computed without applying the threshold between the basins. In the second step, the basins are successively merged if they are separated by a watershed that is smaller than a given threshold. The threshold value is tuned on the training set in order to produce the best result. The thresholds are set to 30, 58, and 70 for the datasets 'A1', 'A2', and 'A3' respectively.</p><p>Plant segmentation is done in Matlab 2015a and the perClass classification toolbox (http://perclass.com) on a MacBook with 2.53 GHz Intel Core 2 Duo. Learning the neural network classifier using a training set of 6000 pixels takes about 4 s per image. Plant segmentation using this trained classifier and post-processing take 0.76 s, 0.73 s and 24 s for 'A1', 'A2', and 'A3' respectively. Moss removal and leaf segmentation are performed in Halcon, running on a laptop with 2.70 GHz processor and 8 GB memory. On average, each image takes 160, 110, and 700 ms for 'A1', 'A2', and 'A3' respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>In this section, we discuss the performance of each method as evaluated on testing and training sets. Note that the ground truth was available to participants (authors of this study) for the training set; however, the testing set was only known to the organizers of the LSC (i.e. S. A. Tsaftaris, H. Scharr, and M. Minervini) and was unavailable to all others. Training set numbers are provided by the participants (with the same evaluation function and metrics used also on the testing set).</p><p>Note that since Nottingham is an unsupervised method, the results reflect directly the performance on all the training set. With MSU, since they use some of the leaves in the training set to define their templates some bias could exist, but it is minimal. IPK and Wageningen apply supervised methods to obtain foreground segmentation, using the whole dataset (Wageningen use a random selection of 3000 pixels per class per image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Plant segmentation from background</head><p>Figure <ref type="figure" target="#fig_4">9</ref> shows selected examples of test images from the three datasets. We choose from each dataset two examples: one to show the effectiveness of the methods and one to show limitations. We show visually the segmentation outcomes for each method together with ground truth; we also overlay the numbers of the evaluation measures on the images.</p><p>Overall, we see that most methods perform well in separating plant from background, except when the background presents challenges (e.g. moss) as does the second image shown for 'A1'. Then FBD scores are lower for almost all methods, with IPK and Nottingham showing more robustness. These observations are evident in the whole dataset (cf. FBD numbers in Tables <ref type="table" target="#tab_1">2,</ref><ref type="table" target="#tab_2">3</ref>). Average testing numbers are lower than training for most methods with the exception of Nottingham, which does significantly better in 'A2' and 'A3' in the testing case. Given that their method is unsupervised, this behaviour is not unexpected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Leaf segmentation and counting</head><p>Referring again to Fig. <ref type="figure" target="#fig_4">9</ref> and Tables <ref type="table" target="#tab_1">2,</ref><ref type="table" target="#tab_2">3</ref>, let us evaluate visually and quantitatively how well algorithms do in segmenting leaves. When leaves are not overlapping, all methods perform well. Nevertheless, each method exhibits different behaviour. IPK, MSU, and Wageningen obtain higher SBD scores; however, IPK does produce straight line boundaries that are not natural-they should be more curved to better match leaf shape. There seems to be also an interesting relationship between segmentation error and leaf size (see also next section for effects related to plant size).</p><p>In fact, plotting leaf size vs. Dice per leaf, <ref type="foot" target="#foot_5">9</ref> see Fig. <ref type="figure">10</ref>, we observe that with all methods larger leaves are more accurately delineated, with exception of the largest few leaves in MSU. Dice for smaller leaves shows more scatter and smaller leaves are more frequently not detected, as evidenced by the high symbol density at Dice = 0 (dark blue symbols). For small leaves with (leaf area) 1 2 20 Wageningen performs best, detecting more leaves than the others and with higher accuracy. IPK shows better performance than others in the mid range 40 (leaf area) 1 2 80 due to higher perleaf accuracy (see the more dark/black symbols in the region above Dice = 0.95) and fewer non-detected leaves. In the mid range, only Wageningen performs similarly with respect to leaf detection (fewest symbols at Dice = 0), closely followed by MSU.</p><p>We should note that measuring SBD and FBD with Dice does have some limitations. If a method reports a Dice score of 0.9, this loss of 0.1 can be attributed to either an under-segmentation (e.g. loss of a stem in Arabidopsis, nonprecise leaf boundary) or an over-segmentation (considering background as plant). Therefore, in Sect. 7.5, we apply two measures being more sensitive to shape consistency, in order to investigate the solutions' performance with respect to leaf boundaries. For viewing ease, matching leaves are assigned the same colour as the ground truth. With regard to leaf counting, most methods show their limitations, and in fact using such a metric also highlights errors in leaf segmentation. For example, in Fig. <ref type="figure" target="#fig_4">9</ref>, we see that when the images are more challenging, some methods merge leaves: this lowers SBD scores but affects count numbers even more critically. Other methods (e.g. Wageningen) tend to over-segment and consider other parts as leaves (see for example the second image of 'A1' in Fig. <ref type="figure" target="#fig_4">9</ref>), which sometimes leads to over estimating numbers. These misestimations are evident throughout training and testing sets (cf. Tables <ref type="table" target="#tab_1">2,</ref><ref type="table" target="#tab_2">3</ref>). Stepping away from the summary statistics of the tables, over and under estimation are readily apparent in Fig. <ref type="figure">11</ref>. All algorithms present counting outliers, where MSU yields the least count variability, despite a clear underestimation. The mean DiC of Wageningen is the closest to zero, albeit featuring the highest variances. We also observe that DiC lowers as the number of leaves increases, particularly in the case of 'A3'. </p><p>Average values are shown for metrics described in Sect. 3 and in parenthesis standard deviation. 'ALL' denotes the average (and standard deviation) among the three datasets for each method. Other shorthands and abbreviations as defined in text (Sects.</p><p>The best results for each metric are denoted in bold</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Plant growth and complexity</head><p>Plants are complex and dynamic organisms that grow over time, and move throughout the day and night. They grow differentially, with younger leaves growing faster than mature ones. Therefore, per-leaf growth is a better phenotyping trait when evaluating growth regulation and stress situations. As they grow, new leaves appear and plant complexity changes: in tobacco, more leaves overlap and exhibit higher nastic movements; and in Arabidopsis, younger leaves emerge, overlapping other more mature ones.</p><p>At an individual leaf level, the findings of Fig. <ref type="figure">10</ref>-Dice of smaller leaves showing higher variability scatter and with smaller leaves being missed-illustrate that we need to achieve homogeneous performance and robustness if we want to obtain accurate per-leaf growth estimates.</p><p>Using classical growth stages, which rely on leaf count as a marker of growth, the downwards slope seen in Fig. <ref type="figure">11</ref> could be attributed to growth. This is more clear in Fig. <ref type="figure">12</ref>, where we see that with more leaves, leaf segmentation accuracy (SBD) also decreases.</p><p>Even if we consider plant size (measured as PLA, i.e. the size of the plant in ground truth, obtained as the union of all </p><p>Shorthands and abbreviations as in Table <ref type="table" target="#tab_1">2</ref> The best results for each metric are denoted in bold leaf masks), we observe a decreasing trend in SBD for each method with plant size, see Fig. <ref type="figure" target="#fig_0">13</ref>. Observe the large variability in SBD when plants are smaller. Even isolating it to a single method we see that when plants are small, depending on the plant's leaf arrangement, variability is extremely high: either good (close to 80 %) or rather low SBD values are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Effect of foreground segmentation accuracy</head><p>In Fig. <ref type="figure">14</ref>, we plot FBD versus SBD for each method pooling the testing data together. We see that high SBD can only be achieved when FBD is also high; but obtaining a high FBD is not at all a guarantee for good leaf segmentation (i.e. a high SBD) since we observe large variability in SBD even when FBD is high. This prompted us to evaluate the performance of the leaf segmentation part isolating it from errors in the plant (foreground) segmentation step. Thus, we asked participants to submit results on the training set assuming that also a foreground (plant) mask is given (as obtained by the union of leaf masks), effectively not requiring a plant segmentation step.</p><p>Naturally, all methods benefit when the ground truth plant segmentation is used: compare SBD, DiC, and |DiC| between Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_3">4</ref>. SBD improves considerably in most Fig. <ref type="figure">12</ref> Effect of plant complexity (measured as number of leaves) on leaf segmentation accuracy, i.e. SBD, for 'A3'. Each method is marker coded separately mentation. Overall, additional investment in obtaining better performing foreground segmentation is therefore warranted.</p><p>Comparing the count numbers (DiC and |DiC| in Tables <ref type="table" target="#tab_1">2,</ref><ref type="table" target="#tab_3">4</ref>), the best performer is Wageningen, with slight overestimation in Table <ref type="table" target="#tab_1">2</ref> and slight under-estimation Table <ref type="table" target="#tab_3">4</ref>, while again all other methods under-estimate the number of leaves present. Even when foreground plant mask is given, these numbers do not improve significantly. So it is not errors in the foreground segmentation component that cause such performance, but the inherent assumption of low overlap that each method relies on to find leaves. As a result, most approaches miss small leaves and sometimes miscount other plant parts for leaves. The Wageningen algorithm is more resilient to this problem, presumably due to the optimization of the basins threshold. When the threshold increases, the leaf count decreases. The thresholds were tuned with respect to the best SBD, but apparently this also affects DiC. A positive effect is also due to emphasizing spaces between leaves and stems, avoiding the problem of small spaces between leaves being wrongly segmented as foreground, resulting in a higher number of leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Performance under blinded shape-based metrics</head><p>Most of the metrics we adopted for the challenge rely on segmentation-and area-based measurements (cf. Sect. 3). It is thus of interest to see how the methods perform on metrics that evaluate boundary accuracy and best preserve leaf shape. Notice that these metrics were not available to the participants (hence the term blinded), so methods have not been optimized Shorthands and abbreviations as in Table <ref type="table" target="#tab_1">2</ref> The best results for each metric are denoted in bold for such metrics. For brevity, we present results on the testing set only. We adopt two metrics based, respectively, on the Modified Hausdorff Distance (MHD) <ref type="bibr" target="#b18">[19]</ref> and Pratt's Figure of Merit (FoM) <ref type="bibr" target="#b43">[43]</ref>, to compare point sets A and B denoting leaf object boundaries.</p><p>The Modified Hausdorff Distance (MHD) <ref type="bibr" target="#b18">[19]</ref> measures the displacement of object boundaries as the average of all the distances from a point in A to the closest point in B. With</p><formula xml:id="formula_10">D(A , B) = 1 |A | p∈A min q∈B p -q , (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where • is the Euclidean distance, MHD is defined as:</p><formula xml:id="formula_12">MHD(A , B) = max {D(A , B), D(B, A )} . (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>This metric is known to be suitable for comparing template shapes with targets <ref type="bibr" target="#b18">[19]</ref>. It prioritizes leaf boundary accuracy, being relevant for shape-based leaf recognition purposes. Pratt's Figure of Merit (FoM) <ref type="bibr" target="#b43">[43]</ref> was introduced in the context of edge detection and penalizes missing or displaced points between actual (A ) and ideal (I ) boundaries:</p><formula xml:id="formula_14">FoM(A , I ) = 1 max{|A |, |I |} |A | i=1 1 1 + αd 2 i , (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>where α = 1/9 is a scaling constant penalizing boundary offset, and d i is the distance between an actual boundary point and the nearest ideal boundary point. Let B ar and B gt be sets of leaf object boundaries extracted from leaf segmentation masks L ar and L gt , respectively, where B ar is the algorithmic result and B gt is the ground truth. To evaluate how well leaf object shape and boundaries are preserved, and to follow the spirit of SBD defined in Sect. 3, we use:</p><p>-Symmetric Best Hausdorff (SBH), the symmetric average MHD among all object (leaf) boundaries, where for each input label the ground truth label yielding minimum MHD is used for averaging. Best Hausdorff (BH) is defined as</p><formula xml:id="formula_16">BH(B a , B b ) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ √ w 2 + h 2 if either B a = ∅ or B b = ∅ 1 M M i=1 min 1≤ j≤N MHD(B a i , B b j ) otherwise<label>(8)</label></formula><p>where B a i for 1 ≤ i ≤ M and B b j for 1 ≤ j ≤ N are point sets corresponding to the boundaries, respectively, B a and B b , of leaf object segments belonging to leaf segmentations L a and L b ; w and h denote, respectively, width and height of the image containing the leaf object. SBH is then: SBH(B ar , B gt ) = max BH(B ar , B gt ), BH(B gt , B ar ) . <ref type="bibr" target="#b8">(9)</ref> SBH is expressed in units of length (e.g. pixels or millimetres) and is 0 for perfectly matching boundaries. If B ar is empty, SBH is equal to the image diagonal (i.e. the greatest possible distance between any two points).</p><p>-Best Figure of Merit (BFoM), the average FoM among all leaf objects, where for each input label the ground truth label yielding maximum FoM is used for averaging.</p><formula xml:id="formula_17">BFoM(B ar , B gt ) = 1 M M i=1 max 1≤ j≤N</formula><p>FoM(B ar i , B gt j ), <ref type="bibr" target="#b9">(10)</ref> We express BFoM as a percentage, where 100 % denotes a perfect match. Shorthands and abbreviations as in Table <ref type="table" target="#tab_1">2</ref>. Notice that for SBH lower is better, whereas for BFoM higher is better The best results for each metric are denoted in bold</p><p>In Table <ref type="table" target="#tab_4">5</ref>, we see the results on the testing set using these metrics. SBH values vary widely between dataset 'A3' and the others ('A1' and 'A2'). This indicates an issue when using SBH with images of different size. Being a distance, SBH given in pixels is dependent on resolution. We therefore also provide this in real-world units i.e. mm, even though object resolution depends on (the non-constant) local object distance from the camera.</p><p>Overall, MSU reports best average performance according to SBH (although this result is largely influenced by the 'A3' dataset) with IPK performing best on 'A1' and 'A2'. The good performance of MSU does not come unexpected, as optimizing the Chamfer Matching distance boils down to minimizing D(A , B) from Eq. ( <ref type="formula" target="#formula_10">5</ref>), leading to SBH.</p><p>With respect to BFoM, IPK again performs best on 'A1' and 'A2', while Nottingham outperforms the other methods on 'A3'. Interestingly, the overall ranking of the methods according to the two metrics is opposite.</p><p>MSU exhibits lower variance compared to IPK, Nottingham, and Wageningen, since the latter methods include some empty segmentations (i.e. no leaf objects found) in the testing results, which in BFoM evaluates to 0, and in SBH to the image diagonal length. This situation occurs for some images of very small plants, which are probably missed in the plant segmentation steps of the methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Differences among datasets</head><p>Although the tobacco dataset, 'A3', has higher resolution and leaf boundaries are more evident, rich shape variation and large overlap among leaves challenge all methods: almost all achieve lower performance compared to 'A1' and 'A2' (Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref>). Even the variability in accuracy increases for 'A3'. The MSU algorithm shows the least variability among datasets probably due to the fact that it uses templates (rotated and scaled). As such it can adapt better to different shape variability and heavier occlusions and is more robust to plant segmentation errors. It is also due to the reliance on an edge map to fit the templates: on 'A3' it can be estimated more reliably compared to 'A1' and 'A2', where some images can be blurry (due to larger field of view) and resolution is lower. However, when foreground is known (Table <ref type="table" target="#tab_3">4</ref>), variability of the Wageningen solution also becomes lower between datasets.</p><p>While 'A2' does contain images from different mutants, it shows a different image background with respect to 'A1' (black textured tray vs. red smoother tray). When a plant mask is known, SBD results on the training set show (Table <ref type="table" target="#tab_3">4</ref>) that Nottingham, MSU, and Wageningen still do better in 'A1' than in 'A2', and all methods show higher variances in 'A2' than in 'A1'. So it might appear that different mutants play a role; however, this result is not conclusive since 'A2' has fewer images than 'A1'. In fact, a simple unpaired t-test between SBD in 'A1' and 'A2' shows no statistical difference (for any of the methods).</p><p>We should point out that both Nottingham and Wageningen use the same mechanism to segment leaves: a watershed on the distance (from the boundary) map. However, Nottingham relies on finding first the centres and then using those as seeds for leaf segmentation, while Wageningen obtains an over-segmentation and then merges parts using a threshold on the basins. Their performance difference due to this algorithm selection becomes apparent when comparing results with given foreground segmentation (Table <ref type="table" target="#tab_1">2</ref>). We see that the Wageningen algorithm does better compared to the Nottingham solution. We conclude that finding suitable seeds for segmentation is hard and further, comparing Fig. <ref type="figure">10</ref>, that this is true especially for small leaves. On the other hand, it appears that the Wageningen algorithm finds a suitable threshold for merging according to the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Discussions on experimental work</head><p>Through this study, we find that plant segmentation can be achieved with unsupervised approaches reaching average accuracy above 90 %. As we suspected, whenever complications in the background are present, they do lower plant segmentation accuracy (explaining also the large variation in performance). Possibly higher performance (and lower variability) can be obtained with methods relying on learned classifiers and active contour models <ref type="bibr" target="#b33">[34]</ref>. Lower plant segmentation accuracy negatively affects leaf segmentation accuracy in almost all cases. Nevertheless, a first level measurement of plant growth (as PLA) can be achieved with a relatively good accuracy, although methods that obtain high average and low variance should be sought-after.</p><p>On the other hand, measuring individual leaf growth on the basis of leaf segmentations shows currently low accuracy. The algorithms presented here show an average accuracy of 62.0 % (best 63.8 %, see Table <ref type="table" target="#tab_2">3</ref>) in segmenting a leaf and almost always miss some leaves, particularly under heavy occlusions when both small (young) and larger (mature) leaves are present within the same plant. SBD does not necessarily capture that, but it is evident when analysing leaf size vs. Dice accuracy and leaf counts. On several occasions, leaf count is not accurate (missing several leaves), and frequently the algorithms are wrongly labelling disconnected leaf parts (particularly their stems) as leaves. <ref type="foot" target="#foot_6">10</ref>Several approaches (IPK and Nottingham) assume that once a centre of a leaf is found that segmentation can be obtained by region growing methods. Naturally, when leaves heavily overlap they do fail to identify the centres (and find less leaf centres than in the ground truth), which holds for both rosette plants considered here. Also when image contrast is not ideal, lack of discernible edges leads to a misestimation of leaf boundaries. This is particularly evident in the Arabidopsis data ('A1' and 'A2') and affects approaches that rely on edge information (MSU). The tobacco dataset ('A3'), being high resolution, does offer superior contrast, but the amount of overlap and shape variation is significant leading to under-performance for most of the algorithms.</p><p>We also investigate performance on leaf boundaries using SBH and BFoM (cf. Sect. 7.5). SBH penalizes boundary regions being far away from where they should be, while BFoM acknowledges boundaries being in the right position. Thus, from a shape-sensitive viewpoint, low SBH is needed if boundary outliers lead to low performance, whereas high BFoM is advisable if an algorithm is robust against boundary outliers. When choosing from the solutions presented here, a trade-off needs to be found, as high BFoM (good) comes with high SBH (bad) and vice versa.</p><p>Evident by the meta-analysis of all results is the effect of plant complexity (due to plant age, mutant, or treatment) on algorithmic accuracy. Leaf segmentation accuracy decreases with larger leaf count (Fig. <ref type="figure">12</ref>), using leaf count as a proxy for maturity <ref type="bibr" target="#b14">[15]</ref>. This is expected: as the plant grows and becomes more complex, more leaves and higher overlap between young and mature leaves are present. Overall, most methods face greater difficulties in detecting and segmenting smaller (younger) leaves (Fig. <ref type="figure">10</ref>), most likely not due to their size, but overlap: they tend to grow on top of more mature leaves.</p><p>Moving forward, no approach here relies on learning a model on the basis of the training data to obtain leaf segmentations and this might lead to promising algorithms in the future. Interestingly, some of our findings on learning to count leaves do show that leaf count can be estimated without segmentation <ref type="bibr" target="#b20">[21]</ref>. However, individual and accurate leaf segmentation is still important: for example, studying individual leaf growth, tracking leaf position and orientation, classifying young from old leaves, and others.</p><p>One alternative which changes the problem definition and may reduce complexity is to provide additional data such as temporal (time-lapse images) and/or depth (stereo and multiview imagery) information. The former can be used for better leaf segmentation, e.g. via joint segmentation and tracking approaches <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b60">60]</ref>. Both types of information will help in resolving occlusions and obtaining better boundaries. Such data are publicly available in the form of a curated dataset <ref type="bibr" target="#b34">[35]</ref>, and a software tool was released to facilitate leaf annotation <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and outlook</head><p>This paper presents a collation study of a variety of leaf segmentation algorithms as tested within the confines of a common dataset and a true scientific challenge: the Leaf Segmentation Challenge of CVPPP 2014. This is the first of such challenges in the context of plant phenotyping and we believe that such formats will help advance the state of the art of this societally important application of machine vision.</p><p>Having annotated data in the public domain is extremely beneficial and this is one of the greatest outcomes of this work. The data can be used not only to motivate and enlist interest from other communities but also to support future challenges (similar to this one). We all believe that here is the future: it is via such challenges that the state of the art advances rapidly and a new challenge for 2015 has already been held. 11 However, these challenges should happen in a rolling fashion, year-round, with leader boards and automated evaluation systems. It is for this reason that we are considering a web-based system, e.g. similar in concept to Codalab, 12 for people to submit results but also deposit new annotated datasets. This has been proven useful in other areas of computer vision (consider for example PASCAL VOC <ref type="bibr" target="#b19">[20]</ref>) and it will benefit also plant phenotyping. 11 See the new Leaf Counting Challenge of CVPPP 2015 at BMVC (http://www.plant-phenotyping.org/CVPPP2015). 12 https://www.codalab.org/.</p><p>In summary, the better we can "see" plant organs such as leaves via new computer vision algorithms, evaluated on common datasets and collectively presented, the better quality phenotyping we can do and the higher the societal impact.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Workflow of the IPK approach, including main processing components: segmentation, image feature extraction (including leaf detection), and individual leaf segmentation</figDesc><graphic coords="7,325.69,56.63,198.52,562.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 Fig. 5</head><label>45</label><figDesc>Fig. 4 Example images of each of the steps in the Nottingham approach. First row original image (left), SLIC superpixels (centre), thresholded superpixels (right). Second row distance map with superpixel centroids (left), filtered superpixel centroids (middle), watershed segmentation (right)</figDesc><graphic coords="8,308.68,56.12,232.36,163.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6</head><label>6</label><figDesc>Fig.<ref type="bibr" target="#b5">6</ref> Overview of the MSU approach: training is done once for each plant type (i.e. twice for three datasets), and pre-processing and segmentation are performed for each image</figDesc><graphic coords="9,308.68,124.85,232.36,63.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Steps of the Wageningen approach, shown on an image from 'A3' (top left, with zoomed detailed shown in red box): test RGB image (top left), neural network-based foreground segmentation (top middle), inverse distance image transform (top right), watershed basins (bottom left), intersection of basins and the foreground image mask (bottom middle), final leaf segmentation after rejecting small regions (bottom right) (colour figure online)</figDesc><graphic coords="10,53.56,480.47,232.60,154.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 Selected results on test images. From each dataset 'A1'-'A3', an easier and more challenging image is shown, together with ground truth, and results of IPK, Nottingham, MSU, and Wageningen (from top to bottom, respectively). Numbers in the image corners are num-</figDesc><graphic coords="12,53.65,56.78,487.69,460.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure best viewed in colour (colour figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 Fig. 11</head><label>1011</label><figDesc>Fig. 10 Dataset 'A1': Dice score per leaf versus (leaf area) 1 2 , i.e. ground truth average leaf radius (in pixels). Larger symbols refer to larger leaves. Colour also indicates Dice score for better visibility. Figure best viewed in colour (colour figure online)</figDesc><graphic coords="14,53.65,56.72,487.69,294.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 Fig. 14</head><label>1314</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref> Effect of plant size, measured as number of plant pixels in ground truth, (projected leaf area) on leaf segmentation accuracy, i.e. SBD, for 'A3'. Each method is marker coded separately</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,53.56,56.69,232.60,344.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of information of the Arabidopsis and tobacco experiments providing the three datasets</figDesc><table><row><cell>Experiment</cell><cell>Subjects</cell><cell>Wild-types</cell><cell>Mutants</cell><cell>Period (days)</cell><cell>Image resolution</cell><cell>Field of view</cell><cell>Per plant resolution</cell></row><row><cell>Ara2012</cell><cell>19</cell><cell>Col-0</cell><cell>No</cell><cell>21</cell><cell>7 MPixel</cell><cell>Whole tray</cell><cell>0.25 MPixel</cell></row><row><cell>Ara2013</cell><cell>24</cell><cell>Col-0</cell><cell>Yes (4)</cell><cell>49</cell><cell>7 MPixel</cell><cell>Whole tray</cell><cell>0.25 MPixel</cell></row><row><cell>Tobacco (23.01.2012)</cell><cell>20</cell><cell>Samsun</cell><cell>No</cell><cell>18</cell><cell>5 MPixel</cell><cell>Single plant</cell><cell>5 MPixel</cell></row><row><cell>Tobacco (16.02.2012)</cell><cell>20</cell><cell>Samsun</cell><cell>No</cell><cell>20</cell><cell>5 MPixel</cell><cell>Single plant</cell><cell>5 MPixel</cell></row><row><cell>Tobacco (15.05.2012)</cell><cell>20</cell><cell>Samsun</cell><cell>No</cell><cell>18</cell><cell>5 MPixel</cell><cell>Single plant</cell><cell>5 MPixel</cell></row><row><cell>Tobacco (10.08.2012)</cell><cell>20</cell><cell>Samsun</cell><cell>No</cell><cell>30</cell><cell>5 MPixel</cell><cell>Single plant</cell><cell>5 MPixel</cell></row><row><cell cols="5">per plant, resulting in lateral resolutions between 20 and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>25 pixel/mm.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Data used for this study stemmed from experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">aiming at acquiring training data and contained a single</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">plant imaged directly from above (top-view). Images were</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">acquired every hour, 24/7, for up to 30 days.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Segmentation and counting results on the training set</figDesc><table><row><cell></cell><cell>SBD (%)</cell><cell>FBD (%)</cell><cell>|DiC|</cell><cell>DiC</cell></row><row><cell>IPK</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>74.2 (7.7)</cell><cell>97.4 (1.8)</cell><cell>2.6 (1.8)</cell><cell>-1.9 (2.5)</cell></row><row><cell>A2</cell><cell>80.6 (8.7)</cell><cell>99.7 (0.3)</cell><cell>0.9 (1.0)</cell><cell>-0.3 (1.3)</cell></row><row><cell>A3</cell><cell>61.8 (19.1)</cell><cell>98.2 (1.1)</cell><cell>2.1 (1.7)</cell><cell>-2.1 (1.7)</cell></row><row><cell>ALL</cell><cell>73.5 (11.5)</cell><cell>98.0 (1.9)</cell><cell>2.2 (1.7)</cell><cell>-1.7 (2.3)</cell></row><row><cell>Nottingham</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>68.0 (7.4)</cell><cell>94.6 (1.6)</cell><cell>3.8 (2.0)</cell><cell>-3.6 (2.4)</cell></row><row><cell>A2</cell><cell>60.9 (18.5)</cell><cell>87.5 (19.7)</cell><cell>2.5 (1.5)</cell><cell>-2.5 (1.5)</cell></row><row><cell>A3</cell><cell>47.1 (25.0)</cell><cell>79.4 (34.5)</cell><cell>2.3 (1.8)</cell><cell>-2.3 (1.9)</cell></row><row><cell>ALL</cell><cell>63.8 (15.3)</cell><cell>91.2 (16.2)</cell><cell>3.4 (2.0)</cell><cell>-3.2 (2.2)</cell></row><row><cell>MSU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>78.0 (6.4)</cell><cell>95.8 (1.9)</cell><cell>2.3 (1.5)</cell><cell>-2.3 (1.6)</cell></row><row><cell>A2</cell><cell>72.3 (9.5)</cell><cell>94.1 (4.1)</cell><cell>1.6 (1.4)</cell><cell>-1.3 (1.7)</cell></row><row><cell>A3</cell><cell>69.6 (16.5)</cell><cell>95.0 (6.5)</cell><cell>1.4 (1.5)</cell><cell>-1.3 (1.5)</cell></row><row><cell>ALL</cell><cell>75.8 (9.6)</cell><cell>95.4 (3.4)</cell><cell>2.1 (1.5)</cell><cell>-2.0 (1.7)</cell></row><row><cell cols="2">Wageningen</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>72.8 (7.8)</cell><cell>95.0 (2.4)</cell><cell>2.2 (2.0)</cell><cell>0.4 (3.0)</cell></row><row><cell>A2</cell><cell>71.7 (8.0)</cell><cell>95.2 (2.4)</cell><cell>1.3 (1.1)</cell><cell>-0.6 (1.6)</cell></row><row><cell>A3</cell><cell>69.6 (19.9)</cell><cell>96.1 (5.1)</cell><cell>1.7 (2.4)</cell><cell>0.6 (2.9)</cell></row><row><cell>ALL</cell><cell>72.2 (</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Segmentation and counting results on the testing set</figDesc><table><row><cell></cell><cell>SBD (%)</cell><cell>FBD (%)</cell><cell>|DiC|</cell><cell>DiC</cell></row><row><cell>IPK</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>74.4 (4.3)</cell><cell>97.0 (0.8)</cell><cell>2.2 (1.3)</cell><cell>-1.8 (1.8)</cell></row><row><cell>A2</cell><cell>76.9 (7.6)</cell><cell>96.3 (1.7)</cell><cell>1.2 (1.3)</cell><cell>-1.0 (1.5)</cell></row><row><cell>A3</cell><cell>53.3 (20.2)</cell><cell>94.1 (13.3)</cell><cell>2.8 (2.5)</cell><cell>-2.0 (3.2)</cell></row><row><cell>ALL</cell><cell>62.6 (19.0)</cell><cell>95.3 (10.1)</cell><cell>2.4 (2.1)</cell><cell>-1.9 (2.7)</cell></row><row><cell>Nottingham</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>68.3 (6.3)</cell><cell>95.3 (1.1)</cell><cell>3.8 (1.9)</cell><cell>-3.5 (2.4)</cell></row><row><cell>A2</cell><cell>71.3 (9.6)</cell><cell>93.0 (4.2)</cell><cell>1.9 (1.7)</cell><cell>-1.9 (1.7)</cell></row><row><cell>A3</cell><cell>51.6 (16.2)</cell><cell>90.7 (20.4)</cell><cell>2.5 (2.4)</cell><cell>-1.9 (2.9)</cell></row><row><cell>ALL</cell><cell>59.0 (15.6)</cell><cell>92.5 (15.6)</cell><cell>2.9 (2.3)</cell><cell>-2.4 (2.8)</cell></row><row><cell>MSU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>66.7 (7.6)</cell><cell>94.0 (1.9)</cell><cell>2.5 (1.5)</cell><cell>-2.5 (1.5)</cell></row><row><cell>A2</cell><cell>66.6 (7.9)</cell><cell>87.7 (3.6)</cell><cell>2.0 (1.5)</cell><cell>-2.0 (1.5)</cell></row><row><cell>A3</cell><cell>59.2 (17.8)</cell><cell>95.0 (5.2)</cell><cell>2.3 (1.9)</cell><cell>-2.3 (1.9)</cell></row><row><cell>ALL</cell><cell>62.4 (14.8)</cell><cell>94.0 (4.7)</cell><cell>2.4 (1.7)</cell><cell>-2.3 (1.8)</cell></row><row><cell cols="2">Wageningen</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>71.1 (6.2)</cell><cell>94.7 (1.5)</cell><cell>2.2 (1.6)</cell><cell>1.3 (2.4)</cell></row><row><cell>A2</cell><cell>75.7 (8.4)</cell><cell>95.1 (2.0)</cell><cell>0.4 (0.5)</cell><cell>-0.2 (0.7)</cell></row><row><cell>A3</cell><cell>57.6 (24.8)</cell><cell>89.5 (22.3)</cell><cell>3.0 (4.9)</cell><cell>1.8 (5.5)</cell></row><row><cell>ALL</cell><cell>63.8 (</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Segmentation and counting results on the training set assuming foreground segmentation known</figDesc><table><row><cell></cell><cell>SBD (%)</cell><cell>|DiC|</cell><cell>DiC</cell></row><row><cell>IPK</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>79.1 (5.5)</cell><cell>2.1 (1.4)</cell><cell>-1.9 (1.7)</cell></row><row><cell>A2</cell><cell>80.7 (10.8)</cell><cell>1.2 (1.3)</cell><cell>-1.1 (1.4)</cell></row><row><cell>A3</cell><cell>71.0 (20.6)</cell><cell>1.8 (1.8)</cell><cell>-1.8 (1.8)</cell></row><row><cell>ALL</cell><cell>78.2 (10.4)</cell><cell>1.9 (1.5)</cell><cell>-1.8 (1.7)</cell></row><row><cell>Nottingham</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>71.0 (7.2)</cell><cell>4.4 (1.7)</cell><cell>-4.4 (1.7)</cell></row><row><cell>A2</cell><cell>66.5 (21.6)</cell><cell>2.5 (1.5)</cell><cell>-2.7 (1.5)</cell></row><row><cell>A3</cell><cell>59.5 (11.3)</cell><cell>2.4 (1.3)</cell><cell>-2.4 (1.3)</cell></row><row><cell>ALL</cell><cell>68.6 (12.1)</cell><cell>3.9 (1.9)</cell><cell>-3.9 (1.9)</cell></row><row><cell>MSU</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>78.5 (5.5)</cell><cell>2.5 (1.4)</cell><cell>-2.5 (1.4)</cell></row><row><cell>A2</cell><cell>77.4 (8.1)</cell><cell>1.6 (1.3)</cell><cell>-0.9 (1.9)</cell></row><row><cell>A3</cell><cell>76.1 (14.1)</cell><cell>1.2 (1.2)</cell><cell>-1.1 (1.2)</cell></row><row><cell>ALL</cell><cell>78.0 (7.8)</cell><cell>2.2 (1.4)</cell><cell>-2.0 (1.6)</cell></row><row><cell>Wageningen</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>77.3 (4.9)</cell><cell>1.5 (1.3)</cell><cell>-0.3 (2.0)</cell></row><row><cell>A2</cell><cell>75.5 (8.0)</cell><cell>1.3 (1.3)</cell><cell>-0.9 (1.6)</cell></row><row><cell>A3</cell><cell>76.5 (14.6)</cell><cell>1.4 (1.3)</cell><cell>-1.3 (1.4)</cell></row><row><cell>ALL</cell><cell>76.9 (7.6)</cell><cell>1.5 (1.3)</cell><cell>-0.5 (1.9)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Segmentation results on the testing set with respect to leaf shape</figDesc><table><row><cell></cell><cell>SBH (pix)</cell><cell>SBH (mm)</cell><cell>BFoM (%)</cell></row><row><cell>IPK</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>9.2 (3.2)</cell><cell>1.54 (0.53)</cell><cell>62.6 (7.3)</cell></row><row><cell>A2</cell><cell>6.9 (3.6)</cell><cell>1.15 (0.60)</cell><cell>66.9 (8.1)</cell></row><row><cell>A3</cell><cell>174.9 (442.8)</cell><cell>7.00 (17.7)</cell><cell>41.9 (17.3)</cell></row><row><cell>ALL</cell><cell>103.6 (343.5)</cell><cell>4.62 (13.4)</cell><cell>51.1 (17.6)</cell></row><row><cell>Nottingham</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>13.0 (5.9)</cell><cell>2.17 (0.99)</cell><cell>58.7 (9.0)</cell></row><row><cell>A2</cell><cell>9.3 (5.7)</cell><cell>1.55 (0.95)</cell><cell>62.3 (7.4)</cell></row><row><cell>A3</cell><cell>193.6 (589.7)</cell><cell>7.74 (23.6)</cell><cell>49.2 (21.8)</cell></row><row><cell>ALL</cell><cell>115.8 (453.1)</cell><cell>5.30 (17.8)</cell><cell>53.6 (18.1)</cell></row><row><cell>MSU</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>13.3 (5.6)</cell><cell>2.22 (0.94)</cell><cell>50.9 (10.3)</cell></row><row><cell>A2</cell><cell>10.0 (6.3)</cell><cell>1.67 (1.05)</cell><cell>52.0 (12.2)</cell></row><row><cell>A3</cell><cell>81.1 (105.6)</cell><cell>3.24 (4.22)</cell><cell>46.5 (19.0)</cell></row><row><cell>ALL</cell><cell>51.7 (86.6)</cell><cell>2.76 (3.25)</cell><cell>48.5 (16.1)</cell></row><row><cell>Wageningen</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A1</cell><cell>13.0 (5.6)</cell><cell>2.17 (0.94)</cell><cell>54.1 (9.1)</cell></row><row><cell>A2</cell><cell>10.2 (7.8)</cell><cell>1.70 (1.30)</cell><cell>61.1 (9.7)</cell></row><row><cell>A3</cell><cell>109.1 (227.0)</cell><cell>4.36 (9.08)</cell><cell>43.7 (26.7)</cell></row><row><cell>ALL</cell><cell>67.7 (177.6)</cell><cell>3.38 (6.90)</cell><cell>48.8 (21.8)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.plant-phenotyping.org/CVPPP2014.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.phenotiki.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.garnics.eu/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>http://www.iplantcollaborative.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>http://www.plant-phenotyping.org/CVPPP2014-dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>To measure Dice per leaf, we first find matches between a leaf in ground truth and an algorithm's result that maximally overlap, and then report the Dice (Eq. 1) of matched leaves; for non-matched leaves a zero is reported.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>This indicates that additional (possibly tailored) evaluation metrics may be necessary, although our testing with some common in the literature did not yield any improvement.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank participants of the 2014 CVPPP workshop for comments and annotators that have contributed to this work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grant" (Grant #256534) of the EU's Seventh Framework Programme (FP7/2007-2013). HS acknowledges funding from EU-FP7 no. 247947 (GARNICS). HS, JMP, and CK acknowledge the support of the German-Plant-Phenotyping Network, which is funded by the German Federal Ministry of Education and Research (Project Identification Number: 031A053). XY, XL, and DK acknowledge the support of US Department of Energy, Office of Science, Basic Energy Sciences Program (DE-FG02-91ER20021) and the MSU centre for Advanced Algal and Plant Phenotyping.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeded region growing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="647" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling leaf growth of rosette plants using infrared stereo image sequences</title>
		<author>
			<persName><forename type="first">E</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abramov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wörgötter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dellen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="78" to="90" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D modelling of leaves from color and ToF data for robotized plant measuring</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3408" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A growth phenotyping pipeline for Arabidopsis thaliana integrating image analysis and rosette area modeling for robust quantification of genotype effects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arvidsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mueller-Roeber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Phytol</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="895" to="907" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagebased phenotyping of the mature Arabidopsis shoot system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Haxhimusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Kropatsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8928</biblScope>
			<biblScope unit="page" from="231" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Color image segmentation using CIELab color space using ant colony optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="28" to="34" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Parametric correspondence and chamfer matching: two new techniques for image matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
	<note>Tech. rep, DTIC</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The watershed transformation applied to image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scanning Microsc. Int</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="299" to="314" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A stereo imaging system for measuring structural parameters of plant canopies</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biskup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rascher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Cell Environ</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1299" to="1308" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">IFSC/USP at ImageCLEF 2012: plant identification task</title>
		<author>
			<persName><forename type="first">D</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Florindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CLEF</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Online Working Notes/Labs/Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ReVeS participation: tree species classification using random forests and botanical features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding leaves in natural images: a model-based approach for tree species identification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">117</biblScope>
			<biblScope unit="page" from="1482" to="1501" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A scale for coding growth stages in tobacco crops</title>
		<author>
			<persName><forename type="first">C</forename><surname>Coresta</surname></persName>
		</author>
		<ptr target="http://www.coresta.org/Guides/Guide-No07-Growth-Stages_Feb09.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Leaf segmentation and tracking using probabilistic parametric active contours</title>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vylder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ochoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chaerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Der Straeten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision/Computer Graphics Collaboration Techniques</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="75" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rosette Tracker: an open source image analysis tool for automatic quantification of genotype effects</title>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vylder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Vandenbussche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Der Straeten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Physiol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1149" to="1159" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Growth signatures of rosette plants from time-lapse video</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinform</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A modified Hausdorff distance for object matching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IAPR International Conferenced on Pattern Recognition</title>
		<meeting>the 12th IAPR International Conferenced on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="566" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to count leaves in rosette plants</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision Problems in Plant Phenotyping (CVPPP) Workshop</title>
		<meeting>the Computer Vision Problems in Plant Phenotyping (CVPPP) Workshop</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PHENOP-SIS, an automated platform for reproducible phenotyping of plant responses to soil water deficit in Arabidopsis thaliana permitted the identification of an accession with low sensitivity to soil water deficit</title>
		<author>
			<persName><forename type="first">C</forename><surname>Granier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aguirrezabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chenu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Cookson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dauzat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hamard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thioux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bouchier-Combaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lebaudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tardieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Phytol</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="635" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HTPheno: an image analysis pipeline for high-throughput plant phenotyping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Czauderna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An exemplar-based CRF for multi-instance object segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simultaneous phenotyping of leaf growth and chlorophyll fluorescence via GROWSCREEN FLUORO allows detection of stress tolerance in Arabidopsis thaliana and other rosette plants</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biskup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rascher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Briem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dreissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Metzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Funct. Plant Biol</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10/11</biblScope>
			<biblScope unit="page" from="902" to="914" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Corn plant sensing using real-time stereo vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Field Robot</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="591" to="608" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric leaf classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kalyoncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ö</forename><surname>Toygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="102" to="109" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integrated analysis platform: an open-source information system for high-throughput plant phenotyping</title>
		<author>
			<persName><forename type="first">C</forename><surname>Klukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pape</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Physiol</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="506" to="518" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Color image segmentation using histogram multithresholding and fusion</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kurugollu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Harmanci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Still image objective segmentation evaluation using ground truth</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strintzis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
	<note>In: 5th COST 276 Workshop</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-based plant phenotyping with incremental learning and active contours</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Abdelsamea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecol. Inform</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="35" to="48" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Special Issue on Multimedia in Ecology and Environment</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Finelygrained annotated datasets for image-based plant phenotyping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fschbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An interactive tool for semi-automated leaf annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision Problems in Plant Phenotyping (CVPPP) Workshop</title>
		<meeting>the Computer Vision Problems in Plant Phenotyping (CVPPP) Workshop</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="6" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image analysis: the new bottleneck in plant phenotyping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="126" to="131" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The leaf angle distribution of natural plant populations: assessing the canopy with a novel software tool</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller-Linow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pinto-Espinosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rascher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GROWSCREEN-Rhizo is a novel phenotyping robot enabling simultaneous measurements of root and shoot growth for plants grown in soil-filled rhizotrons</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blossfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dimaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kastenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Kleinert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fiorani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schurr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Funct. Plant Biol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="891" to="904" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatially varying color distributions for interactive multilabel segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nieuwenhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1234" to="1247" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3-D histogram-based segmentation and leaf detection for rosette plants</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Workshops</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">8928</biblScope>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An evaluation metric for image segmentation of multiple objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Polak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1223" to="1227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Pratt</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagebased plant modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="599" to="604" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hough regions for joining instance localization and segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sternig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7574</biblScope>
			<biblScope unit="page" from="258" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Annotated image datasets of rosette plants</title>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/2128/5848" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Forschungszentrum Jülich GmbH</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. FZJ-2014-03837</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Comparative assessment of feature selection and classification techniques for visual inspection of pot plant seedlings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cugnasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="47" to="55" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient segmentation of leaves in semi-controlled conditions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V B</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1623" to="1643" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Surface modelling of plants from stereo images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Edmondson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on 3-D Digital Imaging and Modeling (3DIM &apos;07)</title>
		<meeting>the 6th International Conference on 3-D Digital Imaging and Modeling (3DIM &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="312" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Leaf segmentation, classification, and three-dimensional recovery from a few images with close viewpoints</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Watersheds in digital spaces: an efficient algorithm based on immersion simulations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="583" to="598" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SPICY: towards automated phenotyping of large pepper plants in the greenhouse</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Der Heijden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Polder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Palloix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Eeuwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glasbey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Funct. Plant Biol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="870" to="877" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Portable network graphics (PNG) specification</title>
		<author>
			<persName><surname>W3c</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dynamics of seedling growth acclimation towards altered light conditions can be quantified via GROWSCREEN: a setup and procedure designed for rapid optical phenotyping of different plant species</title>
		<author>
			<persName><forename type="first">A</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zierer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Virnich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jünger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schurr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Phytol</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="447" to="455" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The modular character of growth in Nicotiana tabacum plants under steady-state nutrition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schurr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Bot</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="1169" to="1177" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An adaptive thresholding algorithm of field leaf image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="23" to="39" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Detection and segmentation of multiple, partially occluded objects by grouping, merging, assigning part detection responses</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="204" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Automatic plant identification from photographs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tirkaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="1369" to="1383" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multi-leaf alignment from fluorescence plant images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="437" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-leaf tracking from fluorescence plant videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="408" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Kramer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00353</idno>
		<title level="m">Multi-Leaf Segmentation, Alignment and Tracking from Fluorescence Plant Videos</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Edge detection techniques: an overview</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabbone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recogn. Image Anal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="537" to="559" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
