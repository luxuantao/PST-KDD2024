<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deriving and matching image fingerprint sequences for mobile robot localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Lamon</surname></persName>
							<email>pierre.lamon@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country>EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Illah</forename><surname>Nourbakhsh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Björn</forename><surname>Jensen</surname></persName>
							<email>bjoern.jensen@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country>EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
							<email>roland.siegwart@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country>EPFL</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deriving and matching image fingerprint sequences for mobile robot localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3F7F93AE084E27F2F4D38C919B62E992</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a method for creating unique identifiers, called fingerprint sequences, for visually distinct locations by recovering statistically significant features in panoramic color images. Fingerprint sequences are expressive enough for mobile robot localization, as demonstrated using a minimum energy sequence-matching algorithm that is described. Empirical results in two different places demonstrate the reliability of the system for global localization on a Nomad Scout mobile robot.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Vision-based localization has recently witnessed a newfound popularity. The CCD Camera is a popular choice for mobile robot sensing because it is not inherently dependent on environmental geometry like ranging devices <ref type="bibr" target="#b12">[13]</ref>. Therefore, it is hoped that a transition to indoor and outdoor navigation will be more straightforward with vision despite that each of them has their proper challenges.</p><p>Simple ranging devices require integration over time and high-level reasoning to accomplish localization. In contrast, vision has the potential to provide enough information to uniquely identify the robot's position.</p><p>Recent vision-based navigation methods have overcome the challenges of vision to produce mobile robots that can track their position using only a CCD camera. Some of the successful work is currently limited to indoor navigation because of its dependence on ceiling features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, room geometry, or artificial landmark placement <ref type="bibr" target="#b15">[16]</ref>. Other means for visual localization are applicable both indoors and outdoors, however they are designed to collect image statistics while foregoing recognition of specific scene features, or landmarks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>This research aims to create a visual localization system based on recognition of sets of visual features. Our goal is to implement a system with a minimal number of implicit assumptions regarding the environment, such that the system may be directly applicable both outdoors and indoors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE FINGERPRINT SEQUENCE</head><p>As the fingerprints of a person are unique, so each location has its own unique visual characteristics (save in pathological circumstances). The thesis of this localization system is that a unique virtual fingerprint of the current location can be created and that the sequence generation methods can be made insensitive to small changes in robot position. If locations are denoted by unique fingerprints in this manner, then the actual location of a mobile robot may be recovered by constructing a fingerprint using its current view and comparing this test fingerprint to its database of known fingerprints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fingerprint sequence encoding</head><p>We propose to create a fingerprint by assuming that a set of feature extractors can identify significant features in the image. Furthermore, we use a 360 degrees panoramic image because the orientation as well as the position of the robot may not be known a priori.</p><p>We define a fingerprint as a circular list of features, where the ordering of the set matches the relative ordering of the features in the panoramic image. In order to encode efficiently this circular list, we denote the fingerprint sequence using a list of characters, where each character represents the instance of a specific feature type.</p><p>Although any number of feature detectors may be used in an implementation of our system, we have used only two in our implementation thus far: a vertical edge detector and a color patch detector. We use the letter 'v' to characterize a vertical edge and the letters A,B,C,...,P to represent hue bins as detected by the color patch detector (See Fig. <ref type="figure" target="#fig_4">6</ref>,7 and 13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Extraction of edges and color features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge detection</head><p>Edge features are of particular value in artificial environments such as indoor office buildings. For these reasons, they have been popular throughout prior work in vision-based localization <ref type="bibr" target="#b0">[1]</ref>. Like other researchers, we have chosen to concentrate on vertical edges because of the instability and rarity of horizontal edges due to projection effects.</p><p>Because we use a color CCD camera, the channel used to compute the gradient must be chosen carefully. Knowing that the blue channel of such a camera has a remarkably higher noise level than the other channels, we use only the sum of the red and green in order to increase the signal/noise ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Histogram based edge detection</head><p>From the gradient image several methods are used to extract edges. One of them consists of the application of a threshold function on the gradient values followed by the application of a non-maxima suppression algorithm <ref type="bibr" target="#b9">[10]</ref>. The most difficult step then remains, which is to group the resulting edges fragments together in order to obtain true vertical edges. This problem is further exacerbating when luminosity changes along the segment.</p><p>To group the resulting edge fragments together, first, we construct a histogram by adding the red-green gradient intensity of every pixel in the same column. To avoid the apparition of parasite peaks due to the noise, we apply a window filter {1,2,3,2,1} on the raw histogram. Its triangle shape permits to keep the peakiness of the spikes. One can see on the Fig. <ref type="figure" target="#fig_0">1</ref> that the mean value is actually the level of the noise and provides a bad threshold value. One can compute a more noise insensitive threshold by computing the value t = mean + (max -mean) / c, where c is chosen depending on the number of edges desired. This method is unfortunately very sensitive to occlusion and distance. Indeed, a large peak will provide a big value and the threshold will be high. In such a case, the majority of edges will not be considered.</p><p>To solve this problem we use a more statistical approach to choosing the edge threshold. The standard deviation of the values of the histogram is computed and added to the mean in order to fix the base threshold. All edges below the threshold are ignored. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color patches detection</head><p>Color patches can be used for localization as well especially in human environments where one finds often saturated colors. The combination of both edges and patches greatly increases the information for the location. A part of the information is coded in the nature of the features (edge or different colors) and another part in the sequence (order of features).</p><p>In order to get more intuitive and natural color representation, we convert RGB images extracted from the camera into the HSI color space (Hue, Saturation and Intensity). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fuzzy voting scheme</head><p>The colors in the scene are not known in advance and can cover the entire color space. In order to reduce the quantity of different color patches and memory space similar colors are grouped together considering their hue.</p><p>To limit discontinuities and instabilities for pixels near the borders of the intervals, fuzzy sets have been introduced as depicted in Fig. <ref type="figure" target="#fig_3">5</ref>. The column histogram for each base color is generated as follows. Each pixel in the image (those that remain after saturation thresholding) will add a value in one or two histograms depending on the hue. For example, a pixel with hue 0 will add 100 in the corresponding column of the red histogram. A pixel with hue 10 will add a bigger value in the red histogram than in the yellow one (see Fig. <ref type="figure" target="#fig_3">5</ref>). The same method as described for edge detection is applied but some parameters change. A different window filter, {1,2,2,2,1} is used for color histograms because we want to smooth thin peaks in this case.</p><p>The base threshold is also built by adding sigma and the mean of the histogram.</p><p>As we can see in Fig. <ref type="figure" target="#fig_4">6</ref> more patches than expected have been extracted from image in Fig. <ref type="figure" target="#fig_2">4</ref>. In order to avoid inversion between patches, which can change considerably the resulting string, a color fusion step has been introduced. Intermediate colors are used for the new patch and its horizontal coordinate is the mean of the coordinates of the parent's patches 1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GIKCAO GJBO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FINGERPRINT SEQUENCE MATCHING FOR LOCALIZATION</head><p>To introduce the problem of string matching, let us consider the example below. The first string has been extracted from the current location of the robot and the next two strings are strings from the database.  As one can see the new string does not match exactly either of the others because the robot is not exactly located on a map point and/or some change in the environment occurred. Now what sequence match scoring method should we use to determine that the match is Place1 in this case and not Place2 with high confidence?</p><p>Great many string-matching algorithms can be found in the literature. Exact string matching algorithms <ref type="bibr" target="#b7">[8]</ref> are not applicable in this case. They are designed to indicate if text occurrences are found within a text and are optimized to be very fast.</p><p>More elaborate string matching <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> algorithms allow a level of mismatch, such as k-mismatch matching algorithms, and string matching with k differences. The first allows matches where up to k characters in the pattern do not match the text, and the second requires that the pattern have an edit-distance from the text of k or less.</p><p>Another approach consists in considering strings as digital signals and computing the correlation. A measure of similarity will be in this case the height of the maximum peak of the correlation function. But this method works well only if initial strings have a similar length and fail in case of occlusion and addition. The same 1 The colors are fused if the difference between the pixel coordinates is less than 10 pixels problem appears when one computes the SSD (Sum of Square Difference) between two strings.</p><p>One of the main problems of the above methods is that they do not consider the nature of features and specific mismatches. We wish to consider the likelihood of specific types of mismatch errors. For instance confusing a red patch with a blue path is more egregious than confusing the red patch with a yellow patch. Furthermore the standard algorithms are quite sensitive to insertion and deletion errors which cause the string lengths to vary significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Minimum energy algorithm</head><p>The approach we have adopted for sequence matching is inspired by the minimum energy algorithm used in stereovision for finding pixels in two images that correspond to the same point of a scene <ref type="bibr" target="#b10">[11]</ref>. As in the minimum energy case, the problem can be seen as an optimization problem, where the goal is to find the path that spends the minimum energy to go from the beginning to the end of the first sequence considering the values of the second one. The similarity between two sequences is given by the resulting minimum energy of traversal. Value 0 is used to describe a perfect match (e.g. self-similarity).</p><p>We describe our sequence matching algorithm using an example consisting of two particular sequences: "EvHBvKvGA" (length n = 9) and "EBCAvKKv" (length m = 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization</head><p>First the initial n x m matrix must be built. The characters of the first string represent the rows and those of the second string the columns. Because the algorithm is not symmetric, the longest string will always represent the rows. To initialize this matrix only two parameters are needed. The first parameter is a number that represents the maximum mismatch value and the second is used to fix the minimum mismatch value between two different colors. In this particular example Max_init = 20 and Min_col = 5. If the corresponding features are of wholly different types (e.g. a color and an edge) then the corresponding matrix element is initialized to Max_init. If both features are vertical edges or represent exactly the same color the value 0 is used to describe a perfect match. If the comparison is between two colors, then the error is calculated according to the hue distance between the two colors, adjusted to inhabit the range from Min_col to Max_init.</p><formula xml:id="formula_0">Init E B C A v K K v E 0<label>11</label></formula><p>Although a type-mismatch can be generally assigned a score of Max_Init, any newly introduced feature type must not only include the appropriate feature detector but also a mismatch table, identifying the score for various feature value comparisons within that feature type. This is an important aspect of the present work. We have noted that differences in illumination cause color, for instance, to change one bin at times, but rarely will a color change two or more bins. Therefore, some proportionality of the scoring function based on a distance measure between colors is critical to the success of our method.   Finally the minimum value S2 is assigned to Cost(2,3) and the coordinates of the cell (1,2) are stored in Neig(2,3) = 2 (See <ref type="bibr">Fig 10)</ref>. In case of horizontal occlusion we put a negative sign for the neighbor coordinates.</p><formula xml:id="formula_1">Cost E B C A v K K v E 0</formula><formula xml:id="formula_2">Neig E B C A v K K v E - - - - - - - - v 1<label>1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The best path</head><p>The minimum value of the last line of the Cost matrix. This value corresponds inversely to the similarity between the two input sequences. In this particular example the score that results is 381. In order to normalize the result this value is then divided by the worst value that can be obtained with two strings of similar length (in this case, result of the match between a string composed of m edges and one with n colors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IMPLEMENTATION</head><p>The camera used to acquire the images is an inexpensive CCD color camera with a 640 x 480 resolution <ref type="foot" target="#foot_1">3</ref> . The interface to the computer is via the USB.</p><p>Image manipulation is performed with a Microsoft Visual C++ 6.0 application running under Windows'98. The camera is fixed via a 110-CM mast to a Nomad Scout mobile robot research platform<ref type="foot" target="#foot_2">4</ref> . To build the panoramic view of the scene the differential-drive Scout is rotated about its center while a series of 12 images are grabbed from the CCD camera every 30 degrees. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building the panoramic image</head><p>Various methods exist to align corresponding pixels in two adjacent pictures. One method consists of computing the SSD between adjacent images and the best alignment is given by the minimum of the function <ref type="bibr" target="#b13">[14]</ref>.</p><p>This method produces panoramas that are of high quality for human consumption; however, such exact alignment is unnecessary for our purposes of color patch and edge extraction. Instead, we simply attach images end to end, taking into account the resulting "seam" by suppressing detection of edges at these seams. To avoid the additional computational burden of unwarping images, only the central 70% percent of the images is used during construction of the image.</p><p>The point of view of the panoramic is very important and the height of the camera must be chosen carefully. If the camera is too low every item of furniture such as chairs and tables can occupy the view in front of the robot. Since these low objects are apt to move, the resulting image will be highly dynamic. In our implementation we have placed the camera at almost the same height as the eyes of an human so that large-scale features of interest (e.g. door posts, windows, corners) are easily visible while low-level clutter is avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In order to test the system, two maps corresponding to two different environments have been constructed (See Fig. <ref type="figure" target="#fig_12">12</ref>). The left map, called White Hall, corresponds to the entrance hall on the first floor of the Smith Hall building. The map on the right, called Ground Floor, covers a path that ends in the conference room on the ground floor in the same building. For the White Hall 15 locations evenly spaced by 90 cm have been chosen arbitrarily in the map in order to represent the map points. 21 map points have been stored with the same method for the Ground Floor. We use crosses to represent those points in the next figures.  Fig. <ref type="figure" target="#fig_0">13</ref> shows panoramas and strings associated to map points Pc and Pw3 (See Fig. <ref type="figure" target="#fig_2">14</ref>). It is interesting to note that same objects in the scene generate same string fragments even if locations are quite far one from each other. For example, the same sequence "LvBE" has been extracted for the trashcan (blue) and the door (red, green) for both panoramas.</p><p>We intend to test global localization by choosing random positions around the map points and compare the corresponding strings with all the stored map points. For the White Hall 18 locations have been chosen to test the system: they are called test points and are represented by circles in the maps. For the Ground Floor 22 points have been tested.</p><p>In order to determine a percentage of good results the two following criteria have been chosen. For the White Hall 17 test points have been classified as topologically correct that represents 94% of good results. In another hand 82% of locations have been classified as geometrically correct (14 points). 20 test points are topologically correct for the Ground Floor (91%) and 14 have been classified geometrically correct (70%). In order to get more significant statistics the two experiment sets have been fused. The new database consists as 40 test points and 36 map points. 90% of the test points have been classified as topologically corrects and 75% as geometrically corrects. The test points, which were wrong for first test sets, remain wrong when databases are fused. Unfortunately, the fusion of the two sets has generated a new false point (Pw3~). These results make us think about some considerations. The wrong test points are mainly due to two different effects.</p><p>First, major occlusions and/or additions can occur in the string. These defaults are generally due to a pathological combination of dynamic changes in the environment e.g. illumination change, reflections, new objects or persons in the scene. Second, some locations are locally so unique that they are different of all test points even if they are geographically close. Those pathological cases happen mostly in close areas and for points close to object that can hide a big portion of the environment. Indeed, the displacement/(changes in the string) ratio can be very small in these cases. This explains the relative bad results for the second criteria compared to the first.</p><p>This problem makes us think about the necessity to choose carefully the map points. The natural rule is to put more map points when objects are close and fewer points are necessary for open areas. This can be done automatically while the robot is exploring the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>The structure of circular chains and the string matching algorithm allows us to insert other kinds of features. Using different features extracted from several kinds of sensors provides several advantages. One can improve the edge detection by fusing information from the camera and a laser range finder for instance. Or, infrared images and laser range finder can be used in dark scenes. Furthermore probabilities related to features can be easily introduced in the string matching algorithm.</p><p>For the moment the largest computational burden is construction of the panoramic image. Optical solutions can alleviate this problem, and so one should consider using a panoramic vision system, such as an Omnicam, to capture a panorama instantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Filtered gradient histogram ( See Fig 3. ) (mean and threshold)</figDesc><graphic coords="2,76.49,281.05,205.92,55.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Histogram after group and filter algorithms</figDesc><graphic coords="2,72.77,546.01,205.92,56.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Original image Because color information is weak for low level of saturation, only high-saturated pixels are considered for the extraction of patches.</figDesc><graphic coords="2,338.33,211.82,196.20,54.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fuzzy voting scheme</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: String before and after color fusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Strings example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Init matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Cost matrix (3D)</figDesc><graphic coords="4,66.05,351.13,113.76,79.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>7 Figure 10 :</head><label>710</label><figDesc>Figure 10: Cost matrix (3D) and Neig matrix Only two parameters are needed to compute the Cost matrix: the slope penalty (Slope_pen = 10) and the occlusion penalty (Occ_pen = 24). The first line of the Cost matrix is just a copy of the first line of the Init matrix. Let us consider the cell Cost(2,3) 2 to explain the approach adopted to initialize the other elements. • Cost(1,1): The slope between cell(1,1) and cell(2,3) is computed by subtracting the respective column indexes and the following sum is evaluated. S 1 = Cost(1,1) + Init(2,3) + slope(2) * Slope_pen= 40 • Cost(1,2): In this case the slope between cell(1,2) and cell(2,3) is optimal. Indeed, if the two strings were identical the best path will be the diagonal of the matrix and the result of the match must be 0. That means that no penalty is added. S 2 = Cost(1,2) + Init(2,3) = 31</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: System</figDesc><graphic coords="4,492.05,397.33,58.80,140.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>1m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Maps of White Hall and Ground Floor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Figure 13: Panoramas and string examples</figDesc><graphic coords="5,58.49,666.73,464.88,56.88" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Cost(i,j)  is the value at the ith line and jth column of the Cost matrix. Same for Init(i,j) and Neig(i,j)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Logitech QuickCam Pro. Look at www.logitech.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>More information available at www.robots.com/nscout.htm</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>Thanks to Iwan Ulrich who provided much help during the project and Jianbo Shi for his panoramic algorithm and good advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cooperative Robot Localization with Vision-based Mapping</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Goal Directed Reactive Robot Navigation with Relocation Using Laser and Vision</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Asensio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Montano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="2905" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding Landmarks for Mobile Robot Navigation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<biblScope unit="page" from="958" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision Based Navigation System for Autonomous Mobile Robot with Global Matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fukada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic Mapping of Dynamic Office Environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Willeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Nourbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems, Autonomous Robots Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1999</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Appearance-Based Place Recognition for Topological Localization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Nourbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and autonomous systems</title>
		<imprint>
			<date type="published" when="1999-12">December 1999</date>
			<pubPlace>Pittsburgh</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University (CMU)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Faster Approximate String Matching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<imprint>
			<pubPlace>Santiago</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Chile</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast Pattern Matching in Strings</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="323" to="350" />
			<date type="published" when="1977-06">June 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithms for finding patterns in strings</title>
		<author>
			<persName><forename type="first">Alfred</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Theoretical Computer Science, chapter 5</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Van Leeuwen</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier Science Publishers B. V</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="254" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Vision Feedback for Mobile Robots</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Tomatis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-02">February 1998</date>
		</imprint>
		<respStmt>
			<orgName>Institute of Robotics (IfR), Swiss Federal Institute of Technology (ETH) Zurich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma Thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stereo by Intra-and Inter-Scanline Search Using Dynamic Programming</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1985-03">March 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Qualitative representation of scenes along route</title>
		<author>
			<persName><forename type="first">Li-S</forename><surname>Tsuji-S</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-07">July 1999</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="685" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous Mobile Robot Localization: Vision vs. Laser</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="2917" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Good Features to Track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MINERVA: a second-generation museum tour-guide robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hahnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Cat. No.99CH36288C</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An Affective Mobile Educator with a Full-time Job</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bobenage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="124" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using the CONDENSATION algorithm for robust, vision-based mobile robot localization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DERVISH: an officenavigating robot</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="60" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>sum</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Active navigation vision based on eigenspace analysis</title>
		<author>
			<persName><forename type="first">Maeda-S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kuno-Y</forename><surname>Shirai-Y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS &apos;97 Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1018" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
