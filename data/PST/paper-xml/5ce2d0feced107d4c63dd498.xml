<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DECOUPLED WEIGHT DECAY REGULARIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
							<email>ilya.loshchilov@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DECOUPLED WEIGHT DECAY REGULARIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>L 2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is not the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L 2 regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW Now, let's turn to adaptive gradient algorithms like the popular optimizer Adam Kingma &amp; Ba (2014), which scale gradients by their historic magnitudes. Intuitively, when Adam is run on a loss function f plus L 2 regularization, weights that tend to have large gradients in f do not get regularized as much as they would with decoupled weight decay, since the gradient of the regularizer gets scaled along with the gradient of f . This leads to an inequivalence of L 2 and decoupled weight decay regularization for adaptive gradient algorithms:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Adaptive gradient methods, such as AdaGrad <ref type="bibr" target="#b4">(Duchi et al., 2011)</ref>, <ref type="bibr">RMSProp (Tieleman &amp; Hinton, 2012)</ref>, Adam <ref type="bibr" target="#b9">(Kingma &amp; Ba, 2014)</ref> and most recently AMSGrad <ref type="bibr" target="#b15">(Reddi et al., 2018)</ref> have become a default method of choice for training feed-forward and recurrent neural networks <ref type="bibr" target="#b21">(Xu et al., 2015;</ref><ref type="bibr" target="#b13">Radford et al., 2015)</ref>. Nevertheless, state-of-the-art results for popular image classification datasets, such as CIFAR-10 and CIFAR-100 <ref type="bibr">Krizhevsky (2009)</ref>, are still obtained by applying SGD with momentum <ref type="bibr" target="#b5">(Gastaldi, 2017;</ref><ref type="bibr" target="#b2">Cubuk et al., 2018)</ref>. Furthermore, <ref type="bibr" target="#b20">Wilson et al. (2017)</ref> suggested that adaptive gradient methods do not generalize as well as SGD with momentum when tested on a diverse set of deep learning tasks, such as image classification, character-level language modeling and constituency parsing. Different hypotheses about the origins of this worse generalization have been investigated, such as the presence of sharp local minima <ref type="bibr" target="#b8">(Keskar et al., 2016;</ref><ref type="bibr" target="#b3">Dinh et al., 2017)</ref> and inherent problems of adaptive gradient methods <ref type="bibr" target="#b20">(Wilson et al., 2017)</ref>. In this paper, we investigate whether it is better to use L 2 regularization or weight decay regularization to train deep neural networks with SGD and Adam. We show that a major factor of the poor generalization of the most popular adaptive gradient method, Adam, is due to the fact that L 2 regularization is not nearly as effective for it as for SGD. Specifically, our analysis of Adam leads to the following observations:</p><p>L 2 regularization and weight decay are not identical. Contrary to a belief which seems popular among some practitioners, the two techniques are not equivalent. For SGD, they can be made equivalent by a reparameterization of the weight decay factor based on the learning rate; this is not the case for Adam. In particular, when combined with adaptive gradients, L 2 regularization leads to weights with large parameter and/or gradient amplitudes being regularized less than they would be when using weight decay.</p><p>L 2 regularization is not effective in Adam. One possible explanation why Adam and other adaptive gradient methods might be outperformed by SGD with momentum is that common deep learning libraries only implement L 2 regularization, not the original weight decay. Therefore, on tasks/datasets where the use of L 2 regularization is beneficial for SGD (e.g., on many popular image classification datasets), Adam leads to worse results than SGD with momentum (for which L 2 regularization behaves as expected). Weight decay is equally effective in both SGD and Adam. For SGD, it is equivalent to L 2 regularization, while for Adam it is not. Optimal weight decay depends on the total number of batch passes/weight updates. Our empirical analysis of SGD and Adam suggests that the larger the runtime/number of batch passes to be performed, the smaller the optimal weight decay. This effect tends to be neglected because hyperparameters are often tuned for a fixed number of training epochs. As a result, the values of the weight decay found to perform best for short runs do not generalize to much longer runs.</p><p>The main contribution of this paper is to improve regularization in Adam by decoupling the weight decay from the gradient-based update. In a comprehensive analysis, we show that Adam generalizes substantially better with decoupled weight decay than with L 2 regularization, achieving 15% relative improvement in test error (see Figures <ref type="figure" target="#fig_2">2 and 3</ref>); this holds true for various image recognition datasets (CIFAR-10 and ImageNet32x32), training budgets (ranging from 100 to 1800 epochs), and learning rate schedules (fixed, drop-step, and cosine annealing; see Figure <ref type="figure" target="#fig_0">1</ref>). We demonstrate that our decoupled weight decay renders the optimal settings of the learning rate and the weight decay factor much more independent, thereby easing hyperparameter optimization (see Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>The main motivation of this paper is to improve Adam to make it competitive w.r.t. SGD with momentum even for those problems where it did not use to be competitive. We hope that as a result, practitioners do not need to switch between Adam and SGD anymore, which in turn should reduce the common issue of selecting dataset/task-specific training algorithms and their hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DECOUPLING THE WEIGHT DECAY FROM THE GRADIENT-BASED UPDATE</head><p>In the weight decay described by <ref type="bibr" target="#b6">Hanson &amp; Pratt (1988)</ref>, the weights θ decay exponentially as</p><formula xml:id="formula_0">θ t+1 = (1 − λ)θ t − α∇f t (θ t ),<label>(1)</label></formula><p>where λ defines the rate of the weight decay per step and ∇f t (θ t ) is the t-th batch gradient to be multiplied by a learning rate α. For standard SGD, it is equivalent to standard L 2 regularization: Proposition 1 (Weight decay = L 2 reg for standard SGD). Standard SGD with base learning rate α executes the same steps on batch loss functions f t (θ) with weight decay λ (defined in Equation <ref type="formula" target="#formula_0">1</ref>) as it executes without weight decay on</p><formula xml:id="formula_1">f reg t (θ) = f t (θ) + λ 2 θ 2 2 , with λ = λ α .</formula><p>The proofs of this well-known fact, as well as our other propositions, are given in the Appendix A.</p><p>Due to this equivalence, L 2 regularization is very frequently referred to as weight decay, including in popular deep learning libraries. However, as we will demonstrate later in this section, this equivalence does not hold for adaptive gradient methods. One fact that is often overlooked already for the simple case of SGD is that in order for the equivalence to hold, the L 2 regularizer λ has to be set to λ α , i.e., if there is an overall best weight decay value λ, the best value of λ is tightly coupled with the learning rate α. In order to decouple the effects of these two hyperparameters, we advocate to decouple the weight decay step as proposed by <ref type="bibr" target="#b6">Hanson &amp; Pratt (1988)</ref> (Equation <ref type="formula" target="#formula_0">1</ref>).</p><p>Looking first at the case of SGD, we propose to decay the weights simultaneously with the update of θ t based on gradient information in Line 9 of Algorithm 1. This yields our proposed variant of SGD with momentum using decoupled weight decay (SGDW). This simple modification explicitly decouples λ and α (although some problem-dependent implicit coupling may of course remain as for any two hyperparameters). In order to account for a possible scheduling of both α and λ, we introduce a scaling factor η t delivered by a user-defined procedure SetScheduleM ultiplier(t). 1: given α = 0.001, β1 = 0.9, β2 = 0.999, = 10 −8 , λ ∈ IR 2: initialize time step t ← 0, parameter vector θt=0 ∈ IR n , first moment vector mt=0 ← 0, second moment vector vt=0 ← 0, schedule multiplier ηt=0 ∈ IR 3: repeat 4:</p><p>t ← t + 1 5:</p><p>∇ft(θt−1) ← SelectBatch(θt−1) select batch and return the corresponding gradient 6: </p><formula xml:id="formula_2">g t ← ∇ft(θt−1) +λθt−1 7: mt ← β1mt−1 + (1 − β1)</formula><formula xml:id="formula_3">(θ) = f t (θ)+ λ 2 θ 2 2 without weight decay is equivalent to running O on f t (θ) with decay λ ∈ R + .</formula><p>We decouple weight decay and loss-based gradient updates in Adam as shown in line 12 of Algorithm 2; this gives rise to our variant of Adam with decoupled weight decay (AdamW).</p><p>Having shown that L 2 regularization and weight decay regularization differ for adaptive gradient algorithms raises the question of how they differ and how to interpret their effects. Their equivalence for standard SGD remains very helpful for intuition: both mechanisms push weights closer to zero, at the same rate. However, for adaptive gradient algorithms they differ: with L 2 regularization, the sums of the gradient of the loss function and the gradient of the regularizer (i.e., the L 2 norm of the weights) are adapted, whereas with weight decay, only the gradients of the loss function are adapted (with the weight decay step separated from the adaptive gradient mechanism). With L 2 regularization both types of gradients are normalized by their typical (summed) magnitudes, and therefore weights x with large typical gradient magnitude s are regularized by a smaller relative amount than other weights. In contrast, weight decay regularizes all weights with the same rate λ, effectively regularizing weights x with large s more than standard L 2 regularization does. We demonstrate this formally for a simple special case of adaptive gradient algorithm with a fixed preconditioner: Proposition 3 (Weight decay = scale-adjusted L 2 reg for adaptive gradient algorithm with fixed preconditioner). Let O denote an algorithm with the same characteristics as in Proposition 2, and using a fixed preconditioner matrix M t = diag(s) −1 (with s i &gt; 0 for all i). Then, O with base learning rate α executes the same steps on batch loss functions f t (θ) with weight decay λ as it executes without weight decay on the scale-adjusted regularized batch loss</p><formula xml:id="formula_4">f sreg t (θ) = f t (θ) + λ 2α θ √ s 2 2 ,<label>(2)</label></formula><p>where and √ • denote element-wise multiplication and square root, respectively, and λ = λ α .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JUSTIFICATION OF DECOUPLED WEIGHT DECAY VIA A VIEW OF ADAPTIVE GRADIENT METHODS AS BAYESIAN FILTERING</head><p>We now discuss a justification of decoupled weight decay in the framework of Bayesian filtering for a unified theory of adaptive gradient algorithms due to <ref type="bibr" target="#b0">Aitchison (2018)</ref>. After we posted a preliminary version of our current paper on arXiv, Aitchison noted that his theory "gives us a theoretical framework in which we can understand the superiority of this weight decay over L 2 regularization, because it is weight decay, rather than L 2 regularization that emerges through the straightforward application of Bayesian filtering." <ref type="bibr" target="#b0">(Aitchison, 2018)</ref>. While full credit for this theory goes to Aitchison, we summarize it here to shed some light on why weight decay may be favored over L 2 regularization.</p><p>Aitchison (2018) views stochastic optimization of n parameters x 1 , . . . , x n as a Bayesian filtering problem with the goal of inferring a distribution over the optimal values of each of the parameters x i given the current values of the other parameters θ −i (t) at time step t. When the other parameters do not change this is an optimization problem, but when they do change it becomes one of "tracking" the optimizer using Bayesian filtering as follows. One is given a probability distribution P (θ t | y 1:t ) of the optimizer at time step t that takes into account the data y 1:t from the first t mini batches, a state transition prior P (θ t+1 | θ t ) reflecting a (small) data-independent change in this distribution from one step to the next, and a likelihood P (y t+1 | θ t+1 ) derived from the mini batch at step t + 1. The posterior distribution P (θ t+1 | y 1:t+1 ) of the optimizer at time step t + 1 can then be computed (as usual in Bayesian filtering) by marginalizing over θ t to obtain the onestep ahead predictions P (θ t+1 | y 1:t ) and then applying Bayes' rule to incorporate the likelihood P (y t+1 | θ t+1 ). Aitchison (2018) assumes a Gaussian state transition distribution P (θ t+1 | θ t ) and an approximate conjugate likelihood P (y t+1 | θ t+1 ), leading to the following closed-form update of the filtering distribution's mean:</p><formula xml:id="formula_5">µ post = µ prior + Σ post × g,<label>(3)</label></formula><p>where g is the gradient of the log likelihood of the mini batch at time t. This result implies a preconditioner of the gradients that is given by the posterior uncertainty Σ post of the filtering distribution: updates are larger for parameters we are more uncertain about and smaller for parameters we are more certain about. <ref type="bibr" target="#b0">Aitchison (2018)</ref> goes on to show that popular adaptive gradient methods, such as Adam and RMSprop, as well as Kronecker-factorized methods are special cases of this framework.</p><p>Decoupled weight decay very naturally fits into this unified framework can express weight decay as part of the state-transition distribution: Aitchison (2018) assumes a slow change of the optimizer according to the following Gaussian:</p><formula xml:id="formula_6">P (θ t+1 | θ t ) = N ((I − A)θ t , Q), (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where Q is the covariance of Gaussian perturbations of the weights, and A is a regularizer to avoid values growing unboundedly over time. When instantiated as A = λ × I, this regularizer A plays exactly the role of decoupled weight decay as described in Equation <ref type="formula" target="#formula_0">1</ref>, since this leads to multiplying the current mean estimate θ t by (1 − λ) at each step. Notably, this regularization is also directly applied to the prior and does not depend on the uncertainty in each of the parameters (which would be required for L 2 regularization). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL VALIDATION</head><p>We now evaluate the performance of decoupled weight decay under various training budgets and learning rate schedules. Our experimental setup follows that of <ref type="bibr" target="#b5">Gastaldi (2017)</ref>, who proposed, in addition to L 2 regularization, to apply the new Shake-Shake regularization to a 3-branch residual DNN that allowed to achieve new state-of-the-art results of 2.86% on the CIFAR-10 dataset <ref type="bibr">(Krizhevsky, 2009)</ref>. We always used a batch size of 128. The regular data augmentation procedure used for the CIFAR datasets was applied. We used the same model/source code based on fb.resnet.torch<ref type="foot" target="#foot_0">1</ref> . The base networks are a 26 2x64d ResNet (i.e. the network has a depth of 26, 2 residual branches and the first residual block has a width of 64) and a 26 2x96d ResNet with 11.6M and 25.6M parameters, respectively. For a detailed description of the network and the Shake-Shake method, we refer the interested reader to <ref type="bibr" target="#b5">Gastaldi (2017)</ref>. We also perform experiments on the Im-ageNet32x32 dataset <ref type="bibr" target="#b1">(Chrabaszcz et al., 2017)</ref>, a downsampled version of the original ImageNet dataset with 1.2 million 32×32 pixels images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EVALUATING DECOUPLED WEIGHT DECAY WITH DIFFERENT LEARNING RATE SCHEDULES</head><p>In our first experiment, we compare Adam with L 2 regularization to Adam with decoupled weight decay (AdamW), using three different learning rate schedules: a fixed learning rate, a drop-step schedule, and a cosine annealing schedule <ref type="bibr" target="#b11">(Loshchilov &amp; Hutter, 2016)</ref>. For each learning rate schedule and weight decay variant, we trained a 2x64d ResNet for 100 epochs, using different settings of the initial learning rate α and the weight decay factor λ. Figure <ref type="figure" target="#fig_0">1</ref> shows that decoupled weight decay outperforms L 2 regularization for all learning rate schedules, with larger differences for better learning rate schedules. We also note that decoupled weight decay leads to a more separable hyperparameter search space, especially when a learning rate schedule, such as step-drop and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DECOUPLING THE WEIGHT DECAY AND INITIAL LEARNING RATE PARAMETERS</head><p>In order to verify our hypothesis about the coupling of α and λ, in Figure <ref type="figure" target="#fig_1">2</ref> we compare the performance of L 2 regularization vs. decoupled weight decay in SGD (SGD vs. SGDW, top row) and in Adam (Adam vs. AdamW, bottom row). In SGD (Figure <ref type="figure" target="#fig_1">2</ref>, top left), L 2 regularization is not decoupled from the learning rate (the common way as described in Algorithm 1), and the figure clearly shows that the basin of best hyperparameter settings (depicted by color and top-10 hyperparameter settings by black circles) is not aligned with the x-axis or y-axis but lies on the diagonal. This suggests that the two hyperparameters are interdependent and need to be changed simultaneously, while only changing one of them might substantially worsen results. Consider, e.g., the setting at the top left black circle (α = 1/2, λ = 1/8 * 0.001); only changing either α or λ by itself would worsen results, while changing both of them could still yield clear improvements. We note that this coupling of initial learning rate and L 2 regularization factor might have contributed to SGD's reputation of being very sensitive to its hyperparameter settings.</p><p>In contrast, the results for SGD with decoupled weight decay (SGDW) in Figure <ref type="figure" target="#fig_1">2</ref> (top right) show that weight decay and initial learning rate are decoupled. The proposed approach renders the two hyperparameters more separable: even if the learning rate is not well tuned yet (e.g., consider the value of 1/1024 in Figure <ref type="figure" target="#fig_1">2</ref>, top right), leaving it fixed and only optimizing the weight decay factor would yield a good value (of 1/4*0.001). This is not the case for SGD with L 2 regularization (see Figure <ref type="figure" target="#fig_1">2</ref>, top left).</p><p>The results for Adam with L 2 regularization are given in Figure <ref type="figure" target="#fig_1">2</ref> (bottom left). Adam's best hyperparameter settings performed clearly worse than SGD's best ones (compare Figure <ref type="figure" target="#fig_1">2</ref>, top left). While both methods used L 2 regularization, Adam did not benefit from it at all: its best results obtained for non-zero L 2 regularization factors were comparable to the best ones obtained without the L 2 regularization, i.e., when λ = 0. Similarly to the original SGD, the shape of the hyperparameter landscape suggests that the two hyperparameters are coupled.</p><p>In contrast, the results for our new variant of Adam with decoupled weight decay (AdamW) in Figure <ref type="figure" target="#fig_1">2</ref> (bottom right) show that AdamW largely decouples weight decay and learning rate. The results for the best hyperparameter settings were substantially better than the best ones of Adam with L 2 regularization and rivaled those of SGD and SGDW.</p><p>In summary, the results in Figure <ref type="figure" target="#fig_1">2</ref> support our hypothesis that the weight decay and learning rate hyperparameters can be decoupled, and that this in turn simplifies the problem of hyperparameter tuning in SGD and improves Adam's performance to be competitive w.r.t. SGD with momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BETTER GENERALIZATION OF ADAMW</head><p>While the previous experiment suggested that the basin of optimal hyperparameters of AdamW is broader and deeper than the one of Adam, we next investigated the results for much longer runs of 1800 epochs to compare the generalization capabilities of AdamW and Adam.</p><p>We fixed the initial learning rate to 0.001 which represents both the default learning rate for Adam and the one which showed reasonably good results in our experiments. Figure <ref type="figure" target="#fig_2">3</ref> shows the results for 12 settings of the L 2 regularization of Adam and 7 settings of the normalized weight decay of AdamW (the normalized weight decay represents a rescaling formally defined in the Appendix B.1, it amounts to a multiplicative factor which depends on the number of bath passes). Interestingly, while the dynamics of the learning curves of Adam and AdamW often coincided for the first half of the training run, AdamW often led to lower training loss and test errors (see Figure <ref type="figure" target="#fig_2">3</ref> top left and top right, respectively). Importantly, the use of weight decay in Adam did not yield as good results as in AdamW (see also Figure <ref type="figure" target="#fig_2">3</ref>, bottom left). Next, we investigated whether AdamW's better results were only due to better convergence or due to better generalization. The results in Figure <ref type="figure" target="#fig_2">3</ref> (bottom right) for the best settings of Adam and AdamW suggest that AdamW did not only yield better training loss but also yielded better generalization performance for similar training loss values. The results on ImageNet32x32 (see SuppFigure 4 in the Appendix) lead to the same conclusion of substantially improved generalization performance. For a better resolution and with training loss curves, see SuppFigure 5 and SuppFigure 6 in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ADAMWR WITH WARM RESTARTS FOR BETTER ANYTIME PERFORMANCE</head><p>In order to improve anytime performance of SGDW and AdamW we extended them with warm restarts of <ref type="bibr" target="#b11">(Loshchilov &amp; Hutter, 2016)</ref> to obtain SGDWR and AdamWR, respectively (see section B.2 in the Appendix). As Figure <ref type="figure" target="#fig_3">4</ref> shows, AdamWR greatly sped up AdamW on CIFAR-10 and ImageNet32x32, up to a factor of 10 (see the results at the first restart). For the default learning rate of 0.001, AdamW achieved 15% relative improvement in test errors compared to Adam both on CIFAR-10 (also see Figure <ref type="figure" target="#fig_2">3</ref>) and ImageNet32x32 (also see SuppFigure 5). AdamWR achieved the same improved results but with a much better anytime performance. These improvements closed most of the gap between Adam and SGDWR on CIFAR-10 and yielded comparable performance on ImageNet32x32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">USE OF ADAMW ON OTHER DATASETS AND ARCHITECTURES</head><p>Several other research groups have already successfully applied AdamW in citable works. For example, <ref type="bibr" target="#b19">Wang et al. (2018)</ref> used AdamW to train a novel architecture for face detection on the standard WIDER FACE dataset <ref type="bibr" target="#b22">(Yang et al., 2016)</ref>, obtaining almost 10x faster predictions than the previous state of the art algorithms while achieving comparable performance. <ref type="bibr" target="#b18">Völker et al. (2018)</ref> employed AdamW with cosine annealing to train convolutional neural networks to classify and characterize error-related brain signals measured from intracranial electroencephalography (EEG) recordings. While their paper does not provide a comparison to Adam, they kindly provided us with a direct comparison of the two on their best-performing problem-specific network architecture Deep4Net and a variant of ResNet. AdamW with the same hyperparameter setting as Adam yielded higher test set accuracy on Deep4Net (73.68% versus 71.37%) and statistically significantly higher test set accuracy on ResNet (72.04% versus 61.34%). <ref type="bibr" target="#b14">Radford et al. (2018)</ref> employed AdamW to train Transformer <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref> architectures to obtain new state-of-the-art results on a wide range of benchmarks for natural language understanding. <ref type="bibr" target="#b23">Zhang et al. (2018)</ref> compared L 2 regularization vs. weight decay for SGD, Adam and the Kronecker-Factored Approximate Curvature (K-FAC) optimizer <ref type="bibr" target="#b12">(Martens &amp; Grosse, 2015)</ref> on the CIFAR datasets with ResNet and VGG architectures, reporting that decoupled weight decay consistently outperformed L 2 regularization in cases where they differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>Following suggestions that adaptive gradient methods such as Adam might lead to worse generalization than SGD with momentum <ref type="bibr" target="#b20">(Wilson et al., 2017)</ref>, we identified and exposed the inequivalence of L 2 regularization and weight decay for Adam. We empirically showed that our version of Adam with decoupled weight decay yields substantially better generalization performance than the common implementation of Adam with L 2 regularization. We also proposed to use warm restarts for Adam to improve its anytime performance.</p><p>Our results obtained on image classification datasets must be verified on a wider range of tasks, especially ones where the use of regularization is expected to be important. It would be interesting to integrate our findings on weight decay into other methods which attempt to improve Adam, e.g, normalized direction-preserving Adam <ref type="bibr" target="#b24">(Zhang et al., 2017)</ref>. While we focused our experimental analysis on Adam, we believe that similar results also hold for other adaptive gradient methods, such as AdaGrad <ref type="bibr" target="#b4">(Duchi et al., 2011) and</ref><ref type="bibr">AMSGrad (Reddi et al., 2018)</ref>.  <ref type="bibr" target="#b11">&amp; Hutter (2016)</ref>. There, the authors proposed Stochastic Gradient Descent with Warm Restarts (SGDR) to improve anytime performance of SGD by quickly cooling down the learning rate according to a cosine schedule and periodically increasing it. SGDR has been successfully adopted to lead to new state-of-the-art results for popular image classification benchmarks <ref type="bibr" target="#b7">(Huang et al., 2017;</ref><ref type="bibr" target="#b5">Gastaldi, 2017;</ref><ref type="bibr" target="#b25">Zoph et al., 2017)</ref>, and we therefore tried extending it to Adam. However, while our initial version of Adam with warm restarts had better anytime performance than Adam, it was not competitive with SGD with warm restarts, precisely because L 2 regularization was not working as well as in SGD. Now, having fixed this issue by means of the original weight decay regularization (Section 2) and also having introduced normalized weight decay (Section B.1), the original work on cosine annealing and warm restarts by <ref type="bibr" target="#b11">Loshchilov &amp; Hutter (2016)</ref> directly carries over to Adam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>In the interest of keeping the presentation self-contained, we briefly describe how SGDR schedules the change of the effective learning rate in order to accelerate the training of DNNs. Here, we decouple the initial learning rate α and its multiplier η t used to obtain the actual learning rate at iteration t (see, e.g., line 8 in Algorithm 1). In SGDR, we simulate a new warm-started run/restart of SGD once T i epochs are performed, where i is the index of the run. Importantly, the restarts are not performed from scratch but emulated by increasing η t while the old value of θ t is used as an initial solution. The amount by which η t is increased controls to which extent the previously acquired information (e.g., momentum) is used. Within the i-th run, the value of η t decays according to a cosine annealing <ref type="bibr" target="#b11">(Loshchilov &amp; Hutter, 2016)</ref> learning rate for each batch as follows:</p><formula xml:id="formula_8">η t = η (i) min + 0.5(η (i) max − η (i) min )(1 + cos(πT cur /T i )),<label>(14) where η (i)</label></formula><p>min and η (i) max are ranges for the multiplier and T cur accounts for how many epochs have been performed since the last restart. T cur is updated at each batch iteration t and is thus not constrained to integer values. Adjusting (e.g., decreasing) η (i) min and η (i) max at every i-th restart (see also <ref type="bibr" target="#b16">Smith (2016)</ref>) could potentially improve performance, but we do not consider that option here because it would involve additional hyperparameters. For η (i) max = 1 and η (i) min = 0, one can simplify Eq. ( <ref type="formula" target="#formula_8">14</ref>) to η t = 0.5 + 0.5 cos(πT cur /T i ).</p><p>(15)</p><p>In order to achieve good anytime performance, one can start with an initially small T i (e.g., from 1% to 10% of the expected total budget) and multiply it by a factor of T mult (e.g., T mult = 2) at every restart. The (i + 1)-th restart is triggered when T cur = T i by setting T cur to 0. An example setting of the schedule multiplier is given in C.</p><p>Our proposed AdamWR algorithm represents AdamW (see Algorithm 2) with η t following Eq. ( <ref type="formula">15</ref>) and λ computed at each iteration using normalized weight decay described in the previous section. We note that normalized weight decay allowed us to use a constant parameter setting across short and long runs performed within AdamWR and SGDWR (SGDW with warm restarts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C AN EXAMPLE SETTING OF THE SCHEDULE MULTIPLIER</head><p>An example schedule of the schedule multiplier η t is given in SuppFigure 1 for T i=0 = 100 and T mult = 2. After the initial 100 epochs the learning rate will reach 0 because η t=100 = 0. Then, since T cur = T i=0 , we restart by resetting T cur = 0, causing the multiplier η t to be reset to 1 due to Eq. ( <ref type="formula">15</ref>). This multiplier will then decrease again from 1 to 0, but now over the course of 200 epochs because T i=1 = T i=0 T mult = 200. Solutions obtained right before the restarts, when η t = 0 (e.g., at epoch indexes 100, 300, 700 and 1500 as shown in SuppFigure 1) are recommended by the optimizer as the solutions, with more recent solutions prioritized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL RESULTS</head><p>We investigated whether the use of much longer runs (1800 epochs) of "standard Adam" (Adam with L 2 regularization and a fixed learning rate) makes the use of cosine annealing unnecessary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Adam performs better with decoupled weight decay (bottom row, AdamW) than with L 2 regularization (top row, Adam). We show the final test error of a 26 2x64d ResNet on CIFAR-10 after 100 epochs of training with fixed learning rate (left column), step-drop learning rate (with drops at epoch indexes 30, 60 and 80, middle column) and cosine annealing (right column). AdamW leads to a more separable hyperparameter search space, especially when a learning rate schedule, such as step-drop and cosine annealing is applied. Cosine annealing yields clearly superior results.</figDesc><graphic url="image-4.png" coords="5,140.64,192.69,106.92,109.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Top-1 test error of a 26 2x64d ResNet on CIFAR-10 measured after 100 epochs. The proposed SGDW and AdamW (right column) have a more separable hyperparameter space.</figDesc><graphic url="image-9.png" coords="6,153.42,208.98,138.60,115.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves (top row) and generalization results (bottom row) obtained by a 26 2x96d ResNet trained with Adam and AdamW on CIFAR-10. See text for details. SuppFigure 4 in the Appendix shows the same qualitative results for ImageNet32x32.</figDesc><graphic url="image-14.png" coords="6,323.71,515.82,138.59,113.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top-1 test error on CIFAR-10 (left) and Top-5 test error on ImageNet32x32 (right). For a better resolution and with training loss curves, see SuppFigure 5 and SuppFigure 6 in the supplementary material.</figDesc><graphic url="image-16.png" coords="8,323.71,83.12,158.40,130.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-34.png" coords="17,108.00,101.61,356.42,295.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-35.png" coords="17,108.00,398.26,356.39,288.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-36.png" coords="18,108.00,96.37,356.39,292.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-37.png" coords="18,108.00,390.17,356.40,291.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Adam with L 2 regularization and Adam with decoupled weight decay (AdamW)</figDesc><table><row><cell cols="3">Algorithm 1 SGD with L 2 regularization and SGD with decoupled weight decay (SGDW) , both</cell></row><row><cell cols="2">with momentum</cell><cell></cell></row><row><cell cols="3">1: given initial learning rate α ∈ IR, momentum factor β1 ∈ IR, weight decay/L2 regularization factor λ ∈ IR</cell></row><row><cell cols="3">2: initialize time step t ← 0, parameter vector θt=0 ∈ IR n , first moment vector mt=0 ← 0, schedule</cell></row><row><cell></cell><cell>multiplier ηt=0 ∈ IR</cell><cell></cell></row><row><cell cols="2">3: repeat</cell><cell></cell></row><row><cell>4:</cell><cell>t ← t + 1</cell><cell></cell></row><row><cell>5:</cell><cell>∇ft(θt−1) ← SelectBatch(θt−1)</cell><cell>select batch and return the corresponding gradient</cell></row><row><cell>6:</cell><cell>g t ← ∇ft(θt−1) +λθt−1</cell><cell></cell></row><row><cell>7:</cell><cell>ηt ← SetScheduleMultiplier(t)</cell><cell>can be fixed, decay, be used for warm restarts</cell></row><row><cell>8:</cell><cell>mt ← β1mt−1 + ηtαg t</cell><cell></cell></row><row><cell>9:</cell><cell>θt ← θt−1 − mt −ηtλθt−1</cell><cell></cell></row><row><cell cols="2">10: until stopping criterion is met</cell><cell></cell></row><row><cell cols="2">11: return optimized parameters θt</cell><cell></cell></row><row><cell cols="2">Algorithm 2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This work was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme under grant no. 716721, by the German Research Foundation (DFG), under the BrainLinksBrainTools Cluster of Excellence (grant number EXC 1086) and through grant no. INST 37/935-1 FUGG, and by the German state of Baden-Württemberg through bwHPC. We thank Patryk Chrabaszcz for helping running experiments with ImageNet32x32. We thank Matthias Feurer and Robin Schirrmeister for providing valuable feedback on this paper in several iterations. We thank Martin Völker, Robin Schirrmeister, and Tonio Ball for providing us with a comparison of AdamW and Adam on their EEG data. Finally, we thank the following members of the deep learning community for implementing decoupled weight decay in various deep learning libraries: • Jingwei Zhang, Lei Tai, Robin Schirrmeister, and Kashif Rasul for their implementations in PyTorch (see https://github.com/pytorch/pytorch/pull/4429) • Phil Jund for his implementation in TensorFlow described at https://www.tensorflow.org/api_docs/python/tf/contrib/opt/ DecoupledWeightDecayExtension • Sylvain Gugger, Anand Saha, Jeremy Howard and other members of fast.ai for their implementation available at https://github.com/sgugger/Adam-experiments • Guillaume Lambard for his implementation in Keras available at https://github. ADAM WITH COSINE ANNEALING AND WARM RESTARTS We now apply cosine annealing and warm restarts to Adam, following the recent work of Loshchilov</figDesc><table><row><cell>Published as a conference paper at ICLR 2019</cell></row><row><cell>B.2</cell></row><row><cell>com/GLambard/AdamW_Keras</cell></row><row><cell>• Yagami Lin for his implementation in Caffe available at https://github.com/</cell></row><row><cell>Yagami123/Caffe-AdamW-AdamWR</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/xgastaldi/shake-shake</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">In the context of our AdamWR variant discussed in Section B.2, T is the total number of epochs in the current restart.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">SuppFigure 5: Test error curves (top row) and training loss curves (bottom row) for CIFAR-10.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A FORMAL ANALYSIS OF WEIGHT DECAY VS L 2 REGULARIZATION</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 1</head><p>The proof for this well-known fact is straight-forward. SGD without weight decay has the following iterates on</p><p>SGD with weight decay has the following iterates on f t (θ):</p><p>These iterates are identical since λ = λ α .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 2</head><p>Similarly to the Proof of Proposition 1, the iterates of O without weight decay on f reg t (θ) = f t (θ) + 1 2 λ θ 2 2 and O with weight decay λ on f t are, respectively:</p><p>The equality of these iterates for all θ t would imply λθ t = αλ M t θ t . This can only hold for all θ t if M t = kI, with k ∈ R, which is not the case for O. Therefore, no L 2 regularizer λ θ 2 2 exists that makes the iterates equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 3</head><p>O without weight decay has the following iterates on</p><p>where the division by s is element-wise. O with weight decay has the following iterates on f t (θ):</p><p>These iterates are identical since λ = λ α .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL PRACTICAL IMPROVEMENTS OF ADAM</head><p>Having discussed decoupled weight decay for improving Adam's generalization, in this section we introduce two additional components to improve Adam's performance in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 NORMALIZED WEIGHT DECAY</head><p>Our preliminary experiments showed that different weight decay factors are optimal for different computational budgets (defined in terms of the number of batch passes). Relatedly, <ref type="bibr" target="#b10">Li et al. (2017)</ref> demonstrated that a smaller batch size (for the same total number of epochs) leads to the shrinking effect of weight decay being more pronounced. Here, we propose to reduce this dependence by normalizing the values of weight decay. Specifically, we replace the hyperparameter λ by a new (more robust) normalized weight decay hyperparameter λ norm , and use this to set λ as λ = λ norm b BT , where b is the batch size, B is the total number of training points and T is the total number of epochs. 2 Thus, λ norm can be interpreted as the weight decay used if only one batch pass is allowed. We emphasize that our choice of normalization is merely one possibility informed by few experiments; a more lasting conclusion we draw is that using some normalization can substantially improve results. SuppFigure 2 shows the results of standard Adam for a 4 by 4 logarithmic grid of hyperparameter settings (the coarseness of the grid is due to the high computational expense of runs for 1800 epochs). Even after taking the low resolution of the grid into account, the results appear to be at best comparable to the ones obtained with AdamW with 18 times less epochs and a smaller network (see SuppFigure 3, top row, middle). These results are not very surprising given Figure <ref type="figure">2</ref> in the main paper (which demonstrates the effectiveness of AdamW) and SuppFigure 1 (which demonstrates the necessity to use some learning rate schedule such as cosine annealing).</p><p>Our experimental results with Adam and SGD suggested that the total runtime in terms of the number of epochs affect the basin of optimal hyperparameters (see SuppFigure 3). More specifically, the greater the total number of epochs the smaller the values of the weight decay should be. SuppFigure 4 shows that our remedy for this problem, the normalized weight decay defined in Eq. ( <ref type="formula">15</ref>), simplifies hyperparameter selection because the optimal values observed for short runs are similar to the ones for much longer runs. We used our initial experiments on CIFAR-10 to suggest the square root normalization we proposed in Eq. ( <ref type="formula">15</ref>) and double-checked that this is not a coincidence on the ImageNet32x32 dataset <ref type="bibr" target="#b1">(Chrabaszcz et al., 2017)</ref>, a downsampled version of the original ImageNet dataset with 1.2 million 32×32 pixels images, where an epoch is 24 times longer than on CIFAR-10. This experiment also supported the square root scaling: the best values of the normalized weight decay observed on CIFAR-10 represented nearly optimal values for ImageNet32x32 (see SuppFigure 3). In contrast, had we used the same raw weight decay values λ for ImageNet32x32 as for CIFAR-10 and for the same number of epochs, without the proposed normalization, λ would have been roughly 5 times too large for ImageNet32x32, leading to much worse performance. The optimal normalized weight decay values were also very similar (e.g., λ norm = 0.025 and λ norm = 0.05) across SGDW and AdamW. SuppFigure 4 is the equivalent of Figure <ref type="figure">3</ref> in the main paper, but for ImageNet32x32 instead of for CIFAR-10. The qualitative results are identical: weight decay leads to better training loss (crossentropy) than L 2 regularization, and to an even greater improvement of test error. SuppFigure 5 and SuppFigure 6 are the equivalents of Figure <ref type="figure">4</ref> in the main paper but supplemented with training loss curves in its bottom row. The results show that Adam and its variants with decoupled weight decay converge faster (in terms of training loss) on CIFAR-10 than the corresponding SGD variants (the difference for ImageNet32x32 is small). As is discussed in the main paper, when the same values of training loss are considered, AdamW demonstrates better values of test error than Adam. Interestingly, SuppFigure 5 and SuppFigure 6 show that restart variants AdamWR and SGDWR also demonstrate better generalization than AdamW and SGDW, respectively. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A unified theory of adaptive stochastic gradient descent as Bayesian filtering</title>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Aitchison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02030</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A downsampled variant of ImageNet as an alternative to the CIFAR datasets</title>
		<author>
			<persName><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04933</idno>
		<title level="m">Sharp minima can generalize for deep nets</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-Shake regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparing biases for minimal network construction with back-propagation</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lorien Y</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Neural Information Processing Systems</title>
				<meeting>the 1st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00109</idno>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04836</idno>
		<title level="m">On large-batch training for deep learning: Generalization gap and sharp minima</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Alex Krizhevsky. Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2009">2014. 2009</date>
		</imprint>
	</monogr>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09913</idno>
		<title level="m">Visualizing the loss landscape of neural nets</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">SGDR: stochastic gradient descent with warm restarts</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2408" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">Leslie N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01186v3</idno>
	</analytic>
	<monogr>
		<title level="m">Cyclical learning rates for training neural networks</title>
				<imprint>
			<date type="published" when="2012">2016. 2012</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
	<note>COURSERA: Neural networks for machine learning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Intracranial error detection via deep learning</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Völker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiří</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">T</forename><surname>Schirrmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joos</forename><surname>Behncke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lukas Dj Fiederer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Schulze-Bonhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Marusič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tonio</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><surname>Ball</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01667</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sface: An efficient network for face detection in large scale variations</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Jian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06559</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The marginal value of adaptive gradient methods in machine learning</title>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Ashia C Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><surname>Recht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08292</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12281</idno>
		<title level="m">Three mechanisms of weight decay regularization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04546</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Normalized direction-preserving adam</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012[cs.CV</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Test error curves (top row) and training loss curves (bottom row) for Ima-geNet32x32</title>
	</analytic>
	<monogr>
		<title level="j">SuppFigure</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
