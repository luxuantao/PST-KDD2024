<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASDNet: Attention Based Semi-supervised Deep Networks for Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>Nie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<settlement>Chapel Hill</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<settlement>Chapel Hill</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaozong</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<settlement>Chapel Hill</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@med.unc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<settlement>Chapel Hill</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ASDNet: Attention Based Semi-supervised Deep Networks for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F5CC4FB76A009768731EB60EF2FC1EFF</idno>
					<idno type="DOI">10.1007/978-3-030-00937-3_43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segmentation is a key step for various medical image analysis tasks. Recently, deep neural networks could provide promising solutions for automatic image segmentation. The network training usually involves a large scale of training data with corresponding ground truth label maps. However, it is very challenging to obtain the ground-truth label maps due to the requirement of expertise knowledge and also intensive labor work. To address such challenges, we propose a novel semi-supervised deep learning framework, called "Attention based Semisupervised Deep Networks" (ASDNet), to fulfill the segmentation tasks in an end-to-end fashion. Specifically, we propose a fully convolutional confidence network to adversarially train the segmentation network. Based on the confidence map from the confidence network, we then propose a region-attention based semi-supervised learning strategy to include the unlabeled data for training. Besides, sample attention mechanism is also explored to improve the network training. Experimental results on real clinical datasets show that our ASDNet can achieve state-ofthe-art segmentation accuracy. Further analysis also indicates that our proposed network components contribute most to the improvement of performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent development of deep learning has largely boosted the state-of-the-art segmentation methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. Among them, fully convolutional networks (FCN) <ref type="bibr" target="#b7">[8]</ref>, a variant of convolutional neural networks (CNN), is a recent popular choice for semantic image segmentation in both computer vision and medical image fields <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>. FCN trains neural networks in an end-to-end fashion by directly optimizing intermediate feature layers for segmentation, which makes it outperform the traditional methods that often regard the feature learning and segmentation as two separate tasks. UNet <ref type="bibr" target="#b10">[11]</ref>, an evolutionary variant of FCN, has achieved excellent performance by effectively combining high-level and lowlevel features in the network architecture. Generally, while being effective, the training of FCN (or UNet) requires a large amount of labeled data as there are millions of parameters in the network to be optimized. However, it is difficult to acquire a large training set with manually labeled ground-truth maps due to the following three factors: (a) manual annotation requires expertise knowledge; (b) it is time-consuming and tedious to annotate pixel-wise (voxel-wise) label maps; (c) it suffers from large intra-and inter-observer variability.</p><p>Several works have been done to address the aforementioned challenges <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>. To relieve the demand for large-scale labeled data, Bai et al. <ref type="bibr" target="#b0">[1]</ref> proposed a semi-supervised deep learning framework for cardiac MR image segmentation, in which the segmented label maps from unlabeled data are incrementally included into the training set to refine the network. Baur et al. <ref type="bibr" target="#b1">[2]</ref> introduced auxiliary manifold embedding in the latent space to FCN for semi-supervised learning in the MS lesion segmentation. In both cases, the unlabeled data information are fully involved in the model learning. However, certain regions of the unlabeled data may not be suitable for the learning due to their low-quality (automatically-) segmented label maps. To overcome such issues, we propose an attention based semi-supervised learning framework for medical image segmentation. Our framework is composed of two networks: (1) segmentation network and (2) confidence network. Specifically, we propose a fully convolutional adversarial learning scheme (i.e., using confidence network) to better train the segmentation network. The confidence map generated by the confidence network can provide us the trustworthy regions in the segmented label map from the segmentation network. Based on the confidence map, we further propose a region based semi-supervised loss to adaptively use part of unlabeled data for training the network. Since we can adopt unlabeled data to further train the segmentation network, the need of a large-scale training set can be alleviated accordingly. Our proposed algorithm has been applied to the task of pelvic organ segmentation, which is critical for guiding both biopsy and cancer radiation therapy. Experimental results indicate that our proposed algorithm can improve the segmentation accuracy, compared to other state-of-the-art methods. In addition, our proposed training strategies are also proved to be effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>As mentioned above, the proposed ASDNet consists of two subnetworks, i.e., (1) segmentation network (denoted as S) and (2) confidence network (denoted as D). The architecture of our proposed framework is presented in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>To ease the description of the proposed algorithm, we first give the notations used throughout the paper. Given a labeled input image X ∈ R H×W ×T with corresponding ground-truth label map Y ∈ Z H×W ×T , we encode it to onehot format P ∈ R H×W ×T ×C , where C is the number of semantic categories in the dataset. The segmentation network outputs the class probability map P ∈ R H×W ×T ×C . Similarly, we regard an unlabeled image as U ∈ R H×W ×T . Therefore, the whole input image dataset can be defined by O = {X, U}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Segmentation Network with Sample Attention</head><p>In ASDNet as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the segmentation network can be any end-toend segmentation network, such as FCN <ref type="bibr" target="#b7">[8]</ref>, UNet <ref type="bibr" target="#b10">[11]</ref>, VNet <ref type="bibr" target="#b8">[9]</ref>, and DSRe-sUNet <ref type="bibr" target="#b12">[13]</ref>. In this paper, we adopt a simplified VNet <ref type="bibr" target="#b8">[9]</ref> (internal pool-convdeconv layers are removed, and thus is denoted as SVNet) as the segmentation network to balance the performance and memory cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-class Dice Loss:</head><p>The class imbalance problem is usually serious in medical image segmentation tasks. To overcome it, we propose using a generalized multi-class Dice loss <ref type="bibr" target="#b11">[12]</ref> as the segmentation loss, as defined below in Eq. 1:</p><formula xml:id="formula_0">L Dice (X, P; θ s ) = 1 -2 C c=1 π c H h=1 W w=1 T t=1 P h,w,t,c P h,w,t,c C c=1 π c H h=1 W w=1 T t=1 P h,w,t,c + P h,w,t,c ,<label>(1)</label></formula><p>where π c is the class balancing weight of category c, θ S is the parameters of segmentation network, and we set</p><formula xml:id="formula_1">π c = 1/ H h=1 W w=1 T t=1 P h,w,t,c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>. P is the predicted probability maps from the segmentation network: P = S (X, θ s ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-class Dice Loss with Sample Attention:</head><p>Besides the class imbalance problem, the network optimization also suffers from the issue of dominance by easy samples: the large number of easy samples will dominate network training, thus the difficult samples cannot be well considered. To address this issue, inspired by the focal loss <ref type="bibr" target="#b5">[6]</ref> proposed to handle similar issue in detection networks, we propose a sample attention based mechanism to consider the importance of each sample during the training. The multi-class Dice loss with sample attention is thus defined below by Eq. 2:</p><formula xml:id="formula_2">L AttDice (X, P; θ s ) = (1 -dsc) β ⎛ ⎜ ⎜ ⎜ ⎝ 1 -2 C c=1 π c H h=1 W w=1 T t=1 P h,w,t,c P h,w,t,c C c=1 π c H h=1 W w=1 T t=1 P h,w,t,c + P h,w,t,c ⎞ ⎟ ⎟ ⎟ ⎠ ,</formula><p>(2) where dsc is the average Dice similarity coefficient of the sample over different categories, e.g., different organ labels. Note that we re-compute the dsc in each iteration, but we don't back-propagate gradient through it when training the networks. β is the sample attention parameter with a range [0, 5]. Following <ref type="bibr" target="#b5">[6]</ref>, we set β to 2 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Confidence Network for Fully Convolutional Adversarial Learning</head><p>Adversarial learning is derived from the recent popular Generative Adversarial Network (GAN) <ref type="bibr" target="#b2">[3]</ref>. It has achieved a great success in image generation and segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref>. Hence, we also incorporate adversarial learning in our architecture to further improve the segmentation network. Instead of using CNNbased discriminator, we propose to use FCN-based discriminator to generate local confidence at local region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Loss of the Confidence Network:</head><p>The training objective of the confidence network is the summation of binary cross-entropy loss over the image domain, as shown in Eq. 3. Here, we use S and D to denote the segmentation and confidence networks, respectively.</p><formula xml:id="formula_3">L D (X, P; θ d ) = L BCE (D(P, θ d ), 1) + L BCE (D(S(X), θ d ), 0),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">LBCE Q, Q = - H h=1 W w=1 T t=1 Q h,w,t log Q h,w,t + (1 -Q h,w,t ) log 1 -Q h,w,t</formula><p>(4) where X and P represent the input data and its corresponding manual label map (one-hot encoding format), respectively. θ d is network parameters for the confidence network.</p><p>Adversarial Loss of the Segmentation Network: For segmentation network, besides the multi-class Dice loss with sample attention as defined in Eq. 2, there is another loss from D working as "variational" loss. It enforces higher-order consistency between ground-truth segmentation and automatic segmentation. In particular, the adversarial loss ("ADV") to improve S and fool D can be defined by Eq. 5.</p><formula xml:id="formula_5">L ADV (O, θ s ) = L BCE (D (S (O; θ s )) , 1)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Region-Attention Based Semi-supervised Learning</head><p>Since our discriminator (i.e., confidence network) could provide local confidence information over the image domain, we use such information in the semisupervised setting to include unlabeled data for improving segmentation accuracy, and the similar strategy has been explored in <ref type="bibr" target="#b4">[5]</ref>. Specifically, given an unlabeled image U, the segmentation network will first produce the probability map P = S (U), which will be then used by the trained confidence network to generate a confidence map M = D( P), indicating where the confident regions of the prediction results are close enough to the ground truth label distribution. The confident regions can be easily obtained by setting a threshold (i.e., γ) to the confidence map. In this way, we can use these confident regions as masks to select parts of unlabeled data and their segmentation results to enrich the set of supervised training data. Thus, our proposed semi-supervised loss can be defined by Eq. 6.</p><formula xml:id="formula_6">L semi (U, θ s ) = 1 -2 C c=1 π c H h=1 W w=1 T t=1 [M &gt; γ] h,w,t P h,w,t,c P h,w,t,c C c=1 π c H h=1 W w=1 T t=1</formula><p>[M &gt; γ] h,w,t P h,w,t,c + P h,w,t,c <ref type="bibr" target="#b5">(6)</ref> where P is the one-hot encoding of Y, and Y = arg max( P). [] is the indicator function. Similar to dsc in Eq. 2, P and the value of indicator function are recomputed in each iteration.</p><p>Total Loss for Segmentation Network: By summing the above losses, the total loss to train the segmentation network can be defined by Eq. 7.</p><formula xml:id="formula_7">L S = L AttDice + λ 1 L ADV + λ 2 L semi ,<label>(7)</label></formula><p>where λ 1 and λ 2 are the scaling factors to balance the losses. They are selected at 0.03 and 0.3 after trails, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implementation Details</head><p>Pytorch<ref type="foot" target="#foot_0">1</ref> is adopted to implement our proposed ASDNet shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We adopt Adam algorithm to optimize the network. The input size of the segmentation network is 64 × 64 × 16. The network weights are initialized by the Xavier algorithm, and weight decay is set to be 1e-4. For the network biases, we initialize them to 0. The learning rates for the segmentation and confidence network are initialized to 1e-3 and 1e-4, followed by decreasing the learning rate 10 times every 3 epochs. Four Titan X GPUs are utilized to train the networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Our pelvic dataset consists of 50 prostate cancer patients from a cancer hospital, each with one T2-weighted MR image and corresponding manually-annotated label map by medical experts. In particular, the prostate, bladder and rectum in all these MRI scans have been manually segmented, which serve as the ground truth for evaluating our segmentation method. Besides, we have also acquired 20 MR images from additional 20 patients, without manually-annotated label maps. All these images were acquired with 3T MRI scanners. The image size is mostly 256 × 256 × (120-176), and the voxel size is 1 × 1 × 1 mm 3 . Five-fold cross validation is used to evaluate our method. Specifically, in each fold of cross validation, we randomly chose 35 subjects as training set, 5 subjects as validation set, and the remaining 10 subjects as testing set. We use sliding windows to go through the whole MRI for prediction for a testing subject. Unless explicitly mentioned, all the reported performance by default is evaluated on the testing set. As for evaluation metrics, we utilize Dice Similarity Coefficient (DSC) and Average Surface Distance (ASD) to measure the agreement between the manually and automatically segmented label maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison with State-of-the-art Methods</head><p>To demonstrate the advantage of our proposed method, we also compare our method with other five widely-used methods on the same dataset as shown in Table <ref type="table" target="#tab_0">1</ref>: (1) multi-atlas label fusion (MALF), ( <ref type="formula">2</ref>) SSAE <ref type="bibr" target="#b3">[4]</ref>, (3) UNet <ref type="bibr" target="#b10">[11]</ref>, (4) VNet <ref type="bibr" target="#b8">[9]</ref>, and (5) DSResUNet <ref type="bibr" target="#b12">[13]</ref>. Also, we present the performance of our proposed ASDNet. Table <ref type="table" target="#tab_0">1</ref> quantitatively compares our method with the five state-of-the-art segmentation methods. We can see that our method achieves better accuracy than the five state-of-the-art methods in terms of both DSC and ASD. The VNet works well in segmenting bladder and prostate, but it cannot work very well for rectum (which is often more challenging to segment due to the long and narrow shape). Compared to UNet, DSResUNet improves the accuracy by a large margin, indicating that residual learning and deep supervision bring performance gain, and thus it might be a good future direction for us to further improve our proposed method. We also visualize some typical segmentation results in Fig. <ref type="figure" target="#fig_1">2</ref>, which further show the superiority of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Impact of Each Proposed Component</head><p>As our proposed method consists of several designed components, we conduct empirical studies below to analyze them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Sample Attention:</head><p>As mentioned in Sect. 2.1, we propose a sample attention mechanism to assign different importance for different samples so that the network can concentrate on hard-to-segment examples and thus avoid dominance by easy-to-segment samples. The effectiveness of sample attention mechanism (i.e., AttSVNet) is further confirmed by the improved performance, e.g., 0.82%, 1.60% and 1.81% DSC performance improvements (as shown in Table <ref type="table" target="#tab_1">2</ref>) for bladder, prostate and rectum, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Fully Convolutional Adversarial Learning:</head><p>We conduct more experiments for comparing with the following three networks: (1) only segmentation network; (2) segmentation network with a CNN-based discriminator <ref type="bibr" target="#b2">[3]</ref>;</p><p>(3) segmentation network with a FCN-based discriminator (i.e., confidence network). Performance in the middle of Table <ref type="table" target="#tab_1">2</ref> indicates that adversarial learning contributes a little bit to improving the results as it provides a regularization to prevent overfitting. Compared with CNN-based adversarial learning, our proposed FCN-based adversarial learning further improves the performances by 0.90% in average. This demonstrates that fully convolutional adversarial learning works better than the typical adversarial learning with a CNN-based discriminator, which means the FCN-based adversarial learning can better learn structural information from the distribution of ground-truth label map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Semi-supervised Loss:</head><p>We apply the semi-supervised learning strategy with our proposed ASDNet on 50 labeled MRI and 20 extra unlabeled MRI. The comparison methods are semiFCN <ref type="bibr" target="#b0">[1]</ref> and semiEmbedFCN <ref type="bibr" target="#b1">[2]</ref>. We use the AttSVNet as the basic architecture of these two methods for fair comparison. The evaluation of the comparison experiments are all based on the labeled dataset, and the unlabeled data involves only in the learning phase. The experimental results in Table <ref type="table" target="#tab_1">2</ref> show that our proposed semi-supervised strategy works better than the semiFCN and the semiEmbedFCN. Moreover, it is worth noting that the adversarial learning on the labeled data is important to our proposed semi-supervised scheme. If the segmentation network does not seek to fool the discriminator (i.e., AttSVNet+Semi), the confidence maps generated by the confidence network would not be meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Validation on Another Dataset</head><p>To show the generalization ability of our proposed algorithm, we conduct additional experiments on the PROMISE12-challenge dataset <ref type="bibr" target="#b6">[7]</ref>. This dataset contains 50 subjects, each with a pair of MRI and its manual label map (where only prostate was annotated). Five-fold cross validation is performed to evaluate the performance of all comparison methods. Our proposed algorithm again achieves very good performance in segmenting prostate (i.e., 0.900 in terms of DSC), and it is also very competitive compared to the state-of-the-art methods applied to this dataset in the literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. These experimental results indicate a good generalization capability of our proposed ASDNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we have presented a novel attention-based semi-supervised deep network (ASDNet) to segment medical images. Specifically, the semi-supervised learning strategy is implemented by fully convolutional adversarial learning, and also region-attention based semi-supervised loss is adopted to effectively address the insufficient data problem for training the complex networks. By integrating these components into the framework, our proposed ASDNet has achieved significant improvement in terms of both accuracy and robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the architecture of our proposed ASDNet, which consists of a segmentation network and a confidence network.</figDesc><graphic coords="3,79.86,431.00,265.27,126.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pelvic organ segmentation results of a typical subject by different methods. Orange, silver and pink contours indicate the manual ground-truth segmentation, and yellow, red and cyan contours indicate automatic segmentation.</figDesc><graphic coords="6,108.99,388.82,234.07,67.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>DSC and ASD on the pelvic dataset by different methods.</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell></cell><cell></cell><cell>ASD</cell><cell></cell></row><row><cell></cell><cell>Bladder</cell><cell>Prostate</cell><cell>Rectum</cell><cell>Bladder</cell><cell>Prostate</cell><cell>Rectum</cell></row><row><cell>MALF</cell><cell cols="6">.867(.068) .793(.087) .764(.119) 1.641(.360) 2.791(.930) 3.210(2.112)</cell></row><row><cell>SSAE</cell><cell cols="6">.918(.031) .871(.042) .863(.044) 1.089(.231) 1.660(.490) 1.701(.412)</cell></row><row><cell>UNet</cell><cell cols="6">.896(.028) .822(.059) .810(.053) 1.214(.216) 1.917(.645) 2.186(0.850)</cell></row><row><cell>VNet</cell><cell cols="6">.926(.018) .864(.036) .832(.041) 1.023(.186) 1.725(.457) 1.969(.449)</cell></row><row><cell cols="4">DSResUNet .944(.009) .882(.020) .869(.032)</cell><cell cols="3">.914(.168) 1.586(.358) 1.586(.405)</cell></row><row><cell>Proposed</cell><cell cols="6">.970(.006) .911(.016) .906(.026) .858(.144) 1.316(.288) 1.401(.356)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the performance of methods with different strategies on the pelvic dataset in terms of DSC.</figDesc><table><row><cell>Method</cell><cell>Bladder</cell><cell>Prostate</cell><cell>Rectum</cell></row><row><cell>VNet</cell><cell cols="3">.926(.018) .864(.036) .832(.041)</cell></row><row><cell>SVNet</cell><cell cols="3">.920(.015) .862(.037) .844(.037)</cell></row><row><cell>AttSVNet</cell><cell cols="3">.931(.010) .878(.028) .862(.034)</cell></row><row><cell cols="4">AttSVNet+CNN .938(.010) .884(.026) .874(.031)</cell></row><row><cell cols="4">AttSVNet+FCN .944(.008) .893(.022) .887(.025)</cell></row><row><cell>semiFCN</cell><cell cols="3">.959(.006) .895(.024) .885(.030)</cell></row><row><cell cols="4">semiEmbedFCN .964(.007) .902(.022) .891(.028)</cell></row><row><cell cols="4">AttSVNet+Semi .937(.012) .878(.036) .865(.041)</cell></row><row><cell>Proposed</cell><cell cols="3">.970(.006) .911(.016) .906(.026)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/pytorch/pytorch.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>D. Shen-This work was supported by the National Institutes of Health grant 1R01 CA140413.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for network-based cardiac MR image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66185-8_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66185-829" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10434</biblScope>
			<biblScope unit="page" from="253" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for fully convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_36</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-736" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable MR prostate segmentation via deep feature learning and sparse patch matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1077" to="1089" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07934</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<title level="m">Focal loss for dense object detection</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation of prostate segmentation algorithms for MRI: the PROMISE12 challenge</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<editor>3DV</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Medical image synthesis with context-aware generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-748" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67558-9_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67558-928" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">10553</biblScope>
			<biblScope unit="page" from="240" to="248" />
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>DLMIA/ML-CDS -2017</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Volumetric ConvNets with mixed residual connections for automated prostate segmentation from 3D MR images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
