<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi-Lin</forename><surname>Tuan</surname></persName>
							<email>ytuan@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sajjad</forename><surname>Beygi</surname></persName>
							<email>beygi@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Alexa AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maryam</forename><surname>Fazel-Zarandi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Alexa AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
							<email>qzgao@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Alexa AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessandra</forename><surname>Cervone</surname></persName>
							<email>cervon@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Alexa AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response. This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms and the hand-crafted rules that require extensive labor. One possible way to improve user experience and relieve the manual efforts of designers is to build an end-toend dialogue system that can do reasoning itself while perceiving user's utterances. In this work, we propose a novel method to incorporate the knowledge reasoning capability into dialogue systems in a more scalable and generalizable manner. Our proposed method allows a single transformer model to directly walk on a large-scale knowledge graph to generate responses. To the best of our knowledge, this is the first work to have transformer models generate responses by reasoning over differentiable knowledge graphs. We investigate the reasoning abilities of the proposed method on both task-oriented and domain-specific chitchat dialogues. Empirical results show that this method can effectively and efficiently incorporate a knowledge graph into a dialogue system with fully-interpretable reasoning paths.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, dialogue systems are ubiquitous in customer service and voice-based assistants. One of the main uses of this technology is supporting humans in accomplishing tasks that might require accessing and navigating large knowledge bases (e.g., movies search). A dialogue system architecture is typically composed of a natural language understanding (NLU) module, a dialogue management (DM) module, and a natural language generation (NLG) module <ref type="bibr" target="#b14">(Jurafsky and Martin, 2009;</ref><ref type="bibr" target="#b40">Williams et al., 2016)</ref>. First, the NLU component extracts a meaning representation from the user utterance based on which the DM generates the next system action by reasoning over the meaning representation and communicating with external applications if necessary. For example, the DM may retrieve information from external knowledge graphs (KG) to answer the user's query based on the dialogue history. This process requires the DM to convert the output of NLU to a query to be issued to the backend. Given the difficulty of this step, which is often domain-dependent, the DM component might require the design of hand-crafted rules. However, such rules are usually not scalable to different applications. They could require considerable effort to cover all possible cases/dialogue flows, leading to expensive costs to design new applications. Moreover, in several cases, users interacting with such assistants are forced to formulate specific queries in order to accomplish their objective, which might break user engagement.</p><p>To alleviate the problem of having to design expensive hand-crafted rules and breaking user experience, recent works have explored the possibility of building end-to-end dialogue systems <ref type="bibr" target="#b38">(Wen et al., 2017)</ref> and all-in-one response generation models <ref type="bibr">(Serban et al., 2016)</ref>. Among them, since graph is one of the main structure to store knowledge, recent research <ref type="bibr" target="#b7">(Ghazvininejad et al., 2018;</ref><ref type="bibr" target="#b45">Zhou et al., 2018;</ref><ref type="bibr" target="#b24">Moon et al., 2019;</ref><ref type="bibr" target="#b34">Tuan et al., 2019;</ref><ref type="bibr" target="#b42">Yang et al., 2020)</ref> has proposed methods to generate natural language responses according to both the dialogue history and external knowledge graph. Despite these innovative and inspiring methods, there are some shortcomings. For instance, these methods are either not fully-interpretable or limited to small-scale knowledge graphs.</p><p>In this paper, we propose a novel dialogue differentiable knowledge graph model <ref type="bibr">(DiffKG)</ref>. The DiffKG is a single transformer model that directly (1) generates a sequence of relations to perform multi-hop reasoning on a reified KG representation proposed by <ref type="bibr" target="#b2">(Cohen et al., 2019)</ref>, and then (2) generates responses using the retrieved entities. To the best of our knowledge, this is the first dialogue model that can directly walk on a large-scale KG with flexibility and interpretability. DiffKG allows having flexible entity values in the KG and handling novel entity values with an arbitrarily defined number of tokens. The reasoning path of DiffKG consists of the predicted relations, thus allowing for transparency.</p><p>We run extensive experiments to test DiffKG performance on KG-grounded dialogues. We select Stanford Multi-domain Dialogues (SMD) <ref type="bibr" target="#b6">(Eric et al., 2017)</ref> and propose a new dataset, SMD-Reasoning, to simulate scenarios requiring multiple reasoning types and select the OpenDialKG <ref type="bibr" target="#b24">(Moon et al., 2019)</ref> to simulate scenarios requiring largescale KG reasoning without preprocessing. We then compare DiffKG with state-of-the-art models on SMD and OpenDialKG and an additional baseline that flattens KGs into a textual form from which transformers can learn. Empirically, our experiments show that DiffKG can effectively be trained on large-scale KGs and demonstrate its robustness with modified triplets in a KG. From the perspective of computation, DiffKG leads to relatively low extra time and memory usage compared to transformer models not using any KG information.</p><p>In summary, our contributions are: 1) We propose DiffKG, a novel method that can effectively and flexibly incorporate large-scale KG; 2) We demonstrate that DiffKG is a modelagnostic method and can be applied to different model architectures; 3) We show that DiffKG is an interpretable method with low add-on latency at inference time. Our code and processed datasets are released in https://github. com/Pascalson/DiffKG-Dialog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent years have seen a surge of new methods proposing end-to-end models that try to both understand natural language input text and search information. Two of the widely explored tasks are question-answering (QA) and dialogue generation.</p><p>QA. Multiple QA methods <ref type="bibr" target="#b39">(Weston et al., 2015;</ref><ref type="bibr" target="#b43">Yin et al., 2016;</ref><ref type="bibr" target="#b10">Hao et al., 2017;</ref><ref type="bibr" target="#b29">Rajpurkar et al., 2018;</ref><ref type="bibr" target="#b36">Verga et al., 2020;</ref><ref type="bibr" target="#b5">Eisenschlos et al., 2021)</ref> have been proposed to tackle tasks that go beyond what is explicitly stated in the linguistic context <ref type="bibr" target="#b33">(Storks et al., 2019)</ref>. For example, the benchmarks <ref type="bibr" target="#b23">(Mihaylov et al., 2018;</ref><ref type="bibr" target="#b30">Reddy et al., 2019;</ref><ref type="bibr" target="#b16">Khot et al., 2020;</ref><ref type="bibr" target="#b18">Lin et al., 2021)</ref> are particu-larly useful for the model to extract information from external knowledge bases to answer questions. Nonetheless, these studies mostly take the retrieved information from KG as the answer to a single question, while in dialogue we have to formulate an informative response to multi-turn dialogue history.  <ref type="formula">2021</ref>) utilize information from knowledge bases (either graphs or tables) to enhance the dialogue system. They usually train the entities and relations embeddings of the knowledge bases and incorporate these embeddings into the input representation to predict the response. Third, <ref type="bibr" target="#b24">Moon et al. (2019)</ref>; <ref type="bibr" target="#b34">Tuan et al. (2019)</ref>; <ref type="bibr" target="#b13">Jung et al. (2020)</ref> formulate the reasoning process more explicitly, as a path traversal over knowledge graphs. These methods further improve the transparency and explainability of the conversational agent and share the most similar idea with us. However, they either only predict the reasoning path without generating responses or need subgraph sampling to reduce the scale of KG. In this work, our approach uses a transformer model to jointly predict explicit reasoning paths over a large-scale knowledge graph and generate dialogue responses based on the reasoning results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge Graph for Dialogue System</head><p>We assume that the knowledge of the system can be represented by a knowledge graph (KG) G = {E, R}, where E denotes the entities and R denotes the relations. The knowledge graph G contains multiple triples describing the connections among entities and relations. We denote the k-th triple of this graph as (e h k , r k , e are denoted as N T , N E , N R , respectively. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Response Generation in Dialogue System</head><p>If we define the dialogue history as a sequence of tokens that occurred during the user and system interactions, then a flattened dialogue history can be written as:</p><formula xml:id="formula_0">x = (x 1 , x 2 , ..., x m , ..., x M )<label>(1)</label></formula><p>where x m is the m-th token in the dialogue history with M tokens. In an end-to-end dialogue system, we assume a dialogue system parameterized by θ exists that can predict a probability distribution of responses P θ (•|x, G). The generated responses are sampled from this probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Problem Statement</head><p>We focus on understanding the ability of language models in performing reasoning during a conversation. We consider two tasks that are usually required in dialogue scenarios and call them semantic form and natural language (NL) form in Table <ref type="table" target="#tab_0">1</ref>. First, given a dialogue history and a user's query, the task of semantic form is to predict the next system action, corresponding to the output of the DM module, based on the available knowledge. In this case, we assume the expected output is the essential knowledge for an NLG module. We argue that this task could help better evaluate if the response is correct or not and which type of reasoning can be more successfully handled. Second, given a dialogue history and a user's query, the task of the NL form could be to directly output the response given</p><p>1 An example of the triples in G is a triple e h k = gas station, r k = IsTypeOf, and e t k = Chevron. That is, "gas station is the type of Chevron" to this system. by the system. This setting with annotated reasoning path can shed light on understanding if the model can learn to support chit-chat and reasoning at the same time.</p><p>Moreover, we aim to understand models' reasoning capability both in the form of logical reasoning and over the provided knowledge. As illustrated in Table <ref type="table" target="#tab_0">1</ref>, by KG reasoning, we refer to the ability of the model to retrieve information from an arbitrary scaled KG in multiple hops. Meanwhile, we refer to logical reasoning as the ability of the model to conduct operations such as evaluating whether a statement is true or false, selecting min/max from a list of alternatives, and extracting constraints.</p><p>We formulate the task that we focus on as follows: given the dialogue history x and currently accessible KG G, can we extend a transformer model to predict a correct response y in either semantic or NL form? As illustrated in Table <ref type="table" target="#tab_0">1</ref>, this task not only requires the model to accurately retrieve information from the KG, but also needs to do further logical operations on the information. To solve this task, a model should also be able to effectively integrate the dialogue history x with the KG G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proposed Approach</head><p>Figure <ref type="figure" target="#fig_1">1</ref> illustrates our proposed architecture which contains four main parts: a dialogue history encoder, a differentiable KG reasoning module, a learnable logical operations module, and a response decoder (the transformer model). Note that we experiment with two types of transformers: a causal language model GPT2 <ref type="bibr" target="#b27">(Radford et al., 2019)</ref> and an encoder-decoder model T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref>. For GPT2, we reuse the same encoder that is used at the beginning of the process, i.e., f enc in Figure <ref type="figure" target="#fig_1">1</ref>, as the final transformer that generates the response token by token. For T5, we reuse the same encoder as the encoder of the final transformer with a separate decoder that generates the response. Therefore, this method contains a single transformer model. In following sections we present each module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dialogue History Encoder</head><p>We use encoder model to project x and obtain the dialogue history embedding through x = f enc (x) ∈ IR d , where d is the hidden size of the encoder. The embedding x is first fed into an operation layer with parameters W o ∈ IR d×d . The operation layer predicts the operation vector a = W T o x ∈ IR d . At the same time, the embedding x is also fed into a relation layer with parameters W r ∈ IR d×N R H . The relation layer predicts the concatenation of a sequence of relations r = {r h |1 ≤ h ≤ H}, where r h ∈ IR N R is the relation to be used at the h-th hop in the programmed walking block and H is the maximum number of hops. The embedding x is also fed into a checkpoints layer with parameters W c ∈ IR d×2H . This layer produces the concatenation of a sequence of walk-or-check vectors c = {c h |1 ≤ h ≤ H}, where c h ∈ IR 2 is the walk-or-check vector at the hth hop to determine the weights of the programmed walking module and the operation vector.</p><formula xml:id="formula_1">x = f enc (x) , a = W T o x , r = W T r x , c = softmax(W T c x) .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Differential Knowledge Graph Reasoning</head><p>To ensure that our model can scale to larger KGs, we adopt the reified KG representation proposed by <ref type="bibr" target="#b2">(Cohen et al., 2019)</ref>. The reified KG represents the graph G using three sparse matrices: After predicting the relation sequence r, we start the graph traversal from a given set of initial entities E 0 ⊆ E. We first map the initial entities into a vector e 1 = [1(e ∈ E 0 ), ∀e ∈ E]. That is, each entry of e 1 ∈ IR N E has value 1 if that entity is in the initial entities list E 0 , otherwise, the entry is zero. We then predict the next (temporary) entity vector e 2 by conducting a Next module:</p><formula xml:id="formula_2">head matrix M h ∈ IR N T ×N E , relation matrix M r ∈ IR N T ×N R</formula><formula xml:id="formula_3">e r h+1 = Next(e h , r h ) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">Next(e h , r h ) = M T t (M h e h ⊙ M r r h ) ||M T t (M h e h ⊙ M r r h )|| 2 + ϵ , (<label>4</label></formula><p>) Here ϵ is an arbitrary small number to offset the denominator and prevent division by zero. We introduce the normalized Next to solve the issue with the method proposed by <ref type="bibr" target="#b2">(Cohen et al., 2019)</ref> for knowledge graph completion defined as Follow(e h , r h ) = M T t (M h e h ⊙ M r r h ); since in a dialogue model, we can seldom predict the relation vectors that perfectly match the entity vectors. That is, if directly using the Follow module in <ref type="bibr" target="#b2">(Cohen et al., 2019)</ref>, the ||e h || 2 will not be one and will vanish as the hop number h increases. Specifically, note that in our proposed module, the predicted relations r h are independent of the traversed entities e h . For instance, finding the "distance" of "the nearby gas station" is independent of whether the nearby gas station is "Chevron" or "Shell".</p><p>To allow the model to dynamically select the number of reasoning hops, we add a relation type "ToSelf" into R and connect each entity to itself by "ToSelf". More specifically, the KG will contain triples (e h k , r k , e t k ) for all e h k = e t k ∈ E and r k = ToSelf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Entity Embeddings</head><p>At each hop, we further conduct the operation vectors a on the entities weighted by the entity vector e h . First, we tokenize each entity and represent it by the concatenation of its token embeddings. This step allows (1) representing entities with longer texts such as phrases and sentences, and (2) eliminating the effort to retrain entity embeddings whenever new entity values are added. The entity embeddings can then be represented as a tensor E ∈ IR N E ×d×m , where m is the maximum number of tokens of entities<ref type="foot" target="#foot_0">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Learnable Logical Operation and Checkpoints</head><p>We compute the transformed entity embeddings by element-wise multiplication of the entity embeddings E with the entity vector e h at the h-th hop.</p><p>Next, the dot product of the operation vectors and the transformed entity embeddings is passed to a softmax layer as the entity vector at the next hop:</p><formula xml:id="formula_5">e a h+1 = softmax (a(E ⊙ e h )) ,<label>(5)</label></formula><p>Further, at the h-th hop we use the walk-or-check vector c h to combine the Next and operation modules above. The combined entity vector is given by:</p><formula xml:id="formula_6">e h+1 = c T h e r h+1 e a h+1 = c T h Next(e h , r h ) softmax (a(E ⊙ e h )) ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Response Decoder</head><p>After H hops reasoning is done, the entities with top-k values in the entity vector e H are selected, indicating that they have the highest probability to be retrieved from the graph. These entities are converted into their embeddings in E and multiplied by their values in e H . These entity embeddings are then concatenated with the dialogue history x. The concatenated vectors are fed as the input into the transformer model to predict the response token by token. The predicted probability distribution over the output space can be written as P (•|x, M h , M r , M t ). Since all components are differentiable, all modules can be trained end-toend with the dialogue history x and the reified KG representation {M h , M r , M t } using the crossentropy loss with the ground-truth output y as the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L =</head><p>(x,y)</p><formula xml:id="formula_7">− log P (y|x, M h , M r , M t ) .<label>(7)</label></formula><p>During the inference time, the reasoning modules (relation layer, operation layer, and checkpoints layer) work exactly the same as the training stage, the only difference is that the response decoder is fed with predicted tokens in previous time steps (inference stage) instead of the ground-truth output (training stage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments 6.1 Datasets</head><p>We evaluate our proposed approach on three datasets. Among them, we use Stanford Multidomain Dialogues (SMD) <ref type="bibr" target="#b6">(Eric et al., 2017)</ref> and OpenDialKG <ref type="bibr" target="#b24">(Moon et al., 2019)</ref> to test the methods generalizability on different dialogue types (task-oriented / chit-chat) and scales of structured knowledge (pairwise database / universal KG). To further analyze the reasoning ability, we propose a new dataset, SMD-Reasoning, by modifying the output of SMD dataset from natural language responses to actions paired with their reasoning types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stanford Multi-domain Dialogues (SMD)</head><p>The SMD dataset <ref type="bibr" target="#b6">(Eric et al., 2017)</ref> is composed of twospeaker conversations, where a driver talks with the car assistant to tackle tasks in three domains: scheduling, navigation, and weather forecasting. Each dialogue focuses on one domain and is paired with a database having the related information. We convert the original database into two formats: (1) the natural language descriptions (NLD) and (2) the KG. The NLD form allows us to investigate the ability of the model to interpret unstructured knowledge, while the KG form could be a more extensible structured knowledge compared to tables.</p><p>OpenDialKG OpenDialKG dataset <ref type="bibr" target="#b24">(Moon et al., 2019)</ref> is composed of two-speakers recommendation and chit-chat style conversations. Each turn in a dialogue is annotated with the reasoning path on the provided KG, which is filtered from Freebase <ref type="bibr" target="#b1">(Bollacker et al., 2008)</ref>. The resulting KG has 1,190,658 triples, 100,813 entities and 1,358 relations. We randomly split 70/15/15% for train/valid/test sets as described in <ref type="bibr" target="#b24">(Moon et al., 2019;</ref><ref type="bibr" target="#b13">Jung et al., 2020)</ref> since they do not release their splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMD-Reasoning</head><p>To make SMD dataset suitable for more precise evaluation of reasoning abilities, we manually label and convert it to the SMD-Reasoning dataset. We first remove the natural language part from the original responses and only leave the action word (e.g., inform) along with the information being conveyed. We divide the dataset into three main reasoning types: informing items, selecting min/max, and evaluating true/false. To validate if the models can identify whether the needed knowledge is in the database, we add a new reasoning type for extracting constraints, by removing the needed knowledge from the database and changing the output to "include [knowledge description]" as shown in Table <ref type="table" target="#tab_0">1</ref>. See Appendix A,B for statistics of these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Metrics</head><p>We use different evaluation methods for the three datasets. For SMD, we follow prior work <ref type="bibr" target="#b42">(Yang et al., 2020)</ref> and use BLEU <ref type="bibr" target="#b25">(Papineni et al., 2002)</ref>, and Entity F1 scores on each domain. For OpenDialKG, we follow the descriptions in prior works <ref type="bibr" target="#b24">(Moon et al., 2019;</ref><ref type="bibr" target="#b13">Jung et al., 2020)</ref> to evaluate the path@k scores, i.e., if the ground-truth path is ranked top-k in the predicted paths probabilities. Moreover, since our method not only can predict the reasoning path as prior works but also can predict the response, we also use the BLEU score to get the approximated evaluation of the response quality compared to ground-truth. Note that prior work has discussed that BLEU scores may not match human intuition <ref type="bibr" target="#b19">(Liu et al., 2016)</ref>, but we use them here as an approximated evaluation for reference.</p><p>For SMD-Reasoning, the output is more deterministic and does not include diverse sentence structures. Therefore, we compute the F1 score and the exact match (EM) score of prediction and the ground-truth. The EM score is calculated by removing the order of the prediction since the labels of SMD-Reasoning dataset follow the order of knowledge description appearing in the original ground-truth responses and may not have the same order as generated outputs. The EM score can be written as:</p><formula xml:id="formula_8">EM = 1 T 1(sort(ŷ) = sort(y)) . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where ŷ is inferred from the model using argmax sampling and T is total number of examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Implementation Details</head><p>Since the proposed method is model-agnostic, we implement it on GPT2 <ref type="bibr" target="#b27">(Radford et al., 2019)</ref> and T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref>. Specifically for the T5 model, we use the unifiedQA-T5 model <ref type="bibr" target="#b15">(Khashabi et al., 2020)</ref> which is pretrained on question answering tasks that also need to do reasoning. However, we empirically find that T5 generally has better performance than GPT2, thus using T5 model in most experiments. For OpenDialKG, since the groundtruth relations exist, we take them as an additional supervision signal as <ref type="bibr" target="#b24">(Moon et al., 2019)</ref>. Also, since we observe that there is only KG reasoning type in OpenDialKG, we do not use operation layer and checkpoints layer for the dataset. The hyperparameter settings are in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Baselines</head><p>We compare our proposed DiffKG model with the state-of-the-art models on OpenDialKG reported in <ref type="bibr" target="#b24">(Moon et al., 2019;</ref><ref type="bibr" target="#b13">Jung et al., 2020)</ref> and the state-of-the-art graph-based model on SMD <ref type="bibr" target="#b42">(Yang et al., 2020;</ref><ref type="bibr" target="#b8">Gou et al., 2021)</ref> with their reported baselines including sequence-tosequence models with and without attention (S2S and S2S+Attn) <ref type="bibr" target="#b21">(Luong et al., 2015)</ref>, pointer to unknown (Ptr-Unk) <ref type="bibr" target="#b9">(Gulcehre et al., 2016)</ref>   <ref type="bibr" target="#b24">(Moon et al., 2019)</ref> and the other three baselines from Seq2Path to AttnIO-AS are reported from <ref type="bibr" target="#b13">(Jung et al., 2020)</ref>. Our DiffKG achieves the highest path@k scores and is the only one that can simultaneously generate responses.</p><p>GraphLSTM <ref type="bibr" target="#b26">(Peng et al., 2017)</ref>, BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, Mem2Seq <ref type="bibr" target="#b22">(Madotto et al., 2018)</ref> and GLMP <ref type="bibr" target="#b41">(Wu et al., 2019)</ref>. We follow their metrics and train our model on their preprocessed data for fair comparisons. To further analyze the reasoning ability, we propose two more baselines based on different ways of leveraging pretrained language models. (1) NoInfo model does not take any format of knowledge as the input, aiming to test the performance of a fine-tuned vanilla transformer model on each dataset.</p><p>(2) FlatInfo model constructs the input by concatenating the dialogue history with the NLD form of knowledge as <ref type="bibr" target="#b0">(Beygi et al., 2022)</ref>, allowing us to investigate the ability of the model to interpret unstructured knowledge. Table <ref type="table">4</ref>: The results on SMD-Reasoning dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Results</head><p>The results on SMD and OpenDialKG are shown in Table <ref type="table" target="#tab_2">2 and Table 3</ref>. On SMD dataset, we observe that DiffKG outperforms the baselines on BLEU by 11.4% (relative change of 16.04 and 14.4) and achieves comparable entity F1 scores with GLMP, GraphDialog and COMET-graph. Dif-fKG might not improve the entity F1 scores because that prior works group the text inside an entity together (e.g., "road block nearby" becomes a single word "road_block_nearby" in vocabularies). In contrast, we use a universal tokenizer so as to prevent heavy preprocessing and specialized vocabularies. This means that DiffKG can perform similarly with state-of-the-art to retrieve knowledge without a tokenizer specified for each dataset. On OpenDialKG dataset, we observe that DiffKG outperforms the baselines in terms of path@k scores and can simultaneously outperform T5 in terms of Entity F1 and BLEU. These demonstrate that DiffKG can retrieve accurate paths for reasoning and effectively incorporate reasoning into response generation.</p><p>We also investigate the results of SMD-Reasoning dataset as shown in Table <ref type="table">4</ref>. We find that DiffKG improves NoInfo by 16.6% and 44.4% F1 scores respectively on GPT2 and T5 models. This demonstrates that DiffKG can utilize knowledge effectively to improve the generation without access to information. In contrast, although FlatInfo gives similar performances as DiffKG on the SMD-Reasoning dataset, it cannot be run on OpenDialKG due to computational costs. More specifically, FlatInfo requires the knowledge graph to be transformed into sentences, which will result in at least a million tokens as the model inputs for OpenDialKG (since the number of triples is a million without designed subgraph sampling), which is not a practical number.  Table <ref type="table">6</ref>: Generated examples and the reasoning path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Quantitative Analysis</head><p>To test the robustness of the methods towards accurately locating information, we shuffle the information order. This evaluation is to simulate the cases that extra information is arbitrarily added when deploying a dialogue system. Specifically, the order of the knowledge context for FlatInfo and the order of knowledge triples are changed during inference time. As shown in the last two rows in Table <ref type="table">4</ref>, the performance of FlatInfo drops while DiffKG remains about the same. This indicates that the slight superior performance of FlatInfo with the original order can come from the blackbox tricks to group the nearby knowledge in the inputs. When this implicit trick is broken down, the DiffKG shows much better robustness and performance.</p><p>To investigate the difficulty of each domain and reasoning type, we divide the results accordingly in Table <ref type="table">5</ref>. As presented in the domains part, the models achieve the highest EM and F1 on the weather domain. We conjecture the reason is that the weather domain includes more reasoning types (weather:4, navigate:3, schedule:2 as in Appendix A Table <ref type="table">9</ref>), thus reflecting more balanced reasoning ability. In the reasoning types part, we observe that true/false is less well coped by Dif-fKG; however, DiffKG improves the extraction. This shows that DiffKG can effectively check the existence of required knowledge and then query the database.</p><p>Regarding to the computational costs (on SMD-Reasoning dataset using T5 model), we found that DiffKG requires about 5.85GB memory during training and has 30ms inference latency. This could be an acceptable add-on memory usage and inference time compared to a model without knowledge reasoning (3.13GB; 30ms). Especially when a baseline like FlatInfo consumes much more (18.56GB; 50ms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Qualitative Analysis</head><p>We visualize the generated examples and the symbolic reasoning path by DiffKG on SMD-Reasoning and OpenDialKG datasets in Table <ref type="table">6</ref>. The examples show that DiffKG can capture some naturally occurring phenomena in this dataset:</p><p>(1) the KG reasoning path can be 1 to multiple hops; (2) the reasoning will diffuse to multiple paths (e.g., DiffKG simultaneously applies "Has-Date","HasTime","ToSelf" to "Doctor Appointment"). Along with analyses in previous subsections, we observe that DiffKG can extract interpretable reasoning paths and generate corresponding outputs using reasonable computational costs.</p><p>However, even though DiffKG can maintain or improve performance while doing interpretable reasoning on any scaled KG, errors might happen in  (Our comment: The reasoning path is correct to find out the script written by Don Hall. However, the generation process fails to properly utilize the retrieved entity.)</p><p>Table <ref type="table">7</ref>: The error analysis with three major error types across datasets. some cases. As listed in Table <ref type="table">7</ref>, we found that across the datasets, the three main error types of DiffKG are: (1) unclear information requirement in the dataset, (2) incomplete reasoning ability but faithful response generation, and (3) correct reasoning but hallucinated response prediction. We argue that the first error type mainly comes from the mismatch among data points in the dataset and may not be able to be dealt with by models. The second error type indicates that the KG reasoning module sometimes cannot retrieve all the needed information. The third error type indicates that the module producing final output may not fully utilize the retrieved information. These three points might provide a direction for further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>For a dialogue system, an effective reasoning method over structured databases is important. In this work, we proposed DiffKG, an end-to-end model-agnostic method that does symbolic reasoning on any scale of KGs to enhance response generation. Experiments demonstrated that using Dif-fKG, models are able to generate responses with interpretable KG reasoning paths at a modest extra cost.</p><p>This work can be extended in various ways. While we solely consider efficient large-scale KG reasoning in dialogue generation, future work can incorporate domain fusion methods to consider the generalizability over domains or simultaneously use relation information. Moreover, since Dif-fKG is a simple large-scale structured knowledgeempowered transformer with flexible entity values, future work can extend it to dialogue generation that needs to do table and text mixed reasoning and that needs to do both KG reasoning and other goals such as personalized dialogues, storytelling, etc. latency is about twice at inference time. The difference in inference latency is even larger with GPT2 as the backbone model. The reason is that the computational cost of a causal language model such as GPT2 largely depends on the input sequence length, which is one of the main issues of FlatInfo.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Generation. Recent works have investigated the grounded dialogue generation. These methods can be divided into three main categories. First, Dinan et al. (2018); Zhao et al. (2019); Tuan et al. (2020); Kim et al. (2020) extract useful knowledge from unstructured data to generate responses, such as information contained in passages and speaker's profiles. Second, Sordoni et al. (2015); Long et al. (2017); Zhu et al. (2017); Ghazvininejad et al. (2018); Zhou et al. (2018); Veličković et al. (2018); Joshi et al. (2020); Hosseini-Asl et al. (2020); Wang et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The illustration of proposed DiffKG, which leverages a pretrained transformer model (T5 or GPT2) and the Reified KG. The model generates the response depending on the predicted relation sequence [r 1 ; ...; r H ], thus being fully interpretable in terms of the used reasoning path.</figDesc><graphic url="image-1.png" coords="4,127.55,70.85,340.17,165.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, and tail matrix M t ∈ IR N T ×N E . An entry (i, e) in M h or M t with value 1 indicates that the i-th triple in the KG has entity e as the head or the tail; an entry (i, r) in M r with value 1 indicates that the i-th triple in the knowledge graph has the relation r. Since often in practical settings most entries in the three matrices are zero, saving them into sparse matrices can significantly reduce memory consumption<ref type="bibr" target="#b2">(Cohen et al., 2019)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>−</head><label></label><figDesc>−−−−−−−−−−−→ Tuesday, 11am, doctor appointment) DiffKG: inform 11 am tuesday doctor appointment User: Car I need to get to a gas station, please show me the nearest one Assistant: There is Valero 7 miles away with moderate traffic on our way User: Alright, where is it located? −−−−−−→ 200 Alester Ave, Valero) DiffKG: inform 200 Alester Ave Valero OpenDialKG Speaker A: Do you have any info on Toni Kroos? (Reasoning Path: Toni Kroos ∼Player Statistics −−−−−−−−→ Germany national football team) DiffKG: Toni Kroos is German footballer who plays for the Germany national football team.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Speaker A: Do you know Don Hall? Ground-truth: Don Hall wrote the Princess and the Frog a romance story starring Jenifer Lewis. Do you like Romance? Reasoning Path: Don Hall ∼written by − −−−−− → The Princess and the Frog DiffKG: Yes, he wrote The Little Dolls.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example of different reasoning types and output formats (semantic and natural language forms) in a dialogue system with the related information in the accessible KGs.</figDesc><table><row><cell>t k ) , where e h k , r k , e t k are</cell></row><row><cell>respectively the head entity, relation, and tail entity.</cell></row><row><cell>The total numbers of triples, entities, and relations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>,</figDesc><table><row><cell>Model</cell><cell>BLEU</cell><cell>All</cell><cell cols="3">Entity F1 Sche. Wea. Nav.</cell></row><row><cell>S2S</cell><cell>8.4</cell><cell>10.3</cell><cell>9.7</cell><cell>14.1</cell><cell>7.0</cell></row><row><cell>S2S+Attn</cell><cell>9.3</cell><cell>19.9</cell><cell>23.4</cell><cell>25.6</cell><cell>10.8</cell></row><row><cell>Ptr-Unk</cell><cell>8.3</cell><cell>22.7</cell><cell>26.9</cell><cell>26.7</cell><cell>14.9</cell></row><row><cell>GraphLSTM</cell><cell>10.3</cell><cell>50.8</cell><cell>69.9</cell><cell>46.6</cell><cell>43.2</cell></row><row><cell>BERT</cell><cell>9.13</cell><cell>49.6</cell><cell>57.4</cell><cell>47.5</cell><cell>46.8</cell></row><row><cell>Mem2Seq</cell><cell>12.6</cell><cell>33.4</cell><cell>49.3</cell><cell>32.8</cell><cell>20.0</cell></row><row><cell>GLMP</cell><cell>12.2</cell><cell>55.1</cell><cell>67.3</cell><cell>54.1</cell><cell>48.4</cell></row><row><cell>GraphDialog</cell><cell>13.7</cell><cell>57.4</cell><cell>71.9</cell><cell>59.7</cell><cell>48.6</cell></row><row><cell>COMET-graph</cell><cell>14.4</cell><cell>56.7</cell><cell>71.6</cell><cell>48.7</cell><cell>50.4</cell></row><row><cell>T5-DiffKG</cell><cell>16.04</cell><cell>56.2</cell><cell>67.2</cell><cell>61.5</cell><cell>46.7</cell></row><row><cell cols="6">Table 2: The results on SMD dataset. S2S, S2S+Attn,</cell></row><row><cell cols="6">Ptr-Unk, GraphLSTM, BERT, Mem2Seq, GLMP,</cell></row><row><cell cols="6">GraphDialog are reported from (Yang et al., 2020) and</cell></row><row><cell cols="6">COMET-graph from (Gou et al., 2021). Our DiffKG</cell></row><row><cell cols="6">achieves the highest BLEU and comparable F1 scores</cell></row><row><cell>with baselines.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="5">path@1 path@5 path@10 BLEU</cell></row><row><cell>Seq2Seq</cell><cell>3.1</cell><cell>29.7</cell><cell>44.1</cell><cell></cell><cell>-</cell></row><row><cell>Tri-LSTM</cell><cell>3.2</cell><cell>22.6</cell><cell>36.3</cell><cell></cell><cell>-</cell></row><row><cell>EXT-ED</cell><cell>1.9</cell><cell>9.0</cell><cell>13.3</cell><cell></cell><cell>-</cell></row><row><cell>DialKG</cell><cell>13.2</cell><cell>35.3</cell><cell>47.9</cell><cell></cell><cell>-</cell></row><row><cell>Seq2Path</cell><cell>14.92</cell><cell>31.1</cell><cell>38.68</cell><cell></cell><cell>-</cell></row><row><cell>AttnFlow</cell><cell>17.37</cell><cell>30.68</cell><cell>39.48</cell><cell></cell><cell>-</cell></row><row><cell>AttnIO-AS</cell><cell>23.72</cell><cell>43.57</cell><cell>52.17</cell><cell></cell><cell>-</cell></row><row><cell>T5-NoInfo</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>14.51</cell></row><row><cell>T5-DiffKG</cell><cell>26.80</cell><cell>54.33</cell><cell>61.75</cell><cell></cell><cell>15.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The results on OpenDialKG dataset. The four baselines from Seq2Seq to DialKG Walker are reported from</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Although this prediction is considered wrong for the EM metric, the "5 miles" of DiffKG output is the correct distance from "Chevron" and might be needed in a good response.) Inform 4 miles, Whole Foods, Safeway. DiffKG: inform 4 miles, grocery store, 819 Alma St, Whole Foods (Our comment: The 4 miles, grocery store, 819 Alma St are all correct entities about Whole Foods. Nonetheless, this reasoning process neglects another grocery store Safeway which is also 4 miles away.)</figDesc><table><row><cell>Error Type</cell><cell>Example</cell></row><row><cell></cell><cell>User: What gas stations are here?</cell></row><row><cell>Unclear Information</cell><cell>Ground-truth: inform Chevron</cell></row><row><cell>Requirement</cell><cell>DiffKG: inform 5 miles, Chevron</cell></row><row><cell cols="2">User: Where is the closest grocery store? (Our comment: Incomplete Reasoning but Faithful Response Ground-truth: Correct Reasoning</cell></row><row><cell>but Wrong Response</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">In our experiments, we compute the maximum length of all entities and pad shorter entities to the length of m.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Details</head><p>The statistics of datasets are in Table <ref type="table">8 and Table 9</ref>. The OpenDialKG dataset is under CC-BY-NC-4.0 license. These datasets can be used for research purposes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SMD KG Construction</head><p>We write a simple, automatic program to construct KGs for SMD dataset mapped from the original annotated tables.</p><p>For the schedule and navigation domains in SMD, we directly map their table attributes to the relations R in our constructed KG. For the weather domain, we split each weather report into low temperature, high temperature, and weather. The resulting number of relations is 29, and the relations are listed in Table <ref type="table">10</ref>.</p><p>In the schedule and navigation domain, each item in the original database with multiple attributes are transformed to KG triples as (event/point-of-interest, attribute, attribute value), e.g., (tennis activity, HasTime, 7pm) in schedule domain or (Chevron, HasType, gas station) in navigation domain.</p><p>In the weather domain, we add additional entities named "ReportID$digits$", where $digits$ will be replaced with an ID number. Each item in the original database is in the format: (item, location, $location), (item, $date, $weather_report), where the $weather_report contains multiple information not simultaneously needed. To make the KG of weather consistent with the KGs of schedule  and navigation, we transform each item into (Re-portID, location, $location), (ReportID, HasDate, $date), (ReportID, HasWeather, $weather), (Repor-tID, HasLowTemp, $low_temparature), (ReportID, HasHighTemp, $high_temparature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiment Details</head><p>The hyperparameters we set for DiffKG are d=the hidden size of the used pretrained trasnformer (T5small: d=512; GPT2:d=768), H=5, max norm=1.0, batch size=16, and gradient accumulation steps=2 for at most 50 epochs and train the model learning rate∈ {5 × 10 −5 , 6.25 × 10 −5 } (found that 6.25 × 10 −5 is better) without learning rate decay.</p><p>Our experiments were single runs with random initialization and were not further fine-tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Computational Cost Analysis</head><p>As plotted in Figure <ref type="figure">2</ref>, on SMD-Reasoning dataset, the consumed memory of FlatInfo is thrice the memory needed for DiffKG at training time, and its</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Sajjad</forename><surname>Beygi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Fazel-Zarandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Cervone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakash</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><forename type="middle">Reddy</forename><surname>Jonnalagadda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04161</idno>
		<title level="m">Logical reasoning for task oriented dialogue systems</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIG-MOD international conference on Management of data</title>
				<meeting>the 2008 ACM SIG-MOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable neural methods for reasoning with a symbolic knowledge base</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><surname>Siegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mate: Multiview attention for table transformer efficiency</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Martin Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maharshi</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04312</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Key-value retrieval networks for task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshmi</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
				<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A knowledge-grounded neural conversation model</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contextualize knowledge bases with transformer for end-to-end task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxu</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An endto-end model for question answering over knowledge base with cross-attention combining global knowledge</title>
		<author>
			<persName><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple language model for task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20179" to="20191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues</title>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidhisha</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attnio: Knowledge graph exploration with in-andout attention flow for knowledge-grounded dialogue</title>
		<author>
			<persName><forename type="first">Jaehun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwon</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Pearson Education International</publisher>
		</imprint>
	</monogr>
	<note>Prentice Hall series in artificial intelligence</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unifiedqa: Crossing format boundaries with a single qa system</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Findings</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Qasc: A dataset for question answering via sentence composition</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Guerquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8082" to="8090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond domain apis: Task-oriented conversational modeling with unstructured knowledge access</title>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Hedayatnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
				<meeting>the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Differentiable open-ended commonsense reasoning</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A knowledge enhanced generative conversational service agent</title>
		<author>
			<persName><forename type="first">Yinong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongsheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Dialog System Technology Challenges (DSTC6) Workshop</title>
				<meeting>the 6th Dialog System Technology Challenges (DSTC6) Workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02789</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs</title>
		<author>
			<persName><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the</title>
				<meeting>the 57th Annual Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cross-sentence nary relation extraction with graph lstms</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</title>
				<meeting>the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter</title>
				<meeting>the 2015 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Shane</forename><surname>Storks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01172</idno>
		<title level="m">Recent advances in natural language inference: A survey of benchmarks, resources, and approaches</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dykgchat: Benchmarking dialogue generation grounding on dynamic knowledge graphs</title>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio</title>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14614</idno>
		<imprint>
			<date type="published" when="2018">2020. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph attention networks. International Conference on Learning Representation</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Pat</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livio</forename><forename type="middle">Baldini</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00849</idno>
		<title level="m">Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Incorporating specific knowledge into end-to-end task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Qingyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A network-based end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Association for Computational Linguistics (EACL)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<title level="m">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The dialog state tracking challenge series: A review</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4" to="33" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Global-to-local memory pointer networks for task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graphdialog: Integrating graph knowledge into endto-end task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Shiquan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural generative question answering</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human-Computer Question Answering</title>
				<meeting>the Workshop on Human-Computer Question Answering</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Low-resource knowledge-grounded dialogue generation</title>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Commonsense knowledge aware conversation generation with graph attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Flexible end-to-end dialogue system for knowledge grounded conversation</title>
		<author>
			<persName><forename type="first">Wenya</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiang</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangbin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezheng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04264</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
