<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-23">23 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
							<email>mikhail.galkin@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
							<email>wujiapen@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Etienne</forename><surname>Denis</surname></persName>
							<email>deniseti@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
							<email>hamilton@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-23">23 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.12144v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector. Such a shallow lookup results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs when working with real-world KGs. Drawing parallels with subword tokenization commonly used in NLP, we explore the landscape of more parameter-efficient node embedding strategies with possibly sublinear memory requirements. To this end, we propose NodePiece, an anchor-based approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of subword/subentity units is constructed from anchor nodes in a graph with known relation types. Given such a fixed-size vocabulary, it is possible to bootstrap an encoding and embedding for any entity, including those unseen during training. Experiments show that NodePiece performs competitively in node classification, link prediction, and relation prediction tasks while retaining less than 10% of explicit nodes in a graph as anchors and often having 10x fewer parameters. 1 We then concentrate on nodes as usually their size is orders of magnitude larger than that of edge types.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representation learning tasks on knowledge graphs (KGs) often require a parameterization of each unique atom in the graph with a vector or matrix. Traditionally, in multi-relational KGs such atoms constitute a set of all nodes n ∈ N (entities) and relations (edge types) r ∈ R <ref type="bibr" target="#b24">[25]</ref>. Assuming parameterization with vectors, atoms are mapped to d-dimensional vectors through shallow encoders f n : n → R d and f r : r → R d which scale linearly to the number of nodes and edge types 1 , i.e., having O(|N |) space complexity of the entity embedding matrix. Albeit efficient on small conventional benchmarking datasets based on Freebase <ref type="bibr" target="#b39">[40]</ref> (~15K nodes) and WordNet <ref type="bibr" target="#b8">[9]</ref> (~40K nodes), training on larger graphs (e.g.,  of 120K nodes) becomes computationally challenging. Scaling it further up to larger subsets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b32">33]</ref> of Wikidata <ref type="bibr" target="#b42">[43]</ref> requires a top-level GPU or a CPU cluster as done in, e.g., PyTorch-BigGraph <ref type="bibr" target="#b18">[19]</ref> that maintains a 78M × 200d embeddings matrix in memory (we list sizes of current best performing models in Table <ref type="table" target="#tab_0">1</ref>).</p><p>Taking the perspective from NLP, shallow node encoding in KGs corresponds to shallow word embedding popularized with word2vec <ref type="bibr" target="#b22">[23]</ref> and GloVe <ref type="bibr" target="#b27">[28]</ref> that learned a vocabulary of 400K-2M most frequent words, treating rarer ones as out-of-vocabulary (OOV). The OOV issue was resolved with the ability to build infinite combinations with a finite vocabulary enabled by subword units. Subword-powered algorithms such as fastText <ref type="bibr" target="#b4">[5]</ref>, Byte-Pair Encoding <ref type="bibr" target="#b36">[37]</ref>, and WordPiece <ref type="bibr" target="#b35">[36]</ref> became a standard step in preprocessing pipelines of large language models and allowed to construct fixed-size token vocabularies, e.g., BERT <ref type="bibr" target="#b9">[10]</ref> contains ~30K tokens and GPT-2 <ref type="bibr" target="#b29">[30]</ref> employs ~50K tokens. Importantly, relatively small input embedding matrices opened a room for investing the parameters budget into more efficient encoders and decoders <ref type="bibr" target="#b16">[17]</ref>.  <ref type="bibr" target="#b37">[38]</ref> 1000 <ref type="bibr" target="#b37">[38]</ref> 1000 <ref type="bibr" target="#b37">[38]</ref> 200 <ref type="bibr" target="#b48">[49]</ref> 512 <ref type="bibr" target="#b45">[46]</ref> 200 <ref type="bibr" target="#b18">[19]</ref> 1024 <ref type="bibr">[</ref> Drawing inspiration from subword embeddings in NLP, we explore how similar strategies for tokenizing entities in large graphs can dramatically reduce parameter complexity, increase generalization, and naturally represent new unseen entities as using the same fixed vocabulary. To do so, tokenization has to rely on atoms akin to subword units and not the total set of nodes.</p><p>To this end, we propose NodePiece, an anchor-based approach to learn a fixed-size vocabulary V (|V | |N |) of any connected multi-relational graph. In NodePiece, the set of atoms consists of anchors and all relation types that, together, allow to construct a combinatorial number of sequences from a limited atoms vocabulary. In contrast to shallow approaches, each node n is first tokenized into a unique hash(n) of k closest anchors and m immediate relations. A key element to build a node embedding is a proper encoder function enc(n) : hash(n) → R d which can be designed leveraging inductive biases of an underlying graph or downstream tasks. Therefore, the overall parameter budget is now defined by a small fixed-size vocabulary of atoms and the complexity of the encoder function.</p><p>Our experimental findings suggest that a fixed-size NodePiece vocabulary paired with a simple encoder still yields competitive results on a variety of tasks including link prediction, node classification, and relation prediction. Furthermore, anchor-based hashing enables conventional embedding models to work in the out-of-sample scenario when unseen entities arrive at inference time, which otherwise required tailored inductive learning mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Conventional KG embedding approaches. To the best of our knowledge, all contemporary embedding algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2]</ref> for link prediction on KGs employ shallow embedding lookups mapping each entity to a unique embedding vector thus being linear O(|N |) to the total number of nodes |N | and size of an embedding matrix. This holds for different embedding families, e.g., translational <ref type="bibr" target="#b37">[38]</ref>, tensor factorization <ref type="bibr" target="#b17">[18]</ref>, convolutional <ref type="bibr" target="#b8">[9]</ref>, and hyperbolic <ref type="bibr" target="#b6">[7]</ref>. The same applies to relation-aware graph neural network (GNN) encoders <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref> who still initialize each node with a learned embedding or feature vector before message passing. Furthermore, shallow encoding is also used in higher-order KG structures such as hypergraphs <ref type="bibr" target="#b10">[11]</ref> and hyper-relational graphs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Distillation and compression. Several recent techniques for reducing memory footprint of embedding matrices follow successful applications of distilling large language models in NLP <ref type="bibr" target="#b33">[34]</ref>. Such techniques include distillation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b49">50]</ref> of high-dimensional KG embedding models into lowdimensional counterparts, and compression of trained matrices into discrete codes <ref type="bibr" target="#b31">[32]</ref>. However, all of them require a full embedding matrix as input which we aim to avoid designing NodePiece.</p><p>Vocabulary reduction in recommender systems. Commonly, recommender systems operate on thousands of categorical features which are combined in sparse high-dimensional feature vectors of existing entities in a vocabulary. Recent approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref> employ anchor-based hashing techniques to factorize sparse feature vectors into dense embeddings. Contrary to those setups, we do not expect availability of feature vectors for arbitrary KGs and rather learn vocabulary embeddings from scratch.</p><p>Out-of-sample representation learning. This task focuses on predictions involving previously unseen, or out-of-sample entities. When an unseen entity with its edges attaches to some of the existing entities in a KG, these new edges are then utilized as contextual information to compute its embedding. Previous work <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1]</ref> proposed different neighborhood aggregation functions for this process. There are also meta-learning approaches that focus on both few-shot link prediction and relation predictions tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b47">48]</ref>. When applied to this setting, our approach is fundamentally different from the existing methods that are still based on shallow entity embedding. Instead of performing aggregation or apply meta-learned networks, we only use the new edges as a basis for anchor-based tokenization of new nodes in terms of existing vocabulary.  : hash(n) → R d can be applied to embed the hash into a d-dimensional vector. An intuition of the approach is presented in Fig. <ref type="figure" target="#fig_0">1</ref> with each step explained in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Anchor Selection</head><p>Subword tokenization algorithms such as BPE <ref type="bibr" target="#b36">[37]</ref> employ deterministic strategies to create tokens and construct a vocabulary, e.g., based on frequencies of co-occurring n-grams, such that more frequent words are tokenized with fewer subword units. On graphs, such strategies might employ centrality measures like degree centrality or Personalized PageRank <ref type="bibr" target="#b25">[26]</ref>. However, in our preliminary experiments, we found random anchor selection to be as effective as centrality-based strategies.</p><p>A choice for deterministic strategies might be justified when optimizing for certain task-specific topological characteristics, e.g., degree and PPR strategies indeed skew the distribution of shortest anchor distances towards smaller values thus increasing chances to find anchors in 2-or 3-hop neighborhood of any node (we provide more evidence for that in Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Node Tokenization</head><p>Once the vocabulary V = A + R is constructed, each node n can be hashed (or tokenized) into a unique hash(n) using 1) k nearest anchors and their discrete distances; 2) m immediate outgoing relations from the relational context of n. Since anchor nodes are concrete nodes in G, they get hashed in the same way as other non-anchor nodes.</p><p>Anchors per node. Given |A| anchor nodes, it is impractical to use all of them for encoding each node. Instead, we select k anchors per node and describe two possible strategies for that, i.e., random and deterministic. The basic random strategy uniformly samples an unordered set of k anchors from A yielding |A| k possible combinations. To avoid collisions when hashing the nodes, |A| and k are to be chosen according to the lower bound on possible combinations that is defined by the total number of nodes, e.g., |A| k ≥ |N |. Note that running depth-first search (DFS) to random anchors at inference time is inefficient and, therefore, hash(n) of the random strategy has to be pre-computed.</p><p>On the other hand, the deterministic strategy selects an ordered sequence of k nearest anchors. Hence, the anchors can be obtained via breadth-first search (BFS) in the l-hop neighborhood of n at inference time (or pre-computed for speed reasons). However, the combinatorial bound is not applicable in this strategy and we need more discriminative signals to avoid hash collisions since nearby nodes will have similar anchors. Such signals have to better ground anchors to the underlying graph structure, and we accomplish that using anchor distances<ref type="foot" target="#foot_0">2</ref> and relational context described below.</p><p>Anchor Distances. Given a target node n and an anchor a i , we define anchor distance z ai ∈ [0; diameter(G)] as an integer denoting the shortest path distance between a i and n in the original graph G. Note that when tokenizing an anchor a j with the deterministic strategy, the nearest anchor among top-k is always a j itself with distance 0. We then map each integer to a learnable d-dimensional vector f z : z ai → R d akin to relative distance encoding scheme.</p><p>Relational Context. We also leverage the multi-relational nature of an underlying KG. Commonly<ref type="foot" target="#foot_1">3</ref> , the amount of unique edge types in G is orders of magnitude smaller than the total number of nodes, i.e., |R| |N |. This fact allows to include the entire |R| in the NodePiece vocabulary V NP and further featurize each node with a unique relational context. We construct a relational context of a node n by randomly sampling a set of m immediate unique outgoing relations starting from n, i.e., rcon n = {r j } m ⊆ N r (n) where N r (n) denotes all outgoing relation types. Due to a non-uniform degree distribution, if |N r (n)| &lt; m, we add auxiliary [PAD] tokens to complete rcon n to size m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding</head><p>At this step, a node n is tokenized into a sequence of k anchors, their k respective distances, and relational context of size m:</p><formula xml:id="formula_0">hash(n) = {a i } k , {z ai } k , {r j } m<label>(1)</label></formula><p>Taking anchors vectors a n and relation vectors r n from the learnable NodePiece vocabulary V ∈ R |V |×d , and anchor distances z av from Z ∈ R (diameter (G)+1)×d , we obtain a vectorized hash:</p><formula xml:id="formula_1">hash(n) = a n + z an , r n = ân , r n ∈ R (k+m)×d<label>(2)</label></formula><p>Although other operations are certainly possible, in this work, we use anchor distances as positional encodings of corresponding anchors and sum up their representations that helps to maintain the overall hash dimension of (k + m) × d.</p><p>Finally, an injective encoder function enc : R (k+m)×d → R d is applied to the vectorized hash to bootstrap an embedding of n. In our experiments, we probe two basic encoders: 1) MLP that takes as input a concatenated hash vector R 1×(k+m)d projecting it down to R d ; 2) Transformer encoder <ref type="bibr" target="#b41">[42]</ref> with average pooling that takes as input an original sequence R (k+m)×d . While MLP is faster and better scales to graphs with more edges, Transformer is slower but requires less trainable parameters.</p><p>As the two encoders were chosen to illustrate the general applicability of the whole approach, we leave a study of even more efficient and effective encoders for future work.</p><p>While the nearest-neighbor hashing function has a greater number of collisions, its non-arbitrary mapping means that it is effectively permutation invariant. We show this in Proposition 1 through the framework of Janossy pooling and permutation sampling based SGD, π-SGD <ref type="bibr" target="#b23">[24]</ref>. A proof is provided in Appendix F.</p><p>Proposition 1. The nearest-anchor encoder with |A| k anchors and |m| subsampled relations, can be considered a π-SGD approximation of (k + |m|)-ary Janossy pooling with a canonical ordering induced by the anchor distances. Janossy pooling with π-SGD can be used to learn a permutation-invariant function from a broad class of permutation-sensitve functions such as MLPs <ref type="bibr" target="#b23">[24]</ref>. The permutation-invariant nature of the nearest-neighbor encoding scheme combined with the lack of transductive features such as node-specific embeddings mean that NodePiece can be used for inductive learning tasks as well.</p><p>With a fixed-size vocabulary V NP , the overall complexity and parameter budget of downstream models are largely defined by the complexity of the encoder and its inductive biases. By design, the NodePiece smaller vocabulary -larger encoder framework is similar to various Transformer-based language models <ref type="bibr" target="#b28">[29]</ref> whose vocabulary size remains rather stable with the encoder being the most important part responsible for the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We design the experimental program not seeking to outperform the best existing approaches but to show the versatility of NodePiece on a variety of KG-related tasks: transductive and out-of-sample link prediction, node classification, and relation prediction. With this desiderata, we formulate the following research questions: RQ 1) Is it necessary to map each node to a unique vector for an acceptable performance on KG tasks?; RQ 2) What is the effect of hashing features?; RQ 3) Is there an optimal number of anchors per node, after which diminishing returns hit the performance?</p><p>NodePiece is implemented<ref type="foot" target="#foot_2">4</ref> in Python using igraph library (licensed under GNU GPL 2) for computing centrality measures and perform basic tokenization. Downstream tasks employ NodePiece in conjunction with PyTorch <ref type="bibr" target="#b26">[27]</ref> (BSD-style license), PyKEEN <ref type="bibr" target="#b2">[3]</ref> (MIT License), and PyTorch-Geometric <ref type="bibr" target="#b11">[12]</ref> (MIT License). We ran experiments on a machine with one RTX 8000 GPU and 64 GB RAM. All used datasets (Table <ref type="table" target="#tab_3">2</ref>) are available under open licenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transductive Link Prediction</head><p>Setup. We run experiments on four KGs of different sizes (see statistics in Table <ref type="table" target="#tab_3">2</ref>) varying the total number of nodes from ~15K to ~120K. As a baseline, we compare to RotatE <ref type="bibr" target="#b37">[38]</ref> that remains one of state-of-the-art shallow embedding models for transductive link prediction tasks. To balance with NodePiece, RotatE operates on a graph with added inverse edges and inverse relation types as well.</p><p>We report MRR with Hits@10 in the filtered <ref type="bibr" target="#b5">[6]</ref> setting as evaluation metrics, count parameters for all models. On larger KGs, we also compare to a smaller RotatE that has a similar parameter budget.</p><p>In this task, NodePiece is equipped with a 2-layer MLP encoder. For a fair comparison, we also adopt the RotatE scoring function as a link prediction decoder. We train NodePiece + RotatE models in 1-N regime with BCE loss on FB15k-237 and CoDEx-Large, and in the negative sampling self-adversarial mode with margin loss on WN18RR and YAGO 3-10. As to the NodePiece configuration, we generally keep the number of anchors below 10% of total nodes in respective graphs. We select 1k/20 for FB15k-237 (i.e., total 1000 anchors on a 15k nodes graph and 20 anchors per tokenized node) with 15 unique outgoing relations in the relational context; 500/50 with 4 relations for WN18RR; 7k/20 with 6 relations for CoDEx-L; 10k/20 with 5 relations for YAGO 3-10. The rest of the hyperparameters are listed in Appendix A. Discussion. Generally, the results suggest that a fixed-size NodePiece vocabulary of &lt;10% of nodes sustains 80-90% of Hits@10 compared to 10x larger best shallow models. On smaller graphs (Table <ref type="table" target="#tab_4">3</ref>), the parameter saving might not be well pronounced due to the overall small number of nodes to embed. Still, taking even as few as 500 explicit nodes as anchors on WN18RR retains 90% of the best model performance. On bigger graphs (Table <ref type="table" target="#tab_5">4</ref>), parameter efficiency is more pronounced, i.e., on YAGO 3-10, a RotatE model of comparable size is 20 Hits@10 points worse than a NodePiece-based one. This observation can be attributed to the fact the shrinking shallow models results in shrinking the embedding dimension of each node (20d for RotatE) which is inefficient on small parameter budgets. In contrast, a small fixed-size vocabulary allows for larger anchor embedding dimensions (100d for NodePiece with RotatE) since most of the parameter budget is defined by the encoder.</p><p>We further study the effect of different anchor selection combinations (Fig. <ref type="figure" target="#fig_1">2</ref>). On WN18RR, fewer anchors with fewer anchors per node (|A|/k) yield relatively low accuracy but starting from 50/20 (~0.1% of 40k nodes in the graph) the Hits@10 performance starts to saturate. On FB15k-237, even as few as 25 anchors, i.e., 25/20 configuration, already exhibit the signs of saturation where a further increase to 500 or 1000 anchors only marginally improves the performance. We hypothesize such a difference can be explained by graph density, e.g., WN18RR is a sparse graph with a diameter of 23  However, more precise predictions (e.g., Hits@1) reflected in the MRR metric (see Appendix C) still remain a challenging task for small vocabulary NodePiece setups, and bigger |A|/k combinations alleviate this issue. We also observe that diminishing returns, which make further vocabulary increase less rewarding, start to appear from anchor set sizes of ~1% of total nodes.</p><p>Ablations. In the ablation study, we measure the impact of relational context and anchor distances on link prediction (Table <ref type="table" target="#tab_4">3</ref>). Removing relational context and anchor distances does not tangibly affect the denser FB15k-237 data but does impair the accuracy on the sparser WN18RR. Pushing vocabulary sizes to the limit, we also investigate NodePiece behavior in the absence of anchors at all, i.e., when hashes are defined only by the relational context of size m. Interestingly, removing anchors completely still yields fair performance on FB15k-237 with just 7 points Hits@10 drop, but drops to zero the WN18RR performance. The fact that node embeddings might not be at all necessary but relation types are more important (than considered before) supports the recent findings of <ref type="bibr" target="#b38">[39]</ref> that based its reasoning process only on relations seen in a small subgraph around a target node. However, at this point, it seems to be a virtue of graphs with a diverse set of unique relations. That is, FB15k-237 has 20x more unique relations than WN18RR and, therefore, resulting hashes of the relational context have more diverse and unique combinations of relation types which also lead to more discriminative encoded node representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relation Prediction</head><p>Setup. We conduct the relation prediction experiment on the same FB15k-237, WN18RR, and YAGO 3-10 datasets. While link prediction deals with entities, the relation prediction model has to rank a correct relation given a (head, ?, tail) query. We report MRR and Hits@10 in the filtered setting as evaluation metrics here as well. Similar to the link prediction configuration, we use NodePiece + 2-layer MLP and compare against RotatE of the same total parameter count.</p><p>Discussion. The reported results (Table <ref type="table" target="#tab_6">5</ref>) demonstrate a competitive performance of NodePiecebased models with reduced vocabulary sizes bringing more than 97% Hits@10 across graphs of different sizes. In the case of WN18RR and YAGO 3-10, NodePiece models with fewer anchors even slightly improve the accuracy upon the shallow embedding baseline. The ablation study suggests that on dense graphs with a reasonable amount of unique relations having explicit learnable node embeddings might not be needed at all for this task. That is, we see that on FB15k-237 and YAGO 3-10 the NodePiece hashes comprised only of the relational context deliver the same performance without any performance drop confirming the findings from the previous experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Node Classification</head><p>Setup. Due to the lack of established node classification datasets on multi-relational KGs, we design a multi-class multi-label task based on a triple version of a recent WD50K <ref type="bibr" target="#b12">[13]</ref> extracted from Wikidata. The pre-processing steps are described in Appendix D, and the final graph consists of 46K nodes and 222K edges. The task belongs to the family of transductive (the whole graph is seen during training) semi-supervised (only a fraction of nodes are labeled) problems, where labels are 465 classes as seen in Wikidata. In a semi-supervised mode, we test the models on a graph with 5% and 10% of labeled nodes. Node features are not given and have to be learned as node embeddings. As baselines, we compare to a 2-layer MLP and CompGCN <ref type="bibr" target="#b40">[41]</ref> in a full-batch mode which is one of the strongest GNN encoders for multi-relational KGs. Both baselines learn a full entity and relation vocabulary. We report ROC-AUC, PRC-AUC, and Hard Accuracy metrics as commonly done in standard graph benchmarks like OGB <ref type="bibr" target="#b14">[15]</ref>. For PRC-AUC and Hard Accuracy, we binarize predicted logits using a threshold of 0.5. Hard Accuracy corresponds to the task where a predicted sparse 465-dimensional vector is exactly equal to a sparse 465-dimensional labels vector.</p><p>NodePiece is configured to have only 50 anchors and use 10 nearest anchors per node with 5 unique relations in the relational context. The dimensionality of anchors and relations is the same as in the baseline CompGCN. Each epoch, we first materialize all entity embeddings through the NodePiece encoder and then send the materialized matrix to CompGCN with the class predictor.</p><p>Discussion. Surprisingly, ~1000x node vocabulary reduction ratio (50 anchors against 46k for shallow models) greatly outperforms the baselines (Table <ref type="table" target="#tab_7">6</ref>). MLP, as expected, is not able to cope with the task producing random predictions. CompGCN, in turn, outperforms MLP demonstrating nonrandom outputs as seen by the ROC-AUC score of 0.836 and higher PRC-AUC and Hard Accuracy metrics. Still, a NodePiece-equipped CompGCN with 50 anchors reaches even higher ROC-AUC of 0.98 with considerable improvements along other metrics, i.e., +16-19 PRC-AUC points and 3x boost along the hardest accuracy metric. We attribute such a noticeable performance difference to better generalization capabilities of the NodePiece model. That is, a generalization gap between training and validation metrics of the NodePiece + CompGCN is much smaller compared to the baselines who overfit rather heavily (we show the training curves in the Appendix E). The effect remains after increasing the number of labeled nodes to 10%. Even with 50 anchors, the overall performance is saturated as the further increase of the vocabulary size did not bring any improvements.</p><p>Ablations. We probe setups where NodePiece hashes use only anchors or only relational context, and find they both deliver a similar performance. Following the previous experiments on dense graphs with lots of unique relations, it appears that node classification can be performed rather accurately based only on the node relational context which is captured by NodePiece hashes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Out-of-sample Link Prediction</head><p>Setup. In the out-of-sample setup, validation and test splits contain unseen entities that arrive with a few edges connected to the seen nodes. For this experiment, we adopt the out-of-sample FB15k-237 split (oFB15k-237) as designed in <ref type="bibr" target="#b0">[1]</ref>. We do not employ their out-of-sample version of WN18RR as the split contains too many disconnected entities and disconnected components in the train graph. Instead, using the authors script, we sample a much bigger out-of-sample version of YAGO 3-10.</p><p>Conventional link prediction models like the baseline RotatE are not able to work in inductive setups, e.g., out-of-sample cases, as new entities arriving at inference time do not have trained embeddings nor node features. As a baseline, we compare to oDistMult proposed in <ref type="bibr" target="#b0">[1]</ref> which aggregates embeddings of all seen nodes in a neighborhood of a new unseen one. We also adopt the same evaluation protocol -given an unseen node with its connecting edges, we mask one of the edges and predict its tail or head using the rest of the edges, and repeat this procedure for each edge. We report MRR and Hits@10 in the filtered setting as main metrics.</p><p>NodePiece enables traditional transductive-only models to perform inductive inference as both seen and unseen nodes are tokenized using the same vocabulary. For a smaller oFB15k-237 the NodePiece vocabulary has 1000 anchors using 20 nearest per node with 15 unique relations in the node context, Discussion. The results in Table <ref type="table" target="#tab_8">7</ref> show that a simple NodePiece-based model retains ~90% of the baseline performance on oFB15k-237, but achieved faster and computationally inexpensive compared to oDistMult. Moreover, while oDistMult is tailored specifically for the out-of-sample task, we did not do any task-specific modifications to the NodePiece-enabled model as it is inductive by design. Furthermore, oDistMult is not able to scale to a bigger oYAGO 3-10 split even on a 256 GB RAM machine due to the out of memory crash.</p><p>Conversely, a NodePiece-equipped model has the same computational requirements as in other tasks and converges rather quickly (40 epochs). The experiment shows that the NodePiece hashing strategy can be effectively used in inductive scenarios on various KGs with a fixed-size node vocabulary. Performed ablations underline the importance of having both anchors and relational context for tokenizing unseen entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations and Future Work</head><p>Our experimental results demonstrate the promise of using NodePeice to significantly reduce the parameter complexity of node embeddings. While it is difficult to prove, we also hypothesize that the parameters required by NodePeice increase sublinearly according to the size of the graph. The intuition for this is twofold. First, the number of unique anchor combinations of size k that can be encoded increases according to |A| k (i.e. O(|A| k )) if randomly sampled -if the sampling is done via nearest neighbor anchor selection then the number of unique permutations is expected to increase polynomially. Second, increasing the size of the graph will only require sublinear increase in the number of anchors in order to maintain the same average node-anchor distance. Although proving causality is difficult, we believe that maintaining hashing uniqueness and node-anchor distances stable will be sufficient to maintain equivalent performance.</p><p>As currently implemented, the calculation of the distances from each anchor to each node might present a computational bottleneck on graphs with 1M+ nodes, which we aim to resolve in the future work. This will allow to perform even more qualitative experiments on million node-scale graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Societal Impact</head><p>In this paper, we have introduced NodePiece, a compositional approach for representing nodes in multi-relational graphs with a fixed-size vocabulary. Similar to subword units, NodePiece allows to tokenize every node as a combination of anchors and relations where the number of anchors can be 10-100× smaller than the total number of nodes. We show that in some tasks, node embeddings are not even necessary for getting an acceptable accuracy thanks to a rich set of relation types. Moreover, NodePiece is inductive by design and is able to tokenize unseen entities and perform downstream prediction tasks in the same fashion as on seen ones.</p><p>We foresee no immediate societal consequences of this work since node embedding methods are generally available and already used on very large graphs. Current methods require significant resources to scale to large graphs making them financially and environmentally impractical. NodePeice has the potential to make such systems possible for a fraction of the current financial and energy footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Transductive Link Prediction</head><p>The optimizer is Adam for all experiments. As RotatE is a scoring function in the complex space, the reported embedding dimensions are a sum of real and imaginary dimensions, e.g., 1000d means that both real and imaginary vectors are 500d. Configurations (Table <ref type="table" target="#tab_12">10</ref>) for the compared models are almost identical to those of the transductive link prediction experiment. We mostly reduce the number of epochs and negative samples as models converge faster on this task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Node Classification</head><p>In this experiment, NodePiece is used at the initial step to bootstrap a node embeddings matrix which is then sent to the CompGCN graph encoder. In contrast, CompGCN and MLP baselines use directly a trained node embedding matrix as their initial input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Out-of-sample Link Prediction</head><p>The set of NodePiece hyperparameters is similar to the set of the transductive experiments except the scoring function (DistMult), encoder function (Transformer), and number of epochs as the model converges faster. We do not provide a setup for the baseline oDistMult on oYAGO 3-10 as the model was not able to pre-process the dataset on a machine with 256 GB RAM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Anchor Selection Strategies</head><p>Here, we provide more details as to anchor configurations (k nearest from total A anchors) and anchor distances. Recall that there exist several ways to select the total set of anchors A as stated in Section 3.1, i.e., random or centrality-measure based. Then, k anchors per node can be chosen either as k nearest (default NodePiece mode) or k random anchors. Figure <ref type="figure" target="#fig_2">3</ref> depicts the effect of those strategies on the distribution of anchor distances (number of hops between a target node and its anchors). We use the configurations used in the main experiments, i.e., 1000 anchors and 20 anchors per node for FB15k-237, and 500 anchors with 50 anchors per node on WN18RR.</p><p>First, we observe that PPR, degree, and mixed (40% PPR, 40% degree, 20% random) strategies generally skew the distribution towards smaller anchor distances compared to random strategies. This fact supports the hypothesis that deterministic strategies improve the chances to find an anchor in a closer l-hop neighborhood of a target node. Second, varying the way of selecting k anchors per node between nearest (left column) and random (right column), we also observe the skew of a distribution of anchor distances.</p><p>Next, we fix the anchor selection strategy to the mix, fix the number of anchors per node (50 for WN18RR and 20 for FB15k-237), and vary a total number of anchors A (50 to 1000 for WN18RR and 20 to 1000 for FB15k-237) along with the method of sampling k anchors per node, i.e., nearest and random. Figure <ref type="figure" target="#fig_3">4</ref> shows that increasing the total number of anchors together with k nearest anchors again skews the distribution of anchor distances towards smaller values and, hence, to higher probabilities of finding anchors in a closer neighborhood of a target node.</p><p>We would recommend using centrality-based strategies to select A with k nearest anchors per node if anchor distances and probability of finding anchors in a closer neighborhood are of higher importance.</p><p>Finally, we fix the anchor selection strategy as mix, obtain nearest anchors per node, and under this setup study average anchor distances varying k -the number of anchors per node in various combinations of total anchors A. The results presented on Figure <ref type="figure">5</ref> suggest that sparser graphs (like WN18RR) benefit more from increasing the number of anchors A, i.e., the delta between distances is much larger than that of dense FB15k-237. The difference in distances on Figure <ref type="figure">5</ref> might also explain the performance on Figure <ref type="figure" target="#fig_4">6</ref>, i.e., generally, smaller A/k configurations like 25/5 are inferior on sparser graphs but perform competitively on denser ones. Figure <ref type="figure">5</ref>: Average node-anchor distances when varying the total number of anchors A from 25 to 1000 and k nearest anchors per node from 5 to 50. Note that on a sparser WN18RR the gap between min and max values is much wider than of denser FB15k-237. Signs of saturation suggest that further increasing A is not beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Transductive Link Prediction Results: MRR</head><p>In addition to Figure <ref type="figure" target="#fig_1">2</ref> that presents Hits@10, we report variations of mean reciprocal rank (MRR) depending on combinations of A and k on Figure <ref type="figure" target="#fig_4">6</ref> from the same set of experiments. On sparser WN18RR, smaller A/k combinations like 25/5 or 50/10 struggle with more precise predictions like Hits@1 which is captured by low values of MRR. Starting from 500/10, the WN18RR performance starts to saturate. On the other hand, on denser FB15k-237, the difference between minimum and maximum MRR is less than 4 points, and performance exhibits signs of saturation already at 50/10. Note that performance gap on FB15k-237 is very small indicating that saturation has occurred already with small anchor configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Datasets Construction D.1 Node Classification: WD50K NC</head><p>The original WD50K <ref type="bibr" target="#b12">[13]</ref> contains a triple-only KG version on which we base a new dataset for semisupervised multi-class multi-label node classification. First, we remove all triples containing Wikidata properties P31 (instance of ) and P279 (subclass of ) as they already contain class information. We then remove nodes that became disconnected after removing those edges. Third, using SPARQL queries, for each remaining node in a graph, we extract a 3-hop class hierarchy of Wikidata classes and their superclasses. We only keep class labels that occur at least 50 times in the training set. Then, we sample 10% of nodes with labels for validation and 10% for test, and of remaining 80% we sample a set of nodes for the semi-supervised setup, i.e., we keep only 5% and 10% of those nodes. The resulting graph has 46k nodes, 526 distinct relation types, and 465 class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Ouf-of-sample Link Prediction: oYAGO 3-10</head><p>For sampling the out-of-sample version of a bigger YAGO 3-10 we largely follow the same original procedure described in Section 4 of <ref type="bibr" target="#b0">[1]</ref>. We first merge the train, validation and test triples from the original dataset for transductive link prediction. Then, from all entities appearing in at least two triples, we randomly sample 5% of nodes to be the out-of-sample entities for validation and 5% for test. All triples containing the out-of-sample entities on subject or object positions are put into validation or test, respectively, as edges that connect an unseen entity with the seen graph. The MLP baseline quickly overfits but fails to generalize on the validation. The generalization gap of CompGCN is smaller compared to MLP but is still significant, i.e., validation performance is 2-3× smaller than train. Finally, the NodePiece-enabled model has the smallest generalization gaps, especially along the Hard Accuracy metric where the validation performance is very close to that of train. Similarly, the gap on PRC-AUC is smaller than 10 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Node Classification: Training Curves</head><p>As shown in the ablation study in Table <ref type="table" target="#tab_7">6</ref>, it appears that explicit node embeddings do not contribute to the classification performance. Hence, the baseline models tend to be overparameterized where learnable node embeddings add noise, while the NodePiece model has only a few anchors (or no anchors at all when using only the relational context), much fewer parameters, and therefore generalizes better. This hypothesis also explains the observation that the node classification performance does not improve when increasing A/k anchor configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Proofs</head><p>Proposition 2. The nearest-anchor encoder with |A| k anchors and |m| subsampled relations, can be considered a π-SGD approximation of (k + |m|)-ary Janossy pooling with a canonical ordering induced by the anchor distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head><p>We begin by providing the definition of Janossy pooling as it was presented in the original paper <ref type="bibr" target="#b23">[24]</ref>. Definition 1 (Janossy pooling). Let H ∪ be the union of all anchors and relations. Consider a function f : N × H ∪ × R d → F on variable-length but finite sequences h, parameterized by θ (f ) ∈ R d , d &gt; 0. A permutation-invariant function f : N × H ∪ × R d → F is the Janossy function associated with f if</p><formula xml:id="formula_2">f (|h|, h; θ (f ) ) = 1 |h|! π∈Π |h| f (|h|, h π ; θ (f ) ),<label>(3)</label></formula><p>where Π |h| is the set of all permutations of the integers 1 to |h|, and h π represents a particular reordering of the elements of sequence h according to π ∈ Π |h| . We refer the operation used to construct f from f as Janossy pooling.</p><p>While Janossy pooling provides a simple approach to construct permutation-invariant functions from arbitrary permutation sensitive functions, it is computationally intractable due to the need to sum over all computations. Three general strategies proposed under this framework to overcome this combinatorial challenge: canonical orderings, k-ary Janossy pooling, and π-SGD approximations.</p><p>A very effective way of reducing the complexity is to constrain the permutations to a canonical ordering that is independent of a specific adjacency matrix ordering over a given graph. More precisely, one defines as a function CANONICAL : H ∪ → H ∪ such that CANONICAL(h) = CANONICAL(h π )∀π ∈ Π |h| and only considers functions f based on the composition f = CANONICAL • f <ref type="bibr" target="#b23">[24]</ref>. In the case of NodePiece we are able to define this ordering for the anchors according to their distance to the target node. Assuming that the number of relations is fixed or grows at slow rate throughout the life-cycle of a graph we can define an arbitrary ordering for relations as a canonical ordering for the relational context. However, since anchors can be equidistant such a canonical ordering does fully satisfy permutation invariance. We propose a trivial relaxation of the original definition of canonical orderings simply requiring that an ordering greatly reduce the number of unique permutations since in practice an exact canonical ordering is rarely feasible. Specifically, |{CANONICAL(h π )∀π ∈ Π |h| }| |{(h π )∀π ∈ Π |h| }|.</p><p>To further reduce the number of permutations we can truncate our ordered sequence h. This is known as k-ary Janossy pooling pooling (Definition 2) and is implicitly performed by the NodePeice algorithm by varying the anchor per node parameter, k, and the size of the relational context, |m|. Definition 2 (k-ary Janossy pooling). Fix k ∈ N. For any sequence h, define ↓ k (h) as its projection to a length k sequence; in particular, if |h| ≥ k, we keep the first k elements. Then, a k-ary permutation-invariant Janossy function f is given by</p><formula xml:id="formula_3">f (|h|, h; θ (f ) ) = 1 |h|! π∈Π |h| f (|h|, ↓ k (h π ); θ (f ) ).<label>(4)</label></formula><p>Since an imperfect truncated canonical ordering may still result in a potentially intractable number of permutations, we use permutation sampling also known as π-SGD to learn arbitrary functions that approximate (k + |m|)-ary Janossy pooling. This is done by randomly ordering anchors that are equidistant resulting in a uniform sampling of possible permutations during training and evaluation.</p><p>For more details on the formal definition of π-SGD we point the reader to the original paper <ref type="bibr" target="#b23">[24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: NodePiece tokenization strategy. Given three anchors a 1 , a 2 , a 3 , a target node can be tokenized into a hash of top-k closest anchors, their distances to the target node, and the relational context of outgoing relations from the target node. This hash sequence is passed through an injective encoder to obtain a unique embedding. Inverse relations are added to ensure connectivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Combinations of total anchors A and anchors per node. Denser FB15k-237 saturates faster on smaller A while sparse WN18RR saturates at around 500 anchors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of anchor distances under various anchor selection strategies. Top-bottom: mixed, degree-based, PPR-based, random. For each dataset, left: selecting k nearest anchors, right: k random anchors. (a) Selecting a fixed 500/50 configuration on WN18RR; (b) Selecting a fixed 1000/20 configuration on FB15k237. Generally, all strategies except random ones skew the distributions towards nearest anchors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Distribution of anchor distances under the fixed mix anchor selection strategy when varying the total number of anchors A (50-1000 for WN18RR, 20-1000 for Fb15k-237). For each dataset, left column -k nearest anchors, right -k random anchors. On both graphs, increase in A with the nearest anchors always leads to shorter anchor distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Combinations of total anchors A and anchors per node. Denser FB15k-237 saturates faster on smaller A while sparse WN18RR saturates at around 500 anchors. MRR metric captures all ranks.Note that performance gap on FB15k-237 is very small indicating that saturation has occurred already with small anchor configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>Figure 7 depicts train and validation values of Hard Accuracy and PRC-AUC metrics for all the compared models on WD50K NC with 5% of labeled nodes. The NodePiece model has only 50 total anchors with 10 nearest anchors per node, and 5 unique relation types in the relational context. The performance on the dataset with 10% of nodes is almost the same, so we report the charts only on 5% dataset. By the generalization gap we understand the delta between training and validation values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Generalization gap on WD50K (5% labeled nodes). NodePiece-bsed model has observably smaller generalization gaps compared to the baselines.</figDesc><graphic url="image-1.png" coords="19,99.35,52.76,221.91,166.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Node embedding sizes of state-of-the-art KG embedding models compared to BERT Large. Parameters of type float32 take 4 bytes each.</figDesc><table><row><cell></cell><cell cols="7">FB15k-237 WN18RR YAGO3-10 OGB WikiKG2 Wikidata 5M PBG Wikidata BERT Large</cell></row><row><cell>Vocabulary size</cell><cell>15k</cell><cell>40k</cell><cell>120k</cell><cell>2.5M</cell><cell>5M</cell><cell>78M</cell><cell>30k</cell></row><row><cell>Embedding dim</cell><cell>2000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics. LP -link prediction, RP -relation prediction, NC -node classification, OOS -out-of-sample. In OOS-LP, Nodes also shows the amount of unseen nodes in validation/test.</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell>Nodes</cell><cell>Relations</cell><cell>Edges</cell><cell>Train</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>FB15k-237 [40]</cell><cell>LP, RP</cell><cell>14,505</cell><cell>237</cell><cell>310,079</cell><cell>272,115</cell><cell>17,526</cell><cell>20,438</cell></row><row><cell>WN18RR [9]</cell><cell>LP, RP</cell><cell>40,559</cell><cell>11</cell><cell>92,583</cell><cell>86,835</cell><cell>2824</cell><cell>2924</cell></row><row><cell>CoDEx-Large [33]</cell><cell>LP</cell><cell>77,951</cell><cell>69</cell><cell>612,437</cell><cell>551,193</cell><cell>30,622</cell><cell>30,622</cell></row><row><cell>YAGO 3-10 [21]</cell><cell>LP, RP</cell><cell>123,143</cell><cell>37</cell><cell cols="2">1,089,000 1,079,040</cell><cell>4978</cell><cell>4982</cell></row><row><cell>WD50K</cell><cell>NC</cell><cell>46,164</cell><cell>526</cell><cell>222,563</cell><cell>4600 (N)</cell><cell cols="2">4600 (N) 4600 (N)</cell></row><row><cell>oFB15k-237 [1]</cell><cell cols="2">OOS-LP 11k/1395/1395</cell><cell>234</cell><cell>292,173</cell><cell>193,490</cell><cell>44,601</cell><cell>54,082</cell></row><row><cell>oYAGO 3-10</cell><cell cols="2">OOS-LP 117k/2960/2959</cell><cell>37</cell><cell>1,086,416</cell><cell>988,124</cell><cell>47,112</cell><cell>51,180</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Transductive link prediction on smaller KGs. † results taken from<ref type="bibr" target="#b37">[38]</ref>. |V | denotes vocabulary size (anchors + relations), #P is a total parameter count (millions). % denotes the Hits@10 ratio based on the strongest model.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell><cell></cell></row><row><cell></cell><cell>|V |</cell><cell cols="3">#P (M) MRR H@10</cell><cell>%</cell><cell>|V |</cell><cell cols="3">#P (M) MRR H@10</cell><cell>%</cell></row><row><cell>RotatE</cell><cell>15k + 0.5k</cell><cell>29</cell><cell cols="4">0.338  † 0.533  † 100 40k + 22</cell><cell>41</cell><cell cols="3">0.476  † 0.571  † 100</cell></row><row><cell>NodePiece + RotatE</cell><cell>1k + 0.5k</cell><cell>3.2</cell><cell>0.256</cell><cell>0.420</cell><cell cols="2">79 500 + 22</cell><cell>4.4</cell><cell>0.403</cell><cell>0.515</cell><cell>90</cell></row><row><cell cols="2">-no rel. context 1k + 0.5k</cell><cell>2</cell><cell>0.258</cell><cell>0.425</cell><cell cols="2">80 500 + 22</cell><cell>4.2</cell><cell>0.266</cell><cell>0.465</cell><cell>81</cell></row><row><cell cols="2">-no distances 1k + 0.5k</cell><cell>3.2</cell><cell>0.254</cell><cell>0.421</cell><cell cols="2">79 500 + 22</cell><cell>4.4</cell><cell>0.391</cell><cell>0.510</cell><cell>89</cell></row><row><cell>-no anchors, rels only</cell><cell>0 + 0.5k</cell><cell>1.4</cell><cell>0.204</cell><cell>0.355</cell><cell>67</cell><cell>0 + 22</cell><cell>0.3</cell><cell>0.011</cell><cell>0.019</cell><cell>0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Transductive link prediction on bigger KGs. The same denotation as in Table3. Second RotatE has a similar parameter budget as a NodePiece-based model.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CoDEx-L</cell><cell></cell><cell></cell><cell cols="2">YAGO 3-10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>|V |</cell><cell cols="2">#P (M) MRR H@10</cell><cell>%</cell><cell>|V |</cell><cell cols="3">#P (M) MRR H@10</cell><cell>%</cell></row><row><cell>RotatE (500d)</cell><cell>77k + 138</cell><cell>77</cell><cell cols="3">0.258 0.387 100 123k + 74</cell><cell>123</cell><cell cols="3">0.495  † 0.670  † 100</cell></row><row><cell>RotatE</cell><cell>77k + 138</cell><cell>3.8</cell><cell>0.196 0.322</cell><cell cols="2">83 123k + 74</cell><cell>4.8</cell><cell>0.121</cell><cell>0.262</cell><cell>39</cell></row><row><cell>NodePiece + RotatE</cell><cell>7k + 138</cell><cell>3.6</cell><cell>0.190 0.313</cell><cell>81</cell><cell>10k + 74</cell><cell>4.1</cell><cell>0.247</cell><cell>0.488</cell><cell>73</cell></row><row><cell cols="2">-no rel. context 7k + 138</cell><cell>3.1</cell><cell>0.201 0.332</cell><cell>86</cell><cell>10k + 74</cell><cell>3.7</cell><cell>0.249</cell><cell>0.482</cell><cell>72</cell></row><row><cell cols="2">-no distances 7k + 138</cell><cell>3.6</cell><cell>0.179 0.302</cell><cell>78</cell><cell>10k + 74</cell><cell>4.1</cell><cell>0.250</cell><cell>0.491</cell><cell>73</cell></row><row><cell>-no anchors, rels only</cell><cell>0 + 138</cell><cell>0.6</cell><cell>0.063 0.121</cell><cell>31</cell><cell>0 + 74</cell><cell>0.5</cell><cell>0.025</cell><cell>0.041</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Relation prediction results. |V | denotes vocabulary size (anchors + relations).</figDesc><table><row><cell></cell><cell cols="2">FB15k-237</cell><cell cols="2">WN18RR</cell><cell cols="2">YAGO 3-10</cell></row><row><cell></cell><cell>|V |</cell><cell>MRR H@10</cell><cell>|V |</cell><cell>MRR H@10</cell><cell>|V |</cell><cell>MRR H@10</cell></row><row><cell>RotatE</cell><cell cols="6">15k + 0.5k 0.905 0.979 40k + 22 0.774 0.897 123k + 74 0.909 0.992</cell></row><row><cell>NodePiece + RotatE</cell><cell cols="4">1k + 0.5k 0.874 0.971 500 + 22 0.761 0.985</cell><cell cols="2">10k + 74 0.951 0.997</cell></row><row><cell cols="5">-no rel. context 1k + 0.5k 0.876 0.968 500 + 22 0.541 0.958</cell><cell cols="2">10k + 74 0.898 0.993</cell></row><row><cell cols="5">-no distances 1k + 0.5k 0.877 0.970 500 + 22 0.746 0.975</cell><cell cols="2">10k + 74 0.943 0.997</cell></row><row><cell>-no anchors, rels only</cell><cell>0 + 0.5k</cell><cell>0.873 0.971</cell><cell>0 + 22</cell><cell>0.545 0.947</cell><cell>0 + 74</cell><cell>0.951 0.998</cell></row><row><cell cols="7">and anchors are on average 6 hops away from target nodes; while FB15k-237 is a denser graph where</cell></row><row><cell cols="7">anchors can be found in a 2-3 hop neighborhood of any node. Hence, on a sparse graph with longer</cell></row><row><cell cols="4">distances, it takes more anchors to properly encode a node.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Node classification results. |V | denotes vocabulary size (anchors + relations), #P is a total parameter count (millions).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">WD50K (5% labeled)</cell><cell></cell><cell cols="2">WD50K (10% labeled)</cell><cell></cell></row><row><cell></cell><cell>|V |</cell><cell cols="7">#P (M) ROC-AUC PRC-AUC Hard Acc ROC-AUC PRC-AUC Hard Acc</cell></row><row><cell>MLP</cell><cell>46k + 1k</cell><cell>4.1</cell><cell>0.503</cell><cell>0.016</cell><cell>0.001</cell><cell>0.510</cell><cell>0.017</cell><cell>0.002</cell></row><row><cell>CompGCN</cell><cell>46k + 1k</cell><cell>4.4</cell><cell>0.836</cell><cell>0.280</cell><cell>0.176</cell><cell>0.834</cell><cell>0.265</cell><cell>0.161</cell></row><row><cell>NodePiece + GNN</cell><cell>50 + 1k</cell><cell>0.75</cell><cell>0.981</cell><cell>0.443</cell><cell>0.513</cell><cell>0.981</cell><cell>0.450</cell><cell>0.516</cell></row><row><cell cols="2">-no rel. context 50 + 1k</cell><cell>0.64</cell><cell>0.982</cell><cell>0.446</cell><cell>0.534</cell><cell>0.982</cell><cell>0.449</cell><cell>0.530</cell></row><row><cell cols="2">-no distances 50 + 1k</cell><cell>0.74</cell><cell>0.981</cell><cell>0.448</cell><cell>0.516</cell><cell>0.981</cell><cell>0.448</cell><cell>0.513</cell></row><row><cell>-no anchors, rels only</cell><cell>0 + 1k</cell><cell>0.54</cell><cell>0.984</cell><cell>0.453</cell><cell>0.532</cell><cell>0.984</cell><cell>0.456</cell><cell>0.533</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Out-of-sample link prediction. † results are taken from<ref type="bibr" target="#b0">[1]</ref>. |V | denotes vocabulary size (anchors + relations), #P is a total parameter count (millions).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">oFB15k-237</cell><cell></cell><cell></cell><cell></cell><cell cols="2">oYAGO 3-10 (117k)</cell><cell></cell></row><row><cell></cell><cell>|V |</cell><cell cols="3">#P (M) MRR H@10</cell><cell>%</cell><cell>|V |</cell><cell cols="2">#P (M) MRR H@10</cell><cell>%</cell></row><row><cell>oDistMult-ERAvg</cell><cell>11k + 0.5k</cell><cell>2.4</cell><cell cols="4">0.256  † 0.420  † 100 117k + 74</cell><cell>23.4</cell><cell>OOM OOM</cell><cell>-</cell></row><row><cell>NodePiece + DistMult</cell><cell>1k + 0.5k</cell><cell>1</cell><cell>0.206</cell><cell>0.372</cell><cell>88</cell><cell>10k + 74</cell><cell>2.7</cell><cell cols="2">0.133 0.261 100</cell></row><row><cell cols="2">-no rel. context 1k + 0.5k</cell><cell>1</cell><cell>0.173</cell><cell>0.329</cell><cell>78</cell><cell>10k + 74</cell><cell>2.7</cell><cell>0.125 0.245</cell><cell>94</cell></row><row><cell cols="2">-no distances 1k + 0.5k</cell><cell>1</cell><cell>0.208</cell><cell>0.372</cell><cell>88</cell><cell>10k + 74</cell><cell>2.7</cell><cell>0.133 0.260</cell><cell>99</cell></row><row><cell>-no anchors, rels only</cell><cell>0 + 0.5k</cell><cell>0.8</cell><cell>0.069</cell><cell>0.127</cell><cell>30</cell><cell>0 + 74</cell><cell>0.7</cell><cell>0.015 0.017</cell><cell>6</cell></row><row><cell cols="10">while in a bigger oYAGO 3-10 we use 10,000 anchors with 20 nearest anchors and 5 relations in the</cell></row><row><cell cols="10">context. For this task, we apply a transformer encoder instead of MLP. For a fair comparison, we use</cell></row><row><cell cols="3">DistMult as a scoring function as well.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>NodePiece hyperparameters for transductive link prediction experiments</figDesc><table><row><cell>Parameter</cell><cell cols="4">FB15k-237 WN18RR CoDEx-L YAGO 3-10</cell></row><row><cell># Anchors, |A|</cell><cell>1000</cell><cell>500</cell><cell>7000</cell><cell>10000</cell></row><row><cell># Anchors per node, k</cell><cell>20</cell><cell>50</cell><cell>20</cell><cell>20</cell></row><row><cell>Relational context, m</cell><cell>15</cell><cell>4</cell><cell>6</cell><cell>5</cell></row><row><cell>Vocabulary dim, d</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>256</cell><cell>512</cell></row><row><cell>Learning rate</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.00025</cell></row><row><cell>Epochs</cell><cell>400</cell><cell>600</cell><cell>120</cell><cell>600</cell></row><row><cell>Encoder type</cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell></row><row><cell>Encoder dim</cell><cell>400</cell><cell>400</cell><cell>400</cell><cell>400</cell></row><row><cell>Encoder layers</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Encoder dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Loss</cell><cell>BCE</cell><cell>NSSAL</cell><cell>BCE</cell><cell>NSSAL</cell></row><row><cell>Margin</cell><cell>-</cell><cell>15</cell><cell>-</cell><cell>50</cell></row><row><cell># Negative samples</cell><cell>-</cell><cell>20</cell><cell>-</cell><cell>10</cell></row><row><cell>Label smoothing</cell><cell>0.4</cell><cell>-</cell><cell>0.3</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>RotatE hyperparameters for transductive link prediction experiments. CoDEx-L and YAGO 3-10 also list the hyperparameters (after the symbol / ) for smaller models (reported in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>) of the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameters for relation prediction experiments. The content is largely identical to Table 8, only changed parameters are listed</figDesc><table><row><cell></cell><cell cols="3">NodePiece + RotatE</cell><cell></cell><cell>RotatE</cell><cell></cell></row><row><cell>Parameter</cell><cell cols="6">FB15k-237 WN18RR YAGO 3-10 FB15k-237 WN18RR YAGO 3-10</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell>Epochs</cell><cell>20</cell><cell>150</cell><cell>7</cell><cell>150</cell><cell>150</cell><cell>150</cell></row><row><cell>Loss function</cell><cell>NSSAL</cell><cell>NSSAL</cell><cell>NSSAL</cell><cell>NSSAL</cell><cell>NSSAL</cell><cell>NSSAL</cell></row><row><cell>Margin</cell><cell>15</cell><cell>12</cell><cell>25</cell><cell>9</cell><cell>3</cell><cell>5</cell></row><row><cell># Negative samples</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell cols="3">: Hyperparameters for node classification experiments</cell><cell></cell></row><row><cell>Parameter</cell><cell cols="3">NodePiece + CompGCN CompGCN MLP</cell></row><row><cell># Anchors, |A|</cell><cell>50</cell><cell>-</cell><cell>-</cell></row><row><cell># Anchors per node, k</cell><cell>10</cell><cell>-</cell><cell>-</cell></row><row><cell>Relational context, m</cell><cell>5</cell><cell>-</cell><cell>-</cell></row><row><cell>Vocabulary dim, d</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell>Learning rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Epochs</cell><cell>4000</cell><cell>4000</cell><cell>4000</cell></row><row><cell>NodePiece encoder</cell><cell>MLP</cell><cell>-</cell><cell>-</cell></row><row><cell>NodePiece encoder dim</cell><cell>200</cell><cell>-</cell><cell>-</cell></row><row><cell>NodePiece encoder layers</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>NodePiece encoder dropout</cell><cell>0.1</cell><cell>-</cell><cell>-</cell></row><row><cell>GNN (MLP) layers</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>GNN (MLP) dropout</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Loss function</cell><cell>BCE</cell><cell>BCE</cell><cell>BCE</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameters for relation prediction experiments. The content is largely identical to</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="2">, only changed parameters are listed</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">NodePiece + DistMult</cell><cell>oDistMult</cell></row><row><cell>Parameter</cell><cell cols="3">oFB15k-237 oYAGO 3-10 oFB15k-237</cell></row><row><cell># Anchors, |A|</cell><cell>1000</cell><cell>10000</cell><cell>-</cell></row><row><cell># Anchors per node, k</cell><cell>20</cell><cell>20</cell><cell>-</cell></row><row><cell>Relational context, m</cell><cell>15</cell><cell>5</cell><cell>-</cell></row><row><cell>Vocabulary dim, d</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell>Batch size</cell><cell>256</cell><cell>256</cell><cell>1000</cell></row><row><cell>Learning rate</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.01</cell></row><row><cell>Epochs</cell><cell>40</cell><cell>40</cell><cell>1000</cell></row><row><cell>NodePiece encoder</cell><cell cols="2">Transformer Transformer</cell><cell>-</cell></row><row><cell>NodePiece encoder dim</cell><cell>512</cell><cell>512</cell><cell>-</cell></row><row><cell>NodePiece encoder layers</cell><cell>2</cell><cell>2</cell><cell>-</cell></row><row><cell>NodePiece encoder dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>-</cell></row><row><cell>Loss function</cell><cell>Softplus</cell><cell>Softplus</cell><cell>Softplus</cell></row><row><cell># Negative samples</cell><cell>5</cell><cell>5</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">A full relational path can be mined as well but it has proven to be not scalable as each path needs to be encoded separately through a sequence encoder, e.g., GRU.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">As of 2021, one of the largest open KGs Wikidata contains about 100M nodes and 6K edge types</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://github.com/migalkin/NodePiece</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Koustuv Sinha, Gaurav Maheshwari, and Priyansh Trivedi for insightful and valuable discussions at earlier stages of this work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding. This work is partially supported by the Canada CIFAR AI Chair Program and Samsung AI grant (held at Mila). We thank Mila and Compute Canada for access to computational resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Out-of-sample representation learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Albooyeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.241</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.241" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="2657" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bringing light into the dark: A large-scale evaluation of knowledge graph embedding models under a unified framework</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Lehmann</surname></persName>
		</author>
		<idno>CoRR, abs/2006.13365</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Lehmann</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v22/20-825.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">82</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to extrapolate knowledge: Transductive few-shot out-of-graph link prediction</title>
		<author>
			<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Bok Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
		<ptr target="https://www.aclweb.org/anthology/Q17-1010" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Léon</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zoubin</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">December 5-8, 2013. 2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-dimensional hyperbolic knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">July 5-10, 2020. 2020</date>
			<biblScope unit="page" from="6901" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta relational learning for few-shot link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4208" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<editor>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">February 2-7, 2018. 2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge hypergraphs: Prediction beyond binary relations</title>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<editor>
			<persName><forename type="first">Christian</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2191" to="2197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Message passing for hyper-relational knowledge graphs</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7346" to="7359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Knowledge transfer for out-of-knowledge-base entities: A graph neural network approach</title>
		<author>
			<persName><forename type="first">Takuo</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidekazu</forename><surname>Oiwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05674</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/2002.00388</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PyTorch-BigGraph: A Large-scale Graph Embedding System</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference</title>
				<meeting>the 2nd SysML Conference<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anchor &amp; transform: Learning sparse embeddings for large vocabularies</title>
		<author>
			<persName><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Vd7lCMvtLqg" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">YAGO3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Biennial Conference on Innovative Data Systems Research, CIDR 2015</title>
		<title level="s">Online Proceedings. www.cidrdb.org</title>
		<meeting><address><addrLine>Asilomar, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">January 4-7, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">{SOLAR}: Sparse orthogonal learned and random embeddings</title>
		<author>
			<persName><forename type="first">Tharun</forename><surname>Medini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=fw-BHZ1KjxJ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Léon</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zoubin</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">December 5-8, 2013. 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<idno>1999-66</idno>
		<imprint>
			<date type="published" when="1999-11">November 1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Previous number = SIDL-WP-1999-0120</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-10-25">2014. October 25-29, 2014. 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond triplets: Hyper-relational knowledge graph embedding for link prediction</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1885" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding compression</title>
		<author>
			<persName><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.238</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.238" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="2681" to="2691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CoDEx: A Comprehensive Knowledge Graph Completion Benchmark</title>
		<author>
			<persName><forename type="first">Tara</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.669</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.669" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="8328" to="8350" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno>CoRR, abs/1910.01108</idno>
		<ptr target="http://arxiv.org/abs/1910.01108" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference, ESWC 2018</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">June 3-7, 2018. 2018</date>
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2012.6289079</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2012.6289079" />
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-03-25">2012. March 25-30, 2012. 2012</date>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Komal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="9448" to="9457" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4007</idno>
		<ptr target="https://www.aclweb.org/anthology/W15-4007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
				<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Composition-based multi-relational graph convolutional networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BylA_C4tPr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<editor>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samy</forename><surname>Ulrike Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rob</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Wikidata: a free collaborative knowledgebase. Commun</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mulde: Multi-teacher knowledge distillation for low-dimensional knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><surname>Sheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07152</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Logic attention based neighborhood aggregation for inductive knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7152" to="7159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">KEPLER: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1980" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Few-shot knowledge graph completion</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Autosf: Searching scoring functions for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Distile: Distiling knowledge graph embeddings for faster and cheaper reasoning</title>
		<author>
			<persName><forename type="first">Yushan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05912</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
