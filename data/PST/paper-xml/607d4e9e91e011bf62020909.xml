<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Stable Learning for Out-Of-Distribution Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
							<email>xingxuanzhang@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
							<email>cuip@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renzhe</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linjun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Stable Learning for Out-Of-Distribution Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Approaches based on deep neural networks have achieved striking performance when testing data and training data share similar distribution, but can significantly fail otherwise. Therefore, eliminating the impact of distribution shifts between training and testing data is crucial for building performance-promising deep models. Conventional methods assume either the known heterogeneity of training data (e.g. domain labels) or the approximately equal capacities of different domains. In this paper, we consider a more challenging case where neither of the above assumptions holds. We propose to address this problem by removing the dependencies between features via learning weights for training samples, which helps deep models get rid of spurious correlations and, in turn, concentrate more on the true connection between discriminative features and labels. Extensive experiments clearly demonstrate the effectiveness of our method on multiple distribution generalization benchmarks compared with state-of-the-art counterparts. Through extensive experiments on distribution generalization benchmarks including PACS, VLCS, MNIST-M, and NICO, we show the effectiveness of our method compared with state-of-the-art counterparts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many machine learning approaches tend to exploit subtle statistical correlations existing in the training distribution for predictions which have been shown to be effective under the I.I.D. hypothesis, i.e., testing and training data is independently sampled from the identical distribution. In real cases, however, such a hypothesis can hardly be satisfied due to the complex generation mechanism of real data such as data selection biases, confounding factors, or other peculiarities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b20">21]</ref>. The testing distribution may incur uncontrolled and unknown shifts from the *Corresponing author, also with Beijing Key Lab of Networked Multimedia</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StableNet</head><p>(ours) training distribution, which makes most machine learning models fail to make trustworthy predictions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">51]</ref>. To address this issue, out-of-distribution (OOD) generalization is proposed for improving the generalization ability of models under distribution shifts <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b26">27]</ref>. Essentially, when there incurs a distribution shift, the accuracy drop of current models is mainly caused by the spurious correlation between the irrelevant features (i.e. the features that are irrelevant to a given category, such as features of context, figure style, etc.) and category labels, and this kind of spurious correlations are intrinsically caused by the subtle correlations between irrelevant features and relevant features (i.e. the features that are relevant to a given category) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2]</ref>. Taking the recognition task of 'dog' category as an example, as depicted in Figure <ref type="figure" target="#fig_0">1</ref>, if dogs are in the water in most training images, the visual features of dogs and water would be strongly correlated, thus leading to the spurious correlation between visual features of water with the label 'dog'. As a result, when encountering images of dogs without water, or other objects (such as cats) with water, the model is prone to produce false predictions.</p><note type="other">Baseline Input Image</note><p>Recently, such distribution (domain) shift problems have been intensively studied in the domain generalization (DG) literature <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b32">33]</ref>. The basic idea of DG is to divide a category into multiple domains so that irrelevant features vary across different domains while relevant features remain invariant <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref>. Such training data makes it possible for a well-designed model to learn the invariant representations across domains and inhibit the negative effect from irrelevant features, leading to better generalization ability under distribution shifts. Some pioneering methods require clear and significant heterogeneity, namely that the domains are manually divided and labeled <ref type="bibr" target="#b59">[61,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b41">42]</ref>, which cannot be always satisfied in real applications. More recently, some methods are proposed to implicitly learn latent domains from data <ref type="bibr">[44,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b58">60]</ref>, but they implicitly assume that the latent domains are balanced, meaning that the training data is formed by balanced sampling from latent domains. In real cases, however, the assumption of domain balance can be easily violated, leading to the degeneration of these methods. This is also empirically validated in our experiments as shown in Section 4.</p><p>Here we consider a more realistic and challenging setting where the domains of training data are unknown and we do not implicitly assume that the latent domains are balanced. With this goal, a strand of research on stable learning are proposed <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b27">28]</ref>. Given that the statistical dependence between relevant and irrelevant features is a major cause of model crash under distribution shifts, they propose to realize out-of-distribution generalization by decorrelating the relevant and irrelevant features. Since there is no extra supervision for separating relevant features from irrelevant features, a conservative solution is to decorrelate all features. Recently, this notion has been demonstrated to be effective in improving the generalization ability of linear models. <ref type="bibr" target="#b28">[29]</ref> proposes a sample weighting approach with the goal of decorrelating input variables, and <ref type="bibr" target="#b49">[51]</ref> theoretically proves why such sample weighting can make a linear model produce stable predictions under distribution shifts. But they are all developed under the constraints of linear frameworks. When extending these ideas into deep models to tackle more complicated data types like images, we confront two main challenges. First, the complex non-linear dependencies among features are much more difficult to be measured and eliminated than the linear ones. Second, the global sample weighting strategy in these methods requires excessive storage and computational cost in deep models, which is infeasible in practice.</p><p>To address these two challenges, we propose a method called StableNet. In terms of the first challenge, we propose a novel nonlinear feature decorrelation approach based on Random Fourier Features <ref type="bibr" target="#b43">[45]</ref> with linear computational complexity. As for the second challenge, we propose an efficient optimization mechanism to perceive and remove correlations globally by iteratively saving and reloading features and weights of the model. These two modules are jointly optimized in our method. Moreover, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, StableNet can effectively partial out the irrelevant features (i.e. water) and leverage truly relevant features for prediction, leading to more stable performances in the wild non-stationary environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Domain Generalization. Domain generalization (DG) considers the generalization capacities to unseen domains of deep models trained with multiple source domains. A common approach is to extract domain-invariant features over multiple source domains <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b39">40]</ref> or to aggregate domain-specific modules <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Several works propose to enlarge the available data space with augmentation of source domains <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr">44,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b61">63]</ref>. There are several approaches that exploit regularization with meta-learning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b9">10]</ref> and Invariant Risk Minimization (IRM) framework <ref type="bibr" target="#b1">[2]</ref> for DG. Despite the promising results of DG methods in the well-designed experimental settings, some strong assumptions such as the manually divided and labeled domains and the balanced sampling process from each domain actually hinder the DG methods from real applications. Feature Decorrelation. As the correlations between features affect or even impair the model prediction, several works have focused on remove such correlation in the training process. Some pioneering works based on Lasso framework <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b6">7]</ref> propose to decorrelate features by adding a regularizer that imposes the highly correlated features not to be selected simultaneously. Recently, several works theoretically bridge the connections between correlation and model stability under misspecification <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b28">29]</ref>, and propose to address such a problem via a sample reweighting scheme. However, the above methods are all developed under linear frameworks which can not handle complex data types such as images and videos in computer vision applications. More related works and discussions are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sample Weighting for Distribution Generalization</head><p>We address the distribution shifts problem by weighting samples globally to directly decorrelate all the features for every input sample, thus statistical correlations between relevant and irrelevant features are eliminated. Concretely, StableNet gets rid of both linear and non-linear dependencies between features by utilizing the characteristics of Random Fourier Features (RFF) and sample weighting. To adapt the global decorrelation method to modern deep models, we further propose the saving and reloading global correlation mechanism, to decrease the usage of storage and computational cost when the training data are of a large scale. The formulations and theoretical explanations are shown in Section 3.1. In Section 3.2, we introduce the saving and reloading global correlation method, which makes calculating correlation globally possible with deep models. Notations X ⊂ R m X denotes the space of raw pixels, Y ⊂ R m Y denotes the outcome space and Z ⊂ R m Z denotes the representation space. m X , m Y , m Z are the dimensions of space X , Y, Z, respectively. f : X → Z denotes the representation function and g : Z → Y denotes the prediction function. We have n samples X ⊂ R n×m X with labels Y ⊂ R n×m Y and we use X i and y i to denote the i-th sample. The representations learned by neural networks are donated as Z ⊂ R n×m Z and the i-th variable in the representation space is donated as Z :,i . We use w ∈ R n to denote sample weights. u and v are Random Fourier Features mapping functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sample weighting with RFF</head><p>Independence testing statistics To eliminate the dependence between any pair of features Z :,i and Z :,j in the representation space, we introduce hypothesis testing statistics that measures the independence between random variables. Suppose there are two one-dimensional random variables A, B (Here we use A and B to represent random variables instead of Z :,i and Z :,j for simplicity of notation.) and we sample (A 1 , A 2 , . . . A n ) and (B 1 , B 2 , . . . B n ) from the distribution of A and B, respectively. The main problem is how relevant these two variables are based on the samples.</p><p>Consider a measurable, positive definite kernel k A on the domain of random variable A and the corresponding RKHS is denoted by H A . If k B and H B are similarly defined, the cross-covariance operator Σ AB <ref type="bibr" target="#b12">[13]</ref> from H B to H A is as follows:</p><formula xml:id="formula_0">h A , Σ AB h B =E AB [h A (A)h B (B)] − E A [h A (A)]E B [h B (B)]<label>(1)</label></formula><p>for all h A ∈ H A and h B ∈ H B . Then, the independence can be determined by the following proposition <ref type="bibr" target="#b13">[14]</ref>.</p><formula xml:id="formula_1">Proposition 3.1 If the product k A k B is characteristic, E[k A (A, A)] &lt; ∞ and E[k B (B, B)] &lt; ∞, we have Σ AB = 0 ⇐⇒ A ⊥ B<label>(2)</label></formula><p>Hilbert-Schmidt Independence Criterion (HSIC) <ref type="bibr" target="#b17">[18]</ref>, which requires that the squared Hilbert-Schmidt norm of Σ AB should be zero, can be applied as a criterion to supervise feature decorrelation <ref type="bibr" target="#b2">[3]</ref>. However, the calculation of HSIC requires noticeable computational cost which grows as the batch size of training data increases, so it is inapplicable to training deep models on large datasets. More approaches of independence test are discussed in Appendix B.2. Actually, Frobenius norm corresponds to the Hilbert-Schmidt norm in Euclidean space <ref type="bibr" target="#b51">[53]</ref>, so that the independent testing statistic can be based on Frobenius norm.</p><p>Let the partial cross-covariance matrix be:</p><formula xml:id="formula_2">ΣAB = 1 n − 1 n i=1 u(Ai) − 1 n n j=1 u(Aj) T • v(Bi) − 1 n n j=1 v(Bj) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">u(A) = (u 1 (A), u 2 (A), . . . u n A (A)) , u j (A) ∈ H RFF , ∀j, v(B) = (v 1 (B), v 2 (B), . . . v n B (B)) , v j (B) ∈ H RFF , ∀j.<label>(4)</label></formula><p>Here we sample n A and n B functions from H RFF respectively and H RFF denotes the function space of Random Fourier Features with the following form</p><formula xml:id="formula_4">H RFF = h : x → √ 2 cos(ωx + φ) | ω ∼ N (0, 1), φ ∼ Uniform(0, 2π) ,<label>(5)</label></formula><p>i.e. ω is sampled from the standard Normal distribution and φ is sampled from the Uniform distribution. Then, the independence testing statistic I AB is defined as the Frobenius norm of the partial cross-covariance matrix, i.e.,</p><formula xml:id="formula_5">I AB = ΣAB 2 F .</formula><p>Notice that I AB is always non-negative. As I AB decreases to zero, the two variables A and B tends to be independent. Thus I AB can effectively measure the independence between random variables. The accuracy of independence test grows as n A and n B increase. Empirically, setting both n A and n B to 5 is solid enough to judge the independence of random variables <ref type="bibr" target="#b51">[53]</ref>.</p><p>Learning sample weights for decorrelation Inspired by <ref type="bibr" target="#b28">[29]</ref>, we propose to eliminate the dependence between features in the representation space via sample weighting and measure general independence via RFF.</p><p>We use w ∈ R n + to denote the sample weights and n i=1 w i = n. After weighting, the partial cross-covariance matrix for random variables A and B in Equation 3 can be calculated as follows:</p><formula xml:id="formula_6">ΣAB;w = 1 n − 1 n i=1 wiu(Ai) − 1 n n j=1 wju(Aj) T • wiv(Bi) − 1 n n j=1 wjv(Bj) .<label>(6)</label></formula><p>Here u and v are the RFF mapping functions explained in Equation <ref type="formula" target="#formula_3">4</ref>. StableNet targets independence between any pair of features. Specifically, for feature Z :,i and Z :,j , the corresponding partial cross-covariance matrix should be ΣZ:,iZ:,j;w 2 F , shown in Equation <ref type="formula" target="#formula_6">6</ref>. We propose to optimize w by</p><formula xml:id="formula_7">w * = arg min w∈∆n 1≤i&lt;j≤m Z ΣZ:,iZ:,j;w 2 F ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_8">∆ n = w ∈ R n + | n i=1 w i = n .</formula><p>Hence, weighting training samples with the optimal w * can mitigate the dependence between features to the greatest extent Generally, our algorithm iteratively optimize sample weights w, representation function f , and prediction func-tion g as follows:</p><formula xml:id="formula_9">f (t+1) , g (t+1) =arg min f,g n i=1 w (t) i L(g(f (X i )), y i ), w (t+1) =arg min w∈∆n 1≤i&lt;j≤m Z ΣZ (t+1) :,i Z (t+1) :,j ;w 2 F . (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where Z (t+1) = f (t+1) (X), L(•, •) represents the cross entropy loss function and t represents the time stamp. Initially, w (0) = (1, 1, . . . , 1) T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning sample weights globally</head><p>Equation 8 requires a specific weight learned for each sample. However, in practice, especially for deep learning tasks, it requires enormous storage and computational cost to learn sample weights globally. Moreover, with SGD for optimization, only part of the samples are observed in each batch, hence global weights for all samples cannot be learned. In this part, we propose a saving and reloading method, which merges and saves features and sample weights encountered in the training phase and reloads them as global knowledge of all the training data to optimize sample weights.</p><p>For each batch, the features used to optimize the sample weights are generated as follows: While training for each batch, we keep w Gi fixed and only w L is learnable under Equation <ref type="formula" target="#formula_9">8</ref>. At the end of each iteration of training, we fuse the global information (Z Gi , w Gi ) and the local information (Z L , w L ) as follows:</p><formula xml:id="formula_11">Z O = Concat (Z G1 , Z G2 , • • •, Z Gk , Z L ) , w O = Concat (w G1 , w G2 , • • •, w Gk , w L ) .<label>(9</label></formula><formula xml:id="formula_12">Z ′ Gi = α i Z Gi + (1 − α i )Z L , w ′ Gi = α i w Gi + (1 − α i )w L .<label>(10)</label></formula><p>Here for each group of global information (Z Gi , w Gi ), we use k different smoothing parameters α i for considering both long-term memory (α i is large) and short-term memory (α i is small) in global information and k indicates that the presaved features are k times of that of original features.</p><p>Finally, we substitute all (Z Gi , w Gi ) with (Z ′ Gi , w ′ Gi ) for the next batch.</p><p>In the training phase, we iteratively optimize sample weights and model parameters with Equation 8. In the inference phase, the predictive model directly conduct prediction without any calculation of sample weights. The detailed procedure of our method is shown in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental settings and datasets</head><p>We validate StableNet in a variety of settings. To cover more general and challenging cases of distribution shifts, we adopt four experimental settings as follows: Unbalanced. In the common DG setting, the capacities of source domains are assumed to be comparable. However, considering most datasets are a mixture of latent unknown domains, one can hardly assume that the amount of samples from these domains are consistent since these datasets are not generated by equally sampling from latent domains. We simulate this scenario with this setting. Domains are split into source domains and target domains. The capacities of various domains can vary significantly. Note that this setting, where the capacities of available domains are unbalanced while the proportion of each class remains consistent across domains, is completely different from the settings of the class imbalance problem. This setting is to evaluate the generalization ability of models when the heterogeneity is unclear and insignificant. Flexible. We consider a more challenging but common in real-world setting where domains for different categories can be various. For instance, birds can be on trees but hardly in the water while fishes are the opposite. If we consider the backgrounds in images as an indicator of domain division, images for class 'bird' can be divided into domain 'on tree' but cannot into domain 'in water' while images for class 'fish' are otherwise, resulting in the diversity of domains among different classes. Thus this setting simulates a widely existing scenario in the real-world. In such cases, the level of the distribution shifts varies in different classes, requiring a strong ability of generalization given the statistical correlations between relevant features and categoryirrelevant features vary. Adversarial. We consider the most challenging scenario, where the model is under adversarial attack and the spurious correlations between domains and labels are strong and misleading. For instance, we assume a scenario where the category 'dog' is usually associated with the domain 'grass' and the category 'cat' with the domain 'sofa' in the training data, while the category 'dog' is usually associated with the domain 'sofa' and the category 'cat' with the domain 'grass' in the testing data. If the ratio of domain 'grass' in the im-ages from class 'dog' is significantly higher than others, the predictive model may tend to recognize grass as a dog. Classic. This setting is the same as the common setting in DG. The capacities of various domains are comparable. Therefore this setting is to evaluate the generalization ability of models when the heterogeneity of training data is significant and clear, which is less challenging compared with the previous three settings. Datasets. We consider four datasets to carry through these four settings, namely PACS [31], VLCS <ref type="bibr" target="#b56">[58]</ref>, MNIST-M <ref type="bibr" target="#b14">[15]</ref> and NICO <ref type="bibr" target="#b19">[20]</ref>. Introduction to these datasets and details of implementation are in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Unbalanced setting</head><p>Given this setting requires all the classes in the dataset share the same candidate set of domains, which is incompatible with NICO, we adopt PACS and VLCS for this setting. Three domains are considered as source domains and the other one as target. To make the amount of data from heterogeneous sources clearly differentiated, we set one domain as the dominant domain. For each target domain, we randomly select one domain from the source domains as the dominant source domain and adjust the ratio of data from the dominant domain and the other two domains. Details of ratios and partition are shown in Appendix C.2.</p><p>Here we show the results when the capacity ratio of three source domains is 5:1:1 in Table <ref type="table">1</ref> and our method outperforms other methods in all the target domains on both PACS and VLCS. Moreover, StableNet achieves best performance consistently under all the other ratios as shown in Appendix C.2. These results indicate that the subtle statistical correlations between relevant and irrelevant features are strong enough to significantly harm the generalization across domains. When the correlations are eliminated, the model is able to learn the true connections between relevant features and labels and inference according to them only, thus generalize better. For adversarially trained methods like DG-MMLD <ref type="bibr" target="#b38">[39]</ref>, the supervision from minor domains is insufficient and the ability of the model to discriminate irrelevant features is impaired. For augmentation of source domains based methods like M-ADA [44], the impact of the dominant domain is not diminished while the minor ones are still insignificant after the augmentation. Methods like RSC <ref type="bibr" target="#b22">[23]</ref> adopt regularization to prevent the model from overfitting on source domains and the samples from minor domains can be considered as outliers and ignored. Therefore, the subtle correlations between relevant features and irrelevant features especially in minor domains are not eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Unbalanced + flexible setting</head><p>We adopt PACS, VLCS and NICO to evaluate the unbalanced + flexible setting. For PACS and VLCS, we randomly select one domain as the dominant domain for each Table <ref type="table">1</ref>: Results of the unbalanced setting on PACS and VLCS. We reimplement the methods that require no domain labels on PACS and VLCS with ResNet18 <ref type="bibr" target="#b18">[19]</ref> which is pretrained on ImageNet <ref type="bibr" target="#b7">[8]</ref> as the backbone network for all the methods. The reported results are average over three repetitions of each run. The title of each column indicates the name of the domain used as target. The best results of all methods are highlighted with the bold font and the second with underscore. class, and another domain as the target. For NICO, there are 10 domains for each class, 8 out of which are selected as the source and 2 as the target. We adjust the ratio of the dominant domain to minor domains to adjust the level of distribution shifts. Here we report the results when the dominant ratio is 5:1:1. Details and more results of other divisions are shown in Appendix C.3.</p><p>The results are shown in Table <ref type="table" target="#tab_0">2</ref>. M-ADA and DG-MMLD fail to outperform ResNet-18 on NICO under this setting. M-ADA, which generates images for training with an autoencoder, may fail when the training data are largescale real-world images and the distribution shifts are not caused by random disturbance. DG-MMLD generates domain labels with clustering and may fail when the data lack explicit heterogeneity or the number of latent domains is too large for clustering. In contrast, StableNet shows a strong ability of generalization when the input data are with complicated structure especially real-world images from unlimited resources. StableNet can capture various forms of dependencies and balance the distribution of input data. On PACS and VLCS, StableNet also outperforms state-of-theart methods, showing the effectiveness of removing statistical dependencies between features especially when the source domains for different categories are not consistent. More experimental results are in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Unbalanced + flexible + adversarial setting</head><p>To exploit the effect of various levels of adversarial attack, we adopt MNIST-M to evaluate our method owing to the numerous (200) optional domains in MNIST-M. Domains in PACS and VLCS are insufficient to generate multi-ple adversarial levels. Hence, we generate a new MNIST-M dataset with three rules: 1) for a given category, there is no overlap between the domains in training and testing; 2) a background image is randomly chosen for each category in the training set, and contexts cropped in the same image are assigned as dominant contexts (domains) for another category in test data so that there are strong spurious correlations between labels and domains; 3) the ratio of dominant context to other contexts varies from 9.5:1 to 1:1 to generate settings with different levels of distribution shifts. Detailed data generating method, adopted backbone network and sample images are in Appendix C. <ref type="bibr" target="#b3">4</ref>.</p><p>The results are shown in Table <ref type="table" target="#tab_1">3</ref>. As the dominant ratio increases, the spurious correlation between domains and categories becomes stronger so that the performance of predictive models drops. When the imbalance in visual features is significant, our method achieves noticeable improvement compared with baseline methods. For regularization-based methods such as RSC, they tend to weaken the supervision from minor domains which may be considered as outliers and therefore the spurious correlations between irrelevant features and labels are strengthened under adversarial attacks, resulting in even poorer results compared with the vanilla ResNet model. As shown in Table 3, RSC fails to outperform vanilla CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Classic setting</head><p>The classic setting is the same as the common setting in DG. Domains are split into source domains and target domains. The capacities of various domains are comparable. Given this setting requires all the classes in the dataset to We follow the experimental protocol of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref> for both the datasets and utilize three domains as source domains and the remaining one as the target.</p><p>The results are shown in Table <ref type="table" target="#tab_2">4</ref>. On VLCS, StableNet outperforms other state-of-the-art methods in two out of four target cases and achieves the highest average accuracy. On PACS, StableNet achieves the highest accuracy on the target domain 'photo' and comparable average accuracy (0.46% less) compared with the state-of-the-art method, RSC. The accuracy gap between StableNet and baseline indicates that even when the numbers of samples from different source domains are approximately the same, the subtle statistical correlations between relevant features and irrelevant features still hold strong and the model generalizes across domains better when the correlations are eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation study</head><p>StableNet relies on Random Fourier Features sampled from Gaussian to balance the training data. The more features are sampled, the more independent the final representations are. In practice, however, generating more features requires more computational cost. In this ablation study, we exploit the effect of sampling size for Random Fourier Features. Moreover, inspired by <ref type="bibr" target="#b55">[57]</ref>, one can further reduce the feature dimension by randomly selecting features used to calculate dependence with different ratios. Figure <ref type="figure">3</ref> shows the results of StableNet with different dimensions of Random Fourier Features. If we remove all the Random Fourier Features, our regularizer in Equation <ref type="formula" target="#formula_7">7</ref>degenerates and can only model the linear correlation between features. We further exploit the effect of the size of presaved features and weights in Equation 9 and the results are shown in Figure <ref type="figure" target="#fig_3">2(c</ref>). When the size of presaved features is reduced to 0, sample weights are learned inside of each batch, yielding noticeable variance. Generally, as the presaving size increases, the accuracy raises slightly and the variance drops significantly, indicating that presaved features help to learn sample weights globally and therefore the generalization ability of the model is more stable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Saliency map</head><p>An intuitive type of explanation for image classification models is to identify pixels that have a strong influence on the final decision <ref type="bibr" target="#b50">[52]</ref>. To demonstrate whether the model focuses on the object or the context (domain) while conducting prediction, we visualize the gradient of the class score function with respect to the input pixels. In the case of stable learning, we adopt the same backbone architecture for all methods, so that we adopt smoothed gradient as suggested by <ref type="bibr" target="#b0">[1]</ref>, which generates saliency maps depending on the learned parameters of the models instead of the architecture. Visualization results are shown in Figure <ref type="figure">4</ref>. Saliency maps of the baseline model show that various contexts draw noticeable focus of the classifier while fail to make decisive contributions to our model. More visualization results are in Appendix C.6, which further demonstrate that StableNet focuses more on visual parts which are both distinguishing and invariant when the postures or positions of objects vary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In the paper, to improve the generalization of deep models under distribution shifts, we proposed a novel method called StableNet which can eliminate the statistical correlation between relevant and irrelevant features via sample weighting. Extensive experiments across a wide range of settings demonstrated the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of saliency maps produced by the vanilla ResNet-18 model and StableNet when most of the training images containing dogs in the water. The lightness of the saliency map indicates how much attention that the models pay on particular area of the input image (i.e. lighter area plays a more crucial role for the prediction than the darker area). Due to the spurious correlation, the ResNet-18 model tends to focus on both dogs and the water while our model focuses mostly on dogs.</figDesc><graphic url="image-1.png" coords="1,312.62,227.72,232.45,164.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall architecture of the proposed StableNet. LSWD refers to learning sample weighting for decorrelation as described in Section 3.1. Final loss is used to optimized the classification network. Detailed learning procedure of StableNet is in Section 3.1 and Appendix B.1.</figDesc><graphic url="image-3.png" coords="3,110.74,72.00,371.22,136.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>)</head><label></label><figDesc>Here we slightly abuse the notation Z O and w O to mean the features and weights used to optimize the new sample weights, respectively, Z G1 , • • •, Z Gk , w G1 , • • •, w Gk are global features and weights, which are updated at the end of each batch and represent global information of the whole training dataset. Z L and w L are features and weights in the current batch, representing the local information. The operation for merging all features in Equation 9 is the concatenating operation along samples, i.e. if the batch size is B, Z O is a matrix of size ((k + 1)B) × m Z and w O is a ((k + 1)B)-dimensional vector. In this way, we reduce the storage and the computational cost from O(N ) to O(kB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 (</head><label>2</label><figDesc>a) demonstrates the effectiveness of eliminating non-linear dependence between representations. From Figure 2(b), the non-linear dependence is common in vision features and keep deep models from learning true dependence between input images and category labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Results of ablation study on NICO. All the experiments adopt NICO since NICO consists of a wide range of domains and objects and all domains come from real-world images which make the indication of results more reliable. The RFF dimension in (a) indicates the dimension of Fourier features, where 10x indicates that the dimension of Fourier features are 10 times the size of original features and 0.3x indicates the sampling ratio is 30%. StableNet-N and StableNet-L indicate the original StableNet and the degenerated version of StableNet that only eliminates the linear correlation between features. Presaved size in (c) indicates the dimension of the presaved features and 0x indicates no features are saved.</figDesc><graphic url="image-7.png" coords="8,108.92,265.12,380.24,191.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Results of the unbalanced + flexible setting on PACS, VLCS and NICO. For details about the number of runs, meaning of column titles and fonts, see Table1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">PACS</cell><cell></cell><cell></cell><cell>VLCS</cell></row><row><cell></cell><cell>Art.</cell><cell cols="6">Cartoon Sketch Photo Avg. Caltech Labelme Pascal</cell><cell>Sun</cell><cell>Avg.</cell></row><row><cell>JiGen [6]</cell><cell>72.76</cell><cell></cell><cell>69.21</cell><cell cols="2">64.90 91.24 74.53</cell><cell>85.20</cell><cell>59.73</cell><cell>62.64 50.59 64.54</cell></row><row><cell>M-ADA [44]</cell><cell>61.53</cell><cell></cell><cell>68.76</cell><cell cols="2">58.49 83.21 68.00</cell><cell>70.29</cell><cell>55.44</cell><cell>49.96 37.78 53.37</cell></row><row><cell cols="2">DG-MMLD [39] 64.25</cell><cell></cell><cell>70.31</cell><cell cols="2">64.16 91.64 72.59</cell><cell>79.76</cell><cell>57.93</cell><cell>65.25 44.61 61.89</cell></row><row><cell>RSC [23]</cell><cell>75.72</cell><cell></cell><cell>68.50</cell><cell cols="2">66.10 93.93 76.06</cell><cell>83.82</cell><cell>59.92</cell><cell>64.49 49.08 64.33</cell></row><row><cell>ResNet-18</cell><cell>68.41</cell><cell></cell><cell>67.32</cell><cell cols="2">65.75 90.22 72.93</cell><cell>80.02</cell><cell>60.21</cell><cell>58.33 47.59 61.54</cell></row><row><cell cols="2">StableNet (ours) 80.16</cell><cell></cell><cell>74.15</cell><cell cols="2">70.10 94.24 79.66</cell><cell>88.25</cell><cell>62.59</cell><cell>65.77 55.34 67.99</cell></row><row><cell></cell><cell cols="7">JiGen M-ADA DG-MMLD RSC ResNet-18 StableNet (ours)</cell></row><row><cell cols="3">PACS 40.31</cell><cell>30.32</cell><cell>42.65</cell><cell>39.49</cell><cell>39.02</cell><cell>45.14</cell></row><row><cell cols="3">VLCS 76.75</cell><cell>69.58</cell><cell>78.96</cell><cell>74.81</cell><cell>73.77</cell><cell>79.15</cell></row><row><cell cols="3">NICO 54.42</cell><cell>40.78</cell><cell>47.18</cell><cell>57.59</cell><cell>51.71</cell><cell>59.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Results of the unbalanced + flexible + adversarial setting on MNIST-M. Random donates each digit is blended over a randomly chosen background. DR0.5 donates that in each class, the proportion of the dominant domain in all the training data is 50% and other notations with 'DR' are similar.</figDesc><table><row><cell>Settings</cell><cell cols="8">Random DR0.5 DR0.6 DR0.7 DR0.8 DR0.9 DR0.95 Avg.</cell></row><row><cell>JiGen</cell><cell>97.18</cell><cell>94.97</cell><cell>92.99</cell><cell>90.64</cell><cell>78.97</cell><cell>68.79</cell><cell>69.34</cell><cell>84.70</cell></row><row><cell>M-ADA</cell><cell>95.92</cell><cell>94.45</cell><cell>92.29</cell><cell>88.87</cell><cell>85.89</cell><cell>70.32</cell><cell>67.08</cell><cell>84.97</cell></row><row><cell>DG-MMLD</cell><cell>96.89</cell><cell>94.61</cell><cell>92.59</cell><cell>89.72</cell><cell>88.44</cell><cell>69.13</cell><cell>71.39</cell><cell>86.11</cell></row><row><cell>RSC</cell><cell>96.94</cell><cell>93.43</cell><cell>89.44</cell><cell>85.78</cell><cell>81.68</cell><cell>69.15</cell><cell>65.12</cell><cell>83.08</cell></row><row><cell>CNNs</cell><cell>96.93</cell><cell>93.76</cell><cell>91.93</cell><cell>88.13</cell><cell>81.48</cell><cell>68.43</cell><cell>66.11</cell><cell>83.82</cell></row><row><cell>StableNet (ours)</cell><cell>97.35</cell><cell>95.33</cell><cell>93.49</cell><cell>91.24</cell><cell>87.04</cell><cell>75.69</cell><cell>75.46</cell><cell>87.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Results of the classic setting on PACS and VLCS. All the results on PACS are obtained from the original papers of these methods. We reimplement the methods that require no domain labels on VLCS since these methods are tested with AlexNet<ref type="bibr" target="#b25">[26]</ref> in original papers while we adopt ResNet18<ref type="bibr" target="#b18">[19]</ref> as the backbone network for all the methods. The methods that require domain labels are labelled with asterisk.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>PACS</cell><cell></cell><cell></cell><cell>VLCS</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Art.</cell><cell cols="5">Cartoon Sketch Photo Avg. Caltech Labelme Pascal</cell><cell>Sun</cell><cell></cell></row><row><cell>JiGen</cell><cell>79.42</cell><cell>75.25</cell><cell>71.35 96.03 80.51</cell><cell>96.17</cell><cell>62.06</cell><cell cols="3">70.93 71.40 75.14</cell></row><row><cell>M-ADA</cell><cell>64.29</cell><cell>72.91</cell><cell>67.21 88.23 73.16</cell><cell>74.33</cell><cell>48.38</cell><cell cols="3">45.31 33.82 50.46</cell></row><row><cell>DG-MMLD</cell><cell>81.28</cell><cell>77.16</cell><cell>72.29 96.09 81.83</cell><cell>97.01</cell><cell>62.20</cell><cell cols="3">73.01 72.49 76.18</cell></row><row><cell>D-SAM* [11]</cell><cell>77.33</cell><cell>72.43</cell><cell>77.83 95.30 80.72</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Epi-FCR* [33] 82.10</cell><cell>77.00</cell><cell>73.00 93.90 81.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FAR* [24]</cell><cell>79.30</cell><cell>77.70</cell><cell>74.70 95.30 81.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MetaReg* [4]</cell><cell>83.70</cell><cell>77.20</cell><cell>70.30 95.50 81.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RSC</cell><cell>83.43</cell><cell>80.31</cell><cell>80.85 95.99 85.15</cell><cell>96.21</cell><cell>62.51</cell><cell cols="3">73.81 72.10 76.16</cell></row><row><cell>ResNet-18</cell><cell>76.61</cell><cell>73.60</cell><cell>76.08 93.31 79.90</cell><cell>91.86</cell><cell>61.81</cell><cell cols="3">67.48 68.77 72.48</cell></row><row><cell cols="2">StableNet (ours) 81.74</cell><cell>79.91</cell><cell>80.50 96.53 84.69</cell><cell>96.67</cell><cell>65.36</cell><cell cols="3">73.59 74.97 77.65</cell></row><row><cell cols="4">share the same candidate set of domains, which is incompat-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ible with NICO, we adopt PACS and VLCS for this setting.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by National Key R&amp;D Program of China (No. 2018AAA0102004, No. 2020AAA0106300), National Natural Science Foundation of China (No. U1936219, 61521002, 61772304), Beijing Academy of Artificial Intelligence (BAAI), and a grant from the Institute for Guo Qiang, Tsinghua University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency maps</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9505" to="9515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Hyojin</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02806</idno>
		<title level="m">Learning de-biased representations with biased representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards domain generalization using metaregularization</title>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><surname>Metareg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A meta-transfer objective for learning to disentangle causal mechanisms</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasim</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10912</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName><surname>Bucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uncorrelated lasso</title>
		<author>
			<persName><forename type="first">Sibao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">Hq</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep domain generalization with structured low-rank constraint</title>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Coelho De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6450" to="6461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain generalization with domain-specific aggregation modules</title>
		<author>
			<persName><forename type="first">D'innocente</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="187" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring the landscape of spatial robustness</title>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kernel measures of conditional dependence</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Franc ¸ois Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A kernel statistical test of independence</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards non-iid image classification: A dataset and baselines</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">107383</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain generalization via multidomain discriminant analysis</title>
		<author>
			<persName><forename type="first">Shoubo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laiwan</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="292" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-challenging improves cross-domain generalization</title>
		<author>
			<persName><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02454</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Feature alignment and restoration for domain generalization and adaptation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12009</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00688</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stable prediction across unknown environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research Papers</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stable prediction with model misspecification and agnostic distribution shift</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tomer D Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03463</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6979" to="6987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Best sources forward: domain generalization through source-specific nets</title>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1353" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust place categorization with deep domain generalization</title>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2093" to="2100" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00631</idno>
		<title level="m">Deep learning: A critical appraisal</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain generalization using a mixture of multiple latent domains</title>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2007">2020. 2, 5, 6, 7</date>
			<biblScope unit="page" from="11749" to="11756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5715" to="5725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-view domain generalization for visual recognition</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4193" to="4201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient domain generalization via common-specific low-rank decomposition</title>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Vihari Piratla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><surname>Sarawagi ; Fengchun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Random features for largescale kernel machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to compose domain-specific transformations for data augmentation</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Alexander J Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3236" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning to optimize domain specific normalization for domain generalization</title>
		<author>
			<persName><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwoo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04275</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10745</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">On image classification: Correlation v.s. causality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stable learning via sample reweighting</title>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5692" to="5699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">Smoothgrad: removing noise by adding noise</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Approximate kernel-based conditional independence tests for fast non-parametric causal discovery</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Visweswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">Vasconcellos</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kouichi</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="828" to="841" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Test-time training for out-ofdistribution generalization</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Independently interpretable lasso: A new regularizer for sparse regression with uncorrelated variables</title>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Takada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hironori</forename><surname>Fujisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Random selection of factors preserves the correlation structure in a linear factor model to a high degree</title>
		<author>
			<persName><forename type="first">Jani</forename><surname>Antti J Tanskanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kari</forename><surname>Lukkarinen</surname></persName>
		</author>
		<author>
			<persName><surname>Vatanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e0206551</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06256</idno>
		<title level="m">Learning robust representations by projecting superficial statistics out</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Select-additive learning: Improving generalization in multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaksha</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="949" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning from extrinsic and intrinsic supervisions for domain generalization</title>
		<author>
			<persName><forename type="first">Shujun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caizi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09316</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep domain-adversarial image generation for domain generalisation</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13025" to="13032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to generate novel domains for domain generalization</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
