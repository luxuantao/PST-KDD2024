<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupled Self-supervised Learning for Non-Homophilous Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Teng</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengyu</forename><surname>Chen</surname></persName>
							<email>chenzhengyu@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhimeng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeyang</forename><surname>Zhuang</surname></persName>
							<email>zeyangzhuang0315@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decoupled Self-supervised Learning for Non-Homophilous Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the problem of conducting self-supervised learning for node representation learning on non-homophilous graphs. Existing self-supervised learning methods typically assume the graph is homophilous where linked nodes often belong to the same class or have similar features. However, such assumptions of homophily do not always hold true in real-world graphs. We address this problem by developing a decoupled self-supervised learning (DSSL) framework for graph neural networks. DSSL imitates a generative process of nodes and links from latent variable modeling of the semantic structure, which decouples different underlying semantics between different neighborhoods into the self-supervised node learning process. Our DSSL framework is agnostic to the encoders and does not need prefabricated augmentations, thus is flexible to different graphs. To effectively optimize the framework with latent variables, we derive the evidence lower-bound of the self-supervised objective and develop a scalable training algorithm with variational inference. We provide a theoretical analysis to justify that DSSL enjoys better downstream performance. Extensive experiments on various types of graph benchmarks demonstrate that our proposed framework can significantly achieve better performance compared with competitive self-supervised learning baselines.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-structured data is ubiquitous in real world such as social networks, knowledge graphs, and molecular structures. In recent years, graph neural networks (GNNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> have been proven to be powerful in node representation learning over graph-structured data and have achieved state-of-the-art performance across various tasks such as node classification. Typically, GNNs are trained with annotated labeled data in a supervised manner. However, collecting labeled data is expensive and impractical in many applications, especially for those requiring domain knowledge for annotation, such as medicine and chemistry <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b16">17]</ref>. Moreover, supervised learning with labeled data may suffer from problems of less-transferrable, over-fitting and poor generalization particularly when the task-specific labels are scarce <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Recently, self-supervised learning (SSL) provides a promising learning paradigm that reduces the dependence on manual labels in the image domain <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref>. Compared to image data, there are unique challenges of designing self-supervised learning schemes for graph-structured data since nodes in the graph are correlated with each other rather than completely independent, and geometric structures are essential and heavily impact the performance in downstream tasks <ref type="bibr" target="#b24">[25]</ref>. A number of recent works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b54">55]</ref> have studied graph self-supervised learning and confirm that it can learn transferrable and generalizable node representations without any labels. Typically, there are two main self-supervised schemes to capture structure information in graphs <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b38">39]</ref>. The first scheme is reconstructing the vertex adjacency following traditional network-embedding methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40]</ref>, which learns an encoder that imposes the topological closeness of nodes in the graph structure on latent representations. The key assumption behind this scheme is that neighboring nodes have similar representations <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b39">40]</ref>. However, this assumption over-emphasizes proximity <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b52">53]</ref> and does not hold true for heterophlic and non-homophilous (mixing) graphs where linked nodes may have dissimilar features and different class labels. In comparison, contrastive  We can find nodes with similar labels typically have similar neighborhood patterns in all types of graph. This assumption is more general than the standard homophily assumption where nodes with similar semantic label typically be linked with each other. learning methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b40">41]</ref> constructs two graph views via the stochastic augmentation and then learns representations by contrasting views with information maximization principle. While these contrastive methods can capture structure information without directly emphasizing proximity, their performance heavily relies on the topology augmentation and negative sample mining <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b42">43]</ref>. Importantly, conducting augmentation and negative sample mining for non-homophilous or even extreme heterophlic graphs is difficult since linked nodes may be dissimilar and nodes with high similarities might be farther away from each other. As shown in our experiments in Table <ref type="table" target="#tab_1">1</ref>, most of contrastive learning methods fail to generalize to non-homophilous graphs. Although there are many efforts <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b50">51]</ref> trying to deal with the non-homophilous graph by designing new GNN models, they are still supervised learning approaches and heavily rely on the annotated labels, which limits their applications where labeled data is expensive and unavailable. Hence, the above problems pose an important and challenging research question: How to design an effective self-supervised scheme for node representation learning in non-homophilous graphs?</p><p>We approach this question by investigating whether one can take advantage of neighborhood strategies of nodes to help the self-supervised learning for non-homophilous graphs. Our key motivation is that nodes with similar neighborhood patterns should have similar representations. In other words, we expect that the neighborhood distributions can be exploited to distinguish node representations. Our assumption is more general than the standard homophily assumption. For instance, while the gender prediction in common dating networks lacks homophily (individual with gender male has a preference of making friends with people whose identities are female) <ref type="bibr" target="#b0">[1]</ref>, neighborhood distribution are very informative to the node gender labels, i.e., nodes with similar neighborhoods are likely to be similar. Figure <ref type="figure" target="#fig_1">1</ref> illustrates this motivation.</p><p>While the motivation is straightforward, there are two challenges to achieve it. The first challenge is how to capture the local neighborhood distribution in a self-supervised learning manner. The neighborhood distributions typically follow heterogeneous and diverse patterns. For instance, as shown in Figure <ref type="figure" target="#fig_4">2</ref> (c), a node usually connects with others due to the complex interaction of many latent factors, and therefore possesses distribution consisting of local mixing patterns wherein certain parts of the neighborhood are homophilous while others non-homophilous. Since there is no way to directly access the latent strategies of neighborhoods, the lack of supervision obstructs us from modeling the distribution of neighborhood. The second challenge is capturing the long-range semantic dependencies in self-supervised learning objectives. As shown in Figure <ref type="figure" target="#fig_4">2</ref>, in non-homophilous graphs, nodes with high semantic and local structural similarities might be farther away from each other. For this reason, global semantic information is the objective that we would incorporate for self-supervised learning, due to its favorable performance in downstream task.</p><p>This paper presents a new self-supervised framework: the decoupled self-supervised learning (DSSL), which attempts to achieve a good balance among these two challenges. At the core of DSSL is the latent variables, which empower model the flexibility to decouple the heterogeneous and diverse patterns in local neighborhood distributions and capture the global semantic dependencies in a coherent and synergistic framework. Our contributions can be summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose a unified DSSL framework for performing self-supervised learning on non-homophilous graphs, which can leverage both useful local structure and global semantic information. <ref type="bibr" target="#b1">(2)</ref> We develop an efficient training algorithm based on variational inference to simultaneously infer the latent variables and learn encoder parameters. <ref type="bibr" target="#b2">(3)</ref> We analyze the properties of DSSL and theoretically show that the learned representations can achieve better downstream performance. <ref type="bibr" target="#b3">(4)</ref> We conduct experiments on real-world non-homophilous graphs including large-scale ones and the results demonstrate the effectiveness of our self-supervised learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Non-homophilous Graphs. Non-homophilous is known in many settings such as online transaction networks <ref type="bibr" target="#b30">[31]</ref>, dating networks <ref type="bibr" target="#b0">[1]</ref> and molecular networks <ref type="bibr" target="#b56">[57]</ref>. Recently, various GNNs have been proposed to   deal with non-homophilous graphs with different methods such as potential neighbor discovery <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19]</ref>, adaptive message propagation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49]</ref>, exploring high-frequency signals <ref type="bibr" target="#b1">[2]</ref> and higher-order message passing <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b4">5]</ref>. Despite their success, they typically consider the semi-supervised setting and are trained with task-specific labeled data; while in practice, labels are often limited, expensive, and even inaccessible.</p><formula xml:id="formula_0">+ + + (b) (c) V2 V1 (d) V1 V2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K Z j t 2 F + l U O t 6 N q O w D 3 G F m i G k K Z k = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V V I R d V l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k U S i m I W 3 / A r f 6 U + A f 6 F 9 4 Z U 1 C L 6 I Q k Z 8</formula><formula xml:id="formula_1">E i g Q + G A J K w B x s x P U 2 U Y C E i r o 0 J c Y I Q 1 3 G G K X K k T S i L U Y Z N 7 J C + f d o 1 U z a g v f K M t d q l U z x 6 B S l N H J A m p D x B W J 1 m 6 n i i n R X 7 m / d E e 6 q 7 j e n v p F 4 + s R I D Y v / S z T L / q 1 O 1 S P R w p m v g V F O k G V W d m 7 o k u i v q 5 u a X q i Q 5 R M Q p 3 K W 4 I O x q 5 a z P p t b E u n b V W 1 v H 3 3 S m Y t X</formula><formula xml:id="formula_2">B R O x F W m X m Z u Q t 0 o Q F d p d P S r 0 Y X Y = " &gt; A A A C z X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d V l 0 4 8 4 K 9 o F t k S S d t q F 5 M Z k I p d a t P + B W f 0 v 8 A / 0 L 7 4 x T U I v o h C R n z r 3 n z N x 7 3 S T w U 2 F Z r z l j b n 5 h c S m / X F h Z X V v f K G 5 u 1 d M 4 4 x 6 r e X E Q 8 6 b r p C z w I 1 Y T v g h Y M + H M C d 2 A N d z h m Y w 3 b h l P / T i 6 E q O E d U K n H / k 9 3 3 M E U d f t 0 B E D t z c e T m 6 K J a t s q W X O A l u D E v S q x s U X t N F F D A 8 Z Q j B E E I Q D O E j p a c G G h Y S 4 D s b E c U K + i j N M U C B t R l m M M h x i h / T t</formula><p>In contrast, in this paper, we study the problem of self-supervised learning: learning generalizable and transferable node representations without relying on labels for non-homophilous graphs.</p><p>Self-supervised Learning on Graphs. Self-supervised learning holds great promise for improving representations when labeled data are scarce <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref>. In graph domain, earlier combinations of GNNs and self-supervised learning involve GraphSAGE <ref type="bibr" target="#b12">[13]</ref>, VGAE <ref type="bibr" target="#b23">[24]</ref> and Graphite <ref type="bibr" target="#b11">[12]</ref>, which typically follow the traditional network-embedding methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11]</ref> and adopt the link reconstruction or random walk principle. Since these methods over-emphasize node proximity at the cost of structural information <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b52">53]</ref>, various graph contrastive learning methods have been proposed <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b40">41]</ref>, which aim to learn representations by contrasting representation under differently augmented views and have achieve promising performance. However, they heavily rely on complex data-or task-specific augmentations, which are prohibitively difficult for non-homophilous graphs. The standard augmentation scheme usually have limited performance for non-homophilous graphs as shown in our experiments. There are also some recent works trying to design heuristic pretext tasks <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref> on graphs, however, for non-homophilous graphs, building hand-crafted pretext tasks for each node tends to be infeasible when we know little knowledge underlying graph or the node has diverse context. Our work differs from the above methods and aims to answer the question of how to design effective self-supervised learning scheme for non-homophilous graphs.</p><p>Disentangled graph learning. Our work is also related to but different from existing disentangled graph learning that aims to disentangle latent factors in the graph. There are a couple of works that explore the disentangled factors in the node-level <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29]</ref>, edge-level <ref type="bibr" target="#b55">[56]</ref> and graph-level <ref type="bibr" target="#b51">[52]</ref>. Whereas, these methods require task-specific labels that can be extremely scarce for graph datasets. By contrast, we address the problem of learning generalizable and transferrable node representations on non-homophilous graphs without labels. A recent work called disentangled contrastive learning <ref type="bibr" target="#b25">[26]</ref> learns disentangled graph representations without labeled graph. Nevertheless, they are still based on contrastive learning which requires prefabricated augmentations. By contrast, our framework is a generative model and does not rely on graph augmentations. Moreover, their goal is to conduct graph-level classification tasks which are different from ours. We instead tackle a node-level representation problem on non-homophilous graphs where connected nodes may not be similar to each other and we decouple the diverse neighborhood context of a node with a generative process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decoupled Self-supervised Learning</head><p>In this section, we describe our problem setting, and demonstrate our approach. An illustration of and inference processes is depicted in Figure <ref type="figure" target="#fig_4">2</ref> (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We consider a graph G = (V, E), where V is a set of |V| = N nodes and E ? V ? V is a set of |E| edges between nodes. A ? {0, 1} N ?N is the adjacency matrix of G. The (i, j)-th element A ij = 1 if there exists an edge (v i , v j ) between node v i and v j , otherwise A ij = 0. The matrix X ? R N ?D is used to describe node features. The i-th row of X, i.e., x i , is the feature vector of node v i . Given the graph G = (X, A), the objective of self-supervised node representation learning is to learn an encoder function f ? (X, A) : R N ?N ? R N ?D ? R N ?D where ? denotes the set of its parameters, such that the representation of node v i , i.e., f ? (X, A)[i], can be used for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Probabilistic Framework</head><p>In this section, we introduce our framework, decoupled self-supervised learning (DSSL), which can learn meaningful node representations for non-homophilous graphs by capturing its intrinsic graph structure. The core idea of DSSL is to model the distributions of node neighbors via a mixture generative process in the representation learning scenario. Specifically, we model the generation of neighbors by assuming each node has latent heterogeneous factors which are utilized to make connections to its different neighbors. Intuitively, the factor denotes various reasons behind why two nodes are connected. For instance, two nodes in a school network will be connected depending on some factors such as colleagues, friends or classmates; in protein networks, even if they do not have similar features, different amino acid types are likely to be connected due to various interactions.</p><p>Formally, let f ? (X, A)[i] = v i and f ? (X, A)[j] = z j be the representations of nodes v i and v j , respectively.</p><p>Here we utilize different notations, i.e., v and z to distinguish between central and neighbor nodes since each node plays two roles: the node itself and a specific neighbor of other nodes. Our goal is to find the encoder parameter ? which maximizes the likelihood of distribution p(z j |v i ; ?) on central node and its neighbor. To model the unobserved factors, we associate every node v i with a discrete latent variable k to indicate to which factor v i has. Assume that there are K factors in total, the log-likelihood of node neighbors v i can be written by marginalizing out the latent variables:</p><formula xml:id="formula_3">LDSSL(?) = 1 |N (i)| j?N (i) log[p ? (zj|vi)] = 1 |N (i)| j?N (i) log K k=1 p ? (zj|vi, k)p ? (k|vi) ,<label>(1)</label></formula><p>where N (i) is the set of out-neighbors of v i , the distribution p ? (k|v i ) indicates the assignment of latent semantics over central node representation v i , and p ? (z j |v i , k) is the probability that node v i and its neighbor v j are connected under factor k. Unlike previous works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref>, which directly encourages nearby nodes to have similar representations, we provide an alternative way to model node neighbors, and seek to decouple their latent relationship without any prior on neighbor partitions, so that the model is more generalizable for non-homophilous graphs. In Eq. ( <ref type="formula" target="#formula_3">1</ref>), probabilities p ? (k|v i ) and p ? (z j |v i , k) are not specified, and involve latent variables. To make it solvable, we introduce the following generative process.</p><p>Let ? k and ? k be the mean and variance of the latent mixture component k, and ? k be its corresponding mixture probability. The generation of a node and its neighbor typically involves three steps: (1) draw a latent variable k from a categorical distribution p(k) on all mixture components, where p(k) is usually defined as uniform distribution p(k) = 1 K for unknown graphs and better generalization, (2) draw the central node representation v i from the Gaussian distribution p ? (v i |k) = N (v i ; ? k , ? k ), and (3) draw the neighbor representation z j from Gaussian distribution p ? (z j |v i , k) = N (z j ; ? zj , ? j ) where the mean depends both on central representation and latent variable:</p><formula xml:id="formula_4">? zj = v i + g ? (k).</formula><p>Here the projector g ? (?) denotes another single fully connected network which embeds latent variable k (one-hot vector) to the representation space. In practice, to reduce complexity, we consider using isotropic Gaussian with tied variance, i.e., ?k : ? k = I? 2 1 and ?j : ? j = I? 2 2 where I is the identity matrix, ? 1 and ? 2 are hyperparameters. In alignment with this generative process, the joint distribution of observed links and the latent variable can be written as:</p><formula xml:id="formula_5">p ? (v i , z j , k) = p ? (z j |v i , k)p ? (v i |k)p(k).<label>(2)</label></formula><p>Intuitively, p ? (v i |k) can be viewed as the probability for node representation under the k-th mixture component. Regarding p ? (z j |v i , k) = N (z j ; ? zj , ? j ), this formulation is inspired from knowledge graph embedding <ref type="bibr" target="#b2">[3]</ref>, where two entities should be close to each other under a certain relation operation.</p><p>However, unlike knowledge graph embedding, the relation context is latent variable and not observed in our unsupervised setting. Intuitively, instead of enforcing exact representation alignment of two linked nodes <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40]</ref>, our design can relax this exact homophily assumption and account for the semantic shift of representations between the central node and its neighbors via the inferred latent context k.</p><p>Now, the posterior probability p ? (k|v i ) in Eq. ( <ref type="formula" target="#formula_3">1</ref>) can be derived by using Bayesian rules as follows:</p><formula xml:id="formula_6">p ? (k|vi) = p ? (vi|k)p(k) K k =1 p ? (vi|k )p(k ) = N (vi; ? k , I? 2 1 )p(k) K k =1 N (vi; ? k , I? 2 1 )p(k ) ,<label>(3)</label></formula><p>where ? = {? k } K k=1 can be treated as a set of trainable prototype representations, which are the additional distribution parameters. This posterior probability represents the soft semantic assignment of the learned representation to the prototypes, which makes the node with similar semantic properties be close to its prototype and encode both the inter-and intra-cluster variation. Substituting p ? (k|v i ) and p ? (z j |v i , k) in Eq. ( <ref type="formula" target="#formula_3">1</ref>) with the specified probabilities, we get final objective L DSSL (?, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evidence Lower Bound</head><p>Given the framework above, we are interested in: (i) learning the model parameters ? and ? by maximizing the log-likelihood L DSSL (?, ?), and (ii) inferring the posterior of latent variable k for each observed links. However, it is computationally intractable to directly solve these two problems due to the latent variables. To solve this, we resort to amortized variational inference methods <ref type="bibr" target="#b22">[23]</ref>, and maximize the evidence lower-bound (ELBO) of Eq. ( <ref type="formula" target="#formula_3">1</ref>), i.e., L DSSL (?, ?) ? L DSSL (?, ?, ?):</p><formula xml:id="formula_7">L(?, ?, ?) = 1 |N (i)| j?N (i) E q ? (k|v i ,z j ) [log p ? (zj|vi, k) + log p ? (k|vi)] + H(q ? (k|vi, zj)),<label>(4)</label></formula><p>where q ? (k|v i , z j ) is the introduced variational distribution parameterized by ? and H(?) is the entropy operator. z j is included in this variational posterior so that the inference is also conditioned on the neighborhood information. We derive this ELBO in Appendix A.1. Maximizing this ELBO w.r.t. {?, ?, ?} is equivalent to (i) maximize L DSSL (?, ?) and to (ii) make variational q ? (k|v i , z j ) be close to true posterior. Plugging the parameterized probabilities into this ELBO, we obtain the following loss to minimize (see Appendix A.2):</p><formula xml:id="formula_8">L = 1 |N (i)| j?N (i) E q ? (k|v i ,z j ) vi + g ? (k) -zj 2 2 -? 2 2 E q ? (k|v i ) log exp v i ? ? k /? 2 1 K k =1 exp v i ? ? k /? 2 1 -H(q ? (k|vi, zj)),<label>(5)</label></formula><p>where q ? (k|v i ) = 1/|N (i)| j?N (i) q ? (k|v i , z j ) is the posterior probability of semantic assignment for central node v i , by aggregating all its neighbors. Thus, the first term (denoted as L local ) in the loss encourages the model to reconstruct the local neighbors while considering different semantic shifts captured by latent variable k (see Figure <ref type="figure" target="#fig_5">2 (b)</ref>). The second term (denoted as L global ) encourages the model to perform clustering with learned representation where possible, i.e., seeking to push the representation v i to its closest prototype cluster (see Figure <ref type="figure" target="#fig_4">2</ref> (c)). The final entropy term makes the model choose to have high entropy over q ? (k|v i , z j ) such that all of the K-channel losses must be low. Overall, this loss can capture global semantic similarities over neighborhoods and learn to decouple different latent patterns in the local neighbors.</p><p>Regarding the variaitonal distribution q ? (k|v i , z j ), we model it as categorical distribution since k is a discrete multinomial variable. Specifically, the representations v i and z j are encoded to a combined representation and then q ? (k|v i , z j ) is determined by an output softmax inference head as follows:</p><formula xml:id="formula_9">q ? (k|vi, zj) = exp(h ? ([vi; zj])[k]) K k =1 exp(h ? ([vi; zj])[k ]) ,<label>(6)</label></formula><p>where h ? denotes the inference predictor network parameterized by ? and [?, ?] denotes the concatenation operation.</p><formula xml:id="formula_10">h ? ([v i ; z j ])[k]</formula><p>indicates the k th element, i.e., the logit corresponding the latent context k. Instead of introducing variational parameters individually, we consider the amortization inference, which fits a shared network to calculate each local parameter and is more efficient.</p><p>For the expectation terms in Eq. ( <ref type="formula" target="#formula_8">5</ref>), back-propagation through the discrete variable k is not directly feasible. We alleviate this by adopting the Gumbel-Softmax estimator <ref type="bibr" target="#b17">[18]</ref>, which provides a continuous differentiable approximation for drawing samples from a categorical distribution. Specifically, for each sample, a latent cluster vector c ? (0, 1) K is drawn from a reparameterization distribution:</p><formula xml:id="formula_11">c[k] = exp((h ? ([vi; zj])[k] + [k])/?) K k =1 exp((h ? ([vi; zj])[k] + [k ])/?) ,<label>(7)</label></formula><p>where [k] is i.i.d drawn from the Gumbel(0, 1) distribution and ? is a temperature. With this reparameterization trick, we can obtain the surrogate</p><formula xml:id="formula_12">E q ? (k|vi,zj ) [ v i + g ? (k) -z j 2 2 ] E [ v i + g ? (c) -z j 2 2</formula><p>] and the gradients are estimated with Monte Carlo. The expectation term over q ? (k|v i ) can be similarly estimated. Thus, {?, ?, ?} in Eq. ( <ref type="formula" target="#formula_8">5</ref>) can be efficiently solved by gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Algorithm Optimization</head><p>The overall optimization steps involves simultaneously training (1) the encoder f ? , (2) the projector g ? , (3) the inference predictor h ? and (4) the prototype vectors ?. The most canonical way to update the parameters is gradient descent. However, we observe that stochastically updating all parameters suffers from two problems: (1) Since we do not rely on negative samples, the objective admits trivial solutions, e.g., outputting the same representation for all nodes. (2) Updating prototypes without any constraints will lead to a degenerate solution, i.e., all nodes are assigned to a single cluster.</p><p>To address the issue of trivial solutions, inspired by the recent works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10]</ref>, we consider an asymmetric encoder architecture which includes online and target encoders. Specifically, for each node pair (v i , v j ), the online encoder f ? produces the representation of the central node v i ; while the target encoder f ? is used to produce the representation of its neighbor z j . Importantly, the gradient of loss is only used to update the online encoder f ? , while being blocked in the target encoder. The weights of the target encoder ? are instead undated via exponential moving average of online encoder:</p><formula xml:id="formula_13">[?, ?, ?] ? [?, ?, ?] -? (? ?,?,? L) , ? ? ? ? + (1 -? )?,<label>(8)</label></formula><p>where ?(?) indicates a stochastic optimizer and ? ? [0, 1] is the target decay rate. This update introduces an asymmetry between two encoders that prevents collapse to trivial solutions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41]</ref>. To alleviate the second issue, besides the stochastic update, we also apply an analytical global update for the prototype vectors ? = {? i } K i=1 at the end of each training epoch to avoid a degenerate solution:</p><formula xml:id="formula_14">? k = N i=1 ?i(k) ? vi N i=1 ?i(k) ? vi 2 2</formula><p>, where ?i(k</p><formula xml:id="formula_15">) = 1/|N (i)| j?N (i) q ? (k|vi, zj).<label>(9)</label></formula><p>The detailed derivation is provided in Appendix A.3. Intuitively, ? i (k) reflects the degree of relevance of node v i to the k th prototype. Instead of only updating the prototypes in a mini-batch, we also aggregate all the representation as the prototype based on the soft assignment probability.</p><p>After the training is finished, we only keep the online encoder f ? for the downstream task. Our full algorithm and network are provided in Appendix B. The time complexity of DSSL is O(N ? d) where d is the average node degree. Note that we omit the complexity introduced by the GNN encoder and dot-product computation, as it is orthogonal to the SSL design. Compared to the recent SSL approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b14">15]</ref> which requires computation and memory quadratic in the number of nodes, our method scales linearly in the size of nodes, thus is trained in a more efficient way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Theoretical Analysis</head><p>In this section, we provide theoretical analysis of the proposed framework. We first present the connection between the proposed objective and the mutual information maximization, then show that the learned representations by our objective provably enjoy good downstream performance. Due to the space limitation, all proofs of theorems and corollaries are provided in Appendix C.</p><p>We denote the random variable of the input graph as g and the downstream label as y. For clarity, we omit subscript i in what follows. Note that downstream tasks could be the node classification or clustering. From an information-theoretic learning perspective, a desirable way is to maximize the mutual information I(v, y) between the node representation v and its downstream label y. However, due to the lack of the downstream label, self-supervised learning resorts to maximizing I(v, s) where s is the designed self-supervised signal for different methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b7">8]</ref>. In our method, we actually has two self-supervised signal: the global semantic cluster information inferred by k and the local structural roles captured by the representations of the neighbors z</p><formula xml:id="formula_16">= {z i |v i ? N (v)} of node v.</formula><p>Then, we can interpret our objective in Eq. ( <ref type="formula" target="#formula_8">5</ref>) from the information maximization perspective: Theorem 1. Optimizing Eq. ( <ref type="formula" target="#formula_8">5</ref>) is equivalent to maximize the mutual information between the node representation v and the global semantic signal k, and maximize the conditional mutual information between v and the local signal z, conditioned on the global signal k. Formally, we have :</p><formula xml:id="formula_17">max ?,?,? L ? max v I(v; k) + I(v; z|k) = I(v; k, z). (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>This theorem suggests that we essentially combine both local structure and global semantic information as the self-supervised signal and maximize the mutual information between the representation v and their joint distribution (k, z). Next, we discuss how the learned representation affect the downstream task y based on the information bottleneck principle <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref>. The rationality of self-supervised learning is that the task-relevant information lies mostly in the shared information between the input and the self-supervised signals <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref>. Specifically, we formulate our lightweight and reasonable assumption below, which serves as a foundation for our analysis. Assumption 1. Nodes with similar labels should have similar "local structural roles" and "global semantic clusters". In this work, we equate "local structure" with the 1-hop neighborhood and "global semantic" with the clustering membership of a node. Formally, we have the task-relevant information y left in g except that in joint self-supervised signal (z, k) is relatively small: I(g; y|z, k) ? .</p><p>Intuitively, this assumption indicates that most of the task-relevant information in the graph is contained in the self-supervised signal. Based on this assumption, we give the following theorem which reveals that why the downstream tasks can benefit from the learned representations learned. </p><p>Theorem 2 shows that the gap of task-relevant information between supervised representation v sup = arg max v I(v, y) and self-supervised representation v joint is . Thus, we can guarantee a good downstream performance as long as the Assumption 1 is satisfied. It is noteworthy that jointly utilizing local structure and global semantic as the self-supervised signal is expected to contain more task information. As further enlightenment, we can relate Eq. ( <ref type="formula">38</ref>) with the Bayes error rate <ref type="bibr" target="#b43">[44]</ref>: Corollary 1. Suppose that downstream label y is a M-categorical random variable. Then we have the upper bound for the downstream Bayes errors P e v = E v 1max y?y P (? = y|v) on learned representation v, where ? is the estimation for label from our downstream classifier:</p><formula xml:id="formula_20">Th(P e vjoint ) ? log 2 + P e vsup ? log M + I(g; y|z, k) RHS vjoint (<label>12</label></formula><formula xml:id="formula_21">)</formula><p>where Th(x) = min{max{x, 0}, 1 -1/|M |} is a thresholded operation <ref type="bibr" target="#b43">[44]</ref>. This corollary says that our self-supervised signal has a tighter upper bound on the downstream Bayes error. Thus, we can expect that the representation learned by our objective function which utilizes both local structure and global semantic information have superior performance on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we empirically evaluate the proposed self-supervised learning method on real-world datasets, and analyze its behavior on graphs to gain further insights. We include additional experiments in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We perform experiments on widely-used homophilic graph datasets: Cora, Citeseer, and Pubmed <ref type="bibr" target="#b37">[38]</ref>, as well as non-homophilic datasets: Texas, Cornell, Wisconsin <ref type="bibr" target="#b31">[32]</ref>, Penn94 and Twitch. Penn94 and Twitch are two relatively large non-homophilous graph datasets proposed by <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27]</ref>. We provide the detailed descriptions, statistics and homophily measures of datasets in Appendix D.1.</p><p>Baselines. To evaluate the effectiveness of our proposed DSSL, we consider the following representative unsupervised and self-supervised learning methods for the node representation task, including Deepwalk <ref type="bibr" target="#b33">[34]</ref>, LINE <ref type="bibr" target="#b39">[40]</ref>, Struc2vec <ref type="bibr" target="#b35">[36]</ref>, GAE <ref type="bibr" target="#b23">[24]</ref>, VGAE <ref type="bibr" target="#b23">[24]</ref>, DGI <ref type="bibr" target="#b45">[46]</ref>, GraphCL <ref type="bibr" target="#b52">[53]</ref>, MVGRL <ref type="bibr" target="#b14">[15]</ref> and BGRL <ref type="bibr" target="#b40">[41]</ref>.</p><p>The detailed description of baselines and implementations are given in Appendix D.2.</p><p>Evaluation Protocol. We consider two types of downstream tasks: node classification and node clustering.</p><p>For node classification, we follow the standard linear-evaluation protocol on graphs <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b40">41]</ref>, where a linear classifier is trained on top of the frozen representation, and test accuracy (ACC) is used as a proxy for representation quality. For all datasets, we adopt the same random split with train/val/test split ratio of 60%/20%/20% for the training of downstream linear classifier <ref type="bibr" target="#b26">[27]</ref>. For the node clustering task, we perform K-means clustering on the obtained representations and set the number of clusters to the number of classes.</p><p>We utilize the normalized mutual information (NMI) <ref type="bibr" target="#b46">[47]</ref> as the evaluation metric for clustering.</p><p>Setup. For all self-supervised methods, we consider a two-layer GCN <ref type="bibr" target="#b24">[25]</ref> as the encoder and randomly initialize parameters. We also test methods with other encoders such as GAT <ref type="bibr" target="#b44">[45]</ref> (see Appendix E.1). we run experiments with 10 random splits and report the average performance. We select the best configuration of hyper-parameters based on accuracy on the validation. The detailed settings are given in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance Comparison</head><p>In this section, we conduct experiments on real-world graphs compared to state-of-the-art methods.  we have the following observations: (1) Generally, our DSSL achieves the best performance on both node clustering and classification tasks over the best baseline, demonstrating the effectiveness of DSSL on both homophilous and non-homophilous graphs and the robustness to different downstream tasks and graphs. Especially, DSSL achieves an relative improvement of over 6% and 4% on Texas and Squirrel compared to the best baselines (2) Our DSSL cannot achieve the best performance on Cora. This is reasonable since Cora is highly homophilous (see Appendix D.1), and the core design of augmentation in contrastive learning methods such as MVGRL and GraphCL enables them to be effective on homophilous graphs but failed on low-homophily settings. This supports our motivation in the introduction that it is relatively difficult to design effective graph augmentation on non-homophilous graphs due to their heterogeneous and diverse patterns. (3) When compared with GAE, VGAE, and DGI, DSSL consistently and significantly outperforms them. We deem that this improvement is mainly from the joint local semantic shift and global semantic clustering in DSSL, which is not included in existing methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study and Parameter Analysis</head><p>In this section, we conduct an ablation study to validate our motivation and design, and investigate the sensitivity of hyper-parameters. We refer readers to Appendix E.3 for more results.</p><p>Ablation Study. We consider the following ablations: We set posterior q ? (k|v i , z j ) = 1/K as uniform distribution for each link (w/ uniform posterior). We show the ablation study results in Table <ref type="table" target="#tab_5">2</ref>. L global alone does not provide many discriminative information and it does not perform very well. L local as a key component of DSSL alone produces better results than the baselines. We also observe that considering semantic shift and personalized posterior can improve the performance a lot which demonstrates our key motivations. The full model (last row) achieves the best performance, which illustrates that different components designed are complementary to each other.    Effect of Global Update on Prototypes. We also conduct the ablation studies to gain insights into the global update on prototypes. As shown in Figure <ref type="figure" target="#fig_6">3</ref>, we can observe that without Eq. ( <ref type="formula" target="#formula_15">9</ref>), the performance is not very good and we are stuck in bad local optima. As discussed in previous sections, a possible reason is that we will experience strong degeneracy if we only update prototype vectors with mini-batch training. This figure also demonstrates that DSSL can converge within a few hundred steps, which is efficient.</p><p>Effect of Asymmetric Architecture. We then explore the effect of target decay rate ? on the performance. Figure <ref type="figure" target="#fig_6">3</ref> shows the learning curves of DSSL on Squirrel and Twitch. We can observe that the best result is achieved at ? = 0.9.</p><p>When ? = 1.0, i.e., the target network is never updated, DSSL obtains a competitive result but is lower than ? = 0.9. This confirms that slowly updating the target network is crucial in obtaining superior performance. At the other extreme value ? = 0, the target network is the same as online network and DSSL demonstrates a degenerated performance and severe overfitting phenomena on Squirrel.</p><p>Hyper-parameters Analysis. We investigate the hyper-parameters most essential to our framework design, i.e., the standard deviation ? 1 and ? 2 , the temperature of the Gumbel Softmax ?, and the total number of factors K. The corresponding results are shown in Figure <ref type="figure" target="#fig_7">4</ref>. ? 2 1 resembles the temperature scaling in Eq. ( <ref type="formula" target="#formula_8">5</ref>). We observe ? 2  1 is better to be selected from 0.6 to 1.0, and a too small (e.g., 0.2) value may degenerate the performance in the all three datasets. case. ? 2 2 balances the local and global loss in Eq. ( <ref type="formula" target="#formula_8">5</ref>). We can find that having large values of ? 2 2 does not improve the performance, as the local loss plays more essential role in the proposed model. Further, we observe that DSSL is not very sensitive to the Gumbel softmax temperature ?, while a moderate hardness of the Softmax gives the best results. We also find that as K increases from 2 to 8, the performance of DGCL improves, which suggests the importance of decoupling latent factors. However, training with large K will lead performance slightly drop due to the introduced high model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization and Case Study</head><p>In this section, to understand how DSSLL uncovers the latent patterns in neighborhoods, we provide the visualization and case study results. In Figure <ref type="figure" target="#fig_10">5</ref> (a), we first calculate the cross-class neighborhood similarity (see Appendix E.4 for definition), which serves as the ground truth. If nodes with the same label share the same neighborhood distributions, the intra-class similarity should be high. In Figure <ref type="figure" target="#fig_10">5</ref> (b), we calculate average posterior distribution q ? (k|v i ) over all nodes on each class and provide cosine similarity of them. We can observe that our leaned latent factor distribution shares a similar pattern with the cross-class neighborhood similarity, which meets our expectation that the latent factors can capture the latent semantic information related to neighborhood patterns. To evaluate if DSSL can learn the semantic shift of between different neighbors, we plot the latent factor, i.e., q ? (k|v i , z j ) for individual links. In Figures <ref type="figure" target="#fig_16">5 (b)</ref> and<ref type="figure">(c)</ref> , we plot the subgraph of a randomlyone selected node, and the distribution q ? (k|v i , z j ) (K=4) and use different colors to indicate different labels. We find that similar neighbors over class generally have a similar distribution, while different types of links exhibit different latent distributions. This observation matches our motivation that our latent factors can decouple the diverse semantics in the local neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we study the problem of learning unsupervised node representations on non-homophilous graphs data. We present a novel decoupled self-supervised learning (DSSL) framework to decouple the diverse neighborhood context of a node in an unsupervised manner. Specifically, DSSL imitates the generative process of neighbors and explicitly models unobserved factors by latent variables. We show that DSSL can simultaneously capture the global clustering information and the local structure roles with the semantic shift. We theoretically show that DSSL enjoys a good downstream performance. Extensive experiments on several graph benchmarks showed that DSSL could learn meaningful class-discriminative representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The Omitted Derivations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The Evidence Lower-Bound</head><p>In this section, we provide the details of the lower-bound in Eq. ( <ref type="formula" target="#formula_7">4</ref>). By introducing the approximated posterior q ? (k|v i , z j ), the likelihood L(?, ?) in Eq. ( <ref type="formula" target="#formula_5">2</ref>) becomes:</p><formula xml:id="formula_22">L(?, ?) = 1 |N (i)| j?N (i) log K k=1 p ? (z j |v i , k)p ? (k|v i ) = 1 |N (i)| j?N (i) log K k=1 p ? (z j |v i , k)p ? (k|v i ) q ? (k|v i , z j ) q ? (k|v i , z j ) ? 1 |N (i)| j?N (i) K k=1 q ? (k|v i , z j )[log p ? (z j |v i , k) + log p ? (k|v i ) -log q ? (k|v i , z j )] = 1 |N (i)| j?N (i) E q ? (k|vi,zj ) [log p ? (z j |v i , k) + log p ? (k|v i ) -log q ? (k|v i , z j )],<label>(13)</label></formula><p>where the third step uses Jensen's inequality. This completes the derivation of Eq. ( <ref type="formula" target="#formula_7">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 The Optimization Loss</head><p>In this section, we derive the optimization loss for learning parameters in Eq. ( <ref type="formula" target="#formula_8">5</ref>). Introducing the posterior in Eq. ( <ref type="formula" target="#formula_6">3</ref>) and probabilities specification p ? (z j |v i , k) = N (z j ; ? zj , ? j ) to the negative of the lower-bounded likelihood in Eq. ( <ref type="formula" target="#formula_7">4</ref>), we have:</p><formula xml:id="formula_23">L = - 1 |N (i)| j?N (i) E q ? (k|vi,zj ) [log N (z j ; ? zj , ? j ) + log N (v i ; ? k , I?<label>2 1 )p(k)</label></formula><formula xml:id="formula_24">K k =1 N (v i ; ? k , I? 2 1 )p(k ) -log q ? (k|v i , z j )].<label>(14)</label></formula><p>Since we consider isotropic Gaussian with ? j = I? 2 2 , we have:</p><formula xml:id="formula_25">N (z j ; ? zj , ? j ) = 1 (2?) D /2 ? D 2 exp(- 1 2? 2 2 (z j -v i -g ? (k)) T ? (z j -v i -g ? (k))) = 1 (2?) D /2 ? D 2 exp(- v i + g ? (k) -z j 2 2 2? 2 2 )<label>(15)</label></formula><p>Similarly, for N v i ; ? k , I? 2 1 , we have:</p><formula xml:id="formula_26">N (v i ; ? k , I? 2 1 ) = 1 (2?) D /2 ? D 1 exp(- 1 2? 2 1 (v i -? k ) T ? (v i -? k )) = 1 (2?) D /2 ? D 1 exp(- 1 2? 2 1 (v i -? k ) T ? (v i -? k )) = 1 (2?) D /2 ? D 1 exp( (v i ? ? k -1) ?<label>2 1</label></formula><p>),</p><p>where the last equivalence is because we apply L2 normalization to v i and ? k , respectively. Substituting the corresponding terms in Eq. ( <ref type="formula" target="#formula_24">14</ref>) with Eq. ( <ref type="formula" target="#formula_25">15</ref>) and Eq. ( <ref type="formula" target="#formula_27">16</ref>), and removing the constant terms that are irrelevant to model parameters {?, ?, ?} (note that p(k) = 1 K ), we have:</p><formula xml:id="formula_28">L = 1 |N (i)| j?N (i) 1 2? 2 2 E q ? (k|vi,zj ) v i + g ? (k) -z j 2 2 -H(q ? (k|v i , z j )) - 1 |N (i)| j?N (i) E q ? (k|vi,zj ) log exp v i ? ? k /? 2 1 K k =1 exp v i ? ? k /? 2 1 (17) = 1 |N (i)| j?N (i) 1 2? 2 2 E q ? (k|vi,zj ) v i + g ? (k) -z j 2 2 -E q ? (k|vi) log exp v i ? ? k /? 2 1 K k =1 exp v i ? ? k /? 2 1 -H(q ? (k|v i , z j )) = 1 |N (i)| j?N (i) E q ? (k|vi,zj ) v i + g ? (k) -z j 2 2 -2? 2 2 E q ? (k|vi) log exp v i ? ? k /? 2 1 K k =1 exp v i ? ? k /? 2 1 -H(q ? (k|v i , z j )),</formula><p>where q ? (k|v i ) = 1/|N (i)| j?N (i) q ? (k|v i , z j ). In third line, we multiply the constant 2? 2  2 for all terms. By absorbing 2? 2  2 to the hyper-parameter ? 2 2 , we obtain the loss function in Eq. <ref type="bibr" target="#b4">(5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Derivation of Global Update</head><p>In this section, we derive the analytical global update of prototypes based on our optimization loss over all nodes. Specifically, we have:</p><formula xml:id="formula_29">? k ? arg min ? k N i=1 L = arg max ? k N i=1 E q ? (k|vi) log exp v i ? ? k /? 2 1 K k =1 exp v i ? ? k /? 2 1 = arg max ? k N i=1 E q ? (k|vi) log exp(v i ? ? k /? 2 1 )</formula><p>= arg max</p><formula xml:id="formula_30">? k N i=1 E q ? (k|vi) (v i ? ? k /? 2 1 ) .<label>(18)</label></formula><p>In the second line, following <ref type="bibr" target="#b47">[48]</ref>, we treat the normalizing term as the constant. Since we add apply L2 normalization to ? k : ? k = 1, we actually need to solve the Lagrangian of the objective function:</p><formula xml:id="formula_31">L(? k , ?) = max ?k,? N i=1 E q ? (k|vi) (v i ? ? k /? 2 1 ) + ?(1 -? k ? k ).<label>(19)</label></formula><p>Take the gradient over ? k with respect to L(? k , ?) and set it to zero, we have:</p><formula xml:id="formula_32">N i=1 q ? (k|v i )v i /? 2 1 -2? ? ? k = 0<label>(20)</label></formula><formula xml:id="formula_33">? ? k = N i=1 q ? (k|v i ) ? v i 2?? 2 1 = N i=1 ? i (k) ? v i 2?? 2 1 ,<label>(21)</label></formula><p>where ? i (k) = q ? (k|v i ) = 1/|N (i)| j?N (i) q ? (k|v i , z j ). By taking the gradient with respect to Lagrange multiplier ? and setting it to zero, we have: ? k ? k = 1. Combining ? k ? k = 1 and Eq. ( <ref type="formula" target="#formula_33">21</ref>), we have:</p><formula xml:id="formula_34">? = N i=1 ? i (k) ? v i 2? 2 1 .<label>(22)</label></formula><p>Further combining the above equation with Eq. ( <ref type="formula" target="#formula_33">21</ref>), we obtain the analytical global update:</p><formula xml:id="formula_35">? k = N i=1 ? i (k) ? v i N i=1 ? i (k) ? v i 2 2</formula><p>, where Lemma C.2. For any random variables Z and X, the mutual information I(X;Z) has a lower bound:</p><formula xml:id="formula_36">? i (k) = 1/|N (i)| j?N (i) q ? (k|v i , z j )<label>(23</label></formula><formula xml:id="formula_37">I(X; Z) = H(Z) -H(Z|X) = E q(Z) [log q(Z)] -E q(Z)q(X|Z) [log q(Z|X)] (26) ? max ? E q(Z) [log q(Z)] -E q(Z)q(X|Z) [log p ? (Z|X)],</formula><p>where p ? (Z|X) is an introduced variational discriminator with ? representing the parameters.</p><p>Proof.</p><p>E q(Z)q(Z|X) [log q(Z|X)] = max ? E q(Z)q(X|Z) [log p ? (Z|X)] + KL(q(Z|X)||p ? (Z|X))</p><formula xml:id="formula_38">? max ? E q(Z)q(X|Z) [log p ? (Z|X)].<label>(27)</label></formula><p>Plugging the inequality above into Eq. ( <ref type="formula">26</ref>) completes the proof.</p><p>Lemma C.3. For any random variables Z and X, the mutual information I(X, Z) has a lower bound:</p><formula xml:id="formula_39">I(X; Z) ? E p(X,Z) [f ? (x, z) -E q( B) [log z? B exp f ? (x, z)]] + log | B|<label>(28)</label></formula><p>where x and z are the sample instances, f ? ? R is a function parameterized by ? (e.g., a dot product between encoded representations of a x and z), and B is a set of samples drawn from a proposal distribution q( B).</p><p>The set B contains the positive sample b and | B| -1 negative samples.</p><p>Proof. The exact derivation of this InfoNCE bound can be founded in <ref type="bibr" target="#b34">[35]</ref>.</p><p>Now, we are ready to prove our Theorem 1. We first restate Theorem 1:</p><p>Theorem 1. Optimizing Eq. ( <ref type="formula" target="#formula_8">5</ref>) is equivalent to maximize the mutual information between the node representation v and the global semantic signal k, and maximize the conditional mutual information between v and the local signal z, conditioned on the global signal k. Formally, we have:</p><formula xml:id="formula_40">max ?,?,? L ? max v I(v; k) + I(v; z|k) = I(v; k, z).<label>(29)</label></formula><p>Proof. According to Lemma C.1, for any j we have:</p><formula xml:id="formula_41">I(v, z|k) = I(v, z 1 , ? ? ? , z N (v) |k) ? I(v, z j |k),<label>(30)</label></formula><p>which means</p><formula xml:id="formula_42">I(v, z|k) = 1 |N (v)| j?N (v) I(v, z|k) ? 1 |N (v)| j?N (v) I(v, z j |k).<label>(31)</label></formula><p>For I(v, z j |k), we can relate it to our local loss L local in the main paper:</p><formula xml:id="formula_43">max v I(v, z j |k) ? max v H(z j |k) -H(z j |k, v) ? min v,? -E q(v)q(zj )q(k|v,zj ) [log p ? (z j |k, v)] ? min ?,? -E q ? (v)q ? (zj )q ? (k|v,zj ) [log p ? (z j |k, v)] ? min ?,? -E v=f ? (g)[v],zj =f ? (g)[j],q ? (k|v,zj ) [log p ? (z j |k, v)] ? min ?,? -E v=f ? (g)[v],zj =f ? (g)[j],q ? (k|v,zj ) [log p ? (z j |k, v)] ? min ?,? 1 2? 2 2 E q ? (k|v,zj ) [ v + g ? (k) -z j 2 2 ],<label>(32)</label></formula><p>where we use Lemma C.1 in the second line and v and z j are the representations of node v and node z j through deterministic encoder f with input graph g = {X, A}. Combining Eqs. ( <ref type="formula" target="#formula_43">32</ref>) and ( <ref type="formula" target="#formula_42">31</ref>), we have:</p><formula xml:id="formula_44">min ?,? 1 N (v) j?N (v) 1 2? 2 2 E q ? (k|v,zj ) [ v + g ? (k) -z j 2 2 ] ? max v I(v, z|k).<label>(33)</label></formula><p>Thus the gap between supervised representation and supervised representation is I(g; y|k, z). Based on the property of mutual information, we have: I(g; y|k) = I(g; y, z|k) + I(g; y|k, z) ? I(g; y|k, z).</p><p>Similarly, we have I(g; y|z) ? I(g; y|k, z). Combining Eqs. ( <ref type="formula">40</ref>) and ( <ref type="formula" target="#formula_45">41</ref>), we have the following inequalities about the task-relevant information (note that I(g; y|k, z) ? based on our Assumption 1):</p><formula xml:id="formula_46">I(g; y) = max v I(v; y) ? I(v joint ; y) ? max(I(v local ; y), I(v global ; y)) ? I(g; y) -,<label>(42)</label></formula><p>which completes the proof</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Corollary 1</head><p>Restate Corollary 1:</p><p>Corollary 1. Suppose that downstream label y is a M-categorical random variable. Then we have the upper bound for the downstream Bayes errors P e v = E v 1max y?y P (? = y|v) on learned representation v, where ? is the estimation for label from our downstream classifier:</p><formula xml:id="formula_47">Th(P e vjoint ) ? log 2 + P e vsup ? log M + I(g; y|z, k) RHS vjoint<label>(43)</label></formula><p>where Th(x) = min{max{x, 0}, Proof. To prove this Corollary, we use the following inequalities <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref>:</p><formula xml:id="formula_48">Th(P e vjoint ) ? -log(1 -P e vjoint ) ? H(y|v joint )<label>(44)</label></formula><p>H(y|v sup ) ? log 2 + P e vsup log M.</p><p>For H(y|v joint ) and H(y|v sup ), we have the following relations:</p><formula xml:id="formula_50">H(y|v joint ) = H(y) -I(v joint ; y) = H(y) -I(v sup ; y) + I(g; y|z, k) = H(y|v sup ) + I(g; y|z, k),<label>(46)</label></formula><p>where we use Eq. ( <ref type="formula">40</ref>) in the second equality. Combining Eqs. ( <ref type="formula" target="#formula_48">44</ref>), ( <ref type="formula" target="#formula_49">45</ref>) and ( <ref type="formula" target="#formula_50">46</ref>), we have:</p><formula xml:id="formula_51">Th(P e vjoint ) ? log 2 + P e vsup ? log M + I(g; y|z, k) RHS vjoint .<label>(47)</label></formula><p>Further using that I(g; y | z, k) ? I(g; y|z) and I(g; y|z, k) ? I(g; y|k) in Eq. ( <ref type="formula" target="#formula_45">41</ref>), we have</p><formula xml:id="formula_52">RHS vjoint ? min(RHS v local , RHS v global ),<label>(48)</label></formula><p>which completes the proof.  We used the official implementation publicly released by the authors on Github for all baselines. We ran our experiments on GeForce RTX 2080 Ti. In all our experiments, we use the Adam optimizer <ref type="bibr" target="#b21">[22]</ref>. The projector g ? and predictor h ? are both the Multilayer Perceptron (MLP) with a single hidden layer. We also used stabilization techniques like batch normalization on Penn94 and Twitch datasets for all methods. We use ReLU activation in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Experimental setup</head><p>We use the same dataset splits and training procedure for all methods. We tune hyper-parameters for all models individually and randomly initialize the model parameters. For all methods, the hyperparameter search spaces are as follows: learning rate {0.001, 0.005, 0.01}, representation dimension {16, 32, 64}, L2 weight-decay {5e -4, 1e -4, 5e -6, 1e -6}. For our DSSL, we tune the following hyper-parameters: ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Results with Other Encoders</head><p>To further evaluate the effectiveness of the proposed DSSL, we consider the case where the learning method is implemented using another GNN encoder. Table <ref type="table" target="#tab_8">4</ref> shows the results of the selected baseline with GAT as the encoder. As shown in Table <ref type="table" target="#tab_8">4</ref>, our DSSL performs better than all the compared methods in most cases, which once again proves the effectiveness of DSSL and also shows that DSSL is robust to various encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Node Clustering Results</head><p>We provide the node clustering results in Table <ref type="table" target="#tab_9">5</ref>. As shown in Table <ref type="table" target="#tab_9">5</ref>, we can find that our DSSL can consistently improve the node clustering performance compared to the baselines on all datasets except the Cora. This observation once again verifies the effectiveness of DSSL in learning generalizable node representations. In addition, along with the node classification performance, these results show that DSSL can provide transferable and robust node representations for various downstream tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Additional Results on Ablation Study and Parameter Analysis</head><p>Ablation Study. We further conduct experiments to study the effects of the global update on prototypes and asymmetric architecture on homophily datasets. Figures <ref type="figure" target="#fig_16">7</ref> and<ref type="figure" target="#fig_15">9</ref> show the ablation results on Cora, Citeseer and Pubmed. These results also echo the collapse issue as discussed in the main text: without the global update and asymmetric architecture, there is significant performance drop on these three homophily datasets.</p><p>Parameter Analysis. We provide more results on the parameter analysis. Figure <ref type="figure" target="#fig_14">8</ref> shows the results on Citeseer, Cornell, and Squirrel datasets. We can observe that having large values of ? 2 2 also does not improve the overall performance, and DSSL is not very sensitive to the Gumbel softmax temperature ? for these three datasets. We also find that as K increases from 2 to 8 (2 to 16 for Citeseer), the accuracy of DSSL improves.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Additional Results on Visualization and Case Study</head><p>To further examine the learned latent factors, in Figure <ref type="figure" target="#fig_11">10</ref>, we provide the additional case study results on other datasets. We can find that, in most cases, the latent factors share a similar distribution for the same type of link, which illustrates its effectiveness in decoupling the underlying latent semantic meaning of different neighbors. This also interprets the reason why capturing the semantic shift can improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Discussion and Broader impact</head><p>In this paper, we proposed a new framework called decoupled self-supervised learning (DSSL) for unsupervised node representation learning on non-homophilous graphs. Compared with other contrastive learning methods, our DSSL does not need prefabricated augmentations. The theoretic analysis and extensive experiments show the effectiveness of our proposed DSSL.  Figure <ref type="figure" target="#fig_11">10</ref>: The visualization and case study results on other datasets. (best viewed on a computer screen and note that the latent distribution of each link need to be zoomed in to be better visible).</p><p>Limitation of the work. Despite the theoretical grounds and the promising experimental justifications, there is one limitation of the current work which we hope to improve in future work: our theoretic analysis requires some assumptions on the relationship between the downstream labels and the neighborhood distribution (Assumption 1). While, not surprisingly, we have to make some assumptions to expect good generalization for the unsupervised node representation learning, it is an interesting future direction to explore more relaxed assumptions than the ones used in this work.</p><p>Potential negative societal impacts. In this work, we propose a self-supervised learning framework for node representation learning which does not rely on annotated labels, which might reduce the need for label annotation and thus makes a few individuals who focus on labeling or annotating data unemployed. In addition, our learned representation may suffer from malicious adversaries who seek to extract node-level information about sensitive attributes. Thus, the robustness of DSSL could be further improved to prevent attacks from an adversary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Graph with homophilous pattern. (b) Graph with heterophlic pattern.(c) Graph with mixing pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration examples of different types of graphs.We can find nodes with similar labels typically have similar neighborhood patterns in all types of graph. This assumption is more general than the standard homophily assumption where nodes with similar semantic label typically be linked with each other. learning methods<ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b40">41]</ref> constructs two graph views via the stochastic augmentation and then learns representations by contrasting views with information maximization principle. While these contrastive methods can capture structure information without directly emphasizing proximity, their performance heavily relies on the topology augmentation and negative sample mining<ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b42">43]</ref>. Importantly, conducting augmentation and negative sample mining for non-homophilous or even extreme heterophlic graphs is difficult since linked nodes may be dissimilar and nodes with high similarities might be farther away from each other. As shown in our experiments in Table1, most of contrastive learning methods fail to generalize to non-homophilous graphs. Although there are many efforts<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b50">51]</ref> trying to deal with the non-homophilous graph by designing new GNN models, they are still supervised learning approaches and heavily rely on the annotated labels, which limits their applications where labeled data is expensive and unavailable. Hence, the above problems pose an important and challenging research question: How to design an effective self-supervised scheme for node representation learning in non-homophilous graphs?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>6 9 5 8 z c e 5 3 I 4 7 G 0 r N e M s b C 4 t L y S X c 2 t r W 9 s b u W 3 d 2 p x m A i X V d 3 Q C 0 X D s W P m 8 Y B V J Z c e a 0 S C 2 b 7 j s b o z v F D x + o i J m I f B j R x H r O 3 b / Y D 3 u G t L o m 5 b v i 0 H T m 8 y m n Y m f N r J F 6 y i p Z c 5 D 0 o p K C B d l T D / g h a 6 C O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>e T X M T v K t b 0 o B L P 8 c 5 D 2 p H x d J J 8 e j q u F A + T 0 e d x R 7 2 c U j z P E U Z l 6 i g S t 4 C j 3 j C s 3 F t j I 0 7 4 / 4 z 1 c i k m l 1 8 W 8 b D B 5 j A l Y o = &lt; / l a t e x i t &gt; v i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z m m m n w O 7 9V R A R g f c K t i 2 5 x m L j a A = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V d I i 6 r L o x m V F + 4 C 2 l i S d t r F 5 M Z k I N R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v V b o O p E w j N e M N j e / s L i U X c 6 t r K 6 t b + Q 3 t + p R E H O b 1 e z A D X j T M i P m O j 6 r C U e 4 r B l y Z n q W y x r W 6 F T G G z e M R 0 7 g X 4 p x y D q e O f C d v m O b g q i rt m e K o d V P b i f d 5 H r S z R e M o q G W P g t K K S g g X d U g / 4 I 2 e g h g I 4 Y H B h + C s A s T E T 0 t l G A g J K 6 D h D h O y F F x h g l y p I 0 p i 1 GG S e y I v g P a t V L W p 7 3 0 j J T a p l N c e j k p d e y R J q A 8 T l i e p q t 4 r J w l + 5 t 3 o j z l 3 c b 0 t 1 I v j 1 i B I b F / 6 a a Z / 9 X J W g T 6 O F Y 1 O F R T q B h Z n Z 2 6 x K o r 8 u b 6 l 6 o E O Y T E S d y j O C d s K + W 0 z 7 r S R K p 2 2 V t T x d 9 U p m T l 3 k 5 z Y 7 z L W 9 K A S z / H O Q v q 5 W L p s F g + P y h U T t J R Z 7 G D X e z T P I 9 Q w R m q q J E 3 x y O e 8 K x d a G P t T r v / T N U y q W Y b 3 5 b 2 8 A G k t Z W P &lt; / l a t e x i t &gt; z j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of (a) graphical model for DSSL. The latent variable k is used to instantiate a latent mixture-of-Gaussians v i , which is then decoded to z j and (b) a toy subgraph example of non-homophilous graphs. The motivation behind this work. (c) Our model encodes the semantic shift by decoupling local structure and (d) captures global semantic structure by semantic clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 2 .</head><label>2</label><figDesc>Let v joint = arg max v I(v; z, k), v local = arg max v I(v; k), and v global = arg max v I(v; z). Formally, we have the following inequalities about the task-relevant information: I(g; y) = max v I(v; y) ? I(v joint ; y) ? max(I(v local ; y), I(v global ; y)) ? I(g; y) -.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (Left) Performance w and w/o global update on prototypes. (Right) Performance with varying ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Hyper-parameter analysis on PubMed, Twitch and Texas. We provide results on other datasets in Appendix E.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(A1) We remove the key component of DSSL: the local reconstruction loss (w/o L local ). (A2) We remove global clustering loss to see if L local alone is still effective (w/o L global ). (A3) We remove the entropy term in Eq. (5) (w/o entropy). (A4) We remove the semantic shift term g ? (k) in L local (w/o semantic shift). (A5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: The visualization and case study results (best viewed on a computer screen and note that the latent distribution of each link need to be zoomed in to be better visible). More results can be found in Appendix E.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>) Algorithm 1 :</head><label>1</label><figDesc>Training Algorithm of DSSL Input: G = (V, E) Output: Online encoder network parameters ?. Initialize target encoder parameter ? = ? repeat Randomly select a mini-batch nodes from V for each v i in the batch do Randomly sample its neighbors N (i) Sample c with Gumbel softmax end L ? Eq. (5) [?, ?, ?] ? [?, ?, ?] -? ? ?,?,? L Update ? with momentum moving average: ? ? ? ? + (1? )? until convergence or reaching max iteration;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>LPFigure 7 :</head><label>7</label><figDesc>Figure 7: Performance on Cora, Citeseer and Pubmed with varying ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Hyper-parameter analysis on Citeseer, Cornell and Squirrel datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance on Cora, Citeseer and Pubmed w and w/o global update.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>( a )</head><label>a</label><figDesc>Visualization of subgraph (Cora id 6) (b) Visualization of subgraph (CiteSeer id 1) (c) Visualization of subgraph (Pubmed id 0) (d) Visualization of subgraph (Penn94 id 19) (e) Visualization of subgraph (Squirrel id 11) (f) Visualization of subgraph (Cornell id 8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Similarly, we can obtain the error upper bound of other representations v local and v global : RHS v local and RHS v global . Then, we have inequalities on error upper bounds : RHS vjoint ? min(RHS v local , RHS v global ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note><p><p>reports the average classification accuracy with the standard deviation on node classification after 10 runs. Since node clustering results have a similar tendency, we provide them in Appendix E.2. From Table</p>1,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Experimental results (%) with standard deviations on the node classification task. The best and second best performance under each dataset are marked with blue and underline, respectively.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Texas</cell><cell>Cornell</cell><cell>Squirrel</cell><cell>Penn94</cell><cell>Twitch</cell></row><row><cell cols="9">Deepwalk 77.14?0.82 67.85?0.79 79.38?1.22 42.31?2.21 41.55?3.12 37.54?2.19 56.13?0.46 66.37?0.11</cell></row><row><cell>LINE</cell><cell cols="8">78.93?0.55 68.79?0.41 80.56?0.92 48.69?1.39 43.68?2.17 38.92?1.58 57.59?0.17 67.23?0.27</cell></row><row><cell cols="9">Struc2vec 30.26?1.52 53.38?0.62 40.83?1.85 49.31?3.22 30.22?5.87 36.49?1.15 50.29?0.31 63.52?0.35</cell></row><row><cell>GAE</cell><cell cols="8">78.33?0.27 66.39?0.24 78.28?0.77 53.98?3.22 44.18?3.56 30.53?1.33 58.11?0.18 67.98?0.27</cell></row><row><cell>VGAE</cell><cell cols="8">80.59?0.35 69.90?0.57 81.33?0.69 50.27?2.21 48.73?4.19 29.13?1.16 58.29?0.21 65.09?0.08</cell></row><row><cell>DGI</cell><cell cols="8">84.17?1.35 71.80?1.33 81.65?0.71 58.53?2.98 45.33?6.11 26.44?1.12 53.68?0.19 66.97?0.25</cell></row><row><cell cols="9">GraphCL 84.28?0.91 72.46?1.79 81.96?0.73 48.67?4.37 47.22?4.50 22.53?0.98 58.43?0.31 68.37?0.16</cell></row><row><cell cols="9">MVGRL 85.21?1.18 72.13?1.04 82.33?0.88 51.26?0.38 51.16?1.67 38.43?0.87 57.22?0.17 66.03?0.26</cell></row><row><cell>BGRL</cell><cell cols="8">83.29?0.72 71.56?0.87 81.34?0.50 52.77?1.98 50.33?2.29 36.22?1.97 58.98?0.13 67.43?0.22</cell></row><row><cell>DSSL</cell><cell cols="8">83.06?0.53 73.51?0.64 82.98?0.49 62.11?1.53 53.15?1.28 40.51?0.38 60.38?0.32 69.81?0.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Node classification accuracy (%) on the Texas dataset. Ablation results for other datasets are given in Appendix E.3.</figDesc><table><row><cell>Ablation</cell><cell>Accuracy</cell></row><row><cell>A1 w/o local loss L local</cell><cell>25.18?1.31</cell></row><row><cell cols="2">A2 w/o global loss L global 59.34?2.76</cell></row><row><cell>A3 w/o entropy loss</cell><cell>60.57?1.27</cell></row><row><cell>A4 w/o semantic shift</cell><cell>56.19?1.25</cell></row><row><cell>A5 w/ uniform posterior</cell><cell>50.27?1.04</cell></row><row><cell>A2+A3</cell><cell>57.61?1.72</cell></row><row><cell>A2+A4</cell><cell>50.52?2.01</cell></row><row><cell>A2+A5</cell><cell>48.59?1.87</cell></row><row><cell>DGI</cell><cell>58.53?2.98</cell></row><row><cell>DSSL</cell><cell>62.11?1.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1 -1/|M |} is a thresholded operation [44]. Similarly, we can obtain the error upper bound of other representations v local and v global : RHS v local and RHS v global . Then, we have inequalities on error upper bounds : RHS vjoint ? min(RHS v local , RHS v global ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Statistics of used graph datasets .</figDesc><table><row><cell cols="2">Dataset # Nodes</cell><cell># Edges</cell><cell cols="3"># Classes # Features homophily</cell><cell>?</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,278</cell><cell>7</cell><cell>1,433</cell><cell>.766</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,552</cell><cell>6</cell><cell>3,703</cell><cell>.627</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,324</cell><cell>3</cell><cell>500</cell><cell>.664</cell></row><row><cell>Texas</cell><cell>183</cell><cell>309</cell><cell>5</cell><cell>1,793</cell><cell>.001</cell></row><row><cell>Cornell</cell><cell>183</cell><cell>295</cell><cell>5</cell><cell>1,703</cell><cell>.011</cell></row><row><cell>Squirrel</cell><cell>5,201</cell><cell>216,933</cell><cell>5</cell><cell>2,089</cell><cell>.025</cell></row><row><cell>Penn94</cell><cell>41,554</cell><cell>1,362,229</cell><cell>2</cell><cell>5</cell><cell>.046</cell></row><row><cell>Twitch</cell><cell>9,498</cell><cell>76,569</cell><cell>2</cell><cell>2,545</cell><cell>.142</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Experimental results (%) on the node classification task with GAT as the encoder. The best and second best performances under each dataset are marked with blue and underline, respectively.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Texas</cell><cell>Cornell</cell><cell>Squirrel</cell><cell>Penn94</cell><cell>Twitch</cell></row><row><cell cols="9">MVGRL 84.52?0.95 71.52?0.41 81.05?0.68 53.42?0.29 50.29?0.96 37.58?0.95 57.21?0.30 67.11?0.23</cell></row><row><cell>BGRL</cell><cell cols="8">83.91?0.25 72.15?0.42 80.12?0.52 51.02?1.10 48.97?1.03 35.33?1.12 56.52?0.25 66.21?0.33</cell></row><row><cell>DSSL</cell><cell cols="8">82.54?0.46 73.67?0.68 81.69?0.37 59.73?1.28 53.63?1.16 39.42?1.34 58.97?0.26 69.55?0.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Experimental results (%) with standard deviations on the node clustering task. The best and second best performance under each dataset are marked with blue and underline, respectively.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Texas</cell><cell>Cornell</cell><cell>Squirrel</cell><cell>Penn94</cell><cell>Twitch</cell></row><row><cell cols="9">Deepwalk 31.15?0.05 31.23?0.06 34.59?0.09 16.90?0.13 10.23?0.18 16.97?0.13 13.24?0.16 25.08?0.14</cell></row><row><cell>LINE</cell><cell cols="8">33.78?0.03 33.37?0.04 38.31?0.06 18.61?0.15 13.57?0.14 18.59?0.12 14.49?0.09 26.81?0.07</cell></row><row><cell cols="9">Struc2vec 20.17?0.05 19.31?0.08 15.13?0.03 22.14?0.10 9.18?0.12 14.85?0.15 12.58?0.17 22.49?0.14</cell></row><row><cell>GAE</cell><cell cols="8">37.43?0.05 32.45?0.05 34.38?0.06 26.97?0.11 15.39?0.13 11.73?0.08 16.70?0.15 27.45?0.17</cell></row><row><cell>VGAE</cell><cell cols="8">38.92?0.08 36.49?0.03 41.09?0.04 27.75?0.16 17.87?0.13 10.83?0.09 17.34?0.08 25.89?0.08</cell></row><row><cell>DGI</cell><cell cols="8">45.17?0.08 42.03?0.08 45.33?0.02 34.17?0.07 15.92?0.15 8.49?0.13 12.14?0.19 25.84?0.18</cell></row><row><cell cols="9">GraphCL 46.29?0.03 46.38?0.12 46.78?0.03 30.25?0.13 16.86?0.17 7.97?0.13 16.35?0.13 27.55?0.16</cell></row><row><cell cols="9">MVGRL 48.65?0.07 45.33?0.02 48.81?0.21 32.72?0.18 18.02?0.11 17.65?0.08 15.80?0.17 24.53?0.16</cell></row><row><cell>BGRL</cell><cell cols="8">47.35?0.03 44.78?0.04 47.21?0.07 33.59?0.15 19.74?0.14 15.13?0.09 17.49?0.13 25.69?0.09</cell></row><row><cell>DSSL</cell><cell cols="8">46.77?0.04 47.89?0.04 49.39?0.02 38.22?0.15 20.36?0.08 19.85?0.13 18.66?0.15 29.63?0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>2 1 ? {0.2, 0.4, 0.6, 0.8, 1.0}, ? 2 2 ? {0.2, 0.4, 0.6, 0.8, 1.0}, ? ? {0.2, 0.4, 0.6, 0.8, 1.0} and K ? {2, 4, 6, 8, 16, 32}.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 9 A z c B g 7 X / Y s T 2 9 N N y X i y R T C v z I = " &gt; A A A C y X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 r L o R n B T w T 6 g L Z J M p z U 2 y c R k I t b i y h 9 w q z 8 m / o H + h X f G F N Q i O i H J m X P v O T P 3 X j f y v U R a 1 m v O m J m d m 1 / I L x a W l l d W 1 4 r r G 4 1 E p D H j d S Z 8 E b d c J + G + F / K 6 9 K T P W 1 H M n c D 1 e d M d H q t 4 8 4 b H i S f C c z m K e D d w B q H X 9 5 g j i W p 0 R E / I w k W x Z J U t v c x p Y G e g h G z V R P E F H f Q g w J A i A E c I S d i H g 4 S e N m x Y i I j r Y k x c T M j T c Y 5 7 F E i b U h a n D I f Y I X 0 H t G t n b E h 7 5 Z l o N a N T f H p j U p r Y I Y 2 g v J i w O s 3 U 8 V Q 7 K / Y 3 7 7 H 2 V H c b 0 d / N v A J i J S 6 J / U s 3 y f y v T t U i 0 c e h r s G j m i L N q O p Y 5 p L q r q i b m 1 + q k u Q Q E a d w j + I x Y a a V k z 6 b W p P o 2 l V v H R 1 / 0 5 m K V X u W 5 a Z 4 V 7 e k A d s / x z k N G p W y v V + u n O 2 V q k f Z q P P Y w j Z 2 a Z 4 H q O I E N d T J + w q P e M K z c W p c G 7 f G 3 W e q k c s 0 m / i 2 j I c P F g u R W g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U b s x H s d + I 2 R x 9 u o 7 h Z I p P 6 p k I t 8 = " &gt; A A A C 2 H i c j V H L T s J A F D 3 U F + I L Z e m m k Z j g h h R i 1 C W J G 5 e Y y C M C I W 0 Z Y E J f a a c k B E n c G b f + g F v 9 I u M f 6 F 9 4 Z y y J S o x O 0 / b M u f e c m X u v F T g 8 E o b x m t K W l l d W 1 9 L r m Y 3 N r e 2 d 7 O 5 e P f L j 0 G Y 1 2 3 f 8 s G m Z E X O 4 x 2 q C C 4 c 1 g 5 C Z r u W w h j U 6 l / H G m I U R 9 7 0 r M Q l Y x z U H H u 9 z 2 x R E d b O 5 o D C 6 a b u m G F r 9 6 X j W n f L Z U T e b N 4 q G W v o i K C U g j 2 R V / e w L 2 u j B h 4 0 Y L h g 8 C M I O T E T 0 t F C C g Y C 4 D q b E h Y S 4 i j P M k C F t T F m M M k x i R / Q d 0 K 6 V s B 7 t p W e k 1 D a d 4 t A b k l L H I W l 8 y g s J y 9 N 0 F Y + V s 2 R / 8 5 4 q T 3 m 3 C f 2 t x M s l V m B I 7 F + 6 e e Z / d b I W g T 7 O V A 2 c a g o U I 6 u z E 5 d Y d U X e X P 9 S l S C H g D i J e x Q P C d t K O e + z r j S R q l 3 2 1 l T x N 5 U p W b m 3 k 9 w Y 7 / K W N O D S z 3 E u g n q 5 W D o p l i + P 8 5 V y M u o 0 9 n G A A s 3 z F B V c o I o a e U / w i C c 8 a 9 f a r X a n 3 X + m a q l E k 8 O 3 p T 1 8 A P H e l 4 U = &lt; / l a t e x i t &gt; p(k|v i ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + r r s F K R M 5 g I F H i V w y F c R J J t 8 l X o = " &gt; A A A C 1 X i c j V H L S s N A F D 2 N r 1 p f U Z d u g k V w V d I i 6 r L g x m U F + 4 C 2 l M l 0 2 o b m R T I p l N K d u P U H 3 O o v i X + g f + G d M Q W 1 i E 5 I c u b c e 8 7 M v d e J P D e R t v 2 a M 1 Z W 1 9 Y 3 8 p u F r e 2 d 3 T</p><p>+ e m e B e 3 p A a b P 9 s 5 C x o V 3 T z S K x e H 5 a q e t 7 q I X e z h g P p 5 j C r O U E O d v G / w i C c 8 K 2 3 l V r l T 7 j 9 T l U K u 2 c a 3 o T x 8 A P p x m F I = &lt; / l a t e x i t &gt; ? K &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><p>r y J s b X 6 o S 5 B A T J 3 G P 4 g l h T y m n 7 2 w o T a p q l 2 / r q P i b y p S s 3 H t 5 b o Z 3 e U t q s P 2</p><p>R y j h j p 5 X + I R T 3 i 2 T q x r 6 9 a 6 + 0 y 1 c k a z i W / L e v g A 7 e u R p w = = &lt; / l a t e x i t &gt;</p><p>R p X 6 W j 7 q I P e z j k O Z 5 g j o u 0 E D T V P m I J z x b l 5 a 0 J t b 0 U 2 o V c s 8 u v i 3 r 4 Q N n c p H h &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t t + 0 6 u d b 7 J K x c / I 4 T 9</p><p>x / Z 3 M y y d W Y k j s X 7 q P z P / q V C 0 S f R z p G l y q K d K M q o 5 n L q n u i r q 5 9 a k q S Q 4 R c Q r 3 K B 4 T 5 l r 5 0 W d L a x J d u + o t 0 / E X n a l Y t e d Z b o p X d U s a c O X 7 O H + C i 2 q 5 c l C u n u 0 X a 9 V s 1 H l s Y w c l m u c h a j j B K e r k f Y V 7 P O D R 6 B j X x o 1 x + 5 5 q 5 D L N J r 4 s 4 + 4 N 6 v y Y q Q = = &lt; / l a t e x i t &gt; g ? (c) Gumbel &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H 7 b m 0 8 B 8 N q q 5 + u g M</p><p>m X u v F 0 s / V Y 7 z P G F N T k 3 P z J b m 5 h c W l 5 Z X y q t r p 2 m U J V w 0 e S S j 5 N x j q Z B + K J r K V 1 K c x 4 l g g S f F m T c 8 0 P G z K 5 G k f h S e q F E s L g P W D / 2 e z 5 k i q l N e b w d M D</p><p>b X i e m l j c 2 t 7 p 7 y 7 1 0 z j T P j M 8 e M g F m 3 P T The overall training algorithm is shown in Algorithm 1. Concretely, for each iteration of DSSL, we randomly sample the nodes and its neighbors. Then we calculate the loss function with Gumbel softmax approximation. All online trainable parameters are updated with stochastic gradient descent (SGD), while those marked with ? are the target network counterparts to be updated with exponential moving average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 The Network Architecture of DSSL</head><p>The DSSL framework is illustrated in Figure <ref type="figure">6</ref>. DSSL proposes a self-supervised scheme that learns to encode the node using an online encoder along with a set of prototypes, inference projector g ? and predictor h ? . These form the online network. Learning is done by optimizing the local L local and global L global losses. To optimize the online parameters, a target encoder is used to produce presentations of neighbors, and updated by exponential moving average. We stop gradients on target encoder coming from the local loss to introduce asymmetry between the online and target encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Theoretical Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Theorem 1</head><p>To prove Theorem 1, we first present three Lemmas: Lemma C.1. For any random variables X, Y , Z and V , we have the following relations:</p><p>Proof. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I(X, Y</head><p>This completes the proof of the this Lemma.</p><p>The InfoNCE is related to our global loss. Using Lemma 26 on mutual information I(v; k), we have:</p><p>When B always includes all possible values of the latent variable k (i.e., | B| = K) and they are uniformly distributed, and setting</p><p>, we have:</p><p>Given the above, we have:</p><p>Combining Eqs. ( <ref type="formula">32</ref>) and ( <ref type="formula">36</ref>), we have</p><p>Omitting the entropy regularization in the loss L in Eq. ( <ref type="formula">5</ref>) completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Theorem 2</head><p>To prove Theorem 2, we first present the following Lemmas. Lemma C.4. For a representation v that is obtained with a deterministic GNN encoder f ? of input graph g with enough capacity, we have the following data processing Markov chain:</p><p>Proof. Since v is a deterministic function of g, we have the following conditional independence: (k, z) ? v|g and y ? v|g <ref type="bibr" target="#b7">[8]</ref>. This leads to the data processing Markov chain (k, z) ? y ? g ? v.</p><p>Lemma C.5. By optimizing loss in Eq. (5), the learned representation of v joint is both minimal and sufficient: v joint is sufficient v joint = arg max v I(v; k, z); v joint is minimal v joint = arg min v H(v|k, z).</p><p>Proof. We further have I(v; z|k) = H(v|k) -H(v|k, z) in which the entropy H(v|k) is a constant since p(v|k) is a Gaussian distribution. Thus, maximizing I(v; z|k) is equivalent to minimize H(v|k, z).</p><p>Combining this with Theorem 2, we can directly conclude that v joint is sufficient and minimal.</p><p>Next, we restate Theorem 2 and provide a complete proof with the help of the Lemmas above. </p><p>Proof. Based on Lemma C.4, we have the following data processing inequality <ref type="bibr" target="#b41">[42]</ref>:</p><p>Since v joint = arg max v I(z, k; v), we further have: I(k, z; v joint ) = I(k, z; g) and I(k, z; v joint ; y) = I(k, z; g; y). In addition, since v joint is minimal, we also have, I(v joint ; y|k, z) ? H(v joint |k, z) = 0.</p><p>Give the above, we have the following equality: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Datasets Description and Statistics</head><p>In our experiments, we use the following real-world datasets.</p><p>Cora, Citeseerm, and Pubmed are citation and high-homophily graphs, which are among the most widely used benchmarks for semi-supervised node classification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b44">45]</ref>. In these graphs, nodes are documents, and edges are citations. Each node is assigned a class label based on the research field. The features of each node are represented by a bag of words of its abstracts.</p><p>Texas and Cornell are low-homophily graphs representing links between web pages of the corresponding universities and originally collected by the CMU WebKB projects, where nodes and edges represent web pages and hyperlinks, respectively. We use the pre-processed datasets by <ref type="bibr" target="#b31">[32]</ref>.</p><p>Squirrel is the subgraph of web pages in Wikipedia discussing the related topics, collected by <ref type="bibr" target="#b36">[37]</ref>. The Squirrel graph is rather complex, with both homophily and heterophily combined.</p><p>Penn94 is a friendship network from the Facebook 100 networks of university students from 2005 <ref type="bibr" target="#b26">[27]</ref>, where nodes represent students and are labeled with the reported gender of students. The node features are major, second major/minor, dorm/house, year, and high school.</p><p>Twitch is a graph of relationships between accounts on the streaming platform Twitch. Node features are games liked, location, and streaming habits. Nodes are labeled with the explicit language used by a streamer. We utilize the pre-processed sub-graph Twitch-DE <ref type="bibr" target="#b27">[28]</ref>. In Twitch-DE, streamers that do not use explicit content (class 0) also often connect to streamers of class 1, which results in an overall non-homophilous structure.</p><p>We utilize the class-average homophily metric proposed recently by <ref type="bibr" target="#b26">[27]</ref> to measure the homophily level of graphs. Specifically, the metric is the fraction of edges that connect two nodes of the same class:</p><p>where [a] + = max(a, 0) and C is the total number of classes, and h k is the class-wise homophily metric:</p><p>in which d u is the number of neighbors of node u, and d</p><p>(ku) u</p><p>is the number of neighbors of u that have the same class label. Compared to the edge homophily metirc <ref type="bibr" target="#b56">[57]</ref>, this metric is less sensitive to the number of classes and the size of each class. The statistics of datasets are given in Table <ref type="table">3</ref>. We can find that the widely used citation graphs Cora, Citeseer, and Pubmed are highly homophilous, and other graphs show low homophily degrees and thus are non-homophilous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Baselines and Implementations</head><p>Deepwalk <ref type="bibr" target="#b33">[34]</ref> is a network embedding method that generates random walks from all nodes in the graph and then learns latent representations of nodes by treating walks as the equivalent of sentences with SkipGram.</p><p>LINE <ref type="bibr" target="#b39">[40]</ref> is a network embedding model that preserves both the first-order and second-order proximities.</p><p>Struc2vec <ref type="bibr" target="#b35">[36]</ref> is embedding method for learning representations for capturing the node structural identity. GAE <ref type="bibr" target="#b23">[24]</ref> is a GCN encoder trained by reconstructing adjacency matrix.</p><p>VGAE <ref type="bibr" target="#b23">[24]</ref> is a probabilistic version of GAE and is also trained by reconstructing the adjacency matrix. DGI <ref type="bibr" target="#b45">[46]</ref> is an unsupervised node representation method that maximizes the mutual information between node representations and graph summary.</p><p>GraphCL <ref type="bibr" target="#b52">[53]</ref> is a graph contrastive method that learn node representations by contrasting different augmentations of graphs.</p><p>MVGRL <ref type="bibr" target="#b14">[15]</ref> is a contrastive learning method by contrasting encoders from neighbors and a graph diffusion.</p><p>BGRL <ref type="bibr" target="#b40">[41]</ref> is a graph representation learning method that learns node representations by predicting alternative augmentations of the graph.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monophily in social networks introduces similarity among friends-of-friends</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Altenburger</surname></persName>
		</author>
		<author>
			<persName><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature human behaviour</title>
		<imprint>
			<biblScope unit="page" from="284" to="290" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3950" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting domain-specific features to enhance domain generalization</title>
		<author>
			<persName><forename type="first">Manh-Ha</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How well do self-supervised models transfer?</title>
		<author>
			<persName><forename type="first">Linus</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5414" to="5423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning robust representations via multi-view information bottleneck</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjan</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Forr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphite: Iterative generative modeling of graphs</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2434" to="2444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal graph convolutional networks</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiying</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Selfsupervised learning on graphs: Deep insights and new direction</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Node similarity preserving graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Disentangled contrastive learning on graphs</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New benchmarks for learning on nonhomophilous graphs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Independence promoted graph disentangled networks</title>
		<author>
			<persName><forename type="first">Yanbei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4916" to="4923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Netprobe: a fast and scalable system for fraud detection in online auction networks</title>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horng</forename><surname>Duen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Hp</forename><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel R Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial graph augmentation to improve graph contrastive learning</title>
		<author>
			<persName><forename type="first">Susheel</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large-scale representation learning on graphs via bootstrapping</title>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Azabou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><forename type="middle">L</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">Mtcaj</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Puja</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekdeep</forename><surname>Singh Lubana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.03220</idno>
		<title level="m">Augmentations in graph contrastive learning: Current methodological flaws &amp; towards better practices</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Demystifying self-supervised learning: An information-theoretical framework</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05576</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning how to propagate messages in graph neural networks</title>
		<author>
			<persName><forename type="first">Teng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Diverse message passing for attribute with heterophily</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Factorizable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zunlei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="20286" to="20296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks?</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10871" to="10880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">From canonical correlation analysis to self-supervised graph neural networks</title>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Exploring edge disentanglement for node classification</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11245</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7793" to="7804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">An empirical study of graph contrastive learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01116</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Prioritizing network communities</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rok</forename><surname>Sosi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
