<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STAMP: Short-Term Attention/Memory Priority Model for Session-based Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiao</forename><surname>Liu</surname></persName>
							<email>qliu@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifu</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Refuoe</forename><surname>Mokhosi</surname></persName>
							<email>refuoemokhosi@yahoo.com</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haibin</forename><surname>Zhang</surname></persName>
							<email>herb.zhang@std.uestc.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STAMP: Short-Term Attention/Memory Priority Model for Session-based Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3219819.3219950</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Information retrieval</term>
					<term>Recommender systems</term>
					<term>• Computing methodologies → Neural networks</term>
					<term>Behavior modeling, Session-based recommendation, Attention model, Representation learning, Neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting users' actions based on anonymous sessions is a challenging problem in web-based behavioral modeling research, mainly due to the uncertainty of user behavior and the limited information. Recent advances in recurrent neural networks have led to promising approaches to solving this problem, with long short-term memory model proving effective in capturing users' general interests from previous clicks. However, none of the existing approaches explicitly take the effects of users' current actions on their next moves into account. In this study, we argue that a long-term memory model may be insufficient for modeling long sessions that usually contain user interests drift caused by unintended clicks. A novel short-term attention/memory priority model is proposed as a remedy, which is capable of capturing users' general interests from the long-term memory of a session context, whilst taking into account users' current interests from the short-term memory of the last-clicks. The validity and efficacy of the proposed attention mechanism is extensively evaluated on three benchmark data sets from the RecSys Challenge 2015 and CIKM Cup 2016. The numerical results show that our model achieves state-of-the-art performance in all the tests.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Session-based Recommender systems (SRS) are an important component of modern commercial online systems, usually used for improving user experiences by making suggestions based on user behavior encoded in browser sessions, and the recommender's task is to predict users' next actions (click on an item) based on the sequence of the actions in the current session <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>.Recent studies have highlighted the importance of using recurrent neural networks (RNNs) in a wide variety of recommender systems, among which the application of RNNs in session-based recommendation tasks has led to significant progress in the past few years <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>. Although RNN models have been proven useful in capturing users' general interests from a sequence of actions <ref type="bibr" target="#b19">[20]</ref>, learning to predict from sessions is still a challenging problem to tackle largely due to the uncertainty inherent in user behavior and the limited information provided by browser sessions <ref type="bibr" target="#b17">[18]</ref>.</p><p>Based on existing literature, almost all the RNN-based SRS models only consider modeling the session as a sequence of items, without explicitly taking into account that users' interests drift with time <ref type="bibr" target="#b5">[6]</ref>, which could be problematic in practice. For example, if a specific digital camera link has just been clicked by a user and recorded in a session, it is highly likely that the user's next intended action is in response to the current action. <ref type="bibr" target="#b0">(1)</ref> If the current action is to browse the product description before making a purchase decision, then the user is very likely to visit another digital camera brand catalog in the next move. <ref type="bibr" target="#b1">(2)</ref> If the current action is to add a camera into the shopping cart, then the user's browsing interest is likely be changed to other peripherals such as memory cards. In this case, to recommend another digital camera to that user would not be a good idea, albeit that the initial intention of this session is to buy a digital camera (as was reflected in the previous actions).</p><p>In typical SRS tasks, the session consists of a sequence of named items, and the user interests is hidden in these implicit feedbacks(e.g.,clicks). In order to further improve the predictive accuracy of the RNN models, it is important to have the ability to learn both long-term interests and short-term interests of such implicit feedbacks. As Jannach et al. <ref type="bibr" target="#b6">[7]</ref> noted that both the users' short-term and long-term interests are of great importance for recommendation, but traditional RNN architectures are not designed to distinguish and exploit these two types of interests simultaneously <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this study, we consider to solve this problem by introducing a recent action priority mechanism into the SRS model, called Short-Term Attention/Memory Priority (STAMP) model, which can take into account the user's interests in general and his/her current interests simultaneously. In STAMP, the users' interests in general are captured by an external memory built from all the historical clicks in a session prefix (including the last-click), and this is where the term "Memory" enters. The term "last-click" denotes the last action (item) of a session prefix, the objective of SRS is to predict the "next click" with regard to this "last-click". In this study, the embedding of the last-click is used to represent the user's current interests, and the proposed attention mechanism is built on top of it. Since the last-click is a component of the external memory it can be regarded as short-term memory of the users' interests. Similarly, the users' attention built on top of the last-click can be seen as a short-term attention. To our knowledge, this is the first effort to simultaneously take the long/short term memory into account when constructing a neural attention model for session-based recommendations. The major contributions of this study are as follows:</p><p>• We introduce a short-term attention/memory priority model that learns: (a) a uniform embedding space with items across sessions and (b) a novel neural attention model for next-click prediction in session-based recommender systems. • A novel attention mechanism is proposed for implementation of the STAMP model, in which the attention weights are calculated from the session context and being enhanced with the current interests of the users. The output attention vector is read as a compositional representation of the user's temporal interests, and is more sensitive to user's interests drift with time than other neural attention based solutions. Therefore, it is capable of simultaneously capturing both the users' long-term interests in general (in response to the initial purpose) and their short-term attention (current interests). The validity and efficacy of the proposed attention mechanism is verified through comparison studies.</p><p>• The proposed model is evaluated on two real world datasets, the Yoochoose dataset from RecSys 2015, and the Diginetica dataset from CIKM Cup 2016, respectively. Experimental results show that STAMP achieves state-of-the-art, and the proposed attention mechanism plays an important role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Session-based recommendation is a subtask of recommender system, in which the recommendations are made according to the implicit feedbacks within the user session. This is a challenging task because the users are usually assumed to be anonymous, and the user preferences (such as ratings) are not provided explicitly, instead, only some positive observations (e.g. purchases or clicks) are available to the decision makers <ref type="bibr" target="#b3">[4]</ref>. In the past few years, an increasing amount of research attention has been devoted to the challenge of SRS problem, according to their modeling hypothesis, prevalent approaches can be divided into two categorise: global models that focused on identifying users' interests in general <ref type="bibr" target="#b2">[3]</ref>, and localized models that emphasize users' temporal interests <ref type="bibr" target="#b18">[19]</ref>. One approach of capturing user's general interests is through collaborative filtering (CF) methods based on users' whole purchase/click history. For example, the Matrix Factorization (MF) approach <ref type="bibr" target="#b7">[8]</ref> uses latent vectors to represent general interests, which are estimated through factorizing a user-item matrix consisting of the whole historical transaction data. Another approach is called neighborhood methods <ref type="bibr" target="#b13">[14]</ref>, which try to make recommendations based on item similarities calculated from the co-occurrences of items in sessions. The third approach is the Markov chain (MC) based models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>, which utilize sequential connections between the user actions to make prediction.</p><p>The above models explore either general interests or current interests of users. However, previous current interests based recommenders seldom consider the sequential interactions between items that are not adjacent in the session, although the general interests based recommenders are good at capturing users' general taste, but can hardly adapt its recommendations to users' recent purchases without explicitly modeling the adjacent connections <ref type="bibr" target="#b18">[19]</ref>. Ideally, a good recommender should be able to explore the sequential behavior, as well as account for users' general interests for recommendation, because these two factors may interact with each other to influence users' next click. Therefore, some of the researchers tried to improve the SRS models by taking into consideration of both type of user interests. Rendle et al. <ref type="bibr" target="#b12">[13]</ref> proposed a hybrid model FPMC, which combined the power of MF and MC to model both sequential behavior and general interests for next basket recommendation, thus achieve better performance than considering either short-term interests or long-term interests alone. Wang et al. <ref type="bibr" target="#b18">[19]</ref> proposed a hybrid representation learning model, which employs a two-layer hierarchical structure for modeling of the sequential behavior and general interests of users from their last transactions. However, both of them can only model local sequential behaviours between adjacent actions, without considering the global information conveyed by the session context.</p><p>Deep neural networks have proven to be very effective in modeling sequential data recently <ref type="bibr" target="#b8">[9]</ref>. Inspired by recent advances in natural language processing area <ref type="bibr" target="#b15">[16]</ref>, some deep learning based solutions have been developed and some of which represent the state-of-the-art in SRS research field <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>. Hidasi et al. <ref type="bibr" target="#b4">[5]</ref> use deep recurrent neural networks with a gated recurrent unit to model session data, which learns session representation directly from previous clicks in the given session and provides recommendations of the next action. This is the first attempt to apply RNN networks for solving the SRS problem, thanks to the sequential modeling capability provided by the RNNs, their model can take into account the users' historical behavior when making predictions of the next move. Tan et al. <ref type="bibr" target="#b16">[17]</ref>propose a data augmentation technique to improve the performance of the RNNs for session-based recommendation. Yu et al. <ref type="bibr" target="#b19">[20]</ref>propose a dynamic recurrent model, which applies RNN to learn dynamic representation for each basket for user general interests at different times and captures global sequential behavior among baskets.</p><p>Most neural network models mentioned above are implemented in SRS by manipulating each context clicked item with the same operation, allowing the models to capture the relevance between next click and previous clicks in an implicit way. Also the hidden state in the last time step contains information about the sequence with a strong focus on the parts nearest to the next click <ref type="bibr" target="#b0">[1]</ref>, thus some general interest features of items with a long distance may be forgotten. To solve this problem, a variety of models are introduced to capture relevance between items and more accurate general interests. Hu et al. <ref type="bibr" target="#b5">[6]</ref> propose a neural network with wide-in-wideout structure (SWIWO) to learn user-session context. It constructs the session context via combing all the item embeddings in a current session, which gives each item a fixed weight based on the relative distance with response to the target item. Li et al. <ref type="bibr" target="#b9">[10]</ref> propose an RNN based encoder-decoder model (NARM), which takes the last hidden state from the RNN as the sequential behavior, and uses the hidden states of previous clicks for attention computation to capture the main purpose(general interests) in a given session. Another recent related work is the Time-LSTM model <ref type="bibr" target="#b20">[21]</ref> which is a variant of the LSTM. Time-LSTM considers both short-term interests and long-term interests by using time gates to control the influence of last consumed item and store time intervals to model users' long-term interest, however the time stamp is not provided in most real-world datasets, so it is not considered here.</p><p>Differences: Our model has significant differences with SWIWO and NARM. SWIWO determines the weight of each item in the session in a fixed manner, which we consider is arguable in practice. In STAMP, the proposed attention mechanism can help alleviate this contradiction by explicitly considering correlation between each historical click and the last click, and calculating dynamic weights for given session. Alternatively, NARM combines main purpose and sequential behavior to get the session representation which treats them as equally important complementary features. However, STAMP explicitly emphasizes the current interest reflected by the last click to capture the hybrid features of current and general interests from previous clicks, thus explicitly introducing the importance of last click into the recommender system while NARM only captures the general interests. Short-term interests can be enhanced in STAMP so as to accurately capture the current interest of the user in the case of interest drift, especially in a long session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Symbolic Description</head><p>A typical session-based recommender system is built upon historical sessions, and makes prediction based upon current user sessions. Each session, denoted by S = [s 1 , s 2 , . . . , s N ], consists of a sequence of actions (items clicked by the user), where s i represents an item (ID) clicked at time-step i. S t = {s 1 , s 2 , . . . , s t }, 1 ≤ t ≤ N denotes a prefix of the action sequence truncated at time t with regard to session S. Let V = {v 1 , v 2 , . . . , v |V | } denotes a set of unique items in the SRS system, called item dictionary.</p><p>Let X = {x 1 , x 2 , . . . , x |V | } denote the embedding vectors with respect to the item dictionary V . The proposed STAMP model learns a d-dimensional real-valued embedding x i ∈ R d for each of the item i in V . Specifically, symbol x t ∈ R d represents the embedding of the last click s t of the current session prefix S t . The goal of our models is to predict the next possible click (i.e. s t +1 ) based on given session prefix S t . To be exact, our models are constructed and trained as a classifier that learns to generate a score for each of the candidates in item dictionary V , let ŷ = { ŷ1 , ŷ2 , . . . , ŷ |V | } denote the output score vector, where ŷi corresponds to the score of item v i . After getting this prediction result, the elements in ŷ are ranked in descending order, and the items corresponding to the topk scores are used for recommendation. For notational convenience, we define the trilinear product of three vectors as:</p><formula xml:id="formula_0">&lt; a, b, c &gt;= d i=1 a i b i c i = a T (b ⊙ c)<label>(1)</label></formula><p>where a, b, c ∈ R d , and ⊙ denotes the Hadamard product, i.e. the element-wise product between two vectors b and c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Short-Term Memory Priority Model</head><p>The proposed STAMP model is built upon a so-called Short-Term Memory Priority model (STMP), as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>From Figure <ref type="figure" target="#fig_0">1</ref> one can see that the STMP model takes two embeddings (m s and m t ) as inputs, where m s denotes the user's interests in general with respect to the current session, which is defined as the average of the external memory of the session:</p><formula xml:id="formula_1">m s = 1 t t i=1 x i<label>(2)</label></formula><p>where the term external memory means the item embedding sequence of the current session prefix S t . The symbol m t denotes the current interests of the user in that session, in this study, the lastclick x t is used to represent the user's current interests : m t = x t . Since x t is taken from the external memory of the session, we call it the short-term memory of the user's interests. The general interests m s and current interests m t are then processed with two MLP networks for the purpose of feature abstraction. The network structure of the MLP cells illustrated in Figure <ref type="figure" target="#fig_0">1</ref> are identical to each other, except that they have independent parameter settings.</p><p>A simple MLP without hidden layer is used for feature abstraction, the operation on m s is defined as:</p><formula xml:id="formula_2">h s = f (W s m s + b s )<label>(3)</label></formula><p>where h s ∈ R d denotes the output state, W s ∈ R d×d is a weighting matrix, and b s ∈ R d is the bias vector. f (•) is a non-linear activation function (we use tanh in this study). The state vector h t with regard to m t can be calculated similar to h s . And then, for a given candidate item x i ∈ V , the score function is defined as:</p><formula xml:id="formula_3">ẑi = σ (&lt; h s , h t , x i &gt;)<label>(4)</label></formula><p>where σ (•) denotes the sigmoid function. Let ẑ ∈ R |V | denote the vector that consists of the trilinear products ẑi , in which each ẑi (i ∈ [1, . . . , |V |]) represents the unnormalized cosine similarity between the representation of the weighted user interests with regard to the current session prefix S t and the candidate item x i . Then it is processed by a softmax function to obtain the output ŷ:</p><formula xml:id="formula_4">ŷ = so f tmax(ẑ)<label>(5)</label></formula><p>where ŷ ∈ R |V | denotes the output vector of the model, which represents a probability distribution over the items v i ∈ V , each element ŷi ∈ ŷ denotes the probability of the event that item v i is going to appear as the next-click in this session.</p><p>For any given session prefix S t ∈ S (t ∈ [1, . . . , N ]), the loss function is defined as the cross-entropy of the prediction results ŷ:</p><formula xml:id="formula_5">L(ŷ) = − |V | i=1 y i loд(ŷ i ) + (1 − y i )loд(1 − ŷi )<label>(6)</label></formula><p>where y denotes a one-hot vector exclusively activated by s t +1 ∈ S (the ground truth). For example, if s t +1 denotes the i-th element v i in item dictionary V , then</p><formula xml:id="formula_6">y k = 1, if i == k, and y k = 0 if i k.</formula><p>An iterative stochastic gradient descent (SGD) optimizer is then performed to optimize the cross-entropy loss.</p><p>From the definition of the STMP model (Equation <ref type="formula" target="#formula_3">4</ref>) one can see that it makes predictions on the next-click based on the inner product of the candidate item and the weighted user interests, where the weighted user interests are represented through bilinear composition of the long-term memory (averaged historical clicks) and the short-term memory (the last-click). The validity of this trilinear composition model is verified in Section 4.5, the experimental results demonstrate that the proposed short-term memory priority mechanism can be very effective in capturing users' temporal interests that benefit the next-click prediction, and it achieves state-of-the-art performance on all the benchmark data sets.</p><p>However, as can be seen from Equation <ref type="formula" target="#formula_1">2</ref>, when modeling the user's interests in general m s from the external memory of the current session, the STMP model treats each item in the session prefix as equally important, which we consider would be problematic in capturing the user's interests drift (probably caused by unintended clicks), especially in case of long sessions. Therefore, we propose an attention model to tackle this problem -which has been demonstrated effective in capturing the attention drift in long sequences. The proposed attention model is designed based on the STMP model, and it follows the same idea as STMP in that it also gives priority to short-term attention, hence we call it the Short-Term Attention/Memory Priority Model (STAMP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The STAMP Model</head><p>The architecture of the STAMP model is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. As can be seen from Figure <ref type="figure" target="#fig_1">2</ref>, the only difference between these two models is that in the STMP model the abstract feature vector of user's interests in general (the state vector h s ) is calculated from the average of the external memory m s , while in STAMP model the h s is calculated from an attention based user's interests in general (a real-valued vector m a ) as depicted in 2, which is produced by the proposed attention mechanism, called attention net.</p><p>The proposed attention net consists of two components: (1) a simple feed-forward neural network (FNN) that is responsible for generating attention weights for each of the items within the current session prefix S t , and (2) an attention composite function that is responsible for calculating the attention based user's interests in general m a . The FNN used for attention computation is defined as:</p><formula xml:id="formula_7">α i = W 0 σ (W 1 x i + W 2 x t + W 3 m s + b a )<label>(7)</label></formula><p>where</p><formula xml:id="formula_8">x i ∈ R d denotes the i-th item s i ∈ S t , x t ∈ R d denotes the last-click, W 0 ∈ R 1×d is a weighting vector, W 1 , W 2 , W 3 ∈ R d×d</formula><p>are weighting matrices, b a ∈ R d is a bias vector, and σ (•) denotes the sigmoid function. α i represents the attention coefficient of item x i within the current session prefix. From Equation <ref type="formula" target="#formula_7">7</ref>one can see that the attention coefficients of the items in a session prefix are calculated based on the embedding of the target item x i , the last-click x t and session representation m s , therefore, it is capable of capturing the correlations between the target item and the long/short term memory of the user's interests. Note that in Equation 7, the short-term memory is explicitly considered, which is distinctly different from the related works, and this is why the proposed attention model is called the short-term attention priority model.</p><p>After obtaining the attention coefficients vector α = (α 1 , α 2 , . . . , α t ) with respect to the current session prefix S t , the attention based user's interests in general m a with regard to the current session prefix S t can be calculated as follows, and then add the m s in it:</p><formula xml:id="formula_9">m a = t i=1 α i x i (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Short-Term Memory Only Model</head><p>To evaluate the validity of the basic idea of this study, that is, assigning a priority to the short-term attention/memory of the users' behavior when making decisions according to the session (sequence of actions), in this section, we propose a Short-Term Memory Only (STMO) model, which makes predictions of the next-click s t +1 only based on the last-click s t of the current session prefix S t . Similar to the STMP model, a simple MLP without a hidden layer is used for feature abstraction in the STMO model. The MLP takes the last-click s t as input, and outputs a vector h t ∈ R d just as the "MLP CELL B" in STMP (see Figure <ref type="figure" target="#fig_0">1</ref>), defined as:</p><formula xml:id="formula_10">h t = f (W t x t + b t )<label>(9)</label></formula><p>where h t denotes the output state, W t ∈ R d×d is a weighting matrix, and b t ∈ R d is the bias vector. f (•) denotes the activation function tanh. Then for a given candidate item x i ∈ V , the score function is defined as the inner product between x i and h t :</p><formula xml:id="formula_11">ẑi = h T t x i<label>(10)</label></formula><p>After obtaining the score vector ẑ ∈ R |V | , one can make predictions based on the ranking list calculated with Equation <ref type="formula" target="#formula_4">5</ref>, or optimize the parameters of the model based on Equation <ref type="formula" target="#formula_5">6</ref>, just like the situation in STMP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets and Data Preparation</head><p>We evaluate the proposed models on two datasets, the first one is called Yoochoose from the RecSys'15 Challenge 1 , which consists of six months of click-streams gathered from an e-commerce web site, where the training set only contains session events. Another one is the Diginetica dataset coming from the CIKM Cup 2016 2 , for which only the transaction data is used in this study.</p><p>Following <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b9">[10]</ref>, we filter out sessions of length 1 and items that appear less than 5 times in both of the datasets. The test set of Yoochoose consists of the sessions of subsequent days with respect to the training set, and we filter out clicks (items) that did not appear in the training set. And for Diginetica, the only difference is that we use the sessions of subsequent week for testing. After the pre-processing phase, there remains 7,966,257 sessions of 31,637,239 clicks on 37,483 items in Yoochoose dataset, and 202,633 sessions of 982,961 clicks on 43,097 items in Diginetica dataset.</p><p>Same as <ref type="bibr" target="#b16">[17]</ref>, we use a sequence splitting preprocess that for an input session S = {s 1 , s 2 , . . . , s n }, we generate the sequences and corresponding labels([s 1 ], s 2 ), ([s 1 , s 2 ], s 3 )... ([s 1 , s 2 , ...,s n−1 ], s N ) for training and testing on both datasets, which proves to be effective. Because the Yoochoose training set is quite large and training on the recent fractions yields better results than training on the entire fractions as per the experiments of <ref type="bibr" target="#b16">[17]</ref>, we use the recent fractions 1/64 and 1/4 of training sequences. The statistics of the three datasets are shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>The following models, including the state-of-art and closely related work, are used as baselines to evaluate the performance of the proposed STAMP model :</p><p>• POP: A naive SRS model that always recommends items based on occurrence frequency in the training set. • Item-KNN <ref type="bibr" target="#b13">[14]</ref>: An item-to-item model which recommends items similar to the existing items based on cosine similarity between the candidate item and the existing items within the session. A constraint is included to avoid coincidental high similarities between rarely visited items as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>We use the following metrics for evaluation of the performance of the SRS models, which are also widely used in other related works. P@20: The P@K score is widely used as a measure of predictive accuracy in SRS area. P @ K represents the proportion of test cases which has the correctly recommended items in a top k position in a ranking list. In this paper, P@20 is used for all the tests, defined as:</p><formula xml:id="formula_12">P@K = n hit N<label>(11)</label></formula><p>where N denotes the number of test data in the SRS system G, n hit denotes the number of cases which have the desired items in top K ranking lists, a hit occurs when t appears in the top K position of the ranking list of G. MRR@20: The average of reciprocal ranks of the desired item t. The reciprocal rank is set to zero if the rank is above 20.</p><formula xml:id="formula_13">MRR@K = 1 N t ∈G 1 Rank(t)<label>(12)</label></formula><p>The MRR is a normalized score of range [0, 1], an increase in its value reflects that the majority "hits" will appear higher in the ranking order of the recommendation list, which indicates a better performance of the corresponding recommender system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameters</head><p>The hyper-parameters are optimized via extensive grid search on all the data sets, and the best models are selected by early stopping based on the P@20 score on the validation set. Hyper-parameter ranges for the grid search are the following: embedding dimension d in {50, 100, 200, 300}, learning rate η in {0.001, 0.005, 0.01, 0.1, 1}, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Next-Click Prediction</head><p>To demonstrate the overall performance of the proposed model, we compare it with the state-of-the-art item recommendation approaches, and the numerical results on all of the benchmark data sets are illustrated in Table <ref type="table" target="#tab_1">2</ref>, in which the best result of each column is highlighted in boldface. As one can see from Table <ref type="table" target="#tab_1">2</ref>, STAMP achieves state-of-the-art performances in terms of P@20 and MRR@20 on both of the Yoochoose data sets and the Diginetica dataset, which verifies the efficacy and validity of the proposed model. The following observations can be made from table <ref type="table" target="#tab_1">2</ref>:</p><p>The performance of traditional methods such as Item-KNN and FPMC are not competitive, as they only outperform the naive POP model. These results help verify the importance of taking the user's behavior (interactions)into consideration in session-based recommendation tasks as the results show that making recommendations solely based on co-occurrence popularity of the items (POP), or simply taking transitions over successive items could be very problematic in making accurate recommendations. In addition, such global solutions can be time and memory consuming, making them not scalable to for large-scale datasets.</p><p>All of the neural network baselines significantly outperform conventional models, thus proving the effectiveness of deep learning technology in this field. GRU4Rec+ improves the performances of GRU4Rec by using the data augmentation techniques that split a single session into several sub-sessions for training. While GRU4Rec+ does not modify the model structure of GRU4Rec, they both only take the sequential behavior into account which may encounter difficulties with users' interest drift. NARM achieves the best performances among the baselines, because it not only models the sequential behavior using RNN with GRU units but also uses attention mechanism to capture main purpose, which indicates the importance of main purpose information in recommendations. This Table <ref type="table">3</ref>: The results of P@K, MRR@K when K=5,10. is reasonable as part of items in the current session may reflect the user's main purpose and relate to the next item. Among our proposed models, the STAMP model obtains the highest P@20 and MRR@20 on Yoochoose dataset in 2 experiments and achieves comparable results on the Diginetica dataset. The STMO model cannot capture general interest information from previous clicks in the current session, so it generates the same recommendation whenever it encounters the same last-click, although given different sessions. Unsurprisingly the model has the worst performance in our proposed models, since it cannot take advantage of the general interest information. But compared with traditional machine learning methods such as Item-KNN and FPMC, STMO achieves significantly better performances which demonstrates the ability of our proposed model framework to learn effective uniform item embedding representation. The STMP as an extension to STMO simply uses an average pooling function to generate session representation as the long-term interest and applies last-click information to capture short-term interest. It outperforms STMO in all three experiments and performs comparably with GRU4Rec+ but a little inferior to NARM. As expected, considering both session context information and last click information is suitable for this task as STMP is able to better make the session-based recommendations for a given session. Compared with STMP, STAMP applies item-level attention mechanism and achieves 0.95%, 1.25%, 1.12% improvements on P@20 and 1.04%, 1.06%, 2.04% on MRR@20 in three experiments respectively. The results show that the session representation generated in this way is more effective than average pooling function, which confirms that not all items in the current session are equally important in generating the next recommendation, and part of the important items can be captured by the proposed attention mechanism to model useful features of interest; the state-of-the-art results prove the validity of STAMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Compare STAMP with NARM</head><p>Session-based recommender systems have become an indispensable part of many e-commerce systems, helping users to sort out items of interest from large inventories. In fact, there are always more than 10 5 items in an e-commerce website and most users are only interested in viewing recommendations on the first page of real-world recommender systems <ref type="bibr" target="#b5">[6]</ref>. In order to verify the performance of our proposed STAMP model and the recent state-of-the-art NARM model in real production environment, where recommendation systems can only suggest a few items at once, the relevant item should be amongst the first few items in the recommendation list <ref type="bibr" target="#b11">[12]</ref>. We therefore evaluate the recommendation quality in terms of P@5, MRR@5, P@10 and MRR@10 in trying to simulate the practical situation. The results are summarized in Table <ref type="table">3</ref>, and argue that the experimental results may to some extent reflect their performance in the real production environment. We can observe that STAMP performs well on this mission and much more competitively than NARM when evaluated under stricter rules in an simulated production environment. Our model performs consistently better than NARM and shows obvious advantages in three experiments which demonstrates the effectiveness of considering both general interests and short-term interests, and the validity of the learned item embeddings. The results prove that the proposed STAMP tends to make more accurate recommendations as seen in the above experimental results and the main results in subsection 4.5.</p><p>We also record the runtime of the recurrent neural model NARM and the proposed STAMP approach. We implement both models with the same 100-dimensional embedding vectors, and test them on the same GPU server. The training time of each epoch on three datasets is given in Table <ref type="table" target="#tab_3">4</ref>,which illustrates that STAMP is more efficient than NARM. We argue that this is because the NARM model contains a lot of complex operations in each GRU unit, and our proposed model is simpler and faster as it introduces a simplified neural model to save the cost of recurrent calculations in dealing with sequential inputs. All above results imply that STAMP may be more suitable for practical application since computational efficiency is crucial in real-world session-based recommender systems, which always comprise of large amounts of sessions and items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Effects of the last click</head><p>In this section, we design a series of contrast models to verify the validity of applying the last click information on the basis of session context to make session-based recommendations:</p><p>• STMP-: On the basis of STMP, not using last click item embedding in the trilinear layer. • STMP: The STMP model proposed in the paper.</p><p>• STAMP-: On the basis of STAMP, not using last click item embedding in the trilinear layer. • STAMP: The STAMP model proposed in the paper.</p><p>The numerical results in Table <ref type="table" target="#tab_4">5</ref> show that all the models in which the last click is combined with the session context vector have better performance than those without. The results prove that employing the last click positively contributes to recommendations of a given session. Our models are based on simultaneously capturing long-term and short-term interest and enhancing the last click information, which we believe is advantageous in handling long sessions as users' interest may change during a long browsing period and the user's next action may be more related to last click that reflects a short-term interest. In order to verify the effects of last click, we investigate the P@20 with different session lengths and the results on Yoochoose 1/64 dataset are shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>We first present experimental results varying the length of sessions on STMP, STAMP and NARM as shown by Figure <ref type="figure" target="#fig_2">3(a)</ref>. We can observe that when the length of sessions is above 20 the performance of NARM quickly decreases in contrast with STMP and STAMP. This suggests that short-term interests priority based models may be more powerful in handling long sessions than NARM. On the other hand, in Figure <ref type="figure" target="#fig_2">3</ref>(b) we find that the P@20 results of STMP and STAMP when the lengths are between 1 to 30 are significantly higher than each corresponding model without feeding last click into the trilinear layer, respectively. The reason is that with current interests captured in last click or session representation, STMP and STAMP may better model the user interest for the next click recommendation. For longer sessions lengths, the performance margins between STMP-and STMP and between STAMP-and STAMP become larger. This proves that although it is important to capture general interests from the session context, explicitly taking advantage of temporal interests can enhance the quality of recommendations. Moreover, STAMP-outperforms STMP-which results from the hybrid interests captured by the attention mechanism in STAMP-while STMP-only considers the general interests;this demonstrates the importance of the last click information in the session-based recommendation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Comparison among Proposed Models</head><p>To further verify the efficacy and validity of different proposed models being those that capture user interests from only last click, those that combine last click with session context, and lastly those that apply attention mechanism; we compare the models by making comparison studies on different session lengths to show their performances and the advantages in different situations. To achieve this purpose, we partition sessions into two groups: 'Short' indicates that the length of sessions is 5 events or less while 'Long' represents sessions having more than 5 events, where 5 is almost an average length of total sessions in all original data sets. The statistics on the percentage of sessions belonging to Short group are 70.10% and 76.40%, and to Long are 29.90% and 23.60% for both test datasets of Yoochoose and Diginetica. For each approach, we compute the results of P@20 and MRR@20 for each length group on each data set. Experimental results are illustrated in Figure <ref type="figure" target="#fig_3">4</ref> (a) and (b) for Yoochoose and Diginetica respectively.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> (a) shows the results on Yoochoose. We can see that all methods obtain lower P@20 and MRR@20 results in Long group in   comparison to Short group, highlighting the challenge of making session-based recommendations for long sessions on this dataset. We suspect it may because of difficulty in capturing users' interest drift as the session grows in length. In addition both STMP and STAMP outperform STMO in two groups and the margin becomes wider as the session length increases, meaning that a model considering both general and current interests may be more powerful in handling long sessions, in comparison to only applying the last click information for recommendations. This confirms our intuition that session context and last click information can simultaneously and effectively be used to learn user interests and predict the next selected item in session-based recommendations. Figure <ref type="figure" target="#fig_3">4</ref> (b)shows the results on Diginetica. STMO has better MRR@20 results than STMP, and the gap grows from 0.38% to  <ref type="figure" target="#fig_3">4</ref> shows that the trend between Short and Long group on the Yoochoose dataset is much different from that on the Diginetica dataset. To explain this phenomenon, we analyze the two datasets and show the ratio of sessions which have repeated clicks(i.e. the click appears at least twice within a session) in the two datasets with respect to the session length. From Table <ref type="table" target="#tab_6">6</ref> we can see that the ratio of sessions which have repeated clicks in Yoochoose is smaller on Short group and larger on Long group than those in Diginetica dataset. From these results, we find that repeated clicks in the session have an impact on the recommendations, which have an inversely proportional ratio to model performance. It may be because repeated clicks may emphasize invalid information from unimportant items and make it difficult to capture user interests associated with the next action. In STAMP, we model the user interests using short-term attention priority, whereby the attention mechanism selects important items from the given session to model user interests. Both of these can effectively mitigate the impact of repeated clicks in a session. Conversely, only last click or average click information is used in other approaches, these models usually lose important information and are unable to overcome problems associated with repeated clicks. This proves the validity of shortterm attention priority and the proposed attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Further Investigation</head><p>In this section, we repeatedly selected random multiple sets of examples from the Yoochoose test sets for analysis, and they consistently showed the same patterns. Figure <ref type="figure" target="#fig_4">5</ref> illustrates the attention results of the proposed item-level attention mechanism and its advantage.</p><p>In Figure <ref type="figure" target="#fig_4">5</ref>, the depth of the color indicates the importance of an item, the darker the color the more important an item is. Because it's hard to directly evaluate the association between each context item and the target item in the absence of item specific information, the validity of the attention mechanism can be partially explained based on the category of an item. For example, in session 11255991 we can observe that the items which have the same category with the target item have larger attention weights than other items. The category of item can reflect the interest of the user to a certain extent, and the higher weight of the item with the same category as the target item can partially prove that the attention mechanism can capture user interests for the next action. Our method is capable of highlighting a number of factors in determining the next action as shown in Figure <ref type="figure" target="#fig_4">5</ref>. Firstly, not all items are important in determining the next action and our method is able to pick important items and ignore unintended clicks. Secondly, although some important items are not near the current action in a session they can be flagged as important by our method, we believe that this demonstrates that our model is capable of capturing the users' interests in general in response to the initial or main purpose. Thirdly, items whose position is close to the end of the session often have larger weights, especially the last click item in a session with a long length. This proves our intuition that the user's intended action may be more in response to the current action. It shows that the proposed attention mechanism is sensitive to interests drift in a given session and correctly captures the current interests which is one of the reasons why STAMP can outperform other models which mainly focus on long-term interest. Moreover, the results illustrate that important items can be captured regardless of their position(i.e beginning or end of session) in a given session(e.g. session 11255788, 11255819). This proves our conjecture that the proposed item-level attention mechanism can capture pivotal items from a global perspective to construct hybrid features of general interests and current interests. Therefore based on the visualization results, we argue that the proposed item-level attention mechanism captures important parts for predicting next action in a session by computing attention weights, enabling the model to consider both long-term interest and short-term interest and make more accurate and effective recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a short-term attention/memory priority model for session-based recommendations. Two important findings can be made from the study: <ref type="bibr" target="#b0">(1)</ref> The next move of a user is mostly affected by the last-click of a session prefix, and our model can effectively utilize such information through the temporal interests representation. <ref type="bibr" target="#b1">(2)</ref> The proposed attention mechanism can effectively capture long-term and short-term interests of a session, empirical results prove that with the help of the attention mechanism, our model achieves state-of-the-art performance on all datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic illustration of the STMP model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic illustration of the STAMP model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The P@20 evaluated on different lengths of sessions' test cases in Yoochoose 1/64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: P@20 and MRR@20 of different session lengths. (a) on Yoochoose, (b) on Diginetica.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Attention visualization. Attention weights is used for color-coding, the depth of the color indicates the importance of an item. The numbers above the bar are session IDs, and the category ID of each item is given below the item.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the experiment datasets. An RNN based state-of-the-art model which employs attention mechanism to capture main purpose from the hidden states and combines it with the sequential behavior as final representation to generate recommendations.</figDesc><table><row><cell cols="4">Dataset Yoochoose 1/64 Yoochoose 1/4 Diginetica</cell></row><row><cell># train</cell><cell>375,073</cell><cell>5,969,416</cell><cell>719,470</cell></row><row><cell># test</cell><cell>55,898</cell><cell>55,898</cell><cell>60,858</cell></row><row><cell># clicks</cell><cell>565,552</cell><cell>7,980,529</cell><cell>982,961</cell></row><row><cell># items</cell><cell>17,694</cell><cell>30,660</cell><cell>43,097</cell></row><row><cell>avg. len.</cell><cell>6.16</cell><cell>5.71</cell><cell>5.12</cell></row><row><cell cols="4">• FPMC[13]: A state-of-the-art hybrid model for next-basket</cell></row><row><cell cols="4">recommendation. In order to make it work on session-based</cell></row><row><cell cols="4">recommendation, we do not consider the user latent repre-</cell></row><row><cell cols="4">sentations when computing recommendation scores.</cell></row><row><cell cols="4">• GRU4Rec[5]: An RNN based deep learning model for ses-</cell></row><row><cell cols="4">sion based recommendation, which consists of GRU units, it</cell></row><row><cell cols="4">utilizes session-parallel mini-batch training process and also</cell></row><row><cell cols="4">employs ranking-based loss functions during the training.</cell></row><row><cell cols="4">• GRU4Rec+[17]: A improved model based on GRU4Rec which</cell></row><row><cell cols="4">adopts two techniques to improve the performance of GRU4Rec,</cell></row><row><cell cols="4">including a data augmentation process and a method to ac-</cell></row><row><cell cols="3">count for shifts in the input data distribution.</cell><cell></cell></row><row><cell>• NARM[10]:</cell><cell></cell><cell></cell><cell></cell></row></table><note>1 http://2015.recsyschallenge.com/challege.html 2 http://cikm2016.cs.iupui.edu/cikm-cup</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Next-click prediction on 3 benchmark data sets.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Yoochoose 1/64</cell><cell cols="2">Yoochoose 1/4</cell><cell cols="2">Diginetica</cell></row><row><cell>Measures</cell><cell cols="6">P@20 MRR@20 P@20 MRR@20 P@20 MRR@20</cell></row><row><cell>POP</cell><cell>6.71</cell><cell>1.65</cell><cell>1.33</cell><cell>0.30</cell><cell>0.91</cell><cell>0.23</cell></row><row><cell>Item-KNN</cell><cell>51.60</cell><cell cols="2">21.81 52.31</cell><cell cols="2">21.70 28.35</cell><cell>9.45</cell></row><row><cell>FPMC</cell><cell>45.62</cell><cell>15.01</cell><cell>−</cell><cell cols="2">− 31.55</cell><cell>8.92</cell></row><row><cell>GRU4Rec</cell><cell>60.64</cell><cell cols="2">22.89 59.53</cell><cell cols="2">22.60 43.82</cell><cell>15.46</cell></row><row><cell cols="2">GRU4Rec+ 67.84</cell><cell cols="2">29.00 69.11</cell><cell cols="2">29.22 57.95</cell><cell>24.93</cell></row><row><cell>NARM</cell><cell>68.32</cell><cell cols="2">28.76 69.73</cell><cell cols="2">29.23 62.58</cell><cell>27.35</cell></row><row><cell>STMO</cell><cell>64.22</cell><cell cols="2">25.81 66.22</cell><cell cols="2">26.69 58.62</cell><cell>25.90</cell></row><row><cell>STMP</cell><cell>67.79</cell><cell cols="2">28.63 69.19</cell><cell cols="2">28.94 60.91</cell><cell>25.34</cell></row><row><cell>STAMP</cell><cell>68.74</cell><cell cols="2">29.67 70.44</cell><cell cols="2">30.00 62.03</cell><cell>27.38</cell></row><row><cell cols="7">learning rate decay λ in {0.75, 0.8, 0.85, 0.9, 0.95, 1.0}. According</cell></row><row><cell cols="7">to the averaged performance, in this study we use the following</cell></row></table><note>hyper-parameters for all the tests on two datasets : {d : 100, η : 0.005, λ : 1.0}. The mini-batch settings are: batch size : 512, epoch : 30. All weighting matrices are initialized by sampling from a normal distribution N (0, 0.05 2 ), and all biases are set to zeros. All the items embeddings are initialized randomly with a normal distribution N (0, 0.002 2 ), which are then jointly trained with other parameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Runtime of each training epoch.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>Time (seconds)</cell></row><row><cell></cell><cell>Yoochoose 1/64</cell><cell>155.3</cell></row><row><cell>NARM</cell><cell>Yoochoose 1/4</cell><cell>961.4</cell></row><row><cell></cell><cell>Diginetica</cell><cell>99.6</cell></row><row><cell></cell><cell>Yoochoose 1/64</cell><cell>33.3</cell></row><row><cell>STAMP</cell><cell>Yoochoose 1/4</cell><cell>356.1</cell></row><row><cell></cell><cell>Diginetica</cell><cell>52.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Impacts of the last-click.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Yoochoose 1/64</cell><cell cols="2">Yoochoose 1/4</cell><cell cols="2">Diginetica</cell></row><row><cell cols="7">Measures P@20 MRR@20 P@20 MRR@20 P@20 MRR@20</cell></row><row><cell>STMP-</cell><cell>60.59</cell><cell>21.70</cell><cell>62.92</cell><cell>24.52</cell><cell>57.20</cell><cell>21.55</cell></row><row><cell>STMP</cell><cell>67.79</cell><cell>28.63</cell><cell>69.19</cell><cell>28.94</cell><cell>60.91</cell><cell>25.34</cell></row><row><cell>STAMP-</cell><cell>65.19</cell><cell>24.95</cell><cell>67.96</cell><cell>26.67</cell><cell>60.15</cell><cell>24.47</cell></row><row><cell>STAMP</cell><cell>68.74</cell><cell cols="2">29.67 70.44</cell><cell cols="2">30.00 62.03</cell><cell>27.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Statistics of sessions have repeated items.</figDesc><table><row><cell>Dataset</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>&gt;5</cell></row><row><cell cols="6">Diginetica-train 0.1839 0.3272 0.4374 0.5229 0.7016</cell></row><row><cell>Diginetica-test</cell><cell cols="5">0.1880 0.3304 0.4420 0.5351 0.7149</cell></row><row><cell cols="6">YooChoose-train 0.1796 0.3298 0.4272 0.5091 0.7181</cell></row><row><cell>YooChoose-test</cell><cell cols="5">0.1770 0.3166 0.4139 0.5015 0.7563</cell></row><row><cell cols="6">1.11% with increasing session length. This performance probably</cell></row><row><cell cols="6">indicates that average aggregation in STMP has its disadvantages</cell></row><row><cell cols="6">which influence the rank of correct items in recommendations,</cell></row><row><cell cols="6">also the results of STMO may imply the validity of the short-term</cell></row><row><cell cols="6">interests for making accurate recommendations. Overall, STAMP</cell></row><row><cell cols="6">is still the best performing model which also highlights the need</cell></row><row><cell cols="6">for effective session representation to obtain hybrid interests, this</cell></row><row><cell cols="6">proves the advantages of the proposed attention mechanism.</cell></row><row><cell cols="2">Furthermore, Figure</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for taking time to read and make valuable comments on this paper. This work was supported by NSFC under grant 61133016 and 61772117, the General Equipment Department Foundation (61403120102), and the Sichuan Hi-Tech industrialization program (2017GZ0308).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR&apos;15. CoRR</title>
				<meeting>ICLR&apos;15. CoRR<address><addrLine>Scottsdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations</title>
		<author>
			<persName><forename type="first">Hidasi</forename><surname>Balázs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Quadrana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM RecSys&apos;16</title>
				<meeting>ACM RecSys&apos;16<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
	<note>Alexandros Karatzoglou, and Domonkos Tikk</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Increasing recommended effectiveness with markov chains and purchase intervals</title>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoubin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhao</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1153" to="1162" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for online recommendation with implicit feedback</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;16</title>
				<meeting>ACM SIGIR&apos;16<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR&apos;15</title>
				<meeting>ICLR&apos;15<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2004">2015. May 2 -4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diversifying Personalized Recommendation with User-session Context</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI&apos;17. IJCAI</title>
				<meeting>IJCAI&apos;17. IJCAI<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1858" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptation and Evaluation of Recommendations for Short-term Shopping Goals</title>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Lerche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jugovac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM RecSys&apos;15</title>
				<meeting>ACM RecSys&apos;15<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1920">2015. September 16 -20</date>
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Attentive Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CIKM&apos;17</title>
				<meeting>ACM CIKM&apos;17<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP&apos;15</title>
				<meeting>EMNLP&apos;15<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09-17">2015. September 17 -21</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Quadrana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidasi</forename><surname>Balázs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM RecSys&apos;17</title>
				<meeting>ACM RecSys&apos;17<address><addrLine>Como, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factorizing personalized Markov chains for next-basket recommendation</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW&apos;10</title>
				<meeting>WWW&apos;10<address><addrLine>Raleigh, North Carolina, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW&apos;01</title>
				<meeting>WWW&apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An MDP-based recommender system</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1265" to="1295" />
			<date type="published" when="2005-09">2005. Sep (2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS&apos;14</title>
				<meeting>NIPS&apos;14<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014-12-08">2014. December 08 -13</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved Recurrent Neural Networks for Session-based Recommendations</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Kiam Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DLRS&apos;16</title>
				<meeting>DLRS&apos;16<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-09-15">2016. September 15 -15</date>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modelling Contextual Information in Session-Aware Recommender Systems with Neural Networks</title>
		<author>
			<persName><forename type="first">Bartlomiej</forename><surname>Twardowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM RecSys&apos;16</title>
				<meeting>ACM RecSys&apos;16<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-09-15">2016. September 15 -19</date>
			<biblScope unit="page" from="273" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Hierarchical Representation Model for NextBasket Recommendation</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;15</title>
				<meeting>ACM SIGIR&apos;15<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Dynamic Recurrent Model for Next Basket Recommendation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;16</title>
				<meeting>ACM SIGIR&apos;16<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-07-17">2016. July 17 -21</date>
			<biblScope unit="page" from="729" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What to Do Next: Modeling User Behaviors by Time-LSTM</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI&apos;17</title>
				<meeting>IJCAI&apos;17<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19">2017. August 19 -25</date>
			<biblScope unit="page" from="3602" to="3360" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
