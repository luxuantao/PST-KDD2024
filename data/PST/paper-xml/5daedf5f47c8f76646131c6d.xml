<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
							<email>weiyinwei@hotmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<email>xiangwang@u.nus.edu</email>
						</author>
						<author>
							<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<email>xiangnanhe@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
							<email>hongrc@hfut.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<email>chuats@comp.nus.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">§</forename><forename type="middle">Xiang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3343031.3351034</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Convolution Network</term>
					<term>Multi-modal Recommendation</term>
					<term>Micro-video Understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Personalized recommendation plays a central role in many online content sharing platforms. To provide quality micro-video recommendation service, it is of crucial importance to consider the interactions between users and items (i.e., micro-videos) as well as the item contents from various modalities (e.g., visual, acoustic, and textual). Existing works on multimedia recommendation largely exploit multi-modal contents to enrich item representations, while less effort is made to leverage information interchange between users and items to enhance user representations and further capture user's fine-grained preferences on different modalities.</p><p>In this paper, we propose to exploit user-item interactions to guide the representation learning in each modality, and further personalized micro-video recommendation. We design a Multimodal Graph Convolution Network (MMGCN) framework built upon the message-passing idea of graph neural networks, which can yield modal-specific representations of users and micro-videos to better capture user preferences. Specifically, we construct a useritem bipartite graph in each modality, and enrich the representation of each node with the topological structure and features of its neighbors. Through extensive experiments on three publicly available datasets, Tiktok, Kwai, and MovieLens, we demonstrate that our proposed model is able to significantly outperform stateof-the-art multi-modal recommendation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Personalized recommendation has become a core component in many online content sharing services, spanning from image, blog to music recommendation. Recent success of micro-video sharing platforms, such as Tiktok and Kwai, bring increasing attentions to micro-video recommendation. Distinct from these item contents (e.g., image, music) that are solely from a single modality, microvideos contain rich multimedia information -frames, sound tracks, and descriptions -that involve multiple modalities of visual, acoustic, and textual ones <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Incorporating such multi-modal information into historical interactions between users and micro-videos help establish an indepth understanding of user preferences: • There is a semantic gap between different modalities. Take Figure <ref type="figure" target="#fig_0">1</ref> as an example, while having visually similar frames, micro-videos i 1 and i 2 have dissimilar textural representations due to different topic words. In such cases, ignoring such modality difference would mislead the modeling of item representations. • A user may have different tastes on modalities of a micro-video.</p><p>For example, a user is attracted by the frames, but may turn out to be disappointed with its poor sound tracks. Multiple modalities, hence, have varying contributions to user preferences. • Different modalities serve as different channels to explore user interests. In Figure <ref type="figure" target="#fig_0">1</ref>, if user u 1 cares more about frames, i 2 is more suitable to be recommended; whereas, u 1 might click i 3 due to interest in textural descriptions. Therefore, it is of crucial importance to distinguish and consider modal-specific user preferences.</p><p>However, existing works on multimedia recommendation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref> mainly treat multi-modal information as a whole and incorporate them into a collaborative filtering (CF) framework, while lacking the modeling of modal-specific user preferences. Specifically, multimodal features of each item are unified as a single representation, reflecting their content similarity; thereafter, such representations are incorporated with user and item representations derived from CF framework, such as MF <ref type="bibr" target="#b29">[30]</ref>. For instance, VBPR <ref type="bibr" target="#b16">[17]</ref> leverages visual features to enrich ID embeddings of items; ACF <ref type="bibr" target="#b7">[8]</ref> employs the attention mechanism on a user's history to encode two-level personal tastes on historical items and item contents into user representations. Such signals can be summarized as the paths connecting the target user and item based on historical interactions <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41]</ref>. For example, given two paths p 1 u 1 → i 1 → u 2 → i 2 and p 2 u 1 → i 1 → u 2 → i 3 ; this would suggest that i 2 and i 3 are likely to be of interest to u 1 . However, we argue that these signals are not sufficient to draw such conclusion. The key reason is that they ignore the differences and user preferences among modalities.</p><p>To address the limitations, we focus on information interchange between users and items in multiple modalities. Inspired by the recent success of graph convolution networks (GCNs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, we use the information-propagation mechanism to encode high-order connectivity between users and micro-videos in each modality, so as to capture user preference on modal-specific contents. Towards this end, we propose a Multi-modal Graph Convolution Network (MMGCN). Specifically, we construct a user-item bipartite graph on each modality. Intuitively, the historical behaviors of users reflect personal interests; meanwhile, the user groups can also profile items <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41]</ref>. Hence, in each modality (e.g., visual), we aggregate signals from the corresponding contents (e.g., frames) of interacted items and incorporate them into user representations; meanwhile, we boost the representation of an item with its user group. By performing such aggregation and combination operators recursively, we can enforce the user and item representations to capture the signals from multi-hop neighbors, such that a user's modal-specific preference is represented well in his/her representation. Ultimately, the prediction of an unseen interaction can be calculated as similarities between the user and micro-video representations. We validate our framework over three publicly accessible datasets -Tiktok, Kwai, and Movielens. Experimental results show that our model can yield promising performance. Furthermore, we visualize user preference on different modalities, which clearly shows the differences in modal-specific preferences by different users.</p><p>The main contributions of this work are threefold: • We explore how information interchange on various modalities reflects user preferences and affects recommendation performance.</p><p>• We develop a new method MMGCN, which employs information propagation on the modality-aware bipartite user-item graph, to obtain better user representations based on item content information.</p><p>• We perform extensive experiments on three public datasets to demonstrate that our proposed model outperforms several stateof-the-art recommendation methods. In addition, we released our codes, parameters, and the baselines to facilitate further researchers by others<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL FRAMEWORK</head><p>In this section, we elaborate our framework. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, our framework consists of three componentsaggregation layers, combination layers, and prediction layer. By stacking multiple aggregation and combination layers, we encode the information interchange of users and items into the representation learning in each modality. Lastly, we fuse multimodal representations to predict the interaction between each user and each micro-video in the prediction layer. In what follows, we detail each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modality-aware User-Item Graphs</head><p>Instead of unifying multi-modal information, we treat each modality individually. Particularly, we have historical interactions (e.g., view, browse, or click) between users and micro-videos. Here we represent the interaction data as a bipartite user-item graph G = {(u, i)|u ∈ U, i ∈ I}, where U and I separately denote the user and micro-video sets. An edge y ui = 1 indicates an observed interaction between user u and micro-video i; otherwise y ui = 0. Beyond the interactions, we have multiple modalities for each micro-video -visual, acoustic, and textual features. For simplicity, we use m ∈ M = {v, a, t } as the modality indicator, where v, a, and t represent the visual, acoustic, and textual modalities, respectively. To accurately capture the users' preferences on a particular modality m, we split the bipartite graph G m from G by keeping only the features for modality m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Aggregation Layer</head><p>Intuitively, we can utilize the interaction data to enrich the representations of users and items. To be more specific, historical interactions of a user can describe user's interest and capture the behavior similarity with other users. Meanwhile, the user group of a micro-video can provide complementary data to its multi-modal contents. We hence incorporate the information interchange into the representation learning.</p><p>Inspired by the message-passing mechanism of GCN, for a user (or micro-video) node in the bipartite graph G m , we employ an aggregation function f (•) to quantify the influence (i.e., the representation being propagated) from its neighbors and output a representation as follows:</p><formula xml:id="formula_0">h m = f (N u ),<label>(1)</label></formula><p>where N u = {j |(u, j) ∈ G m } denotes the neighbors of user u, i.e., interacted micro-videos. We implement f (•) via: • Mean Aggregation employs the average pooling operation on the modal-specific features, and applies a nonlinear transformation, as follows:</p><formula xml:id="formula_1">f avg (N u ) = LeakyReLU 1 |N u | j ∈N u W 1,m j m ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">j m ∈ R d m is the d m -dimension representation of micro- video j in modality m; W 1,m ∈ R d ′ m</formula><p>×d m is the trainable transformation matrix to distill useful knowledge, where d ′ m is the transformation size; and we select LeakyReLU(•) as the nonlinear activation function <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41]</ref>. Such aggregation method assumes that different neighbors would have the same contributions to the representation of user u, namely, user u is influenced equally by his/her neighbors.</p><p>• Max Aggregation leverages the max pooling operation to perform dimension-aware feature selection, as follows:</p><formula xml:id="formula_3">f max (N u ) = LeakyReLU max j ∈N u W 1,m j m ,<label>(3)</label></formula><p>where each dimension of h m is set as the max-num of the corresponding neighbor values. As such, different neighbors have varying contributions to the output representations.</p><p>Hence, the aggregation layer is capable of encoding the structural information and distribution of neighbors into the representation of the ego user; analogously, we can update the representations for item nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combination Layer</head><p>While containing the information being propagated from the neighbors, such representations forgo user u's own feature and the interaction among different modalities. However, existing GNN efforts (e.g., GCN <ref type="bibr" target="#b21">[22]</ref>, GraphSage <ref type="bibr" target="#b13">[14]</ref>, GAT <ref type="bibr" target="#b33">[33]</ref>) only consider homogeneous features from one data source. Hence, directly applying their combination operations fails to capture the interactions between different modalities.</p><p>In this section, we present a new combination layer, which integrates the structural information h m , the intrinsic information u m , and the modality connection u id into a unified representation, which is formulated as:</p><formula xml:id="formula_4">u (1) m = д(h m , u m , u id ),<label>(4)</label></formula><p>where u m ∈ R d m is the representation of user u in modality m; and u id ∈ R d is the d-dimension embedding of user ID, remaining invariant and serves as the connection across modalities.</p><p>Inspired by prior work <ref type="bibr" target="#b2">[3]</ref> on multi-modal representation, we first apply the idea of coordinated fashion, namely, separately projecting u m , ∀m ∈ M into the latent space that is the same as u id :</p><formula xml:id="formula_5">ûm = LeakyReLU(W 2,m u m ) + u id ,<label>(5)</label></formula><p>where W 2,m ∈ R d×d m is the trainable weight matrix to transfer u m into the ID embedding space. As such, the representations from different modalities are comparable in the same hyperplane. Meanwhile, the ID embedding u id essentially bridges the gap between modal-specific representations, and propagates information across modalities during the gradient back-propagation process. In this work, we implement the combination function д(•) via the following two methods:</p><p>• Concatenation Combination which concatenates the two representations, using a nonlinear transformation:</p><formula xml:id="formula_6">д co (h m , u m , u id ) = LeakyReLU W 3,m (h m || ûm ) , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where || is the concatenation operation, and</p><formula xml:id="formula_8">W 3,m ∈ R d ′ m ×(d ′ m +d</formula><p>) is the trainable model parameters. • Element-wise Combination that considers the element-wise feature interaction between two representations:</p><formula xml:id="formula_9">д ele (h m , u m , u id ) = LeakyReLU W 3,m h m + ûm ,<label>(7)</label></formula><p>where W 3,m ∈ R d×d ′ m denotes a weight matrix to transfer the current representations into the common space. In the elementwise combination, the interactions between two representations are taken into consideration, while two representations are assumed to be independent in Concatenation Combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model Prediction</head><p>By stacking more aggregation and combination layers, we explore the higher-order connectivity inherent in the user-item graphs. As such, we can gather the information propagated from the lhop neighbors in modality m, mimicking the exploration process of users. Formally, the representation from l-hop neighbors of user u and the output of l-th multi-modal combination layer are recursively formulated as:</p><formula xml:id="formula_10">h (l ) m = f (N u ) and u (l ) m = д(h (l ) m , u (l −1) m , u id ),<label>(8)</label></formula><p>where u</p><formula xml:id="formula_11">(l −1) m</formula><p>is the representation generated from the previous layer, memorizing the information from its (l − 1)-hop neighbors. u (0) m is set as u m at the initial iteration. Wherein, user u is associated with trainable vectors u m , ∀m ∈ M, which are randomly initialized; whereas, item i is associated with the pre-extracted features i m , ∀m ∈ M. As a result, u (l −1) m characterizes the user preferences on item features in modality m, and considers the influence of modality interactions that reflect the underlying relationships between modalities.</p><p>After stacking L single-modal aggregation and multi-modal combination layers, we obtain the final representations for user u and micro-video i via the linear combination of multi-modal representations, as:</p><formula xml:id="formula_12">u * = m ∈M u (L) m and i * = m ∈M i (L) m (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Optimization</head><p>To predict the interaction between the users and micro-videos, we fuse their modal-specific representations and apply Bayesian Personalized Ranking (BPR) <ref type="bibr" target="#b29">[30]</ref>, which is a well-known pairwise ranking optimization framework, as the learning model. In particular, we model a triplet of one user and two micro-videos, in which one of the micro-videos is observed and the other one is not, formally as,</p><formula xml:id="formula_13">R = {(u, i, i ′ )|(u, i) ∈ G, (u, i ′ ) G},<label>(10)</label></formula><p>where N (u) consists of all micro-videos associated u, and R is a set of triples for training. Further, it is assumed that the user prefers the observed micro-video rather than the unobserved one.</p><p>Hence, the objective function can be formulated as,</p><formula xml:id="formula_14">L = (u,i,i ′ )∈R − ln µ(u * ⊤ i * − u * ⊤ i ′ * ) + λ ∥Θ∥ 2 2 ,<label>(11)</label></formula><p>where µ(•) is the sigmoid function; λ and Θ represent the regularization weight and the parameters of the model, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we conduct experiments on three publicly available datasets, aiming to answer the following research questions:</p><p>• RQ1: How does MMGCN perform compared with the state-ofthe-art multi-modal recommendation systems and other GNNbased methods on our task? In what follows, we first present the experimental settings (i.e., datasets, baselines, evaluation protocols, and parameter settings), followed by answering the above three questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Datasets. To evaluate our model, we experimented with three publicly available datasets: Tiktok, Kwai, and MovieLens. The characteristics of these datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>• Tiktok 2 : It is published by Tiktok, a micro-video sharing platform that allows users to create and share micro-videos with duration of 3-15 seconds. It consists of users, micro-videos and their interactions (e.g., click). The micro-video features in each modality are extracted and published without providing the raw data. In particular, the textual features are extracted from the micro-video captions given by users. • Kwai 3 : As a popular micro-video service provider, Kwai has constructed a large-scale micro-video dataset. Similar with the Tiktok dataset, it contains the privacy-preserving user information, content features of micro-videos, and the interaction data. However, the acoustic information of micro-videos is missing. • MovieLens 4 : This dataset has been widely used to evaluate recommendations. To construct the dataset, we collected the titles and descriptions of movies from the MoiveLens-10M dataset and crawled the corresponding trailers instead of the full-length videos from Youtube 5 . We use the pre-trained ResNet50 <ref type="bibr" target="#b15">[16]</ref> models to extract the visual features from key frames extracted from micro-video. In terms of acoustic modality, we separate audio tracks with FFmpeg 6 and adopt VGGish <ref type="bibr" target="#b19">[20]</ref> to learn the acoustic deep learning features. For textual modality, we use Sentence2Vector <ref type="bibr" target="#b0">[1]</ref> to derive the textual features from microvideos' descriptions. Baselines. To evaluate the effectiveness of our model, we compare MMGCN with the following state-of-the-art baselines. The baselines can be grouped into two categories: CF-based (VBPR and ACF) and GCN-based (NGCF and GraphSAGE) methods.</p><p>• VBPR <ref type="bibr" target="#b16">[17]</ref>. Such model integrates the content features and ID embeddings of each item as its representation, and uses the    matrix factorization (MF) framework to reconstruct the historical interactions between users and items. In the experiments, we use the concatenation of multi-modal features as the content information to predict the interactions between users and microvideos.</p><p>• ACF <ref type="bibr" target="#b7">[8]</ref>. This is the first framework that is designed to tackle the implicit feedback in multimedia recommendation. It introduces two attention modules to address the item-level and componentlevel implicit feedbacks. To explore the modal-specific user preference and micro-video characteristic, we treat each modality as one component of the micro-video, which is consistent with the idea of standard ACF. • GraphSAGE <ref type="bibr" target="#b13">[14]</ref>. Such model is based on the general inductive framework that leverages node feature information to update node representations for the previously unseen data. In particular, it considers the structure information as well as the distribution of node features in the neighborhood. For a fair comparison, we integrate multi-modal features as the node features to learn the representation of each node. • NGCF <ref type="bibr" target="#b41">[41]</ref>. This method represent a novel recommendation framework to integrate the user-item interactions into the embedding process. By exploiting the higher-order connectivity from user-item interactions, the modal encodes the collaborative filtering signal into the representation. For a fair comparison, we regard the multi-modal features of micro-video as side information and feed them into the framework to predict the interactions between the users and items.</p><p>Evaluation Protocols and Parameter Settings. We randomly split the dataset into training, validation, and testing sets with 8:1:1 ratio, and create the training triples based on random negative sampling. For the testing set, we pair each observed user-item pair with 1000 unobserved micro-videos that the user has not interacted before. We use the widely-used protocols <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>: Precision@K, Recall@K, and NDCG@K to evaluate the performance of top-K recommendation. Here we set K = 10 and report the average scores in testing set. To train our proposed model, we randomly initialize model parameters with a Gaussian distribution and use the LeakyReLU as the activation function, and optimizing the model with stochastic gradient descent (SGD). We search the batch size in {128, 256, 512}, the latent feature dimension in {32, 64, 128}, the learning rate in {0.0001, 0.0005, 0.001.0.005, 0.01} and the regularizer in {0, 0.00001, 0.0001, 0.001, 0.01, 0.1}. As the findings are consistent across the dimensions of latent vectors, if not otherwise specified, we only show the result of 64, a relatively large number that returns good accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparison (RQ1)</head><p>The comparative results are summarized in Table <ref type="table" target="#tab_2">2</ref>. From this table, we have the following observations:</p><p>• MMGCN substantially outperforms all the other baselines in most cases, verifying the effectiveness of our model. In particular, MMGCN improves the strongest baselines w.r.t. Recall by 15.59%, 10.23%, and 8.76%, on the three datasets respectively. We attribute such significant improvements to the learning of modal-specific representations, so as to capture users' preference effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• The GNN-based model outperforms the CF-based model on</head><p>Kwai and Tiktok. The improvements are attributed to the graph convolution layers. Such operations not only capture the local structure information but also learn the distribution of neighbors' features for each ego node, thus boosting the expressiveness of representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Study of MMGCN (RQ2)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Effects of Modalities.</head><p>To explore the effects of different modalities, we compare the results on different modalities over the three datasets, as shown in Figure <ref type="figure" target="#fig_3">3</ref>. It shows the performance of top-K recommendation lists where K ranges from 1 to 10. From Figure <ref type="figure" target="#fig_3">3</ref>, we have the following observations:</p><p>• As expected, the method with multi-modal features outperforms those with single-modal features in MMGCN, on all three datasets. It demonstrates that representing users with multi-modal information achieves higher accuracy. It further demonstrates that user representations are closely related to the content of items. Moreover, it shows that our model could capture the user's modal-specific preference from content information. • The visual modality is the most effective among the three modalities. It makes sense since as, when a user chooses what to play, one usually pays more attention to the visual information than other modality information. • The acoustic modality provides more important information for recommendation, compared with the textual features. In particular, for Tiktok dataset, the acoustic information even has comparable expressiveness to that of the visual modality. • Textual modality is the least descriptive for interaction prediction, particularly on Kwai and Tiktok datasets. This is reasonable since we find the texts are of low quality, that is, the descriptions are noisy, incomplete, and even irrelevant to the micro-video content on these two datasets. However, this modality offers important cues on the MovieLens dataset. Because the textual description is the storyline of the video, which highly relates to the content, and some users may play the video according to the storyline.</p><p>This phenomenon is consistent with our argument that the user preference are closely related to the content information. • As K increases, Recall@K of MMGCN is consistently higher than the variants. It shows that user preference representations based on each modality are closer to the real preferences of users, which contribute to the prediction of user-item interactions. Modeling with user preferences on a variety of modalities can lead to quality multi-modal personalized recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Effect of Combination</head><p>Layers. In our model, we design a novel combination layer to integrate the local structure information with the node's features, facilitating the multiple modal-specific representation fusion. Wherein, the combination function can be implemented with two different way (cf. Equations ( <ref type="formula" target="#formula_6">6</ref>) and ( <ref type="formula" target="#formula_9">7</ref>)).</p><p>Here we compare these different implementations and evaluate the effectiveness of the proposed combination layer, in which д co−id and д el e−id represent two type of implements without id embedding, respectively. As illustrated in Table <ref type="table" target="#tab_3">3</ref>, we have the following findings:</p><p>• In terms the three metrics, the д el e achieves the best performance on the three datasets. This may be due to that the combination layer retains the modal-specific features to represent the users and micro-videos. It demonstrate the effectiveness of our combination layers. • Comparing these methods, we found that the methods with id embedding significantly outperforms the others. This agains demonstrates the effectiveness of our novel combination layers. Besides, we suggest that the shared id embedding connects the different modalities by propagating the shared information during backpropagation. • Comparing the two implementations, we observed that the element-wise one is better than the concatenate one. We conjecture that the concatenate one with fully connected layer is more difficult to train, especially on the spare datasets, like Kwai.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Effect of Model Depth.</head><p>To evaluate the effectiveness of layers stack, we conduct experiments on the three different layers, as shown in Table <ref type="table" target="#tab_4">4</ref>. From the results, we observed that:</p><p>• In terms three metrics, the two-layer model achieves better results, which show that increasing of layers does not lead to better results. This seems to indicate that the discrimination of the nodes is decreasing as the number of layers increases. We suggest </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Case Study (RQ3)</head><p>We conducted experiments to visualize our modal-specific representations. In particular, we randomly sampled 5 users and collected the micro-videos they have played. To verify our assumption that the user preferences on different modalities are different, we visualized these representations using t-Distributed Stochastic Neighbor Embedding (t-SNE) in 2-dimension, as illustrated in Figure <ref type="figure" target="#fig_4">4</ref>.</p><p>The coordinate graphs from left to right represent the visual and textual modality, respectively. The points in the graphs represent the videos that the users have played, and their colors represent different users. Because the acoustic modality is hard to be visualized, we only analyze the results on visual and textual modalities.</p><p>• In visual modality, the points of user1 are dispersive and some of them are mixed with the points of user2, user3, and user4. On the other hand, the points from user1 form two concentrated regions in the textual modality, and they are far apart from each other. The distribution of the points means that users have two distinct preferred themes in textual modality. While he/she has no particular preference on visual modality. The points of user2 clustered in three regions in the visual modality; while in the textual modality, they are diffuse and mixed with the points of other users. The distribution pattern of user2 shows that his/her has three preferred themes in the visual modality. The points of user3 are obviously well clustered distribution in the two modalities, which indicates that user has particular preference in each modality. The distribution of the points of user4 and user5 are scattered and mixed with other points of users. • It is still abstract to use the distribution of points for analysis. The multi-modal information of videos represented by each point is displayed on the graph for further explanation. Take the example of user1 and user2, the visual and textual modality of some of their preferred videos are displayed in Figure <ref type="figure" target="#fig_4">4</ref>, which are represented by video posters and storylines. We observed that the videos played by user1 have no obvious themes visually, because the posters he/she preferred cover many different varieties. However, the storylines or textual modality of these videos cover just two themes: war and romance. From user2, we observed that his/her preference on visual modality are clearly divided into animation and classicism, while he/she has no distinct preference on storylines. These phenomena supports our assumption that the users have different preference in different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>In this section, we introduce some works that are related to our research, including multi-modal personalized recommendation, multi-modal fusion and graph convolution network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-modal Personalized Recommendation</head><p>Due to the success of CF method in recommendation systems, early multi-modal recommendation algorithms mainly based on CF models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref><ref type="bibr" target="#b40">[40]</ref>. CF-based models leverage users' feedbacks (e.g. implicit feedback and explicit feedback) to predict the interactions between users and items. Although these approaches work well for items with sufficient feedbacks, they are less applicable to those with few feedbacks, which cause the low-quality recommendations. Therefore, the CF-based methods are limited by the sparsity of the data.</p><p>To remedy the disadvantage of CF-based model, researchers have developed hybrid approaches which incorporate the items' content information and the collaborative filtering effects for recommendation. For instance, Chen et al. <ref type="bibr" target="#b6">[7]</ref> constructed a uservideo-query tripartite graph and performed graph propagation to combine the content and feedback information between users and videos. Recently, Chen et al. <ref type="bibr" target="#b7">[8]</ref> explored the fine-grained user preference on the items and introduced a novel attention mechanism to address the challenging item-and component-level feedback in multimedia recommendation. In this method, the user is characterized by both collaborative filtering effect and the attended items' content information. Although this method has learned the two levels of the user preference, it fails to model the user preferences on different modalities, which is the key in multi-modal recommendation as mentioned in Section 1. To fill the void in modalspecific features representation, our model constructs the graph in each modality and represents the model-specific features using GCN techniques, which integrates the local structure information and distribution of content information in neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-modal Representation</head><p>The multi-modal representation is one of the most important problem in multi-modal applications <ref type="bibr" target="#b26">[27]</ref>. However, there are few prior works that focus on multi-modal representation in the area of multi-modal personalized recommendations.</p><p>Existing multi-modal representations can be grouped into two categories: joint representations and coordinated representations <ref type="bibr" target="#b2">[3]</ref>. Joint representations usually combine the various single-modal information into a single representation and project it into the same representation space. The simplest implementation of the joint representation is the concatenation of single-modal features. Recently, with its success in computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref> and natural language processing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">32]</ref>, neural networks are increasingly used in the multi-modal domain, on multimodal representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b43">43]</ref>. Using neural networks, the function fusing the different modalities information into a joint representation can be learned. Besides, the probabilistic graphical models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref> are another way to construct a joint representation for multi-modal information using the latent random variable. Although these methods learn a joint representation to model the multi-modal data, they are suited for situations when all of the modalities are present during inference, which is hardly guaranteed in social platforms.</p><p>Different from joint representations, the coordinated ones learn separate representations for each modality but coordinate them with constraints. To represent the multi-modal information, Frome et al. <ref type="bibr" target="#b11">[12]</ref> proposed a deep visual-semantic embedding model which projects the visual information and semantic information into a common space constrained by distance between the visual embedding and the corresponding word embedding. Similarly, Wang et al. <ref type="bibr" target="#b34">[34]</ref> constructed a coordinated space which enforces images with similar meanings to be closer to each other. However, since the modal-specific information is the factor causing the difference in each modality signals, the model-specific features are inevitably discarded via those similar constrains.</p><p>In contrast, in our model, we introduced a novel representation, which respectively models the common part and specific part of features, to resolve the abovementioned problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph Convolution Network</head><p>As mentioned above, our proposed model uses the GCN techniques to represent the users and micro-videos, which is widespread in recommendation systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Towards video recommendation, Hamilton et al. <ref type="bibr" target="#b13">[14]</ref> proposed a general inductive framework which leverages the content information to generate node representation for unseen data. Based on this method, Ying et al. <ref type="bibr" target="#b42">[42]</ref> developed and deployed a large-scale deep recommendation engine at for image recommendation. In this model, the graph convolutions and random walks are combined to generate the representations of nodes. Concurrently, Berg et al. <ref type="bibr" target="#b4">[5]</ref> treated the recommender systems as the view of link prediction on graphs and proposed a graph auto-encoder framework based on message passing on the bipartite interaction graph. Moreover, the side information can be integrated into the node representation via a separate processing channel. However, as can be seen, these methods fail to capture the modal-specific representation for each node in the multi-modal recommendation, which is the major concern of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this paper, we explicitly modeled modal-specific user preferences to enhance micro-video recommendation. We devised a novel GCN-based framework, termed MMGCN, to leverage information interchange between users and micro-videos in multiple modalities, refine their modal-specific representations, and further model users' fine-grained preferences on micro-videos. Experimental results on three publicly available micro-video datasets well validated our model. In addition, we visualized some samples to illustrate the modal-specific user preferences.</p><p>This work investigates how the information exchange in different modalities influences user preference. This is an initial attempt to encode modality-aware structural information into representation learning. It is a promising solution to understand user behaviors and provide more accurate, diverse, and explainable recommendation. In future, we will extend MMGCN in several directions. First, we would construct multi-modal knowledge graph to present objects and relations between them in microvideos <ref type="bibr" target="#b31">[31]</ref>, and then use it into MMGCN to model finer-grained content analysis. It will be used to explore user interests in a more fine-grained manner, and offer an in-depth understanding of user intents. It can also provide more accurate, diverse, and explainable recommendation. Second, we would explore how social leaders influence the recommendation, that is, integrating social network with user-item graphs. We would also like to incorporate multimedia recommendation into dialogue systems towards more intelligent conversational recommendations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of modal-specific user preferences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic illustration of our proposed MMGCN model. It constructs the user-microvideo bipartite graph for each modality to capture the modal-specific user preference for the personalized recommendation of micro-video.</figDesc><graphic url="image-234.png" coords="3,438.31,213.08,65.34,62.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance in terms of Recall@K w.r.t. different modalities on the three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of users' played micro-videos distribution in different modalities, where each color indicates a user.</figDesc><graphic url="image-252.png" coords="7,392.30,84.35,152.45,127.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the evaluation dataset. (V, A, and T denote the dimensions of visual, acoustic, and textual modalities, respectively.)</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Interactions #Items #Users Sparsity</cell><cell>V</cell><cell>A</cell><cell>T</cell></row><row><cell>Tiktok</cell><cell>726,065</cell><cell cols="4">76,085 36,656 99.99% 128 128 128</cell></row><row><cell>Kwai</cell><cell>1,664,305</cell><cell cols="4">329,510 22,611 99.98% 2,048 -128</cell></row><row><cell>MovieLens</cell><cell>1,239,508</cell><cell cols="4">5,986 55,485 99.63% 2,048 128 100</cell></row><row><cell cols="6">• RQ2: How do different designs (e.g., number of modalities,</cell></row><row><cell cols="6">number of layers, selection of combination layer) influence the</cell></row><row><cell cols="3">performance of MMGCN?</cell><cell></cell><cell></cell></row><row><cell cols="6">• RQ3: Can MMGCN capture the inconsistent preference of users</cell></row><row><cell cols="3">on different modalities?</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison between our model and the baselines.</figDesc><table><row><cell>Model</cell><cell>Precision</cell><cell>Kwai Recall</cell><cell>NDCG</cell><cell>Precision</cell><cell>Tiktok Recall</cell><cell>NDCG</cell><cell>Precision</cell><cell>MovieLens Recall</cell><cell>NDCG</cell></row><row><cell>VBPR</cell><cell>0.2673</cell><cell>0.3386</cell><cell>0.1988</cell><cell>0.0972</cell><cell>0.4878</cell><cell>0.3136</cell><cell>0.1172</cell><cell>0.4724</cell><cell>0.2852</cell></row><row><cell>ACF</cell><cell>0.2559</cell><cell>0.3248</cell><cell>0.1874</cell><cell>0.8734</cell><cell>0.4429</cell><cell>0.2867</cell><cell>0.1078</cell><cell>0.4304</cell><cell>0.2589</cell></row><row><cell>GraphSAGE</cell><cell>0.2718</cell><cell>0.3412</cell><cell>0.2013</cell><cell>0.1028</cell><cell>0.4972</cell><cell>0.3210</cell><cell>0.1132</cell><cell>0.4532</cell><cell>0.2647</cell></row><row><cell>NGCF</cell><cell>0.2789</cell><cell>0.3463</cell><cell>0.2058</cell><cell>0.1065</cell><cell>0.5008</cell><cell>0.3226</cell><cell>0.1156</cell><cell>0.4626</cell><cell>0.2732</cell></row><row><cell>MMGCN</cell><cell>0.3057*</cell><cell>0.3996*</cell><cell>0.2298*</cell><cell>0.1164*</cell><cell>0.5520*</cell><cell>0.3423*</cell><cell>0.1215*</cell><cell>0.5138*</cell><cell>0.3062*</cell></row><row><cell>%Improv.</cell><cell>9.61%</cell><cell>15.59%</cell><cell>11.66%</cell><cell>9.03%</cell><cell>10.23%</cell><cell>6.11%</cell><cell>3.67%</cell><cell>8.76%</cell><cell>7.36%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of MMGCN with different aggregation and combination layers.</figDesc><table><row><cell>Variant</cell><cell>Precision</cell><cell>Kwai Recall</cell><cell>NDCG</cell><cell>Precision</cell><cell>Tiktok Recall</cell><cell>NDCG</cell><cell>Precision</cell><cell>Recall</cell><cell>NDCG</cell></row><row><cell>д co−id д co</cell><cell>0.2812 0.2927</cell><cell>0.3689 0.3841</cell><cell>0.2146 0.2188</cell><cell>0.1056 0.1132</cell><cell>0.5289 0.5482</cell><cell>0.3143 0.3372</cell><cell>0.1034 0.1209</cell><cell>0.4632 0.5090</cell><cell>0.2702 0.3001</cell></row><row><cell>д el e−id д el e</cell><cell>0.2840 0.3057</cell><cell>0.3729 0.3996</cell><cell>0.2172 0.2298</cell><cell>0.1071 0.1164</cell><cell>0.5312 0.5520</cell><cell>0.3186 0.3423</cell><cell>0.1064 0.1215</cell><cell>0.4704 0.5138</cell><cell>0.2743 0.3062</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of MMGCN w.r.t. the number of layers. Generally speaking, NGCF achieves better performance than other baselines over three datasets in most cases. It is reasonable since NGCF are easily generalized to leverage the content information to characterize the users and micro-videos.• Unexpectedly, ACF performs poorly on all datasets. The reason of this may be due to the modification that we did during the implementation of ACF model, in which we replaced the component-level features modeling by the modal-specific information for a fair comparison.</figDesc><table><row><cell>Layer</cell><cell>Precision</cell><cell>Kwai Recall</cell><cell>NDCG</cell><cell>Precision</cell><cell>Tiktok Recall</cell><cell>NDCG</cell><cell>Precision</cell><cell>MovieLens Recall</cell><cell>NDCG</cell></row><row><cell>One</cell><cell>0.2814</cell><cell>0.3728</cell><cell>0.2123</cell><cell>0.1084</cell><cell>0.5371</cell><cell>0.3263</cell><cell>0.1174</cell><cell>0.5017</cell><cell>0.2950</cell></row><row><cell>Two</cell><cell>0.3057</cell><cell>0.3996</cell><cell>0.2298</cell><cell>0.1164</cell><cell>0.5520</cell><cell>0.3423</cell><cell>0.1215</cell><cell>0.5138</cell><cell>0.3062</cell></row><row><cell>Three</cell><cell>0.2983</cell><cell>0.3910</cell><cell>0.2216</cell><cell>0.1103</cell><cell>0.5431</cell><cell>0.3361</cell><cell>0.1181</cell><cell>0.5032</cell><cell>0.2957</cell></row><row><cell>•</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/weiyinwei/MMGCN.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work is supported by the National Natural Science Foundation of China, No.: 61772310, No.:61702300, No.:61702302, No.: 61802231, and No. U1836216; the Project of Thousand Youth Talents 2016; the Shandong Provincial Natural Science and Foundation, No.: ZR2019JQ23, No.:ZR2019QF001; the Future Talents Research Funds of Shandong University, No.: 2018WLJH 63. This research is also part of NExT++ project, supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@Singapore Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence PP</title>
		<imprint>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dimensional affect recognition using continuous conditional random fields</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ntombikayise</forename><surname>Banda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
				<meeting>IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Personalized video recommendation through tripartite graph propagation</title>
		<author>
			<persName><forename type="first">Bisheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM international conference on Multimedia</title>
				<meeting>ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1133" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR conference on Research and Development in Information Retrieval</title>
				<meeting>the International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia Conference on Multimedia Conference</title>
				<meeting>ACM Multimedia Conference on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MMALFM: Explainable recommendation by leveraging reviews and images</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><forename type="middle">C</forename><surname>Kanjirathinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On effective personalized music retrieval by exploring online user behaviors</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Jialie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Ch</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
				<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing Systems</title>
				<meeting>International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic modality weighting for multi-stream hmms inaudio-visual speech recognition</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Gurban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Thiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Dutoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimodal Interfaces</title>
				<meeting>International Conference on Multimodal Interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="237" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing Systems</title>
				<meeting>International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">VBPR: visual bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial personalized ranking for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aren</forename><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What Dress Fits Me Best?: Fashion Recommendation on the Clothing Style for Personal Body Shape</title>
		<author>
			<persName><forename type="first">Chusnul</forename><surname>Shintami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Chun</forename><surname>Hidayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Ting</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Lung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Huang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia Conference on Multimedia Conference</title>
				<meeting>ACM Multimedia Conference on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="438" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single image deraining: A comprehensive benchmark analysis</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iago</forename><forename type="middle">Breno</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><forename type="middle">Hirata</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cesar-Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3838" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards Microvideo Understanding by Joint Sequential-Sparse Modeling</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia Conference on Multimedia Conference</title>
				<meeting>ACM Multimedia Conference on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="970" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Online Data Organizer: Micro-Video Categorization by Structure-Guided Multimodal Dictionary Learning</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1235" to="1247" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing Systems</title>
				<meeting>International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from multiple social networks</title>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Information Concepts, Retrieval, and Services</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="118" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enhancing Micro-video Understanding by Harnessing External Sounds</title>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia Conference on Multimedia Conference</title>
				<meeting>ACM Multimedia Conference on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1192" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International conference on machine learning</title>
				<meeting>International conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2014">2016. 2014-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on uncertainty in artificial intelligence</title>
				<meeting>the conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Annotating Objects and Relations in User-Generated Videos</title>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>ICMR. 279-287</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Black Holes and White Rabbits: Metaphor Identification with Visual Features</title>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the North American Chapter</title>
				<meeting>Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="160" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Event driven web video summarization by tag localization and key-shot identification</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="975" to="985" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">First-person daily activity recognition with manipulated object proposals and non-linear feature fusion</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2946" to="2955" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint Global and Co-Attentive Representation Learning for Image-Sentence Retrieval</title>
		<author>
			<persName><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia Conference on Multimedia Conference</title>
				<meeting>ACM Multimedia Conference on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1398" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">KGAT: Knowledge Graph Attention Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tem: Tree-enhanced embedding model for explainable recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1543" to="1552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Item silk road: Recommending items from information domains to social users</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR conference on Research and Development in Information Retrieval</title>
				<meeting>the International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR conference on Research and Development in Information Retrieval</title>
				<meeting>the International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural Multimodal Belief Tracker with Adaptive Attention for Dialogue Systems</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2401" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
