<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Invertible Image Rescaling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingqing</forename><surname>Xiao</surname></persName>
							<email>mingqingxiao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<email>changliu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaolong</forename><surname>Wang</surname></persName>
							<email>yaolong.wang@mail.utoronto.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">Toronto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>He</surname></persName>
							<email>dihe@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<email>zlin@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Invertible Image Rescaling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-resolution digital images are usually downscaled to fit various display screens or save the cost of storage and bandwidth, meanwhile the postupscaling is adpoted to recover the original resolutions or the details in the zoomin images. However, typical image downscaling is a non-injective mapping due to the loss of high-frequency information, which leads to the ill-posed problem of the inverse upscaling procedure and poses great challenges for recovering details from the downscaled low-resolution images. Simply upscaling with image super-resolution methods results in unsatisfactory recovering performance. In this work, we propose to solve this problem by modeling the downscaling and upscaling processes from a new perspective, i.e. an invertible bijective transformation, which can largely mitigate the ill-posed nature of image upscaling. We develop an Invertible Rescaling Net (IRN) with deliberately designed framework and objectives to produce visually-pleasing low-resolution images and meanwhile capture the distribution of the lost information using a latent variable following a specified distribution in the downscaling process. In this way, upscaling is made tractable by inversely passing a randomly-drawn latent variable with the low-resolution image through the network. Experimental results demonstrate the significant improvement of our model over existing methods in terms of both quantitative and qualitative evaluations of image upscaling reconstruction from downscaled images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With exploding amounts of high-resolution (HR) images/videos on the Internet, image downscaling is quite indispensable for storing, transferring and sharing such large-sized data, as the downscaled counterpart can significantly save the storage, efficiently utilize the bandwidth <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b33">34]</ref> and easily fit for screens with different resolution while maintaining visually valid information <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b48">49]</ref>. Meanwhile, many of these downscaling scenarios inevitably raise a great demand for the inverse task, i.e., upscaling the downscaled image to a higher resolution or its original size <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b18">19]</ref>. However, details are lost and distortions appear when users zoom in or upscale the low-resolution (LR) Work done during an internship at Microsoft Research Asia. arXiv:2005.05650v1 [eess.IV] 12 May 2020 images. Such an upscaling task is quite challenging since image downscaling is wellknown as a non-injective mapping, meaning that there could exist multiple possible HR images resulting in the same downscaled LR image. Hence, this inverse task is usually considered to be ill-posed <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Many efforts have been made to mitigate this ill-posed problem, but the gains fail to meet the expectation. For example, most of previous works choose super-resolution (SR) methods to upscale the downscaled LR images. However, mainstream SR algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b49">50]</ref> focus only on recovering HR images from LR ones under the guidance of a predefined and non-adjustable downscaling kernel (e.g., Bicubic interpolation), which omits its compatibility to the downscaling operation. Intuitively, as long as the target LR image is pre-downscaled from an HR image, taking the image downscaling method into consideration would be quite invaluable for recovering the high-quality upscaled image.</p><p>Instead of simply treating the image downscaling and upscaling as two separate and independent tasks, most recently, there have been efforts <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> attempting to model image downscaling and upscaling as a united task by an encoder-decoder framework. Specifically, they proposed to use an upscaling-optimal downscaling method as an encoder which is jointly trained with an upscaling decoder <ref type="bibr" target="#b25">[26]</ref> or existing SR modules <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b48">49]</ref>. Although such an integrated training approach can significantly improve the quality of the HR images recovered from the corresponding downscaled LR images, neither can we do a perfect reconstruction. These efforts didn't tackle much on the illposedness since they link the two processes only through the training objectives and conduct no attempt to capture any feature of the lost information.</p><p>In this paper, with inspiration from the reciprocal nature of this pair of image rescaling tasks, we propose a novel method to largely mitigate this ill-posed problem of the image upscaling. According to the Nyquist-Shannon sampling theorem, high-frequency contents are lost during downscaling. Ideally, we hope to keep all lost information to perfectly recover the original HR image, but storing or transferring the high-frequency information is unacceptable. In order to well address this challenge, we develop a novel invertible model called Invertible Rescaling Net (IRN) which captures some knowledge on the lost information in the form of its distribution and embeds it into model's parameters to mitigate the ill-posedness. Given an HR image x, IRN not only downscales it into a visually-pleasing LR image y, but also embed the case-specific high-frequency content into an auxiliary case-agnostic latent variable z, whose marginal distribution obeys a fixed pre-specified distribution (e.g., isotropic Gaussian). Based on this model, we use a randomly drawn sample of z from the pre-specified distribution for the inverse upscaling procedure , which holds the most information that one could have in upscaling.</p><p>Yet, there are still several great challenges needed to be addressed during the IRN training process. Specifically, it is essential to ensure the quality of reconstructed HR images, obtain visually pleasing downscaled LR ones, and accomplish the upscaling with a case-agnostic z, i.e., z ? p(z) instead of a case-specific z ? p(z|y). To this end, we design a novel compact and effective objective function by combining three respective components: an HR reconstruction loss, an LR guidance loss and a distribution matching loss. The last component is for the model to capture the true HR image mani-fold as well as for enforcing z to be case-agnostic. Neither the conventional adversarial training techniques of generative adversarial nets (GANs) <ref type="bibr" target="#b20">[21]</ref> nor the maximum likelihood estimation (MLE) method for existing invertible neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4]</ref> could achieve our goal, since the model distribution doesn't exist here, meanwhile these methods don't guide the distribution in the latent space. Instead, we take the pushedforward empirical distribution of x as the distribution on y, which, in independent company with p(z), is the actually used distribution to inversely pass our model to recover the distribution of x. We thus match this distribution with the empirical distribution of x (the data distribution). Moreover, due to the invertible nature of our model, we show that once this matching task is accomplished, the matching task in the (y, z) space is also solved, and z is made case-agnostic. We minimize the JS divergence to match the distributions, since the alternative sample-based maximum mean discrepancy (MMD) method <ref type="bibr" target="#b2">[3]</ref> doesn't generalize well to the high dimension data in our task.</p><p>Our contributions are concluded as follows:</p><p>-To our best knowledge, the proposed IRN is the first attempt to model image downscaling and upscaling, a pair of mutually-inverse tasks, using an invertible (i.e., bijective) transformation. Powered by the deliberately designed invertibility, our proposed IRN can largely mitigate the ill-posed nature of image upscaling reconstruction from the downscaled LR image. -We propose a novel model design and efficient training objectives for IRN to enforce the latent variable z, with embedded lost high-frequency information in the downscaling direction, to obey a simple case-agnostic distribution. This enables efficient upscaling based on the valuable samples of z drawn from the certain distribution. -The proposed IRN can significantly boost the performance of upscaling reconstruction from downscaled LR images compared with state-of-the-art downscaling-SR and encoder-decoder methods. Moreover, the amount of parameters of IRN is significantly reduced, which indicates the light-weight and high-efficiency of the new IRN model.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Upscaling after Downscaling</head><p>Super resolution (SR) is a widely-used image upscaling method and get promising results in low-resolution (LR) image upscaling task. Therefore, SR methods could be used to upscale downscaled images. Since the SR task is inherently ill-posed, previous SR works mainly focus on learning strong prior information by example-based strategy <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b26">27]</ref> or deep learning models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b49">50]</ref>. However, if the targeted LR image is pre-downscaled from the corresponding high-resolution image, taking the image downscaling method into consideration would significantly help the upscaling reconstruction.</p><p>Traditional image downscaling approaches employ frequency-based kernels, such as Bilinear, Bicubic, etc. <ref type="bibr" target="#b40">[41]</ref>, as a low-pass filter to sub-sample the input HR images into target resolution. Normally, these methods suffer from resulting over-smoothed images since the high-frequency details are suppressed. Therefore, several detail-preserving or structurally similar downscaling methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b37">38]</ref> are proposed recently. Besides those perceptual-oriented downscaling methods, inspired by the potentially mutual reinforcement between downscaling and its inverse task, upscaling, increasing efforts have been focused on the upscaling-optimal downscaling methods, which aim to learn a downscaling model that is optimal to the post-upscaling operation. For instance, Kim et al. <ref type="bibr" target="#b25">[26]</ref> proposed a task-aware downscaling model based on an auto-encoder framework, in which the encoder and decoder act as the downscaling and upscaling model, respectively, such that the downscaling and upscaling processes are trained jointly as a united task. Similarly, Li et al. <ref type="bibr" target="#b33">[34]</ref> proposed to use a CNN to estimate downscaled compact-resolution images and leverage a learned or specified SR model for HR image reconstruction. More recently, Sun et al. <ref type="bibr" target="#b48">[49]</ref> proposed a new contentadaptive-resampler based image downscaling method, which can be jointly trained with any existing differentiable upscaling (SR) models. Although these attempts have an effect of pushing one of downscaling and upscaling to resemble the inverse process of the other, they still suffer from the ill-posed nature of image upscaling problem. In this paper, we propose to model the downscaling and upscaling processes by leveraging the invertible neural networks.</p><p>Difference from SR. Note that image upscaling is a different task from superresolution. In our scenario, the ground-truth HR image is available at the beginning but somehow we have to discard it and store/transmit the LR version instead. We hope that we can recover the HR image afterwards using the LR image. While for SR, the real HR is unavailable in applications and the task is to generate new HR images for LR ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Invertible Neural Network</head><p>The invertible neural network (INN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref> is a popular choice for generative models, in which the generative process x = f ? (z) given a latent variable z can be specified by an INN architecture f ? . The direct access to the inverse mapping z = f -1 ? (x) makes inference much cheaper. As it is possible to compute the density of the model distribution in INN explicitly, one can use the maximum likelihood method for training. Due to such flexibility, INN architectures are also used for many variational inference tasks <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>INN is composed of invertible blocks. In this study, we employ the invertible architecture in <ref type="bibr" target="#b15">[16]</ref>. For the l-th block, input h l is split into h l 1 and h l 2 along the channel axis, and they undergo the additive affine transformations <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_0">h l+1 1 = h l 1 + ?(h l 2 ), h l+1 2 = h l 2 + ?(h l+1 1 ),<label>(1)</label></formula><p>where ?, ? are arbitrary functions. The corresponding output is [h l+1 1 , h l+1 2 ]. Given the output, its inverse transformation is easily computed:</p><formula xml:id="formula_1">h l 2 = h l+1 2 -?(h l+1 1 ), h l 1 = h l+1 1 -?(h l 2 ),<label>(2)</label></formula><p>To enhance the transformation ability, the identity branch is often augmented <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_2">h l+1 1 = h l 1 exp(?(h l 2 )) + ?(h l 2 ), h l+1 2 = h l 2 exp(?(h l+1 1 )) + ?(h l+1 1 ), h l 2 = (h l+1 2 -?(h l+1 1 )) exp(-?(h l+1 1 )), h l 1 = (h l+1 1 -?(h l 2 )) exp(-?(h l 2 )).<label>(3)</label></formula><p>Some prior works studied using INN for paired data (x, y). Ardizzone et al. <ref type="bibr" target="#b2">[3]</ref> analyzed real-world problems from medicine and astrophysics. Compared to their tasks, image downscaling and upscaling bring more difficulties because of notably larger dimensionality, so that their losses do not work for our task. In addition, the ground-truth LR image y does not exist in our task. Guided image generation and colorization using INN is proposed in <ref type="bibr" target="#b3">[4]</ref> where the invertible modeling between x and z is conditioned on a guidance y. The model cannot generate y given x thus is unsuitable for the image upscaling task. INN is also applied to the image-to-image translation task <ref type="bibr" target="#b42">[43]</ref> where the paired domain (X, Y ) instead of paired data is considered, thus is again not the case of image upscaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Compression</head><p>Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Image compression may be lossy (e.g., JPEG, BPG) or lossless (e.g., PNG, BMP). Recently, deep learning based image compression methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">40]</ref> show promising results on both visual effect and compression ratio. However, the resolution of image won't be changed by compression, which means there is no visually meaningful low-resolution image but only bit-stream after compressing. Thus our task can't be served by image compression methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Specification</head><p>The sketch of our modeling framework is presented in Fig. <ref type="figure" target="#fig_0">1</ref>. As explained in Introduction, we mitigate the ill-posed problem of the upscaling task by modeling the distribution of lost information during downscaling. We note that according to the Nyquist-Shannon sampling theorem <ref type="bibr" target="#b46">[47]</ref>, the lost information during downscaling an HR image amounts to high-frequency contents. Thus we firstly employ a wavelet transformation to decompose the HR image x into low and high-frequency component, denote as x L and x H respectively. Since the case-specific high-frequency information will be lost after downscaling, in order to best recover the original x as possible in the upscaling procedure, we use an invertible neural network to produce the visually-pleasing LR image y meanwhile model the distribution of the lost information by introducing an auxiliary latent variable z. In contrast to the case-specific x H (i.e., x H ? p(x H |x L )), we force z to be case-agnostic (i.e., z ? p(z)) and obey a simple specified distribution, e.g., an isotropic Gaussian distribution. In this way, there is no further need to preserve either x H or z after downscaling, and z can be randomly sampled in the upscaling procedure, which is used to reconstruct x combined with LR image y by inversely passing the model. The general architecture of our proposed IRN is composed of stacked Downscaling Modules, each of which contains one Haar Transformation block and several invertible neural network blocks (InvBlocks), as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. We will show later that both of them are invertible, and thus the entire IRN model is invertible accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Invertible Architecture</head><p>The Haar Transformation We design the model to contain certain inductive bias, which can efficiently learn to decompose x into the downscaled image y and caseagnostic high-frequency information embedded in z. To achieve this, we apply the Haar Transformation as the first layer in each downscaling module, which can explicitly decompose the input images into an approximate low-pass representation, and three directions of high-frequency coefficients <ref type="bibr">[53][35][4]</ref>. More concretely, the Haar Transformation transforms the input raw images or a group of feature maps with height H, width W and channel C into a tensor of shape ( 1 2 H, 1 2 W, 4C). The first C slices of the output tensor are effectively produced by an average pooling, which is approximately a low-pass representation equivalent to the Bilinear interpolation downsampling. The rest three groups of C slices contain residual components in the vertical, horizontal and diagonal directions respectively, which are the high-frequency information in the original HR image. By such a transformation, the low and high-frequency information are effectively separated and will be fed into the following InvBlocks.</p><p>InvBlock Taking the feature maps after the Haar Transformation as input, a stack of InvBlocks is used to further abstract the LR and latent representations. We leverage the general coupling layer architecture proposed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, i.e. Eqs. <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3)</ref>.</p><p>Utilizing the coupling layer is based on our considerations that (1) the input has already been split into low and high-frequency components by the Haar transformation;</p><p>(2) we want the two branches of the output of a coupling layer to further polish the low and high-frequency inputs for a suitable LR image appearance and an independent and properly distributed latent representation of the high-frequency contents. So we match the low and high-frequency components respectively to the split of h l 1 , h l 2 in Eq. ( <ref type="formula" target="#formula_0">1</ref>). Furthermore, as the shortcut connection is proved to be important in the image scaling tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b49">50]</ref>, we employ the additive transformation (Eq. 1) for the low-frequency part h l 1 , and the enhanced affine transformation (Eq. 3) for the high-frequency part h l 2 to increase the model capacity, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Note that the transformation functions ?(?), ?(?), ?(?) in Fig. <ref type="figure" target="#fig_1">2</ref> can be arbitrary. Here we employ a densely connected convolutional block, which is referred as Dense Block in <ref type="bibr" target="#b49">[50]</ref> and demonstrated for its effectiveness of image upscaling task. Function ?(?) is further followed by a centered sigmoid function and a scale term to prevent numerical explosion due to the exp(?) function. Note that Figure <ref type="figure" target="#fig_1">2</ref> omits the exp(?) in function ?.</p><p>Quantization To save the output images of IRN as common image storage format such as RGB (8 bits for each R, G and B color channels), a quantization module is adopted which converts floating-point values of produced LR images to 8-bit unsigned int. We simply use rounding operation as the quantization module, store our output LR images by PNG format and use it in the upscaling procedure. There is one obstacle should be noted that the quantization module is nondifferentiable. To ensure that IRN can be optimized during training, we use Straight-Through Estimator <ref type="bibr" target="#b8">[9]</ref> on the quantization module when calculating the gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Objectives</head><p>Based on Section 3.1, our approach for invertible downscaling constructs a model that specifies a correspondence between HR image x and LR image y, as well as a caseagnostic distribution p(z) of z. The goal of training is to drive these modeled relations and quantities to match our desiderata and HR image data {x (n) } N n=1 . This includes three specific goals, as detailed below.</p><p>LR Guidance Although the invertible downscaling task does not pose direct requirements on the produced LR images, we do hope that they are valid visually pleasing LR images. To achieve this, we utilize the widely acknowledged Bicubic method <ref type="bibr" target="#b40">[41]</ref> to guide the downscaling process of our model. Let y (n) guide be the LR image corresponding to x (n) that is produced by the Bicubic method. To make our model follow the guidance, we drive the model-produced LR image f y ? (x (n) ) to resemble y (n) guide :</p><formula xml:id="formula_3">L guide (?) := N n=1 Y (y (n) guide , f y ? (x (n) )),<label>(4)</label></formula><p>where Y is a difference metric on Y, e.g., the L 1 or L 2 loss. We call it the LR guidance loss. This practice has also been adopted in the literature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b48">49]</ref>. HR Reconstruction Although f ? is invertible, it is not for the correspondence between x and y when z is not transmitted. We hope that for a specific downscaled LR image y, the original HR image can be restored by the model using any sample of z from the case-agnostic p(z). Inversely, this also encourages the forward process to produce a disentangled representation of z from y. As described in Section 3.1, given a HR image x (n) , the model-downscaled LR image f y ? (x (n) ) is to be upscaled by the model as f -1 ? (f y ? (x (n) ), z) with a randomly drawn z ? p(z). The reconstructed HR image should match the original one x (n) , so we minimize the expected difference and traverse over all the HR images:</p><formula xml:id="formula_4">L recon (?) := N n=1 E p(z) [ X (x (n) , f -1 ? (f y ? (x (n) ), z))],<label>(5)</label></formula><p>where X measures the difference between the original image and the reconstructed one. We call L recon (?) the HR reconstruction loss. For practical minimization, we estimate the expectation w.r.t. z by one random draw from p(z) for each evaluation. Distribution Matching The third part of the training goal is to encourage the model to catch the data distribution q(x) of HR images, demonstrated by its sample cloud {x (n) } N n=1 . Recall that the model reconstructs a HR image x (n) by f -1 ? (y (n) , z (n) ), where y (n) := f y ? (x (n) ) is the model-downscaled LR image, and z (n) ? p(z) is the randomly drawn latent variable. When traversing over the sample cloud of true HR images {x (n) } N n=1 , {y (n) } N n=1 also form a sample cloud of a distribution. We denote this distribution with the push-forward notation as f y ? # [q(x)], which represents the distribution of the transformed random variable f y ? (x) where the original random variable x obeys distribution q(x), x ? q(x). Similarly, the sample cloud {f -1 ? (y</p><formula xml:id="formula_5">(n) , z (n) )} N n=1</formula><p>represents the distribution of model-reconstructed HR images, and we denote it as</p><formula xml:id="formula_6">f -1 ? # f y ? # [q(x)] p(z) since (y (n) , z (n) ) ? f y ? # [q(x</formula><p>)] p(z) (note that y (n) and z (n) are independent due to the generation process). The desideratum of distribution matching is to drive the model-reconstructed distribution towards data distribution, which can be achieved by minimizing their difference measured by some metric of distributions:</p><formula xml:id="formula_7">L distr (?) := L P f -1 ? # f y ? # [q(x)] p(z) , q(x) .<label>(6)</label></formula><p>The distribution matching loss pushes the model-reconstructed HR images to lie on the manifold of true HR images so as to make the recovered images appear more realistic. It also drives the case-independence of z from y in the forward process. To see this, we note that if f ? is invertible, then in the asymptotic case, the two distributions match on X , i.e., f -1 ? # f y ? # [q(x)] p(z) = q(x), if and only if they match on</p><formula xml:id="formula_8">Y ? Z, i.e., f y ? # [q(x)] p(z) = f ? # [q(x)].</formula><p>The loss thus drives the coupled distribution</p><formula xml:id="formula_9">f ? # [q(x)] = (f y ? , f z ? ) # [q(x)</formula><p>] of (y, z) from the forward process towards the decoupled distribution f y ? # [q(x)] p(z). Neither effect can be fully guaranteed by the reconstruction and guidance losses.</p><p>As mentioned in Introduction, the minimization is generally hard since both distributions are high-dimensional and have unknown density function. We employ the JS divergence as the probability metric L P , and our distribution matching loss can be estimated in the following way:</p><formula xml:id="formula_10">L distr (?) = JS(f -1 ? # f y ? # [q(x)] p(z) , q(x)) ? 1 2N max T n log ?(T (x (n) )) + log 1 -? T f -1 ? (f y ? (x (n) ), z (n) ) + log 2,<label>(7)</label></formula><p>where {z (n) } N n=1 are i.i.d. samples from p(z), ? is the sigmoid function, T : X ? R is a function on X (?(T (?)) is regarded as a discriminator in GAN literatures), and "?" is due to Monte Carlo estimation. The appendix provides the details. For practical computation, the function T is parameterized as a neural network T ? and max T amounts to max ? . The expression ( <ref type="formula" target="#formula_10">7</ref>) is also suitable for estimating its gradient w.r.t. ? and ?, thus optimization is made practical.</p><p>Total Loss We optimize our IRN model by minimizing the compact loss L total (?) with the combination of HR reconstruction loss L recon (?), LR guidance loss L guide (?) and distribution matching loss L distr (?):</p><formula xml:id="formula_11">L total := ? 1 L recon + ? 2 L guide + ? 3 L distr ,<label>(8)</label></formula><p>where ? 1 , ? 2 , ? 3 are coefficients for balancing different loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Minimization in Practice</head><p>As an issue in practice, we find that directly minimizing the total loss L total (?) is difficult to train, due to the unstable training process of GANs <ref type="bibr" target="#b4">[5]</ref>. We propose a pre-training stage that adopts a weakened but more stable surrogate of the distribution matching loss. Recall that the distribution matching loss L P f -1 ? # f y ? # [q(x)] p(z) , q(x) on X has the same asymptotic effect as the loss</p><formula xml:id="formula_12">L P (f y ? # [q(x)] p(z), (f y ? , f z ? ) # [q(x)]) on Y ? Z.</formula><p>The surrogate considers partial distribution matching on Z, i.e., L P (p(z), f z ? # [q(x)]). Since the density function of one of the distributions, p(z), is now made available, we can choose more stable distribution metrics for minimization, such as the cross entropy (CE):</p><formula xml:id="formula_13">L distr (?) := CE(f z ? # [q(x)], p(z)) = -E f z ? # [q(x)] [log p(z)] = -E q(x) [log p(z = f z ? (x))].<label>(9)</label></formula><p>A related training method is the maximum likelihood estimation (MLE), i.e., max ? E q(x) [log f -1 ? # [p(y, z)]], which is widely adopted by prevalent flow-based generative models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4]</ref>. It is equivalent to minimizing the Kullback-Leibler (KL) divergence KL(q(x), f -1 ? # [p(y, z)]). The mentioned models explicitly specify the density function of p(y, z), thus the density function of f -1 ? # [p(y, z)] is made available together with the tractable Jacobian determinant computation of f ? . However, the same objective cannot be leveraged for our model since we do not have the density function for f y ? # [q(x)] p(z); only that of p(z) is known ? . The invertible neural network (INN) <ref type="bibr" target="#b2">[3]</ref> meets the same problem and cannot use MLE either.</p><p>We call IRN as our model trained by minimizing the following total objective:</p><formula xml:id="formula_14">L IRN := ? 1 L recon + ? 2 L guide + ? 3 L distr .<label>(10)</label></formula><p>After the pre-training stage, we restore the full distribution matching loss L distr in the objective in place of L distr . Additionally, we also employ a perceptual loss <ref type="bibr" target="#b24">[25]</ref> L percp on X , which measures the difference of two images via their semantic features extracted by benchmarking models. It enhances the perceptual similarity between generated and true images thus helps to produce more realistic images. The perceptual loss has several slightly modified variants which mainly differ in the position of the objective features <ref type="bibr" target="#b32">[33]</ref> <ref type="bibr" target="#b49">[50]</ref>. We adopt the variant proposed in <ref type="bibr" target="#b49">[50]</ref>. We call IRN+ as our model trained by minimizing the following total objective:</p><formula xml:id="formula_15">L IRN+ := ? 1 L recon + ? 2 L guide + ? 3 L distr + ? 4 L percp .</formula><p>Difference with GAN On one hand, although the JS divergence is adopted to instead of MLE as distribution matching loss for optimizing IRN, there is one thing should be noted that our model is totally different from typical GAN models: besides the latent variable z which has a prior, there exists y in IRN model which is subject to some distributional constraints, and our model does not have a standalone distribution on x. Therefore, the conventional way to use adversarial loss simply cannot be applied, and we match towards the data distribution with an essentially different distribution from the GAN model distribution. On the other hand, except for JS divergence, a CE loss for L distr is also adopted as distribution matching loss of IRN. In general, the distribution matching loss reflects the essential idea of IRN, which is totally different from GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Settings</head><p>We employ the widely used DIV2K <ref type="bibr" target="#b0">[1]</ref> image restoration dataset to train our model, which contains 800 high-quality 2K resolution images in the training set, and 100 in the validation set. Besides, we evaluate our model on 4 additional standard datasets, i.e. the Set5 <ref type="bibr" target="#b10">[11]</ref>, Set14 <ref type="bibr" target="#b57">[58]</ref>, BSD100 <ref type="bibr" target="#b38">[39]</ref>, and Urban100 <ref type="bibr" target="#b22">[23]</ref>. Following the setting in <ref type="bibr" target="#b35">[36]</ref>, we quantitatively evaluate the peak noise-signal ratio (PSNR) and SSIM <ref type="bibr" target="#b50">[51]</ref> on the Y channel of images represented in the YCbCr (Y, Cb, Cr) color space. Due to space constraint, we leave training strategy details in the appendix. ? MLEs corresponding to minimizing KL(q(x|y),</p><formula xml:id="formula_16">f -1 ? (y, ?) # [p(z)]) or KL q(x), E f y ? # [q(x)] [f -1 ? (y, ?)] # [p(z)</formula><p>] are also impossible, since the pushed-forward distributions have a.e. zero density in X so the KL is a.e. infinite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on Reconstructed HR Images</head><p>This section reports the quantitative and qualitative performance of HR image reconstruction with different downscaling and upscaling methods. We consider two kinds of reconstruction methods as our baselines: (1) downscaling with Bicubic interpolation and upscaling with state-of-the-art SR models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b13">14]</ref>; (2) downscaling with upscaling-optimal models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> and upscaling with SR models. For the method of <ref type="bibr" target="#b49">[50]</ref>, we denote ESRGAN as their pre-trained model, and ESRGAN+ as their GAN-based model. We further investigate the influence of different z samples on the reconstructed image x. Finally, we empirically study the effectiveness of the different types of loss in the pre-training stage.  <ref type="table" target="#tab_0">1</ref>, upscaling-optimal downscaling models largely enhance the reconstruction of HR images by state-of-the-art SR models compared with downscaling with Bicubic interpolation. However, they still hardly achieve satisfying results due to the ill-posed nature of upscaling. In contract, with the invertibility, IRN significantly boosts the PSNR metric about 4-5 dB and 2-3 dB on each benchmark dataset in 2? and 4? scale downsampling and reconstruction, and the improvement goes as large as 5.94 dB compared with the state-of-the-art downscaling and upscaling model. These results indicate an exponential improvement of IRN in the reduction of information loss, which also accords with the significant improvement in SSIM.</p><p>Moreover, the number of parameters of IRN is relatively small. When Bicubic downscaling and super-resolution methods require large model size (&gt;15M) for better results, our IRN only has 1.66M and 4.35M parameters in scale 2? and 4? respectively. It indicates that our model is light-weight and efficient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head><p>We then qualitatively evaluate IRN and IRN+ by demonstrating details of the upscaled images. As shown in Fig. <ref type="figure">8</ref>, HR images reconstructed by IRN and IRN+ achieve better visual quality and fidelity than those of previous state-ofthe-art methods. IRN recovers richer details, which contributes to the pleasing visual quality. IRN+ further produces sharper and more realistic images as the effect of the distribution matching objective. For the 'Comic' example, we observe that the IRN and IRN+ are the only models that can recover the complicated textures on the headwear and necklace, as well as the sharp and realistic fingers. Previous perceptual-driven methods such as ESRGAN <ref type="bibr" target="#b49">[50]</ref> also claim that the sharpness and reality of their generated HR images are satisfied. However, the visually unreasonable and unpleasing details produced by their model often lead to dissimilarity to the original images. We leave the high-resolution version and more results in the appendix for spacing reason. Visualisation on the Influence of z As described in previous sections, we aim to let z ? p(z) focus on the randomness of high-frequency contents only. In Table <ref type="table" target="#tab_0">1</ref>, the PSNR difference is less than 0.02 dB for each image with different samples of z. In order to verify whether z has learned only to influence high-frequency information, we calculate and present the difference between different draws of z in Fig. <ref type="figure">7</ref>. We can see in the figure that there is only a tiny noisy distinction in high-frequency regions without typical textures, which can hardly be perceived when combined with low-frequency contents. This indicates that our IRN has learned to reconstruct most meaningful highfrequency contents, while embedding senseless noise into randomness. As mentioned above, we train the model to encourage p(z) to obey a simple and easy-to-sample distribution, i.e., isotropic Gaussian distribution. In order to further verify the effectiveness of the learned model, we feed (y, ?z) into our IRN+ to obtain x ? by controlling the scale of sampled z with different values of ?. As shown in Fig. <ref type="figure">5</ref>, a larger deviation to the original distribution results in more noisy textures and distortion. It demonstrates that our model transforms z faithfully to follow the specified distribution, and is also robust to slight distribution deviation. Analysis on the Losses We conduct experiments to analyze the components in the loss of Eqs. <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9)</ref>. As shown in Table <ref type="table" target="#tab_1">2</ref>, IRN performs the best when the LR guidance loss is the L 2 loss and the HR reconstruction loss is the L 1 loss. The reason is that the L 1 loss encourages more pixel-wise similarity, while the L 2 loss is less sen-sitive to minor changes. In the forward procedure, we utilize the Bicubic-downscaled images as guidance, but we do not aim to exactly learn the Bicubic downscaling, which may harm the inverse procedure. The forward reconstruction loss only acts as a constraint to maintain visually pleasing downscaling, so the L 2 loss is more suitable. In the backward procedure, on the other hand, our goal is to reconstruct the ground truth image accurately. Therefore, the L 1 loss is more appropriate, as also identified by other super-resolution works. Table <ref type="table" target="#tab_1">2</ref> also demonstrates the necessity of the partial distribution matching loss of Eq. ( <ref type="formula" target="#formula_13">9</ref>), which restricts the marginal distributions on Z, and benefits the forward distribution learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on Downscaled LR Images</head><p>We also evaluate the quality of LR images downscaled by our IRN. We demonstrate the similarity index between our LR images and Bicubic-based LR images, and present similar visual perception of them, to show that IRN is able to perform as well as Bicubic. As shown in Table <ref type="table" target="#tab_2">3</ref>, images downscaled by IRN are extremely similar to those by Bicubic. Fig. <ref type="figure" target="#fig_1">12</ref> and more figures in the appendix illustrate the visual similarity between them, which demonstrates the proper perception of our downscaled images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel invertible network for the image rescaling task, with which the ill-posed nature of the task is largely mitigated. We explicitly model the statistics of the case-specific high-frequency information that is lost in downscaling as a latent variable following a specified case-agnostic distribution which is easy to sample from. The network models the rescaling processes by invertibly transforming between an HR image and an LR image with the latent variable. With the statistical knowledge of the latent variable, we draw a sample of it for upscaling from a downscaled LR image (whose specific high-frequency information was lost during downscaling, of course). We design a specific invertible architecture tailored for image rescaling, and an effective training objective to enforce the model to have desired downscaling and upscaling behavior, as well as to output the latent variable with the specified properties. Extensive experiments demonstrate that our model significantly improves both quantitative and qualitative performance of upscaling reconstruction from downscaled LR images, while being light-weighted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Invertible Image Rescaling</head><p>A Details of the distribution loss</p><p>According to the main text, we choose the Jensen-Shannon (JS) divergence as the distribution metric and minimize the difference between f -1 ? # f y ? # [q(x)] p(z) and q(x):</p><formula xml:id="formula_17">L distr (?) = JS(f -1 ? # f y ? # [q(x)] p(z) , q(x)) = 1 2 max T E q(x) [log ?(T (x))] + E x ?f -1 ? # f y ? # [q(x)] p(z) [log (1 -?(T (x )))] + log 2 = 1 2 max T E q(x) [log ?(T (x))] + E (y,z)?f y ? # [q(x)] p(z) log 1 -?(T (f -1 ? (y, z))) + log 2 ? 1 2N max T n log ?(T (x (n) )) + log 1 -?(T (f -1 ? (f y ? (x (n) ), z (n) ))) + log 2.<label>(11)</label></formula><p>The first equality stems from the variational form of the JS divergence which is composed for training generative adversarial nets <ref type="bibr" target="#b20">[21]</ref>. The second equality is a reformulation using the definition of pushed-forward distribution. The third approximate equality leads to a Monte Carlo estimation to the objective function using the corresponding samples: {z (n) } N n=1 i.i.d. drawn from p(z), and {x (n) } N n=1 ? q(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed Training Strategies on DIV2K dataset</head><p>We train and compare our model in 2? and 4? downscaling scale with one and two downscaling modules respectively. Each downscaling module has 8 InvBlocks and downscale the original image by 2?. We use Adam optimizer <ref type="bibr" target="#b27">[28]</ref> with ? 1 = 0.9, ? 2 = 0.999 to train our model. The learning rate is initialized as 1 ? 10 -4 and halved at [5k, 10k] iterations. We set ? 1 = 0.01, ? 2 = 16, ? 3 = 1, ? 4 = 0.01 in Eqn.11 and pre-train the discriminator for 5000 iterations. The discriminator is similar to <ref type="bibr" target="#b32">[33]</ref>, which contains eight convolutional layers with 3 ? 3 kernels, whose numbers increase from 64 to 512 by a factor 2 each two layers, and two dense layers with 100 hidden units. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Different samples of z</head><p>As shown in Fig. <ref type="figure">7</ref>, there are only tiny noisy distinction in high-frequency areas without typical textures, which can hardly perceived when combined with low-frequency contents. Different samples lead to different but perceptually meaningless noisy distinctions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More qualitative results</head><p>As shown in Fig. <ref type="figure">8</ref>,9,10,11, images reconstructed by IRN and IRN+ significantly outperforms previous both PSNR-oriented and perceptual-driven methods in visual quality and similarity to original images. IRN is able to reconstruct rich details including detailed lines and textures, which contributes to the pleasing perception. IRN+ further produce sharper and more realistic images as a result of the distribution matching objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Evaluation on downscaled images</head><p>As shown in Fig. <ref type="figure" target="#fig_1">12</ref>, images downscaled by IRN share a similar visual perception with images downscaled by bicubic. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the problem formulation. In the forward downscaling procedure, HR image x is transformed to visually pleasing LR image y and case-agnostic latent variable z through a parameterized invertible function f ? (?); in the inverse upscaling procedure, a randomly drawn z combined with LR image y are transformed to HR image through the inverse function f -1 ? (?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our framework. The invertible architecture is composed of Downscaling Modules, in which InvBlocks are stacked after a Haar Transformation. Each Downscaling Module reduces the spatial resolution by 2?. The exp(?) of ? is omit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative results of upscaling the 4? downscaled images. IRN recovers rich details, leading to both visually pleasing performance and high similarity to the original images. IRN+ produces even sharper and more realistic details. See the appendix for more results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Visualisation of the difference of upscaled HR images from multiple draws of z. (a): original image; (b-d): HR image differences of three z drawn from a common z sample. Darker color means larger difference. It shows that the differences are random noise in high-frequency regions without a typical texture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Demonstration of the downscaled images from Set14 and DIV2K validation sets. Left column (a,c): Image downscaled by Bicubic. Right column (b,d): Image downscaled by IRN. They share a similar visual perception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 11 .Fig. 12 .</head><label>71112</label><figDesc>Fig. 7. Difference between upscaled images by different samples of z. (a): Original image. (bd): Residual of three randomly upscaled images with another sample (averaged over the three channels). (e-g): Detailed difference of (b-d). The darker the larger difference. To ensure the visual perception, we set rebalance factor by 20.</figDesc><graphic url="image-56.png" coords="22,158.04,446.05,90.00,90.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation results (PSNR / SSIM) of different downscaling and upscaling methods for image reconstruction on benchmark datasets: Set5, Set14, BSD100, Urban100, and DIV2K validation set. For our method, differences on average PSNR / SSIM from different z samples are less than 0.02. We report the mean result over 5 draws.Quantitative Results Table1summarizes the quantitative comparison results of different reconstruction methods where IRN significantly outperforms previous state-ofthe-art methods regarding PSNR and SSIM in all datasets. We leave the results of IRN+ in the appendix because it is a visual-perception-oriented model. As shown in Table</figDesc><table><row><cell cols="3">Downscaling &amp; Upscaling Scale Param</cell><cell>Set5</cell><cell>Set14</cell><cell>BSD100</cell><cell>Urban100</cell><cell>DIV2K</cell></row><row><cell>Bicubic &amp; Bicubic</cell><cell>2?</cell><cell>/</cell><cell cols="5">33.66 / 0.9299 30.24 / 0.8688 29.56 / 0.8431 26.88 / 0.8403 31.01 / 0.9393</cell></row><row><cell cols="7">Bicubic &amp; SRCNN [17] 2? 57.3K 36.66 / 0.9542 32.45 / 0.9067 31.36 / 0.8879 29.50 / 0.8946</cell><cell>-</cell></row><row><cell>Bicubic &amp; EDSR [36]</cell><cell cols="7">2? 40.7M 38.20 / 0.9606 34.02 / 0.9204 32.37 / 0.9018 33.10 / 0.9363 35.12 / 0.9699</cell></row><row><cell>Bicubic &amp; RDN [60]</cell><cell cols="6">2? 22.1M 38.24 / 0.9614 34.01 / 0.9212 32.34 / 0.9017 32.89 / 0.9353</cell><cell>-</cell></row><row><cell>Bicubic &amp; RCAN [59]</cell><cell cols="6">2? 15.4M 38.27 / 0.9614 34.12 / 0.9216 32.41 / 0.9027 33.34 / 0.9384</cell><cell>-</cell></row><row><cell>Bicubic &amp; SAN [14]</cell><cell cols="6">2? 15.7M 38.31 / 0.9620 34.07 / 0.9213 32.42 / 0.9028 33.10 / 0.9370</cell><cell>-</cell></row><row><cell>TAD &amp; TAU [26]</cell><cell>2?</cell><cell>-</cell><cell>38.46 / -</cell><cell>35.52 / -</cell><cell>36.68 / -</cell><cell>35.03 / -</cell><cell>39.01 / -</cell></row><row><cell cols="2">CNN-CR &amp; CNN-SR [34] 2?</cell><cell>-</cell><cell>38.88 / -</cell><cell>35.40 / -</cell><cell>33.92 / -</cell><cell>33.68 / -</cell><cell>-</cell></row><row><cell>CAR &amp; EDSR [49]</cell><cell cols="7">2? 51.1M 38.94 / 0.9658 35.61 / 0.9404 33.83 / 0.9262 35.24 / 0.9572 38.26 / 0.9599</cell></row><row><cell>IRN (ours)</cell><cell cols="7">2? 1.66M 43.99 / 0.9871 40.79 / 0.9778 41.32 / 0.9876 39.92 / 0.9865 44.32 / 0.9908</cell></row><row><cell>Bicubic &amp; Bicubic</cell><cell>4?</cell><cell>/</cell><cell cols="5">28.42 / 0.8104 26.00 / 0.7027 25.96 / 0.6675 23.14 / 0.6577 26.66 / 0.8521</cell></row><row><cell cols="7">Bicubic &amp; SRCNN [17] 4? 57.3K 30.48 / 0.8628 27.50 / 0.7513 26.90 / 0.7101 24.52 / 0.7221</cell><cell>-</cell></row><row><cell>Bicubic &amp; EDSR [36]</cell><cell cols="7">4? 43.1M 32.62 / 0.8984 28.94 / 0.7901 27.79 / 0.7437 26.86 / 0.8080 29.38 / 0.9032</cell></row><row><cell>Bicubic &amp; RDN [60]</cell><cell cols="6">4? 22.3M 32.47 / 0.8990 28.81 / 0.7871 27.72 / 0.7419 26.61 / 0.8028</cell><cell>-</cell></row><row><cell>Bicubic &amp; RCAN [59]</cell><cell cols="7">4? 15.6M 32.63 / 0.9002 28.87 / 0.7889 27.77 / 0.7436 26.82 / 0.8087 30.77 / 0.8460</cell></row><row><cell cols="8">Bicubic &amp; ESRGAN [50] 4? 16.3M 32.74 / 0.9012 29.00 / 0.7915 27.84 / 0.7455 27.03 / 0.8152 30.92 / 0.8486</cell></row><row><cell>Bicubic &amp; SAN [14]</cell><cell cols="6">4? 15.7M 32.64 / 0.9003 28.92 / 0.7888 27.78 / 0.7436 26.79 / 0.8068</cell><cell>-</cell></row><row><cell>TAD &amp; TAU [26]</cell><cell>4?</cell><cell>-</cell><cell>31.81 / -</cell><cell>28.63 / -</cell><cell>28.51 / -</cell><cell>26.63 / -</cell><cell>31.16 / -</cell></row><row><cell>CAR &amp; EDSR [49]</cell><cell cols="7">4? 52.8M 33.88 / 0.9174 30.31 / 0.8382 29.15 / 0.8001 29.28 / 0.8711 32.82 / 0.8837</cell></row><row><cell>IRN (ours)</cell><cell cols="7">4? 4.35M 36.19 / 0.9451 32.67 / 0.9015 31.64 / 0.8826 31.41 / 0.9157 35.07 / 0.9318</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Analysis results (PSNR/SSIM) of training IRN with L1 or L2 LR guide and HR reconstruction loss, with/without partial distribution matching loss, on Set5, Set14, BSD100, Urban100 and DIV2K validation sets with scale 4?.</figDesc><table><row><cell cols="3">L guide Lrecon L distr</cell><cell>Set5</cell><cell>Set14</cell><cell>BSD100</cell><cell>Urban100</cell><cell>DIV2K</cell></row><row><cell>L1</cell><cell>L1</cell><cell cols="5">Yes 34.75 / 0.9296 31.42 / 0.8716 30.42 / 0.8451 30.11 / 0.8903 33.64 / 0.9079</cell></row><row><cell>L1</cell><cell>L2</cell><cell cols="5">Yes 34.93 / 0.9296 31.76 / 0.8776 31.01 / 0.8562 30.79 / 0.8986 34.11 / 0.9116</cell></row><row><cell>L2</cell><cell>L1</cell><cell cols="5">Yes 36.19 / 0.9451 32.67 / 0.9015 31.64 / 0.8826 31.41 / 0.9157 35.07 / 0.9318</cell></row><row><cell>L2</cell><cell>L2</cell><cell cols="5">Yes 35.93 / 0.9402 32.51 / 0.8937 31.64 / 0.8742 31.40 / 0.9105 34.90 / 0.9308</cell></row><row><cell>L2</cell><cell>L1</cell><cell cols="5">No 36.12 / 0.9455 32.18 / 0.8995 31.49 / 0.8808 30.91 / 0.9102 34.90 / 0.9308</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>SSIM results between the images downscaled by IRN and by Bicubic on the Set5, Set14, BSD100, Urban100 and DIV2K validation sets.</figDesc><table><row><cell>Scale</cell><cell>Set5</cell><cell>Set14</cell><cell cols="3">BSD100 Urban100 DIV2K</cell></row><row><cell>2?</cell><cell cols="2">0.9957 0.9936</cell><cell>0.9936</cell><cell>0.9941</cell><cell>0.9945</cell></row><row><cell>4?</cell><cell cols="2">0.9964 0.9927</cell><cell>0.9923</cell><cell>0.9916</cell><cell>0.9933</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>60. Zhang, Y., Tian, Y., Kong, Y., Zhong, B., Fu, Y.: Residual dense network for image superresolution. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2472-2481 (2018) 2, 3, 11, 20</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The mini-batch size is set to<ref type="bibr" target="#b15">16</ref>. The input HR image is randomly cropped into 144 ? 144 and augmented by applying random horizontal and vertical flips. In the pre-training stage, the total number of iteration is 50K, and the learning rate is initialized as 2 ? 10 -4 where halved at [10k, 20k, 30k, 40k] mini-batch updates. The hyper-parameters in Eqn.10 are set as ? 1 = 1, ? 2 = 16, ? 3 = 1. After pre-training, we finetune our model for another 20K iterations as described in Sec.3.3.   </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Quantitative evaluation results (PSNR / SSIM) of different 4? image downscaling and upscaling methods on benchmark datasets: Set5, Set14, BSD100, Urban100, and DIV2K validation set. For our model, differences on average PSNR / SSIM of different samples for z are less than 0.02. We report the mean result. The best result is in red, while the second is in blue. .8104  26.00 / 0.7027 25.96 / 0.6675 23.14 / 0.6577 26.66 / 0.8521 Bicubic &amp; SRCNN [17] 4? 57.3K 30.48 / 0.8628 27.50 / 0.7513 26.90 / 0.7101 24.52 / 0.7221 -Bicubic &amp; EDSR [36] 4? 43.1M 32.62 / 0.8984 28.94 / 0.7901 27.79 / 0.7437 26.86 / 0.8080 29.38 / 0.9032 Bicubic &amp; RDN [60] 4? 22.3M 32.47 / 0.8990 28.81 / 0.7871 27.72 / 0.7419 26.61 / 0.8028 -Bicubic &amp; RCAN [59] 4? 15.6M 32.63 / 0.9002 28.87 / 0.7889 27.77 / 0.7436 26.82 / 0.8087 30.77 / 0.8460 Bicubic &amp; ESRGAN [50] 4? 16.3M 32.74 / 0.9012 29.00 / 0.7915 27.84 / 0.7455 27.03 / 0.8152 30.92 / 0.8486 Bicubic &amp; SAN [14] 4? 15.7M 32.64 / 0.9003 28.92 / 0.7888 27.78 / 0.7436 26.79 / 0.8068 8M 33.88 / 0.9174 30.31 / 0.8382 29.15 / 0.8001 29.28 / 0.8711 32.82 / 0.8837 IRN (ours) 4? 4.35M 36.19 / 0.9451 32.67 / 0.9015 31.64 / 0.8826 31.41 / 0.9157 35.07 / 0.9318 IRN+ (ours) 4? 4.35M 33.59 / 0.9147 29.97 / 0.8444 28.94 / 0.8189 28.24 / 0.8684 32.24 / 0.8921 C Quantitive results of IRN+ IRN+ aims at producing more realistic images by minimizing the distribution difference, not exactly matching details of original images as IRN does. The difference will lead to lower PSNR and SSIM, which is the same as GAN-based super-resolution methods. Despite the difference, IRN+ still outperforms most methods in PSNR and SSIM as shown in Table.4, demonstrating the good similarity between the reconstructed images and original HR images.</figDesc><table><row><cell cols="3">Downscaling &amp; Upscaling Scale Param</cell><cell>Set5</cell><cell>Set14</cell><cell>BSD100</cell><cell>Urban100</cell><cell>DIV2K</cell></row><row><cell>Bicubic &amp; Bicubic</cell><cell>4?</cell><cell>/</cell><cell cols="5">28.42 / 0-</cell></row><row><cell>TAD &amp; TAU [26]</cell><cell>4?</cell><cell>-</cell><cell>31.81 / -</cell><cell>28.63 / -</cell><cell>28.51 / -</cell><cell>26.63 / -</cell><cell>31.16 / -</cell></row><row><cell>CAR &amp; EDSR [49]</cell><cell cols="2">4? 52.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for extreme learned image compression</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analyzing inverse problems with invertible neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wirkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rahner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Klessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>K?the</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning and Representations</title>
		<meeting>the International Conference on Learning and Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>L?th</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>K?the</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02392</idno>
		<title level="m">Guided image generation with conditional invertible neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning and Representations</title>
		<meeting>the International Conference on Learning and Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01704</idno>
		<title level="m">End-to-end optimized image compression</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01436</idno>
		<title level="m">Variational image compression with a scale hyperprior</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sylvester normalizing flows for variational inference</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Low-complexity singleimage super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Down-scaling for better transform compression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02735</idno>
		<title level="m">Residual flows for invertible generative modeling</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1920">2019) 2, 3, 11, 20</date>
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NICE: Non-linear independent components estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of the International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2009">2017) 3, 4, 5, 7, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time artifact-free image upscaling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giachetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Asuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>NIPS Foundation, Montral, Canada</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FFJORD: Freeform continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Betterncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning and Representations</title>
		<meeting>the International Conference on Learning and Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed selfexemplars</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G S B M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and superresolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Task-aware image downscaling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="1920">2018) 1, 2, 4, 8, 11, 20</date>
			<biblScope unit="page" from="399" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Content-adaptive image downscaling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01434</idno>
		<title level="m">Videoflow: A flow-based generative model for video</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for image compact-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An extended set of haar-like features for rapid object detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. international conference on image processing</title>
		<editor>
			<persName><forename type="first">I-I</forename><surname>Ieee</surname></persName>
		</editor>
		<meeting>international conference on image processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="1920">2017) 2, 3, 7, 10, 11, 20</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive downsampling to improve image compression at low bit rates</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">l {0}-regularized image downscaling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">10</biblScope>
			<pubPlace>Vancouver</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint autoregressive and hierarchical priors for learned image compression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reconstruction filters in computer-graphics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Netravali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Siggraph Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1988">1988</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Perceptually based downscaling of images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Oeztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reversible gans for memory-efficient image-to-image translation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Van Der Ouderaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time adaptive image compression</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with superresolution forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Communication in the presence of noise</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IRE</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Down-sampling based video coding using super-resolution technique</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learned image downscaling for upscaling using content adaptive resampler</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4027" to="4040" />
			<date type="published" when="1920">2020) 1, 2, 4, 8, 11, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Change Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018) 2, 3, 7, 10, 11</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rapid, detail-preserving image downscaling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Waechter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Amend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Facial feature detection using haar classifiers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computing Sciences in Colleges</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Low bit-rate image compression via adaptive down-sampling and constrained least squares upconversion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">How will deep learning change internet video delivery?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM Workshop on Hot Topics in Networks</title>
		<meeting>the 16th ACM Workshop on Hot Topics in Networks</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural adaptive content-aware internet video delivery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
