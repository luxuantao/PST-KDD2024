<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimization Strategies for Complex Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
							<email>strohman@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst, Howard Turtle</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Cogitech Jackson Hole</orgName>
								<address>
									<postCode>83001 W</postCode>
									<settlement>Bruce Croft</settlement>
									<region>WY</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimization Strategies for Complex Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">83EEFAE89C39CCFCCB94371BF51D5038</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H3.3 Information Storage and Retrieval: Information Search and Retrieval Algorithms</term>
					<term>Design</term>
					<term>Performance Indexing</term>
					<term>Query Processing</term>
					<term>Efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous research into the efficiency of text retrieval systems has dealt primarily with methods that consider inverted lists in sequence; these methods are known as term-at-a-time methods. However, the literature for optimizing documentat-a-time systems remains sparse.</p><p>We present an improvement to the max score optimization, which is the most efficient known document-at-a-time scoring method. Like max score, our technique, called term bounded max score, is guaranteed to return exactly the same scores and documents as an unoptimized evaluation, which is particularly useful for query model research. We simulated our technique to explore the problem space, then implemented it in Indri, our large scale language modeling search engine. Tests with the GOV2 corpus on title queries show our method to be 23% faster than max score alone, and 61% faster than our document-at-a-time baseline. Our optimized query times are competitive with conventional termat-a-time systems on this year's TREC Terabyte task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>It might seem that the performance of retrieval systems would not be a major issue anymore, as computer speeds continue to increase. Unfortunately, the document collections that users wish to access continue to grow. Perhaps more importantly, users have been taught by web search engines to expect query results in less than one second, no matter how large the collection or how complex the query.</p><p>Many query optimization strategies have been presented in the literature in order to deal with this problem. In general, these methods take a scoring method, such as cosine similarity or language modeling, and develop an approximation of this method that can be computed efficiently. The best of these optimization methods have impressive speedups over baseline systems and have only a small effect on retrieval effectiveness.</p><p>The fastest known query processing techniques are termat-a-time algorithms, some of which use term frequency sorted lists. These methods use accumulators to store partial scores of documents during query evaluation. This works well because these accumulators are small, and by using optimization strategies the number of accumulators can be trimmed to keep memory usage low. However, when term position information is involved these accumulators must be much larger. Furthermore, traditional accumulator trimming techniques do not apply to queries using term position information.</p><p>These problems suggest a document-at-a-time evaluation strategy for complex queries. In document-at-a-time evaluation, all inverted lists are considered simultaneously, so there is no need to store proximity information in memory. However, there has been much less research into optimizing document-at-a-time systems than term-at-a-time systems.</p><p>Brown <ref type="bibr" target="#b2">[2]</ref> and Turtle and Flood <ref type="bibr" target="#b8">[8]</ref> present two of the most promising document-at-a-time query processing strategies. In this paper we show that these strategies are in fact complementary; by combining the essence of these methods, we have an optimization method that is significantly faster than the max score method of Turtle and Flood. However, it manages to maintain the most desirable properties of max score; most importantly, the results returned by our method are exactly those that would be returned by an unoptimized system. This makes our optimization method particularly useful for query model research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MOTIVATION</head><p>Our research is motivated by the Indri retrieval system, a new language modeling search engine developed at the University of Massachusetts Amherst. This system incorporates recent work by Metzler and Croft <ref type="bibr" target="#b6">[6]</ref> which combines the language modeling and inference network approaches to information retrieval. By leveraging this work, Indri supports many of the structured query operators from INQUERY <ref type="bibr" target="#b4">[4]</ref>. In addition, Indri supports new operators for dealing with document fields, part of speech and named entity tagging, passage retrieval and numeric quantities.</p><p>A subset of the Indri query language constructs is shown which scores all sentences in the corpus that contain the phrase "George W. Bush" followed by a number, followed by "president". This query could be used to find sentences of the form "George W. Bush is the 43rd president of the United States." Quickly finding passages of text that meet a particular form is a critical part of many question answering systems.</p><p>The field structure of the query language can also be used for structured data, as in this query:</p><p>#combine( #uw1( howard turtle ).author #uw1( james flood ).author #1( query evaluation ).title )</p><p>which searches for the paper on which much of this work is based. It is clear that the Indri query language relies heavily on the location of terms within documents, and therefore a document-at-a-time evaluation strategy is appropriate. However, we would still like to be able to evaluate simpler queries quickly, such as these from the 2004 TREC Terabyte task: pearl farming prostate cancer treatments Our goal with this work is to achieve good performance on these simpler queries without resorting to an entirely different evaluation strategy than we use for complex queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RELATED WORK</head><p>All modern retrieval systems use inverted lists to evaluate queries efficiently <ref type="bibr">[9]</ref>. The differences between the optimizations discussed here lie in how these inverted lists are processed. In term-at-a-time systems, the inverted list for each term is considered separately. These systems use table of score accumulators to keep track of partial scores for documents that have been seen. At the end of evaluation, the accumulators are sorted, and the top k documents (where k is a parameter given by the user) are returned to the user. In document-at-a-time systems, the inverted lists are considered simultaneously, much like the classical merge sort algorithm <ref type="bibr" target="#b5">[5]</ref>. In this case, there are no partial scores; this allows the system to maintain a list of the top k scores it has seen so far.</p><p>One of the earliest papers on modern query optimization comes from Buckley <ref type="bibr" target="#b3">[3]</ref>. In this approach, terms are evaluated one at a time, from the least frequent term to the most frequent term. At some point during query evaluation, it may be possible to show that it is not necessary to consider any more terms, as the document at rank k + 1 cannot surpass the score of the document at rank k.</p><p>This approach, and approaches based on it, have the advantage that some inverted lists will not need to be read from disk. As the gap between disk access speed and processing speed continues to increase, this is an attractive feature. However, this approach has trouble with duplicate (or nearly duplicate) documents in the collection. If two documents at ranks k and k + 1 will evaluate to the same score, the Buckley algorithm will read the inverted lists for all the terms in the query.</p><p>For large collections, the likelihood that two documents will evaluate to the same score is greatly increased, even among documents that are not exactly the same. For example, many legal documents follow specific templates, with only minor changes to the template text. It is likely that many documents generated with the same template will be the same length, and will therefore have the same score for many queries. In order to guard against this effect, the exactness conditions for ranking order must be relaxed. Buckley suggests a method for doing this.</p><p>Moffat and Zobel <ref type="bibr" target="#b7">[7]</ref> evaluate two heuristics, Quit and Continue, which reduce the time necessary to evaluate termat-a-time queries. The Quit heuristic dynamically adds accumulators while query processing continues, until the num-ber of accumulators meets some fixed threshold. At this point, documents are ranked by the partial scores in the accumulators and returned to the user. The Continue strategy is similar, in that it uses only a fixed number of accumulators. However, when the accumulator threshold is reached, it continues query evaluation, but only considers those documents that already have accumulators allocated. The Continue method was found to be particularly effective; at times it was more effective than the baseline system.</p><p>Brown <ref type="bibr" target="#b2">[2]</ref> presents a method for efficiently finding a small list of candidate documents for scoring. These candidate documents are considered to be the most likely set of documents to appear in the top k results. This short list of candidates can be scored quickly by skipping through inverted lists. To create the candidates list for a query, the search engine takes the union of term-specific candidates lists created at index time. For a given term t, its candidate list contains the top documents in the ranked list for the query t. Brown finds excellent speedups for this approach.</p><p>The method presented in this paper uses candidate lists as well, but it uses these lists as a hint for finding top scoring documents. In a query where the terms have extremely low co-occurrence, following only term candidates lists is likely to lead to poor effectiveness. For instance, the best results for the query "howard turtle" are unlikely to come from the candidates for "howard" or "turtle".</p><p>Broder, et al. <ref type="bibr" target="#b1">[1]</ref>, consider query evaluation in a two stage process. First, the query is run as a Boolean and query, where a document is only scored if all terms appear. If this process finds at least k documents, the process stops. However, if the number of documents found is less than k, a second query is issued that considers all documents that contain any query term.</p><p>Turtle and Flood <ref type="bibr" target="#b8">[8]</ref>, consider a series of query optimization techniques, including some exact methods and some approximate methods.</p><p>In the document-at-a-time max score method, the top candidate document for each term is stored in the index, like the approach taken by Brown, except only one document is stored.</p><p>Document-at-a-time max score works much like a traditional document-at-a-time system, until k documents have been scored. At this point, the smallest score becomes a lower bound for document scores returned for this query. As the query continues, the algorithm maintains this k th smallest score, which will become more accurate as query processing continues. In this paper, we refer to this score as the threshold score.</p><p>The max score algorithm also sorts terms in order by frequency of occurrence. The terms that appear most frequently will contribute the least to document scores. Let the most frequent term be t0. We hope that at some point during query processing, the threshold score will be large enough to prove that any document appearing in the top k results must contain some query term other than t0. This allows only documents containing at least one of t1, ...tn to be scored. When conditions are favorable, max score scores only documents that contain a few discriminating terms. This is somewhat like the Continue approach of Moffat and Zobel, except the process is dynamic and guarantees correct results.</p><p>The term-at-a-time max score method is similar to the method presented by Buckley. However, Turtle and Flood explore a rank-safe optimization, where terms are evaluated until the top k documents are guaranteed to be in the correct order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Finding Top Documents</head><p>In the query likelihood formulation of language modeling, we rank documents by P (Q|D): the probability that the query was generated by the same language model as a given document. We estimate this probability by considering the probability of generation of each term independently, as follows:</p><formula xml:id="formula_0">P (Q|D) = q i ∈Q P (qi|D)</formula><p>Since the document is likely to be short, and therefore may not contain enough data to reliably estimate P (qi|D), we estimate P (qi|D) by smoothing against the term distribution in the collection, C. For this research, we use linear interpolation as follows:</p><formula xml:id="formula_1">P (qi|D) = (1 -λ) c(qi; D) |D| + λ c(qi; C) |C|</formula><p>In this case, c(qi; D)/|D| represents the number of times the word qi appears in the document D divided by the length of D. The second term, c(q i ;C) |C| is similar, but represents the count of qi in the entire corpus divided by the corpus length.</p><p>Note that in the formulation above, only the c(q i ;D)</p><p>|D| component varies with respect to the current document; therefore, the documents for which P (qi|D) is largest will be those for which c(q i ;D) |D| is largest. In the term bounded max score method, we rank all documents in the collection by c(t;D)</p><p>|D| for each term t. If we stored the entire ranking, we would have inverted lists sorted by term frequency. However, our goal is for these lists to be quite small, so that they do not substantially increase the size of our index. We call these lists topdocs lists, as they were called in the INQUERY system. In Section 6.1, we test different methods for determining how long these lists should be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ALGORITHM</head><p>Our method, term bounded max score, is related to the max score work of Turtle and Flood <ref type="bibr" target="#b8">[8]</ref>. However, like Brown <ref type="bibr" target="#b2">[2]</ref>, we use topdocs lists of for each term.</p><p>The key to this optimization, like many other methods, is that we are not concerned with ranking every document in the collection; we are only concerned with the top k documents in the ranked list. Our goal is to score as few documents as possible while still finding the top k documents for our query.</p><p>At the beginning of query evaluation, we take the union of the topdocs lists for each term. This gives us a candidates list of documents that we must score. We suppose that the lengths of these topdocs lists are short enough that scoring these documents will take a small fraction of the query time.</p><p>Based just on the information in these topdocs lists, we compute an approximate score for each candidate document. This score is guaranteed to be less than or equal to the true score for each document. We then take the approximate Pruning Method Description Fixed</p><p>The top documents list contains the same fixed number of documents for each term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fraction</head><p>The top documents list for a term contains some constant fraction of all the documents that contain that term. In this case, very frequent words, like "the", will have more top documents than less frequent words, like "abstruse".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frequency</head><p>The top documents list contains all documents where the term constitutes more than some fixed fraction of the document. For instance, the lists might contain all documents where the term accounted more than 1% of word occurrences.   score of the k th document to be the threshold score; we know that the k th document will have a score at least as high as this one.</p><p>We also take the lowest scoring document from each of the top documents lists, and use these documents to bound the score of the remaining documents in the collection. For some document d that contains term t, document d either appears in the top documents list for term t, or it has a lower term score for t than any document in the top documents list for t. This gives us a bound on the term score of any document that is not in the top documents list for t.</p><p>We then execute the max score algorithm, but we use the tighter term score bounds and the threshold computed from the candidates list. We make sure to score every document in the candidates list, plus every document we are directed to score by the max score algorithm. Like the max score algorithm, this technique is guaranteed to give exactly the same results as a system that scored every document.</p><p>As we show in the experimental section, this optimization gives an excellent increase in performance in practice. One intuition about this effect is that the "good" documents are in the top documents list, and this method pinpoints them more effectively. However, early experimentation does not necessarily support this hypothesis; it is surprising how often a high scoring document is not in any of the topdocs lists.</p><p>We prefer to think of the top documents lists as containing the outliers for each term. These high scoring documents keep us from forming a strong bound on the remaining documents in each list. Once we have segregated them from the rest of the data, we get bounds that better reflect the actual data in the collection, and it is these bounds that allow max score to work more effectively.</p><p>We find that these outliers are more prominent in web data than in collections that have been considered in the past. It is not uncommon to find extremely short documents in web collections; sometimes we find documents containing a single word. These single word documents have extremely high term scores, since c(w;d) |d| = 1. A single one-word document containing the word "the" can cause max score to score hundreds of thousands of documents unnecessarily. The top documents lists remove these outliers from general consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL APPROACH</head><p>We tested our algorithm on the TREC GOV2 collection, which consists of approximately 25 million web pages crawled from the .gov domain in January 2004. The collection is 426GB uncompressed, and contains 22.7 billion words. We considered using a news corpus as well for comparison, but we did not have access to a news corpus large enough to reliably estimate query timings. We expect that our system is particularly well suited for the irregular data found on the web, and may not perform as well on news corpus data.</p><p>We tested our approach using the 50 title queries (701-750) from the 2004 TREC Terabyte track. These queries average 3.02 terms per query, and are content queries, not named-page finding queries. These queries do not contain known stopwords, and every query contains at least two terms.</p><p>We indexed the GOV2 collection using Indri, a new scalable language modeling search engine developed at the University of Massachusetts. For our initial experimentation, we exported the inverted lists from this index to an simulation system that recorded the actions of each query algorithm in detail. We used this simulation system to explore different heuristics for finding an optimal length for the top documents lists. We then implemented the term bounded max score algorithm in Indri, where we verified that this method could achieve real performance gains in a real retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Topdocs List Pruning Methods</head><p>We tried three methods for pruning topdocs lists: fixed, frequency and fraction (see Figure <ref type="figure" target="#fig_0">2</ref>). In the fixed method, we store a fixed number of documents in each topdocs list. In the frequency method, we store all documents where c(t,d) |d| is greater than some constant. In the fraction method, we store some constant fraction of the number of documents that contain a given term.</p><p>In all cases, we decided not to use topdocs lists for terms occurring in fewer than 1000 documents, as this provided significant space savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Simulation Results: Title</head><p>We evaluated the three topdocs trimming methods in simulation Each method was compared against the baseline, where every document in the collection that contains any query term is scored, and the max score approach, where only some of these documents are scored.</p><p>Each of these pruning strategies has a single parameter. We ran each of the 50 queries using approximately 10 parameter values for each trimming method. We then found the mean number of documents scored for each trimming method parameter setting.</p><p>At the best parameter setting, the frequency method scored 11.3% of the documents in the collection, while the fraction method scored 11.4%, and the fixed policy scored 18.1%.</p><p>For the fraction method, we show its performance across its parameter space (Figure <ref type="figure" target="#fig_1">3</ref>) when compared to the baseline and the max score method. Note that results are normalized to the baseline, so the baseline is the top bar of the graph. The max score method scores an average of 22% of the collection. The term bounded max score method with fraction pruning scores fewer documents than max score over its useful parameter space. As the graph shows, the number of documents scored changes only slightly as the fraction of documents in the topdocs lists changes from 0.5% to 5%.</p><p>Although the frequency approach appears promising, it has the disadvantage that its disk space usage is difficult to predict. The fraction method has the advantage that its disk usage is highly predictable-choosing to store 5% of each inverted list in a topdocs list will result in roughly 5% additional disk space used. The fixed method is somewhat less predictable, since its disk usage is proportional to vocabulary size and not corpus length. However, vocabulary growth has been well studied, and tends to be strongly correlated to corpus length; this makes the fixed method predictable as well. Because using only a small amount of disk space was an important goal for us, we decided to only consider the fixed and fraction methods further.</p><p>We compared the performance of the fraction and fixed methods based on efficiency at different topdocs list lengths. The results are shown in Figure <ref type="figure" target="#fig_2">4</ref>. Note that the fraction scores fewer documents than the fixed method for all topdocs list sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Simulation Results: Expanded</head><p>In order to test our method on longer queries, we took our set of 50 title queries and used query expansion to create a set of 50 10-word queries.</p><p>To do this, we ran each title query against the GOV2 collection, and recorded the top 15 documents returned. We sorted the terms in these top 15 documents by order of occurrence, and removed any known stopwords, keeping only the top 10 words. This process generated 50 queries, each with exactly 10 words.</p><p>We found that, counter to general wisdom regarding the max score optimization, max score and our technique did not work as effectively on our expanded query set as on title queries. In Figure <ref type="figure">5</ref>, we show results for the fraction pruning method and the max score optimization on the expanded query set. Compared to Figure <ref type="figure" target="#fig_1">3</ref>, the graphs have a similar shape, but both max score and term bounded max score do not perform as well against the baseline.</p><p>To explain why this is so, we looked at the performance individual expanded queries. The following query is one of the poorest performing queries in our query set: loan census transportation application work area year report state data</p><p>We have ordered the query terms from least frequent to most frequent in the GOV2 collection; this is the way they would be evaluated by a term-at-a-time scoring system. This query contains many terms that are very close in terms of contribution bounds. In particular, application, work, area and report are very similar in terms of their contribution to the final score. This makes it very difficult for us to skip any one of them. In practice, this is exactly what happens; a max score query without topdocs is only able to skip through three terms (year, state, and data) and must examine every posting of the remaining terms. As report and area are each in approximately 6 million documents, this leads to a very expensive query.</p><p>We expect that, in general, smaller queries are more likely to have a few infrequent terms, paired with perhaps one frequent term. In this case, it is very easy to skip through the frequent term, providing excellent improvement. In expanded queries, it is likely that many of the terms are frequent, which makes it less likely that any one frequent term will be able to be skipped.</p><p>It should be noted that many large queries come out of query expansion, where each term in the query is assigned a weight during the expansion process. While these queries can be very long, many of the queries will have weights that are quite small, which will serve to bound the effect that term has on the query score. We suspect that our technique will perform much better on these kinds of queries, and anecdotal evidence seems to support this conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Indri Results</head><p>In order to verify that our optimization works well in practice, we implemented the term bounded max score optimization in Indri. For the following tests, we chose the fraction topdocs pruning method, taking the top 1% of documents from each inverted list. We chose only inverted lists containing more than 1000 documents. As before, we tested against the GOV2 collection. The inverted lists in our index totaled 44GB, while the topdocs lists took an additional 270MB.</p><p>While Indri is capable of running on a cluster of machines, our tests used a single Pentium IV machine, with a CPU clock speed of 2.6GHz. The machine had 2GB of RAM and 1.4TB of storage in a RAID 5 configuration, and was running Red Hat Linux version 9.</p><p>For each query method, we ran a warm up run that was not timed, followed by 5 consecutive runs. The times shown are the mean of those 5 runs, in seconds. Each query returned 10 documents, and the time reported includes the time necessary to look up the names of the documents returned from a B-Tree. The variance is shown for each time figure reported.</p><p>The results for the GOV2 title queries are reported in Figure <ref type="figure">6</ref>. We see a 23% improvement over max score and 61% over the baseline in time per query on this task. In Figure <ref type="figure">7</ref>, the improvement is 28% over max score, 57% over the baseline. In the title query case, the baseline scores 4.62 times as many documents as our method, while in the long query case, this factor drops to 3.39. We believe that certain constant factors, like the time to look up document names and the time to decompress blocks of inverted list postings, account for the reason that user CPU time does not track documents scored closely.</p><p>Our time of 1.7 seconds per title query is competitive with the fastest standard term-at-a-time systems at this year's TREC Terabyte task. The fastest single machine entry, from SABIR Research, averaged 1 second per title query. The second fastest system, from RMIT, using the Zettair retrieval system, took 2 seconds per title query. All query times at TREC were rounded to the nearest second, so it is difficult to make precise comparisons here. Given the data that we have, our system appears to be competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this work, we have shown that significant query optimization is possible in document-at-a-time systems, even beyond the max score method. We achieve a 23% speedup versus max score and 61% versus scoring all documents that contain any of the query terms. We find that this method makes it possible to get good query performance with documentat-a-time systems. Our method, term bounded max score, allows our Indri system to quickly evaluate simple queries while still handling more complex query operators. and Jamie Callan, whose comments were invaluable for strengthening this paper.</p><p>This work was supported in part by the Center for Intelligent Information Retrieval and in part by Advanced Research and Development Activity and NSF grant #CCF-0205575 . Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pruning strategies attempted in simulation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean documents scored with max score and term bounded max score using the fraction pruning method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Mean documents scored with the fixed and fraction pruning methods across varying topdocs list sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><label></label><figDesc>Figure Mean documents scored for expanded queries with the max score method, and with term bounded max score with the fraction pruning method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The following table shows the upper and lower bounds on scor-</figDesc><table><row><cell></cell><cell cols="7">Method Seconds/query User (s) User (σ 2 ) Elapsed (s) Elapsed (σ 2 ) Documents scored</cell></row><row><cell></cell><cell>Baseline</cell><cell>4.339</cell><cell>177.93</cell><cell>0.998</cell><cell>216.92</cell><cell>1.025</cell><cell>112425031</cell></row><row><cell cols="2">Max score</cell><cell>2.226</cell><cell>74.15</cell><cell>0.065</cell><cell>111.32</cell><cell>0.025</cell><cell>41697980</cell></row><row><cell cols="2">Term bounded max score</cell><cell>1.728</cell><cell>47.77</cell><cell>0.212</cell><cell>86.39</cell><cell>0.157</cell><cell>24300922</cell></row><row><cell cols="8">Figure 6: Time to evaluate TREC topics 701-750 against the TREC GOV2 collection</cell></row><row><cell></cell><cell cols="7">Method Seconds/query User (s) User (σ 2 ) Elapsed (s) Elapsed (σ 2 ) Documents scored</cell></row><row><cell></cell><cell>Baseline</cell><cell>34.70</cell><cell>1473.3</cell><cell>2.12</cell><cell>1735.1</cell><cell>1.72</cell><cell>508223689</cell></row><row><cell cols="2">Max score</cell><cell>20.64</cell><cell>779.4</cell><cell>1.03</cell><cell>1031.9</cell><cell>1.14</cell><cell>255740580</cell></row><row><cell cols="2">Term bounded max score</cell><cell>15.04</cell><cell>489.1</cell><cell>5.37</cell><cell>752.0</cell><cell>5.55</cell><cell>150479904</cell></row><row><cell cols="8">Figure 7: Time to evaluate 50 10-word queries against the TREC GOV2 collection</cell></row><row><cell cols="4">ing contributions of these terms, as found by a traditional</cell><cell></cell><cell></cell><cell></cell></row><row><cell>max score run:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>term</cell><cell cols="3">lower bound upper bound</cell><cell></cell><cell></cell><cell></cell></row><row><cell>loan</cell><cell>-10.2585</cell><cell cols="2">-0.2876</cell><cell></cell><cell></cell><cell></cell></row><row><cell>census</cell><cell>-10.2190</cell><cell cols="2">-0.2876</cell><cell></cell><cell></cell><cell></cell></row><row><cell>transportation</cell><cell>-9.5817</cell><cell cols="2">-0.2876</cell><cell></cell><cell></cell><cell></cell></row><row><cell>application</cell><cell>-8.7923</cell><cell cols="2">-0.2875</cell><cell></cell><cell></cell><cell></cell></row><row><cell>work</cell><cell>-8.6065</cell><cell cols="2">-0.2874</cell><cell></cell><cell></cell><cell></cell></row><row><cell>area</cell><cell>-8.1856</cell><cell cols="2">-0.2873</cell><cell></cell><cell></cell><cell></cell></row><row><cell>report</cell><cell>-8.1148</cell><cell cols="2">-0.2873</cell><cell></cell><cell></cell><cell></cell></row><row><cell>year</cell><cell>-7.8581</cell><cell cols="2">-0.2872</cell><cell></cell><cell></cell><cell></cell></row><row><cell>state</cell><cell>-7.1777</cell><cell cols="2">-0.2867</cell><cell></cell><cell></cell><cell></cell></row><row><cell>data</cell><cell>-6.9300</cell><cell cols="2">-0.2864</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGEMENTS</head><p>We are grateful for comments from the anonymous reviewers, as well as Donald Metzler, Kevyn Collins-Thompson</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient query evaluation using a two-level retrieval process</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twelfth international conference on Information and knowledge management</title>
		<meeting>the twelfth international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast evaluation of structured queries for information retrieval</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 18th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimization of inverted vector searches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Lewit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 8th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The INQUERY retrieval system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications</title>
		<meeting>DEXA-92, 3rd International Conference on Database and Expert Systems Applications</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">of The Art of Computer Programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Addison-Wesley</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Reading, Massachusetts</pubPlace>
		</imprint>
	</monogr>
	<note>Sorting and Searching. 2nd edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining the language model and inference network approaches to retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="735" to="750" />
			<date type="published" when="2004-09">September 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-indexing inverted files for fast text retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="349" to="379" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Query evaluation: strategies and optimizations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="831" to="850" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
