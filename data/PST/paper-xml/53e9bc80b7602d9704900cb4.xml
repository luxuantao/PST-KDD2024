<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximum Mutual Information Principle for Dynamic Sensor Query Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emre</forename><surname>Ertin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
							<email>fisher@ai.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">MIT Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lee</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
							<email>potter@osu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Maximum Mutual Information Principle for Dynamic Sensor Query Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9ECC7B7D85C0E177F1BBA49644AA6802</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we study a dynamic sensor selection method for Bayesian filtering problems. In particular we consider the distributed Bayesian Filtering strategy given in <ref type="bibr" target="#b0">[1]</ref> and show that the principle of mutual information maximization follows naturally from the expected uncertainty minimization criterion in a Bayesian filtering framework. This equivalence results in a computationally feasible approach to state estimation in sensor networks. We illustrate the application of the proposed dynamic sensor selection method to both discrete and linear Gaussian models for distributed tracking as well as to stationary target localization using acoustic arrays.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been renewed interest in the notion of deploying large numbers of networked sensors for applications ranging from environmental monitoring to surveillance to "intelligent" rooms(c.f. <ref type="bibr" target="#b1">[2]</ref>). Envisioned are smart sensor nodes with on-board sensing, computation, storage and communication capability. Such sensor networks simultaneously present unprecedented opportunities and unique challenges in collaborative signal processing. A particular challenge in the wireless sensor network setting is the need for distributed estimation algorithms which balance the limited energy resources at a node with costs of communication and sensing.</p><p>If one considers the distributed tracking problem, for example, it is not hard to imagine that one need not incorporate every sensor measurement in order to compute a reliable, if not optimal, estimate of the state of an object (or more properly the posterior distribution thereof). This is particularly true in the case where sensors have a limited field of regard with limited overlap between sensors. Distributed processing strategies that use a subset of sensor measurements directly mitigate the volume of inter-node communication thereby conserving power. The challenge is to decide in an intelligent manner which sensor measurements to use.</p><p>In the context of just such a scenario, Zhao et al. <ref type="bibr" target="#b0">[1]</ref> recently suggested a novel approach, the Information-Driven Sensor Querying (IDSQ) algorithm, as a means of selecting the "best" sensor measurement for updating the posterior belief of an object's state. In that work, a utility measure of a node measurement was proposed based on an estimate of the expected posterior state distribution conditioned on the, as yet unobserved, measurement at that node. This led to a direct method of selecting which node to query.</p><p>In this paper we further investigate aspects of one of the information utility functions suggested in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, specifically state uncertainty as quantified by conditional entropy. We begin by formulating the problem in a Bayesian estimation framework (as is commonly done) and decomposing state estimation into prediction (prior belief) and update (posterior belief) steps. We first show that, not surprisingly, functions which attempt to select the next sensor measurement based on expected posterior belief do nothing more than exploit information already contained in the prior belief as both are the same prior to taking a measurement. Consequently, utility functions based on expected posterior beliefs are more properly cast as utility functions on the prior belief (i.e. the belief over the current set of measurements). Next we consider the expected posterior uncertainty as quantified by conditional entropy (conditioned on previous measurements and a single new measurement) indexed by sensors. We show that this utility function simplifies to selecting the sensor measurement which has maximum mutual information with the object state at the next time step. The primary consequence of this analysis is that the utility function can be computed in a lower-dimensional space and, importantly, in a computationally feasible manner.</p><p>We present three experimental examples. The first example uses a simple discrete model to illustrate the maximum mutual information principle. The second example discusses the application of the maximum mutual information based sensor selection method to linear Gaussian Models. The third example is a simulation study of a stationary target localization problem using an acoustic array.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bayesian Filtering with Dynamic Sensor Selection</head><p>We adopt a probabilistic state space model for the tracking problem. The state of the target at time step (t) is denoted by x (t) . In this paper we will assume that the state space for the tracking problem can be approximated with a finite state space {x i } N i=1 . The sensor network consists of M sensors. Each sensor can be queried to provide a noisy measurements z (t) j of the state of the target. The state transition and observation model is given as:</p><formula xml:id="formula_0">x (t+1) = F (x (t) , v (t) ) ⇒ q(x (t+1) |x (t) )<label>(1)</label></formula><formula xml:id="formula_1">z (t) j = H j (x (t) , w (t) ) ⇒ f j (z (t) j |x j )<label>(2)</label></formula><p>where F and H j are arbitrary functions of the state and unknown disturbance variables v (t) and w (t) . The state space model suggests a conditional probability distribution q(x (t+1) |x (t) ) for the target state at time (t + 1) and a conditional probability density f j (z (t) j |x j ) for the j'th sensors measurement. The Bayesian filtering solution recursively calculates degree of belief in a state x (t+1) , given the sensor measurements. The prediction step computes the prior belief in state x (t+1) before a measurement is taken at (t + 1):</p><formula xml:id="formula_2">p(x (t+1) |z (t) ) = i q(x (t+1) |x (t) i )p(x (t)</formula><p>i |z (t) ) .</p><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>The update step computes the posterior belief in state x (t+1) after the measurement at (t + 1):</p><formula xml:id="formula_4">p(x (t+1) |z (t+1) ) = f j (z (t+1) |x (t+1) )p(x (t+1) |z (t) ) g j (z (t+1) |z (t) ) , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where z (t) denotes the measurements {z (1) , z (2) , . . . , z (t) } up to time (t). The normalization constant g j (z (t+1) |z (t) ) can be computed using:</p><formula xml:id="formula_6">g j (z (t+1) |z (t) ) = i f j (z (t+1) |x i )p(x i |z (t+1) ) . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>Zhao et al. <ref type="bibr" target="#b0">[1]</ref> describes a strategy for tracking problems to implement Bayesian Filtering in a distributed setting. At each time step one sensor node labeled as the leader makes a measurement and computes the belief p(x (t+1) |z (t) ). Then it select a sensor node to lead the tracking effort and passes the current belief to the chosen leader node. The next sensor to lead the tracking algorithm can be chosen to maximize a utility function of the form:</p><formula xml:id="formula_8">U (z (t) ∪ z (t+1) j ) = -H[p(x (t+1) |z (t) ∪ z (t+1) j )] = i p(x (t+1) i |z (t) ∪ z (t+1) j ) log p(x (t+1) i |z (t) ∪ z (t+1) j ) ,</formula><p>where U (z (t) ∪ z t j ) is the utility received from decreased uncertainty in the state of the target, which is measured as the entropy of the conditional probability density of x (t+1) given the sensor measurements up to time (t + 1). This utility function can be augmented with the communication cost of relaying the current belief from the current leader to the next. For example, the communication cost component of the utility can encompass the bandwidth utilization, transmission and reception power costs.</p><p>In this paper we focus on estimable measures of information utility, but a suitable communication cost can easily be integrated with our approach. Typically the measurement for the next sensor is unknown at (t + 1). Further, the expectation of the posterior belief p(x (t+1) |z (t) ∪ z t j ) is equal to the predicted belief p(x (t+1) |z (t) )).</p><formula xml:id="formula_9">E[p(x (t+1) |z (t) ∪ z (t+1) j )|z (t) ] = E f j (z (t+1) j |x (t+1) )p(x (t+1) |z (t) ) g j (z (t+1) j |z (t) ) z (t) = p(x (t+1) |z (t) )E f j (z (t+1) j |x (t+1) ) g j (z (t+1) j |z (t) ) z (t) = p(x (t+1) |z (t) ) z∈Zj f j (z|x (t+1) ) g j (z|z (t) ) g j (z|z (t) )dz = p(x (t+1) |z (t) ) z∈Zj f j (z|x (t+1) )dz = p(x (t+1) |z (t) ) . Zhao et al. [1] compute a proxy p((x (t+1) |z (t) ∪ z (t+1) j</formula><p>) to the expected posterior belief by averaging f j (z</p><formula xml:id="formula_10">(t+1) j |x (t+1)</formula><p>) over estimated measurement values using predicted belief. Although, this approximation to the expected posterior belief will not be equal to the predicted belief, the above result indicates that any utility measure based on expected posterior belief will be of limited use for sensor selection. Instead we employ an expected posterior uncertainty measure for sensor selection. In particular, we consider the expected posterior entropy, one of the information measures suggested in <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_11">ĵ = arg max j∈V E -H(p(x (t+1) |z (t) ∪ z t j ))|z (t) .</formula><p>In other words the sensor which will result in the smallest expected posterior uncertainty of the target state will be chosen to be the leader node of the tracking algorithm. In general, a direct computation of the expected posterior entropy is computationally infeasible. It requires computing the posterior belief for each possible measurement value and then averaging the entropy of the computed posterior belief over all possible measurement values. Even if the measurement space is discretized it requires computationally expensive calculations in the high dimensional state space. In the following, we show that maximizing the mutual information between the sensor output and target state is equivalent to minimizing expected posterior uncertainty. This observation yields a computationally feasible sensor selection method based on a maximum mutual information principle. The expected entropy of the posterior density can be evaluated using 3 and 4.</p><formula xml:id="formula_12">E -H(p(x (t+1) |z (t) ∪ z t j ))|z (t) = z∈Zj i p(x (t+1) i |z (t) ∪ {z}) log p(x (t+1) i |z (t) ∪ {z}) q j (z|z (t) )dz = z∈Zj i f j (z|x (t+1) i )p(x (t+1) i |z (t) ) log f j (z|x (t+1) i )p(x (t+1) i |z (t) ) g j (z|z (t) ) dz = i z∈Zj f j (z|x (t+1) i ) log f j (z|x (t+1) i )dz p(x (t+1) i |z (t) ) - z∈Zj g j (z|z (t) ) log g j (z|z (t) )dz + i p(x (t+1) i |z (t) ) log p(x (t+1) i |z (t) ) = -H(Z (t+1) j |X (t+1) ) + H(Z (t+1) j ) -H(X (t+1) ) = I(Z (t+1) j ; X (t+1) ) -H(X (t+1) )</formula><p>We note that the second term does not depend on the sensor measurement at (t+1). Hence, in a Bayesian Filtering framework minimizing the expected uncertainty in the posterior belief is equivalent to maximizing the mutual information between the state X (t+1) and measurement vector Z (t+1) j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Example 1: Discrete Observations</head><p>We consider a simple two state, two sensor problem to illustrate the concepts presented in Section 2. There are two possible states for the target x (t) ∈ {-1, 1}. The state transition model is given by:</p><formula xml:id="formula_13">x (t+1) = F (x (t) , v (t) ) = x (t) v (t) . (<label>6</label></formula><formula xml:id="formula_14">)</formula><p>where v (t) is a binary random variable which takes values {-1, 1} with probability q and 1q respectively. The observation model for the two sensors is given by.</p><formula xml:id="formula_15">z (t) 1 = H 1 (x (t) , w (t) ) = sgn(x (t) + w (t) 1 )<label>(7)</label></formula><formula xml:id="formula_16">z (t) 2 = H 2 (x (t) , w (t) ) = sgn(x (t) -w (t) 2 )<label>(8)</label></formula><p>The state space model suggests the following conditional probability distributions for state x (t+1) and sensor measurement z j (t).</p><formula xml:id="formula_17">p(x (t+1) = 1|x (t) ) = 1 -p(x (t+1) = -1|x (t) ) = 1 -q if x (t) = 1 q if x (t) = -1 f 1 (z (t) 1 = 1|x (t) ) = 1 -f 1 (z (t) 1 = -1|x (t) ) = 1 if x (t) = 1 r if x (t) = -1 f 2 (z (t) 2 = -1|x (t) ) = 1 -f 2 (z (t) 2 = 1|x (t) ) = r if x (t) = 1 1 if x (t) = -1</formula><p>i.e., sensor 1 makes an erroneous measurement with probability r if the state is -1, and sensor 2 makes an erroneous measurement with probability r if the state is 1. For this simple model we can parametrize the prior and posterior belief using a scalar variable:</p><formula xml:id="formula_18">p (t+1)|(t) def = p(x (t+1) = 1|z (t) ) (9) p (t+1)|(t+1) def = p(x (t+1) = 1|z (t+1) )<label>(10)</label></formula><p>We can verify that the expected posterior belief E[p (t+1)|(t+1) |z (t) ] is equal to the prior belief p (t+1)|(t) irrespective of the sensor choice at time (t + 1). If sensor 1 is queried at time (t + 1),</p><formula xml:id="formula_19">E[p (t+1)|(t+1) |z (t) ] = p(x (t+1) = 1|z (t) ∪ {z (t+1) 1 = 1})p(z (t+1) 1 = 1|z (t) ) +p(x (t+1) = 1|z (t) ∪ {z (t+1) 1 = -1})p(z (t+1) 1 = -1|z (t) ) = p (t+1)|(t) • 1 p (t+1)|(t) • 1 + (1 -p (t+1)|(t) ) • r (p (t+1)|(t) • 1 + (1 -p (t+1)|(t) ) • r) + p (t+1)|(t) • 0 p (t+1)|(t) • 0+(1 -p (t+1)|(t) ) • (1 -r) (p (t+1)|(t) • 0+(1 -p (t+1)|(t) ) • (1 -r)) = p (t+1)|(t) .</formula><p>Similarly if sensor 2 is queried at time (t + 1),</p><formula xml:id="formula_20">E[p (t+1)|(t+1) |z (t) ] = p(x (t+1) = 1|z (t) ∪ {z (t+1) 2 = 1})p(z (t+1) 2 = 1|z (t) ) +p(x (t+1) = 1|z (t) ∪ {z (t+1) 2 = -1})p(z (t+1) 2 = -1|z (t) ) = p (t+1)|(t) • (1 -r) p (t+1)|(t) • (1 -r)1+(1 -p (t+1)|(t) ) • 0 (p (t+1)|(t) • (1 -r)+(1 -p (t+1)|(t) ) • 0) + p (t+1)|(t) • r p (t+1)|(t) • r + (1 -p (t+1)|(t) ) • 1 (p (t+1)|(t) • r + (1 -p (t+1)|(t) ) • 1) = p (t+1)|(t) .</formula><p>The mutual information between state at time (t + 1) and sensor j's output is given by:</p><formula xml:id="formula_21">I(Z (t+1) 1 ; X (t+1) ) = H(Z (t+1) 1 ) -H(Z (t+1) 1 |X (t+1) ) = H((1 -p (t+1)|(t) )(1 -r)) -(1 -p (t+1)|(t) )H((1 -r)) I(Z (t+1) 2 ; X (t+1) ) = H(Z (t+1) 2 ) -H(Z (t+1) 2 |X (t+1) ) = H(p (t+1)|(t) (1 -r)) -p (t+1)|(t) H((1 -r)) ,</formula><p>where the function H is defined as</p><formula xml:id="formula_22">H(x) = -x log(x) -(1 -x) log(1 -x). It is easy to verify that I(Z (t+1) 1 ; X (t+1) ) &gt; I(Z (t+1) 2</formula><p>; X (t+1) ) for p (t+1)|(t) &gt; 0.5. For this example minimizing the expected entropy of posterior belief is equivalent to choosing the sensor that is ideal for the most likely state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Example 2: Linear Gaussian Model</head><p>In this section we consider the sensor selection for the Bayesian filtering problem with linear Gaussian models. We assume the following linear state space model:</p><formula xml:id="formula_23">x (t+1) = F x (t) + v (t) (11) z (t) j = H j x (t) + w (t)<label>(12)</label></formula><p>We assume the disturbances v (t) , w (t) are zero mean Gaussian processes with covariances Σ v and Σ w respectively. For a linear Gaussian model and Gaussian prior belief p(x (t) |z (t) ), it can be proved that both p(x (t+1) |z (t) ) and p(x (t+1) |z (t+1) ) are also Gaussian <ref type="bibr" target="#b3">[4]</ref>. The mean and covariance for p(x (t+1) |z (t) ) and p(x (t+1) |z (t+1) ) can be computed using the mean and covariance of p(x (t) |z (t) ) and the measurement z (t+1) through Kalman filter recursions. The observation model in 12 suggests a normal conditional distribution for z (t+1) j :</p><formula xml:id="formula_24">f j (z (t+1) j |x (t+1) ) = N (z (t+1) j ; H j x (t+1) , Σ w ) , (<label>13</label></formula><formula xml:id="formula_25">)</formula><p>where N (y; µ, Σ) denotes the Gaussian distribution with mean µ and Σ:</p><formula xml:id="formula_26">N (y; µ, Σ) def = ((2π) n |Σ|) -0.5 exp -(y -µ) T Σ -1 (y -µ) .</formula><p>Given the predicted belief p(x (t+1) |z (t) ) = N (x (t+1) ; µ (t+1)|(t+1) , Σ (t+1)|(t+1) ) we can derive the distribution for j'th sensors measurement at time (t + 1) as:</p><formula xml:id="formula_27">g j (z (t+1) j |z (t) ) = f j (z (t+1) j |x (t+1) )p(x (t+1) |z (t) )dx (t+1) = N (z (t+1) j ; H j x (t+1) , Σ w )N (x (t+1) ; µ (t+1)|(t+1) , Σ (t+1)|(t+1) )dx (t+1) = N (z (t+1) j ; H j µ (t+1)|(t+1) , Σ w + H j Σ (t+1)|(t+1) H T j ) . (<label>14</label></formula><formula xml:id="formula_28">)</formula><p>The mutual information between the sensor measurement and target state at time (t + 1) can be calculated using 13,14</p><formula xml:id="formula_29">I(Z (t+1) j ; X (t+1 ) = H(Z (t+1)<label>1</label></formula><p>) -H(Z</p><formula xml:id="formula_30">(t+1) 1 |X (t+1) ) = H[N (z (t+1) j ; H j µ (t+1)|(t+1) , Σ w + H j Σ (t+1)|(t+1) H T j )] -H[N (z (t+1) j ; H j x (t+1) , Σ w )]p(x (t+1) |z (t) )dx (t+1) = c log |Σ w + H j Σ (t+1)|(t+1) H T j | -c log |Σ w |p(x (t+1) |z (t) )dx (t+1) = c log |Σ w + H j Σ (t+1)|(t+1) H T j | |Σ w |</formula><p>To summarize the sensor selection rule for minimizing the expected posterior entropy is given as:</p><formula xml:id="formula_31">ĵ = arg max j∈V |Σ w + H j Σ (t+1)|(t+1) H T j | |Σ w | .</formula><p>Since the posterior density is Gaussian, this sensor selection rule minimizes the covariance determinant for the posterior belief. We should note that since the covariance (or equivalently the entropy) of the updated belief p(x (t+1) |z (t+1) ) does not depend on the measurement value z (t+1) j</p><p>, sensor selection for the linear Gaussian model is straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Example 3: Acoustic Array</head><p>In this section we consider the distributed localization of a single target using an acoustic sensor network. We assume a single target is present in a square 1 km × 1km region, which is divided into 50m × 50m cells. We assume the target is stationary:</p><formula xml:id="formula_32">x (t+1) = F (x (t) , v (t) ) = x (t)</formula><p>There are five microphones (range sensors) randomly placed in the region. Each sensor makes a time of arrival measurement (TOA) from an acoustic emission of the target. The sensor measurement model is given as:</p><formula xml:id="formula_33">z j = x -y j c + n j .</formula><p>where x denotes the target location and y j denotes the location of the j'th sensor. The speed of sound is given by c and the disturbances n j 's are Gaussian random variables with variance σ j . The error variance of the maximum likelihood TOA detector is inversely proportional to the signal to noise ratio, which in general depends on the distance of the target to the sensor location <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. In part A below, we assume constant noise variance for all the sensors and in part B, we consider the general case where the noise variance increases with increasing distance to the target. We assume each sensor can be interrogated only once. We also assume that the sensor locations and target emission time are known. A self localization method for microphone arrays is given in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part A:</head><p>In this case we assume the noise variance σ j = σ 0 = 50msec is constant for all five sensors. For this case the mutual information between the state X (t+1) and measurement vector Z j is given by</p><formula xml:id="formula_34">I(Z (t+1) j ; X (t+1) ) = H(Z (t+1) j ) -H((Z (t+1) j |X (t+1) ) ,</formula><p>where:</p><formula xml:id="formula_35">H(Z (t+1) j ) = - xi N (z, x i -y j c , σ 2 0 )p(x i |z (t) ) × log xi N (z, x i -y j c , σ 2 0 )p(x i |z (t) ) dz H(Z (t+1) j |X) = 1 2 log 2πeσ 2 0 .</formula><p>We note that for constant noise variance, maximizing mutual information I(Z (t+1) j</p><p>; X (t+1) ) is equivalent to maximizing the entropy of the Gaussian mixture H(Z (t+1) j</p><p>). The entropy of the Gaussian mixture can be calculated using numerical integration. Alternatively, we can obtain an approximation to H(Z (t+1) j</p><p>) by fitting a single Gaussian to the mixture distribution.</p><formula xml:id="formula_36">H(Z (t+1) j ) ≈ 1 2 log 2πeσ 2 Zj ,</formula><p>where</p><formula xml:id="formula_37">σ 2 Zj = xi p(x i |z (t) ) ( x i -y j c ) 2 + σ 2 0 - xi p(x i |z (t) ) x i -y j c 2 .</formula><p>In our simulations we observed virtually no difference in sensor selection performance between actual H(Z (t+1) j</p><p>) and its approximation. We used 500 monte carlo simulations, for three methods of sensor selection: Random sensor selection, Maximum Mutual Information based sensor selection and Mahalanobis distance based sensor selection discussed in <ref type="bibr" target="#b2">[3]</ref>. The results are given in Figure <ref type="figure" target="#fig_0">1</ref>. We consider root mean square error as a measure of target localization performance. For this experiment Maximum Mutual Information based sensor selection results in the best localization performance, followed by Mahalanobis distance based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part B:</head><p>In this case we assume the noise variance is dependent on the target distance</p><formula xml:id="formula_38">σ j = σ(r) = ( r r 0 ) α/2 σ 0</formula><p>In general the value of alpha depends on temperature and wind conditions and can be anisotropic. For this experiment we used α = 2, r 0 = 0.5km, and σ 0 = 30msec. For the distance dependent noise model, the mutual information between the state X (t+1) and measurement vector Z j is given by</p><formula xml:id="formula_39">I(Z (t+1) j ; X (t+1) ) = H(Z (t+1) j ) -H((Z (t+1) j |X (t+1) ) ,</formula><p>where: Again if the distribution of Z j can be approximated with a Gaussian we can approximate H(Z (t+1) j ) ≈ 1  2 log 2πeσ 2 Zj , where</p><formula xml:id="formula_40">H(Z (t+1) j ) = - xi N (z, x i -y j c , σ( x i -y j ) 2 )p(x i |z (t) ) × log xi N (z, x i -y j c , σ( x i -y j ) 2 )p(x i |z (t) ) dz H(Z j |X) = xi 1 2 log 2πeσ( x i -y j ) 2 p(x i |z (t) ) .</formula><formula xml:id="formula_41">σ 2 Zj = xi p(x i |z (t) )(( x i -y j c ) 2 +σ( x i -y j ) 2 )- xi p(x i |z (t) ) x i -y j c 2 .</formula><p>We used 500 monte carlo simulations for the range dependent noise case. The results are given in Figure <ref type="figure" target="#fig_1">2</ref>. We consider root mean square error as a measure of target localization performance. For this experiment Maximum Mutual Information and Mahalanobis distance based methods are very close in performance. The advantage of dynamic sensor selection over random sensor selection is again evident from the simulation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Motivated by the work of Zhao et al. <ref type="bibr" target="#b0">[1]</ref> we have presented an extension to the problem of distributed tracking in sensor networks. Specifically, we considered the expected posterior uncertainty, quantified by conditional entropy as the utility function for choosing the next measurement node in a distributed Bayesian sequential estimation framework. The demonstrated equivalence of expected conditional entropy (over measurements) to the mutual information between future state and the node measurements led to a computationally feasible method for employing the suggested utility function. Additionally we presented three example problems for which the method could be used along with empirical results. The results indicate that maximum mutual information principle presents a computationally attractive method for dynamic sensor selection problems. Some interesting questions arise in the context of sensor networks which motivate future research. For example, how does additional attribution of object state (e.g., class) complicate the analysis? How might one incorporate these ideas into heterogeneous networks where measurement models are less well understood? It is unlikely that such modifications will lead to such tractable measurement models; however, it is also the case that estimation of statistical dependence (i.e., mutual information) remains tractable in lower dimensional spaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Performance of the Sensor Selection Methods for constant noise variance. (Solid: Mutual Information, Dashed: Mahalanobis Distance Based Method, Dotted:Random)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Performance of the Sensor Selection Methods for range dependent noise variance. (Solid: Mutual Information, Dashed: Mahalanobis Distance Based Method, Dotted:Random)</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information-driven dynamic sensor collaboration</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="72" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collaborative signal and information processing in micro-sensor networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable information-driven sensor querying and routing for ad hoc heteregeneneous sensor networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High-Performance Compu. Applicat</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A bayesian approach to problems in stochastic estimation and control</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="333" to="339" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The generalized correlation method for estimation of time delay</title>
		<author>
			<persName><forename type="first">C</forename><surname>Knapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on ASSP</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="320" to="326" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Self calibration techniques for acoustic sensor arrays</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnamurty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>The Ohio State University</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An auto-calibration method for unattended ground sensors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in icassp</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2002">2002</date>
			<pubPlace>Orlando, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
