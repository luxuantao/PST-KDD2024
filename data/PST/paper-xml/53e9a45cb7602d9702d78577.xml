<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust image hash in Radon transform domain for authentication $</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-04-28">28 April 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yanqiang</forename><surname>Lei</surname></persName>
							<email>yanqianglei@yahoo.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuangen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiwu</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Information Security</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Information Science and Tech-nology</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust image hash in Radon transform domain for authentication $</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-04-28">28 April 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">550B8FB9260F76D4E1EE939AEF65E219</idno>
					<idno type="DOI">10.1016/j.image.2011.04.007</idno>
					<note type="submission">Received 18 November 2010 Accepted 7 April 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Image authentication Robust image hashing Radon transform Invariant moment Discrete Fourier transform</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image authentication has become an emergency issue in the digital world as it can be easily tampered with the image editing techniques. In this paper, a novel robust hashing method for image authentication is proposed. The reported scheme first performs Radon transform (RT) on the image, and calculates the moment features which are invariant to translation and scaling in the projection space. Then discrete Fourier transform (DFT) is applied on the moment features to resist rotation. Finally, the magnitude of the significant DFT coefficients is normalized and quantized as the image hash bits. Experimental results show that the proposed algorithm can tolerate almost all the typical image processing manipulations, including JPEG compression, geometric distortion, blur, addition of noise, and enhancement. Compared with other approaches in the literature, the reported method is more effective for image authentication in terms of detection performance and the hash size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the popular image processing techniques an image can be easily tampered. In this situation, it is very important to verify the authenticity and integrity of image content. The traditional cryptographic hash functions, such as MD5 and SHA-1 <ref type="bibr" target="#b0">[1]</ref>, have been used for data authentication. However, these hash functions are not suitable for image authentication. Because they are so sensitive that even one bit change of the input data will lead to a significant change of the output hash. Besides, image authentication system requires the main content sensitive. In order to make up for the disadvantage of the traditional cryptographic hash functions in image authentication, robust image hashing was first introduced by Schneider et al. <ref type="bibr" target="#b1">[2]</ref>. Subsequently, it is has been widely applied in image authentication <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>.</p><p>Image hashing extracts the robust features from the image to generate a compact representation, namely hash. Image hashing achieves image authentication by comparing the similarity of two image hashes that are extracted from the original image and the suspect image. As we know, robustness, fragility and security are the three key issues of image hashing. Robustness requires that image hashing should be invariant to incidental modifications, such as JPEG compression, geometric distortion, blur, addition of noise, enhancement and some other perceptually similar operations. Fragility means that the image hashing should have the ability to distinguish the visually distinct images. Security is the degree to prevent the attacker from tricking the authentication system with a maliciously tampered image. To effectively avoid the forgery, the security requires image hash should vary with the input secret key <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. The focus of this paper is the robustness and fragility, which are two contradictories of image hashing.</p><p>In recent years, many image hashing schemes have been reported. The image hashing techniques can be roughly classified into the following categories based on the improved classification of <ref type="bibr" target="#b17">[18]</ref>:</p><p>(1) Statistic-based schemes <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>: The hashes are extracted by calculating the image statistics, such as mean, variance, higher moments of image blocks and histogram. However, the image statistics cannot capture the content changes, particularly those that are maliciously tampered.</p><p>(2) Relation-based schemes <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>: The hashes are extracted based on some invariant relationships between two coefficients of histogram <ref type="bibr" target="#b4">[5]</ref>, discrete cosine transform <ref type="bibr" target="#b5">[6]</ref> or wavelet transform <ref type="bibr" target="#b6">[7]</ref>. However, method in <ref type="bibr" target="#b4">[5]</ref> has the same drawback as the statistic-based schemes.</p><p>Besides, the remaining schemes are very sensitive to global as well as local geometric distortion, which may not cause perceptually significant changes to the image. (3) Coarse representation-based schemes <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>: The hashes are extracted by making use of coarse information of the whole image, such as the auto-correlation of each projection in the Radon transform (RT) domain <ref type="bibr" target="#b8">[9]</ref>, the spatial distribution of significant wavelet coefficients <ref type="bibr" target="#b9">[10]</ref>, the low-frequency coefficients of Fourier transform (FT) <ref type="bibr" target="#b10">[11]</ref>, and so on. These hashes achieve excellent robustness under perceptually insignificant modifications, but it remains vulnerable to geometric distortion. (4) Matrix-based schemes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>: The hashes are constructed based on the matrix factorization, such as non-negative matrix factorization (NMF) <ref type="bibr" target="#b13">[14]</ref> and singular value decomposition (SVD) <ref type="bibr" target="#b14">[15]</ref>. Similarly, these methods are also very sensitive to global geometric distortion, in particular image rotation. 5) Low-level feature-based schemes <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>: The hashes are constructed from the image edge or salient feature points. These approaches still have limitations, due to that they are sensitive to some perceptually insignificant modifications, such as scaling, high quantization, and resolution reduction.</p><p>Some other works related to image hashing have also been studied. Monga et al. <ref type="bibr" target="#b18">[19]</ref> proposed a clusteringbased scheme for the compression of intermediate hash to generate a short binary string. Zhu et al. <ref type="bibr" target="#b19">[20]</ref> proposed a measure called expected discriminability to theoretically analyze the fragility of adaptive quantization-based image hashing. Zhu et al. <ref type="bibr" target="#b20">[21]</ref> also studied the randomness measure to evaluate the security of image hashing.</p><p>In this paper, we focus on the robustness and fragility of image hashing. The proposed algorithm is motivated by the geometric distortion (translation, scaling and rotation), which can be easily expressed by mathematical model. However, to our best knowledge, few of image hashing schemes theoretically analyze their robustness to these operations. So we propose a provably robust image hashing scheme based on Radon transform (RT). Many researchers have paid attention to the applications of RT in recent years <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>, due to its excellent properties for the geometric transformation. The proposed scheme can be described in brief as follows. RT is first performed on the image. Then, we calculate the moment features which are invariant to translation and scaling in the projection space. Subsequently, discrete Fourier transform (DFT) is applied on the moment features to resist image rotation. Finally, the magnitude of the significant DFT coefficients are normalized and quantized as the image hash bits.</p><p>The rest of this paper is organized as follows. In Section 2, RT and the proposed hashing method are described in detail. Section 3 shows the experimental and comparison results with other image hashing schemes. Finally, the conclusions are given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed image hashing method</head><p>The image hashing system extracts image features to identify its authenticity and integrity. These features should tolerate the content-preserving operations, while being sensitive to visually distinct images. In the rest of this section, we first introduce the RT and discuss its characteristics under translation, scaling and rotation operations. Then the invariant moment features in the RT domain are constructed in Section 2.2. Subsequently, the image hash generation and authentication are described in two phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Radon transform</head><p>RT is an effective method to analyze signal between the spatial domain and its projection space. Let gðr,yÞ denote the RT of a two-dimension image f(x,y), which is defined as its linear integral along the line inclined at an angle y form the y-axis and a distance r form the origin as shown in Fig. <ref type="figure" target="#fig_0">1</ref>  <ref type="bibr" target="#b21">[22]</ref>. The mathematical expression can be written as</p><formula xml:id="formula_0">gðr,yÞ ¼ Rff ðx,yÞg ¼ ZZ f ðx,yÞdðrÀxcosyÀysinyÞ dx dy<label>ð1Þ</label></formula><p>where dðÃÞ is the pulse function, r ¼ xcosy þ ysiny, and 0 r yo2p. RT has excellent properties for the geometric transformations, i.e. translation, scaling and rotation. Translation: if an image is shifted by (x 0 , y 0 ) in the spatial domain, the RT will be translated along r as Rff ðxÀx 0 ,yÀy 0 Þg ¼ gðrÀx 0 cosyÀy 0 siny,yÞ ð 2Þ</p><p>Scaling: scaling of an image f(x,y) by a factor (l 40) will cause the RT to be scaled as follows:</p><formula xml:id="formula_1">R f x l , y l n o ¼ lg r l ,y ¼ g l ðr,yÞ ð<label>3Þ</label></formula><p>Rotation: if an image is rotated by the angle y r , the corresponding RT will be shifted with the same angle. That is, Rff ðxcosy r Àysiny r ,xsiny r þycosy r Þg ¼ gðr,y þy r Þ ð 4Þ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Invariant features in Radon transform domain</head><p>Galigekere et al. <ref type="bibr" target="#b21">[22]</ref> reported a kind of moment pattern in the RT space. They regarded the projection signal as the probability of projection coordinates and developed a set of invariant moments. However, this kind of moment pattern is not suitable for image authentication. The moment pattern is effective in the continuous space, but it seems not to perform well in the discrete case. Motivated by this, we introduce another kind of invariant moment for image hashing, which only considers the magnitude of the projection signal. The proposed invariant features are proven to be robust to the translation, scaling, rotation, intensity change. The detailed analysis is given as follows.</p><p>According to the statistical theory, the moments and central moments can capture the statistical characters of random variable. The k-th order moments of the original and scaled image by the factor l in projection domain at direction y are defined as follows:</p><formula xml:id="formula_2">m k ðyÞ ¼ 1 L Z b a g k ðr,yÞ dr<label>ð5Þ</label></formula><formula xml:id="formula_3">m k l ðyÞ ¼ 1 lL Z lb la g k l ðr,yÞ dr<label>ð6Þ</label></formula><p>where L¼b Àa. Then</p><formula xml:id="formula_4">m k l ðyÞ ¼ 1 lL Z lb la l k g k r l ,y dr ¼ l k m k ðyÞ ð<label>7Þ</label></formula><p>Similarly, the k-th order central moments of the original and scaled image in the projection space at direction y can be defined as</p><formula xml:id="formula_5">m k ðyÞ ¼ 1 L Z b a ðgðr,yÞÀmðyÞÞ k dr<label>ð8Þ</label></formula><formula xml:id="formula_6">m k l ðyÞ ¼ 1 lL Z lb la ðg l ðr,yÞÀm l ðyÞÞ k dr<label>ð9Þ</label></formula><p>Thus, we can get m k l ðyÞ ¼ l k m k ðyÞ with the similar principle as Eq. ( <ref type="formula" target="#formula_4">7</ref>). Let</p><formula xml:id="formula_7">Z k ðyÞ ¼ m k ðyÞ=m k ðyÞ ð<label>10Þ</label></formula><p>Then we have</p><formula xml:id="formula_8">Z k l ðyÞ ¼ m k l ðyÞ=m k l ðyÞ ¼ m k ðyÞ=m k ðyÞ ¼ Z k ðyÞ ð<label>11Þ</label></formula><p>Now Z k ðyÞ is the scaling invariance. Assume that the intensity of an image is scaled by a factor b, then</p><formula xml:id="formula_9">Z k b ðyÞ ¼ b k m k ðyÞ=b k m k ðyÞ ¼ Z k ðyÞ. So Z k<label>ðyÞ</label></formula><p>is also invariant to intensity change, but it will create truncation error when the scaled intensity exceeds the resolution.</p><p>In fact, RT can be considered as a symmetrical projection over angle p. Because the only difference between gðr,yÞ and gðr,y þ pÞ is the opposite direction. However, the direction information is neglected in the calculation of Z k ðyÞ. So we can obtain Eq. ( <ref type="formula">12</ref>), and constrain y 2 ½0,pÞ.</p><p>When an image is rotated by the angle y r , Eq. ( <ref type="formula" target="#formula_10">13</ref>) gives the relationship between Z k yr and Z k based on Eqs. ( <ref type="formula">4</ref>) and <ref type="bibr" target="#b11">(12)</ref>, where Z k yr and Z k represent the intermediate features of the rotated image and the original one, respectively. It means that Z k yr is a circular shift version of Z k . We know that DFT has one property called translation invariant. So the rotation invariance can be achieved with the onedimensional DFT. Thus, the magnitude of the Fourier spectrum of Z k is invariant to scaling and rotation.</p><formula xml:id="formula_10">Z k ðyÞ ¼ Z k ðy þ pÞ ð 12Þ Z k yr ðyÞ ¼ Z k ðy þy r Þ, 0 r yopÀy r Z k ðy þy r ÀpÞ, pÀy r r yop (<label>ð13Þ</label></formula><p>The translation of an image in the spatial domain will only shift the projection signal along the r axis by padding extra zeros, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. So we only select the significant projection coefficients to generate the invariant moments discussed above. Therefore, the extracted features are invariant to translation, scaling and rotation, whose order (central) moment chosen in the feature extraction will be discussed in Section 3. Similarly, how many significant DFT coefficients are selected to generate the image hash will also be analyzed in detail in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hash generation</head><p>The proposed hashing algorithm can be described as Fig. <ref type="figure" target="#fig_1">2</ref>. RT is first performed on the input image to obtain the projections in various orientations. Then the insignificant coefficients are discarded. Next, we calculate the invariant moments which are defined in Section 2.2. Subsequently, DFT is performed on the invariant moments. Finally, the image hash H is defined as the normalized and quantized version of the significant DFT coefficients, which will satisfy Eq. ( <ref type="formula" target="#formula_11">14</ref>), where q is the dimension of H. In the proposed scheme, the 3rd-order invariant moment and the first 15 DFT coefficients are adopted. For the detailed analysis refer to Section 3.1.</p><formula xml:id="formula_11">X q i ¼ 1 HðiÞ ¼ 1, HðiÞ Z 0<label>ð14Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Authentication</head><p>In the authentication stage, hash H 0 is first generated from the suspect image I 0 with the same process as the original hash H. Then H 0 is compared with the original H with one distance metric, such as Minkowski metric and Hamming distance. In this paper, we employ the modified L1 distance to measure the similarity between H and H 0 , which can be defined as Eq. <ref type="bibr" target="#b14">(15)</ref>. C is the normalization factor, which is the maximum distance between two hashes. In our case, note that the hash is normalized in the range of [0,1], thus C ¼2, refer to Eqs. ( <ref type="formula" target="#formula_11">14</ref>) and ( <ref type="formula" target="#formula_13">16</ref>). The greater the distance dðH,H 0 Þ, the smaller the similarity. If dðH,H 0 Þ is less than the setting threshold T, the suspect image I 0 is authentic, or it is unauthentic, as <ref type="bibr" target="#b16">(17)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Radon</head><formula xml:id="formula_12">dðH,H 0 Þ ¼ 1 C X q i ¼ 1 jðHðiÞÀH 0 ðiÞÞj<label>ð15Þ</label></formula><formula xml:id="formula_13">C ¼ max X q i ¼ 1 jHðiÞÀH 0 ðiÞj ! ¼ X q i ¼ 1 HðiÞþ X q i ¼ 1 H 0 ðiÞ ¼ 1þ 1 ¼ 2<label>ð16Þ</label></formula><formula xml:id="formula_14">Image I 0 is authentic if dðH,H 0 Þ o T unauthentic others<label>ð17Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results</head><p>We evaluate the proposed algorithm on four image databases. UCID <ref type="bibr" target="#b25">[26]</ref> (an uncompressed color image database, version 2) including 1338 images with the size of either 512 Â 384 or 384 Â 512, McGill Calibrated Color Image Database <ref type="bibr" target="#b26">[27]</ref> including 1103 images with the size of either 768 Â 576 or 576 Â 768, BOWS2 <ref type="bibr" target="#b27">[28]</ref> dataset containing 10,000 images with the size of 512 Â 512, and our database including 1000 images with the size of either 512 Â 768 or 768 Â 512. We randomly select 1000 images from the four image databases, respectively. Thus, there are total 4000 test images, which contain (but not limit to) different topics including animal, landscape, plant, building, texture and so on. All the color images are converted as gray-scale images in the experiments.</p><p>We use the receiver operating characteristics (ROC) curve to evaluate the authentication performance, which is defined by true positive rate (TPR) and false positive rate (FPR) as y and x axes, respectively. TPR represents the ratio of images which are correctly considered as the authentic versions among all the authentic images, while FPR denotes the percentage of images that are misclassified as the authentic images, which actually are unauthentic. TPR and FPR can be defined as Eqs. ( <ref type="formula" target="#formula_15">18</ref>) and <ref type="bibr" target="#b18">(19)</ref>, respectively. T is a suitable threshold. In the ROC curve, the threshold T is variable to satisfy both TPR and FPR can distribute in [0,1]. Higher TPR means stronger robustness at the same FPR. For the same TPR, smaller FPR shows better discriminative capability.</p><formula xml:id="formula_15">TPRðTÞ ¼ # of true detected authentic images total # of authentic images Â 100%<label>ð18Þ</label></formula><formula xml:id="formula_16">FPRðTÞ ¼ # of false detected authentic images total # of unauthentic images Â 100%<label>ð19Þ</label></formula><p>Many commonly used image manipulations are considered to obtain the perceptually similar versions of the original image. The operations and corresponding parameters are listed below. It means that we create 68 similar versions for each image. Therefore, it will produce 4000 Â (68þ1)¼276,000 candidate images. The five geometric distortions are illustrated in Fig. <ref type="figure" target="#fig_4">3</ref>  Translation. The image is first scaled by the factor 0.5, then it shifts with the parameters ðDx,DyÞ at horizontal and vertical direction in the region, whose size is the same as the original image: ( À 16, 0), (32, 0), (0, À48), (0, 64) and (32, 32) pixels. # 16 $ 20.</p><p>Shearing. The degree of shearing is from 1% to 9% with a step 2%, which is implemented with the open source code <ref type="bibr" target="#b28">[29]</ref>. # 21 $ 25.</p><p>Cropping. Crop the outer border of image by 1%, 3%, 5%, 7% and 9%. # 26 $ 30.</p><p>Gaussian noise. The mean is zero, and the variance is Intensity change. Alter the intensity by 80%, 90% (darkened) and 110%, 120% (brightened). # 64 $ 68.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature modeling</head><p>In the experiments, we employ 180 projection angles from 01 to 1791 with a step 11. Generally speaking, it is unnecessary to adopt the orders which are higher than five, so we only test the 1st-to 5th-order invariant moments calculated from Eqs. ( <ref type="formula" target="#formula_2">5</ref>)- <ref type="bibr" target="#b10">(11)</ref>, which are performed by DFT separately. Owing to the symmetry property of DFT, only the magnitude of the first 91 DFT coefficients are extracted as the original features. Subsequently, we will discuss the selection of feature modeling in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Selection of moment order</head><p>Fig. <ref type="figure" target="#fig_5">4</ref>(a)-(d) shows the ROC curves of the 1st-to 5th-order invariant moments for image hash based on four different image databases. They all show the same conclusions. Compared with the 1st-, 2nd-and 4th-order invariant moments, the 3rd-and 5th-order invariant moments achieve better performance with lower FPR and higher TPR. It means that the 3rd-and 5th-order invariant moments perform less collision for the visually distinct images and are more robust against the perceptually similar operations. When Fig. <ref type="figure" target="#fig_5">4(a)-(d</ref>) is locally magnified, we can observe that the 3rd-order invariant moment gives better performance than the 5th-order one. This conclusion is particularly evident in Fig. <ref type="figure" target="#fig_5">4(c</ref>). Meanwhile, considering the computational complexity, we select the 3rd-order invariant moment in our feature extraction. In the following experiments, the four image databases are integrated into one set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Selection of significant DFT coefficients</head><p>As we all know that the DC and low-frequency AC coefficients contain the most energy of the signal in DFT domain. Considering the hash size, it is an important issue that how many coefficients should be selected to generate the hash. Assume that we select the first n coefficients to satisfy Eq. ( <ref type="formula" target="#formula_18">20</ref>) and set x ¼ 95%, where F k denotes the magnitude and x is the most commonly used threshold in the Fourier spectrum analysis. In our experiments, most images can be represented very well by fewer than 10 coefficients, some require 15. So the parameter n is set to be <ref type="bibr" target="#b14">15</ref>   selected as features, they all should be normalized to satisfy</p><formula xml:id="formula_17">P n k ¼ 1 F k ¼ 1</formula><p>, where F k is the normalized version of F k . Fig. <ref type="figure">5</ref> shows the authentication results of the selected 15 coefficients and total 91 original ones in terms of ROC curve. It is shown that the degradation of detection performance is almost negligible, which demonstrates the rationality of our decision.</p><formula xml:id="formula_18">X n k ¼ 1 F 2 k , X 91 k ¼ 1 F 2 k 4 x<label>ð20Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Selection of quantization bits</head><p>Another question rises. How many bits are required to quantize the selected coefficients? Without loss of generality, we assume that the original normalized DFT coefficients are represented by 32 bits. In the experiments, we uniformly quantize the DFT coefficients with 4, 6 and 8 bits. Fig. <ref type="figure">6</ref> shows their ROC curves for image authentication. Note that only 200 samples of TPR and FPR are tested in Fig. <ref type="figure">6</ref>. It can be seen that the normalized DFT coefficients represented by 32 and 8 bits can achieve almost the same performance. Considering that higher precision may be needed for the TPR and FPR in some situations, we employ 10 bits to uniformly quantize each normalized DFT coefficient, which can dramatically reduce the hash size. Thus, each image can be represented by only 15 Â 10 ¼150 bits in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance of the proposed method</head><p>Here, we test the robustness and fragility of the proposed hashing. Note that there are 4000 original images, and each has 68 distorted versions in our dataset. Assume that we use the original images as queries, thus, there are total of 4000 Â (68þ1) ¼276,000 (including the original image) pairs of perceptually similar images, while 4000 Â ð68 þ1À1=2Þ Â ð4000À1Þ ¼ 1,095,726,000 pairs of visually distinct images. Figs. <ref type="figure">7</ref> and<ref type="figure"></ref>    image pairs, respectively. It can be seen that the distances of similar image pairs are close to zero, while concentrate on 0.25 for those distinct image pairs. So the similar versions can be easily separated from the distinct images by a suitable threshold T 2 ½0:05,0:15.</p><p>Fig. <ref type="figure" target="#fig_8">9</ref> shows the average distances between the original images and their distorted versions. The horizontal axis depicts the indices of the distorted types. It can be seen that the distances are all less than 0.11 for the manipulations mentioned in Section 3, which shows the proposed hashing scheme is very robust. In particular, the extracted hash can tolerate JPEG compression, rotation, scaling, translation, blur and intensity change very well, since the distances between the original images and these distorted versions are all less than 0.05. Our theoretical analysis has demonstrated that the moment features are invariant to rotation, scaling, translation and intensity change. Meanwhile, the linear statistical property can retain its invariance under JPEG compression and blur. Therefore, our method shows excellent performance on these image operations. Shearing and cropping will lose much image information, and addition of noise will introduce extra information into the image. Thus, the greater the distortion, the worse the performance for these manipulations. In addition, the gamma correction does not change the intensity proportionally, and the proposed method cannot perform very good effectiveness for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Performance comparison</head><p>In this subsection, we compare our method with four reported hashing schemes. Since the extracted invariant moment being a kind of statistic in the RT domain, thus methods in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref> are selected. Since the proposed features are invariant to rotation, scaling and translation, we also compare it with FT-based scheme <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Comprehensive performance evaluation</head><p>Here, we first compare the comprehensive performance of the five hashing algorithms in terms of ROC curve, which can be used to represent the tradeoff between TPR and FPR. The comparison results are shown in Fig. <ref type="figure" target="#fig_9">10</ref>. It can be seen that our proposed algorithm can achieve better performance than the other four methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. For example, when FPR¼10%, TPR is almost 99% in our method which is much larger than TPR¼89% in <ref type="bibr" target="#b4">[5]</ref>, TPR¼ 84% in <ref type="bibr" target="#b8">[9]</ref>, TPR¼67% in <ref type="bibr" target="#b10">[11]</ref> and TPR¼ 93% in <ref type="bibr" target="#b11">[12]</ref>. These results show that our scheme is more robust against the perceptually similar operations mentioned in the experiment. On the other hand, the proposed hash has stronger ability for distinguishing the visually distinct images. According to the ROC curve, one can select the suitable T to implement different applications, emphasize either TPR or FPR. For instance, we can maximize TPR(T)-FPR(T) to obtain the optimal threshold T. By doing so, the optimal threshold is 0.10 in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Evaluation of robustness and fragility for each distortion type</head><p>We compare the authentication effectiveness for each distortion type in terms of TPR and FPR under the optimal threshold T. Table <ref type="table" target="#tab_2">1</ref> shows the comparison results of the five hashing algorithms. All methods perform very well for the JPEG compression, due to that they can retain the main image content. For the rotation, scaling and translation manipulations, the proposed scheme shows excellent performance which is the same as the theoretical analysis. Xiang et al. <ref type="bibr" target="#b4">[5]</ref> used Gaussian low-pass filter to pre-process the image. So its performance is poor for the translation (the image is first scaled), as illustrated in Fig. <ref type="figure" target="#fig_4">3(d</ref>). Seo et al. <ref type="bibr" target="#b8">[9]</ref> first calculated the auto-correlation of each projection in the RT domain. Then two-dimensional FT were performed to generate the image hash. In the case of rotation and translation, many redundant insignificant pixels are padded, so the features based on two-dimensional FT seem not to be effective. Swaminathan et al. <ref type="bibr" target="#b10">[11]</ref> extracted image hash based on two-dimensional FT magnitude spectrum. Similarly, it gives poor detection effectiveness for image rotation and translation. Ou et al. <ref type="bibr" target="#b11">[12]</ref> randomly selected 40 projection angles, meaning that it is not robust against rotation. For Xiang <ref type="bibr" target="#b4">[5]</ref> Seo <ref type="bibr" target="#b8">[9]</ref> Swaminathan <ref type="bibr" target="#b10">[11]</ref> Ou <ref type="bibr" target="#b11">[12]</ref> Proposed the shearing operation, the proposed scheme is inferior to <ref type="bibr" target="#b4">[5]</ref> with TPR¼82.52% and FPR ¼1.41%. In the cropping case, our method can achieve better results since the proposed feature is less affected. For the addition of noise, all algorithms indicate good performance. Method in <ref type="bibr" target="#b8">[9]</ref> shows the best results, since the auto-correlation could effectively suppress noise. For the blur category, the reported method and <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref> perform better. The TPR is more than 90% and FPR is about 2-4% for them. The blur process changes the intensity of the image globally, which will lead to the severe distortion of the FT magnitude spectrum. So method in <ref type="bibr" target="#b10">[11]</ref> shows poor results. As for the gamma correction, method in <ref type="bibr" target="#b8">[9]</ref> performs very well, while the proposed scheme achieves general effectiveness with TPR¼ 80.95% and FPR¼ 2.99%. Finally, our method behaves very well when the intensity of the image is changed proportionally. The theoretical analysis has supported this claim. To some degree, we can claim that the proposed hashing scheme is superior to the other four methods in terms of detection performance, especially in the case of geometric distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Evaluation of fragility under malicious operations</head><p>As we know, the image hash should be able to distinguish the malicious operations from the contentpreserving ones. This property is usually called uniqueness, which is a special case of fragility. Fig. <ref type="figure" target="#fig_0">11</ref> shows two examples of malicious operations. Images (a) and (b) are two original images with distinct contents. Image (c) is created by cut-and-paste image editing, which directly combines parts of images (a) and (b). Image (d) is created by cut-rotate-paste operation, which first cuts a region from image (b), then the region is randomly rotated and pasted to image (a). We consider that both images (c) and (d) are maliciously tampered versions of image (a). In our work, we also select the 4000 original images as image (a). For each image (a), we randomly select an image from the four databases (UCID, McGill, BOWS2 and ours) as image (b). Note that images (a) and (b) are not the same. In the case of images (c) and (d), the regions of malicious operations are both from 10% to 70% with a step 15% of the original image size. Thus, 4000 Â 5¼20,000 images are created for both images (c) and (d).</p><p>Since FPR denotes the percentage of images that are misclassified as the authentic images, which actually are unauthentic. So FPR can represent the ability of distinguishing the malicious operations. Fig. <ref type="figure" target="#fig_1">12</ref> shows the detection results of the five hashing schemes. When the percentage of malicious operations is less than 55%, the proposed method has the smallest FPR. Our method is inferior to <ref type="bibr" target="#b4">[5]</ref> when the percentage of malicious operations is larger than 55-60%. Generally, we are more concerned about the malicious  attacks on small region since they are rational attacks. From this comparison result, we can claim that the proposed hashing scheme can achieve good fragility under malicious operations to some degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">Comparison of hash size</head><p>It is necessary to study the hash size of various hashing methods, especially for large image database. Table <ref type="table">2</ref> shows the hash size of the five hashing methods. We can observe that the proposed scheme needs the smallest storage, only 150 bits for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>This paper proposes an effective hashing technique for image authentication based on RT. Our theoretical analysis demonstrates that the proposed feature is robust to the typical content-preserving operations, such as translation, scaling, rotation and intensity change. The experimental results also show that such a new method has the ability of distinguishing two visually distinct images, and can tolerate the perceptually similar operations. Compared with the existing works, the proposed scheme also shows better performance to the malicious operations. Only 150 bits are needed to retain the image hash. Therefore, the proposed hash is a very promising feature for image authentication and some similar applications. Note that the proposed feature can be treated as a linear statistical property, so it may be inevitable that two visually distinct images have the similar features. In real application, some other features can be combined with ours to enhance the fragility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The projection of an image at direction y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The proposed image hashing algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>using Lena image. JPEG compression. Quality factor: 10, 30, 50, 70 and 90. Label as # 1 $ 5. Rotation. Angle: 2, 15, 30, 45 and 60. # 6 $ 10. Scaling. Scaling factor: 0.5, 0.8, 1.2, 1.5 and 2. # 11 $ 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>from 0.01 to 0.05 with a step 0.01. # 31 $ 35. Salt &amp; pepper. The noise density is from 1% to 9% with a step 2%. # 36 $ 40. Gaussian filtering. The filter size is from 3 Â 3 to 11 Â 11 with a step 2. # 41 $ 45. Average filtering. The filter size is from 3 Â 3 to 11 Â 11 with a step 2. # 46 $ 50. Median filtering. The filter size is from 3 Â 3 to 11 Â 11 with a step 2. # 51 $ 55. Motion blurring. The length is 9, and the angle is from 0 to 90 with a step 22.5. # 56 $ 60.Gamma correction. The gamma correction factor is 0.8, 0.9, 1.1, and 1.2. # 61 $ 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of original and five geometric distorted versions of Lena image . (a) Original, (b) Rotation, (c) Scaling, (d) Translation, (e) Shearing and (f) Cropping.</figDesc><graphic coords="4,283.92,56.57,217.44,169.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. ROC curves of the 1st-to 5th-order invariant moments based on four different image databases. (a) Based on UCID, (b) Based on McGill, (c) Based on BOWS2 and (d) Based on our image database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. The distribution of distances for perceptually similar image pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Average distances between original images and their distorted versions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. ROC curves of five hashing schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Fig. 11. Illustration of malicious operations. Both (a) and (b) are two original images. Image (c) is created by cut-and-paste. Image (d) is created by cut-rotate-paste.</figDesc><graphic coords="8,283.92,252.30,217.44,64.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>empirically. It means that only the first 15 coefficients are selected. Note that we only consider the magnitude of the coefficients. No matter how many coefficients are</figDesc><table><row><cell></cell><cell></cell><cell cols="5">Receiver Operating Characteristics</cell><cell></cell><cell></cell><cell cols="4">Receiver Operating Characteristics</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.96 0.97 0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1st-order 2nd-order 3rd-order 4th-order 5th-order</cell><cell>True Positive Rate</cell><cell>0.94 0.95 0.96 0.97 0.98</cell><cell></cell><cell></cell><cell></cell><cell>1st-order 2nd-order 3rd-order 4th-order 5th-order</cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.93</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.92</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0</cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">False Positive Rate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">False Positive Rate</cell></row><row><cell></cell><cell></cell><cell cols="5">Receiver Operating Characteristics</cell><cell></cell><cell></cell><cell cols="4">Receiver Operating Characteristics</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.94 0.95 0.96 0.97 0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1st-order 2nd-order 3rd-order 4th-order 5th-order</cell><cell>True Positive Rate</cell><cell>0.94 0.95 0.96 0.97 0.98</cell><cell></cell><cell></cell><cell></cell><cell>1st-order 2nd-order 3rd-order 4th-order 5th-order</cell></row><row><cell></cell><cell>0.93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.93</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.92</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">False Positive Rate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">False Positive Rate</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Robustness and fragility evaluation on various distorted types (average TPR and FPR).</figDesc><table><row><cell>Distortion types</cell><cell>Xiang [5]</cell><cell></cell><cell>Seo [9]</cell><cell></cell><cell cols="2">Swaminathan [11]</cell><cell>Ou [12]</cell><cell></cell><cell>Proposed</cell><cell></cell></row><row><cell></cell><cell>TPR (%)</cell><cell>FPR (%)</cell><cell>TPR (%)</cell><cell>FPR (%)</cell><cell>TPR (%)</cell><cell>FPR (%)</cell><cell>TPR (%)</cell><cell>FPR (%)</cell><cell>TPR (%)</cell><cell>FPR (%)</cell></row><row><cell>JPEG compression</cell><cell>94.58</cell><cell>3.50</cell><cell>100</cell><cell>2.55</cell><cell>99.85</cell><cell>3.52</cell><cell>94.40</cell><cell>2.84</cell><cell>99.94</cell><cell>2.86</cell></row><row><cell>Rotation</cell><cell>80.72</cell><cell>3.75</cell><cell>10.96</cell><cell>1.83</cell><cell>10.11</cell><cell>3.05</cell><cell>13.54</cell><cell>0.58</cell><cell>100</cell><cell>2.85</cell></row><row><cell>Scaling</cell><cell>92.59</cell><cell>3.64</cell><cell>100</cell><cell>2.25</cell><cell>100</cell><cell>3.51</cell><cell>88.69</cell><cell>2.84</cell><cell>100</cell><cell>2.85</cell></row><row><cell>Translation</cell><cell>33.65</cell><cell>3.02</cell><cell>5.05</cell><cell>0.95</cell><cell>0.09</cell><cell>0.61</cell><cell>83.44</cell><cell>2.74</cell><cell>100</cell><cell>2.86</cell></row><row><cell>Shearing</cell><cell>85.05</cell><cell>3.71</cell><cell>37.87</cell><cell>2.13</cell><cell>29.19</cell><cell>3.13</cell><cell>62.72</cell><cell>1.96</cell><cell>82.52</cell><cell>1.41</cell></row><row><cell>Cropping</cell><cell>76.14</cell><cell>3.56</cell><cell>79.24</cell><cell>2.20</cell><cell>30.50</cell><cell>1.87</cell><cell>63.07</cell><cell>2.81</cell><cell>96.80</cell><cell>2.89</cell></row><row><cell>Gaussian noise</cell><cell>87.37</cell><cell>4.24</cell><cell>99.57</cell><cell>2.45</cell><cell>80.86</cell><cell>3.32</cell><cell>81.88</cell><cell>2.85</cell><cell>93.20</cell><cell>3.16</cell></row><row><cell>Salt &amp; pepper noise</cell><cell>90.30</cell><cell>3.94</cell><cell>99.92</cell><cell>2.64</cell><cell>92.86</cell><cell>3.43</cell><cell>92.15</cell><cell>2.85</cell><cell>96.80</cell><cell>3.14</cell></row><row><cell>Gaussian filtering</cell><cell>95.99</cell><cell>3.58</cell><cell>100</cell><cell>2.58</cell><cell>46.84</cell><cell>2.72</cell><cell>91.96</cell><cell>2.84</cell><cell>100</cell><cell>2.89</cell></row><row><cell>Average filtering</cell><cell>94.70</cell><cell>3.55</cell><cell>100</cell><cell>2.55</cell><cell>40.70</cell><cell>2.37</cell><cell>91.47</cell><cell>2.84</cell><cell>100</cell><cell>2.89</cell></row><row><cell>Median filtering</cell><cell>95.15</cell><cell>3.52</cell><cell>99.97</cell><cell>2.53</cell><cell>83.27</cell><cell>3.09</cell><cell>79.86</cell><cell>2.83</cell><cell>99.62</cell><cell>2.76</cell></row><row><cell>Motion blurring</cell><cell>96.38</cell><cell>3.59</cell><cell>100</cell><cell>2.54</cell><cell>41.55</cell><cell>2.84</cell><cell>91.82</cell><cell>2.84</cell><cell>100</cell><cell>2.89</cell></row><row><cell>Gamma correction</cell><cell>39.93</cell><cell>3.67</cell><cell>100</cell><cell>2.55</cell><cell>74.45</cell><cell>3.30</cell><cell>79.49</cell><cell>2.85</cell><cell>80.95</cell><cell>2.99</cell></row><row><cell>Intensity change</cell><cell>51.75</cell><cell>3.61</cell><cell>99.99</cell><cell>2.52</cell><cell>14.87</cell><cell>3.29</cell><cell>90.35</cell><cell>2.85</cell><cell>97.94</cell><cell>2.93</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Y. Lei et al. / Signal Processing: Image Communication 26 (2011) 280-288</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>$ Supported by the 973 Program (2011CB302204), NSFC (61003243) and China Postdoctoral Science Special Foundation (201003376).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Hash size of various hashing schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hashing methods</head><p>Hash size (bits)</p><p>Xiang <ref type="bibr" target="#b4">[5]</ref> 435 Seo <ref type="bibr" target="#b8">[9]</ref> 400 Swaminathan <ref type="bibr" target="#b10">[11]</ref> 420 Ou <ref type="bibr" target="#b11">[12]</ref> 240 Proposed 150</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Van Oorschot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Vanstone</surname></persName>
		</author>
		<title level="m">Handbook of Applied Cryptography</title>
		<meeting><address><addrLine>Boca Raton, FL</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A robust content based digital signature for image authentication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="227" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust image hashing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Koon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Jakubowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="664" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust image hashing based on radial variance of pixels</title>
		<author>
			<persName><forename type="first">C</forename><surname>De Roover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Macq</surname></persName>
		</author>
		<idno>III-77-III-80</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histogram-based image hashing scheme robust against geometric deformations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Workshop on Multimedia and Security</title>
		<meeting>the ACM Workshop on Multimedia and Security<address><addrLine>Dallas, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A robust image authentication method distinguishing JPEG compression from malicious manipulation</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structural digital signature for image authentication: an incidental distortion resistant scheme</title>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust hash functions for digital watermarking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goljan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Information Technology: Coding and Computing</title>
		<meeting>the IEEE International Conference on Information Technology: Coding and Computing<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="178" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haitsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
		<title level="m">A robust image fingerprinting system using the Radon transform, Signal Process.: Image Commun</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="325" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust image hashing based on SPIHT</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Information Technology: Research and Education</title>
		<meeting>the IEEE International Conference on Information Technology: Research and Education</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="110" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust and secure image hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="230" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A key-dependent secure image hashing scheme by using Radon transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Intelligent Signal Processing and Communication Systems</title>
		<meeting>the IEEE International Symposium on Intelligent Signal Processing and Communication Systems<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="595" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uddin Abbas, A secure and robust hashbased scheme for image authentication</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Siyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1456" to="1470" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust and secure image hashing via nonnegative matrix factorizations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Mhcak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="376" to="390" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust perceptual image hashing via matrix invariants</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Mihcak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3443" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compression tolerant DCT based image hash</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kailasanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Naini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Distributed Computing Systems Workshops</title>
		<meeting>the IEEE International Conference on Distributed Computing Systems Workshops</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="562" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geometric distortion-resilient image hashing scheme and its applications on copy detection and authentication</title>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="173" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual image hashing via feature points: Performance evaluation and tradeoffs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3452" to="3465" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A clustering based approach to perceptual image hashing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="79" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fragility analysis of adaptive quantization-based image hashing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="133" to="147" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study on the randomness measure of image hashing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="928" to="932" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Moment patterns in the Radon space</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Galigekere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Holdsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N S</forename><surname>Swamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fenster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1088" to="1097" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust image watermarking based on generalized Radon transformations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Simitopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Koutsonanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="732" to="745" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rotation-invariant multiresolution texture analysis using Radon and wavelet transforms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jafari-Khouzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltanian-Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="783" to="795" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Identification of reflected, scaled, translated, and rotated objects from their Radon projections</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hjouj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Kammler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="310" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">UCID-an uncompressed color image database</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Storage and Retrieval Methods and Applications for Multimedia</title>
		<meeting>the SPIE Storage and Retrieval Methods and Applications for Multimedia<address><addrLine>San Jose, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Olmos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A A</forename><surname>Kingdom</surname></persName>
		</author>
		<ptr target="http://tabby.vision.mcgill.caS" />
		<title level="m">McGill Calibrated Color Image Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="http://bows2.gipsa-lab.inpg.frS" />
		<title level="m">BOWS2-Original Image Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Second generation benchmarking and application oriented evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Voloshynovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Madueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Hiding Workshop</title>
		<meeting><address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2137</biblScope>
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
