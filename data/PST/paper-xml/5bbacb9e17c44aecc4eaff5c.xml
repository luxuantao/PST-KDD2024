<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
							<email>wotaoyin@math.ucla.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1CF5767113EF0CA12CAE1718B5606B2B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, unfolding iterative algorithms as neural networks has become an empirical success in solving sparse recovery problems. However, its theoretical understanding is still immature, which prevents us from fully utilizing the power of neural networks. In this work, we study unfolded ISTA (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We introduce a weight structure that is necessary for asymptotic convergence to the true sparse signal. With this structure, unfolded ISTA can attain a linear convergence, which is better than the sublinear convergence of ISTA/FISTA in general cases. Furthermore, we propose to incorporate thresholding in the network to perform support selection, which is easy to implement and able to boost the convergence rate both theoretically and empirically. Extensive simulations, including sparse vector recovery and a compressive sensing experiment on real image data, corroborate our theoretical results and demonstrate their practical usefulness. We have made our codes publicly available. 2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper aims to recover a sparse vector x * from its noisy linear measurements:</p><formula xml:id="formula_0">b = Ax * + ε,<label>(1)</label></formula><p>where b ∈ R m , x ∈ R n , A ∈ R m×n , ε ∈ R m is additive Gaussian white noise, and we have m n.</p><p>(1) is an ill-posed, highly under-determined system. However, it becomes easier to solve if x * is assumed to be sparse, i.e. the cardinality of support of x * , S = {i|x * i = 0}, is small compared to n. A popular approach is to model the problem as the LASSO formulation (λ is a scalar):</p><formula xml:id="formula_1">minimize x 1 2 b -Ax 2 2 + λ x 1<label>(2)</label></formula><p>and solve it using iterative algorithms such as the iterative shrinkage thresholding algorithm (ISTA) <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_2">x k+1 = η λ/L x k + 1 L A T (b -Ax k ) , k = 0, 1, 2, . . .<label>(3)</label></formula><p>where η θ is the soft-thresholding function <ref type="foot" target="#foot_0">3</ref> and L is usually taken as the largest eigenvalue of A T A.</p><p>In general, ISTA converges sublinearly for any given and fixed dictionary A and sparse code x * <ref type="bibr" target="#b1">[2]</ref> In <ref type="bibr" target="#b2">[3]</ref>, inspired by ISTA, the authors proposed a learning-based model named Learned ISTA (LISTA). They view ISTA as a recurrent neural network (RNN) that is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>(a), where</p><formula xml:id="formula_3">W 1 = 1 L A T , W 2 = I -1 L A T A, θ = 1 L λ.</formula><p>LISTA, illustrated in Figure <ref type="figure" target="#fig_1">1</ref>(b), unrolls the RNN and truncates it into K iterations:</p><formula xml:id="formula_4">x k+1 = η θ k (W k 1 b + W k 2 x k ), k = 0, 1, • • • , K -1,<label>(4)</label></formula><p>leading to a K-layer feed-forward neural network with side connections.</p><p>Different from ISTA where no parameter is learnable (except the hyper parameter λ to be tuned), LISTA is treated as a specially structured neural network and trained using stochastic gradient descent (SGD), over a given training dataset {(x * i , b i )} N i=1 sampled from some distribution P(x, b). All the parameters Θ = {(W k 1 , W k 2 , θ k )} K-1 k=0 are subject to learning. The training is modeled as:</p><formula xml:id="formula_5">minimize Θ E x * ,b x K Θ, b, x 0 -x * 2 2 . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Many empirical results, e.g., <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, show that a trained K-layer LISTA (with K usually set to 10 ∼ 20) or its variants can generalize more than well to unseen samples (x , b ) from the same P(x, b) and recover x from b to the same accuracy within one or two order-of-magnitude fewer iterations than the original ISTA. Moreover, the accuracies of the outputs {x k } of the layers k = 1, .., K gradually improve.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Works</head><p>Many recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> followed the idea of <ref type="bibr" target="#b2">[3]</ref> to construct feed-forward networks by unfolding and truncating iterative algorithms, as fast trainable regressors to approximate the solutions of sparse coding models. On the other hand, progress has been slow towards understanding the efficient approximation from a theoretical perspective. The most relevant works are discussed below.</p><p>[12] attempted to explain the mechanism of LISTA by re-factorizing the Gram matrix of dictionary, which tries to nearly diagonalize the Gram matrix with a basis that produces a small perturbation of the 1 ball. They re-parameterized LISTA into a new factorized architecture that achieved similar acceleration gain to LISTA. Using an "indirect" proof, <ref type="bibr" target="#b11">[12]</ref> was able to show that LISTA can converge faster than ISTA, but still sublinearly. Lately, <ref type="bibr" target="#b12">[13]</ref> tried to relate LISTA to a projected gradient descent descent (PGD) relying on inaccurate projections, where a trade-off between approximation error and convergence speed was made possible.</p><p>[14] investigated the convergence property of a sibling architecture to LISTA, proposed in <ref type="bibr" target="#b3">[4]</ref>, which was obtained by instead unfolding/truncating the iterative hard thresholding (IHT) algorithm rather than ISTA. The authors argued that they can use data to train a transformation of dictionary that can improve its restricted isometry property (RIP) constant, when the original dictionary is highly correlated, causing IHT to fail easily. They moreover showed it beneficial to allow the weights to decouple across layers. However, the analysis in <ref type="bibr" target="#b13">[14]</ref> cannot be straightforwardly extended to ISTA although IHT is linearly convergent <ref type="bibr" target="#b14">[15]</ref> under rather strong assumptions.</p><p>In <ref type="bibr" target="#b15">[16]</ref>, a similar learning-based model inspired by another iterative algorithm solve LASSO, approximated message passing (AMP), was studied. The idea was advanced in <ref type="bibr" target="#b16">[17]</ref> to substituting the AMP proximal operator (soft-thresholding) with a learnable Gaussian denoiser. The resulting model, called Learned Denoising AMP (L-DAMP), has theoretical guarantees under the asymptotic assumption named "state evolution." While the assumption is common in analyzing AMP algorithms, the tool is not directly applicable to ISTA. Moreover, <ref type="bibr" target="#b15">[16]</ref> shows L-DAMP is MMSE optimal, but there is no result on its convergence rate. Besides, we also note the empirical effort in <ref type="bibr" target="#b17">[18]</ref> that introduces an Onsager correction to LISTA to make it resemble AMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Motivations and Contributions</head><p>We attempt to answer the following questions, which are not fully addressed in the literature yet:</p><p>• Rather than training LISTA as a conventional "black-box" network, can we benefit from exploiting certain dependencies among its parameters {(W k 1 , W k 2 , θ k )} K-1 k=0 to simplify the network and improve the recovery results?</p><p>• Obtained with sufficiently many training samples from the target distribution P(x, b), LISTA works very well. So, we wonder if there is a theoretical guarantee to ensure that LISTA (4) converges <ref type="foot" target="#foot_1">4</ref> faster and/or produces a better solution than ISTA (3) when its parameters are ideal? If the answer is affirmative, can we quantize the amount of acceleration? • Can some of the acceleration techniques such as support detection that were developed for LASSO also be used to improve LISTA?</p><p>Our Contributions: this paper aims to introduce more theoretical insights for LISTA and to further unleash its power. To our best knowledge, this is the first attempt to establish a theoretical convergence rate (upper bound) of LISTA directly. We also observe that the weight structure and the thresholds can speedup the convergence of LISTA:</p><p>• We give a result on asymptotic coupling between the weight matrices W k 1 and W k 2 . This result leads us to eliminating one of them, thus reducing the number of trainable parameters. This elimination still retains the theoretical and experimental performance of LISTA.</p><p>• ISTA is generally sublinearly convergent before its iterates settle on a support. We prove that, however, there exists a sequence of parameters that makes LISTA converge linearly since its first iteration. Our numerical experiments support this theoretical result. • Furthermore, we introduce a thresholding scheme for support selection, which is extremely simple to implement and significantly boosts the practical convergence. The linear convergence results are extended to support detection with an improved rate.</p><p>Detailed discussions of the above three points will follow after Theorems 1, 2 and 3, respectively. Our proofs do not rely on any indirect resemblance, e.g., to AMP <ref type="bibr" target="#b17">[18]</ref> or PGD <ref type="bibr" target="#b12">[13]</ref>. The theories are supported by extensive simulation experiments, and substantial performance improvements are observed when applying the weight coupling and support selection schemes. We also evaluated LISTA equipped with those proposed techniques in an image compressive sensing task, obtaining superior performance over several of the state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Algorithm Description</head><p>We first establish the necessary condition for LISTA convergence, which implies a partial weight coupling structure for training LISTA. We then describe the support-selection technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Necessary Condition for LISTA Convergence and Partial Weight Coupling</head><p>Assumption 1 (Basic assumptions). The signal x * and the observation noise ε are sampled from the following set:</p><formula xml:id="formula_7">(x * , ε) ∈ X (B, s, σ) (x * , ε) |x * i | ≤ B, ∀i, x * 0 ≤ s, ε 1 ≤ σ .<label>(6)</label></formula><p>In other words, x * is bounded and s-sparse<ref type="foot" target="#foot_2">5</ref> (s ≥ 2), and ε is bounded.</p><formula xml:id="formula_8">Theorem 1 (Necessary Condition). Given {W k 1 , W k 2 , θ k } ∞</formula><p>k=0 and x 0 = 0, let b be observed by (1) and {x k } ∞ k=1 be generated layer-wise by LISTA (4). If the following holds uniformly for any (x * , ε) ∈ X (B, s, 0) (no observation noise):</p><formula xml:id="formula_9">x k {W τ 1 , W τ 2 , θ τ } k-1 τ =0 , b, x 0 → x * , as k → ∞ and {W k 2 } ∞ k=1 are bounded W k 2 2 ≤ B W , ∀k = 0, 1, 2, • • • , then {W k 1 , W k 2 , θ k } ∞ k=0 must satisfy W k 2 -(I -W k 1 A) → 0, as k → ∞<label>(7)</label></formula><formula xml:id="formula_10">θ k → 0, as k → ∞.<label>(8)</label></formula><p>Proofs of the results throughout this paper can be found in the supplementary. The conclusion <ref type="bibr" target="#b6">(7)</ref> demonstrates that the weights</p><formula xml:id="formula_11">{W k 1 , W k 2 } ∞ k=0</formula><p>in LISTA asymptotically satisfies the following partial weight coupling structure:</p><formula xml:id="formula_12">W k 2 = I -W k 1 A.<label>(9)</label></formula><p>We adopt the above partial weight coupling for all layers, letting W k = (W k 1 ) T ∈ m×n , thus simplifying LISTA (4) to:</p><formula xml:id="formula_13">x k+1 = η θ k x k + (W k ) (b -Ax k ) , k = 0, 1, • • • , K -1,<label>(10)</label></formula><p>where {W k , θ k } K-1 k=0 remain as free parameters to train. Empirical results in Fig. <ref type="figure">3</ref> illustrate that the structure (9), though having fewer parameters, improves the performance of LISTA.</p><p>The coupled structure (9) for soft-thresholding based algorithms was empirically studied in <ref type="bibr" target="#b15">[16]</ref>. The similar structure was also theoretically studied in Proposition 1 of <ref type="bibr" target="#b13">[14]</ref> for IHT algorithms using the fixed-point theory, but they let all layers share the same weights, i.e.</p><formula xml:id="formula_14">W k 2 = W 2 , W k 1 = W 1 , ∀k.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LISTA with Support Selection</head><p>We introduce a special thresholding scheme to LISTA, called support selection, which is inspired by "kicking" <ref type="bibr" target="#b18">[19]</ref> in linearized Bregman iteration. This technique shows advantages on recoverability and convergence. Its impact on improving LISTA convergence rate and reducing recovery errors will be analyzed in Section 3. With support selection, at each LISTA layer before applying soft thresholding, we will select a certain percentage of entries with largest magnitudes, and trust them as "true support" and won't pass them through thresholding. Those entries that do not go through thresholding will be directly fed into next layer, together with other thresholded entires.</p><p>Assume we select p k % of entries as the trusted support at layer k. LISTA with support selection can be generally formulated as</p><formula xml:id="formula_15">x k+1 = η ss p k θ k W k 1 b + W k 2 x k , k = 0, 1, • • • , K -1,<label>(11)</label></formula><p>where η ss is the thresholding operator with support selection, formally defined as:</p><formula xml:id="formula_16">(η ss p k θ k (v)) i =              v i : v i &gt; θ k , i ∈ S p k (v), v i -θ k : v i &gt; θ k , i / ∈ S p k (v), 0 : -θ k ≤ v i ≤ θ k v i + θ k : v i &lt; -θ k , i / ∈ S p k (v), v i : v i &lt; -θ k , i ∈ S p k (v),</formula><p>where S p k (v) includes the elements with the largest p k % magnitudes in vector v:</p><formula xml:id="formula_17">S p k (v) = i 1 , i 2 , • • • , i p k |v i1 | ≥ |v i2 | ≥ • • • |v i p k | • • • ≥ |v in | . (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>To clarify, in <ref type="bibr" target="#b10">(11)</ref>, p k is a hyperparameter to be manually tuned, and θ k is a parameter to train. We use an empirical formula to select p k for layer k: p k = min(p • k, p max ), where p is a positive constant and p max is an upper bound of the percentage of the support cardinality. Here p and p max are both hyperparameters to be manually tuned.</p><p>If we adopt the partial weight coupling in <ref type="bibr" target="#b8">(9)</ref>, then <ref type="bibr" target="#b10">(11)</ref> is modified as</p><formula xml:id="formula_19">x k+1 = η ss p k θ k x k + (W k ) T (b -Ax k ) , k = 0, 1, • • • , K -1.<label>(13)</label></formula><p>Algorithm abbreviations For simplicity, hereinafter we will use the abbreviation "CP" for the partial weight coupling in <ref type="bibr" target="#b8">(9)</ref>, and "SS" for the support selection technique. LISTA-CP denotes the LISTA model with weights coupling <ref type="bibr" target="#b9">(10)</ref>. LISTA-SS denotes the LISTA model with support selection <ref type="bibr" target="#b10">(11)</ref>. Similarly, LISTA-CPSS stands for a model using both techniques <ref type="bibr" target="#b12">(13)</ref>, which has the best performance. Unless otherwise specified, LISTA refers to the baseline LISTA (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convergence Analysis</head><p>In this section, we formally establish the impacts of ( <ref type="formula" target="#formula_13">10</ref>) and ( <ref type="formula" target="#formula_19">13</ref>) on LISTA's convergence. The output of the k th layer x k depends on the parameters {W τ , θ τ } k-1 τ =0 , the observed measurement b and the initial point x 0 . Strictly speaking, x k should be written as x k {W τ , θ τ } k-1 τ =0 , b, x 0 . By the observation model b = Ax * + ε, since A is given and x 0 can be taken as 0, x k therefore depends on {(W τ , θ τ )} k τ =0 , x * and ε. So, we can write</p><formula xml:id="formula_20">x k {W τ , θ τ } k-1 τ =0 , x * , ε .</formula><p>For simplicity, we instead just write x k (x * , ε).</p><p>Theorem 2 (Convergence of LISTA-CP). Given {W k , θ k } ∞ k=0 and x 0 = 0, let {x k } ∞ k=1 be generated by <ref type="bibr" target="#b9">(10)</ref>. If Assumption 1 holds and s is sufficiently small, then there exists a sequence of parameters {W k , θ k } such that, for all (x * , ε) ∈ X (B, s, σ), we have the error bound:</p><formula xml:id="formula_21">x k (x * , ε) -x * 2 ≤ sB exp(-ck) + Cσ, ∀k = 1, 2, • • • ,<label>(14)</label></formula><p>where c &gt; 0, C &gt; 0 are constants that depend only on A and s. Recall s (sparsity of the signals) and σ (noise-level) are defined in <ref type="bibr" target="#b5">(6)</ref>.</p><p>If σ = 0 (noiseless case), ( <ref type="formula" target="#formula_21">14</ref>) reduces to</p><formula xml:id="formula_22">x k (x * , 0) -x * 2 ≤ sB exp(-ck).<label>(15)</label></formula><p>The recovery error converges to 0 at a linear rate as the number of layers goes to infinity. Combined with Theorem 1, we see that the partial weight coupling structure ( <ref type="formula" target="#formula_13">10</ref>) is both necessary and sufficient to guarantee convergence in the noiseless case. Fig. <ref type="figure">3</ref> validates ( <ref type="formula" target="#formula_21">14</ref>) and ( <ref type="formula" target="#formula_22">15</ref>) directly.</p><p>Discussion: The bound (15) also explains why LISTA (or its variants) can converge faster than ISTA and fast ISTA (FISTA) <ref type="bibr" target="#b1">[2]</ref>. With a proper λ (see (2)), ISTA converges at an O(1/k) rate and FISTA converges at an O(1/k 2 ) rate <ref type="bibr" target="#b1">[2]</ref>. With a large enough λ, ISTA achieves a linear rate <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. With x(λ) being the solution of LASSO (noiseless case), these results can be summarized as: before the iterates x k settle on a support<ref type="foot" target="#foot_3">6</ref> ,</p><formula xml:id="formula_23">x k → x(λ) sublinearly, x(λ) -x * = O(λ), λ &gt; 0 x k → x(λ) linearly, x(λ) -x * = O(λ), λ large enough.</formula><p>Based on the choice of λ in LASSO, the above observation reflects an inherent trade-off between convergence rate and approximation accuracy in solving the problem <ref type="bibr" target="#b0">(1)</ref>, see a similar conclusion in <ref type="bibr" target="#b12">[13]</ref>: a larger λ leads to faster convergence but a less accurate solution, and vice versa.</p><p>However, if λ is not constant throughout all iterations/layers, but instead chosen adaptively for each step, more promising trade-off can arise <ref type="foot" target="#foot_4">7</ref> . LISTA and LISTA-CP, with the thresholds {θ k } K-1 k=0 free to train, actually adopt this idea because {θ k } K-1 k=0 corresponds to a path of LASSO parameters</p><formula xml:id="formula_24">{λ k } K-1 k=0 . With extra free trainable parameters, {W k } K-1 k=0 (LISTA-CP) or {W k 1 , W k 2 } K-1 k=0 (LISTA)</formula><p>, learning based algorithms are able to converge to an accurate solution at a fast convergence rate. Theorem 2 demonstrates the existence of such sequence {W k , θ k } k in LISTA-CP <ref type="bibr" target="#b9">(10)</ref>. The experiment results in Fig. <ref type="figure">4</ref> show that such {W k , θ k } k can be obtained by training. Assumption 2. Signal x * and observation noise ε are sampled from the following set:</p><formula xml:id="formula_25">(x * , ε) ∈ X (B, B, s, σ) (x * , ε) |x * i | ≤ B, ∀i, x * 1 ≥ B, x * 0 ≤ s, ε 1 ≤ σ . (<label>16</label></formula><formula xml:id="formula_26">)</formula><p>Theorem 3 (Convergence of LISTA-CPSS). Given {W k , θ k } ∞ k=0 and x 0 = 0, let {x k } ∞ k=1 be generated by <ref type="bibr" target="#b12">(13)</ref>. With the same assumption and parameters as in Theorem 2, the approximation error can be bounded for all (x * , ε) ∈ X (B, s, σ):</p><formula xml:id="formula_27">x k (x * , ε) -x * 2 ≤ sB exp - k-1 t=0 c t ss + C ss σ, ∀k = 1, 2, • • • , (<label>17</label></formula><formula xml:id="formula_28">)</formula><p>where c k ss ≥ c for all k and C ss ≤ C. If Assumption 2 holds, s is small enough, and B ≥ 2Cσ (SNR is not too small), then there exists another sequence of parameters { W k , θk } that yields the following improved error bound: for all (x * , ε) ∈ X (B, B, s, σ),</p><formula xml:id="formula_29">x k (x * , ε) -x * 2 ≤ sB exp - k-1 t=0 ct ss + Css σ, ∀k = 1, 2, • • • , (<label>18</label></formula><formula xml:id="formula_30">)</formula><p>where ck ss ≥ c for all k, ck ss &gt; c for large enough k, and Css &lt; C.</p><p>The bound in <ref type="bibr" target="#b16">(17)</ref> ensures that, with the same assumptions and parameters, LISTA-CPSS is at least no worse than LISTA-CP. The bound in <ref type="bibr" target="#b17">(18)</ref> shows that, under stronger assumptions, LISTA-CPSS can be strictly better than LISTA-CP in both folds: ck ss &gt; c is the better convergence rate of LISTA-CPSS; Css &lt; C means that the LISTA-CPSS can achieve smaller approximation error than the minimum error that LISTA can achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical Results</head><p>For all the models reported in this section, including the baseline LISTA and LAMP models , we adopt a stage-wise training strategy with learning rate decaying to stabilize the training and to get better performance, which is discussed in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulation Experiments</head><p>Experiments Setting. We choose m = 250, n = 500. We sample the entries of A i.i.d. from the standard Gaussian distribution, A ij ∼ N (0, 1/m) and then normalize its columns to have the unit 2 norm. We fix a matrix A in each setting where different networks are compared. To generate sparse vectors x * , we decide each of its entry to be non-zero following the Bernoulli distribution with p b = 0.1. The values of the non-zero entries are sampled from the standard Gaussian distribution. A test set of 1000 samples generated in the above manner is fixed for all tests in our simulations.</p><p>All the networks have K = 16 layers. In LISTA models with support selection, we add p% of entries into support and maximally select p max % in each layer. We manually tune the value of p and p max for the best final performance. With p b = 0.1 and K = 16, we choose p = 1.2 for all models in simulation experiments and p max = 12 for LISTA-SS but p max = 13 for LISTA-CPSS. The recovery performance is evaluated by NMSE (in dB):</p><formula xml:id="formula_31">NMSE(x, x * ) = 10 log 10 E x -x * 2 E x * 2</formula><p>, where x * is the ground truth and x is the estimate obtained by the recovery algorithms (ISTA, FISTA, LISTA, etc.).</p><p>Validation of Theorem 1. In Fig 2 , we report two values, W k 2 -(I -W k 1 A) 2 and θ k , obtained by the baseline LISTA model (4) trained under the noiseless setting. The plot clearly demonstrates that W k 2 → I -W k 1 A, and θ k → 0, as k → ∞. Theorem 1 is directly validated. Validation of Theorem 2. We report the test-set NMSE of LISTA-CP <ref type="bibr" target="#b9">(10)</ref> in Fig. <ref type="figure">3</ref>. Although <ref type="bibr" target="#b9">(10)</ref> fixes the structure between W k 1 and W k 2 , the final performance remains the same with the baseline LISTA (4), and outperforms AMP, in both noiseless and noisy cases. Moreover, the output of interior layers in LISTA-CP are even better than the baseline LISTA. In the noiseless case, NMSE converges exponentially to 0; in the noisy case, NMSE converges to a stationary level related with the noise-level. This supports Theorem 2: there indeed exist a sequence of parameters {(W k , θ k )} K-1 k=0 leading to linear convergence for LISTA-CP, and they can be obtained by data-driven learning.     <ref type="formula" target="#formula_1">2</ref>)) as well as an adaptive threshold rule similar to one in <ref type="bibr" target="#b22">[23]</ref>, which is described in the supplementary. As we have discussed after Theorem 2, LASSO has an inherent tradeoff based on the choice of λ. A smaller λ leads to a more accurate solution but slower convergence. The adaptive thresholding rule fixes this issue: it uses large λ k for small k, and gradually reduces it as k increases to improve the accuracy <ref type="bibr" target="#b22">[23]</ref>. Except for adaptive thresholds {θ k } k (θ k corresponds to λ k in LASSO), LISTA-CP has adaptive weights {W k } k , which further greatly accelerate the convergence. Note that we only ran ISTA and FISTA for 16 iterations, just enough and fair to compare them with the learned models. The number of iterations is so small that the difference between ISTA and FISTA is not quite observable.</p><formula xml:id="formula_32">(a) Weight W k 2 → I -W k 1 A as k → ∞.</formula><p>Validation of Theorem 3. We compare the recovery NMSEs of LISTA-CP <ref type="bibr" target="#b9">(10)</ref> and LISTA-CPSS <ref type="bibr" target="#b12">(13)</ref> in Fig. <ref type="figure" target="#fig_7">5</ref>. The result of the noiseless case (Fig. <ref type="figure" target="#fig_7">5</ref>(a)) shows that the recovery error of LISTA-SS converges to 0 at a faster rate than that of LISTA-CP. The difference is significant with the number of layers k ≥ 10, which supports our theoretical result: "c k ss &gt; c as k large enough" in Theorem 3. The result of the noisy case (Fig. <ref type="figure" target="#fig_7">5(b</ref>)) shows that LISTA-CPSS has better recovery error than LISTA-CP. This point supports Css &lt; C in Theorem 3. Notably, LISTA-CPSS also outperforms LAMP <ref type="bibr" target="#b15">[16]</ref>, when k &gt; 10 in the noiseless case, and even earlier as SNR becomes lower.</p><p>Performance with Ill-Conditioned Matrix. We train LISTA, LAMP, LISTA-CPSS with illconditioned matrices A of condition numbers κ = 5, 30, 50. As is shown in Fig. <ref type="figure" target="#fig_8">6</ref>, as κ increases, the performance of LISTA remains stable while LAMP becomes worse, and eventually inferior to LISTA when κ = 50. Although our LISTA-CPSS also suffers from ill-conditioning, its performance always stays much better than LISTA and LAMP.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Natural Image Compressive Sensing</head><p>Experiments Setting. We perform a compressive sensing (CS) experiment on natural images (patches). We divide the BSD500 <ref type="bibr" target="#b24">[25]</ref> set into a training set of 400 images, a validation set of 50 images, and a test set of 50 images. For training, we extract 10,000 patches f ∈ R 16×16 at random positions of each image, with all means removed. We then learn a dictionary D ∈ R 256×512 from them, using a block proximal gradient method <ref type="bibr" target="#b25">[26]</ref>. For each testing image, we divide it into non-overlapping 16 × 16 patches. A Gaussian sensing matrices Φ ∈ R m×256 is created in the same manner as in Sec. 4.1, where m 256 is the CS ratio. Since f is typically not exactly sparse under the dictionary D, Assumptions 1 and 2 no longer strictly hold. The primary goal of this experiment is thus to show that our proposed techniques remain robust and practically useful in non-ideal conditions, rather than beating all CS state-of-the-arts.</p><p>Network Extension. In the real data case, we have no ground-truth sparse code available as the regression target for the loss function <ref type="bibr" target="#b4">(5)</ref>. In order to bypass pre-computing sparse codes f over D on the training set, we are inspired by <ref type="bibr" target="#b10">[11]</ref>: first using layer-wise pre-training with a reconstruction loss w.r.t. dictionary D plus an l 1 loss, shown in <ref type="bibr" target="#b18">(19)</ref>, where k is the layer index and Θ k denotes all parameters in the k-th and previous layers; then appending another learnable fully-connected layer (initialized by D) to LISTA-CPSS and perform an end-to-end training with the cost function <ref type="bibr" target="#b19">(20)</ref>. Results. The results are reported in Table <ref type="table" target="#tab_1">1</ref>. We build CS models at the sample rates of 20%, 30%, 40%, 50%, 60% and test on the standard Set 11 images as in <ref type="bibr" target="#b26">[27]</ref>. We compare our results with three baselines: the classical iterative CS solver, TVAL3 <ref type="bibr" target="#b27">[28]</ref>; the "black-box" deep learning CS solver, Recon-Net <ref type="bibr" target="#b26">[27]</ref>;a l 0 -based network unfolded from IHT algorithm <ref type="bibr" target="#b14">[15]</ref>, noted as LIHT; and the baseline LISTA network, in terms of PSNR (dB) <ref type="foot" target="#foot_5">8</ref> . We build 16-layer LIHT, LISTA and LISTA-CPSS networks and set λ = 0.2. For LISTA-CPSS, we set p% = 0.4% more entries into the support in each layer for support selection. We also select support w.r.t. a percentage of the largest magnitudes within the whole batch rather than within a single sample as we do in theorems and simulated experiments, which we emprically find is beneficial to the recovery performance. Table <ref type="table" target="#tab_1">1</ref> confirms LISTA-CPSS as the best performer among all. The advantage of LISTA-CPSS and LISTA over Recon-Net also endorses the incorporation of the unrolled sparse solver structure into deep networks.</p><formula xml:id="formula_33">L k (Θ k ) = N i=1 f i -D • x k i (Θ k ) 2 2 + λ x k i (Θ k ) 1<label>(19)</label></formula><formula xml:id="formula_34">L(Θ, W D ) = N i=1 f i -W D • x K i (Θ) 2 2 + λ x K i (Θ) 1<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have introduced a partial weight coupling structure to LISTA, which reduces the number of trainable parameters but does not hurt the performance. With this structure, unfolded ISTA can attain a linear convergence rate. We have further proposed support selection, which improves the convergence rate both theoretically and empirically. Our theories are endorsed by extensive simulations and a real-data experiment. We believe that the methodology in this paper can be extended to analyzing and enhancing other unfolded iterative algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) RNN structure of ISTA. (b) Unfolded learned ISTA Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagrams of ISTA and LISTA.</figDesc><graphic coords="2,240.18,332.30,269.29,84.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The threshold θ k → 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Validation of Theorem 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Validation of Theorem 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Validation of Theorem 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance in ill-conditioned situations (SNR = ∞).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The Average PSRN (dB) for Set 11 test images with CS ratio ranging from 0.2 to 0.6 .39 29.76 31.51 33.16 Recon-Net 27.18 29.11 30.49 31.39 32.44 LIHT 25.83 27.83 29.93 31.73 34.00 LISTA 28.17 30.43 32.75 34.26 35.99 LISTA-CPSS 28.25 30.54 32.87 34.60 36.39</figDesc><table><row><cell>Algorithm</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell></row><row><cell>TVAL3</cell><cell cols="2">25.37 28</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Soft-thresholding function is defined in a component-wise way: η θ (x) = sign(x) max(0, |x| -θ)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>The convergence of ISTA/FISTA measures how fast the k-th iterate proceeds; the convergence of LISTA measures how fast the output of the k-th layer proceeds as k increases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>A signal is s-sparse if it has no more than s non-zero entries.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>After x k settles on a support, i.e. as k large enough such that support(x k ) is fixed, even with small λ, ISTA reduces to a linear iteration, which has a linear convergence rate<ref type="bibr" target="#b21">[22]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>This point was studied in<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> with classical compressive sensing settings, while our learning settings can learn a good path of parameters without a complicated thresholding rule or any manual tuning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>We applied TVAL3, LISTA and LISTA-CPSS on 16 × 16 patches to be fair. For Recon-Net, we used their default setting working on 33 × 33 patches, which was verified to perform better than using smaller patches.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work by X. Chen and Z. Wang is supported in part by NSF RI-1755701. The work by J. Liu and W. Yin is supported in part by NSF DMS-1720237 and ONR N0001417121. We would also like to thank all anonymous reviewers for their tremendously useful comments to help improve our work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Iterative thresholding for sparse approximations</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="629" to="654" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on imaging sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep l0 encoders</title>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2194" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep dual-domain based fast restoration of jpeg-compressed images</title>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingzhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2764" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a task-specific deep architecture for clustering</title>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM International Conference on Data Mining</title>
		<meeting>the 2016 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="369" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning a deep ∞ encoder for hashing</title>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingzhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2174" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning efficient sparse and low rank models</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingzhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Sparse Coding and its Applications in Computer Vision</title>
		<imprint>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zheng Qin, and Rick Siow Mong Goh. SC2Net: Sparse LSTMs for sparse coding</title>
		<author>
			<persName><forename type="first">Joey Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding trainable sparse coding with matrix factorization</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tradeoffs between convergence speed and reconstruction accuracy in inverse problems</title>
		<author>
			<persName><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonina</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maximal sparsity with deep networks?</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4340" to="4348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iterative hard thresholding for compressed sensing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and computational harmonic analysis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="274" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AMP-inspired deep networks for sparse linear inverse problems</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Borgerding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Schniter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sundeep</forename><surname>Rangan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learned D-AMP: Principled neural network based compressive image recovery</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Christopher A Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1770" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Onsager-corrected deep learning for sparse linear inverse problems</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Borgerding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Schniter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Global Conference on Signal and Information Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast linearized bregman iteration for compressive sensing and sparse denoising</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematical Sciences</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linear convergence of iterative soft-thresholding</title>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><forename type="middle">A</forename><surname>Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="813" to="837" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new linear convergence result for the iterative soft thresholding algorithm</title>
		<author>
			<persName><forename type="first">Lufang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jen-Chih</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1177" to="1189" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local linear convergence of ista and fista on the lasso problem</title>
		<author>
			<persName><forename type="first">Shaozhe</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Boley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzhong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="313" to="336" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fixed-point continuation for 1 -minimization: methodology and convergence</title>
		<author>
			<persName><forename type="first">Elaine</forename><forename type="middle">T</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1107" to="1130" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A proximal-gradient homotopy method for the sparse least-squares problem</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1062" to="1091" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion</title>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on imaging sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1758" to="1789" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recon-Net: Non-iterative reconstruction of images from compressively sensed measurements</title>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Lohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Kerviche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Ashok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An efficient augmented lagrangian method with applications to total variation minimization</title>
		<author>
			<persName><forename type="first">Chengbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="507" to="530" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific Belmont</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>MA</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
