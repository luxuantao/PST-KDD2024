<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structural Entropy Guided Graph Hierarchical Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junran</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Devel-opment Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xueyuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Devel-opment Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shangzhe</forename><surname>Li</surname></persName>
							<email>&lt;shangzheli@buaa.edu.cn&gt;.</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Devel-opment Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematical Science</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Devel-opment Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structural Entropy Guided Graph Hierarchical Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following the success of convolution on non-Euclidean space, the corresponding pooling approaches have also been validated on various tasks regarding graphs. However, because of the fixed compression quota and stepwise pooling design, these hierarchical pooling methods still suffer from local structure damage and suboptimal problem. In this work, inspired by structural entropy, we propose a hierarchical pooling approach, SEP, to tackle the two issues. Specifically, without assigning the layer-specific compression quota, a global optimization algorithm is designed to generate the cluster assignment matrices for pooling at once. Then, we present an illustration of the local structure damage from previous methods in the reconstruction of ring and grid synthetic graphs. In addition to SEP, we further design two classification models, SEP-G and SEP-N for graph classification and node classification, respectively. The results show that SEP outperforms state-of-the-art graph pooling methods on graph classification benchmarks and obtains superior performance on node classifications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Chasing the great success of deep learning in natural language processing and images, plenty of research efforts have been devoted to the adoption of neural networks in tasks without data on the Euclidean domain, i.e., in graphs <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b24">Veličković et al., 2018)</ref>. Thus, recent years, graph neural networks (GNNs) become ubiquitous within deep learning for graphs, and have obtained great accomplishments in various domains, such as node classification <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017)</ref>, link prediction <ref type="bibr">(Zhang &amp;</ref> Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). Chen, 2018) and graph classification <ref type="bibr" target="#b29">(Xu et al., 2019)</ref>. In these works, a key direction is the convolutional mechanism of GNNs <ref type="bibr" target="#b27">(Wu et al., 2019;</ref><ref type="bibr" target="#b34">Zhu &amp; Koniusz, 2020;</ref><ref type="bibr" target="#b28">Wu et al., 2022)</ref>, which aims to learn the structural information in graphs. Meanwhile, another research direction in GNNs is the pooling mechanism, which follows the customs in CNN models that compress a set of nodes into a compact representation <ref type="bibr" target="#b12">(Lee et al., 2019;</ref><ref type="bibr" target="#b2">Bianchi et al., 2020)</ref>.</p><p>Besides the simplest pooling methods, that sum or average all nodes, various well-designed pooling approaches have been proposed to aggregate the node information in a hierarchical manner. However, despite the effectiveness of these methods on various tasks, there are still many issues that hinder the development of GNNs. First, pooling methods based on node drop, like TopKPool <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019)</ref>, SAGPool <ref type="bibr" target="#b12">(Lee et al., 2019)</ref> and ASAP <ref type="bibr" target="#b20">(Ranjan et al., 2020)</ref>, unnecessarily cut nodes based on designed ranking strategy in each pooling layer, resulting in information loss <ref type="bibr" target="#b1">(Baek et al., 2021)</ref>. Although other methods based on node clustering avoid this issue <ref type="bibr" target="#b30">(Ying et al., 2018;</ref><ref type="bibr" target="#b2">Bianchi et al., 2020)</ref>, the damage on the graph local structure still can not be prevented due to the artificially specified node compression quota <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b20">Ranjan et al., 2020)</ref>, which also exists in the node drop methods<ref type="foot" target="#foot_0">1</ref> . Furthermore, the cluster assignments produced by these works only rely on the topology of graph in the current layer without any consideration of the relationships among pooling layers. This would probably lead to suboptimal results in their tasks. Thus, a pooling operation that is globally optimized and has natural node partition is preferred.</p><p>In this paper, we present a hierarchical pooling method, termed SEP, to address the above two issues that hinder the development of GNNs (Figure <ref type="figure" target="#fig_0">1</ref>). Specifically, inspired by structural entropy <ref type="bibr" target="#b13">(Li &amp; Pan, 2016)</ref>, a metric designed to assess the graph structural information, the essential structure of a graph can be decoded by this metric as a measure of the complexity of its hierarchical structure. In particular, the cluster assignments for hierarchical pooling can be directly obtained from the proposed algorithm for structural entropy minimization. Note that, the proposed algorithm is globally optimized and free from learning, which means the cluster assignments will be generated together to avoid the suboptimal problem. Moreover, the algorithm does not rely on a fixed layer-specific compression quota but the number of compression layers, which would help retain the local structure of graphs.</p><p>Before the validation of SEP on classification benchmarks, we first present an illustration of the damage caused by previous hierarchical pooling methods on local structure of graphs. With seven benchmarks from bioinformatics and social networks, we then experimentally validate the effectiveness of SEP on tasks regarding graph classification, and conclude that SEP surpasses the state-of-the-art (SOTA) hierarchical pooling methods on most benchmarks especially on the social network datasets. Besides the superior performance on global attribute discerning, we further evaluate SEP on node classification tasks to better validate its capability of information retaining within the process of pooling. The results show that SEP outperforms the model with the same architecture (i.e., g-U-Nets) and most baselines. To sum up, our contributions are listed as follows:</p><p>• We uncover two crucial issues in previous hierarchical pooling works that hinder the development of GNNs, including the local structure damage and suboptimal problem because of the fixed compression quota and stepwise pooling design.</p><p>• Through the introduction of the structural information theory, we present a novel hierarchical pooling approach, termed SEP, to address the unveiled issues.</p><p>• We extensively validate SEP on graph reconstruction, graph classification, and node classification tasks, in which outperformances are observed in comparison the SOTA hierarchical pooling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Hierarchical pooling. To pursue better generalization and performance, pooling operations have been adopted to amplify the receptive fields and reduce the input sizes.</p><p>Several designs are proposed from the angle of selecting the most important k nodes from the original graph to organize a new one, such as TopKPool <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019)</ref>, SAGPool <ref type="bibr" target="#b12">(Lee et al., 2019)</ref> and ASAP <ref type="bibr" target="#b20">(Ranjan et al., 2020)</ref>. Though efficient, this node-drop design would result in information loss and isolated subgraphs, which will deteriorate the performance of GNNs. Thus, another design based on node clustering emerges and avoids this issue, including DiffPool <ref type="bibr" target="#b30">(Ying et al., 2018)</ref> and MinCutPool <ref type="bibr" target="#b2">(Bianchi et al., 2020)</ref>, in which the nodes of original graph are merged into a bunch of clusters. Although preventing information loss, the damage on graph local structures still exists because of the fixed node compression quota.</p><p>Structural entropy. Information entropy stems from the demand for information measurement in communication systems <ref type="bibr" target="#b22">(Shannon, 1948)</ref>. Correspondingly, to measure the information in graphs, structural entropy was proposed and used to evaluate the complexity of the hierarchical structure of a graph <ref type="bibr" target="#b13">(Li &amp; Pan, 2016)</ref>, which is also a natural node clustering method for graphs. Furthermore, twoand three-dimensional structural entropy, which measure the complexity of two-and three-level hierarchical structures, respectively, have been applied in medicine <ref type="bibr" target="#b15">(Li et al., 2016b)</ref>, bioinformatics <ref type="bibr" target="#b16">(Li et al., 2018)</ref>, and the security of networks <ref type="bibr" target="#b14">(Li et al., 2016a)</ref>. In the light of this global measurement of graph information, structural entropy can be used to decode the essential structure of graphs, which further sparks us the yielding of SEP to address the two issues that impede the development of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, under the guidance of structural entropy, we present our key idea and an algorithm for the cluster assignments construction. Then, we design a GNN model, which has several convolutional layers and pooling layers, to learn global representations for graph classification. Furthermore, we develop another model, which is made up of additional convolutional layers and unpooling layers, to obtain local representations for node classification. Before elaborating on them, we first show some notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>A graph G can be represented as a multi-tuple (V, E, X), where |V| = n is the node set, |E| = m is the edge set, and X ∈ R n×d is the feature matrix for n nodes with ddimensional feature vector. The topology structure of graph G can be found in its adjacency matrix A ∈ R n×n .</p><p>Graph neural networks. In this work, we select Graph Convolutional Network (i.e., GCN <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017</ref>)) as the convolutional layers of our models. GCN and its variants have achieved excellent performance in different kinds of tasks regarding graphs. There is no doubt that our proposed pooling operator can also work with other GNNs like GAT <ref type="bibr" target="#b24">(Veličković et al., 2018)</ref> and GIN <ref type="bibr" target="#b29">(Xu et al., 2019)</ref>. This will be discussed in the experimental section. For a stacked graph neural networks, the i-th convolutional layer in the form of GCN can be formally written as:</p><formula xml:id="formula_0">H i+1 = ReLU ( D− 1 2 Ã D− 1 2 H i W i ),<label>(1)</label></formula><p>where ReLU is a non-linear activation function, W i ∈ R h×h is the trainable matrix for this layer, H i+1 ∈ R n×h is the output of this layer and H 0 = X. In particular, Ã = A + I denotes the adjacency matrix with self-loops, and Dii = j Ãij . In the process of hyper-parameter tuning, we fix the same hidden dimension for all layers.</p><p>Hierarchical pooling. In general, hierarchical pooling is a graph coarsening process to dig out a subset of representative nodes and form a new graph. Let S i ∈ R ni+1×ni denote the cluster assignment matrix at i-th pooling layer, where n i and n i+i are the number of nodes before and after graph coarsening. The new adjacency matrix and node feature matrix after pooling are calculated by the next equations:</p><formula xml:id="formula_1">A i+1 = S i A i S i ; P i+1 = S i H i ,<label>(2)</label></formula><p>where A i ∈ R ni×ni is the adjacency matrix at the i-th layer and H i ∈ R ni×h refers to the node feature matrix produced by the i-th graph convolutional layer. Specifically, according to the cluster assignments S i , P i+1 receives the node hidden features H i and merges these features to refine the initial representations for the n i+1 clusters in the novel graph. Correspondingly, A i+1 employs the node connectivities A i to weave a more delicate origination among n i+1 clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cluster Assignments via Structural Entropy Minimization</head><p>Here, we present our methodology for hierarchical pooling.</p><p>As claimed above, although plenty of works have been presented for graph coarsening based on heuristics or theories, little attention has been paid to the relationships among pooling layers. These studies only produce the cluster assignments based on the graph in the current layer. Meanwhile, in various tasks regarding graphs, there are graphs constructed from specific domain (e.g., social networks) or weaved by human (e.g., synthetic graphs), which are usually not optimal for GNNs because of noisy information. It is thus vital to eliminate such noisy structure from graphs in the process of cluster assignments generation. In this paper, inspired by structural entropy <ref type="bibr" target="#b13">(Li &amp; Pan, 2016)</ref>, we propose a novel hierarchical pooling approach, denoted as SEP, to address the aforementioned issues in previous works. Besides the measurement of graph information, structural entropy can also be used to decode the hierarchical structure of a given graph as a metric of the complexity of its underlying essential structure. Thus, through structural entropy minimization, the hierarchical structure of a graph can be decoded into a corresponding coding tree, in which disturbance derived from noise or stochastic variation can be minimized <ref type="bibr" target="#b16">(Li et al., 2018)</ref>. We believe an effective structural entropy minimization algorithm could uncover the connections among hierarchical pooling layers and eliminate the noisy structure in graphs.</p><p>Based on the definition in <ref type="bibr" target="#b13">(Li &amp; Pan, 2016)</ref>, let a two-tuple (V, E) be a graph G, the formal equation of the structural entropy for G on coding tree T can be written as:</p><formula xml:id="formula_2">H T (G) = − vt∈T g vt vol(V) log vol(v t ) vol(v + t ) ,<label>(3)</label></formula><p>where v t is a nonroot node in T that can also be viewed as a node subset ⊂ V according to its leaf node partition in T , v + t is the parent of v t , g vt refers to the number of edges with an endpoint in the leaf node partition of v t , and vol(V) and vol(v t ) are the sums of degrees of leaf nodes in V and v t , respectively. The minimum entropy realized by the optimal coding tree T is the structural entropy of G, which follows this target equation: H(G) = min ∀T {H T (G)}. According to the definition of structural entropy, we know that the coding tree is a natural hierarchical division for graphs, and the connections among different layers are established for the purpose of structural entropy minimization. Furthermore, the local structure in graphs will be retained because we do not need to allocate layer-specific node compression quotas.</p><p>Besides the optimal coding tree that realizes the minimized structural entropy, in most cases, a natural coding tree with a certain height is preferred, because we only need a few fixed times of graph coarsening for most tasks <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b1">Baek et al., 2021)</ref>. In this context, the k-dimensional structural entropy of G is proposed to decode the optimal coding tree with a fixed height k:</p><formula xml:id="formula_3">H (k) (G) = min ∀T :Height(T )=k {H T (G)}.<label>(4)</label></formula><p>In this paper, under the guidance of k-dimensional structural entropy, we aim to investigate the solution for decrypting the coding tree with a certain height k. Firstly, we define three functions.</p><p>Definition 3.1. Let T be any coding tree for graph G = (V, E), v r is the root node of T and V are the leaf nodes of T . Given any two nodes</p><formula xml:id="formula_4">(v i , v j ) in T , in which v i ∈ v r .children and v j ∈ v r .children. Define a function MERGE T (v i , v j ) for T to insert a new node v ε between v r and (v i , v j ): v ε .children ← v i ; (5) v ε .children ← v j ; (6) v r .children ← v ε ;<label>(7)</label></formula><p>Definition 3.2. Following the setting in Definition 3.1, given any two nodes</p><formula xml:id="formula_5">(v i , v j ), in which v i ∈ v j .children. Define a function REMOVE T (v i ) for T to remove v i from T and merge v i .children to v j .chileren: v j .children ← v i .children;<label>(8)</label></formula><p>Definition 3.3. Following the setting in Definition 3.1, given any two nodes</p><formula xml:id="formula_6">(v i , v j ), in which v i ∈ v j .children and |Heigth(v j ) − Height(v i )| &gt; 1.</formula><p>Define a function FILL(v i , v j ) for T to insert a new node v ε between v i and v j :</p><formula xml:id="formula_7">v ε .children ← v i ; (9) v j .children ← v ε ;<label>(10)</label></formula><p>Based on the three defined functions, a greedy algorithm to compute the coding tree with a certain height k via structural entropy minimization can be found in Algorithm 1. Specifically, a full-height binary coding tree is first generated from bottom to top. In this stage, two child nodes of root are merged to form a new division in each iteration, which aims to maximize the structural entropy reduction. In the second stage, in order to satisfy a fixed number of graph coarsening, we need squeeze the previous full-height binary coding tree by dropping nodes. Each time, we select an inner-node from T , which makes T have the minimized structural entropy after removing this node. At the end of the second stage, we have already obtained a coding tree with a certain height k under the guidance of structural entropy. However, there may be nodes that do not have immediate successor in the next layer because of cross-layer links, which will cause node missing when realizing hierarchical pooling based on such a coding tree. Therefore, we need perform the third stage to ensure the integrity of information transmission between layers, and also need not interfere with the structural entropy of G on coding tree T (see Proposition 3.4). Finally, a coding tree T for the given graph G can be obtained, where</p><formula xml:id="formula_8">T = (V T , E T ), V T = (V T 0 , . . . , V T k ) and V T 0 = V.</formula><p>In addition, the cluster assignment matrices can also be obtained from E T , that is, S = (S 1 , . . . , S k ). Proposition 3.4. Let T be a coding tree after the second stage of Algorithm 1, and given two adjacent nodes</p><formula xml:id="formula_9">(v i , v j ) in T , in which v i ∈ v j .children and |Heigth(v j )− Height(v i )| &gt; 1. Then, H T (G) = H T FILL(v i ,v j ) (G).</formula><p>Algorithm 1 Coding tree with height k via structural entropy minimization Input: a graph G = (V, E), a positive integer k &gt; 1 Output: a coding tree T with height k 1: Generate a coding tree T with a root node v r and all nodes in V as leaf nodes; 2: // Stage 1: Bottom to top construction; 3: while |v r .children| &gt; 2 do 4:</p><formula xml:id="formula_10">Select v i and v j from v r .children, conditioned on argmax (vi,vj ) {H T (G) − H T MERGE(v i ,v j ) (G)}; 5: MERGE(v i , v j ); 6: end while 7: // Stage 2: Compress T to the certain height k; 8: while Height(T ) &gt; k do 9: Select v i from T , conditioned on argmin vi {H T REMOVE(v i ) (G) − H T (G)| v i = v r &amp; v i / ∈ V}; 10:</formula><p>REMOVE(v i ); 11: end while 12: // Stage 3: Fill T to avoid cross-layer links; 13: for v i ∈ T do 14:</p><formula xml:id="formula_11">if |Height(v i .parent) − Height(v i )| &gt; 1 then 15: FILL(v i , v i .parent); 16:</formula><p>end if 17: end for 18: return T ;</p><p>Proof. Equation 3 shows that the structural entropy of graph G on T is the summation of</p><formula xml:id="formula_12">H T v = − gv vol(V) log vol(v) vol(v + ) for all nonroot nodes in T . That is, H T (G) = H T vi + H T vj + . . . and H T F (G) = H T F v i + H T F vε + H T F v j + . . . , denote</formula><p>T F = T FILL(vi,vj ) for simplicity and (v i , v j ) corresponds to (v i , v j ) after FILL. According to Equation 3, we have:</p><formula xml:id="formula_13">H T F v i = − g v i vol(V) log vol(v i ) vol(v + i ) = − g v i vol(V) log vol(v i ) vol(v ε ) = 0 with vol(v i ) = vol(v ε ),<label>(11)</label></formula><formula xml:id="formula_14">H T F vε = − g vε vol(V) log vol(v ε ) vol(v + ε ) = − g vi vol(V) log vol(v i ) vol(v j ) = H T vi ,<label>(12)</label></formula><formula xml:id="formula_15">H T F v j = − g v j vol(V) log vol(v j ) vol(v + j ) = − g vj vol(V) log vol(v j ) vol(v + j ) = H T vj .<label>(13)</label></formula><p>Thus, we have</p><formula xml:id="formula_16">H T (G) = H T FILL(v i ,v j ) (G).</formula><p>Complexity analysis. The runtime complexity of Algorithm 1 is O(2n + h max (m log n + n)), and h max is the height of coding tree T after the first stage. Meanwhile, since coding tree T tends to be balanced in the process of structural entropy minimization, h max will be around log n. Furthermore, graph generally has more edges than nodes, i.e., m n, thus the runtime of Algorithm 1 almost scales linearly in the number of edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Graph Neural Network for Graph Classification</head><p>In this subsection, we present the architecture based on SEP for graph classification, and name it SEP-G. As shown in Figure <ref type="figure">2</ref>, the hierarchical pooling architecture follows the setting in previous pooling studies <ref type="bibr" target="#b12">(Lee et al., 2019;</ref><ref type="bibr" target="#b2">Bianchi et al., 2020;</ref><ref type="bibr" target="#b1">Baek et al., 2021)</ref>, which consists of three blocks and each block has a GCN layer and a SEP layer.</p><p>For the graph-level representation, the outputs after each block are summarized and then fed to a prediction layer for classification. Based on Equations 1 and 2, the graph representation with SET-G can be formally written as:</p><formula xml:id="formula_17">h G = Concat(Readout(SEP i (GCN i (H i , A i ), S i )) |∀i ∈ {1, 2, 3}). (<label>14</label></formula><formula xml:id="formula_18">) GCN SEP GCN SEP GCN SEP MLP Figure 2:</formula><p>The SEP-G architecture for graph classification. Following the design of previous works in hierarchical pooling, the architecture is comprised of three GCN layers and each is followed by corresponding SEP layer.</p><p>Permutation invariance. In graph classification, it is important to ensure the permutation invariance of designed graph neural network. In our proposed model for graph classification, there are two main components, that is, GCN layer and SEP layer. The permutation invariance of GCN layer has been confirmed by previous works <ref type="bibr" target="#b30">(Ying et al., 2018;</ref><ref type="bibr" target="#b17">Ma et al., 2019)</ref>. Thus, the SEP layer should be invariant with permutations.</p><p>Proposition 3.5. Given a permutation matrix P ∈ {0, 1} n×n , then SEP(A, H) = SEP(PAP , PH) (i.e., SEP is permutation invariant).</p><p>Proof. The cluster assignments are derived from the coding tree via Algorithm 1, which is a traversal algorithm that does not depend on the order of nodes. Therefore, the generated assignments S will not change with any permutation. In addition, we know that the permutation matrix is orthogonal, thus PP = I with Equation 2 finishes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Graph Neural Network for Node Classification</head><p>Beyond the functionality of cluster assignments to convert graphs into high-level representations, we can also adopt the same matrix S i to unpool the compressed graph representation H i and structure A i to the original space by:</p><formula xml:id="formula_19">A i+1 = S i A i S i ; P i+1 = S i H i .<label>(15)</label></formula><p>In this context, we present an illustration of the architecture for node classification in Figure <ref type="figure" target="#fig_1">3</ref>, and we call it SEP-N. SEP-N is an encoder-decoder network analogous to the design of g-U-nets <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019)</ref>. In the encoder, several down-sampling blocks are applied to encode higher-order features. Each block consists of a GCN layer and a SEP layer like our SEP-G. In the decoder, we employ the consistent number of decoding components. Thus, we can see the same GCN layer but with a SEP-U layer for unpooling in the decoder block. The skip-connections linking corresponding encoders and decoders are also adopted to enable spatial information transmission for better performance. Finally, a GCN layer is used to perform final predictions. There are two encoder and two decoder blocks and each block is composed of a GCN layer and a pooling (unpooling) layer. Skip connection links the same-level encoder and decoder to enhance spatial feature transmission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we describe the experiment setup for graph classification and node classification in detail, and validate the effectiveness of our proposed methods, SEP-G and SEP-N, in several corresponding benchmarks<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Graph Reconstruction</head><p>Before the presentation of experiments regarding two major classification tasks in GNNs, we first employ a graph reconstruction experiment, which quantifies the structural information retained by pooling layer, to directly reveal the damage caused by previous hierarchical pooling methods to graph's local structures.</p><p>Configuration. An autoencoder is trained to reconstruct the input graph with pooling and unpooling layers. The learning objective is to minimize the mean square error (MSE) between input features X and output features X r , i.e., min X − X r 2 . For configuration, we employ the Synthetic graphs <ref type="bibr" target="#b2">(Bianchi et al., 2020)</ref>, including a ring and a grid that the input features are the coordinates of nodes in a 2-D Euclidean space. Detailed experiment configuration and model description can be found in Appendix A.1. Reconstruction results. Figure <ref type="figure" target="#fig_2">4</ref> shows the original graphs and the reconstructed graphs by model with various pooling methods. We select TopKPool to represent methods with node drop design and MinCutPool to represent methods with node clustering design 3 . We first notice the reconstructed results of TopKPool, where the basic shape of original graphs can not even be identified. This confirms that node drop methods will lead to severe information missing and implies the poor performance of node drop methods in graph classification. On the other hand, MinCutPool indeed retains the basic shape of original graphs. However, we can still see the significant distortion in the edge of ring and the center of grid, which represent the key structures of ring and grid, and this validates the assumption that the local structure in graphs would be ruined by the artificially specified node compression quota. On the contrary, SEP 3 Reconstruction results of all hierarchical pooling baselines can be found in Appendix A.1. almost reconstructs the ring and retains the essential structure in grid center which suggests that our pooling method obtains the key structural information of original graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Graph Classification</head><p>Graph classification aims to label a given graph G with the maximum probability among several seed categories. To this end, we need learn the high-level representation from its component nodes, which enables the final classifier evaluate the likelihood of category that the graph belongs to.</p><p>Datasets. Seven benchmarks for graph classification are selected from TU datasets <ref type="bibr" target="#b19">(Morris et al., 2020)</ref>. Specifically, we employ three social network datasets, including IMDB-BINARY, IMDB-MULTI, and COLLAB; and four bioinformatics datasets, including MUTAG, PROTEINS, D&amp;D and NCI1. Table <ref type="table" target="#tab_0">1</ref> summarizes the characteristics of the seven employed datasets, and more detailed descriptions can be found in the Appendix A.2.</p><p>Baselines. We first employ two popular backbones in GNNs for comparison, that is, GCN <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017)</ref> and GIN <ref type="bibr" target="#b29">(Xu et al., 2019)</ref>. Then, we adopt the next five hierarchical pooling methods as baselines: DiffPool <ref type="bibr" target="#b30">(Ying et al., 2018)</ref>, SAGPool(H) <ref type="bibr" target="#b12">(Lee et al., 2019)</ref>, TopKPool <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019)</ref>, ASAP <ref type="bibr" target="#b20">(Ranjan et al., 2020)</ref>, and Min-CutPool <ref type="bibr" target="#b2">(Bianchi et al., 2020)</ref>. Besides various hierarchical pooling methods, plenty of efforts have also been devoted to global pooling for graph classification. Thus, we also select the following five global pooling techniques for comparison: Set2Set <ref type="bibr" target="#b26">(Vinyals et al., 2016)</ref>, SortPool <ref type="bibr">(Zhang et al., 2018)</ref>, SAGPool(G) <ref type="bibr" target="#b12">(Lee et al., 2019)</ref>, StructPool <ref type="bibr" target="#b31">(Yuan &amp; Ji, 2020)</ref>, and GMT <ref type="bibr" target="#b1">(Baek et al., 2021)</ref>.</p><p>Configurations. Following <ref type="bibr" target="#b29">(Xu et al., 2019;</ref><ref type="bibr" target="#b12">Lee et al., 2019)</ref>, 10-fold cross-validation is conducted, and we present the average accuracies achieved to validate the performance of SEP-G in graph classification. In addition, the initial feature inputs is in line with the fair comparison setting in <ref type="bibr" target="#b7">(Errica et al., 2020)</ref>. Additional details about experiment setup can be found in the Appendix A.2.</p><p>Classification results. The classification accuracies of SEP-G and other baselines are shown in Table <ref type="table" target="#tab_0">1</ref>, and we can see that our method consistently achieves better or competitive performance as compared to these SOTA methods. In particular, SEP-G obtains a unified improvement in social network datasets, which differs from the performance in bioinformatics datasets. This performance divergence may be because SEP only relies on the network structure for hierarchical pooling, while the structural information in social network datasets is more redundant than that in bioinformatics datasets <ref type="bibr" target="#b3">(Centola, 2010)</ref>. It is worth noting that there are not any pooling methods suppress GIN in NCI1, or, put differently, pooling methods also do not show unified promotion in comparison with backbones. Considering the recent finding that message-passing is the crucial mechanism in graph classification <ref type="bibr" target="#b18">(Mesquita et al., 2020)</ref>, this phenomenon may not be so disappointing.</p><p>Variants of SEP-G. As discussed in Section 3.1, besides GCN, our proposed pooling operator can also work with other GNNs like GAT and GIN. Here, we delve deeper into the collaboration ability of our pooling method with other GNNs. Specifically, we employ the ChebNet <ref type="bibr" target="#b6">(Defferrard et al., 2016)</ref>, GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b24">(Veličković et al., 2018)</ref> and GIN <ref type="bibr" target="#b29">(Xu et al., 2019)</ref>. The classification results are shown in Table <ref type="table" target="#tab_1">2</ref>. As can be seen, the overall superior performance is obtained by these variants, which suggests the effectiveness and kindness of SEP. Notably, a better performance on IMDB-MULTI, PROTEINS, DD and NCI1 is obtained by SEP with GAT, which further indicates the huge potential of SEP in collaboration with other SOTA backbones.</p><p>Visualization case study. To better demonstrate the effectiveness of SEP on essential structure preserving, we present the clustering results of DiffPool, MinCutPool, and SEP on samples from the MUTAG dataset after the first graph coarsening. As shown in Figure <ref type="figure" target="#fig_3">5</ref>, DiffPool and Min-CutPool severely damage the essential structures of the two compounds, in which the ring structures of two molecular formulas are arbitrarily torn into several pieces. Fortunately, SEP manages to take good care of these vital structures in the process of cluster assignment generation, and this shows the effectiveness of SEP in issues addressing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Node Classification</head><p>Node classification is another important task regarding graphs in GNNs, which aims to label each node in a given graph G. Here, we conduct corresponding experiments on ubiquitous benchmarks to validate the effectiveness of our proposed SEP-N.</p><p>Datasets. We evaluate SEP-N under the transductive learning setting, which includes three datasets Cora, Citeseer and Pubmed <ref type="bibr" target="#b21">(Sen et al., 2008)</ref>. The three benchmarks are constructed on the connections of document citations, which means nodes are documents and edges are citation links. The node features are different among three datasets. Specifically, the input features of Cora and Citeseer are one- Following the experiment setup in previous works <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b8">Gao &amp; Ji, 2019)</ref>, the designated training/validation/testing splits on Cora, Citeseer and Pubmed are adopted, that is, training set has 20 nodes for each class, validation set has 500 nodes and testing set has 1,000 nodes.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows the dataset statistics.</p><p>Baselines. In addition to g-U-nets <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019</ref>) that has the same encoder-decoder design, we also include three SOTA backbones in GNNs: GCN <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b24">(Veličković et al., 2018)</ref>, and GIN <ref type="bibr" target="#b29">(Xu et al., 2019)</ref>. Moreover, other following works based on the three models are also employed for a comprehensive comparison, such as FastGCN <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, APPNP <ref type="bibr">(Klicpera et al., 2019)</ref>, MixHop (Abu-El-Haija et al., 2019), SGC <ref type="bibr" target="#b27">(Wu et al., 2019)</ref>, DGI <ref type="bibr" target="#b25">(Velickovic et al., 2019)</ref>, S 2 GC <ref type="bibr" target="#b34">(Zhu &amp; Koniusz, 2020)</ref>, and GCNII <ref type="bibr" target="#b5">(Chen et al., 2020)</ref>.</p><p>Configurations. For node classification tasks, we fix our SEP-N with two blocks of encoders and decoders as presented in Figure <ref type="figure" target="#fig_1">3</ref>, and plan to obtain the coding tree for each dataset under the guidance of three-dimensional structural entropy. In particular, each block has a GCN layer followed by a SEP (or SEP-U) layer. Finally, a GCN layer is adopted to make final prediction. There is no doubt that skip connections between layers are also established between encoder and decoder analogous to g-U-nets. Note that we add an additional linear layer after each SEP or SEP-U layer to learn more task-specific node representations. Dropout <ref type="bibr" target="#b23">(Srivastava et al., 2014)</ref> with ReLU on feature matrices is applied to all layers in SEP-N, and L2 regularization is also adopted to avoid over-fitting. Detailed descriptions for experimental setup are shown in Appendix A.3.</p><p>Classification results. The accuracies of our proposed SEP-N and baselines on three benchmarks are shown in Table <ref type="table" target="#tab_2">3</ref>. In general, we can observe that the SEP-N does not achieve the best performance on any datasets, which are shown in S 2 GC on Citeseer and GCNII on Cora and Pubmed. This problem may be attributed to the cluster assignments generation, which only relies on structural in-formation, while the tasks regarding node classification are more dependent on the input features of nodes. However, SEP-N still obtains competitive results with only 5 GCN layers, which is much less than the design of S 2 GC ( <ref type="formula">16</ref>) and GCNII (64 for Cora, 32 for Citeseer, 16 for Pubmed). In particular, the accuracies of SEP-N on three datasets are consistently better than the three backbones (i.e., GCN, GAT and GIN). Furthermore, compared with g-U-nets, which is also designed with GCN and hierarchical pooling mechanism, our method also surpasses it on two out of three datasets (Cora and Pubmed) even with fewer learning blocks. Note that superior performance of SEP-N on Citeseer has also been achieved with different numbers of blocks, which we will describe it in the next experiments. In summary, we can confirm the effectiveness of the proposed hierarchical pooling operation in node representation learning.   <ref type="table" target="#tab_3">4</ref>. As we mentioned above, SEP-N achieves superior performance on Citeseer with only one encoder and decoder. In particular, compared with g-U-Nets, we can see that the best performance of SEP-N on three datasets is achieved with at most 2 blocks, which once again proves the capacity of shallow networks in high-level feature encoding <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019)</ref>. Moreover, this scene also reveals that additional connection information among layers for cluster assignments generation is helpful for representation learning, as well as benefits model optimization that better performance with fewer parameters and computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we develop an optimization algorithm to address several limitations of existing hierarchical pooling approaches. In particular, under the guidance of structural entropy minimization, our pooling method, SEP, can not only capture the connectivities among pooling layers but also fix the problem of destroying local structure due to the hyper-parameter for node compression. Based on the proposed SEP, we introduce two learning models, SEP-G and SEP-N, for graph classification and node classification, respectively. Experimental results suggest that SEP-G achieves significant improvements on graph classification, and SEP-N obtains superior performance as compared to other GNNs on node classification tasks. An interesting direction for future work is shown in the combination of structural entropy and node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Setup</head><p>In this section, we introduce the experimental details about graph reconstruction, graph classification, and node classification tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Graph Reconstruction</head><p>Dataset. Graph reconstruction experiments with synthetic graphs represented in a 2-D Euclidean space, such as ring and grid structures. The node features of a graph consist of their location in a 2-D coordinate space, and the adjacency matrix indicates the connectivity pattern of nodes. The goal here is to restore all node locations from compressed features after pooling, with the intact adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN SEP GCN GCN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEP-U</head><p>GCN GCN Implementation details. Following <ref type="bibr" target="#b2">(Bianchi et al., 2020)</ref>, we use the two message passing layers both right before the pooling operation and right after the unpooling operation. Also, both pooling and unpooling operations are performed once and sequentially connected, as illustrated in the Figure A.1. We compare our methods against both the node drop methods, including TopKPool <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019)</ref>, SAGPool <ref type="bibr" target="#b12">(Lee et al., 2019)</ref> and ASAP <ref type="bibr" target="#b20">(Ranjan et al., 2020)</ref>, and node clustering methods, including DiffPool <ref type="bibr" target="#b30">(Ying et al., 2018)</ref> and MinCutPool <ref type="bibr" target="#b2">(Bianchi et al., 2020)</ref>. For the node drop methods, we use the unpooling operation proposed in the graph U-net <ref type="bibr" target="#b8">(Gao &amp; Ji, 2019)</ref>. For the node clustering methods, we use the graph coarsening schemes described in the Equation <ref type="formula" target="#formula_19">15</ref>, with their specific implementations on generating an assignment matrix.</p><p>For our proposed method, we follow the setting of node clustering methods. In particular, we finely tuned the height of coding tree produced by Algorithm 1 to make sure the number of nodes compressed in the first layer close to the setting of baselines. For model configuration, the pooling ratio of all models is set to 25%, the learning rate is set to 5 × 10 −3 , and hidden size is set to 32. For the loss function, we use the Mean Squared Error (MSE) to train models. We then optimize the network with Adam optimizer. We use the early stopping criterion, where we stop the training if there is no further improvement on the training loss during 1,000 epochs. Further, the maximum number of epochs is set to 10,000. Note that, there is no other available graphs for validation of the synthetic graph, such that we train and test the models only with the given graph in the Figure <ref type="figure" target="#fig_1">3(a)</ref>.</p><p>Reconstruction results on all hierarchical pooling methods. 3, the node drop methods suffer from the issue of information loss, resulting in the basic shape of original graphs can not be identified. For node clustering methods, DiffPool and MinCutPool, the basic shape of original graphs is basically retained. However, we can still see the significant distortion in the edge of ring and the center of grid, which are almost prevented in SEP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Graph Classification</head><p>Social network datasets. IMDB-BINARY and IMDB-MULTI are derived from the collaboration of a movie set. In these two datasets, every graph consists of actors or actresses, and each edge between two nodes represents their cooperation in a certain movie. Each graph is derived from a prespecified movie, and its label corresponds to the genre of this movie. Similarly, COLLAB is also a collaboration dataset but from a scientific realm, which includes three public collaboration datasets (i.e., Astro Physics, High Energy Physics and Condensed Matter Physics). Many researchers from each field form various ego networks for the graphs in this benchmark. The label of each graph is the research field to which the nodes belong.</p><p>Bioinformatic datasets. D&amp;D contains graphs of protein structures. A node represents an amino acid and edges are constructed if the distance of two nodes is less than 6 Å. A label denotes whether a protein is an enzyme or non-enzyme. PROTEINS is a dataset where the nodes are secondary structure elements (SSEs), and there is an edge between two nodes if they are neighbors in the given amino acid sequence or in 3D space. The dataset has 3 discrete labels, representing helixes, sheets or turns. PTC is a dataset containing 344 chemical compounds that reports the carcinogenicity of male and female  rats and has 19 discrete labels. NCI1 is a dataset made publicly available by the National Cancer Institute (NCI) and is a subset of balanced datasets containing chemical compounds screened for their ability to suppress or inhibit the growth of a panel of human tumor cell lines; this dataset possesses 37 discrete labels. MUTAG has seven kinds of graphs that are derived from 188 mutagenic aromatic and heteroaromatic nitro compounds. PTC includes 19 discrete labels and reports the carcinogenicity of 344 chemical compounds for male and female rats.</p><p>Initial inputs. The data of the bioinformatic datasets and social network datasets differ in that the nodes in bioinformatics graphs have categorical labels that do not exist in social networks. Thus, the initial node features of the HRN inputs are set to one-hot encodings of the node degrees for social networks and a combination of the one-hot encodings of the degrees and categorical labels for bioinformatic graphs.</p><p>Implementation details. We evaluate the model performance with a 10-fold cross validation setting, where the dataset split is based on the conventionally used training/test splits <ref type="bibr">(Zhang et al., 2018;</ref><ref type="bibr" target="#b2">Bianchi et al., 2020;</ref><ref type="bibr" target="#b1">Baek et al., 2021)</ref>.</p><p>In addition, we use the 10 percent of the training data as a validation data following the fair comparison setup <ref type="bibr" target="#b7">(Errica et al., 2020)</ref>. We use the early stopping criterion, where we stop the training if there is no further improvement on the validation loss during 50 epochs. Furthermore, the maximum number of epochs is set to 500. We then report the average performances on test sets, by performing overall experiments 10 times. In particular, following the implementation of <ref type="bibr" target="#b29">(Xu et al., 2019)</ref>, we train each epoch with a fixed number of iterations (i.e., 50) for small datasets. We set the pooling ratio as 25% in each pooling layer for baselines as previous works <ref type="bibr" target="#b1">(Baek et al., 2021;</ref><ref type="bibr" target="#b2">Bianchi et al., 2020)</ref>, while our model follows the natural cluster assignments produced by Algorithm 1 with given height 3. For model configuration, the learning rate is set to 5 × 10 −<ref type="foot" target="#foot_2">4</ref> , the hidden size is set ∈ {64, 128}, the batch size is set ∈ {32, 128}, weight decay is set to 1 × 10 −4 , and dropout rate is set ∈ {0, 0.5}. Then we optimize the network with Adam optimizer. For a fair comparison of baselines <ref type="bibr" target="#b12">(Lee et al., 2019)</ref>, we use the three GCN layers <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017)</ref> as a message passing function for all models with skip connections, and only change the pooling architecture throughout all models. Because GMT is the most recent work that replicates these popular pooling approaches in previous studies, and we implement SEP-G based on the code of GMT 4 , thus the accuracies of baselines are derived from <ref type="bibr" target="#b1">(Baek et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Node Classification</head><p>Citation datasets. We utilize three standard citation network benchmark datasets: Cora, Citeseer and Pubmed <ref type="bibr" target="#b21">(Sen et al., 2008)</ref>. In all of these datasets, nodes correspond to documents and edges to (undirected) citations. Node features correspond to elements of a bag-of-words representation of a document. Each node has a class label. In particular, the node features are different among three datasets. Specifically, the input features of Cora and Citeseer are one-hot embedding of words in each document, while the node features of Pubmed are the TF-IDF weighted word vectors. The Cora dataset contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. The Citeseer dataset contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. The Pubmed dataset contains 19717 nodes, 44338 edges, 3 classes and 500 features per node.</p><p>Implementation details. We closely follow the transductive experimental setup in <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017)</ref>. Each class is only allowed 20 nodes for training, which honors the transductive setup, and the training algorithm has access to all of the nodes' feature vectors. The predictive power of the trained models is evaluated on 1000 test nodes, and we use 500 additional nodes for validation purposes. We also use the early stopping criterion, where we stop the training if there is no further improvement on the validation loss during 50 epochs. Furthermore, the maximum number of epochs is set to 1000. We obtain cluster assignment matrices of each dataset under the guidance of three-dimensional structural entropy, and adopt the first two layers for hierarchical pooling. For model configuration, the learning rate is set to 0.01, the hidden size is set ∈ {16, 32, 128, 256}, weight decay is set ∈ {0.02, 5 × 10 −4 }, and dropout rate for each layer is set ∈ {0, . . . , 0.9}. Finally, we optimize the network with Adam optimizer.</p><p>Fair comparison with SOTA methods. To make a fair comparison with S 2 GC and GCNII, we present the results of the two methods with similar convolutional layers. As shown in Table <ref type="table">A</ref>.1, SEP-N obtains competitive results with only 5 GCN layers, which is much less than the requirement of S 2 GC (16) and GCNII (64 for Cora, 32 for Citeseer, 16 for Pubmed). Furthermore, SEP-N outperforms S 2 GC and GCNII when putting similar number of convolution layers, which reveals the effectiveness of hierarchical graph pooling in node classification tasks when facing limited computing resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of proposed SEP operator combined with graph neural network. The graph neural network consists of message passing layers and hierarchical pooling layers. A separate algorithm is proposed for cluster assignments generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The SEP-N architecture for node classification.There are two encoder and two decoder blocks and each block is composed of a GCN layer and a pooling (unpooling) layer. Skip connection links the same-level encoder and decoder to enhance spatial feature transmission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of local structure damage from node drop and clustering methods in reconstruction of ring and grid synthetic graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Essential structure preserving on MUTAG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A. 1 :</head><label>1</label><figDesc>Figure A.1: The architecture for graph reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure A.2 and A.3 show the results of reconstructed graphs by model with various pooling methods. As can be seen from the first row of Figure A.2 and A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure A.2: Reconstruction results of ring synthetic graphs, compared to node drop and clustering methods.</figDesc><graphic url="image-1.png" coords="12,112.48,219.90,123.90,123.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure A.3: Reconstruction results of grid synthetic graphs, compared to node drop and clustering methods.</figDesc><graphic url="image-2.png" coords="12,112.48,535.64,123.90,123.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Graph classification accuracies on seven benchmarks (%). The shown accuracies are mean and standard deviation over 10 different runs. We highlight the best results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Social Network</cell><cell></cell><cell></cell><cell cols="2">Bioinformatics</cell></row><row><cell></cell><cell></cell><cell cols="2">IMDB-BINARY IMDB-MULTI</cell><cell>COLLAB</cell><cell>MUTAG</cell><cell>PROTEINS</cell><cell>D&amp;D</cell><cell>NCI1</cell></row><row><cell># Graphs</cell><cell></cell><cell>1,000</cell><cell>1,500</cell><cell>5,000</cell><cell>188</cell><cell>1,113</cell><cell>1,178</cell><cell>4,110</cell></row><row><cell># Classes</cell><cell></cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Avg. # Nodes</cell><cell></cell><cell>19.8</cell><cell>13.0</cell><cell>74.5</cell><cell>17.9</cell><cell>39.1</cell><cell>284.3</cell><cell>29.8</cell></row><row><cell>Backbones</cell><cell>GCN GIN</cell><cell>73.26±0.46 72.78±0.86</cell><cell>50.39±0.41 48.13±1.36</cell><cell>80.59±0.27 78.19±0.63</cell><cell cols="4">69.50±1.78 73.24±0.73 72.05±0.55 76.29±1.79 81.39±1.53 71.46±1.66 70.79±1.17 80.00±1.40</cell></row><row><cell></cell><cell>Set2Set</cell><cell>72.90±0.75</cell><cell>50.19±0.39</cell><cell>79.55±0.39</cell><cell cols="4">69.89±1.94 73.27±0.85 71.94±0.56 68.55±1.92</cell></row><row><cell>Global Pooling</cell><cell>SortPool SAGPool(G) StructPool</cell><cell>72.12±1.12 72.16±0.88 72.06±0.64</cell><cell>48.18±0.83 49.47±0.56 50.23±0.53</cell><cell>77.87±0.47 78.85±0.56 77.27±0.51</cell><cell cols="4">71.94±3.55 73.17±0.88 75.58±0.72 73.82±1.96 76.78±2.12 72.02±1.08 71.54±0.91 74.18±1.20 79.50±0.75 75.16±0.86 78.45±0.40 78.64±1.53</cell></row><row><cell></cell><cell>GMT</cell><cell>73.48±0.76</cell><cell>50.66±0.82</cell><cell>80.74±0.54</cell><cell cols="4">83.44±1.33 75.09±0.59 78.72±0.59 76.35±2.62</cell></row><row><cell></cell><cell>DiffPool</cell><cell>73.14±0.70</cell><cell>51.31±0.72</cell><cell>78.68±0.43</cell><cell cols="4">79.22±1.02 73.03±1.00 77.56±0.41 62.32±1.90</cell></row><row><cell></cell><cell>SAGPool(H)</cell><cell>72.55±1.28</cell><cell>50.23±0.44</cell><cell>78.03±0.31</cell><cell cols="4">73.67±4.28 71.56±1.49 74.72±0.82 67.45±1.11</cell></row><row><cell>Hierarchical</cell><cell>TopKPool</cell><cell>71.58±0.95</cell><cell>48.59±0.72</cell><cell>77.58±0.85</cell><cell cols="4">67.61±3.36 70.48±1.01 73.63±0.55 67.02±2.25</cell></row><row><cell>Pooling</cell><cell>ASAP</cell><cell>72.81±0.50</cell><cell>50.78±0.75</cell><cell>78.64±0.5</cell><cell cols="4">77.83±1.49 73.92±0.63 76.58±1.04 71.48±0.42</cell></row><row><cell></cell><cell>MinCutPool</cell><cell>72.65±0.75</cell><cell>51.04±0.70</cell><cell>80.87±0.34</cell><cell cols="4">79.17±1.64 74.72±0.48 78.22±0.54 74.25±0.86</cell></row><row><cell></cell><cell>SEP-G</cell><cell>74.12±0.56</cell><cell>51.53±0.65</cell><cell>81.28±0.15</cell><cell cols="4">85.56±1.09 76.42±0.39 77.98±0.57 78.35±0.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Graph classification accuracies of SEP-G with various backbones. The default backbone is GCN, and we denote it as SEP-G-GCN for a better illustration.</figDesc><table><row><cell>Variants</cell><cell cols="2">Social Network IMDB-BINARY IMDB-MULTI</cell><cell>COLLAB</cell><cell>MUTAG</cell><cell>Bioinformatics PROTEINS DD</cell><cell>NCI1</cell></row><row><cell>SEP-G-GCN</cell><cell>74.12±0.56</cell><cell>51.53±0.65</cell><cell>81.28±0.15</cell><cell cols="3">85.56±1.09 76.42±0.39 77.98±0.57 78.35±0.33</cell></row><row><cell>SEP-G-GIN</cell><cell>73.37±0.95</cell><cell>51.81±0.98</cell><cell>79.18±0.60</cell><cell cols="3">83.22±1.28 74.77±1.42 75.98±1.15 76.59±1.65</cell></row><row><cell>SEP-G-GAT</cell><cell>73.24±0.81</cell><cell>51.87±0.45</cell><cell>79.26±0.39</cell><cell cols="3">84.45±1.81 76.72±0.92 78.07±0.74 78.43±1.07</cell></row><row><cell>SEP-G-ChebNet</cell><cell>73.72±0.42</cell><cell>50.84±0.68</cell><cell>80.73±0.43</cell><cell cols="3">83.25±1.13 74.67±0.75 76.69±0.71 77.68±0.97</cell></row><row><cell>SEP-G-GraphSAGE</cell><cell>73.14±0.87</cell><cell>50.43±1.31</cell><cell>79.88±0.58</cell><cell cols="3">83.75±1.43 75.26±0.86 77.95±0.55 77.65±1.21</cell></row><row><cell cols="4">hot embedding of words in each document, while the node</cell><cell></cell><cell></cell></row><row><cell cols="4">features of Pubmed are the TF-IDF weighted word vectors.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Node classification accuracies on Cora, Citeseer, and Pubmed (%). We highlight our results and those that are significantly higher than all other methods.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell># Nodes</cell><cell>2,708</cell><cell>3,327</cell><cell>19,717</cell></row><row><cell># Edges</cell><cell>5,429</cell><cell>4,732</cell><cell>44,338</cell></row><row><cell># Features</cell><cell>1,433</cell><cell>3,703</cell><cell>4,500</cell></row><row><cell># Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell></row><row><cell>GCN</cell><cell>81.4±0.4</cell><cell>70.9±0.5</cell><cell>79.0±0.4</cell></row><row><cell>GAT</cell><cell>83.3±0.7</cell><cell>72.6±0.6</cell><cell>78.5±0.3</cell></row><row><cell>GIN</cell><cell>77.6±1.1</cell><cell>66.1±0.9</cell><cell>77.0±1.2</cell></row><row><cell>FastGCN</cell><cell>79.8±0.3</cell><cell>68.8±0.6</cell><cell>77.4±0.3</cell></row><row><cell>APPNP</cell><cell>83.3±0.5</cell><cell>71.7±0.6</cell><cell>80.1±0.2</cell></row><row><cell>MixHop</cell><cell>81.8±0.6</cell><cell>71.4±0.8</cell><cell>80.0±1.1</cell></row><row><cell>DGI</cell><cell>82.5±0.7</cell><cell>71.6±0.7</cell><cell>78.4±0.7</cell></row><row><cell>SGC</cell><cell cols="3">81.0±0.03 71.9±0.11 78.9±0.01</cell></row><row><cell>S 2 GC</cell><cell cols="3">83.5±0.02 73.6±0.09 80.2±0.02</cell></row><row><cell>GCNII</cell><cell>85.5±0.5</cell><cell>73.4±0.6</cell><cell>80.3±0.4</cell></row><row><cell>g-U-Nets</cell><cell>84.4±0.6</cell><cell>73.2±0.5</cell><cell>79.6±0.2</cell></row><row><cell>SEP-N</cell><cell>84.8±0.4</cell><cell>72.9±0.7</cell><cell>80.2±0.8</cell></row></table><note>Ablation Study of Network Depth. Besides the other bunch of hyper-parameters in GNNs, the network depth, corresponding to the number of blocks, is also another cru-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Node classification accuracies with different network depths (%). We highlight the best results with different depths for SEP-N and g-U-Nets.</figDesc><table><row><cell></cell><cell cols="2">Cora</cell><cell cols="2">Citeseer</cell><cell cols="2">Pubmed</cell></row><row><cell cols="2">Depth g-U-Nets</cell><cell>SEP-N</cell><cell>g-U-Nets</cell><cell>SEP-N</cell><cell>g-U-Nets</cell><cell>SEP-N</cell></row><row><cell>1</cell><cell>−</cell><cell>84.3±0.6</cell><cell>−</cell><cell>73.3±0.6</cell><cell>−</cell><cell>78.9±0.6</cell></row><row><cell>2</cell><cell cols="6">82.6±0.6 84.8±0.4 71.8±0.5 72.9±0.7 79.1±0.3 80.2±0.8</cell></row><row><cell>3</cell><cell cols="6">83.8±0.7 84.5±0.3 72.7±0.7 72.1±0.6 79.4±0.4 79.5±0.5</cell></row><row><cell>4</cell><cell cols="6">84.4±0.6 83.6±0.6 73.2±0.5 72.1±0.2 79.6±0.2 78.5±0.3</cell></row><row><cell>5</cell><cell cols="6">84.1±0.5 83.9±0.5 72.8±0.6 72.4±0.6 79.5±0.3 79.8±0.7</cell></row><row><cell cols="4">cial setting in model construction. We, thus, delve deeper</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">into the effect of model depth on node classification perfor-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">mance. We iteratively test SEP-N with various numbers of</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">blocks ∈ {1, 2, 3, 4, 5}, and the results are shown in Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The clustering-based pooling methods require the fixed number of clusters and the node drop methods require the fixed node compression ratio.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The implementation of Algorithm 1, SEP-G and SEP-N can be found at https://github.com/Wu-Junran/SEP.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://github.com/JinheonBaek/GMT</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by NSFC (Grant No. 61932002).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><surname>Mixhop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Accurate learning of graph representations with graph multiset pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The spread of behavior in an online social network experiment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Centola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">329</biblScope>
			<biblScope unit="issue">5996</biblScope>
			<biblScope unit="page" from="1194" to="1197" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Fastgcn</surname></persName>
		</author>
		<idno>ICLR, 01</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeruIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeruIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural information and dynamical complexity of networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3290" to="3339" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resistance and security index of networks: structural information perspective of network security</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26810</biblScope>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Three-dimensional gene maps of cancer cell types: Structural entropy minimisation principle for defning tumour subtypes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">20412</biblScope>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoding topologically associating domains with ultra-low resolution hi-c data by graph structural entropy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3265</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with eigenpooling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking pooling in graph neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPs</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2220" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><surname>Asap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication. The Bell system technical journal</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948">1948</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple yet effective method for graph classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First International Joint Conference on Artificial Intelligence<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">July 23-29, 2022. ijcai.org, 2022</date>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPs, volume 2018-Decem</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured graph pooling via conditional random fields</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Structpool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An end-toend deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple spectral graph convolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
