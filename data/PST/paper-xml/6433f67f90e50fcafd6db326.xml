<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UTC-IE: A Unified Token-pair Classification Architecture for Information Extraction</title>
				<funder ref="#_k6xp2Z5 #_W8pwcrx">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">CCF-Baidu Open Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunhua</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Rowling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Harry</forename><surname>Potter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UTC-IE: A Unified Token-pair Classification Architecture for Information Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Information Extraction (IE) spans several tasks with different output structures, such as named entity recognition, relation extraction and event extraction. Previously, those tasks were solved with different models because of diverse task output structures. Through re-examining IE tasks, we find that all of them can be interpreted as extracting spans and span relations. They can further be decomposed into tokenpair classification tasks by using the start and end token of a span to pinpoint the span, and using the start-to-start and end-to-end token pairs of two spans to determine the relation. Based on the reformulation, we propose a Unified Token-pair Classification architecture for Information Extraction (UTC-IE), where we introduce Plusformer on top of the tokenpair feature matrix. Specifically, it models axis-aware interaction with plus-shaped selfattention and local interaction with Convolutional Neural Network over token pairs. Experiments show that our approach outperforms task-specific and unified models on all tasks in 10 datasets, and achieves better or comparable results on 2 joint IE datasets. Moreover, UTC-IE speeds up over state-of-the-art models on IE tasks significantly in most datasets, which verifies the effectiveness of our architecture. 1 * Equal contribution. ? Corresponding author. 1 Code is available at https://github.com/yhcc/utcie. 2 Joint entity relation extraction aims to extract both entities and relations. In our paper, we call it relation extraction (RE) for simplicity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information Extraction (IE) aims to identify and classify structured information from unstructured texts <ref type="bibr" target="#b0">(Andersen et al., 1992;</ref><ref type="bibr" target="#b5">Grishman, 2019)</ref>. IE consists of a wide range of tasks, such as named entity recognition (NER), joint entity relation extraction (RE) 2 and event extraction (EE) .</p><p>In the last decade, many paradigms have been proposed to solve IE tasks, such as sequence label-ing <ref type="bibr" target="#b24">(McCallum and Li, 2003;</ref><ref type="bibr" target="#b10">Huang et al., 2015;</ref><ref type="bibr" target="#b46">Zheng et al., 2017;</ref><ref type="bibr">Yu et al., 2020a)</ref>, span-based classification <ref type="bibr" target="#b11">(Jiang et al., 2020;</ref><ref type="bibr">Yu et al., 2020b;</ref><ref type="bibr" target="#b38">Wang et al., 2021;</ref><ref type="bibr" target="#b42">Ye et al., 2022)</ref>, MRC-based methods <ref type="bibr" target="#b13">(Levy et al., 2017;</ref><ref type="bibr" target="#b16">Li et al., 2020;</ref><ref type="bibr" target="#b18">Liu et al., 2020)</ref> and generation-based methods <ref type="bibr" target="#b45">(Zeng et al., 2018;</ref><ref type="bibr">Yan et al., 2021a;</ref><ref type="bibr" target="#b9">Hsu et al., 2022)</ref>. The above work mainly concentrates on solving individual tasks, but it is desired to unify all IE tasks without designing dedicated modules, as tackling all IE tasks with one model can facilitate knowledge sharing between different tasks. Therefore, various attempts have been made to unify all IE tasks with one model structure. <ref type="bibr" target="#b35">Wadden et al. (2019)</ref>; <ref type="bibr" target="#b17">Lin et al. (2020)</ref>; <ref type="bibr" target="#b25">Nguyen et al. (2021)</ref> encode all IE tasks' target structure as graphs and design graph-based methods to predict them; <ref type="bibr" target="#b26">Paolini et al. (2021)</ref>; <ref type="bibr" target="#b21">Lu et al. (2022)</ref> solve general IE tasks in a generative way with text-to-text or text-to-structure frameworks. However, graph-based models tend to be complex to design, while generative models are time-consuming to decode.</p><p>In our work, we creatively propose a simple yet effective paradigm for unified IE. Inspired by <ref type="bibr" target="#b11">Jiang et al. (2020)</ref>, we re-examine IE tasks and consider that all of them are fundamentally span extraction (entity extraction in NER and RE, trigger extraction and argument span detection in EE) or relational extraction<ref type="foot" target="#foot_0">3</ref> (relation extraction in RE and argument role classification in EE). Based on this perspective, we further simplify and unify all IE tasks into tokenpair classification tasks. Figure <ref type="figure" target="#fig_0">1</ref> shows how each task can be converted. Specifically, a span is decomposed into start-to-end and end-to-start token pairs. As depicted, the entity "School of Computer Science" in Figure <ref type="figure" target="#fig_0">1</ref>(a) is decomposed into indices of (School, Science) and (Science, School). As for it can be classified into pre-defined types. e, r, t, a and rol in figures mean entity, relation, event trigger, event argument and event role. For the span extraction, we use the start-to-end and end-to-start token pairs to pinpoint the span, such as entity spans e 1 , e 2 , argument spans a 1 , a 2 and trigger span t (cells with pure color). For the relational extraction, we use the start-to-start and end-to-end token pairs to represent the relation, such as r and rol 1 , rol 2 (cells with gradient color). It is worth mentioning that both relations and event roles are regarded as directional, namely from start entity to end entity and from event trigger to argument spans. Therefore, all IE tasks can be decomposed into token-pair classifications. After the reformulation, the local dependency and interaction from the plus-shaped orientation (as the orange and blue dotted lines depict) can provide vital information to classify the central token pair.</p><p>detecting the relation between two spans, we convert it into start-to-start and end-to-end token pairs from head mention to tail mention. For example, in Figure <ref type="figure" target="#fig_0">1</ref>(b), the relation "Author" between "J.K.</p><p>Rowling" and "Harry Potter novels" is decomposed into indices of (J.K., Harry) and (Rowling, novels).</p><p>Based on the above decomposition, we propose a Unified Token-pair Classification architecture for Information Extraction (UTC-IE). Specifically, we first apply Biaffine model on top of the pre-trained language model (PLM) to get representations of token pairs. Then we design a novel Transformer to obtain interactions between them. As the plusshaped dotted lines depicted in Figure <ref type="figure" target="#fig_0">1</ref>, token pairs in horizontal and vertical directions cover vital information for the central token pair. For span extraction, token pairs in the plus-shaped orientation are either clashing or nested with the central token pair, for example, e 2 is contained by e 1 in Figure <ref type="figure" target="#fig_0">1(a)</ref>; for relational extraction, the central token pair's two constituent mentions locate in the plus-shaped orientation, such as in Figure <ref type="figure" target="#fig_0">1</ref>(b), r is determined by e 1 and e 2 . Therefore, we make one token pair only attend horizontally and vertically in the token pair feature matrix. Additionally, position embeddings are incorporated to keep the token pairs position-aware. Moreover, neighboring token pairs are highly likely to be informative to determine the types of the central token pair, so we apply Convolutional Neural Network (CNN) to model the local interaction after the plus-shaped attention. Since the attention map for one token pair is intuitively similar to the plus operator, we name this whole novel module as Plusformer.</p><p>We conduct numerous experiments in two settings. When training separately on each task (named as single IE task), our model outperforms previous task-specific and unified models on 10 datasets of all IE tasks. When training a single model simultaneously on all IE tasks in one dataset (named as joint IE task), UTC-IE achieves better or comparable results than 2 joint IE baselines. To thoroughly analyze why UTC-IE is useful under the token-pair paradigm, we execute several ablation studies. We observe that CNN module in Plusformer plays a significant role in IE tasks by the abundant local dependency between token pairs after the reformulation. Besides, owing to the good parallelism of self-attention and CNN, UTC-IE is one to two orders of magnitude faster than prior unified IE models and some task-specific work. To summarize, our key contributions are as follows </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Decomposition and Decoding</head><p>We first introduce how we decompose IE tasks to conduct training, then present the decoding procedure for decomposition. More discussions about the decomposition are presented in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Decomposition</head><p>Formally, given an input sentence of L tokens x = [x 1 , x 2 , ..., x L ], the potential token pairs can form a score matrix Y ? R L?L? (|S|+|R|) , where S is span classes, R is relational classes. We stipulate Joint IE aims to jointly extract entities, relations, and events in the text. Extracting entities and relations are generally the same as those in NER and RE. When extracting events, there is no need to extract argument spans purposely because all argument candidates are entities. Therefore, in joint IE, S = S t ? S e and R = R r ? R o .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoding</head><p>The decoding essentially extracts spans and relations from the score matrix Y. If Y (s,e,t) = Y (e,s,t) = 1 and t ? [1, |S|], then the span (s, e) is of type t. And for two spans (s 1 , e 1 ) and</p><formula xml:id="formula_0">(s 2 , e 2 ), if Y (s 1 ,s 2 ,r) = Y (e 1 ,e 2 ,r) = 1 and r ? [|S| + 1, |S| + |R|],</formula><p>then the span (s 1 , e 1 ) forms relation r with the span (s 2 , e 2 ). The above decoding is for the ideal situation, where no span clash exists. However, for model's predictions, we need to first resolve the conflicts. The decoding with model's predictions will be presented in Appendix B. novel Transformer-like structure named Plusformer to model interactions between token pairs. Finally, we describe loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Biaffine Model</head><p>Given an input sentence, we first apply a PLM as our sentence encoder to obtain the contextualized representation as follows H = [h1, h2, ..., hL] = PLM <ref type="bibr">([x1, x2, ..., xL]</ref>), <ref type="bibr">(1)</ref> where H ? R L?d , d is the PLM's hidden size.</p><p>Next, we use the Biaffine mechanism to get features for each token pair as follows</p><formula xml:id="formula_1">H s , H e = MLPstart(H), MLP end (H), Si,j = (H s i ) T W1H e j + W2(H s i ? H e j ) + v,<label>(2)</label></formula><p>where MLP start , MLP end are multi-layer perceptron layers,</p><formula xml:id="formula_2">H s , H e ? R L?d , W 1 ? R d?c?d , W 2 ? R c?2d , v ? R c</formula><p>, ? refers to concatenation; S ? R L?L?c provides features for all possible token pairs, and c is the feature dimension size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Plusformer</head><p>As illustrated in Section 1, when modeling the interaction between token pairs, the plus-shaped and local interaction should be beneficial. Therefore, we introduce the axis-aware plus-shaped self-attention and position embeddings to conduct plus-shaped interaction, we name this self-attention PlusAttention.</p><p>Then, we leverage CNN to model local dependencies. We name this whole structure Plusformer. PlusAttention. We first apply the self-attention mechanism <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> horizontally and vertically as follows</p><formula xml:id="formula_3">Z h i,: = Attention(Si,:W Q h , Si,:W K h , Si,:W V h ), Z v :,j = Attention(S:,jW Q v , S:,jW K v , S:,jW V v ), Attention(Q, K, V) = softmax( QK T ? c )V,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">W Q h , W K h , W V h , W Q v , W K v , W V v ? R c?c , Z h , Z v ? R L?L?c .</formula><p>After the self-attention, we use the following method to merge Z h , Z v</p><formula xml:id="formula_5">S ? = MLP(Z h ? Z v ),<label>(4)</label></formula><p>where S ? ? R L?L?c . We make the plus-shaped self-attention axis-awareness by using two groups of attention parameters and using concatenation instead of an addition to merge Z h , Z v . Position Embeddings. Although the model should be able to distinguish between horizontal and vertical directions through axis-aware plusshaped attention, it still lacks the sense of distances between token pairs and the area the token pair locates. Hence, we use two kinds of position embeddings to enable the model with these abilities.</p><p>? Rotary Position Embedding (RoPE) <ref type="bibr" target="#b32">(Su et al., 2021)</ref> can encode the relative distance between two token pairs. It is utilized in both horizontal and vertical self-attention.</p><p>? Triangle position embedding is incorporated to mark the position of token pairs in the feature map, indicating whether the cell is in the upper or lower triangles. It adds to S in Eq.(3) before Attention.</p><p>CNN Layer. After the PlusAttention, we apply CNN with kernel size 3 ? 3 on the S ? to help the model exploit the local dependency between neighboring token pairs. The formulation is as follows</p><formula xml:id="formula_6">S ?? = Conv(?(Conv(S ? )))<label>(5)</label></formula><p>where S ?? ? R L?L?c , and ? is the activation function; and the bias term of CNN is not used to avoid result inconsistencies for a sample when it is in batches of different lengths.</p><p>The Plusformer layer will be repeatedly used to interact fully between token pairs. Layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> is ignored in the formulation for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>Finally, we get final scores as follows ?S , ?R = Sigmoid( ?(:,:,: |S|) ), ?(:,:,|S| :) ,</p><formula xml:id="formula_7">? = MLP(S ?? + S),<label>(6)</label></formula><p>where For the span extraction, we use the binary crossentropy (BCE) loss as follows</p><formula xml:id="formula_8">?S ? R L?L?|S| , ?R ? R L?L?(|R|+1</formula><formula xml:id="formula_9">L 1 = - L i,j=1 |S| r=1 [Y (i,j,r) log ?(i,j,r) + (1 -Y (i,j,r) )log(1 -?(i,j,r) )] (7)</formula><p>For the relational extraction, we utilize the ATL as follows</p><formula xml:id="formula_10">L2 = - L i,j=1 r?P T log exp( ?R(i,j,r) ) r ? ?P T ?{TH} exp( ?R(i,j,r ? ) )</formula><p>log exp( ?R(i,j,|R|+1) )</p><formula xml:id="formula_11">r ? ?N T ?{TH} exp( ?R(i,j,r ? ) )<label>(8)</label></formula><p>where P T and N T denote the positive and negative classes, ?R(:,:,|R| +1) is the score for the threshold class TH. Only token pairs with scores higher than their corresponding adaptive thresholds are considered when decoding. We do not use ATL for span extraction because we need to sort span scores when decoding spans. The total loss L = L 1 +L 2 is used for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We conduct experiments on 10 datasets across three IE tasks, including NER, RE, and EE, and on 2 joint IE datasets. We evaluate NER task with <ref type="bibr">CoNLL03 (Sang and Meulder, 2003)</ref> and OntoNotes <ref type="bibr" target="#b28">(Pradhan et al., 2013)</ref> on flat NER, and with ACE04 <ref type="bibr" target="#b2">(Doddington et al., 2004)</ref>, ACE05-Ent <ref type="bibr" target="#b36">(Walker et al., 2006)</ref> and GENIA <ref type="bibr" target="#b12">(Kim et al., 2003)</ref>  proves the effectiveness of interaction between token pairs. Although the performance increment of span extraction is not as significant as that of relational extraction, UTC-IE consistently improves on various span extraction tasks. Besides, we also test UTC-IE without Plusformer. Surprisingly, this simple model surpasses previous SOTA models on four results marked with ? , which proves the effectiveness of the task decomposition. The comparison between models with and without Plusformer clearly shows that Plusformer is effective in all tested datasets, and the performance improvement ranges from +0.40 (on OntoNotes) to +3.00 (on SciERC + ). Notably, the average performance gain of adding Plusformer on symmetric RE (+2.06) is more remarkable than that on RE (+1.03). We presume this is because the interaction between token pairs are more beneficial for symmetric relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Joint IE task</head><p>Multi-task learning has proven to be useful in the IE area <ref type="bibr" target="#b17">(Lin et al., 2020;</ref><ref type="bibr" target="#b25">Nguyen et al., 2021)</ref>. Since UTC-IE unifies all IE tasks into a token-pair classification scenario, it is natural to test whether one UTC-IE model can benefit from jointly learning all IE tasks. In Table <ref type="table" target="#tab_5">2</ref>, the performance of UTC-IE single is from the entity F1 of NER, relation F1 of RE, trigger F1 of EE and argument F1 of EE, respectively. Based on the comparison between UTC-IE single and UTC-IE joint , it is obvious that jointly learning these three tasks consistently improves performance in the 2 joint IE datasets. Moreover, UTC-IE joint outperforms previous SOTA joint IE models in Table <ref type="table" target="#tab_5">2</ref>, the average performance enhancement is +0.69 in ACE05-E+ and +0.75 in ERE-EN. Specifically, UTC-IE joint increases the average performance of relational extraction by +1.30. Thusly, through unifying different IE tasks through our task decomposition, Plusformer can enjoy the benefit of multi-tasking learning, and achieve better performance than previous SOTA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Speed Comparison</head><p>To get a sense of the speed superiority of UTC-IE, we compare the inference speed of UTC-IE with previous unified models on ACE05 series datasets and with task-specific SOTA models on every IE tasks. The former comparison is presented in Table <ref type="table" target="#tab_6">3</ref> and the latter locates in the Appendix E. Compared with the generative UIE <ref type="bibr" target="#b21">(Lu et al., 2022)</ref>, UTC-IE improves F1 from 1.73 (on ACE05-R) to 2.89 (on ACE05-E+), and obtains one order magnitude of speed boost. Compared with OneIE <ref type="bibr" target="#b17">(Lin et al., 2020)</ref>, UTC-IE fundamentally enhances the performance for relational extractions (e.g., Rel. and Arg.) with an average of 4.47 F1 increment in joint IE. At the same time, UTC-IE is one order of magnitude faster than OneIE. In a nutshell, compared with previous SOTA models (whether taskspecific, unified or joint), UTC-IE achieves substantial performance gain across several datasets with a significant speed boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>To analyze the effectiveness of each component in Plusformer, we ablate each of them and list the outcomes in Table <ref type="table" target="#tab_7">4</ref>, and results on more datasets are presented in Appendix F. Besides, we study how many Plusformer layers are suitable in Appendix F.5. Based on the ablation, CNN is the most useful component among all IE tasks. The reason behind this improvement is that once token pairs are organized in the square feature map, the spatial correlations between neighboring token pairs become allusive, and CNN excels at exploiting these local interactions. More comprehensive analysis of CNN in Plusformer locates in Appendix F.1. To deepen our understanding of UTC-IE, we try an- The bottom line of Table <ref type="table" target="#tab_7">4</ref> shows that the CNN-IE model can surpass or approach previous SOTA performance in almost all datasets, which proves the universality of our proposed task formulation. However, CNN is not a panacea for UTC-IE. From Table <ref type="table" target="#tab_7">4</ref>, removing position embeddings or axis-awareness<ref type="foot" target="#foot_1">4</ref> from UTC-IE will lead to an average of 0.39 or 0.44 performance degradation, respectively. Moreover, based on the performance of CNN-IE and UTC-IE, the average performance shrinks from 74.53 to 74.17 if the PlusAttention is deprived of Plusformer, which means the plusshaped self-attention is a desideratum. In addition, we present some intuitive examples and deeper analysis for position embeddings and axis-aware in Appendix F.3 and F.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Information extraction tasks, which consist of named entity recognition, relation extraction, and event extraction, have long been a fundamental and well-researched task in the natural language processing (NLP) field. Previous researches mainly only focus on one or two tasks. Recently, building joint neural models of unified IE tasks has attracted increasing attention. Some of them incorporate graphs into IE structure. <ref type="bibr" target="#b35">Wadden et al. (2019)</ref> propose a unified framework called DYGIE++ to We unify all IE tasks as several token-pair classification tasks, which are fundamentally similar to the span-based methods on the IE task, for the start and end tokens can locate a span. Numerous NER studies emerge on span-based models, which are compatible with both flat and nested entities and perform well <ref type="bibr" target="#b4">(Eberts and Ulges, 2020;</ref><ref type="bibr">Yu et al., 2020b;</ref><ref type="bibr" target="#b14">Li et al., 2021;</ref><ref type="bibr" target="#b49">Zhu and Li, 2022)</ref>. In addition to entities, the span-based method is also used in RE. Some models <ref type="bibr" target="#b38">(Wang et al., 2021;</ref><ref type="bibr" target="#b42">Ye et al., 2022)</ref> only leverage span representations to locate entities and simply calculate the interaction between entity pair, while others <ref type="bibr" target="#b37">(Wang et al., 2020;</ref><ref type="bibr" target="#b47">Zhong and Chen, 2021</ref>) encode span pair information explicitly to extract relations. With regard to event extraction, as far as we know, there is little work on injecting span information into EE explicitly. <ref type="bibr" target="#b35">Wadden et al. (2019)</ref> leverage span representations on general IE, but their model is complicated and only considers span at the embedding layer without further interaction. Conceptually, Jiang et al. (2020)'s work is similar to ours, but they need a two-stage model to determine the span type and span relations, respectively. Detailed analysis are depicted in Appendix G. Although many spanbased IE models exist, they are task-specific and lack interaction between token pairs. Decomposing IE tasks as token-pair classification and conducting interaction between token pairs can uniformly model span-related knowledge and advance SOTA performance.</p><p>The most novel component of Plusformer is the plus-shaped attention mechanism, which can make token pairs interact with each other in an efficient way. A similar structure called Axial Transformers <ref type="bibr" target="#b8">(Ho et al., 2019)</ref> is proposed in the Computer Vision (CV) field, which is designed to deal with data organized as high-dimension tensors. <ref type="bibr" target="#b33">Tan et al. (2022)</ref> incorporate axial attention into relation classification to improve the performance on two-hop relation. However, CNN was not used in these works, while CNN has been proven to be vital to the IE tasks. Another similar structure named Twin Transformer <ref type="bibr" target="#b6">(Guo et al., 2021)</ref> used in CV, where they encode pixels of image from row and column sequentially, and leverage CNN on top of them. But the position embeddings, which are important for IE tasks, are not used in the Twin Transformer. Besides, we want to point out that the usage of plus-shaped attention and CNN originates from the reformulation of IE tasks, any other modules which can directly enable interaction between constituent spans of a relation and between adjacent token pairs should be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we decompose NER, RE and EE tasks into token-pair classifications. Through the decomposition, we unify all IE tasks under the same formulation. After scrutinizing the token-pair feature matrix, we find the adjacent and plus-shaped interactions between token pairs should be informative. Therefore, we propose Plusformer, which uses an axis-aware plus-shaped self-attention followed by CNN layers to help token pairs interact with each other. Experiments on 10 single IE datasets and 2 joint IE datasets all outperform or approach the SOTA performance. Besides, owing to the parallelism of self-attention and CNN, our model's inference speed is substantially faster than previous SOTA models in RE and EE. Lastly, most of the previous IE models limit the interaction in the 1-D sequential dimension, while the reformulation of IE tasks opens a new angle to broaden the communication to the 2-D feature matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>While we unify diverse IE tasks into token-pair classification tasks and propose a simple but useful architecture to help token pairs interact with each other in an effective way, there are still several limitations that are worth discussing. Firstly, all modules in our UTC-IE are based on pre-trained language models, and experiments proves that different PLMs may influence the performance on the same dataset. Hence, our model relies on the capability of the PLM, which need a lot of GPU resources to complete the experiments. Additionally, although incorporating PlusAttention instead of self-attention can effectively reduce the memory and computational complexity from O(L 4 ) to O(2L 3 ), it still require a little large computation. Future work can leverage the backbone of our unification and model, and focus on the acceleration on each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussion on Task decomposition</head><p>In this section, we will discuss two issues of the decomposition. The first is the inconsistency stipulation about the relation decomposition, the second is the false positive issue when decoding relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The inconsistency</head><p>As our stipulation in Section 2, if a span (s, e) has an expected span type t, both the Y (s,e,t) and Y (e,s,t) are 1. If two spans (s 1 , e 1 ) and (s 2 , e 2 ) have relation r, this means the relation should also exist between spans (s 1 , e 1 ) and (e 2 , s 2 ) (the endto-start version of the span (s 2 , e 2 )), then based on our stipulation on the relation, the Y (s 1 ,e 2 ,r) and Y (e 1 ,s 2 ,r) should also equal 1, but we only define the Y (s 1 ,s 2 ,r) = 1 and Y (e 1 ,e 2 ,r) = 1, this causes an inconsistency between the stipulations. We ignore Y (s 1 ,e 2 ,r) and Y (e 1 ,s 2 ,r) to make the decoding less cluttered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 False Positive Relation</head><p>A potential risk of the decomposition and decoding is that it may cause false positive relations. Given four spans p 1 = (s 1 , e 1 ), p 2 = (s 2 , e 2 ), p 3 = (s 3 , e 3 ), p 4 = (s 4 , s 4 ), if p 4 has relation r with p 1 and p 2 , and no relation exist between p 4 and p 3 , then Y (s 4 ,s 1 ,r) = Y (e 4 ,e 1 ,r) = 1, Y (s 4 ,s 2 ,r) = Y (e 4 ,e 2 ,r) = 1. However, if s 1 = s 3 , e 2 = e 3 . Namely, p 1 shares start token with p 3 and p 2 shares end token with p 3 . Then, based on Y (s 4 ,s 1 ,r) = Y (e 4 ,e 2 ,r) = 1, we get Y (s 4 ,s 3 ,r) = Y (e 4 ,e 3 ,r) = 1" the decoding process will mistakenly think p 4 has relation r with p 3 . However, this situation should be rare, and none is found in the tested datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Decoding with model's predictions</head><p>In this section, we will detail the decoding process for models' predictions. The process described in Section 2.2 is not directly applicable to models' predictions since spans may conflict with each other<ref type="foot" target="#foot_4">7</ref> .</p><p>With prediction score matrix ?S from Eq.( <ref type="formula" target="#formula_7">6</ref>), we follow previous work <ref type="bibr">(Yu et al., 2020b)</ref> to first filter out spans whose scores are less than 0.5; for the remaining spans, we sort the spans based on their scores, then choose spans in descending order and make sure the span has no boundary clash with chosen spans. For relational extraction, we first decode all spans, then we get a binary matrix ?R = ?R(:,:,: |R|+1) &gt; ?R(:,:,|R|+1) , then we pair spans to check whether they form relations. Take two spans (s 1 , e 1 ) and (s 2 , e 2 ) for instance, if ?R(s 1 ,s 2 ,r) = ?R(e 1 ,e 2 ,r) = 1, we claim the first span has relation r with the second span. For the RE task, we pair all entity spans to check if they form relations; for the EE task, we pair the trigger spans and argument spans to check if they form a role relationship; and for the joint IE task, we pair entity spans to check if they form relations, we pair the trigger spans and entity spans (because all argument spans are entity spans) to check if they form a role relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Settings</head><p>In this section, we describe all experimental settings in detail, such as the statistics of datasets, baseline models, and more implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Datasets</head><p>We conduct experiments on 10 single IE datasets and 2 joint IE datasets, and we detail the statistics of all datasets in Table <ref type="table" target="#tab_9">5</ref>.</p><p>Named entity recognition. We perform experiments on both flat and nested NER benchmarks. In flat NER, we adopt CoNLL03 <ref type="bibr" target="#b29">(Sang and Meulder, 2003)</ref> and OntoNotes<ref type="foot" target="#foot_5">8</ref> (Pradhan et al., 2013) datasets. In nested NER, we experiment on ACE04<ref type="foot" target="#foot_6">9</ref>  <ref type="bibr" target="#b2">(Doddington et al., 2004</ref>), ACE05<ref type="foot" target="#foot_7">10</ref> ( <ref type="bibr" target="#b36">Walker et al., 2006)</ref> and GENIA <ref type="bibr" target="#b12">(Kim et al., 2003)</ref>. To distinguish ACE05 dataset used in other tasks, we name ACE05 in named entity recognition as ACE05-Ent. Specifically, we use the same preprocessing and splitting procedure on nested datasets as <ref type="bibr" target="#b40">Yan et al. (2022)</ref>, for they fix some annotation problems to unify different versions of these datasets and make a strictly fair comparison.</p><p>Relation extraction. We conduct experiments on two relation extraction datasets, ACE05 <ref type="bibr" target="#b36">(Walker et al., 2006)</ref> and SciERC<ref type="foot" target="#foot_8">11</ref>  <ref type="bibr" target="#b22">(Luan et al., 2018)</ref>. The ACE05 dataset, named as ACE05-R in our paper, is collected from various domains, such as newswire and online forums. The SciERC dataset provides entity, coreference and relation annotations from AI conference/workshop proceedings. In our experiments, we only use entity and relation annotations. We follow the data preprocessing in <ref type="bibr" target="#b23">Luan et al. (2019)</ref> to split ACE05-R and SciERC into train, dev and test sets.</p><p>In typical RE, it is crucial to distinguish which entity comes first (head entity) and which comes next (tail entity). As for symmetric relational instance, the relation exists from both head-to-tail and tail-to-head directions. There are one such relation type in ACE05-R and two in SciERC. Some papers <ref type="bibr" target="#b38">(Wang et al., 2021;</ref><ref type="bibr" target="#b42">Ye et al., 2022</ref>) regard each symmetric relational example as two directed relations, while others regard them as one relation. We find that this setting will hugely influence the performance. Therefore, we name the setting of having two directed relations as Symmetric Relation Extraction and name the corresponding datasets ACE05-R + and SciERC + .</p><p>Event extraction. We evaluate UTC-IE on two widely used event extraction datasets, ACE2005 <ref type="bibr" target="#b2">(Doddington et al., 2004)</ref> and ERE <ref type="bibr" target="#b30">(Song et al., 2015)</ref>. Following the prior preprocessing step <ref type="bibr" target="#b35">(Wadden et al., 2019;</ref><ref type="bibr" target="#b17">Lin et al., 2020;</ref><ref type="bibr" target="#b20">Lu et al., 2021)</ref> on them, we obtain three datasets, ACE05-E, ACE05-E+ and ERE-EN. ACE05-E+ additionally takes relation arguments, pronouns and multi-token event triggers into consideration compared with ACE05-E. We use the same train/dev/test split as <ref type="bibr" target="#b20">Lu et al. (2021)</ref> for all datasets to ensure a fair comparison. Furthermore, we still use ACE05-E+ and ERE-EN on joint IE, for they have annotations on all IE tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Baselines</head><p>TANL <ref type="bibr" target="#b26">(Paolini et al., 2021)</ref> and UIE <ref type="bibr" target="#b21">(Lu et al., 2022)</ref> are both unified information extraction models in the generative way, with different input and output formats. TANL uses T5-base as the backbone model, while UIE uses T5-large. We compare our model with them in every IE task. For TANL, we report single-task results for our model is trained under each task. For UIE, we report results with pre-training, which have better performance. In addition to these two baselines, each task also compares with a series of recently proposed task-specific methods as follows.  CNN-IE is the baseline model we design to prove the necessity of PlusAttention. The only difference between CNN-IE and UTC-IE is the former ignores the PlusAttention in Figure <ref type="figure">2</ref>. We tune the number of CNN layers in CNN-IE from 2 to 6, and the best results are reported.</p><p>Named entity recognition. We compare our model's performance on NER with several recently proposed NER methods.</p><p>? BART-NER <ref type="bibr">(Yan et al., 2021a)</ref> formulates unified NER model as entity span sequence generation task. They use BART-large as the pre-trained model.</p><p>? W 2 NER <ref type="bibr" target="#b15">(Li et al., 2022)</ref>  ? CNN-NER <ref type="bibr" target="#b40">(Yan et al., 2022)</ref> utilizes CNN to model local spatial correlations between spans and surpass recently proposed methods on nested NER. We report results using RoBERTa-base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation extraction.</head><p>For relation extraction, we compare our model with several SOTA models.</p><p>? UniRE <ref type="bibr" target="#b38">(Wang et al., 2021)</ref> jointly extracts entities and relations using a table containing all word pairs.</p><p>? PURE (Zhong and Chen, 2021) adopts a pipeline approach to solve NER and RE independently, using distinct contextual representations for entities and relations.</p><p>? PFN <ref type="bibr">(Yan et al., 2021b)</ref> claims that some information should be shared between named entity recognition and relation extraction, while other information should be independent. They propose PFN to model two-way interaction (partition and filter) between two tasks.</p><p>? PL-Marker <ref type="bibr" target="#b42">(Ye et al., 2022)</ref> ? TEXT2EVENT <ref type="bibr" target="#b20">(Lu et al., 2021)</ref> is a sequence-to-structure model which outputs a tree-like event structure with a given input sentence. The model uses T5-large as the base model. and span offsets match a golden entity. We use "Ent." to represent entity F1 through all tables.</p><p>? Relation: a relation is correct if its type and its head and tail entities are correct, and the offsets and type of entities should also match the golden instance. We use "Rel." to represent relation F1 through all tables.</p><p>? Event trigger: a trigger is correct if its span offset and event type is correct. We use "Trig." to represent trigger F1 through all tables.</p><p>? Event argument: an argument is correct if its span offset, event type and role type all match the ground truth. We use "Arg." to represent argument F1 through all tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Hyper-Parameters</head><p>The detailed hyper-parameters used in each dataset are listed in Table <ref type="table" target="#tab_11">7</ref>. We use AdamW optimizer <ref type="bibr" target="#b19">(Loshchilov and Hutter, 2019)</ref> with weight decay 1e-2 for all datasets. Experiments are conducted five times with five different random seeds. We report the performance on test sets based on the model which achieves the best dev results in each dataset. For NER, the best results are calculated by the entity F1; for RE, the best results are calculated by the sum of entity F1 and relation F1; for EE, the best results are calculated by the best argument F1; for joint IE, the best results are calculated by the sum of relation F1 and trigger F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Complete Results</head><p>We present the complete results of UTC-IE and that without Plusformer in Table <ref type="table" target="#tab_12">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Speed Comparison</head><p>We test the speed of other models through their released code. For models, such as OneIE <ref type="bibr" target="#b17">(Lin et al., 2020)</ref>, DEGREE <ref type="bibr" target="#b9">(Hsu et al., 2022)</ref> and PL-  <ref type="bibr" target="#b42">(Ye et al., 2022)</ref>, they also released a trained model along with their code, and we used their released model to test the inference speed. For UIE <ref type="bibr" target="#b21">(Lu et al., 2022)</ref> and BS <ref type="bibr" target="#b49">(Zhu and Li, 2022)</ref>, we trained a model with their code. The speed test is conducted in one RTX 3090 GPU and the batch size is set as 32 for all models (if the model goes out of memory, we choose the largest batch size that can accommodate the GPU); the test corpus is the test set of each dataset. The speed is measured by the number of sentences in the test set divided by the number of seconds that elapsed. And each inference is repeated three times, the average speed is reported.</p><p>The speed comparison can be roughly categorized into two kinds. The first kind is the comparison with previous universal IE models, namely OneIE <ref type="bibr" target="#b17">(Lin et al., 2020)</ref> and UIE <ref type="bibr" target="#b21">(Lu et al., 2022)</ref>, and results are depicted in Table <ref type="table" target="#tab_6">3</ref>. Compared with UIE in five chosen datasets, UTC-IE is x19.7 faster and improves performance by 1.86 averagely. Besides, for the joint IE task, UTC-IE is 18.4 times faster than OneIE and improves performance by 2.72 on average. The second kind is the comparison between UTC-IE and SOTA models targeted 12 PL-Marker used a two-stage pipeline to conduct prediction. Therefore, the time is measured by the total seconds elapse to finish two stages.</p><p>for each IE task, and results are presented in Table <ref type="table" target="#tab_13">9</ref>. Compared with previous SOTA models, the average performance increments for entity F1, relation F1 and argument F1 are 0.31, 0.94 and 2.15. In the meantime, UTC-IE speeds up for x1.0, x5.5 and x101.9 averagely.</p><p>In short, using UTC-IE for IE tasks can not only substantially enhance the performance in most cases, but also significantly speed up the inference speed in almost all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Ablation Study</head><p>For ablation, we will choose two datasets for each IE task to study the effect of each component in Plusformer. We separately list the performance for span extraction (including entity extraction in NER and RE, trigger extraction in EE) in Table <ref type="table" target="#tab_14">10</ref> and relational extraction (including relation extraction in RE and argument extraction in EE) in Table <ref type="table" target="#tab_15">11</ref>. Besides, we also study how the performance varies with the change of the number of Plusformer layers in Figure <ref type="figure" target="#fig_11">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 CNN</head><p>Based on our ablations in Table <ref type="table" target="#tab_14">10</ref> and<ref type="table" target="#tab_15">Table 11</ref>, the CNN module in Plusformer contributes most to the performance enhancement. To reveal why CNN is so effective in both span extraction and relational extraction, we first present an intuitive example in Figure <ref type="figure" target="#fig_3">3</ref> to show how CNN helps to extract entities and relations in the RE task. Like in Figure <ref type="figure" target="#fig_3">3</ref>(a), for NER, the entity e 2 can interact with entity e 1 and relation r 1 through CNN. Besides, for RE, CNN can contribute in two ways. On the one hand, CNN helps the relational token pair to directly gather from its constituent entities, like the r 1 in Figure <ref type="figure" target="#fig_3">3</ref>(b). On the other hand, the start-to-start and end-to-end relational token pairs, like two r 2 cells, can directly interact with each other through CNN.</p><formula xml:id="formula_12">? ! David Perkins ? " village ? ! ORG-AFF ? # doctor CNN Receptive Field ? " PER-JOB ? ! ? " ? ! ? " ? " ? ! ? %</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>David Perkins is a village doctor in</head><p>To quantitatively present the effectiveness of CNN in UTC-IE, we propose further ablations to show how the distance between the relational token pair and its constituent spans affects the relational F1, and how the distance between start-to-start and end-to-end token pairs affects the relational F1. Furthermore, we conduct experiments on UTC-IE with different kernel sizes and choose the most proper size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.1 Distance Between the Relational Token</head><p>Pair and Its Constituent spans VS. Relational F1</p><p>In this section, we will show how the relational F1 (relation F1 in RE and argument F1 in EE) will change when the distance between the relational token pair and its constituent spans varies. For two spans (s 1 , e 1 ) and (s 2 , e 2 ) (we ignore their diagonally symmetric counterparts, since they will not affect the calculation here), the span relation from (s 1 , e 1 ) to (s 2 , e 2 ) is represented by two token pairs (s 1 , s 2 ) and (e 1 , e 2 ). The distance between the two token pairs and its constituent spans is calculated as follows</p><formula xml:id="formula_13">d = max(|s 2 -e 1 |, |s 1 -e 2 |) + 1,<label>(9)</label></formula><p>where the distance d is named as "Span-Rel-Span Distance", it represents the longest distance between the relational token pairs to their constituent spans. The relation between d and the relational F1 is shown in Figure <ref type="figure" target="#fig_4">4</ref>. Without CNN, the performance for extracting relations between nearby constituent spans will drop sightly, while less affected for further ones, which proves that CNN is effective for exploiting local dependency to predict relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.2 Distance Between Start-to-Start and</head><p>End-to-End Token Pairs VS. Relational F1</p><p>As shown in Figure <ref type="figure" target="#fig_3">3</ref>(b), if the distance between the start-to-start and end-to-end relational token pairs is small, the CNN should be helpful. To verify this assumption, we first define the "Inner Relational Distance" as follows, for two spans (s 1 , e 1 ) and (s 2 , e 2 ), the relational token pairs are (s 1 , s 2 ) and (e 1 , e 2 ), then the distance between two relational token pairs is calculated as follows</p><formula xml:id="formula_14">d = max(e 1 -s 1 , e 2 -s 2 ) + 1,<label>(10)</label></formula><p>where d reveals the distance between start-to-start and end-to-end token pairs, and it is actually decided by the max constituent span length. And its relation with the relational F1 is shown in Figure <ref type="figure" target="#fig_5">5</ref>. It is clear that, most of the start-to-start token pairs are near to their end-to-end token pairs, and CNN takes advantage of this adjacency to make better predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.3 CNN Kernel Size VS. F1</head><p>We study the relation between the kernel size of CNN and F1 performance in Figure <ref type="figure" target="#fig_6">6</ref>. We observe that CNN with kernel size 3 obtains the best performance on almost all datasets and tasks. Specifically, From the results, it is clear that without CNN, the performance of Plusformer will drop when extracting relations (relation for RE and argument for EE) between nearby spans, while the performance is less effected for relations with further constituent spans. We conjecture this is because the receptive field of CNN is limited to a relatively small distance.</p><p>reducing CNN kernel size to 1 significantly harms the performance on all datasets, for CNN will lose the capability of interacting with neighboring token pairs. In contrast, F1 also slightly decreases with larger CNN kernel size. We presume that CNN with a larger kernel size may introduce more noise and harm performance. Therefore, we choose kernel size 3 for all datasets.</p><p>F.2 Is CNN All We Need?</p><p>Since CNN is so effective in the Plusformer, it is natural to ask whether it is enough only to use CNN. Therefore, we conduct experiments on models without the plus-shaped self-attention and named this model CNN-IE. We conduct experiments for CNN-IE in six datasets, and results are listed in Table <ref type="table" target="#tab_14">10</ref> and Table <ref type="table" target="#tab_15">11</ref>. With only the CNN module, the model can achieve SOTA or near SOTA performance in all six datasets, which depicts the effectiveness of the proposed token-pair decomposition and CNN module. However, it still lags behind the UTC-IE model, which reveals the necessity of the PlusAttention.</p><p>It is worth noting that CNN-IE is different from CNN-NER <ref type="bibr" target="#b40">(Yan et al., 2022)</ref>. As for model structures, CNN-IE is one of our baseline models and reserves the general framework of Plusformer, namely self-attention with CNN layers. However, CNN-NER only uses residual CNN layers. As we can see in Table <ref type="table" target="#tab_4">1</ref>, CNN-IE has different results on ACE05-Ent than CNN-NER does. Besides, as for tasks, CNN-NER only formulates the nested NER task and can not transfer to other IE tasks directly. However, our CNN-IE can easily apply to NER, RE and EE. In each IE task on CNN-IE, CNN modules have their specific functions to capture different neighboring token pairs, as depicted in Figure <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Position Embeddings</head><p>The RoPE embedding aims to help token pairs be aware of the spatial relationships between each other, and the triangle position embedding tries to enable spans to be informed of their areas in the feature map. From Table <ref type="table" target="#tab_14">10</ref> and<ref type="table" target="#tab_15">Table 11,</ref>  F.4 Axis-aware Plus-shaped Self-Attention Lastly, we study the effect of PlusAttention. We present an example to delineate why the axis-aware is valuable for span extraction and relational extraction in Figure <ref type="figure" target="#fig_9">8</ref>. From Figure <ref type="figure" target="#fig_9">8</ref>, axis-aware should be worthwhile no matter what the task is, span extraction or relational extraction. As expected, from Table <ref type="table" target="#tab_14">10</ref> and Table <ref type="table" target="#tab_15">11</ref>, if we discard the axis-aware in Plusformer, the average performance of span extraction and relational extraction diminish 0.28 and 0.86, respectively, which reveals the necessity of axis-aware in the PlusAttention module.</p><p>Besides, we show two case studies of the plusshaped attention in Figure <ref type="figure" target="#fig_10">9</ref>. The sentences are from the test dataset of ACE2005-Ent and ACE2005-R. Both cases put larger attention scores on informative token pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Number of Plusformer Vs. F1</head><p>We study the relation between the number of Plusformer layers and F1 performance in Figure <ref type="figure" target="#fig_11">10</ref>. For the NER datasets, we use two layers of Plusformer, and for the RE and EE we use three. <ref type="bibr" target="#b11">Jiang et al. (2020)</ref> claims that many NLP tasks can be regarded as the span prediction and prediction of relations between pairs of spans (named as span extraction and relational extraction in our paper), which is conceptually similar to our insights. However, our work is fundamentally distinct from theirs on both formulation and model architecture. <ref type="bibr" target="#b11">Jiang et al. (2020)</ref> classify various NLP tasks into two separate tasks and design different modules for them. To contrast, we unify all traditional IE tasks into a single formulation, namely token-pair classification. Therefore, we only need one model for all tasks. Besides, <ref type="bibr" target="#b11">Jiang et al. (2020)</ref> simply use the concatenation of the start and end token representations to represent a span, and for relations, they concatenate the head and tail span representations. Therefore, in their work, the interaction between spans are weak. In our work, we obtain the feature matrix of all token pairs and add well-designed Plusformer module on top of all token pairs, where token pairs can interact with others thoroughly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Comparison with GLAD</head><p>In order to prove the superiority of our reformulation and UTC-IE model, we make a fair comparison on several tasks from the GLAD bench- <ref type="bibr" target="#b11">(Jiang et al., 2020)</ref>. We choose 3 additional IE tasks, including Open Information Extraction (OIE), Semantic Role Labeling (SRL) and Aspect Based Sentiment Analysis (ABSA), and NER and RE. We use WLP <ref type="bibr" target="#b7">(Hashimoto et al., 2017)</ref>   <ref type="table" target="#tab_16">12</ref>.</p><p>The table shows that UTC-IE outperforms GLAD on all chosen tasks exceedingly, with +2.76 improvement on average. Moreover, we observe that UTC-IE without Plusformer also surpasses GLAD benchmarks on all tasks with +0.84 improvement on average, which proves the superiority of our unified reformulation.  In the left figure, spans in e's vertical direction share the same end token as e except for spans in the lower triangle, since they clash with e in the back (because "Airway" is the end token of e but the start token for these spans); spans in e's horizontal direction have common start token as e, but not spans in the lower triangle, because they clash with e in the front (since "US" is the start token of e but the end token for these spans). Therefore, both the axis-aware and triangle position embedding are crucial for spans to figure out their relationships with each other. In the right figure, for a relational token pair, the spans from its horizontal direction must be the head span, while the tail span must come vertically. Thusly, axis-aware is informative for relational extractions.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An illustration of the token-pair decomposition for IE tasks. Each cell represents one token pair, and it can be classified into pre-defined types. e, r, t, a and rol in figures mean entity, relation, event trigger, event argument and event role. For the span extraction, we use the start-to-end and end-to-start token pairs to pinpoint the span, such as entity spans e 1 , e 2 , argument spans a 1 , a 2 and trigger span t (cells with pure color). For the relational extraction, we use the start-to-start and end-to-end token pairs to represent the relation, such as r and rol 1 , rol 2 (cells with gradient color). It is worth mentioning that both relations and event roles are regarded as directional, namely from start entity to end entity and from event trigger to argument spans. Therefore, all IE tasks can be decomposed into token-pair classifications. After the reformulation, the local dependency and interaction from the plus-shaped orientation (as the orange and blue dotted lines depict) can provide vital information to classify the central token pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 Figure 2 :</head><label>22</label><figDesc>Figure2shows an overview of the architecture. Firstly, we present Biaffine<ref type="bibr" target="#b3">(Dozat and Manning, 2017)</ref> model based on PLMs. Then, we propose a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>formulates unified NER model as word-to-word classification task. The model employs BioBERT on GE-NIA and BERT-large on other datasets. ? BS (Zhu and Li, 2022): authors use spanbased NER model as baseline and propose boundary smoothing as a regularization technique to improve model performance. It leverages RoBERTa-base as the base encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An intuitive example of the influence of CNN on span extraction and relation extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distance between the relational token pair and its constituent spans (Span-Rel-Span Distance) VS. relational F1 when with or without CNN in Plusformer. The upper and lower figures are for RE and EE tasks, respectively.From the results, it is clear that without CNN, the performance of Plusformer will drop when extracting relations (relation for RE and argument for EE) between nearby spans, while the performance is less effected for relations with further constituent spans. We conjecture this is because the receptive field of CNN is limited to a relatively small distance.</figDesc><graphic url="image-3.png" coords="19,78.57,211.95,217.71,124.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distance between two relational token pairs of the same span pair (Inner Relational Distance) VS. relational F1 when with or without CNN in Plusformer. The upper and lower figures are for RE and EE tasks, respectively. Since almost all spans are of a length of less than 5, CNN is valuable to model the interaction between start-to-start and end-to-end relational pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The performance varies with the kernel size of CNN. NER, RE and EE results are listed from top to bottom. CNN with kernel size 3 has the best performance over almost all datasets.</figDesc><graphic url="image-13.png" coords="21,78.57,261.97,217.71,77.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>on NER and RE, OIE2016 (Stanovsky and Dagan, 2016) on OIE, OntoNotes (Pradhan et al., 2013) on SRL and SemEval14 (Pontiki et al., 2014) on ABSA. The detailed experimental settings are the same as those in GLAD, to ensure a fair comparison. Results are present in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Distance between the relational token pair and its constituent spans (Span-Rel-Span Distance) VS. relational F1 when with or without position embeddings in Plusformer. The upper and lower figures are for RE and EE tasks, respectively. Without position embeddings, the relational performance is lower almost in all "Span-Rel-Span" distances. We presume this is because, with position embedding, Plusformer can exploit the distance inductive bias to determine the relations.</figDesc><graphic url="image-17.png" coords="22,78.57,222.36,217.71,124.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Examples to show why axis-aware is meaningful for IE tasks. In the left figure, spans in e's vertical direction share the same end token as e except for spans in the lower triangle, since they clash with e in the back (because "Airway" is the end token of e but the start token for these spans); spans in e's horizontal direction have common start token as e, but not spans in the lower triangle, because they clash with e in the front (since "US" is the start token of e but the end token for these spans). Therefore, both the axis-aware and triangle position embedding are crucial for spans to figure out their relationships with each other. In the right figure, for a relational token pair, the spans from its horizontal direction must be the head span, while the tail span must come vertically. Thusly, axis-aware is informative for relational extractions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Two case studies of the PlusAttention. The horizontal and vertical attention scores are from the horizontal and vertical self-attentions of last layer of Plusformer. The center cells are with two colors, one for the horizontal attention scores and the other for the vertical attention scores. For NER, the center cell attends more on other entities. And for RE, the center relational cell attends more on its constituent entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The performance varies with the number of Plusformer layers. NER, RE and EE results are listed from top to bottom. For the NER tasks, the performance peaks at the two layers of Plusformer, and for RE and EE, the performance plateaus after three layers of Plusformer.</figDesc><graphic url="image-30.png" coords="23,78.57,598.38,217.71,77.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>In this way, we can unify all single IE tasks under the same task formulation, and use one model to fit all IE tasks without designing taskspecific modules. Besides, this unified decomposition is much faster than recently proposed generation-based unified frameworks.</figDesc><table><row><cell>2. After the reformulation of different IE tasks,</cell></row><row><cell>we propose the Plusformer to model interac-</cell></row><row><cell>tion between different token pairs. The plus-</cell></row><row><cell>shaped self-attention and CNN in Plusformer</cell></row><row><cell>are well-motivated and effective in the refor-</cell></row><row><cell>mulated IE scenario. Experiments in 12 IE</cell></row><row><cell>datasets all achieve state-of-the-art (SOTA)</cell></row><row><cell>performance which justifies the superiority of</cell></row><row><cell>Plusformer in IE tasks.</cell></row></table><note><p><p><p>3. The reformulation enables us to use one model</p>to fit all IE tasks concurrently. Therefore, we can train one model on three IE tasks, and results on two joint IE datasets show that the proposed unification can effectively benefit each IE task through multi-task learning.</p>4. Extensive ablation experiments reveal that components in Plusformer are necessary and beneficial. Among them, CNN module in Plusformer can be essential to the overall performance. Analysis shows that this performance gain is well-explained because when reformulating IE tasks into token-pair classifications, the adjacent token pairs can be informative and CNN can take good advantage of the local dependency between them.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>|R|] with another span (s 2 , e 2 ), then Y (s 1 ,s 2 ,r) = Y (e 1 ,e 2 ,r) = 1.NER aims to extract all entities {(s i , e i , t i )}, where t i ? S e and S e is pre-defined entity types. Therefore, in NER, S = S e and R = ?. , r i ? R r and S e , R r are pre-defined entity types, and relation types. Therefore, in RE, S = S e and R = R r . , e i ) means the trigger span, t i ? S t is the event type, S t is pre-defined event types; s ia , e ia ? [1, L] are the start and end token indices of an argument span, k is the number of arguments of the trigger. To extract argument spans, we identify argument span into S a which binarily denotes "has argument / no argument", thus |S a | = 1; rol i ? R o is the role type of the argument and R o is pre-defined role types. Following the formulation in RE, we can view role types from the trigger to the arguments as relations. Therefore, in EE, S = S t ? S a and R = R o .</figDesc><table><row><cell>RE</cell><cell>aims</cell><cell>to</cell><cell>extract</cell><cell>all</cell><cell>relations</cell></row><row><cell cols="6">{((s h i , e h i , t h i ), r i , (s t i , e t i , t t i ))}, where the su-perscript h and t denotes the head and tail entities,</cell></row><row><cell cols="2">t h i , t t i ? S EE aims</cell><cell>to</cell><cell>extract</cell><cell>all</cell><cell>events</cell></row><row><cell cols="6">{{(s i , e i , t i ), (s 1 ia , e 1 ia , rol 1 i ), . . . , (s k ia , e k ia , rol k i )}}, where (s i</cell></row></table><note><p>? When a span (s, e) is of type t, then Y (s,e,t) = Y (e,s,t) = 1, where s, e ? [1, L] and t ? [1, |S|] are the start, end token indices and span type; ? When the span (s 1 , e 1 ) forms the relation r ? [|S| + 1, |S| + e</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Overall F1 on single IE tasks. Results of UTC-IE are the average of 5 runs, and the subscript means the standard deviation (e.g., 93.45 24 means 93.45?0.24). Datasets marked as * have nested entities. Results marked as ? are from<ref type="bibr" target="#b40">Yan et al. (2022)</ref>. ? means results from their Github repo or our reproduction. ? means that the UTC-IE without Plusformer surpasses previous SOTA performance.</figDesc><table><row><cell>Named Entity Recognition</cell><cell cols="2">CoNLL03</cell><cell></cell><cell cols="2">OntoNotes</cell><cell cols="2">ACE04*</cell><cell>ACE05-Ent*</cell><cell>GENIA*</cell></row><row><cell>BART-NER (Yan et al., 2021a) TANL (Paolini et al., 2021) W 2 NER (Li et al., 2022) UIE (Lu et al., 2022) BS (Zhu and Li, 2022) CNN-NER (Yan et al., 2022) UTC-IE -Plusformer</cell><cell cols="2">93.24 91.7 93.07 92.99 93.39 ? 9 -93.4524 92.9813</cell><cell></cell><cell>90.38 89.8 90.50 -91.51 ? 7 -91.775 91.375</cell><cell></cell><cell>86.84 -87.43  ? 86.89 87.08  ? 87.31  ? 87.5433 86.5123</cell><cell></cell><cell>84.74 84.9 86.77  ? 85.78 87.20  ? 87.42  ? 87.7535 86.5920</cell><cell>78.93 76.4 80.32  ? --80.33  ? 80.4522 79.3417</cell></row><row><cell>Relation Extraction</cell><cell cols="4">ACE05-R bert Ent. Rel.</cell><cell cols="3">ACE05-R albert Ent. Rel.</cell><cell>Ent.</cell><cell>SciERC</cell><cell>Rel.</cell></row><row><cell>TANL (Paolini et al., 2021) PURE (Zhong and Chen, 2021) PFN (Yan et al., 2021b) UIE (Lu et al., 2022) UTC-IE -Plusformer</cell><cell>-88.7 --88.8212 88.5019</cell><cell></cell><cell cols="2">-63.9 --64.9433 63.3472</cell><cell cols="2">88.9 89.7 89.0 -89.8715 89.80 ? 23</cell><cell>63.7 65.6 66.8 66.06 67.7945 66.2187</cell><cell>-66.6 67.2 ? 67 -69.0345 68.05 ? 63</cell><cell>-35.6 37.6 ? 99 36.53 38.7796 37.1240</cell></row><row><cell>Symmetric Relation Extraction</cell><cell cols="2">Ent.</cell><cell cols="2">ACE05-R +</cell><cell>Rel.</cell><cell></cell><cell cols="2">Ent.</cell><cell>SciERC +</cell><cell>Rel.</cell></row><row><cell>UniRE (Wang et al., 2021) PL-Marker (Ye et al., 2022) UTC-IE -Plusformer</cell><cell cols="2">88.8 89.8 90.1621 88.9829</cell><cell></cell><cell cols="2">64.3 66.5 67.4774 64.5865</cell><cell></cell><cell cols="2">68.4 69.9 69.9541 68.7863</cell><cell>36.9 41.6 42.5142 39.5156</cell></row><row><cell>Event Extraction</cell><cell>Trig.</cell><cell cols="2">ACE05-E</cell><cell>Arg.</cell><cell cols="3">ACE05-E+ Trig. Arg.</cell><cell>Trig.</cell><cell>ERE-EN</cell><cell>Arg.</cell></row><row><cell>TANL (Paolini et al., 2021) TEXT2EVENT (Lu et al., 2021) UIE (Lu et al., 2022) DEGREE (Hsu et al., 2022) UTC-IE -Plusformer</cell><cell cols="2">68.4 71.9 -73.3 73.4699 72.8878</cell><cell cols="2">47.6 53.8 -55.8 56.5153 55.4199</cell><cell cols="2">-71.8 73.36 70.9 73.4455 72.9294</cell><cell cols="2">-54.4 54.79 56.3 57.6878 56.63 ? 89</cell><cell>-59.4 -57.1 60.2094 59.2877</cell><cell>-48.3 -49.6 52.5195 51.33 ? 99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results on joint IE. UTC-IE single shows results by separately trained model on NER, RE and EE, while UTC-IE joint shows results by jointly trained model. ? means that UTC-IE without Plusformer surpasses previous SOTA performance.</figDesc><table><row><cell>Joint IE</cell><cell>Ent.</cell><cell cols="2">ACE05-E+ Rel. Trig.</cell><cell>Arg.</cell></row><row><cell cols="5">OneIE (2020) FourIE (2021) UTC-IE single 91.3710 65.0049 71.9865 56.0176 89.6 58.6 72.8 54.8 91.1 63.6 73.3 57.5 UTC-IEjoint 91.4820 65.5490 73.6347 57.6230 -Plusformer 90.7230 62.9475 72.9962 55.6874</cell></row><row><cell></cell><cell></cell><cell cols="2">ERE-EN</cell></row><row><cell></cell><cell>Ent.</cell><cell>Rel.</cell><cell>Trig.</cell><cell>Arg.</cell></row><row><cell cols="5">OneIE (2020) FourIE (2021) UTC-IE single 86.3548 55.5792 57.0139 48.2960 87.0 53.2 57.0 46.5 87.4 56.1 57.9 48.6 UTC-IEjoint 87.3018 56.9290 57.8898 50.9193 -Plusformer 86.9413 54.2894 57.7983 48.72 ? 51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The F1 and efficiency comparison with UIE and OneIE. "Ent.", "Rel." and "Arg." denote F1 of corresponding test sets. "Speed" is measured in "sentence/s" on inference procedure. The improvement shows the changes in performance and speed. We use ALBERT as encoder for ACE05-R.</figDesc><table><row><cell>Single IE</cell><cell cols="4">ACE05-Ent ACE05-R ACE05-E+ Ent. Speed Rel. Speed Arg. Speed</cell></row><row><cell>UIE (2022) UTC-IE</cell><cell cols="4">85.78 8.6 66.06 11.4 54.79 4.0 87.75 304.3 67.79 85.4 57.68 88.1</cell></row><row><cell>Improvement</cell><cell cols="4">+1.97 x35.4 +1.73 x7.5 +2.89 x22.0</cell></row><row><cell>Joint IE</cell><cell>Ent.</cell><cell>Rel.</cell><cell>ACE05-E+ Trig.</cell><cell>Arg. Speed</cell></row><row><cell>OneIE (2020) UTC-IE</cell><cell cols="4">89.6 91.48 65.54 73.63 57.62 121.6 58.6 72.8 54.8 4.8</cell></row><row><cell>Improvement</cell><cell cols="4">+1.88 +6.94 +0.83 +2.82 x25.3</cell></row></table><note><p>other variant of Plusformer where the PlusAttention is discarded, and we name this variant CNN-IE.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies in the NER, RE and EE datasets. CNN-IE is similar to UTC-IE except that it is deprived of the PlusAttention. Underlines mean the most dropped factor. ? means that the CNN-IE surpasses previous SOTA performance.</figDesc><table><row><cell></cell><cell>ACE05-Ent</cell><cell cols="2">ACE05-R bert</cell><cell>ACE05-E+</cell><cell></cell></row><row><cell></cell><cell>Ent.</cell><cell>Ent.</cell><cell>Rel.</cell><cell>Trig.</cell><cell>Arg.</cell></row><row><cell>UTC-IE -CNN -positon embeddings -axis-aware CNN-IE</cell><cell>87.7535 87.3922 87.5334 87.5927 87.45 ? 20</cell><cell>88.8212 88.7122 88.7320 88.7919 88.70 ? 16</cell><cell>64.9433 63.5583 64.2956 63.9155 64.67 ? 26</cell><cell>73.4455 72.9834 73.1298 73.2946 73.0499</cell><cell>57.6878 56.7499 57.0280 56.8798 56.97 ? 63</cell></row><row><cell cols="3">extract entities, relations and events by leverag-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ing span representations via span graph updates.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Lin et al. (2020) and Nguyen et al. (2021) extend</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DYGIE++ by incorporating global features to ex-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tract cross-task and cross-instance interactions with</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">multi-task learning. In addition to the graph-based</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">models mentioned above, other studies focus on</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tackling general IE by generative models. Paolini</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">et al. (2021) construct a framework called TANL,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">which enhances the generation model using aug-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">mented language methods. Moreover, Lu et al.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>(2022)  </p>regard IE task as a text-to-structure generation task, and leveraging prompt mechanism.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Datasets statistics. "#Types" denotes the number of classes. Note that "#Types" in the last column mean (#event types; #role type) pairs. Every block represents datasets of different tasks, which are flat NER, nested NER, RE and EE from top to bottom. For the joint IE setting, the "ACE05-E+" and "ERE-EN" are used. In the RE block, datasets following + mean that each symmetric relational instance is regarded as two directional instances, leading to more relations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>The hyper-parameters used in each dataset.</figDesc><table><row><cell>Named Entity Recognition</cell><cell>CoNLL03</cell><cell>OntoNotes</cell><cell>ACE04*</cell><cell>ACE05-Ent*</cell><cell>GENIA*</cell></row><row><cell># Epochs Learning Rate Batch Size # Plusformer Layers Biaffine Dimension d Feature Dimension c Warmup Ratio</cell><cell>30 1e-5 12 2 200 32 0.1</cell><cell>10 1e-5 12 1 200 100 0.1</cell><cell>50 2e-5 48 2 200 100 0.2</cell><cell>50 2e-5 48 2 200 100 0.2</cell><cell>5 7e-6 8 2 200 100 0.1</cell></row><row><cell>Relation Extraction</cell><cell>ACE05-R bert</cell><cell>ACE05-R albert</cell><cell>SciERC</cell><cell>ACE05-R +</cell><cell>SciERC +</cell></row><row><cell># Epochs Learning Rate Batch Size # Plusformer Layers Biaffine Dimension d Feature Dimension c Warmup Ratio</cell><cell>100 3e-5 32 3 200 200 0.1</cell><cell>100 3e-5 32 3 200 200 0.1</cell><cell>70 3e-5 16 3 200 200 0.1</cell><cell>50 3e-5 32 3 200 200 0.1</cell><cell>100 3e-5 16 3 200 200 0.1</cell></row><row><cell>Event Extraction</cell><cell cols="2">ACE05-E</cell><cell>ACE05-E+</cell><cell cols="2">ERE-EN</cell></row><row><cell># Epochs Learning Rate Batch Size # Plusformer Layers Biaffine Dimension d Feature Dimension c Warmup Ratio</cell><cell>70 1e-5 32 3 300 150 0.1</cell><cell></cell><cell>70 1e-5 32 3 300 150 0.1</cell><cell></cell><cell>70 1e-5 32 3 300 150 0.1</cell></row><row><cell>Joint IE</cell><cell cols="2">ACE05-E+</cell><cell></cell><cell>ERE-EN</cell><cell></cell></row><row><cell># Epochs Learning Rate Batch Size # Plusformer Layers Biaffine Dimension d Feature Dimension c Warmup Ratio</cell><cell></cell><cell>70 1e-5 12 3 300 150 0.1</cell><cell></cell><cell>30 3e-5 12 3 300 150 0.1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Completed results for precision (P), recall (R) and F1 (F) of UTC-IE on different tasks. Bold results represent the most improved metrics on UTC-IE without Plusformer between precision and recall.</figDesc><table><row><cell>Named Entity Extraction</cell><cell>P</cell><cell cols="2">CoNLL03 R</cell><cell>F</cell><cell cols="2">P</cell><cell cols="2">OntoNotes R</cell><cell>F</cell><cell>P</cell><cell cols="2">ACE04* R</cell><cell>F</cell><cell cols="3">ACE05-Ent* P R F</cell><cell>P</cell><cell>GENIA* R</cell><cell>F</cell></row><row><cell cols="19">UTC-IE -Plusformer 93.0 93.0 93.0 91.0 91.8 91.4 86.8 86.2 86.5 85.8 87.4 86.6 81.6 77.2 79.3 93.4 93.6 93.5 91.7 91.9 91.8 87.3 87.7 87.5 86.8 88.8 87.8 81.6 79.4 80.5</cell></row><row><cell>Relation Extraction</cell><cell></cell><cell>Ent.</cell><cell cols="3">ACE05-R bert</cell><cell cols="2">Rel.</cell><cell></cell><cell></cell><cell cols="4">ACE05-R albert Ent. Rel.</cell><cell></cell><cell></cell><cell>Ent.</cell><cell cols="2">SciERC</cell><cell>Rel.</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell></cell><cell></cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell cols="19">UTC-IE -Plusformer 88.1 88.9 88.5 66.7 58.4 63.3 89.7 90.0 89.8 70.2 62.7 66.2 67.4 68.9 68.1 43.0 32.7 37.1 88.7 89.0 88.8 70.1 60.5 64.9 89.3 90.5 89.9 70.4 65.4 67.8 68.0 70.1 69.0 43.6 34.9 38.8</cell></row><row><cell>Symmetric Relation Extraction</cell><cell>P</cell><cell></cell><cell>Ent. R</cell><cell cols="5">ACE05-R + F P</cell><cell>Rel. R</cell><cell></cell><cell>F</cell><cell>P</cell><cell></cell><cell>Ent. R</cell><cell cols="3">SciERC + F P</cell><cell>Rel. R</cell><cell>F</cell></row><row><cell cols="3">UTC-IE -Plusformer 87.5 90.0</cell><cell>90.5 90.5</cell><cell cols="2">90.2 89.0</cell><cell></cell><cell></cell><cell>69.3 67.3</cell><cell>65.8 64.0</cell><cell cols="2">67.5 64.6</cell><cell cols="2">68.5 68.0</cell><cell>71.5 69.6</cell><cell>70.0 68.8</cell><cell cols="2">45.7 43.5</cell><cell>39.8 36.2</cell><cell>42.5 39.5</cell></row><row><cell>Event Extraction</cell><cell></cell><cell>Ent.</cell><cell cols="2">ACE05-E</cell><cell></cell><cell cols="2">Rel.</cell><cell></cell><cell></cell><cell>Ent.</cell><cell cols="2">ACE05-E+</cell><cell>Rel.</cell><cell></cell><cell></cell><cell>Ent.</cell><cell cols="2">ERE-EN</cell><cell>Rel.</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell></cell><cell></cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell cols="19">UTC-IE -Plusformer 70.1 76.0 72.9 52.5 58.7 55.4 70.5 75.5 72.9 55.6 57.7 56.6 56.0 63.0 59.3 52.3 50.3 51.3 70.9 76.2 73.5 55.5 57.6 56.5 70.8 76.1 73.4 57.8 57.6 57.7 58.1 62.5 60.2 54.5 50.7 52.5</cell></row><row><cell>Marker 12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>The F1 and inference time comparison on UTC-IE and currently SOTA models on each IE task. "Ent.", "Rel." and "Arg." denote F1 of corresponding test sets. "Speed" is measured in "sentence/s" on inference procedure. Improvement shows the changes in performance and speed.</figDesc><table><row><cell>NER</cell><cell cols="2">CoNLL03 Ent. Speed</cell><cell cols="2">ACE05-Ent Ent. Speed</cell></row><row><cell>BS (2022) UTC-IE</cell><cell>93.39 93.45</cell><cell>265.6 285.3</cell><cell>87.20 87.75</cell><cell>355.4 344.3</cell></row><row><cell>Improvement</cell><cell>+0.06</cell><cell>x1.1</cell><cell>+0.55</cell><cell>x1.0</cell></row><row><cell>RE</cell><cell cols="2">ACE05-R albert Rel. Speed</cell><cell cols="2">SciERC Rel. Speed</cell></row><row><cell>UIE (2022) UTC-IE</cell><cell>66.06 67.79</cell><cell>11.4 85.4</cell><cell>36.53 38.77</cell><cell>8.7 165.7</cell></row><row><cell>Improvement</cell><cell>+1.73</cell><cell>x7.5</cell><cell>+2.24</cell><cell>x19.0</cell></row><row><cell>Symmetric RE</cell><cell cols="2">ACE05-R + Rel. Speed</cell><cell cols="2">SciERC + Rel. Speed</cell></row><row><cell cols="2">PL-Marker (2022) 66.5 UTC-IE 67.47</cell><cell>30.1 173.8</cell><cell>41.6 42.51</cell><cell>26.0 134.7</cell></row><row><cell>Improvement</cell><cell>+0.97</cell><cell>x5.8</cell><cell>+0.91</cell><cell>x5.2</cell></row><row><cell>EE</cell><cell cols="2">ACE05-E+ Arg. Speed</cell><cell cols="2">ERE-EN Arg. Speed</cell></row><row><cell>DEGREE (2022) UTC-IE</cell><cell>56.3 57.68</cell><cell>0.8 88.1</cell><cell>49.6 52.51</cell><cell>1.2 114.6</cell></row><row><cell>Improvement</cell><cell>+1.38</cell><cell>x107.4</cell><cell>+2.91</cell><cell>x96.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Ablation Study for span extraction. Underlines mean the most dropped factor. ? means the CNN-IE surpasses previous SOTA performance.</figDesc><table><row><cell></cell><cell>CoNLL03 Ent.</cell><cell cols="2">ACE05-Ent ACE05-R bert Ent. Ent.</cell><cell>SciERC Ent.</cell><cell>ACE05-E+ Trig.</cell><cell>ERE-EN Trig.</cell></row><row><cell>UTC-IE -CNN -position embeddings -axis-aware CNN-IE</cell><cell>93.4524 93.1011 93.2511 93.2310 93.3216</cell><cell>87.7535 87.3922 87.5334 87.5927 87.45 ? 20</cell><cell>88.8212 88.7122 88.7320 88.7919 88.70 ? 16</cell><cell>69.0345 68.3556 68.6958 68.5348 68.11 ? 71</cell><cell>73.4455 72.9834 73.1298 73.2946 73.0499</cell><cell>60.2094 58.9131 59.0370 59.5699 59.47 ? 63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Ablation Study for relational extraction. Underlines mean the most dropped factor. ? means that the CNN-IE surpasses previous SOTA performance.</figDesc><table><row><cell></cell><cell>ACE05-R bert Rel.</cell><cell>SciERC Rel.</cell><cell>ACE05-E+ Arg.</cell><cell>ERE-EN Arg.</cell></row><row><cell>UTC-IE -CNN -position embeddings -axis-aware CNN-IE</cell><cell>64.9433 63.5583 64.2956 63.9155 64.67 ? 26</cell><cell>38.7796 37.5683 37.9899 37.7683 37.64 ? 65</cell><cell>57.6878 56.7499 57.0280 56.8798 56.97 ? 63</cell><cell>52.5195 51.5999 52.0670 51.9269 51.78 ? 50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Results comparison between a similar work GLAD and UTC-IE on NER, RE, SRL and ABSA. We leverage BERT-base as base model for fair comparison. GLAD performs NER and RE jointly on WLP dataset, and report them separately. We use the same settings as theirs. ? means that the UTC-IE without Plusformer surpasses previous SOTA performance.</figDesc><table><row><cell></cell><cell>NER WLP</cell><cell>RE WLP</cell><cell>OIE OIE2016</cell><cell>SRL OntoNotes</cell><cell>ABSA SemEval14</cell></row><row><cell>GLAD (Jiang et al., 2020) UTC-IE -Plusformer</cell><cell>78.1 82.51?31 79.47 ? ?42</cell><cell>64.7 68.57?54 66.07 ? ?37</cell><cell>36.7 37.9077 36.73 ? ?79</cell><cell>83.3 84.90?39 83.75 ? ?35</cell><cell>70.8 73.53?45 71.80 ? ?52</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>In this paper, we use relational extraction to encompass the extraction of any kind of relationship or other interaction between spans, which as broader meanings than relation extraction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Removing axis-aware means using the same self-attention parameters for both directions and adding Z h and Z v instead of concatenation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://github.com/fastnlp/fastNLP. FastNLP is a natural language processing python package.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://github.com/fastnlp/fitlog. Fitlog is an experiment tracking package.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>All IE tasks forbid span boundary clashes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>https://catalog.ldc.upenn.edu/LDC2013T19</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>https://catalog.ldc.upenn.edu/LDC2005T09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>https://catalog.ldc.upenn.edu/LDC2006T06</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>http://nlp.cs.washington.edu/sciIE/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their insightful comments. We also thank the developers of fastNLP 5 and fitlog 6 . Thank <rs type="person">Yuntao Chen</rs> for helping us preparing the code for publishing. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62236004</rs> and No. <rs type="grantNumber">62022027</rs>) and <rs type="funder">CCF-Baidu Open Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_k6xp2Z5">
					<idno type="grant-number">62236004</idno>
				</org>
				<org type="funding" xml:id="_W8pwcrx">
					<idno type="grant-number">62022027</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">6</ref>: Overall pre-trained model on all IE baselines. Abbreviations before "-" denote pre-trained model names. Specifically, "BA" means BART, "BE" means BERT, "RoB" means RoBERTa, "ALB" means ALBERT, "DeB" means DeBERTa. The letters after "-" means the size of the model, such as base model ("b"), large model ("l"), xx-large model <ref type="bibr">("xxl")</ref>. The number of parameters of each pre-trained model is as follows: BE-b (110M), BE-l (340M), RoB-b (125M), ALB-xxl (233M), DeB-l (390M), T5-b (220M), T5-l (770M), BA-l (406M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recognition</head><p>CoNLL03 OntoNotes ACE04* ACE05-Ent* GENIA* BART-NER <ref type="bibr">(Yan et al., 2021a)</ref> BA-l BA-l BA-l BA-l BA-l TANL <ref type="bibr" target="#b26">(Paolini et al., 2021)</ref> T5 <ref type="bibr" target="#b15">(Li et al., 2022)</ref> BE-l BE-l BE-l BE-l BioBERT UIE <ref type="bibr" target="#b21">(Lu et al., 2022)</ref> T5-l -T5-l T5-l -BS <ref type="bibr" target="#b49">(Zhu and Li, 2022)</ref> RoB ? DEGREE <ref type="bibr" target="#b9">(Hsu et al., 2022)</ref> leverages manually designed prompts to generate event records in natural language. We report the end-to-end performance of DEGREE instead of the pipeline way. The model leverages BART-large as encoder-decoder.</p><p>Joint IE. There are only two previous models that consider the joint IE in ACE05-E and ERE-EN datasets.</p><p>? OneIE <ref type="bibr" target="#b17">(Lin et al., 2020)</ref> proposes an end-toend IE model, which employs global features and type dependency constraint at decoding step.</p><p>? FourIE (Nguyen et al., 2021) further improves the model by incorporating interaction dependency on representation level and label level.</p><p>For a fair comparison, we list the pre-trained model used for all baselines and our model on every IE dataset in Table <ref type="table">6</ref>. When choosing our pre-trained language model in different IE tasks' datasets, we pick the same pre-trained model as the most recently published papers, such as BioBERT for GENIA and RoBERTa-base for other NER datasets. For RE and joint IE tasks, we choose the same pre-trained model as previous work. For tasks where previous work applied a generative pretrained model, we choose pre-trained model that has a similar size. For example, in event extraction, we use DeBERTa-large, whose number of parameters is 390M, which is closest to BART-large and T5-large used by previous EE papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Evaluation Metrics</head><p>We report micro-F1 on all tasks:</p><p>? Entity: an entity is correct if its entity type B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? No response.</p><p>B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? No response.</p><p>B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.</p><p>B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. No response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Did you run computational experiments?</head><p>Section 4 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Appendix C</p><p>The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance. D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.</p><p>D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.</p><p>D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p><p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic extraction of facts from press releases to generate news stories</title>
		<author>
			<persName><forename type="first">Peggy</forename><forename type="middle">M</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">P</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alison</forename><forename type="middle">K</forename><surname>Huettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">M</forename><surname>Schmandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><forename type="middle">B</forename><surname>Nirenburg</surname></persName>
		</author>
		<idno type="DOI">10.3115/974499.974531</idno>
	</analytic>
	<monogr>
		<title level="m">3rd Applied Natural Language Processing Conference</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="1992-03-31">1992. 1992. March 31 -April 3, 1992</date>
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR, abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program -tasks, data, and evaluation</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2004-05-26">2004. 2004. May 26-28, 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. Open-Review</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<idno type="DOI">10.3233/FAIA200321</idno>
	</analytic>
	<monogr>
		<title level="m">Including 10th Conference on Prestigious Applications of Artificial Intelligence</title>
		<title level="s">Frontiers in Artificial Intelligence and Applications</title>
		<meeting><address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2020-08-08">2020. August-8 September 2020. August 29 -September 8, 2020</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2006" to="2013" />
		</imprint>
	</monogr>
	<note>ECAI 2020 -24th European Conference on Artificial Intelligence. PAIS 2020</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Twenty-five years of information extraction</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324919000512</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="677" to="692" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SOTR: segmenting objects with transformers</title>
		<author>
			<persName><forename type="first">Ruohao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dantong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00707</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10-10">2021. October 10-17, 2021</date>
			<biblScope unit="page" from="7137" to="7146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A joint many-task model: Growing a neural network for multiple NLP tasks</title>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09">2017. 2017. September 9-11, 2017</date>
			<biblScope unit="page" from="1923" to="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno>CoRR, abs/1912.12180</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DEGREE: A data-efficient generation-based event extraction model</title>
		<author>
			<persName><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07-10">2022. July 10-15, 2022</date>
			<biblScope unit="page" from="1890" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalizing natural language analysis through span-relation representations</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.192</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="2120" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GENIA corpus -a semantically annotated corpus for bio-textmining</title>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Intelligent Systems for Molecular Biology</title>
		<meeting>the Eleventh International Conference on Intelligent Systems for Molecular Biology<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06-29">2003. June 29 -July 3, 2003</date>
			<biblScope unit="page" from="180" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08-03">2017. CoNLL 2017. August 3-4, 2017</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A span-based model for joint overlapped and discontinuous named entity recognition</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.372</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4814" to="4828" />
		</imprint>
	</monogr>
	<note>Long Papers), Virtual Event. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unified named entity recognition as word-word relation classification</title>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-02-22">2022. February 22 -March 1, 2022</date>
			<biblScope unit="page" from="10965" to="10973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A joint neural model for information extraction with global features</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.713</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7999" to="8009" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Event extraction as machine reading comprehension</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020</date>
			<biblScope unit="page" from="1641" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text2event: Controllable sequence-tostructure generation for end-to-end event extraction</title>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoyi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.217</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2795" to="2806" />
		</imprint>
	</monogr>
	<note>Virtual Event. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unified structure generation for universal information extraction</title>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5755" to="5772" />
		</imprint>
	</monogr>
	<note>ACL 2022</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1308</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2003-05-31">2003. May 31 -June 1, 2003</date>
			<biblScope unit="page" from="188" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-task instance representation interactions and label dependencies for joint information extraction with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Van Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viet Dac</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06-06">2021. June 6-11, 2021</date>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">C?cero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/s14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation, Se-mEval@COLING 2014</title>
		<meeting>the 8th International Workshop on Semantic Evaluation, Se-mEval@COLING 2014<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2014-08-23">2014. August 23-24, 2014</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-08-08">2013. 2013. August 8-9, 2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
	<note>CoNLL</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2003-05-31">2003. May 31 -June 1, 2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From light to rich ERE: annotation of entities, relations, and events</title>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Riese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neville</forename><surname>Ryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W15-0812</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, EVENTS@HLP-NAACL 2015</title>
		<meeting>the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, EVENTS@HLP-NAACL 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06-04">2015. June 4, 2015</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Creating a large benchmark for open information extraction</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1252</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01">2016. 2016. November 1-4, 2016</date>
			<biblScope unit="page" from="2300" to="2305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2104.09864</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive focal loss and knowledge distillation</title>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.132</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="page" from="1672" to="1681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="5783" to="5788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">Ace 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pre-training entity relation encoder with intra-span and inter-span information</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020</date>
			<biblScope unit="page" from="1692" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unire: A unified label space for entity relation extraction</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="220" to="231" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">2021a. A unified generative framework for various NER subtasks</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5808" to="5822" />
		</imprint>
	</monogr>
	<note>Long Papers), Virtual Event. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An embarrassingly easy but strong baseline for nested named entity recognition</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2208.04534</idno>
		<idno>CoRR, abs/2208.04534</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">2021b. A partition filter network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.17</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11-11">7-11 November, 2021</date>
			<biblScope unit="page" from="185" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Packed levitated marker for entity and relation extraction</title>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4904" to="4917" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2022</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel decomposition strategy</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3233/FAIA200356</idno>
	</analytic>
	<monogr>
		<title level="m">ECAI 2020 -24th European Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2020-08-08">2020. August-8 September 2020. August 29 -September 8, 2020</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2282" to="2289" />
		</imprint>
	</monogr>
	<note>-Including 10th Conference on Prestigious Applications of Artificial Intelligence. PAIS 2020</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.577</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1113</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30">2017. 2017. July 30 -August 4</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06-06">2021. June 6-11, 2021</date>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02">2021. February 2-9, 2021</date>
			<biblScope unit="page" from="14612" to="14620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Boundary smoothing for named entity recognition</title>
		<author>
			<persName><forename type="first">Enwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.490</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7096" to="7108" />
		</imprint>
	</monogr>
	<note>ACL 2022</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
