<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Compression Accelerator on IBM POWER9 and z15 Processors Industrial Product</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bulent</forename><surname>Abali</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bart</forename><surname>Blaner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Reilly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashutosh</forename><surname>Mishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Craig</forename><forename type="middle">B</forename><surname>Agricola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bedri</forename><surname>Sendir</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Cloud</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alper</forename><surname>Buyuktosunoglu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Jacobi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Starke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haren</forename><surname>Myneni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charlie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Systems</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Middlesex University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Data Compression Accelerator on IBM POWER9 and z15 Processors Industrial Product</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ISCA45697.2020.00012</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lossless data compression is highly desirable in enterprise and cloud environments for storage and memory cost savings and improved utilization I/O and network. While the value provided by compression is recognized, its application in practice is often limited because it's a processor intensive operation resulting low throughput and high elapsed time for compression intense workloads.</p><p>The IBM POWER9 and IBM z15 systems overcome the shortcomings of existing approaches by including a novel on-chip integrated data compression accelerator. The accelerator reduces processor cycles, I/O traffic, memory and storage footprint of many applications practically with zero hardware cost. The accelerator also eliminates the cost and I/O slots that would have been necessary with FPGA/ASIC based compression adapters. On the POWER9 chip, a single accelerator uses less than 0.5% of the processor chip area, but provides a 388x speedup factor over the zlib compression software running on a general-purpose core and provides a 13x speedup factor over the entire chip of cores. On a POWER9 system, the accelerators provide an end-to-end 23% speedup to Apache Spark TPC-DS workload compared to the software baseline. The z15 chip doubles the compression rate of POWER9 resulting in even much higher speedup factors over the compression software running on general-purpose cores. On a maximally configured z15 system topology, on-chip compression accelerators provide up to 280 GB/s data compression rate, the highest in the industry. Overall, the on-chip accelerators significantly advance the state of the art in terms of area, throughput, latency, compression ratio, reduced processor utilization, power/energy efficiency, and integration into the system stack.</p><p>This paper describes the architecture, and novel elements of the POWER9 and z15 compression/decompression accelerators with emphasis on trade-offs that made the on-chip implementation possible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Lossless data compression is used in computing systems to reduce the amount of data to be processed, stored and transmitted. Compression is highly valuable and desirable in enterprise and cloud environments, where it can lead to storage and memory cost savings. It also reduces I/O and network utilization and processor cycles, improving overall performance. Figure <ref type="figure">1</ref> illustrates yearly cost savings that may be realized on an example cloud infrastructure with 2-4x compression. Savings can be tens to hundreds of thousands of USD for petabyte of storage per year, or petabyte of network traffic This paper is part of the Industry Track of ISCA 2020's program. Speedup relative to baseline Infrastructure Price, 1000 USD Fig. <ref type="figure">1</ref>. Value of compression in two example environments: (A) 2x to 4x reduction in capacity or bandwidth utilization using compression would result in savings of tens to hundreds of thousands of USD per year <ref type="bibr" target="#b0">[1]</ref>: for a single in-memory caching server, 1 PB of object storage, or for 1 PB of network traffic in the cloud, (B) The POWER9 on-chip accelerator provides 23% speedup to the Spark TPC-DS workload.</p><p>or a single memory-cache server according to the published prices <ref type="bibr" target="#b0">[1]</ref>. An on-chip compression accelerator can deliver those savings with nearly zero hardware cost. Figure <ref type="figure">1</ref> shows another advantage of accelerators: Compared to a system with no accelerator, an end-to-end 23% speedup is achieved for the TPC-DS on Spark workload where compression is heavily used in the data processing pipeline.</p><p>While the value provided by compression is widely recognized, its application is often limited because of the high processing cost and the resulting low throughput and high elapsed time for compression intense workloads. In the past, IBM z13 and many other systems have used FPGA based PCIe attached compression accelerators <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. However, the FPGA cost and limited number of PCIe slots restrict their usage to high-end servers <ref type="bibr" target="#b1">[2]</ref> and specialized applications such as storage controllers.</p><p>IBM POWER9 (launched in 2017), and IBM z15 (launched in 2019) overcome the shortcomings of existing approaches by including a novel compression accelerator called NXU. Every z15 and POWER9 processor chip contains NXU. Product level software and libraries exploiting the accelerator are available for the IBM z/OS, zLinux, and AIX operating systems. For POWER Linux, open source drivers and libraries are currently going through the Linux community upstreaming process.</p><p>NXU is an industry-leading hardware accelerator implementing the Deflate standard with significantly higher throughput than any other implementation. On the largest z15 system topology with 20 processor chips, 20 NXU units provide 280 GB/s total throughput <ref type="bibr" target="#b5">[6]</ref>. We would need 68 PCIe based compression cards such as <ref type="bibr" target="#b1">[2]</ref>, with 4GB/s peak throughput to match the NXU performance. Besides substantial improvements in performance, the compression function on IBM z15 features a novel approach to integrate hardware acceleration residing outside the processor core into the system stack: Although NXU utilizes classic I/O mechanisms like Direct Memory Access <ref type="bibr">[DMA]</ref> in terms of data transmission and memory interactions, it is operated on behalf of applications by an architected z15 machine instruction implemented with millicode communicating with the hardware in lockstep, without operating system or hypervisor support.</p><p>The z15 and POWER9 on-chip accelerators feature several novel techniques to improve throughput and compression ratio: 1) CAM and pseudo-CAM based hybrid encoder that balances area and compression ratio in different ranges of the compression dictionary, 2) A processor-cache like area efficient hash table that uses search tokens and data tags to search up to 64 locations in the dictionary, 3) A dynamic Huffman encoder for improving compression ratio, and various other optimizations such as 2-pass, 1.1-pass modes, and cached code tables, 4) A "length limited" code table generator that enables the dynamic Huffman mode, 5) A decompressor including a parallel and speculative variable length code decoder designed for area efficiency, which breaks the 1 code per cycle limit of prior art, decoding up to 8 codes per clock cycle.</p><p>Overall, the resulting compression engine in IBM POWER9 and IBM z15 provide significant improvements in elapsed time, compression ratio, and higher capacity for existing workloads and reduces the cost of ownership. This paper describes the architecture, design, implementation, of the novel on-chip POWER9 and z15 compression and decompression accelerators with emphasis on the design trade-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND RELATED WORK</head><p>Deflate (RFC1951) is a compressed data format <ref type="bibr" target="#b6">[7]</ref> with wide-spread support due to its inter-operability and software implementations available virtually on all platforms, the most popular being the zlib library and the gzip file compression utility <ref type="bibr" target="#b7">[8]</ref>. Therefore, Deflate represents a natural choice for providing accelerated compression functionality in highly interconnected enterprise and cloud environments.</p><p>Deflate uses the LZ77 variant of the Lempel-Ziv (LZ) compression algorithm <ref type="bibr" target="#b8">[9]</ref>, followed by an entropy coding algorithm <ref type="bibr" target="#b6">[7]</ref>. Few other compressed data formats and software exist making various trade-offs between the compression ratio and throughput. LZ4 has a minimum match length of 4 bytes and no entropy encoding. Snappy has a byte-oriented format and no entropy encoding. ZSTD has large search windows and uses the tANS encoder in addition to Huffman <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Compression and decompression throughput of these are ordered highest to lowest as LZ4, Snappy, ZSTD, and Deflate (as implemented by the zlib library and the gzip tool). Their average compression ratios are ordered from best to worst as Deflate (zlib/gzip), ZSTD, LZ4, and Snappy. For few exceptional files ZSTD may compress better than zlib/gzip because of its larger window size and other algorithmic improvements. We chose to implement a Deflate accelerator because it is wellestablished with widespread support and has the potential for good compression ratio. We compare our work to previous work in Sections VIII-IX.</p><p>LZ77 replaces duplicate strings with references to earlier copies in the most recent 32KB of the source stream <ref type="bibr" target="#b8">[9]</ref>, called the sliding window or dynamic dictionary (called "History" in this paper). An LZ reference is a distance/length pair, a copy instruction from an earlier string to the current point in the stream. Deflate distances are 1-32768 bytes and lengths are 3-258 bytes. Source bytes not encoded with an LZ reference are encoded as LZ literals. The goal of an LZ77 encoder is to find the longest matching string in the sliding window resulting in the smallest encoded output. This goal challenges both hardware and software implementations, often resulting in a tradeoff between area (or memory), performance, and compression ratio. We describe an area efficient LZ77 encoding hardware that improves state of the art in Section IV, where the related work <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b18">[19]</ref> are also discussed.</p><p>Deflate uses variable length prefix-free codes produced by the Huffman algorithm to compress an LZ77 stream by another 10-20% <ref type="bibr" target="#b19">[20]</ref>. Frequent symbols are encoded with fewer bits. For example, the most frequent character 'e' in English may be encoded with 6-bits vs. the usual 8-bits. Deflate specifies a maximum code length of 15-bits, called "length limited encoding" <ref type="bibr" target="#b20">[21]</ref>. A "Dynamic Huffman Table" (DHT) maps the LZ77 alphabet of symbols to 1 to 15-bit codes. DHTs are custom made according to LZ77 symbol statistics.</p><p>Producing a dynamic Huffman table (DHT) with a high throughput is a challenge. Many hardware designs do not solve this problem and instead they implement a static table with a predetermined LZ symbol distribution which typically degrades the compression ratio. In contrast, we implemented a true "Dynamic Huffman" mode in both POWER9 and z15 to achieve highest possible compression ratio, as described in Section V, where related work <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref> and hardware issues are also discussed.</p><p>The prefix-free property allows concatenation of variable length codes with no delimiters in-between which challenges high throughput decoders. Simple hardware designs can handle at most one code per cycle <ref type="bibr" target="#b26">[27]</ref>, because the next code's first bit position cannot be known until the current one is decoded. We describe a speculative decoder capable of decoding 8 codes per cycle in Section VI where the related work <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> is also discussed.</p><p>Low latency access to an accelerator is important to make it useful for a broad range of software applications. We developed user-mode interfaces and a user-mode page fault handler to provide low latency access, described in Section VII. Related works include <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b31">[32]</ref>.</p><p>IBM has been employing compression hardware and software extensively in its products. IBM z13 and POWER systems used PCIe based Deflate accelerators <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b27">[28]</ref>. POWER7+ processor implemented a proprietary on-chip memory compression accelerator <ref type="bibr" target="#b32">[33]</ref>. The MXT system (x86 architecture based) implemented transparent compression in the memory controller <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Compression is standard on DS8000 and Storewize SAN storage systems, and TS7700 tape storage systems <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HIGH-LEVEL ACCELERATOR OVERVIEW</head><p>The z15 and POWER9 processor chips use 14nm process. NXU occupies less than 0.5% of the chip area. The z15 unit is slightly larger than the POWER9 unit due to differences in the hash arrays and the Huffman encoding logic. Figure <ref type="figure" target="#fig_1">2</ref> shows the z15 chip floor plan and the NXU unit detail at the southern edge of the chip. On both chips, NXU attach to the internal processor fabric as shown in Fig. <ref type="figure">3</ref>. NXU handles data at a rate of 8 bytes/clock at 2 GHz on POWER9 and 2.5 GHz on z15 (NXU runs at a lower clock frequency than the processor cores). The DMA engine moves data between the processor fabric and the Compress and Uncompress macros. and Silesia corpus <ref type="bibr" target="#b38">[39]</ref>. With increasing window size the compression ratio levels off.</p><p>The History FIFO is an SRAM that stores the sliding window data. The Compress macro leverages an SRAM based hash table for searching strings in the History FIFO. The z15 and POWER9 NXU cores are mostly identical. Since the z15 processor chip was developed later, few performance enhancements were added such as increased number of hash table ports (Section IV-D) and hardware produced code tables (Section V). The user interfaces to the accelerators were also implemented differently according to the ISA requirements of the two systems (Section VII).</p><p>Both processors enable user-mode access to NXU to eliminate OS kernel and hypervisor overheads. z15 cores use the machine instruction DFLTCC to directly control NXU <ref type="bibr" target="#b37">[38]</ref>. POWER9 cores instead, send control messages to NXU. Software uses virtual memory addresses and page pinning is not necessary.</p><p>Figure <ref type="figure" target="#fig_9">4</ref> shows the compressor data flow. Source data enters from the left and compressed data exits from the right. The LZ77 stage comprises two types of Content Addressable Memory (CAM) for searching the sliding window with input phrases and an LZ77 compression encoder that outputs up to 4 LZ77 symbols per cycle (LZ references and LZ literals). Symbols are then fed to a Huffman encoder that further compresses the data using a code-table generated by DHTGEN. NXU elements are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMPRESSOR: THE LZ77 STAGE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Hybrid LZ77 Encoder</head><p>Area is an important concern in an on-chip design. The accelerator is one of many tenants on a processor chip with cores/caches. Therefore, area is at premium compared to an FPGA or an ASIC chip which may have more resources available. Trade-offs exist between silicon area, throughput and compression ratio. We designed a hybrid encoder that makes these tradeoffs by budgeting the area where it is most effective.</p><p>Few LZ encoders search for string matches using Content Addressable Memories (CAM). CAMs are typically implemented with custom circuits <ref type="bibr" target="#b18">[19]</ref> or with latches and random logic such as in our design. Thousands of byte-wide comparators find every match of a search key in the CAM in the same cycle. However, CAMs are area and power hungry and have timing challenges at our target 2-2.5 GHz frequency. A pseudo-CAM on the other hand, is SRAM based: multiple banks of SRAM arrays function as a hash table for storing search items. A pseudo-CAM is area efficient because we estimated that SRAMs store about 14 to 16 times as many bits per unit area than latches in the 14nm CMOS process. But pseudo-CAM are imprecise and lossy: a limited number of SRAM locations may contain a search item. SRAM port count, hash and bank collisions gate throughput and accuracy.</p><p>We recognized that increasing the sliding window size has a diminishing return on the compression ratio as shown in Fig. <ref type="figure" target="#fig_2">5</ref>. Highest compression ratio change occurs in the nearhistory (near the origin in Fig. <ref type="figure" target="#fig_2">5</ref>) and levels off in the farhistory. Reasons are two-fold: duplicate strings are often found close to each other and Deflate uses extra bits for encoding far pointers <ref type="bibr" target="#b6">[7]</ref>.</p><p>Insights in Fig. <ref type="figure" target="#fig_2">5</ref> led to the novel "hybrid" compressor in Fig. <ref type="figure" target="#fig_9">4</ref> combining an area hungry true CAM for the nearhistory (? 512 bytes) with an area efficient pseudo-CAM for the far-history (513-32KB). The design budgets the silicon area between two different style of CAM according to the compression ratio benefit derived from the two ranges. Area and timing constraints didn't permit building a larger than 512 byte near-history CAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Near-History CAM</head><p>The CAM was implemented with random logic based on a design used in an IBM storage controller <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, shown in Fig. <ref type="figure">6</ref>. Note that this CAM is specialized for data compression reporting partial matches of a search key at any byte alignment unlike the traditional CAMs such as those used in network switches <ref type="bibr" target="#b18">[19]</ref>. Input is latched to the M = 8 byte column of latches on the left edge. Previous cycle's input bytes shift into the row of N = 512 byte history latches at the top, in FIFO order. M ? N byte comparators compare the input to the history and their match status are reported to the Priority Encoder on the right edge (fan-in of thousands of wires are omitted in Fig. <ref type="figure">6</ref>). A column of adjacent matches indicates a match between the input and the stored bytes (illustrated by the circles). Each comparator receives a match count from the comparator above it in the same column. It increments by the count by 1 and passes down to the comparator below. Therefore, each comparator knows how many bytes matched above it (0 to 7 bytes) and the maximum match length is known in each column and row offset.</p><p>A string match longer than 8 bytes spanning multiple cycles is tracked by the the 1-bit "continue" register C at the top, with one register allocated per column. C = 1 indicates that there was a match in the same column in the previous cycle. Therefore, a long match spanning the two cycles is present and the logic has deferred its encoding to this cycle. Likewise, in the current cycle the logic sets C ? 1 when a match ends at the bottom row. It indicates that the match may potentially continue in the next cycle.</p><formula xml:id="formula_0">0 0 0 1 0 0 T H E R E _ I S H E R E _ I N T H E N J A N E _ S I T H E R E _ I S 1 0 0 1 0 0 1 CONTINUE REGS HISTORY FIFO PRIORITY ENCODER LZ1 LZ0 LZ3 LZ2</formula><p>COUNT Fig. <ref type="figure">6</ref>. Near history CAM and the 4-way parallel LZ encoder illustrating 6, 3, and 4-byte matches. The 3-byte match is a continuation from the previous cycle. The 4-byte match may continue into the next cycle.</p><p>The COU N T register accumulates the length of the longest match continuing from one cycle to the next. When a string match terminates, the Priority Encoder calculates the LZ distance using the column offset and the LZ string length using the row offset plus COU N T .</p><p>The Priority Encoder (Fig. <ref type="figure">6</ref>) outputs up to 4 LZ symbols per cycle which was sized to sustain 8-byte per cycle input data rate on lightly compressible data with a mix of short LZ references and LZ literals, e.g. English text.</p><p>Figure <ref type="figure">6</ref> contains a mock example in which the 8 byte input data matches 3 different strings with match sizes of 6, 3, and 4 bytes. The 3-match is adjacent to the previous cycle's boundary. Since C = 1, the 3-match is continuation of a string match from the previous cycle. Since the 4-match is adjacent to the next cycle, its encoding may be deferred by setting C ? 1 and COU N T ? 4.</p><p>A CAM may produce hundreds of matches but not all will be encoded since some overlap each other as shown in Fig. <ref type="figure">6</ref>. The Priority Encoder chooses a subset of matches producing the smallest possible output, yielding the highest compression ratio. Matches are prioritized according to their length and taking the COU N T register and C register values into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Far-History CAM</head><p>The Far-history pseudo-CAM is constructed with SRAMs. In essence it is an 8-way set-associative cache with 8 R/W ports as detailed in this section. Area efficiency is the primary concern in this design. The 32KB sliding window is stored in 1-port SRAMs called the "History-FIFO" in Fig. <ref type="figure" target="#fig_3">7</ref>, written sequentially from the input stream in FIFO order. Virtual 2read ports are implemented with multiple banks of SRAM. The problem solved here is finding the longest match (3 to 258 bytes) of the input data in the History FIFO at a rate of 8-byte/cycle.</p><p>Long strings are matched through their substrings, called "tokens" (tokens have a fixed size for ease of hardware implementation). The 8-byte input is decomposed into overlapping K-byte search tokens (K = 5 is the firmware default.) A token can span two cycles of input. Each token is hashed to an 11-bit index of the 2048-entry hash table (Fig. <ref type="figure" target="#fig_3">7</ref>). A hash  replaced in FIFO order. In effect, the hash table is an 8-way set-associative cache directory with FIFO replacement policy and with 8 R/W ports. The 8 virtual ports are implemented with 16 banks of 1-port SRAMs in POWER9 and 2-port SRAMs in z15. Bank conflicts are resolved either by stalling the compressor pipeline or by dropping R/W requests to be discussed in Section IV-D.</p><p>Eight tokens of the input word hash to 8 different hash table entries returning 64 addresses in total. However, the History-FIFO SRAM has only 2-read ports and therefore 2 addresses out of the 64 History-FIFO addresses must be selected. The FILTER in Fig. <ref type="figure" target="#fig_3">7</ref> uses probabilistic data tags and address crosscomparisons to identify two candidates, one of which may potentially yield the longest match.</p><p>Non-equal tokens may hash to the same table entry. False positives are reduced with yet another hash of the search token, a 6-bit digest called "tag", stored in the hash table (Fig. <ref type="figure" target="#fig_3">7</ref>). Tags are saved in SRAM since they are smaller than tokens (6-bits vs. K-bytes). The FILTER dismisses a large fraction of nonequal tokens simply by comparing the input tags to the stored tags. False positive probability is 1/2 6 which may degrade compression ratio, a trade-off made in exchange for saving area. Note that false positives are not a correctness issue since any data read from the History-FIFO and the source input are eventually compared to verify a match. The FILTER uses additional heuristics for identifying long matches not detailed in this paper. The final two addresses out of the FILTER are used to read two data chunks from the History-FIFO. The two chunks are then fed to the LZ77 Encoder (Fig. <ref type="figure" target="#fig_9">4</ref>) which selects from multiple matches reported by the near and far history CAMs.</p><p>An example hybrid encoder result is shown in Fig. <ref type="figure">8</ref>. The benchmark file is compressed with the gzip tool and NXU. The LZ length distribution weighted by the length itself (minus an estimate of 2 bytes per pointer overhead) are plotted. Area under the two curves represent the source bytes compressed and eliminated by the two methods. We observe that NXU does not encode as many desirable long matches (&gt; 8 byte) as the gzip tool, evidenced by the NXU curves generally lower than gzip in that x-axis region. NXU encodes more 3-8 byte matches than gzip. NXU finds fewer 4-byte matches than gzip because 5-byte tokens cannot identify 4-byte strings as they will hash to different entries to be discussed next, in Section IV-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion on Design Trade-offs and a Comparison</head><p>Originally, the hashing logic was designed for K = 3-byte tokens since 3-byte is the minimum string length in Deflate. A short token may identify many matches, 3-bytes and longer. But only 8 positions are available in a hash table entry. For example, consider the phrase the which is contained in many longer strings, the , then , there , therefore whose first 3bytes all hash to the same entry when K = 3. Items are aged and lost faster in such hot table entries due to FIFOreplacement. On the other hand, with a long search token size, e.g. K = 8, hot entries are fewer and items age slower but they cannot find shorter (&lt; K byte) substrings of the token.</p><p>We experimentally determined that K = 5 performs the best on average for a range of benchmarks. Outliers exist such as the internal CUST2 benchmark dataset shown in Fig. <ref type="figure">9</ref> for which K = 8 yields 10% higher compression than the default; it indicates that the Far-history range contains many 8-byte or longer matches, which K = 8 is better at identifying. Note that the near-history logic is not affected by the token size and it still identifies as short as 3-byte matches.</p><p>The hash table is constructed using 16 banks of 1-read and 1-write port SRAM on the POWER9 chip. With 8 requests per cycle, bank conflicts occur 87% of the time on average, stalling the pipeline to resolve the conflict with an average access latency of 2.1 cycles (1.0 ideal). A firmware configuration option is available to drop conflicted access requests: higher throughput may be achieved at the expense of increased hash table misses and therefore reduced compression ratio. The z15 processor in comparison to POWER9, adopted 2-port SRAMs to reduce bank conflicts resulting in an average 1.12 cycle access latency, however at the expense of larger SRAMs. Performance benefits of this change will be discussed later on Fig. <ref type="figure">13</ref> in Section VIII-C.</p><p>The NXU hash table organization is quite efficient in terms of SRAM bits. Total capacity used for the sliding window is 32KB for History-FIFO plus 36KB for the hash table. Since the z15 hash table uses 2-port SRAMs; its area is larger in z15 than POWER9. Prior art, mostly FPGA based, often replicate logic to meet throughput and compression ratio objectives which is not suitable for an on-chip design. In <ref type="bibr" target="#b3">[4]</ref> In <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, the 32KB sliding window data is replicated M times stored directly in M SRAMs organized as hash tables (M = 16). Each hash table has M -read and 1-write ports (M 2read ports total). M overlapping substrings of the 2M -byte two-cycle input string are searched in the M SRAM modules (M 2 reads total). Compared to our design, O(M ) times more storage capacity and M times as many read ports are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. COMPRESSOR: HUFFMAN STAGE</head><p>The Huffman encoder (Fig. <ref type="figure" target="#fig_9">4</ref>) encodes the four LZ77 symbols received per cycle with 1 to 15-bit variable length codes. The accelerator must produce a Dynamic Huffman Table (DHT) customized according to the LZ symbol statistics which maps LZ77 symbols to the codes. The main problems of Huffman encoding are (a) generating the table (DHTGEN) fast and (b) encoding the LZ77 encoder output (Fig. <ref type="figure" target="#fig_9">4</ref>) with the DHT at a 8-byte/cycle input data rate.</p><p>The number of literals, lengths, and distances produced by the LZ77 encoder are accumulated in an SRAM array of counters at the output of the LZ77 encoder (LZ STATS, Fig. <ref type="figure" target="#fig_9">4</ref>.) Then, DHTGEN uses a length limiting (15-bit max) algorithm to generate a DHT based on those counter values. POWER9 and z15 implementations differ in this step. POWER9 NXU expects software to supply a DHT which takes longer to produce than in a hardware implementation. z15 NXU on the other hand has a self-contained hardware based DHTGEN unit. The two processors use different strategies to minimize the DHTGEN overheads as detailed in Section V-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Producing Dynamic Huffman Tables</head><p>The basic Huffman algorithm runs in O(n log n) time where n is the number of symbols in the table; n = 286 for the literal and length symbols and n = 30 for the distances <ref type="bibr" target="#b19">[20]</ref>. The algorithm is sequential and a basic hardware implementation would have a relatively long execution time negatively impacting overall throughput.</p><p>We developed a novel parallelized and length limiting Huffman algorithm in the hardware. A priority queue-based Huffman algorithm runs in O(n) time when symbols are pre-sorted by their count <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. We implemented the 2dimensional parallel Shear-Sort algorithm to sort n counter values in O( ? n log 2 ? n) cycles <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The sorter stores symbols and their counts in latches to enable n/2 pairs of compare-exchanges per cycle. Counts are converted to 10-bit floating point values to reduce the number of latches. Once sorted, the priority queue based Huffman algorithm is executed. Performance of this unit is discussed in Section VIII-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Length-Limited Encoding</head><p>Maximum code length permitted in the Deflate format is 15-bits. The Huffman algorithm builds a binary tree with LZ symbols at the leaves arranged according to their probabilities <ref type="bibr" target="#b19">[20]</ref>. The path from the tree root is the binary code of the LZ symbol assigned to the leaf. In the unlimited case, the tree depth may excessively grow as much as n -1 (n = 286 max.), which is the subject of "length-limiting" algorithms. The algorithms in the literature were found to be complex for a hardware implementation <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Our algorithm boosts small counter values to the effect of limiting the Huffman tree depth and then uses the basic unlimited length Huffman algorithm. An upper bound on the Huffman tree depth is given as -log ? p min , where ? = (1 + ? 5)/2 and p min is the probability of least frequent symbol <ref type="bibr" target="#b23">[24]</ref>. However, this upper bound, when used as a predictor of tree depth, is pessimistic in practice and difficult to implement in hardware due to the noninteger log base. We observed that -log 2 p min is a more accurate predictor of tree depth, although not a strict upper bound. Our algorithm iteratively normalizes LZ counts by the following integer division operation, count ? (count + 1)/2 which has the effect of boosting the probability of small counts therefore reducing the tree depth, but at the same time nearly preserving the probability of large counts and their sorted order. Normalization is repeated until the predictor is satisfied</p><formula xml:id="formula_1">-log 2 p min ? 15</formula><p>For ease of hardware implementation, the inequality may be raised to the power of 2 and integer counts may be used (e.g. reciprocal of p min ). If the maximum code length exceeds 15, the normalization step and the Huffman algorithm are repeated. A second iteration occurs rarely with simulated random trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 1.1-Pass Mode for High Throughput</head><p>The second problem of Huffman encoding is making two compression passes over the source data. The first pass is needed only to collect statistics which reduces the accelerator throughput by half. In the first pass, compressed output is Fig. <ref type="figure">10</ref>. The NXU decoder architecture consist of a parallel speculative Huffman decoder followed by an LZ77 decoder.</p><p>discarded but the LZ counts are kept for processing in DHT-GEN. In the second pass, the DHT is used to finally produce the actual Huffman encoded compressed output (recall that the DHT maps the LZ symbol alphabet to 1 to 15-bit codes.) To avoid two passes, we hypothesized that sampling the source data might be sufficient to obtain representative LZ symbol frequencies. Most data streams have "locality" in a small window of few hundred KB: literals, length and distance symbols are similarly distributed throughout a data stream. For example, if English text is used at the beginning the same is likely for the rest. Therefore, the z15 implementation makes the first pass only on the first 32KB of input data (sample size is configurable.) Resulting DHT is used in the second compression pass for up to 256KB of source data. We refer to this as the 1.1-pass method; it increases throughput from 50% to ideally 256/(256+32) = 89% of the hardware peak. We experimentally determined that sampling first 32KB is satisfactory in most cases. Although, as a future optimization different regions of sampling and sample sizes may be considered. Compression ratio impact of the 1.1-pass over the 2-pass method is presented in Section VIII-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion on Design Trade-offs</head><p>Many hardware designs implement a static table with an assumed LZ symbol distribution which typically degrades compression ratio <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>. For example, Fig. <ref type="figure" target="#fig_2">5</ref> shows the compression ratio difference between static and dynamic tables. IBM z13 adapter uses a "pseudo-dynamic" approach where one of 15 different static Huffman tables yielding the smallest output is selected at run time <ref type="bibr" target="#b1">[2]</ref>. We implemented in contrast, a true "Dynamic Huffman" mode for both POWER9 and z15 to achieve highest possible compression ratio.</p><p>On POWER9, software must generate a DHT <ref type="bibr" target="#b39">[40]</ref> which often takes long time to compared to the actual compression time. As a result, recently used DHTs are stored in a software cache which amortizes the software overhead over multiple jobs <ref type="bibr" target="#b39">[40]</ref>. Based on the POWER9 lessons learned, the z15 system added a DHTGEN hardware unit that eliminated the software overheads. The DHTGEN hardware unconditionally produces a new DHT for each job.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DECOMPRESSOR</head><p>Deflate's 1 to 15-bit variable length codes are concatenated with no delimiters in-between. In principle, codes must be decoded serially one at a time, since the next code's starting bit position cannot be known before the current code's length is determined. However, our design requires decoding up to 4 LZ references per cycle (4 length/distance pairs = 8 codes/cycle.)</p><p>We designed a parallel speculative Huffman decoder (Fig. <ref type="figure">10</ref>) to overcome the bottleneck. Compressed source data enters 32-bit/cycle from the left and uncompressed raw data exits 64-bit/cycle from the right. At one extreme, the 32-bit input word may contain 32 codes 1-bit each. At the other extreme, it may contain 2 codes 15-bits each and some fraction of a third code extending into the next cycle.</p><p>The speculative decoder, without knowing the code lengths, assumes that 32 different codes of any length start at every bit offset of the 32-bit input word. It decodes them in parallel, 32 codes per cycle. Wrongly speculated codes are invalidated later in the decoder pipeline since their assumed first-bit positions are wrong.</p><p>The design was inspired by an FPGA based decoder described in <ref type="bibr" target="#b27">[28]</ref>. We reduced the SRAM and area consumption of that design which made the on-chip integration possible. Our main insight was that 32 full-decoders were not necessary. Each full-decoder requires a large SRAM mapping 1 to 15bit codes to 286 length and literal symbols and 30 distance symbols. We instead implemented a speculative "First-Bit Decoder" (FB) which only identifies the starting bit positions of codes in the 32-bit input. The FB decoder does not store any LZ symbols and therefore it is smaller than a full-decoder. The FB decoder identifies the first bit position of the first Huffman code in the input word and hands it off to one of eight full-decoders shown in Fig. <ref type="figure">10</ref>. The 8 full-decoders work independently and in parallel starting at their respective firstbit positions and they produce LZ77 references which are then queued at the LZ77 decoder's input. Finally, the LZ77 decoder (Fig. <ref type="figure">10</ref>) processes up to 4 LZ references per cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. INTEGRATION OF THE ACCELERATOR INTO THE SYSTEM STACK A. User Interface</head><p>Traditionally, hardware devices are managed by kernelmode drivers performing tasks such as memory protection, address translation, page pinning, and coordinating shared access. User/kernel mode task switching penalizes throughput for short running device tasks. POWER9 and z15 both have user-mode accelerator interfaces to reduce software overheads which we describe in this section. A novel user-mode page fault handler is also used in POWER9. Main goal of the usermode page fault handler is to have low latency access to the accelerator by eliminating system calls, privileged instructions, and pinned pages. Retry capability of the accelerator helped accomplish that goal on POWER9. Performance of the usermode interfaces are detailed in Section VIII-D.</p><p>The z15 architecture includes the "Deflate Conversion Call" (DFLTCC) unprivileged instruction <ref type="bibr" target="#b37">[38]</ref>. DFLTCC enables user-mode access to NXU with no operating system or device driver support. DFLTCC was implemented in low level firmware called millicode running on the CISC type processor cores <ref type="bibr" target="#b40">[41]</ref>. Millicode interacts with the NXU on the same chip and performs essential functions such as checking access rights, virtual to absolute address translation, and exception handling. Since compression is a complex operation with many arguments, both z15 and POWER9 architected their own message formats for communicating with NXU. On z15, the DFLTCC instruction's general purpose register arguments contain the user source and target buffer addresses/lengths, and the sliding window address, and a Deflate specific parameter-block <ref type="bibr" target="#b37">[38]</ref>. The POWER architecture, in contrast, uses a 128 byte length message format called Coprocessor Request Block (CRB). CRB has fields for device function codes, source, target, and parameter-block memory addresses, similar to that of z15. User threads atomically post CRBs to NXU job queues, using the POWER9 "copy-paste" instruction pair bypassing the OS kernel <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p><p>POWER9 NXU has two 256 deep hardware managed job queues (high and low priority). The queues are mapped to the user address space with the mmap() system call. Synchronization among multiple processes and threads is handled in hardware; semaphores and mutexes are not necessary to share NXU. Multiple processes have fair access to the NXU thanks to a "job-limit" register that suspends long running jobs. When suspended, NXU dumps its internal state to the parameterblock in caller's memory. The dumped state includes partial checksums and stream offsets. The software thread resumes a job by submitting it again with the NXU state restored through the same parameter-block.</p><p>Another difference between z15 and POWER9 is that a POWER9 processor thread can send jobs to any other chip's NXU which can be used for balancing accelerator workloads and increasing overall throughput. The POWER9 processor includes a "Nest MMU" (NMMU), and an "effective to real address translation" table (ERAT) enabling virtually addressed NXU <ref type="bibr" target="#b41">[42]</ref>.</p><p>A notable design goal of these features and functions in both z15 and POWER9 is that NXU are accessed in the usermode without making system calls and without page pinning which are necessary for low latency access (any system calls are made once per device initialization.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. User mode page fault handling</head><p>While z15 does not need OS or device drivers, POWER9 needs kernel support to handle page faults caused by NXU.</p><p>To minimize Linux kernel code changes, we implemented a user-mode page-fault handler in the compression library <ref type="bibr" target="#b39">[40]</ref>. Figure <ref type="figure" target="#fig_4">11</ref> shows the handler control/data flow. The user library touches one byte in source and target data pages to make them present in memory; no new kernel support is required for this step. Posting a CRB to the NXU queue starts the job (steps U1-U4 in Fig. <ref type="figure" target="#fig_4">11</ref>). NXU depends on NMMU (Fig. <ref type="figure">3</ref>) and ERAT for virtual/physical address translation. In the common case, job executes without page faults (steps H1-H4, U5). Hardware interrupts are available through Linux signals but currently not used in the library.</p><p>When the system free page counts are low, the operating system may page-out user pages before a queued job starts. In the simple-fault case, NXU reports a missing page via the Command Status Block (CSB) found in the user memory (Fig. <ref type="figure" target="#fig_4">11</ref>). Kernel is not involved in this code path, H4. The problem is if the CSB containing page was also paged out. Then, NXU interrupts the kernel driver (the H5 path in Fig. <ref type="figure" target="#fig_4">11</ref>). The kernel driver pages in the CSB containing page and then copies the fault status back to the user thread waiting on job completion (steps H5, K1, K2, F5.) In both the H4 and H5 scenarios, the CSB status indicates that the user thread must retry the job.</p><p>To adapt to the low memory condition and to prevent infinite retries, the library shrinks the source and target buffer lengths and then retries the NXU job, effectively executing the original job using multiple smaller jobs. Suppose, an accelerator job requires 100 resident pages but some of them were paged out due to insufficient number of free pages. Through the retry mechanism, the library may cut the original job size down to 3 resident pages only, namely the source, target, and parameter-block pages, therefore making forward progress. The benefit of this approach is that the OS kernel is not involved in compression specific operations and page pinning is not required therefore enabling low latency access.</p><p>The value of user-mode access and page fault handling is also recognized in <ref type="bibr" target="#b28">[29]</ref> for network adapters. User-mode page fault handlers have been used in other contexts. The Userfaultfd and CRIU mechanisms were proposed for live migration of virtual machines <ref type="bibr" target="#b43">[44]</ref> and for applications to optimize access (e.g. caching, prefetching) to backing stores <ref type="bibr" target="#b44">[45]</ref>. Our method is for managing page faults caused by a physical device and has no relation to those mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Software Exploitation</head><p>Zlib API compliant libraries were implemented for leveraging the accelerators transparently <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b47">[48]</ref>. z/OS, z/Linux and AIX product level implementations are available. Power Linux implementation is going through the opensource upstreaming process. Existing zlib enabled applications can link to the accelerator enabled libraries without a new programming effort. New applications wanting to use compression can be coded for the zlib library virtually available on every O/S distribution. The accelerator can be leveraged when hardware is available.</p><p>Since Deflate is a widely adopted standard, general purpose cores and the on-chip accelerators may be employed in parallel for higher throughput. For example, one can choose to deploy the accelerator for compression-only since it can be much faster than an entire set of cores, whereas cores can be used for decompression-only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. RESULTS: MICROBENCHMARKS</head><p>Performance evaluation of the POWER9 and z15 on-chip accelerators is presented in this section. Accelerator throughput with a single thread and multiple threads and latency were experimentally determined with micro-benchmarks. Endto-end application performance of the POWER9 compression accelerator using the Spark TPC-DS benchmark is presented in Section IX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>1) Hardware: The POWER9 system model AC922 with two sockets (2 chips) is used. The system has 20 cores per chip operating at 3.8 GHz in simultaneous multi-threading (SMT4) mode with a total of 80 threads per chip. The POWER9 chip has one NXU accelerator operating at 2.0 GHz. Results reported here are for a single chip. The z15 experimental setup is an 8-core logical machine partition (LPAR) allocated from a single z15 chip. NXU runs at 2.5 GHz.</p><p>2) Software: Both POWER9 and z15 ran with Ubuntu 18.0.4 OS. POWER9 was booted with a Linux kernel and firmware that is customized for NXU <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. The POWER9 setup uses a zlib API compliant library written from ground up for supporting NXU <ref type="bibr" target="#b39">[40]</ref>. The z15 setup instead uses the original zlib library with NXU support added <ref type="bibr" target="#b47">[48]</ref>.</p><p>The compdecomp th.c utility was used for microbenchmarking <ref type="bibr" target="#b39">[40]</ref>. For comparison, we linked the utility either with the original zlib 1.2.11 on general purpose cores, or the accelerator enabled zlib-API compliant library. Single NXU throughput is measured in all cases.</p><p>For compression ratio measurements the GZIP, ZSTD v1.4.5, and LZ4 v1.9.2 command line utilities were used with default compression levels and options <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Silesia, Canterbury, and Calgary compression benchmark datasets were used <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compression Ratio</head><p>Figure <ref type="figure" target="#fig_1">12</ref> compares the compression ratio of LZ4, ZSTD, GZIP tools and libraries and the z15 NXU accelerator. (Note that results may vary depending on future revisions of hardware and software library). NXU is used in both the 2-pass and 1.1-pass modes for collecting symbol statistics as described in Section V-C. First observation is that the NXU results are 4-5% larger than GZIP -6, the default compression level.</p><p>The NXU 1.1-pass method (Section V-C) works well compared to the 2-pass method for all files except for ptt5 and pic. Inspection of the two files reveal that the first 32KB in both files consist of almost entirely zeros which do not represent the remainder of the file contents. Therefore, the 1.1-pass method does not get a good sample and has worse compression ratio than the 2-pass method for those two files.</p><p>NXU performs worse than GZIP for nci and xml. Nci contains long runs of the triple "0 ". Since each hash table entry can only store 8 addresses in FIFO order (Section IV-C), the hash table loses track of long string matches. Likewise, xml contains many repetitive XML tags for which the NXU hash table does not perform well.</p><p>The LZ4 tool <ref type="bibr" target="#b9">[10]</ref> has been optimized for speed and not for compression ratio which is also apparent in Fig. <ref type="figure" target="#fig_1">12</ref>. ZSTD compression ratio is generally comparable to that of GZIP except for kennedy.xls. ZSTD uses a 2MB sliding window (vs. 32KB in GZIP, ZLIB, and NXU, all Deflate based), which increases the probability of finding and encoding longer matches for some files, such as kennedy.xls. Additionally, ZSTD uses an entropy algorithm called tANS that can encode some highly compressible files with fewer bits per symbol than Huffman codes <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Throughput</head><p>Figure <ref type="figure">13</ref> shows the compression throughput of POWER9 and z15 NXU, and the zlib linked compdecomp th utility running on the POWER9 cores in the SMT4 mode (1-80 threads). In the NXU case, processor threads share a single NXU. POWER9 NXU has a total throughput in excess of 7 GB/s for 64KB source size and larger. Since the zlib results are quite low, to highlight the ratio between zlib on cores and NXU we also present a set of speedup calculations on Table <ref type="table">I</ref> (for 256KB source size) which show that NXU is 388x times faster than a POWER9 thread and 13x times faster than 80 threads on a POWER9 chip. In other words, a single NXU has the equivalent Deflate throughput of 13 processor chips full of cores which is substantial considering that it occupies less than 0.5% of the chip area.</p><p>z15 NXU compression throughput at 15 GB/s peak is double the POWER9 NXU throughput (Fig. <ref type="figure">13</ref>). As described in Section IV-D, z15 reduces compressor pipeline stalls by using 2-port SRAMs in the hash table, and the z15 NXU operating frequency is higher than POWER9 NXU.</p><p>Finally, Fig. <ref type="figure" target="#fig_9">14</ref> shows the decompression throughput of POWER9 and z15 NXU and the zlib library running on the POWER9 and z15 cores. Table <ref type="table">I</ref> shows that POWER9 and z15 NXU have 34 and 30-times speedup over a single thread, respectively. On POWER9, NXU decompression throughput is about 75% of the total throughput of 80-threads for large data. Note however that NXU is still faster than 1-thread zlib decompress for all data sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Latency</head><p>Low latency is as important as high throughput to make an accelerator useful for a broad range of software applications. On Figs.13-14 we observe that with a single thread, POWER9 NXU and z15 NXU compression throughputs are low for small data sizes, then reach their peak at about 256 KB data size. Although a detailed accelerator model LogCA <ref type="bibr" target="#b49">[50]</ref> exists, we used a simpler model to estimate the data granularity   we get T = 2T 0 which means that half the time is spent in the setup and the other half in the accelerator doing actual processing. In other words, the user thread receives 50% of the ideal accelerator bandwidth when Data Size = T 0 ? P eakBandwidth. With smaller data, the accelerator is underutilized because the setup time T 0 dominates. With small data, one way to increase the accelerator utilization is by multiple threads sharing it. On Figs.13-14 we can determine the half-peak bandwidths and the half-peak data sizes and then arrive at T 0 , given in Table <ref type="table">II</ref>. z15 compress single thread has the highest latency of 5.2?s, higher than 2.2?s of POWER9. The z15 NXU support library is a port of the existing zlib source code which contains some software overheads not found on the POWER9 implementation of the zlib-API compliant library supporting NXU.</p><p>The setup time T 0 can be decomposed into two parts, the software setup time on core and the setup time in the NXU hardware. In a multithreaded access to NXU, software can run in parallel on different processor threads. Whereas the NXU components of the threads must run sequentially on a single NXU. Therefore, with multithreading software delays nearly disappear in the total delay calculations and the 8-thread latencies in Table <ref type="table">II</ref> reveal the actual NXU hardware latencies in the 0.55 -0.73?s range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Power and Energy Efficiency</head><p>Figure <ref type="figure" target="#fig_2">15</ref> shows active power measurement of a POWER9 system <ref type="bibr" target="#b50">[51]</ref> with the accelerator and with cores-only. One or 80 independent copies of the zlib/zpipe.c utility running on a single POWER9 chip are used. Active power is defined as total system power minus the idle system power when no workload is running. Throughput includes file system overheads. Energy  Fig. <ref type="figure" target="#fig_2">15</ref>. Power/energy utilization of 1 NXU and 1 and 80 SMT4 threads on a single POWER9 chip are compared. Total system power minus the system idle power is shown. Energy unit is Joules per GB and calculated as Power/Throughput, the delta energy required to compress or decompress 1 GB of raw data. per unit of data processed (Joule/GB) is calculated by dividing active power consumption with throughput. System power increased by 27 to 42 watts with the accelerator. A large fraction of that is due to processor, cache, and memory activity during the compress and decompress operations. Power and energy characteristics of the accelerator are very favorable compared to the cores. Active energy is up to 2 orders of magnitude less as the results show.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. RESULTS: END TO END APPLICATION PERFORMANCE</head><p>Data compression is a vital component in large scale big data applications for improving storage space efficiency and application performance <ref type="bibr" target="#b51">[52]</ref>.</p><p>In this section, we first describe the integration of NXU into two components of the Hadoop ecosystem: Apache Spark <ref type="bibr" target="#b52">[53]</ref>, a popular distributed computing framework, and Apache Parquet <ref type="bibr" target="#b53">[54]</ref>, a columnar file format. Then, we evaluate the performance of NXU on Apache Spark with a popular industry-standard decision support benchmark, TPC-DS <ref type="bibr" target="#b54">[55]</ref>.</p><p>The accelerators speed up Spark by reducing storage I/O utilization and network traffic among the cluster nodes during the "shuffle" and "HDFS read/write" steps with a trivial amount of processor cycles. Terabytes of data are moved to/from storage and among Spark servers for each TPC-DS query. Spark uses software compression by default to reduce data amounts. Without acceleration, state of the art software compression codecs must use many processor cycles for compression therefore slowing down queries. Alternatively, compression quality is degraded to save processor cycles which results in higher I/O cost. Accelerators address both of these problems therefore speeding up Spark queries.</p><p>In order to integrate NXU to Java-based big data applications, we implement the common compression interfaces used by these applications in a thin JNI library that communicates with the NXU user-level libraries. Our JNI library implements two common streaming classes from Java standard library: OutputStream with a configurable block size to adjust maximum number of bytes to compress at once and InputStream for decompression. Additionally, it implements gzip compliant header and trailer for each block during compression and validates the integrity of uncompressed block using the NXU generated CRC at the time of decompression.</p><p>The stream classes are built on top of compress and decompress functions. For input and output buffers used in compression and decompression, implemented functions accept both primitive byte buffers that are allocated on JVM heap and off-JVM-heap allocated memory buffers. Such functions and stream interfaces provided in the JNI library allow us to integrate NXU to Apache Spark, Apache Hadoop (MapReduce and Hadoop Distributed File System), Apache Parquet (columnar storage format), Apache Kafka (a distributed streaming platform), Apache Cassandra (NoSQL database) and many other applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. TPC-DS Benchmark on Apache Spark</head><p>TPC-DS is a de-facto standard benchmark in academia <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b55">[56]</ref>- <ref type="bibr" target="#b57">[58]</ref> and industry <ref type="bibr" target="#b58">[59]</ref>- <ref type="bibr" target="#b60">[61]</ref> for evaluating performance of big data SQL platforms and underlying hardware infrastructure. The TPC-DS benchmark models a data warehouse for a large retail product supplier. It offers a dataset generator and 99 distinct SQL queries that cover the entire dataset. TPC-DS benchmark includes analytical queries from different workload classes, such as iterative OLAP, data mining and reporting. These queries are designed to perform an intense activity to stress the I/O, memory and CPU subsystem of the target database server.</p><p>TPC-DS benchmark interfaces with Spark SQL <ref type="bibr" target="#b61">[62]</ref>, a Spark module for relational data processing. Spark SQL integrates with various data sources. In our evaluation, we use Parquet files as data source because of its compatibility with Spark SQL, efficiency <ref type="bibr" target="#b62">[63]</ref> and performance <ref type="bibr" target="#b63">[64]</ref>. In the TPC-DS benchmark, we store the Parquet files in the Hadoop Distributed File System (HDFS) replicated across the cluster. HDFS is typically used as a storage backend for Apache Spark due to its high resiliency and scalability.</p><p>In Parquet files, data is horizontally partitioned into rows. Collections of rows form a row group and a group consists of a column chunk for each column. Parquet divides column chunks into column pages which is the smallest unit that must be read fully to access a single record. Compression and decompression are performed per column page. Hadoop libraries for Parquet, by default, define the page size as 1 MB and Snappy as the default compression codec.  Each dataset in Spark is represented as a data structure called resilient distributed dataset (RDD) <ref type="bibr" target="#b64">[65]</ref>, which is a fault-tolerant collection of partitions that can be operated on in parallel. Spark partitions RDDs into multiple partitions. A partition in Spark represents an atomic chunk of data. When a transformation (e.g join, groupbykey) is applied to the Spark dataset, the transformation is applied to each of its partition which will run inside the executor (JVM). Spark may require data from other partitions residing in another executor. Spark "shuffles" the data to compute the transformation. In this case, Spark will collect the required data from each partition and combine it into a new partition. Shuffle operations involve heavy data copying between executors and random reads and writes to disk. During a shuffle operation, data is first written to disk and transferred across the network. In order to speed up the workload by reducing the disk and network I/O bandwidth consumption, Spark compresses the shuffle data while writing to the disk. Compressed data is transferred over the network and uncompressed at the destination executor. Since TPC-DS performs many complex transformative queries on the dataset, shuffle compression plays a critical role in end-to-end performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>We run our Spark TPC-DS experiments on the following software and hardware stack:  executors to use the NXU local to the chip that they are running on. Therefore, we use all the cores and NXU units in the cluster to process the workload requests. We pick this Spark executor allocation and pinning strategy to run the TPC-DS workload because it delivered the best throughput for the default Spark in our experiments. 1) TPC-DS Data Generation: Table <ref type="table" target="#tab_8">III</ref> shows the elapsed time of generating the TPC-DS dataset with the scale factor of 3000 using Apache Spark as the driver and Apache Parquet as the output format. Snappy compression codec is the default codec used for Parquet files. Compared to Zlib, Snappy is faster but provides lower compression ratio. We observe that heavy disk write I/O to HDFS is the bottleneck in TPC-DS data generation. Compared to Snappy, Zlib reduces the execution time by 20% during the data generation and provides 34% better data compression. This is because Zlib reduces the I/O bottleneck by shrinking the size of the data at the cost of CPU cycles -shifting workload bottleneck to CPU and compression speed. Using POWER9 NXU provides 29% speed-up over Snappy by alleviating the I/O bottleneck and improving the compression speed during the data generation.</p><p>2) Running TPC-DS benchmark queries: In our 3 data nodes cluster, each TPC-DS query stream is processed by 6 Spark executors running concurrently, which would unavoidably cause Spark in-memory data partitions to be shuffled within the Spark executors and possible data spills to the disk due to memory pressure. Also, during the execution of the queries, each stream may read and write a few thousand partitions depending on the tables used by the TPC-DS query which would cause heavy read and write I/O to the disk. Fig. <ref type="figure" target="#fig_3">17</ref> shows queries executed per hour while running 99 queries in the TPC-DS benchmark with Spark SQL with 4 concurrent streams. time of the slowest stream to calculate the workload duration. We observe that using a strong compression algorithm for the Parquet data speeds up the workload execution time. Compared to the baseline, using Zlib as Parquet codec and LZ4 for Spark shuffle provides around 14% speed-up in execution time. However, using POWER9 NXU for both Parquet data and Spark provides 23% faster execution time compared to baseline by significantly reducing the I/O contention. Using POWER9 NXU for both Parquet data and Spark reduce the data transferred (read/write) during the shuffle by 32%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSIONS</head><p>While the value provided by compression is widely recognized, its application is often limited because of the high processing cost and the resulting low throughput and high elapsed time for compression intense workloads. To this end, this paper demonstrates POWER9 and IBM z15 novel on-chip compression accelerators that overcome the shortcomings of existing approaches. Overall, the on-chip accelerators advance the state of the art in terms of area, throughput, latency, compression ratio, reduced processor utilization, integration into the system stack, cost, and ease of use.</p><p>We see several opportunities as part of future work. One concern with a hard-coded accelerator is not being able to implement new compression algorithms. New algorithms are typically implemented to overcome the low throughput of general-purpose cores, however they often result in degraded compression quality. This paper's accelerator design with two orders of magnitude speedup over a core made the performance problem irrelevant. Furthermore, Deflate is a widely used standard which justified implementing a hard-coded accelerator on the chip. Improving compression quality however may still need a configurable/programmable accelerator or a compression-optimized ISA which we consider as valuable future research work.</p><p>In addition, we see plenty of other compression opportunities for capacity and bandwidth growth as part of future work. Caches, memory and I/O links are prime candidates for compression that can result in considerable performance improvement. For example, compression may be used to increase cache <ref type="bibr" target="#b65">[66]</ref> and memory capacity <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> and memory bandwidth <ref type="bibr" target="#b66">[67]</ref>. Compression can also be leveraged to improve bandwidth in I/O links and on-chip/off-chip coherent fabrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. z15 processor chip floorplan and detail of the NXU accelerator located at the southern edge of the chip.</figDesc><graphic url="image-1.png" coords="1,316.62,585.52,198.45,115.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Compression ratio as a function of sliding window size (zlib 1.2.11) and Silesia corpus<ref type="bibr" target="#b38">[39]</ref>. With increasing window size the compression ratio levels off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Far history pseudo-CAM uses a hash table to search the recent 32KB input. Data tags eliminate false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. User-mode page fault handler control flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 12 .Fig. 13 .Fig. 14 .</head><label>121314</label><figDesc>Fig. 12. Compression Ratio of three benchmark suites, Canterbury, Calgary, and Silesia with 5 different compression methods. (Note that both the GZIP utility and NXU use the Deflate compressed data format.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Data compression pipeline in Spark and HDFS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 16</head><label>16</label><figDesc>Fig.16shows data processing pipeline in Spark. Compression and decompression are used heavily in multiple components in the pipeline. Firstly, input and output files on HDFS are compressed and decompressed to save storage space and reduce I/O bandwidth usage. HDFS files are read and written sequentially on the disk. Secondly, compression is used to compress internal data such as in-memory data partitions, broadcast variables, and shuffle outputs.Each dataset in Spark is represented as a data structure called resilient distributed dataset (RDD)<ref type="bibr" target="#b64">[65]</ref>, which is a fault-tolerant collection of partitions that can be operated on in parallel. Spark partitions RDDs into multiple partitions. A partition in Spark represents an atomic chunk of data. When a transformation (e.g join, groupbykey) is applied to the Spark dataset, the transformation is applied to each of its partition which will run inside the executor (JVM). Spark may require data from other partitions residing in another executor. Spark "shuffles" the data to compute the transformation. In this case, Spark will collect the required data from each partition and combine it into a new partition. Shuffle operations involve heavy data copying between executors and random reads and writes to disk. During a shuffle operation, data is first written to disk and transferred across the network. In order to speed up the workload by reducing the disk and network I/O bandwidth consumption, Spark compresses the shuffle data while writing to the disk. Compressed data is transferred over the network and uncompressed at the destination executor. Since TPC-DS performs many complex transformative queries on the dataset, shuffle compression plays a critical role in end-to-end performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>? 4 IBM</head><label>4</label><figDesc>POWER9 LC922 servers (1 master, 3 worker nodes), 512 GB RAM, 2 chips (2 sockets), 20 cores per chip in SMT2 mode, 10x8TB 7200 RPM SAS 12Gb/s HDDs, 10Gbps private network ? Hadoop version 3.1, Apache Spark version 2.3.3 ? TPC-DS version 1.4, 4 concurrent streams of 99 queries On each chip on a worker node, we configure a Spark executor for each of the four TPC-DS streams. We exclusively allocate five cores per executor (JVM) and configure the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 4. Compressor data flow left to right: the hybrid design contains two types of CAM and an LZ77 encoder followed by a Huffman entropy encoder.</figDesc><table><row><cell></cell><cell>Memory Controller</cell><cell></cell><cell>L3</cell><cell cols="2">Cores L1/L2</cell><cell></cell><cell>NMMU (P9)</cell></row><row><cell cols="4">Processor Fabric (16B)</cell><cell cols="3">Direct Ctrl (Z15)</cell></row><row><cell cols="2">NXU</cell><cell></cell><cell>DMA</cell><cell></cell><cell></cell><cell cols="2">UMAC/ERAT(P9)</cell></row><row><cell></cell><cell></cell><cell cols="3">Channel I/F</cell><cell></cell><cell></cell></row><row><cell cols="2">Decompress (Decomp MAC)</cell><cell cols="2">History FIFO 32KB</cell><cell cols="3">Compress (Encode MAC)</cell><cell>Hash Table (Hash MAC)</cell></row><row><cell cols="8">Fig. 3. Top level architecture and processor attachment of the accelerator.</cell></row><row><cell></cell><cell cols="2">FAR HISTORY</cell><cell>2</cell><cell>LZ</cell><cell></cell><cell>DHT</cell><cell>table</cell></row><row><cell></cell><cell cols="2">pseudo-CAM</cell><cell></cell><cell>STATS</cell><cell></cell><cell>GEN</cell></row><row><cell>DMA/</cell><cell cols="2">NEAR HISTORY</cell><cell>4</cell><cell>LZ77</cell><cell>4</cell><cell cols="2">HUFFMAN</cell><cell>DMA/</cell></row><row><cell>SRC MEM</cell><cell>CAM</cell><cell></cell><cell cols="2">ENCODER</cell><cell></cell><cell cols="2">ENCODER</cell><cell>DST MEM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>table entry contains 8 addresses which are the candidate History-FIFO locations of the search token. Items in the hash table entry are</figDesc><table><row><cell cols="2">Hash Table Entry</cell><cell>ADDR0 TAG0</cell><cell>?</cell><cell>ADDR7 TAG7</cell></row><row><cell>FUNCTIONS 8 x HASH</cell><cell>8 addrs 11b each</cell><cell cols="2">HASH TABLE 16 SRAM banks 2048 x 144b in</cell><cell>64 FIFO ADDR/TAG FILTER pairs</cell></row><row><cell cols="2">8 x K-byte tokens search</cell><cell cols="3">FIFO ADDR</cell><cell>64 to 2 2 x 16B strings,</cell></row><row><cell>INPUT 8B/clk</cell><cell></cell><cell cols="2">HISTORY FIFO SRAM 32KB</cell><cell>probable INPUT matches, sent to the LZ encoder</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, four separate compression engines work in parallel on four different</figDesc><table><row><cell>Compression Ratio Relative to</cell><cell>5_Byte Token Size</cell><cell>-20% -15% -10% -5% 0% 5% 10%</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Token Size (Bytes)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>CUST1</cell><cell>CUST2</cell><cell>CUST3</cell><cell>CUST4</cell><cell>CUST5</cell><cell></cell></row><row><cell cols="9">Fig. 9. Compression ratio change as a function of search token size for five</cell></row><row><cell cols="8">datasets. Base compression ratio is for 5-byte tokens. Higher is better.</cell><cell></cell></row><row><cell cols="9">segments of the input stream and compressed outputs are</cell></row><row><cell cols="9">concatenated to achieve high throughput, reportedly with some</cell></row><row><cell cols="9">degradation of compression ratio. 16,177,152 bits (1.92MB) of</cell></row><row><cell cols="9">FPGA block memory was used. In [14], 7 independent engines</cell></row><row><cell cols="9">with 2.4 Gbit/s throughput were used to meet total throughput</cell></row><row><cell cols="9">requirements in a multithreaded environment. 26,401,536 bits</cell></row><row><cell cols="9">(3.15MB) of FPGA block memory was used. In comparison,</cell></row><row><cell cols="6">66KB of SRAM is used in our design.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE III GENERATING</head><label>III</label><figDesc>3TB TPC-DS DATASET( S C A L E F A C T O R=3000) WITH VARIOUS COMPRESSION CODECS. Fig. 17. Running TPC-DS benchmark (4 concurrent streams) on Spark SQL with various compression configurations. Each stream contains 99 queries in the order specified by TPC-DS specification.</figDesc><table><row><cell>Codec</cell><cell cols="3">Compressed(GB) Ratio Duration(hour)</cell></row><row><cell>Snappy</cell><cell>1019</cell><cell>2.94</cell><cell>4.1</cell></row><row><cell>Zlib</cell><cell>761</cell><cell>3.94</cell><cell>3.26</cell></row><row><cell>P9 NXU</cell><cell>804</cell><cell>3.73</cell><cell>2.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Middlesex University. Downloaded on July 20,2020 at 22:36:04 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We thank <rs type="person">Charles Lefurgy</rs> for power/energy discussions, to <rs type="person">K. Ekanadham</rs> for his formal verification efforts, and to the IBM engineers who took part in the design, verification, and software efforts.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Amazon S3 pricing and Amazon ElastiCache (US East)</title>
		<ptr target="https://aws.amazon.com/s3/pricing/" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrated high-performance data compression in the IBM z13</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sofia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jacobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jamsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Riedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4/5</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A scalable highbandwidth architecture for lossless compression on FPGAs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hauck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-throughput lossless compression on tightly coupled CPU-FPGA platforms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FPGA-based application acceleration: Case study with gzip compression/decompression streaming engine</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jamsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCAD Special Session C</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GHz microprocessor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Surprise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Isakson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cichanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fredeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saporito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Webel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parashurama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bertran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chidambarrao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bruen</surname></persName>
		</author>
		<idno>IBM z15: A 12-core 5.2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference (ISSCC)</title>
		<imprint>
			<date type="published" when="2020-02">February 2020</date>
			<biblScope unit="page" from="54" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">RFC1951 DEFLATE compressed data format specification version 1.3</title>
		<author>
			<persName><forename type="first">P</forename><surname>Deutsch</surname></persName>
		</author>
		<ptr target="https://tools.ietf.org/html/rfc1951" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Zlib: a massively spiffy yet delicately unobtrusive compression library</title>
		<ptr target="https://github.com/madler/zlib" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A universal algorithm for sequential data compression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="343" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">LZ4: Extremely fast compression algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Collet</surname></persName>
		</author>
		<ptr target="https://github.com/lz4/lz4" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
	<note>version v1.9.2</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A fast compressor/decompressor</title>
		<ptr target="https://github.com/google/snappy" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kucherawy</surname></persName>
		</author>
		<ptr target="https://github.com/facebook/zstd" />
		<title level="m">RFC8478: Zstandard compression and the application/zstd media type</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
	<note>v1.4.5</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The use of asymmetric numeral systems as an accurate replacement for Huffman coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tahboub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Gadgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Picture Coding Symposium (PCS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="65" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A scalable multi-engine xpress9 compressor with asynchronous data transfer</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hauck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="161" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High bandwidth compression to encoded data streams</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Hofstee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jamsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">US Patent</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">686</biblScope>
			<date type="published" when="2014">Apr. 22 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Method and arrangement for data compression according to the LZ77 algorithm</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hawes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">US Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">265</biblScope>
			<date type="published" when="2007-06-19">Jun. 19, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Circuit and method for use in data compression</title>
	</analytic>
	<monogr>
		<title level="s">US Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">787</biblScope>
			<date type="published" when="2006-04-18">Apr. 18, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data parsing and tokenizing apparatus, method and program</title>
	</analytic>
	<monogr>
		<title level="s">US Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">103</biblScope>
			<date type="published" when="2009-03">Mar. 3 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Content-addressable memory (CAM) circuits and architectures: A tutorial and survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pagiamtzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheikholeslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="712" to="727" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum-redundancy codes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1952">1952</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast algorithm for optimal length-limited Huffman codes</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Larmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="464" to="473" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the construction of Huffman trees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICALP</title>
		<imprint>
			<biblScope unit="page" from="382" to="410" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Warm-up algorithm: A Lagrangian construction of length restricted Huffman codes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Milidi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Laber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1405" to="1426" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the maximum length of Huffman codes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="219" to="223" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>Note: see Theorem 2.1, &quot;In the case that we only know a lower bound p &gt; 0 on the minimum probability</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shear-Sort: A true two dimensional sorting technique for VLSI networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Scherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Parallel Processing</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="903" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two nearly optimal sorting algorithms for mesh-connected processor arrays using Shear-Sort</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Scherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="165" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stateful hardware decompression in networking environment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Golander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nelms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Bass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th ACM/IEEE Symposium on Architectures for Networking and Communications Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High bandwidth decompression of variable length encoded data streams</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Hofstee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jamsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">US Patent</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">569</biblScope>
			<date type="published" when="2014-02">Sep. 2, 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Page fault support for network controllers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lesokhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="449" to="466" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dune: Safe user-level access to privileged CPU features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bittau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mashtizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazi?res</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;12)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="335" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A user-mode port of the Linux kernel</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Linux Showcase &amp; Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Virtual memory primitives for user programs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="96" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">IBM POWER7+ processor on-chip accelerators for cryptography and Active Memory Expansion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Blaner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Bass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lauricella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leavens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Sandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Memory expansion technology (MXT): software support and performance</title>
		<author>
			<persName><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Herger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="301" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Performance of hardware compressed main memory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Coyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guaitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Maestas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vollmar</surname></persName>
		</author>
		<ptr target="http://www.redbooks.ibm.com/redpapers/pdfs/redp4873.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IBM Private, Public, and Hybrid Cloud Storage Solutions. IBM Redbooks</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">To zip or not to zip: Effective resource usage for real-time compression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sotnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Traeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Margalit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on File and Storage Technologies (FAST&apos;13)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="229" to="241" />
		</imprint>
	</monogr>
	<note>Presented as part of the 11th</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">IBM z/Architecture Principles of Operation</title>
		<author>
			<persName><forename type="first">Ed</forename><surname>Ibm</surname></persName>
		</author>
		<idno>SA22-7832-12. IBM</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Thirteenth Edition</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Silesia compression corpus</title>
		<ptr target="http://sun.aei.polsl.pl/?sdeor/index.php?page=silesia" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">NX-GZIP: POWER9 gzip engine documentation and code samples</title>
		<ptr target="https://github.com/libnxz/power-gzip" />
		<imprint>
			<biblScope unit="page" from="2020" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Millicode in an IBM zSeries processor</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Farrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3.4</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">IBM POWER9 processor and system features for computing in the cognitive era</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Arimilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blaner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Drerup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Lais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Campisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leavens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Willenborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4/5</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">IBM Power ISA version 3.0B</title>
		<idno>ac- cessed: 2020-01-03</idno>
		<ptr target="https://openpowerfoundation.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">User space memory management for post-copy migration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rapoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ACM International Systems and Storage Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">UMap: Enabling application-driven optimizations for page management</title>
		<author>
			<persName><forename type="first">I</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcfadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iwabuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gokhale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Workshop on Memory Centric High Performance Computing (MCHPC)</title>
		<imprint>
			<biblScope unit="page" from="71" to="78" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">NX-GZIP: Linux source code</title>
		<ptr target="https://github.com/hmyneni/linux/tree/4.19-nx-gzip-v2" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">NX-GZIP: SKIBOOT firmware source code</title>
		<ptr target="https://github.com/hmyneni/linux/tree/4.19-nx-gzip-v2" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">IBM Z hardware-accelerated Deflate</title>
		<ptr target="https://github.com/iii-i/zlib/tree/dfltcc" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Canterbury and Calgary compression corpora</title>
		<ptr target="http://corpus.canterbury.ac.nz/descriptions/" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">LogCA: A high-level performance model for hardware accelerators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Altaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079856.3080216</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1145/3079856.3080216" />
	</analytic>
	<monogr>
		<title level="m">44th ACM/IEEE Annual International Symposium on Computer Architecture (ISCA 2017</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="375" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">IBM POWER9 circuit design and energy optimization for 14nm technology</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Fluhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Monfort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4/5</biblScope>
			<biblScope unit="page" from="4" to="5" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Making sense of performance in data analytics frameworks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Chun</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2789770.2789791" />
	</analytic>
	<monogr>
		<title level="m">12th USENIX Conference on Networked Systems Design and Implementation (NSDI&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="293" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Apache Spark</title>
		<ptr target="https://spark.apache.org" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Apache Parquet</title>
		<ptr target="https://parquet.apache.org/" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">TPC-DS website</title>
		<author>
			<persName><surname>Tpc</surname></persName>
		</author>
		<ptr target="http://www.tpc.org/tpcds/" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Analysis of TPC-DS: The first standard benchmark for SQL-based big data systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rabl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-A</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3127479.3128603</idno>
		<ptr target="https://doi.org/10.1145/3127479.3128603" />
	</analytic>
	<monogr>
		<title level="m">2017 Symposium on Cloud Computing (SoCC&apos;17)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="573" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Why you should run TPC-DS: A workload analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Walrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Conference on Very Large Data Bases (VLDB-07)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1138" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Designing a high performance cluster for large-scale SQL-on-Hadoop analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dholakia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Venkatachar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Durgavajhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sheard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2017-12">Dec 2017</date>
			<biblScope unit="page" from="1701" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<ptr target="https://developer.ibm.com/linuxonpower/perfcol/perfcol-bigdata/" />
		<title level="m">Apache Spark SQL TPC-DS on IBM POWER9</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Benchmarking big data SQL platforms in the cloud</title>
		<ptr target="https://databricks.com/blog/2017/07/12/benchmarking-big-data-sql-platforms-in-the-cloud.html" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Spark SQL adaptive execution at 100 TB</title>
		<ptr target="https://software.intel.com/en-us/articles/spark-sql-adaptive-execution-at-100-tb" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Spark SQL: Relational data processing in Spark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaftan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2723372.2742797</idno>
		<ptr target="https://doi.org/10.1145/2723372.2742797" />
	</analytic>
	<monogr>
		<title level="m">2015 ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1383" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Spark + Parquet in depth</title>
		<ptr target="https://databricks.com/session/spark-parquet-in-depth" />
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The impact of columnar file formats on SQL-on-Hadoop engine performance: A study on ORC and Parquet</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pergolesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/nsdi12/technical-sessions/presentation/zaharia" />
	</analytic>
	<monogr>
		<title level="m">9th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;12)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Touche: Towards ideal and efficient cache compression by mitigating tag area overheads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;52</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="453" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attache: Towards ideal memory compression by mitigating metadata bandwidth overheads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;51)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="326" to="338" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
