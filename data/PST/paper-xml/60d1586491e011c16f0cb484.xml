<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-18">18 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Elad</forename><surname>Ben-Zaken</surname></persName>
							<email>benzakenelad@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
							<email>shauli.ravfogel@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<email>yoav.goldberg@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-18">18 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.10199v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that with small-to-medium training data, fine-tuning only the bias terms (or a subset of the bias terms) of pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, bias-only fine-tuning is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large pre-trained transformer based language models, and in particular bidirectional masked language models from the BERT family <ref type="bibr" target="#b4">(Devlin et al., 2018;</ref><ref type="bibr" target="#b14">Liu et al., 2019;</ref><ref type="bibr" target="#b11">Joshi et al., 2019)</ref>, are responsible for significant gains in many NLP tasks. Under the common paradigm, the model is pre-trained on large, annotated corpora with the LM objective, and then finetuned on task-specific supervised data. The large size of these models make them expensive to train and, more importantly, expensive to deploy. This, along with theoretical questions on the extent to which finetuning must change the original model, has led researchers to consider finetuning variants where one identifies a small subset of the model parameters which need to be changed for good performance in end-tasks, while keeping all others intact ( §2).</p><p>We present a simple and effective approach to fine tuning ( §3), which has the following benefits:</p><p>1. Changing very few parameters per fine-tuned task. 2. Changing the same set of parameters for every tasks (task-invariance). 3. The changed parameters are both isolated and localized across the entire parameter space.</p><p>4. For small to medium training data, changing only these parameters reaches the same task accuracy as full fine-tuning, and sometimes even improves results.</p><p>Specifically, we show that freezing most of the network and fine-tuning only the bias-terms is surprisingly effective. Moreover, if we allow the tasks to suffer a small degradation in performance, we can fine-tune only two bias components (the "query" and "middle-of-MLP" bias terms), amounting to half of the bias parameters in the model, and only 0.04% of all model parameters. This result has a large practical utility in deploying multi-task fine-tuned models in memoryconstrained environments, as well as opens the way to trainable hardware implementations in which most of the parameters are fixed. It also opens up a set of research directions regarding the role of bias terms in pre-trained networks, and the dynamics of the fine-tuning process.</p><p>2 Background: fine-tuning and parameter-efficient fine-tuning</p><p>In transfer-learning via model fine-tuning, a pretrained encoder network takes the input and produces contextualized representations. Then, a taskspecific classification layer (here we consider linear classifiers) is added on top of the encoder, and the entire network (encoder+task specific classifiers) is trained end-to-end to minimize the task loss. Desired properties. While fine-tuning per-task is very effective, it also results in a unique, large model for each pre-trained task, making it hard reason about as well as hard to deploy, especially as the number of tasks increases. Ideally, one would want a fine-tuning method that: (i) matches the results of a fully fine-tuned model;</p><p>(ii) changes only a small portion of the model's parameters; and (iii) enables tasks to arrive in a stream, instead of requiring simultaneous access to all datasets. For efficient hardware based deployments, it is further preferred that (iv): the set of parameters that change values is consistent across different tasks.</p><p>Learning vs. Exposing. The feasibility of fulfilling the above requirements depends on a fundamental question regarding the nature of the fine-tuning process of large pre-trained LMs: to what extent does the fine-tuning process induces the learning of new capabilities, vs. the exposing of existing capabilities, which were learned during the pre-training process. If fine-tuning can be cast as exposure of existing capabilities, this can allow for more efficient fine-tuning and deployment, by building on the frozen, pre-trained model, and constraining the fine-tuning to a "small", task-specific modification, rather than unconstrained fine tuning over the entire parameter space.</p><p>Existing approaches. Two recent works have demonstrated that adaptation to various end-tasks can in fact be achieved by changing only a small subset of parameters. The first work, by <ref type="bibr" target="#b10">Houlsby et al. (2019)</ref> ("Adapters"), achieves this goal by injecting small, trainable task-specific "adapter" modules between the layers of the pre-trained model, where the original parameters are shared between tasks. The second work, by <ref type="bibr" target="#b7">Guo et al. (2020)</ref> ("Diff-Pruning"), achieves the same goal by adding a sparse, task-specific difference-vector to the original parameters, which remain fixed and are shared between tasks. The difference-vector is regularized to be sparse. Both methods allow adding only a small number of trainable parameters per-task (criteria ii), and each task can be added without revisiting previous ones (criteria iii). They also partially fulfill criteria (i), suffering only a small drop in performance compared to full fine-tuning. This supports, to some extent, the "fine-tuning-asexposing" hypothesis. The Adapter method, but not the Diff-Pruning method, also supports criteria (iv). However, Diff-Pruning is more parameter efficient than the Adapter method, and also achieves better task scores. We compare against Diff-Pruning and Adapters in the experiments section, and show that we perform favorably on many tasks while also satisfying criteria (iv).</p><p>3 Bias-terms Fine-tuning (BitFit)</p><p>We propose a method we call BitFit (BIas-Term FIne-Tuning), in which we freeze most of the transformer-encoder parameters, and train only the bias-terms and the task-specific classification layer.</p><p>The approach is parameter-efficient: each new task requires storing only the bias terms parameter vectors (which amount to less than 0.1% of the total number of parameters), and the task-specific final linear classifier layer.</p><p>Concretely, the BERT encoder is composed of L layers, where each layer starts with M selfattention heads, where a self attention head (m, ) has key, query and value encoders, each taking the form of a linear layer:</p><formula xml:id="formula_0">Q m, (x) = W m, q x + b m, q K m, (x) = W m, k x + b m, k V m, (x) = W m, v x + b m, v</formula><p>Where x is the output of the former encoder layer (for the first encoder layer x is the output of the embedding layer). These are then combined using an attention mechanism that does not involve new parameters:</p><formula xml:id="formula_1">h 1 = att Q 1, , K 1, , V 1, , . . . , Q m, , K m, , V m,l</formula><p>and then fed to an MLP with layer-norm (LN):</p><formula xml:id="formula_2">h 2 = Dropout W m 1 • h 1 + b m 1<label>(1)</label></formula><formula xml:id="formula_3">h 3 = g LN 1 (h 2 + x) − µ σ + b LN 1 (2) h 4 = GELU W m 2 • h 3 + b m 2 (3) h 5 = Dropout W m 3 • h 4 + b m 3 (4) out = g LN 2 (h 5 + h 3 ) − µ σ + b LN 2 (5)</formula><p>The collection of all matrices W ,(•) The bias terms are additive, and correspond to a very small fraction of the network, in BERT BASE and BERT LARGE bias parameters make up 0.09% and 0.08% of the total number of parameters in each model, respectively.</p><formula xml:id="formula_4">(•) and vectors g (•) , b ,(•) (•) ,</formula><p>We show that by freezing all the parameters W (•) and g (•) and fine-tuning only the additive bias terms b (•) , we achieve transfer learning performance which is comparable (and sometimes better!) than fine-tuning of the entire network.</p><p>We also show that we can fine-tune only a subset of the bias parameters, namely those associated with the query and the second MLP layer (only b</p><formula xml:id="formula_5">(•) q and b (•)</formula><p>m 2 ), and still achieve accuracies that rival full-model fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Datasets. We evaluate BitFit on the GLUE benchmark <ref type="bibr" target="#b23">(Wang et al., 2018</ref>). 2 Consistent with previous work <ref type="bibr" target="#b10">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b7">Guo et al., 2020)</ref> we exclude the WNLI task, on which BERT models do not outperform the majority baseline. Models and Optimization. We use the publicly available pre-trained BERT BASE , BERT LARGE <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref> and RoBERTa BASE <ref type="bibr" target="#b14">(Liu et al., 2019)</ref> models, using the HuggingFace interface and implementation. Appendix §A.2 lists optimization details.</p><p>Comparison to Diff-Pruning and Adapters (Table 1) In the first experiment, we compare Bit-Fit to Diff-Pruning method and Adapters method, when using a fewer number of parameters. Table <ref type="table" target="#tab_0">1</ref> reports the dev-set and test-set accuracies compared to the Diff-Pruning and Adapters numbers reported by <ref type="bibr" target="#b7">Guo et al. (2020)</ref> and <ref type="bibr" target="#b10">Houlsby et al. (2019)</ref> (respectively), on their least-parameters setting. This experiment used the BERT LARGE model.</p><p>On validation set, BitFit outperforms Diff-Pruning on 5 out of 9 tasks, and underperforms in 3, while using fewer trainable parameters 3 . Test-set results are less conclusive (only one clear win compared to Diff-Pruning and 3 clear wins compared to Adapters), though BitFit is still competitive with both Diff-Pruning and Adapters. Different Base-models (Table <ref type="table" target="#tab_1">2</ref>) We repeat the BERT LARGE results on different base-models (the smaller BERT BASE and the better performing RoBERTa BASE ). The results in Table <ref type="table" target="#tab_1">2</ref> show that 2 Appendix §A.3 lists the tasks and evaluation metrics. 3 QNLI results are not directly comparable, as the GLUE benchmark updated the test set since then. the trends remain consistent.</p><p>Are bias parameters special? are the bias parameters special, or will any random subset do? We sampled the same amount of parameters as in BitFit from the entire model, and fine-tuned only them ("rand 100k" line in Table <ref type="table" target="#tab_2">3</ref>). The result are substantially worse across all tasks.</p><p>Fewer bias parameters (Table <ref type="table" target="#tab_2">3</ref>) Can we finetune on only a subset of the bias-parameter?</p><p>We define the amount of change in a bias vector b to be</p><formula xml:id="formula_6">1 dim(b) b 0 − b F 1 ,</formula><p>that is, the average absolute change, across its dimensions, between the initial LM values b 0 and its fine-tuned values b F . Figure <ref type="figure" target="#fig_1">1</ref> shows the change per bias term and layer, for the RTE task (other tasks look very similar, see Appendix §A.4). The 'key' bias b k has zero change, consistent with the theoretical observation in <ref type="bibr" target="#b3">(Cordonnier et al., 2020)</ref>. In contrast, b q , the bias of the queries, and b m2 , the bias of the intermediate MLP layers (which take the input from 768-dims to 3072), change the most.  To test this (and to validate another token-level task), we train on increasing-sized subsets of SQuAD v1.0 <ref type="bibr" target="#b19">(Rajpurkar et al., 2016)</ref>. The results on Figure <ref type="figure" target="#fig_2">2</ref> show a clear trend: BitFit dominates over Full-FT in the smaller-data regime, while the trend is reversed when more training data is available. We conclude that BitFit is a worthwhile targetted finetuning method in small-to-medium data regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The problem of identifying the minimal set of parameters that need to be fine-tuned to achieve good performance in end-tasks relates both to practi- cal questions of model compression, and also to more fundamental question on the nature of the pre-training and finetuning process, the "linguistic knowledge" induced by each of them, and the extent to which it generalizes to different tasks.</p><p>Large LM models were shown to be overparameterized: they contain more parameters than needed in inference <ref type="bibr" target="#b1">(Buciluǎ et al., 2006;</ref><ref type="bibr" target="#b9">Hinton et al., 2015;</ref><ref type="bibr" target="#b21">Urban et al., 2017;</ref><ref type="bibr" target="#b12">Karnin, 1990;</ref><ref type="bibr" target="#b20">Reed, 1993;</ref><ref type="bibr" target="#b0">Augasta and Kathirvalavakumar, 2013;</ref><ref type="bibr" target="#b13">Liu et al., 2014;</ref><ref type="bibr" target="#b8">Han et al., 2015;</ref><ref type="bibr" target="#b17">Molchanov et al., 2017)</ref>. <ref type="bibr" target="#b6">Gordon et al. (2020)</ref> have demonstrated that overparmeterization can be exploited in finetuning: pruned network perform well in transfer setting. We work in a complementary setting, where the entire model is kept, but only some parameters are updated. <ref type="bibr" target="#b5">Frankle et al. (2020)</ref> have demonstrated that randomly-initialized CNNs achieve reasonable accuracy after training the batch-norm layers alone.</p><p>Bias terms and their importance are rarely dis-cussed in the literature<ref type="foot" target="#foot_1">4</ref> . <ref type="bibr" target="#b25">Zhao et al. (2020)</ref> describe a masking-based fine-tuning method, and explicitly mention ignoring the bias terms, as handling them "did not observe a positive effect on performance".</p><p>An exception is the work of <ref type="bibr" target="#b24">Wang et al. (2019)</ref> who analyzed bias terms from the perspective of attribution method. They demonstrate that the last layer bias values are responsible for the predicted class, and propose a way to back-propagate their importance. <ref type="bibr" target="#b16">Michel and Neubig (2018)</ref> finetuned the biases of the output softmax in an NMT systems, to personalize the output vocabulary. Finally, <ref type="bibr" target="#b2">Cai et al. (2020)</ref> demonstrate that bias-only finetuning similar to ours is effective also for adaptation of pre-trained computer vision models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have proposed a novel method for localized, fast fine-tuning of pre-trained transformers for endtasks. The method focuses the finetuning on a specific fraction of the model parameters-the biasesand maintains good performance in all the tasks we evaluated on. The ability to focus on the same small group of parameters eases deployment, as the vast majority of the parameters of the model are shared between various NLP tasks. It also allows for efficient hardware implementations that hard-wire most of the network computation with the pre-trained weights, while only allowing few changeable parts for inference time. Besides its empirical utility, the remarkable effectiveness of bias-only fine-tuning raises intriguing questions on the fine-tuning dynamics of pre-trained transformers, and the relation between the bias terms and transfer between LM and new tasks. We aim to study those questions in a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Layer naming</head><p>For convenience, we relate the notation used in the paper with the names of the corresponding parameters in the popular HuggingFace implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>To perform classification with BERT, we follow the approach of <ref type="bibr" target="#b4">Devlin et al. (2018)</ref>, and attach a linear layer to the contextual embedding of the CLS token to predict the label. The GLUE tasks are fed into BERT using the standard procedures. We optimize using AdamW <ref type="bibr" target="#b15">(Loshchilov and Hutter, 2017)</ref>, with batch sizes of 8. For full finetuning, we used initial learning rates in {1e-5, 2e-5, 3e-5, 5e-5}, and for the bias-only experiments we used initial learning rates in {1e-4, 4e-4, 7e-4, 1e-3} as the smaller rates took a very long time to converge on some of the tasks. With the larger learning rates, the bias-only fine-tuning converged in 7 or fewer epochs for most tasks, and up to 20 epochs on the others. We did not perform hyperparameter optimization beyond the minimal search over 4 learning rates.</p><p>As <ref type="bibr" target="#b18">Mosbach et al. (2020)</ref> show, fine-tuning BERT LARGE and RoBERTa BASE is a unstable due to vanishing gradients. BitFit allows for the usage of bigger learning rates, and overall the optimization process is much more stable, when compared with a full fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 GLUE Benchmark</head><p>We provide information on the GLUE tasks we evaluated on, as well as on the evaluation metrics. We test our approach on the following subset of the GLUE <ref type="bibr" target="#b23">(Wang et al., 2018)</ref>    The metrics that we used to evaluate GLUE Benchmark are in Table <ref type="table" target="#tab_5">5</ref>. Learning rate configurations for best performing models are in Table <ref type="table" target="#tab_6">6</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Amount of change in bias terms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 SQuAD F1 Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>indicated in blue and purple are the network's parameters Θ, where the subset of purple vectors b ,(•) (•) are the bias terms. 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Change in bias components (RTE task).</figDesc><graphic url="image-1.png" coords="3,317.06,239.00,196.44,155.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of BitFit and Full-FT with BERT BASE exact match score on SQuAD validation set.</figDesc><graphic url="image-2.png" coords="4,311.05,308.82,208.45,140.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Research</head><label></label><figDesc>Paraphrase Corpus (MRPC), The Quora Question Pairs (QQP), The Semantic Textual Similarity Benchmark (STS-B), The Multi-Genre Natural Language Inference Corpus (MNLI), The Stanford Question Answering Dataset (QNLI) and The Recognizing Textual Entailment (RTE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Change in bias components (CoLA task).</figDesc><graphic url="image-3.png" coords="7,306.14,579.28,218.27,169.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Change in bias components (MRPC task).</figDesc><graphic url="image-4.png" coords="8,70.87,70.87,218.27,173.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Change in bias components (STS-B task).</figDesc><graphic url="image-5.png" coords="8,70.87,290.44,218.27,175.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of BitFit and Full-FT with BERT BASE F1 score on SQuAD validation set.</figDesc><graphic url="image-6.png" coords="8,311.88,99.65,206.80,141.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>%Param QNLI SST-2 MNLI m MNLI mm CoLA MRPC STS-B RTE QQP</figDesc><table><row><cell>Train size</cell><cell></cell><cell>105k</cell><cell>67k</cell><cell>393k</cell><cell>393k</cell><cell>8.5k</cell><cell>3.7k</cell><cell>7k</cell><cell>2.5k 364k</cell></row><row><cell>(V) Full-FT †</cell><cell>100%</cell><cell>93.5</cell><cell>94.1</cell><cell>86.5</cell><cell>87.1</cell><cell>62.8</cell><cell>91.9</cell><cell>89.8</cell><cell>71.8 87.6</cell></row><row><cell>(V) Full-FT</cell><cell>100%</cell><cell>91.5</cell><cell>93.4</cell><cell>85.5</cell><cell>85.8</cell><cell>60.1</cell><cell>90.1</cell><cell>89.9</cell><cell>71.7 87.5</cell></row><row><cell>(V) Diff-Prune †</cell><cell>0.1%</cell><cell>92.7</cell><cell>93.3</cell><cell>85.6</cell><cell>85.9</cell><cell>58</cell><cell>87.4</cell><cell>86.3</cell><cell>68.6 85.2</cell></row><row><cell>(V) BitFit</cell><cell>0.08%</cell><cell>91.8</cell><cell>93.3</cell><cell>84.6</cell><cell>84.8</cell><cell>63.4</cell><cell>91.5</cell><cell>90.3</cell><cell>75.1 85.6</cell></row><row><cell>(T) Full-FT ‡</cell><cell>100%</cell><cell>91.1</cell><cell>94.1</cell><cell>86.7</cell><cell>86.0</cell><cell>59.6</cell><cell>88.9</cell><cell>86.6</cell><cell>71.2 71.7</cell></row><row><cell>(T) Full-FT †</cell><cell>100%</cell><cell>93.4</cell><cell>94.9</cell><cell>86.7</cell><cell>85.9</cell><cell>60.5</cell><cell>89.3</cell><cell>87.6</cell><cell>70.1 72.1</cell></row><row><cell>(T) Adapters ‡</cell><cell>3.6%</cell><cell>90.7</cell><cell>94.0</cell><cell>84.9</cell><cell>85.1</cell><cell>59.5</cell><cell>89.5</cell><cell>86.9</cell><cell>71.5 71.8</cell></row><row><cell>(T) Diff-Prune †</cell><cell>0.5%</cell><cell>93.3</cell><cell>94.1</cell><cell>86.4</cell><cell>86.0</cell><cell>61.1</cell><cell>89.7</cell><cell>86.0</cell><cell>70.6 71.1</cell></row><row><cell>(T) BitFit</cell><cell>0.08%</cell><cell>92.0</cell><cell>94.1</cell><cell>84.5</cell><cell>84.8</cell><cell>59.7</cell><cell>88.9</cell><cell>85.5</cell><cell>72.0 70.5</cell></row></table><note>BERT LARGE model performance on the GLUE benchmark validation set (V) and test set (T). Lines with † and ‡ indicate results taken from Guo et al. (2020) and Houlsby et al. (2019) (respectively).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Table 3 reports dev-set results when fine-tuning only the b m2 bias terms, for the BERT BASE model. Dev-set results for different base models. BB: BERT BASE . BL: BERT LARGE . Ro: RoBERTa BASE .</figDesc><table><row><cell>(•) q and b</cell><cell>(•)</cell></row></table><note>% Param QNLI SST-2 MNLI m MNLI mm CoLA MRPC STS-B RTE QQP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Fine-tuning using a subset of the bias parameters. Reported results are for the BERT BASE model.</figDesc><table><row><cell>(•)</cell></row><row><cell>m2</cell></row></table><note>Results are only marginally lower than when tuning all bias parameters. Tuning either b (•) q or b</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Mapping the HuggingFace's BertLayer bias parameters names to BitFit paper bias notation.</figDesc><table><row><cell cols="2">HuggingFace Parameter Name BitFit notation</cell></row><row><cell>attention.self.query.bias</cell><cell>b q</cell></row><row><cell>attention.self.key.bias</cell><cell>b k</cell></row><row><cell>attention.self.value.bias</cell><cell>b v</cell></row><row><cell>attention.output.dense.bias</cell><cell>b m 1</cell></row><row><cell cols="2">attention.output.LayerNorm.bias b LN 1</cell></row><row><cell>intermediate.dense.bias</cell><cell>b m 2</cell></row><row><cell>output.dense.bias</cell><cell>b m 3</cell></row><row><cell>output.LayerNorm.bias</cell><cell>b LN 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Metrics that we use to evaluate GLUE Benchmark.</figDesc><table><row><cell cols="3">Task Name BERT BASE BERT LARGE</cell></row><row><cell>QNLI</cell><cell>1e-4</cell><cell>7e-4</cell></row><row><cell>SST-2</cell><cell>4e-4</cell><cell>4e-4</cell></row><row><cell>MNLI</cell><cell>1e-4</cell><cell>1e-4</cell></row><row><cell>CoLA</cell><cell>7e-4</cell><cell>4e-4</cell></row><row><cell>MRPC</cell><cell>7e-4</cell><cell>1e-3</cell></row><row><cell>STS-B</cell><cell>1e-4</cell><cell>1e-4</cell></row><row><cell>RTE</cell><cell>1e-3</cell><cell>4e-4</cell></row><row><cell>QQP</cell><cell>4e-4</cell><cell>4e-4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Learning rate configurations for best performing models.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In Appendix §A.1 we relate this notation with parameter names in HuggingFace implementation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">Indeed, the equations in the paper introducing the Transformer model<ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> do not include bias terms at all, and their existence in the BERT models might as well be a fortunate mistake.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pruning algorithms of neural networks -a comparative study. Central Eur</title>
		<author>
			<persName><forename type="first">M</forename><surname>Augasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kathirvalavakumar</surname></persName>
		</author>
		<idno type="DOI">10.2478/s13537-013-0109-x</idno>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="115" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Tiny transfer learning: Towards memory-efficient on-device learning</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno>CoRR, abs/2007.11622</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multi-head attention: Collaborate instead of concatenate</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">Andreas Loukas, and Martin Jaggi. 2020. 2006.16362</date>
			<pubPlace>CoRR, abs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training batchnorm and only batchnorm: On the expressive power of random features in cnns</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename></persName>
		</author>
		<idno>CoRR, abs/2003.00152</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Morcos</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Compressing BERT: studying the effects of weight pruning on transfer learning</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">A</forename><surname>Gordon</surname></persName>
		</author>
		<idno>CoRR, abs/2002.08307</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Kevin Duh, and Nicholas Andrews</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>CoRR, abs/1902.00751</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno>CoRR, abs/1907.10529</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple procedure for pruning back-propagation trained neural networks</title>
		<author>
			<persName><forename type="first">Ehud</forename><forename type="middle">D</forename><surname>Karnin</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.80236</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="242" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pruning deep neural networks by optimal brain damage</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association</title>
				<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2014-09-14">2014. September 14-18, 2014</date>
			<biblScope unit="page" from="1092" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR, abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extreme adaptation for personalized neural machine translation</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2050</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="312" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-01">Jan Kautz. 2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>CoRR, abs/1606.05250</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pruning algorithms-a survey</title>
		<author>
			<persName><forename type="first">Russell</forename><surname>Reed</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.248452</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="740" to="747" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do deep convolutional nets really need to be deep and convolutional</title>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Özlem</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthai</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>CoRR, abs/1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bias also matters: Bias attribution for deep neural network explanation</title>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06">2019. 9-15 June 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6659" to="6667" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Masking as an efficient alternative to finetuning for pretrained language models</title>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2226" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
