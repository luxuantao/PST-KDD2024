<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stealing Machine Learning Models via Prediction APIs</title>
				<funder ref="#_DEfCuDR #_Q9ZA4kC #_nqcwpZh">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Florian</forename><surname>Tram?r</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Cornell Tech</orgName>
								<orgName type="institution" key="instit2">Jacobs Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ari</forename><surname>Juels</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
							<affiliation key="aff4">
								<address>
									<addrLine>000 2 2 Moons Yes 5, 000 2 2 Blobs Yes 5, 000 3 2 5-Class Yes 1, 000 5 20 Adult (Income) No 48, 842 2 108 Adult (Race) No 48, 842 5 105 Iris No 150 3 4 Steak Survey No 331 5 40 GSS Survey No 16, 127 3 101 Digits No 1, 797 10 64 Breast Cancer No 683 2 10 Mushrooms No 8, 124 2 112 Diabetes No 768 2 8</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stealing Machine Learning Models via Prediction APIs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service ("predictive analytics") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.</p><p>The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., "steal") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data set</head><p>Synthetic # records # classes # features Circles Yes</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning (ML) aims to provide automated extraction of insights from data by means of a predictive model. A predictive model is a function that maps feature vectors to a categorical or real-valued output. In a supervised setting, a previously gathered data set consisting of possibly confidential feature-vector inputs (e.g., digitized health records) with corresponding output class labels (e.g., a diagnosis) serves to train a predictive model that can generate labels on future inputs. Popular models include support vector machines (SVMs), logistic regressions, neural networks, and decision trees.</p><p>ML algorithms' success in the lab and in practice has led to an explosion in demand. Open-source frameworks such as PredictionIO and cloud-based services offered by Amazon, Google, Microsoft, BigML, and others have arisen to broaden and simplify ML model deployment.</p><p>Cloud-based ML services often allow model owners to charge others for queries to their commercially valuable models. This pay-per-query deployment option exemplifies an increasingly common tension: The query interface of an ML model may be widely accessible, yet the model itself and the data on which it was trained may be proprietary and confidential. Models may also be privacy-sensitive because they leak information about training data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. For security applications such as spam or fraud detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">55]</ref>, an ML model's confidentiality is critical to its utility: An adversary that can learn the model can also often evade detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>In this paper we explore model extraction attacks, which exploit the tension between query access and confidentiality in ML models. We consider an adversary that can query an ML model (a.k.a. a prediction API) to obtain predictions on input feature vectors. The model may be viewed as a black box. The adversary may or may not know the model type (logistic regression, decision tree, etc.) or the distribution over the data used to train the model. The adversary's goal is to extract an equivalent or near-equivalent ML model, i.e., one that achieves (close to) 100% agreement on an input space of interest.</p><p>We demonstrate successful model extraction attacks against a wide variety of ML model types, including decision trees, logistic regressions, SVMs, and deep neural networks, and against production ML-as-a-service (MLaaS) providers, including Amazon and BigML. <ref type="foot" target="#foot_0">1</ref> In nearly all cases, our attacks yield models that are func- tionally very close to the target. In some cases, our attacks extract the exact parameters of the target (e.g., the coefficients of a linear classifier or the paths of a decision tree). For some targets employing a model type, parameters or features unknown to the attacker, we additionally show a successful preliminary attack step involving reverse-engineering these model characteristics.</p><p>Our most successful attacks rely on the informationrich outputs returned by the ML prediction APIs of all cloud-based services we investigated. Those of Google, Amazon, Microsoft, and BigML all return high-precision confidence values in addition to class labels. They also respond to partial queries lacking one or more features. Our setting thus differs from traditional learning-theory settings <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b52">53]</ref> that assume only membership queries, outputs consisting of a class label only. For example, for logistic regression, the confidence value is a simple log-linear function 1/(1+e -(w?x+? ) ) of the ddimensional input vector x. By querying d + 1 random d-dimensional inputs, an attacker can with high probability solve for the unknown d + 1 parameters w and ? defining the model. We emphasize that while this model extraction attack is simple and non-adaptive, it affects all of the ML services we have investigated.</p><p>Such equation-solving attacks extend to multiclass logistic regressions and neural networks, but do not work for decision trees, a popular model choice. (BigML, for example, initially offered only decision trees.) For decision trees, a confidence value reflects the number of training data points labeled correctly on an input's path in the tree; simple equation-solving is thus inapplicable. We show how confidence values can nonetheless be exploited as pseudo-identifiers for paths in the tree, facilitating discovery of the tree's structure. We demonstrate successful model extraction attacks that use adaptive, iterative search algorithms to discover paths in a tree.</p><p>We experimentally evaluate our attacks by training models on an array of public data sets suitable as standins for proprietary ones. We validate the attacks locally using standard ML libraries, and then present case studies on BigML and Amazon. For both services, we show computationally fast attacks that use a small number of queries to extract models matching the targets on 100% of tested inputs. See Table <ref type="table" target="#tab_0">1</ref> for a quantitative summary.</p><p>Having demonstrated the broad applicability of model extraction attacks to existing services, we consider the most obvious potential countermeasure ML services might adopt: Omission of confidence values, i.e., output of class labels only. This approach would place model extraction back in the membership query setting of prior work in learning theory <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b52">53]</ref>. We demonstrate a generalization of an adaptive algorithm by Lowd and Meek <ref type="bibr" target="#b35">[36]</ref> from binary linear classifiers to more complex model types, and also propose an attack inspired by the agnostic learning algorithm of Cohn et al. <ref type="bibr" target="#b17">[18]</ref>. Our new attacks extract models matching targets on &gt;99% of the input space for a variety of model classes, but need up to 100? more queries than equation-solving attacks (specifically for multiclass linear regression and neural networks). While less effective than equation-solving, these attacks remain attractive for certain types of adversary. We thus discuss further ideas for countermeasures.</p><p>In summary, we explore model extraction attacks, a practical kind of learning task that, in particular, affects emerging cloud-based ML services being built by Amazon, Google, Microsoft, BigML, and others. We show:</p><p>? Simple equation-solving model extraction attacks that use non-adaptive, random queries to solve for the parameters of a target model. These attacks affect a wide variety of ML models that output confidence values. We show their success against Amazon's service (using our own models as stand-ins for victims'), and also report successful reverse-engineering of the (only partially documented) model type employed by Amazon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? A new path-finding algorithm for extracting decision</head><p>trees that abuses confidence values as quasi-identifiers for paths. To our knowledge, this is the first example of practical "exact" decision tree learning. We demonstrate the attack's efficacy via experiments on BigML. ? Model extraction attacks against models that output only class labels, the obvious countermeasure against extraction attacks that rely on confidence values. We show slower, but still potentially dangerous, attacks in this setting that build on prior work in learning theory. We additionally make a number of observations about the implications of extraction. For example, attacks against Amazon's system indirectly leak various summary statistics about a private training set, while extraction against kernel logistic regression models <ref type="bibr" target="#b56">[57]</ref> recovers significant information about individual training data points.</p><p>The source code for our attacks is available online at https://github.com/ftramer/Steal-ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>For our purposes, a ML model is a function f : X ? Y. An input is a d-dimensional vector in the feature space</p><formula xml:id="formula_0">X = X 1 ? X 2 ? ??? ? X d . Outputs lie in the range Y.</formula><p>We distinguish between categorical features, which assume one of a finite set of values (whose set size is the arity of the feature), and continuous features, which assume a value in a bounded subset of the real numbers. Without loss of generality, for a categorical feature of arity k, we let X i = Z k . For a continuous feature taking values between bounds a and b, we let</p><formula xml:id="formula_1">X i = [a, b] ? R.</formula><p>Inputs to a model may be pre-processed to perform feature extraction. In this case, inputs come from a space M, and feature extraction involves application of a function ex : M ? X that maps inputs into a feature space. Model application then proceeds by composition in the natural way, taking the form f (ex(M)). Generally, feature extraction is many-to-one. For example, M may be a piece of English language text and the extracted features counts of individual words (so-called "bag-ofwords" feature extraction). Other examples are input scaling and one-hot-encoding of categorical features.</p><p>We focus primarily on classification settings in which f predicts a nominal variable ranging over a set of classes. Given c classes, we use as class labels the set Z c . If Y = Z c , the model returns only the predicted class label. In some applications, however, additional information is often helpful, in the form of real-valued measures of confidence on the labels output by the model; these measures are called confidence values. The output space is then Y = [0, 1] c . For a given x ? X and i ? Z c , we denote by f i (x) the i th component of f (x) ? Y. The value f i (x) is a model-assigned probability that x has associated class label i. The model's predicted class is defined by the value argmax i f i (x), i.e., the most probable label.</p><p>We associate with Y a distance measure d Y . We drop the subscript Y when it is clear from context. For Y = Z c we use 0-1 distance, meaning d(y, y ) = 0 if y = y and d(y, y ) = 1 otherwise. For Y = [0, 1] c , we use the 0-1 distance when comparing predicted classes; when comparing class probabilities directly, we instead use the total variation distance, given by d(y,</p><formula xml:id="formula_2">y ) = 1 2 ? |y[i]-y [i]|.</formula><p>In the rest of this paper, unless explicitly specified otherwise, d Y refers to the 0-1 distance over class labels.</p><p>Training algorithms. We consider models obtained via supervised learning. These models are generated by a training algorithm T that takes as input a training set {(x i , y i )} i , where (x i , y i ) ? X ? Y is an input with an associated (presumptively correct) class label. The output of T is a model f defined by a set of parameters, which are model-specific, and hyper-parameters, which specify the type of models T generates. Hyper-parameters may be viewed as distinguished parameters, often taken from a small number of standard values; for example, the kernel-type used in an SVM, of which only a small set are used in practice, may be seen as a hyper-parameter. has a model f trained on its data and allows others to make prediction queries. An adversary uses q prediction queries to extract an f ? f .</p><formula xml:id="formula_3">DB# Data#owner# Train# model## Extrac3on# adversary# f ML#service# f (x 1 ) f (x q ) x q x 1 ?#</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Extraction Attacks</head><p>An ML model extraction attack arises when an adversary obtains black-box access to some target model f and attempts to learn a model f that closely approximates, or even matches, f (see Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>As mentioned previously, the restricted case in which f outputs class labels only, matches the membership query setting considered in learning theory, e.g., PAC learning <ref type="bibr" target="#b52">[53]</ref> and other previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. Learning theory algorithms have seen only limited study in practice, e.g., in <ref type="bibr" target="#b35">[36]</ref>, and our investigation may be viewed as a practice-oriented exploration of this branch of research. Our initial focus, however, is on a different setting common in today's MLaaS services, which we now explain in detail. Models trained by these services emit data-rich outputs that often include confidence values, and in which partial feature vectors may be considered valid inputs. As we show later, this setting greatly advantages adversaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine learning services.</head><p>A number of companies have launched or are planning to launch cloud-based ML services. A common denominator is the ability of users to upload data sets, have the provider run training algorithms on the data, and make the resulting models generally available for prediction queries. Simple-to-use Web APIs handle the entire interaction. This service model lets users capitalize on their data without having to set up their own large-scale ML infrastructure. Details vary greatly across services. We summarize a number of them in Table <ref type="table" target="#tab_2">2</ref> and now explain some of the salient features.</p><p>A model is white-box if a user may download a representation suitable for local use. It is black-box if accessible only via a prediction query interface. Amazon and Google, for example, provide black-box-only services. Google does not even specify what training algorithm their service uses, while Amazon provides only partial documentation for its feature extraction ex (see <ref type="bibr">Section 5)</ref>. Some services allow users to monetize trained models by charging others for prediction queries.</p><p>To use these services, a user uploads a data set and optionally applies some data pre-processing (e.g., field removal or handling of missing values). She then trains a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree</head><p>Amazon <ref type="bibr" target="#b0">[1]</ref> Microsoft <ref type="bibr" target="#b37">[38]</ref> BigML <ref type="bibr" target="#b10">[11]</ref> PredictionIO <ref type="bibr" target="#b42">[43]</ref> Google <ref type="bibr" target="#b24">[25]</ref>  For black-box models, the service provides users with information needed to create and interpret predictions, such as the list of input features and their types. Some services also supply the model class, chosen training parameters, and training data statistics (e.g., BigML gives the range, mean, and standard deviation of each feature).</p><p>To get a prediction from a model, a user sends one or more input queries. The services we reviewed accept both synchronous requests and asynchronous 'batch' requests for multiple predictions. We further found varying degrees of support for 'incomplete' queries, in which some input features are left unspecified <ref type="bibr" target="#b45">[46]</ref>. We will show that exploiting incomplete queries can drastically improve the success of some of our attacks. Apart from PredictionIO, all of the services we examined respond to prediction queries with not only class labels, but a variety of additional information, including confidence scores (typically class probabilities) for the predicted outputs.</p><p>Google and BigML allow model owners to monetize their models by charging other users for predictions. Google sets a minimum price of $0.50 per 1,000 queries. On BigML, 1,000 queries consume at least 100 credits, costing $0.10-$5, depending on the user's subscription.</p><p>Attack scenarios. We now describe possible motivations for adversaries to perform model extraction attacks. We then present a more detailed threat model informed by characteristics of the aforementioned ML services.</p><p>Avoiding query charges. Stepping stone to evasion. In settings where an ML model serves to detect adversarial behavior, such as identification of spam, malware classification, and network anomaly detection, model extraction can facilitate evasion attacks. An adversary may use knowledge of the ML model to avoid detection by it <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>In all of these settings, there is an inherent assumption of secrecy of the ML model in use. We show that this assumption is broken for all ML APIs that we investigate.</p><p>Threat model in detail. Two distinct adversarial models arise in practice. An adversary may be able to make direct queries, providing an arbitrary input x to a model f and obtaining the output f (x). Or the adversary may be able to make only indirect queries, i.e., queries on points in input space M yielding outputs f (ex(M)). The feature extraction mechanism ex may be unknown to the adversary. In Section 5, we show how ML APIs can further be exploited to "learn" feature extraction mechanisms. Both direct and indirect access to f arise in ML services. (Direct query interfaces arise when clients are expected to perform feature extraction locally.) In either case, the output value can be a class label, a confidence value vector, or some data structure revealing various levels of information, depending on the exposed API.</p><p>We model the adversary, denoted by A, as a randomized algorithm. The adversary's goal is to use as few queries as possible to f in order to efficiently compute an approximation f that closely matches f . We formalize "closely matching" using two different error measures:</p><p>? Test error R test : This is the average error over a test set D, given by</p><formula xml:id="formula_4">R test ( f , f ) = ? (x,y)?D d( f (x), f (x))/|D|.</formula><p>A low test error implies that f matches f well for inputs distributed like the training data samples.<ref type="foot" target="#foot_2">2</ref> ? Uniform error R unif : For a set U of vectors uniformly chosen in</p><formula xml:id="formula_5">X , let R unif ( f , f ) = ? x?U d( f (x), f (x))/|U|.</formula><p>Thus R unif estimates the fraction of the full feature space on which f and f disagree. (In our experiments, we found |U| = 10, 000 was sufficiently large to obtain stable error estimates for the models we analyzed.)</p><p>We define the extraction accuracy under test and uniform error as 1 -R test ( f , f ) and 1 -R unif ( f , f ). Here we implicitly refer to accuracy under 0-1 distance. When assessing how close the class probabilities output by f are to those of f (with the total-variation distance) we use the notations R TV test ( f , f ) and R TV unif ( f , f ). An adversary may know any of a number of pieces of information about a target f : What training algorithm T generated f , the hyper-parameters used with T , the feature extraction function ex, etc. We will investigate a variety of settings in this work corresponding to different APIs seen in practice. We assume that A has no more information about a model's training data, than what is provided by an ML API (e.g., summary statistics). For simplicity, we focus on proper model extraction: If A believes that f belongs to some model class, then A's goal is to extract a model f from the same class. We discuss some intuition in favor of proper extraction in Appendix D, and leave a broader treatment of improper extraction strategies as an interesting open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Extraction with Confidence Values</head><p>We begin our study of extraction attacks by focusing on prediction APIs that return confidence values. As per Section 2, the output of a query to f thus falls in a range [0, 1] c where c is the number of classes. To motivate this, we recall that most ML APIs reveal confidence values for models that support them (see Table <ref type="table" target="#tab_2">2</ref>). This includes logistic regressions (LR), neural networks, and decision trees, defined formally in Appendix A. We first introduce a generic equation-solving attack that applies to all logistic models (LR and neural networks). In Section 4.2, we present two novel path-finding attacks on decision trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Equation-Solving Attacks</head><p>Many ML models we consider directly compute class probabilities as a continuous function of the input x and real-valued model parameters. In this case, an API that reveals these class probabilities provides an adversary A with samples (x, f (x)) that can be viewed as equations in the unknown model parameters. For a large class of Table <ref type="table">3</ref>: Data sets used for extraction attacks. We train two models on the Adult data, with targets 'Income' and 'Race'. SVMs and binary logistic regressions are trained on data sets with 2 classes. Multiclass regressions and neural networks are trained on multiclass data sets. For decision trees, we use a set of public models shown in Table <ref type="table" target="#tab_6">5</ref>. models, these equation systems can be efficiently solved, thus recovering f (or some good approximation of it).</p><p>Our approach for evaluating attacks will primarily be experimental. We use a suite of synthetic or publicly available data sets to serve as stand-ins for proprietary data that might be the target of an extraction attack. Table <ref type="table">3</ref> displays the data sets used in this section, which we obtained from various sources: the synthetic ones we generated; the others are taken from public surveys (Steak Survey <ref type="bibr" target="#b25">[26]</ref> and GSS Survey <ref type="bibr" target="#b48">[49]</ref>), from scikit <ref type="bibr" target="#b41">[42]</ref> (Digits) or from the UCI ML library <ref type="bibr" target="#b34">[35]</ref>. More details about these data sets are in Appendix B.</p><p>Before training, we remove rows with missing values, apply one-hot-encoding to categorical features, and scale all numeric features to the range [-1, 1]. We train our models over a randomly chosen subset of 70% of the data, and keep the rest for evaluation (i.e., to calculate R test ). We discuss the impact of different pre-processing and feature extraction steps in Section 5, when we evaluate equation-solving attacks on production ML services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Binary logistic regression</head><p>As a simple starting point, we consider the case of logistic regression (LR). A LR model performs binary classification (c = 2), by estimating the probability of a binary response, based on a number of independent features. LR is one of the most popular binary classifiers, due to its simplicity and efficiency. It is widely used in many scientific fields (e.g., medical and social sciences) and is supported by all the ML services we reviewed.</p><p>Formally, a LR model is defined by parameters w ? R d , ? ? R, and outputs a probability</p><formula xml:id="formula_6">f 1 (x) = ? (w ? x + ? ), where ? (t) = 1/(1 + e -t )</formula><p>. LR is a linear classifier: it defines a hyperplane in the feature space X (defined by w ? x + ? = 0), that separates the two classes.</p><p>Given an oracle sample (x, f (x)), we get a linear equation w?x+? = ? -1 ( f 1 (x)). Thus, d +1 samples are both necessary and sufficient (if the queried x are linearly independent) to recover w and ? . Note that the required samples are chosen non-adaptively, and can thus be obtained from a single batch request to the ML service.</p><p>We stress that while this extraction attack is rather straightforward, it directly applies, with possibly devastating consequences, to all cloud-based ML services we considered. As an example, recall that some services (e.g., BigML and Google) let model owners monetize black-box access to their models. Any user who wishes to make more than d + 1 queries to a model would then minimize the prediction cost by first running a crossuser model extraction attack, and then using the extracted model for personal use, free of charge. As mentioned in Section 3, attackers with a final goal of model-inversion or evasion may also have incentives to first extract the model. Moreover, for services with black-box-only access (e.g., Amazon or Google), a user may abuse the service's resources to train a model over a large data set D (i.e., |D| d), and extract it after only d + 1 predictions. Crucially, the extraction cost is independent of |D|. This could undermine a service's business model, should prediction fees be used to amortize the high cost of training.</p><p>For each binary data set shown in Table <ref type="table">3</ref>, we train a LR model and extract it given d + 1 predictions. In all cases, we achieve R test = R unif = 0. If we compare the probabilities output by f and f , R TV test and R TV unif are lower than 10 -9 . For these models, the attack requires only 41 queries on average, and 113 at most. On Google's platform for example, an extraction attack would cost less than $0.10, and subvert any further model monetization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Multiclass LRs and Multilayer Perceptrons</head><p>We now show that such equation-solving attacks broadly extend to all model classes with a 'logistic' layer, including multiclass (c &gt; 2) LR and deeper neural networks. We define these models formally in Appendix A.</p><p>A multiclass logistic regression (MLR) combines c binary models, each with parameters w i , ? i , to form a multiclass model. MLRs are available in all ML services we reviewed. We consider two types of MLR models: softmax and one-vs-rest (OvR), that differ in how the c binary models are trained and combined: A softmax model fits a joint multinomial distribution to all training samples, while a OvR model trains a separate binary LR for each class, and then normalizes the class probabilities.</p><p>A MLR model f is defined by parameters w ? R cd , ? ? ? ? R c . Each sample (x, f (x)) gives c equations in w and ? ? ? . The equation system is non-linear however, and has no analytic solution. For softmax models for instance, the equations take the form e w i ?x+? i /(? c-1 j=0 e w j ?x+? j ) = f i (x). A common method for solving such a system is by minimizing an appropriate loss function, such as the logistic loss. With a regularization term, the loss function is strongly convex, and the optimization thus con- This approach naturally extends to deeper neural networks. We consider multilayer perceptrons (MLP), that first apply a non-linear transform to all inputs (the hidden layer), followed by a softmax regression in the transformed space. MLPs are becoming increasingly popular due to the continued success of deep learning methods; the advent of cloud-based ML services is likely to further boost their adoption. For our attacks, MLPs and MLRs mainly differ in the number of unknowns in the system to solve. For perceptrons with one hidden layer, we have w ? R dh+hc , ? ? ? ? R h+c , where h is the number of hidden nodes (h = 20 in our experiments). Another difference is that the loss function for MLPs is not strongly convex. The optimization may thus converge to a local minimum, i.e., a model f that does not exactly match f 's behavior.</p><p>To illustrate our attack's success, we train a softmax regression, a OvR regression and a MLP on the Adult data set with target 'Race' (c = 5). For the non-linear equation systems we obtain, we do not know a priori how many samples we need to find a solution (in contrast to linear systems where d + 1 samples are necessary and sufficient). We thus explore various query budgets of the form ? ? k, where k is the number of unknown model parameters, and ? is a budget scaling factor. For MLRs, we solve the equation system with BFGS <ref type="bibr" target="#b40">[41]</ref> in scikit <ref type="bibr" target="#b41">[42]</ref>. For MLPs, we use theano <ref type="bibr" target="#b50">[51]</ref> to run stochastic gradient descent for 1,000 epochs. Our experiments were performed on a commodity laptop (2-core Intel CPU @3.1GHz, 16GB RAM, no GPU acceleration).</p><p>Table <ref type="table" target="#tab_4">4</ref> shows the extraction success for each model, as we vary ? from 0.5 to at most 5. For MLR models (softmax and OvR), the attack is extremely efficient, requiring around one query per unknown parameter of f (each query yields c = 5 equations). For MLPs, the system to solve is more complex, with about 4 times more unknowns. With a sufficiently over-determined system, we converge to a model f that very closely approximates f . As for LR models, queries are chosen non-adaptively, so A may submit a single 'batch request' to the API. We further evaluated our attacks over all multiclass data sets from Table <ref type="table">3</ref>. For MLR models with k = c?(d + 1) parameters (c is the number of classes), k queries were sufficient to achieve perfect extraction (R test = R unif = 0, R TV test and R TV unif below 10 -7 ). We use 260 samples on average, and 650 for the largest model (Digits). For MLPs with 20 hidden nodes, we achieved &gt;99.9% accuracy with 5,410 samples on average and 11,125 at most (Adult). With 54,100 queries on average, we extracted a f with 100% accuracy over tested inputs. As for binary LRs, we thus find that cross-user model extraction attacks for these model classes can be extremely efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Training Data Leakage for Kernel LR</head><p>We now move to a less mainstream model class, kernel logistic regression <ref type="bibr" target="#b56">[57]</ref>, that illustrates how extraction attacks can leak private training data, when a model's outputs are directly computed as a function of that data. Kernel methods are commonly used to efficiently extend support vector machines (SVM) to nonlinear classifiers <ref type="bibr" target="#b13">[14]</ref>, but similar techniques can be applied to logistic regression <ref type="bibr" target="#b56">[57]</ref>. Compared to kernel SVMs, kernel logistic regressions (KLR) have the advantage of computing class probabilities, and of naturally extending to multiclass problems. Yet, KLRs have not reached the popularity of kernel SVMs or standard LRs, and are not provided by any MLaaS provider at the time. We note that KLRs could easily be constructed in any ML library that supports both kernel functions and LR models.</p><p>A KLR model is a softmax model, where we replace the linear components</p><formula xml:id="formula_7">w i ? x + ? i by a mapping ? s r=1 ? i,r K(x, x r ) + ? i .</formula><p>Here, K is a kernel function, and the representers x 1 ,. ..,x s are a chosen subset of the training points <ref type="bibr" target="#b56">[57]</ref>. More details are in Appendix A.</p><p>Each sample (x, f (x)) from a KLR model yields c equations over the parameters ? ? ? ? R sc , ? ? ? ? R c and the representers x 1 ,. ..,x s . Thus, by querying the model, A obtains a non-linear equation system, the solution of which leaks training data. This assumes that A knows the exact number s of representers sampled from the data. However, we can relax this assumption: First, note that f 's outputs are unchanged by adding 'extra' representers, with weights ? = 0. Thus, over-estimating s still results in a consistent system of equations, of which a solution is the model f , augmented with unused representers. We will also show experimentally that training data may leak even if A extracts a model f with s s representers. We build two KLR models with a radial-basis function (RBF) kernel for a data set of handwritten digits. We select 20 random digits as representers for the first model, and all 1,257 training points for the second. We extract the first model, assuming knowledge of s, by solving a system of 50,000 equations in 1,490 unknowns. We use the same approach as for MLPs, i.e., logistic-loss minimization using gradient descent. We initialize the extracted representers to uniformly random vectors in X , as we assume A does not know the training data distribution. In Figure <ref type="figure" target="#fig_1">2a</ref>, we plot 5 of the model's representers from the training data, and the 5 closest (in l 1 norm) extracted representers. The attack clearly leaks information on individual training points. We measure the attack's robustness to uncertainty about s, by attacking the second model with only 10 local representers (10,000 equations in 750 unknowns). Figure <ref type="figure" target="#fig_1">2b</ref> shows the average image of training points classified as a 3, 4, 5, 6 or 7 by the target model f , along with 5 extracted representers of f . Surprisingly maybe, the attack seems to be leaking the 'average representor' of each class in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Model Inversion Attacks on Extracted Models</head><p>Access to a model may enable inference of privacydamaging information, particularly about the training set <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. The model inversion attack explored by Fredrikson et al. <ref type="bibr" target="#b22">[23]</ref> uses access to a classifier f to find the input x opt that maximizes the class probability for class i, i.e., x opt = argmax x?X f i (x). This was shown to allow recovery of recognizable images of training set members' faces when f is a facial recognition model.</p><p>Their attacks work best in a white-box setting, where the attacker knows f and its parameters. Yet, the authors also note that in a black-box setting, remote queries to a prediction API, combined with numerical approximation techniques, enable successful, albeit much less efficient, attacks. Furthermore, their black-box attacks inherently require f to be queried adaptively. They leave as an open question making black-box attacks more efficient.</p><p>We explore composing an attack that first attempts to extract a model f ? f , and then uses it with the <ref type="bibr" target="#b22">[23]</ref> white-box inversion attack. Our extraction techniques replace adaptive queries with a non-adaptive "batch" query to f , followed by local computation. We show that extraction plus inversion can require fewer queries and less time than performing black-box inversion directly.</p><p>As a case study, we use the softmax model from <ref type="bibr" target="#b22">[23]</ref>, trained over the AT&amp;T Faces data <ref type="bibr" target="#b4">[5]</ref>. The data set consists of images of faces (92 ? 112 pixels) of 40 people. The black-box attack from <ref type="bibr" target="#b22">[23]</ref> needs about 20,600 queries to reconstruct a recognizable face for a single training set individual. Reconstructing the faces of all 40 individuals would require around 800,000 online queries.</p><p>The trained softmax model is much larger than those considered in Section 4.1, with 412,160 unknowns (d = 10,304 and c = 40). We solve an under-determined system with 41,216 equations (using gradient descent with 200 epochs), and recover a model f achieving R TV test , R TV unif in the order of 10 -3 . Note that the number of model parameters to extract is linear in the number of people c, whose faces we hope to recover. By using f in white-box model inversion attacks, we obtain results that are visually indistinguishable from the ones obtained using the true f . Given the extracted model f , we can recover all 40 faces using white-box attacks, incurring around 20? fewer remote queries to f than with 40 black-box attacks.</p><p>For black-box attacks, the authors of <ref type="bibr" target="#b22">[23]</ref> estimate a query latency of 70 milliseconds (a little less than in our own measurements of ML services, see Table <ref type="table" target="#tab_0">1</ref>). Thus, it takes 24 minutes to recover a single face (the inversion attack runs in seconds), and 16 hours to recover all 40 images. In contrast, solving the large equation system underlying our model-extraction attack took 10 hours. The 41,216 online queries would take under one hour if executed sequentially and even less with a batch query. The cost of the 40 local white-box attacks is negligible.</p><p>Thus, if the goal is to reconstruct faces for all 40 training individuals, performing model inversion over a previously extracted model results in an attack that is both faster and requires 20? fewer online queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decision Tree Path-Finding Attacks</head><p>Contrary to logistic models, decision trees do not compute class probabilities as a continuous function of their input. Rather, decision trees partition the input space into discrete regions, each of which is assigned a label and confidence score. We propose a new path-finding attack, that exploits API particularities to extract the 'decisions' taken by a tree when classifying an input.</p><p>Prior work on decision tree extraction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref> has focused on trees with Boolean features and outputs. While of theoretical importance, such trees have limited practical use. Kushilevitz and Mansour <ref type="bibr" target="#b32">[33]</ref> showed that Boolean trees can be extracted using membership queries (arbitrary queries for class labels), but their algorithm does not extend to more general trees. Here, we propose attacks that exploit ML API specificities, and that apply to decision tree models used in MLaaS platforms.</p><p>Our tree model, defined formally in Appendix A, al-lows for binary and multi-ary splits over categorical features, and binary splits over numeric features. Each leaf of the tree is labeled with a class label and a confidence score. We note that our attacks also apply (often with better results) to regression trees. In regression trees, each leaf is labeled with a real-valued output and confidence.</p><p>The key idea behind our attack is to use the rich information provided by APIs on a prediction query, as a pseudo-identifier for the path that the input traversed in the tree. By varying the value of each input feature, we then find the predicates to be satisfied, for an input to follow a given path in the tree. We will also exploit the ability to query incomplete inputs, in which each feature x i is chosen from a space X i ? {?}, where ? encodes the absence of a value. One way of handling such inputs ( <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46]</ref>) is to label each node in the tree with an output value. On an input, we traverse the tree until we reach a leaf or an internal node with a split over a missing feature, and output that value of that leaf or node.</p><p>We formalize these notions by defining oracles that A can query to obtain an identifier for the leaf or internal node reached by an input. In practice, we instantiate these oracles using prediction API peculiarities.</p><p>Definition 1 (Identity Oracles). Let each node v of a tree T be assigned some identifier id v . A leaf-identity oracle O takes as input a query x ? X and returns the identifier of the leaf of the tree T that is reached on input x.</p><p>A node-identity oracle O ? takes as input a query x ? X 1 ? {?} ? ??? ? X d ? {?} and returns the identifier of the node or leaf of T at which the tree computation halts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Extraction Algorithms</head><p>We now present our path-finding attack (Algorithm 1), that assumes a leaf-identity oracle that returns unique identifiers for each leaf. We will relax the uniqueness assumption further on. The attack starts with a random input x and gets the leaf id from the oracle. We then search for all constraints on x that have to be satisfied to remain in that leaf, using procedures LINE SEARCH (for continuous features) and CAT SPLIT (for categorical features) described below. From this information, we then create new queries for unvisited leaves. Once all leaves have been found, the algorithm returns, for each leaf, the corresponding constraints on x. We analyze the algorithm's correctness and complexity in Appendix C.</p><p>We illustrate our algorithm with a toy example of a tree over continuous feature Size and categorical feature Color (see Figure <ref type="figure" target="#fig_2">3</ref>). The current query is x = {Size = 50, Color = R} and O(x) = id 2 . Our goal is two-fold:</p><p>(1) Find the predicates that x has to satisfy to end up in leaf id 2 (i.e., Size ? (40, 60], Color = R), and (2) create new inputs x to explore other paths in the tree.</p><p>Algorithm 1 The path-finding algorithm. The notation id ? O(x) means querying the leaf-identity oracle O with an input x and obtaining a response id. By x[i] ? v we denote the query x obtained from x by replacing the value of x i by v.</p><formula xml:id="formula_8">1: x init ? {x 1 ,...,x d } random initial query 2: Q ? {x init }</formula><p>Set of unprocessed queries 3: P ? {} Set of explored leaves with their predicates 4: while Q not empty do 5:</p><p>x The LINE SEARCH procedure (line 12) tests continuous features. We start from bounds on the range of a feature X i = [a, b]. In our example, we have Size ? [0, 100]. We set the value of Size in x to 0 and 100, query O, and obtain id 1 and id 5 . As the ids do not match, a split on Size occurs on the path to id 2 . With a binary search over feature Size (and all other features in x fixed), we find all intervals that lead to different leaves, i.e., [0, 40], (40, 60], (60, 100]. From these intervals, we find the predicate for the current leaf (i.e., Size ? <ref type="bibr" target="#b39">(40,</ref><ref type="bibr">60]</ref>) and build queries to explore new tree paths. To ensure termination of the line search, we specify some precision ?. If a split is on a threshold t, we find the value t that is the unique multiple of ? in the range (t -?,t]. For values x i with granularity ?, splitting on t is then equivalent to splitting on t.</p><p>The CATEGORY SPLIT procedure (line 20) finds splits on categorical features. In our example, we vary the value of Color in x and query O to get a leaf id for each value. We then build a set S of values that lead to the current leaf, i.e., S = {R}, and a set V of values to set in x to explore other leaves (one representative per leaf). In our example, we could have V = {B, G, Y} or V = {B, G, O}.</p><p>Using these two procedures, we thus find the predicates defining the path to leaf id 2 , and generate new queries x for unvisited leaves of the tree.</p><p>A top-down approach. We propose an empirically more efficient top-down algorithm that exploits queries over partial inputs. It extracts the tree 'layer by layer',  starting at the root: We start with an empty query (all features set to ?) and get the root's id by querying O ? .</p><note type="other">Color Size</note><p>We then set each feature in turn and query O again. For exactly one feature (the root's splitting feature), the input will reach a different node. With similar procedures as described previously, we extract the root's splitting criterion, and recursively search lower layers of the tree.</p><p>Duplicate identities. As we verify empirically, our attacks are resilient to some nodes or leaves sharing the same id. We can modify line 7 in Algorithm 1 to detect id duplicates, by checking not only whether a leaf with the current id was already visited, but also whether the current query violates that leaf's predicates. The main issue with duplicate ids comes from the LINE SEARCH and CATEGORY SPLIT procedures: if two queries x and x differ in a single feature and reach different leaves with the same id, the split on that feature will be missed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Attack Evaluation</head><p>Our tree model (see Appendix A) is the one used by BigML. Other ML services use similar tree models. For our experiments, we downloaded eight public decision trees from BigML (see Table <ref type="table" target="#tab_6">5</ref>), and queried them locally using available API bindings. More details on these models are in Appendix B. We show online extraction attacks on black-box models from BigML in Section 5.</p><p>To emulate black-box model access, we first issue online queries to BigML, to determine the information contained in the service's responses. We then simulate black-box access locally, by discarding any extra information returned by the local API. Specifically, we make use of the following fields in query responses: Table <ref type="table">6</ref>: Performance of extraction attacks on public models from BigML. For each model, we report the number of leaves in the tree, the number of unique identifiers for those leaves, and the maximal tree depth. The chosen granularity ? for continuous features is 10 -3 .</p><p>? Prediction. This entry contains the predicted class label (classification) or real-valued output (regression). ? Confidence. For classification and regression trees, BigML computes confidence scores based on a confidence interval for predictions at each node <ref type="bibr" target="#b10">[11]</ref>. The prediction and confidence value constitute a node's id. ? Fields. Responses to black-box queries contain a 'fields' property, that lists all features that appear either in the input query or on the path traversed in the tree. If a partial query x reaches an internal node v, this entry tells us which feature v splits on (the feature is in the 'fields' entry, but not in the input x). We make use of this property for the top-down attack variant.</p><p>Table <ref type="table">6</ref> displays the results of our attacks. For each tree, we give its number of leaves, the number of unique leaf ids, and the tree depth. We display the success rate for Algorithm 1 and for the "top-down" variant with incomplete queries. Querying partial inputs vastly improves our attack: we require far less queries (except for the Steak Survey model, where Algorithm 1 only visits a fraction of all leaves and thus achieves low success) and achieve higher accuracy for trees with duplicate leaf ids. As expected, both attacks achieve perfect extraction when all leaves have unique ids. While this is not always the case for classification trees, it is far more likely for regression trees, where both the label and confidence score take real values. Surprisingly maybe, the top-down approach also fully some trees with a large number of duplicate leaf ids. The attacks are also efficient: The top-down approach takes less than 10 seconds to extract a tree, and Algorithm 1 takes less than 6 minutes for the largest tree. For online attacks on ML services, discussed next, this cost is trumped by the delay for the inherently adaptive prediction queries that are issued.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Online Model Extraction Attacks</head><p>In this section, we showcase online model extraction attacks against two ML services: BigML and Amazon. For BigML, we focus on extracting models set up by a user, who wishes to charge for predictions. For Amazon, our goal is to extract a model trained by ourselves, to which we only get black-box access. Our attacks only use ex- posed APIs, and do not in any way attempt to bypass the services' authentication or access-control mechanisms. We only attack models trained in our own accounts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case Study 1: BigML</head><p>BigML currently only allows monetization of decision trees <ref type="bibr" target="#b10">[11]</ref>. We train a tree on the German Credit data, and set it up as a black-box model. The tree has 26 leaves, two of which share the same label and confidence score. From another account, we extract the model using the two attacks from Section 4.2. We first find the tree's number of features, their type and their range, from BigML's public gallery. Our attacks (Algorithm 1 and the top-down variant) extract an exact description of the tree's paths, using respectively 1,722 and 1,150 queries. Both attacks' duration (1,030 seconds and 631 seconds) is dominated by query latency (? 500ms/query). The monetary cost of the attack depends on the perprediction-fee set by the model owner. In any case, a user who wishes to make more than 1,150 predictions has economic incentives to run an extraction attack. i.e., ex increases the number of features. If A reverseengineers ex, she can query the service on samples M in input space, compute x = ex(M) locally, and extract f in feature-space using equation-solving. We apply this approach to models trained by Amazon. Our results are summarized in Table <ref type="table" target="#tab_8">7</ref>. We first train a model with no categorical features, and quantile binning disabled (this is a manually tunable parameter), over the Digits data set. The attack is then identical to the one considered in Section 4.1.2: using 650 queries to Amazon, we extract a model that achieves R test = R unif = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case Study 2: Amazon Web Services</head><p>We now consider models with feature extraction enabled. We assume that A knows the input space M, but not the training data distribution. For one-hot-encoding, knowledge of M suffices to apply the same encoding locally. For quantile binning however, applying ex locally requires knowledge of the training data quantiles. To reverse-engineer the binning transformation, we use linesearches similar to those we used for decision trees: For each numeric feature, we search the feature's range in input space for thresholds (up to a granularity ?) where f 's output changes. This indicates our value landed in an adjacent bin, with a different learned regression coefficient. Note that learning the bin boundaries may be interesting in its own right, as it leaks information about the training data distribution. Having found the bin boundaries, we can apply both one-hot-encoding and binning locally, and extract f over its feature space. As we are restricted to queries over M, we cannot define an arbitrary system of equations over X . Building a well-determined and consistent system can be difficult, as the encoding ex generates sparse inputs over X . However, Amazon facilitates this process with the way it handles queries with missing features: if a feature is omitted from a query, all corresponding features in X are set to 0. For a linear model for instance, we can trivially re-construct the model by issuing queries with a single feature specified, such as to obtain equations with a single unknown in X .</p><p>We trained models for the Circles, Iris and Adult data sets, with Amazon's default feature-extraction settings. Table <ref type="table" target="#tab_8">7</ref> shows the results of our attacks, for the reverseengineering of ex and extraction of f . For binary models (Circles and Adult), we use d + 1 queries to solve a linear equation-system over X . For models with c &gt; 2 classes, we use c ? (d + 1) queries. In all cases, the extracted model matches f on 100% of tested inputs. To optimize the query complexity, the queries we use to find quantile bins are re-used for equation-solving. As line searches require adaptive queries, we do not use batch predictions. However, even for the Digits model, we resorted to using real-time predictions, because of the service's significant overhead in evaluating batches. For attacks that require a large number of non-adaptive queries, we expect batch predictions to be faster than real-time predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Additional feature extractors. In some ML services we considered, users may enable further feature extractors. A common transformation is feature scaling or normalization. If A has access to training data statistics (as provided by BigML for instance), applying the transformation locally is trivial. More generally, for models with a linear input layer (i.e., logistic regressions, linear SVMs, MLPs) the scaling or normalization can be seen as being applied to the learned weights, rather than the input features. We can thus view the composition f ? ex as a model f that operates over the 'un-scaled' input space M and extract f directly using equation-solving.</p><p>Further extractors include text analysis (e.g., bag-ofwords or n-gram models) and Cartesian products (grouping many features into one). We have not analyzed these in this work, but we believe that they could also be easily reverse-engineered, especially given some training data statistics and the ability to make incomplete queries.</p><p>Learning unknown model classes or hyper-parameters. For our online attacks, we obtained information about the model class of f , the enabled feature extraction ex, and other hyper-parameters, directly from the ML service or its documentation. More generally, if A does not have full certainty about certain model characteristics, it may be able to narrow down a guess to a small range. Model hyper-parameters for instance (such as the free parameter of an RBF kernel) are typically chosen through cross-validation over a default range of values.</p><p>Given a set of attack strategies with varying assumptions, A can use a generic extract-and-test approach: each attack is applied in turn, and evaluated by computing R test or R unif over a chosen set of points. The adversary succeeds if any of the strategies achieves a low error. Note that A needs to interact with the model f only once, to obtain responses for a chosen set of extraction samples and test samples, that can be re-used for each strategy.</p><p>Our attacks on Amazon's service followed this approach: We first formulated guesses for model characteristics left unspecified by the documentation (e.g., we found no mention of one-hot-encoding, or of how missing inputs are handled). We then evaluated our assumptions with successive extraction attempts. Our results indicate that Amazon uses softmax regression and does not create binary predictors for missing values. Interestingly, BigML takes the 'opposite' approach (i.e., BigML uses OvR regression and adds predictors for missing values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Extraction Given Class Labels Only</head><p>The successful attacks given in Sections 4 and 5 show the danger of revealing confidence values. While current ML services have been designed to reveal rich information, our attacks may suggest that returning only labels would be safer. Here we explore model extraction in a setting with no confidence scores. We will discuss further countermeasures in Section 7. We primarily focus on settings where A can make direct queries to an API, i.e., queries for arbitrary inputs x ? X . We briefly discuss indirect queries in the context of linear classifiers.</p><p>The Lowd-Meek attack. We start with the prior work of Lowd and Meek <ref type="bibr" target="#b35">[36]</ref>. They present an attack on any linear classifier, assuming black-box oracle access with membership queries that return just the predicted class label. A linear classifier is defined by a vector w ? R d and a constant ? ? R, and classifies an instance x as positive if w ? x + ? &gt; 0 and negative otherwise. SVMs with linear kernels and binary LRs are examples of linear classifiers. Their attack uses line searches to find points arbitrarily close to f 's decision boundary (points for which w ? x + ? ? 0), and extracts w and ? from these samples.</p><p>This attack only works for linear binary models. We describe a straightforward extension to some non-linear models, such as polynomial kernel SVMs. Extracting a polynomial kernel SVM can be reduced to extracting a linear SVM in the transformed feature space. Indeed, for any kernel K poly (x, x )=(x T ? x + 1) d , we can derive a projection function ? (?), so that K poly (x, x )=? (x) T ? ? (x ). This transforms the kernel SVM into a linear one, since the decision boundary now becomes w F ? ? (x) + ? = 0 where w F = ? t i=1 ? i ? (x i ). We can use the Lowd-Meek attack to extract w F and ? as long as ? (x) and its inverse are feasible to compute; this is unfortunately not the case for the more common RBF kernels. <ref type="foot" target="#foot_3">3</ref>The retraining approach. In addition to evaluating the Lowd-Meek attack against ML APIs, we introduce a number of other approaches based on the broad strategy of re-training a model locally, given input-output examples. Informally, our hope is that by extracting a model that achieves low training error over the queried samples, we would effectively approximate the target model's decision boundaries. We consider three retraining strategies, described below. We apply these to the model classes that we previously extracted using equation-solving attacks, as well as to SVMs. <ref type="foot" target="#foot_4">4</ref>(1) Retraining with uniform queries. This baseline strategy simply consists in sampling m points x i ? X uniformly at random, querying the oracle, and training a model f on these samples.  <ref type="table">3</ref>. The left shows R test and the right shows R unif .</p><p>(2) Line-search retraining. This strategy can be seen as a model-agnostic generalization of the Lowd-Meek attack. It issues m adaptive queries to the oracle using line search techniques, to find samples close to the decision boundaries of f . A model f is then trained on the m queried samples. (3) Adaptive retraining. This strategy applies techniques from active learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b46">47]</ref>. For some number r of rounds and a query budget m, it first queries the oracle on m r uniform points, and trains a model f . Over a total of r rounds, it then selects m r new points, along the decision boundary of f (intuitively, these are points f is least certain about), and sends those to the oracle before retraining f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Linear Binary Models</head><p>We first explore how well the various approaches work in settings where the Lowd-Meek attack can be applied. We evaluate their attack and our three retraining strategies for logistic regression models trained over the binary data sets shown in Table <ref type="table">3</ref>. These models have d + 1 parameters, and we vary the query budget as ? ? (d + 1), for 0.5 ? ? ? 100. Figure <ref type="figure" target="#fig_3">4</ref> displays the average errors R test and R unif over all models, as a function of ?.</p><p>The retraining strategies that search for points near the decision boundary clearly perform better than simple uniform retraining. The adaptive strategy is the most efficient of our three strategies. For relatively low budgets, it even outperforms the Lowd-Meek attack. However, for budgets large enough to run line searches in each dimension, the Lowd-Meek attack is clearly the most efficient.</p><p>For the models we trained, about 2,050 queries on average, and 5,650 at most, are needed to run the Lowd-Meek attack effectively. This is 50? more queries than what we needed for equation-solving attacks. With 827 queries on average, adaptive retraining yields a model f that matches f on over 99% of tested inputs. Thus, even if an ML API only provides class labels, efficient extrac-tion attacks on linear models remain possible.</p><p>We further consider a setting where feature-extraction (specifically one-hot-encoding of categorical features) is applied by the ML service, rather than by the user. A is then limited to indirect queries in input space. Lowd and Meek <ref type="bibr" target="#b35">[36]</ref> note that their extraction attack does not work in this setting, as A can not run line searches directly over X . In contrast, for the linear models we trained, we observed no major difference in extraction accuracy for the adaptive-retraining strategy, when limited to queries over M. We leave an in-depth study of model extraction with indirect queries, and class labels only, for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Multiclass LR Models</head><p>The Lowd-Meek attack is not applicable in multiclass (c &gt; 2) settings, even when the decision boundary is a combination of linear boundaries (as in multiclass regression) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50]</ref>. We thus focus on evaluating the three retraining attacks we introduced, for the type of ML models we expect to find in real-world applications.</p><p>We focus on softmax models here, as softmax and onevs-rest models have identical output behaviors when only class labels are provided: in both cases, the class label for an input x is given by argmax i (w i ? x + ? i ). From an extractor's perspective, it is thus irrelevant whether the target was trained using a softmax or OvR approach.</p><p>We evaluate our attacks on softmax models trained on the multiclass data sets shown in Table <ref type="table">3</ref>. We again vary the query budget as a factor ? of the number of model parameters, namely ? ? c ? (d + 1). Results are displayed in Figure <ref type="figure">5</ref>. We observe that the adaptive strategy clearly performs best and that the line-search strategy does not improve over uniform retraining, possibly because the line-searches have to be split across multiple decisionboundaries. We further note that all strategies achieve lower R test than R unif . It thus appears that for the models we trained, points from the test set are on average 'far' from the decision boundaries of f (i.e., the trained models separate the different classes with large margins).</p><p>For all models, 100 ? c ? (d + 1) queries resulted in extraction accuracy above 99.9%. This represents 26,000 queries on average, and 65,000 at the most (Digits data set). Our equation-solving attacks achieved similar or better results with 100? less queries. Yet, for scenarios with high monetary incentives (e.g., intrusion detector evasion), extraction attacks on MLR models may be attractive, even if APIs only provide class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Neural Networks</head><p>We now turn to attacks on more complex deep neural networks. We expect these to be harder to retrain than multiclass regressions, as deep networks have more pa- rameters and non-linear decision-boundaries. Therefore, we may need to find a large number of points close to a decision boundary in order to extract it accurately.</p><p>We evaluated our attacks on the multiclass models from Table <ref type="table">3</ref>. For the tested query budgets, line-search and adaptive retraining gave little benefit over uniform retraining. For a budget of 100 ? k, where k is the number of model parameters, we get R test = 99.16% and R unif = 98.24%, using 108,200 queries per model on average. Our attacks might improve for higher budgets but it is unclear whether they would then provide any monetary advantage over using ML APIs in an honest way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">RBF Kernel SVMs</head><p>Another class of nonlinear models that we consider are support-vector machines (SVMs) with radial-basis function (RBF) kernels. A kernel SVM first maps inputs into a higher-dimensional space, and then finds the hyperplane that maximally separates the two classes. As mentioned in Section 6, SVMs with polynomial kernels can be extracted using the Lowd-Meek attack in the transformed feature space. For RBF kernels, this is not possible because the transformed space has infinite dimension.</p><p>SVMs do not provide class probability estimates. Our only applicable attack is thus retraining. As for linear models, we vary the query budget as ? ? (d + 1), where d is the input dimension. We further use the extract-andtest approach from Section 5 to find the value of the RBF kernel's hyper-parameter. Results of our attacks are in Figure <ref type="figure">6</ref>. Again, we see that adaptive retraining performs best, even though the decision boundary to extract is nonlinear (in input space) here. Kernel SVMs models are overall harder to retrain than models with linear decision boundaries. Yet, for our largest budgets (2,050 queries on average), we do extract models with over 99% accuracy, which may suffice in certain adversarial settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Extraction Countermeasures</head><p>We have shown in Sections 4 and 5 that adversarial clients can effectively extract ML models given access to rich prediction APIs. Given that this undermines the financial models targeted by some ML cloud services, and potentially leaks confidential training data, we believe researchers should seek countermeasures.</p><p>In Section 6, we analyzed the most obvious defense against our attacks: prediction API minimization. The constraint here is that the resulting API must still be useful in (honest) applications. For example, it is simple to change APIs to not return confidences and not respond to incomplete queries, assuming applications can get by without it. This will prevent many of our attacks, most notably the ones described in Section 4 as well as the feature discovery techniques used in our Amazon case study (Section 5). Yet, we showed that even if we strip an API to only provide class labels, successful attacks remain possible (Section 6), albeit at a much higher query cost.</p><p>We discuss further potential countermeasures below.</p><p>Rounding confidences. Applications might need confidences, but only at lower granularity. A possible defense is to round confidence scores to some fixed precision <ref type="bibr" target="#b22">[23]</ref>. We note that ML APIs already work with some finite precision when answering queries. For instance, BigML reports confidences with 5 decimal places, and Amazon provides values with 16 significant digits.</p><p>To understand the effects of limiting precision further, we re-evaluate equation-solving and decision tree pathfinding attacks with confidence scores rounded to a fixed decimal place. For equation-solving attacks, rounding the class probabilities means that the solution to the obtained equation-system might not be the target f , but some truncated version of it. For decision trees, rounding confidence scores increases the chance of node id collisions, and thus decreases our attacks' success rate.</p><p>Figure <ref type="figure" target="#fig_5">7</ref> shows the results of experiments on softmax models, with class probabilities rounded to 2-5 decimals. We plot only R test , the results for R unif being similar. We observe that class probabilities rounded to 4 or 5 decimal places (as done already in BigML) have no effect on the attack's success. When rounding further to 3 and 2 decimal places, the attack is weakened, but still vastly outperforms adaptive retraining using class labels only.  <ref type="table">3</ref>), as we vary the number of significant digits in reported class probabilities. Extraction with no rounding and with class labels only (adaptive retraining) are added for comparison.</p><p>For regression trees, rounding has no effect on our attacks. Indeed, for the models we considered, the output itself is unique in each leaf (we could also round outputs, but the impact on utility may be more critical). For classification trees, we re-evaluated our top-down attack, with confidence scores rounded to fewer than 5 decimal places. The attacks on the 'IRS Tax Patterns' and 'Email Importance' models are the most resilient, and suffer no success degradation before scores are rounded to 2 decimal places. For the other models, rounding confidences to 3 or 4 decimal places severely undermines our attack. Differential privacy. Differential privacy (DP) <ref type="bibr" target="#b21">[22]</ref> and its variants <ref type="bibr" target="#b33">[34]</ref> have been explored as mechanisms for protecting, in particular, the privacy of ML training data <ref type="bibr" target="#b53">[54]</ref>. DP learning has been applied to regressions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b55">56]</ref>, SVMs <ref type="bibr" target="#b43">[44]</ref>, decision trees <ref type="bibr" target="#b30">[31]</ref> and neural networks <ref type="bibr" target="#b47">[48]</ref>. As some of our extraction attacks leak training data information (Section 4.1.3), one may ask whether DP can prevent extraction, or at least reduce the severity of the privacy violations that extraction enables.</p><p>Consider na?ve application of DP to protect individual training data elements. This should, in theory, decrease the ability of an adversary A to learn information about training set elements, when given access to prediction queries. One would not expect, however, that this prevents model extraction, as DP is not defined to do so: consider a trivially useless learning algorithm for binary logistic regression, that discards the training data and sets w and ? to 0. This algorithm is differentially private, yet w and ? can easily be recovered using equation-solving.</p><p>A more appropriate strategy would be to apply DP directly to the model parameters, which would amount to saying that a query should not allow A to distinguish between closely neighboring model parameters. How exactly this would work and what privacy budgets would be required is left as an open question by our work.</p><p>Ensemble methods. Ensemble methods such as random forests return as prediction an aggregation of pre-dictions by a number of individual models. While we have not experimented with ensemble methods as targets, we suspect that they may be more resilient to extraction attacks, in the sense that attackers will only be able to obtain relatively coarse approximations of the target function. Nevertheless, ensemble methods may still be vulnerable to other attacks such as model evasion <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Our work is related to the extensive literature on learning theory, such as PAC learning <ref type="bibr" target="#b52">[53]</ref> and its variants <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. Indeed, extraction can be viewed as a type of learning, in which an unknown instance of a known hypothesis class (model type) is providing labels (without error). This is often called learning with membership queries <ref type="bibr" target="#b2">[3]</ref>. Our setting differs from these in two ways. The first is conceptual: in PAC learning one builds algorithms to learn a concept -the terminology belies the motivation of formalizing learning from data. In model extraction, an attacker is literally given a function oracle that it seeks to illicitly determine. The second difference is more pragmatic: prediction APIs reveal richer information than assumed in prior learning theory work, and we exploit that.</p><p>Algorithms for learning with membership queries have been proposed for Boolean functions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> and various binary classifiers <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref>. The latter line of work, initiated by Lowd and Meek <ref type="bibr" target="#b35">[36]</ref>, studies strategies for model evasion, in the context of spam or fraud detectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b54">55]</ref>. Intuitively, model extraction seems harder than evasion, and this is corroborated by results from theory <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref> and practice <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Evasion attacks fall into the larger field of adversarial machine learning, that studies machine learning in general adversarial settings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>. In that context, a number of authors have considered strategies and defenses for poisoning attacks, that consist in injecting maliciously crafted samples into a model's train or test data, so as to decrease the learned model's accuracy <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>In a non-malicious setting, improper model extraction techniques have been applied for interpreting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b51">52]</ref> and compressing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref> complex neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We demonstrated how the flexible prediction APIs exposed by current ML-as-a-service providers enable new model extraction attacks that could subvert model monetization, violate training-data privacy, and facilitate model evasion. Through local experiments and online attacks on two major providers, BigML and Amazon, we illustrated the efficiency and broad applicability of attacks that exploit common API features, such as the availability of confidence scores or the ability to query arbitrary partial inputs. We presented a generic equationsolving attack for models with a logistic output layer and a novel path-finding algorithm for decision trees.</p><p>We further explored potential countermeasures to these attacks, the most obvious being a restriction on the information provided by ML APIs. Building upon prior work from learning-theory, we showed how an attacker that only obtains class labels for adaptively chosen inputs, may launch less effective, yet potentially harmful, retraining attacks. Evaluating these attacks, as well as more refined countermeasures, on production-grade ML services is an interesting avenue for future work.</p><p>A kernel is a function K : X ?X ? R. Typical kernels include the quadratic kernel K quad (x, x ) = (x T ? x + 1) 2 and the Gaussian radial basis function (RBF) kernel K rbf (x, x ) = e -?||x-x || 2 , parameterized by a value ? ? R. A kernel's projection function is a map ? defined by K(x, x ) = ? (x) ? ? (x ). We do not use ? explicitly, indeed for RBF kernels this produces an infinite-dimension vector. Instead, classification is defined using a "kernel trick": f (x) = sign([? t i=1 ? i K(x, x i )] + ? ) where ? is again a learned threshold, ? 1 ,. ..,? t are learned weights, and x 1 ,. ..,x t are feature vectors of inputs from a training set. The x i for which ? i = 0 are called support vectors. Note that for non-zero ? i , it is the case that ? i &lt; 0 if the training-set label of x i was zero and ? i &gt; 0 otherwise. Logistic regression. SVMs do not directly generalize to multiclass settings c &gt; 2, nor do they output class probabilities. Logistic regression (LR) is a popular classifier that does. A binary LR model is defined as</p><formula xml:id="formula_9">f 1 (x) = ? (w ? x + ? ) = 1/(1 + e -(w?x+? ) ) and f 0 (x) = 1 -f 1 (x). A class label is chosen as 1 iff f 1 (x) &gt; 0.5.</formula><p>When c &gt; 2, one fixes c weight vectors w 0 ,. ..,w c-1 each in R d , thresholds ? 0 ,. ..,? c-1 in R and defines</p><formula xml:id="formula_10">f i (x) = e w i ?x+? i /(? c-1 j=0 e w j ?x+? j ) for i ? Z c .</formula><p>The class label is taken to be argmax i f i (x). Multiclass regression is referred to as multinomial or softmax regression. An alternative approach to softmax regression is to build a binary model ? (w i ? x + ? i ) per class in a one-vs-rest fashion and then set f i (x) = ? (w i ? x + ? i )/? j ? (w j ? x + ? j ).</p><p>These are log-linear models, and may not be suitable for data that is not linearly separable in X . Again, one may use kernel techniques to deal with more complex data relationships (c.f., <ref type="bibr" target="#b56">[57]</ref>). Then, one replaces</p><formula xml:id="formula_11">w i ? x + ? i with ? t r=1 ? i,r K(x, x r ) + ? i .</formula><p>As written, this uses the entire set of training data points x 1 ,. ..,x t as socalled representors (here analogous to support vectors). Unlike with SVMs, where most training data set points will never end up as support vectors, here all training set points are potentially representors. In practice one uses a size s &lt; t random subset of training data <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep neural networks.</head><p>A popular way of extending softmax regression to handle data that is non linearly separable in X is to first apply one or more non-linear transformations to the input data. The goal of these hidden layers is to map the input data into a (typically) lowerdimensional space in which the classes are separable by the softmax layer. We focus here on fully connected networks, also known as multilayer perceptrons, with a single hidden layer. The hidden layer consists of a number h of hidden nodes, with associated weight vectors w The i-th hidden unit applies a non linear transformation h i (x) = g(w</p><formula xml:id="formula_12">(1) i ? x + ? (1)</formula><p>i ), where g is an activation function such as tanh or ? . The vector h(x) ? R h is then input into a softmax output layer with weight vectors w</p><formula xml:id="formula_13">(2) 0 ,. ..,w (2) c-1 in R h and thresholds ? (2) 0 ,. ..,? (2)</formula><p>c-1 in R. Decision trees. A decision tree T is a labeled tree. Each internal node v is labeled by a feature index i ? {1,. ..,d} and a splitting function ? : X i ? Z k v , where k v ? 2 denotes the number of outgoing edges of v.</p><p>On an input x = (x 1 , x 2 ,. ..,x d ), a tree T defines a computation as follows, starting at the root. When we reach a node v, labeled by {i, ?}, we proceed to the child of v indexed by ?(x i ). We consider three types of splitting functions ? that are typically used in practice ( <ref type="bibr" target="#b10">[11]</ref>):</p><p>(1) The feature x i is categorical with</p><formula xml:id="formula_14">X i = Z k . Let {S, T } be some partition of Z k . Then k v = 2 and ?(x i ) = 0 if x i ? S and ?(x i ) = 1 if x i ? T . This is a binary split on a categorical feature. (2) The feature x i is categorical with X i = Z k . We have k v = k and ?(x i ) = x i .</formula><p>This corresponds to a k-ary split on a categorical feature of arity k. (3) The feature x i is continuous with X i = [a, b]. Let a &lt; t &lt; b be a threshold. Then k v = 2 and ?(x i ) = 0 if x i ? t and ?(x i ) = 1 if x i &gt; t. This is a binary split on a continuous feature with threshold t. When we reach a leaf, we terminate and output that leaf's value. This value can be a class label, or a class label and confidence score. This defines a function f : X ? Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details on Data Sets</head><p>Here we give some more information about the data sets we used in this work. Refer back to Table <ref type="table">3</ref> and<ref type="table" target="#tab_6">Table 5</ref>.</p><p>Synthetic data sets. We used 4 synthetic data sets from scikit <ref type="bibr" target="#b41">[42]</ref>. The first two data sets are classic examples of non-linearly separable data, consisting of two concentric Circles, or two interleaving Moons. The next two synthetic data sets, Blobs and 5-Class, consist of Gaussian clusters of points assigned to either 3 or 5 classes.</p><p>Public data sets. We gathered a varied set of data sets representative of the type of data we would expect ML service users to use to train logistic and SVM based models. These include famous data sets used for supervised learning, obtained from the UCI ML repository (Adult, Iris, Breast Cancer, Mushrooms, Diabetes). We also consider the Steak and GSS data sets used in prior work on model inversion <ref type="bibr" target="#b22">[23]</ref>. Finally, we add a data set of digits available in scikit, to visually illustrate training data leakage in kernelized logistic models (c.f. Section 4. <ref type="bibr">1.3)</ref>.</p><p>Public data sets and models from BigML. For experiments on decision trees, we chose a varied set of models publicly available on BigML's platform. These models were trained by real MLaaS users and they cover a wide range of application scenarios, thus providing a realistic benchmark for the evaluation of our extraction attacks.</p><p>The IRS model predicts a US state, based on administrative tax records. The Steak and GSS models respectively predict a person's preferred steak preparation and happiness level, from survey and demographic data. These two models were also considered in <ref type="bibr" target="#b22">[23]</ref>. The Email Importance model predicts whether Gmail classifies an email as 'important' or not, given message metadata. The Email Spam model classifies emails as spam, given the presence of certain words in its content. The German Credit data set was taken from the UCI library <ref type="bibr" target="#b34">[35]</ref> and classifies a user's loan risk. Finally, two regression models respectively predict Medical Charges in the US based on state demographics, and the Bitcoin Market Price from daily opening and closing values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis of the Path-Finding Algorithm</head><p>In this section, we analyze the correctness and complexity of the decision tree extraction algorithm in Algorithm 1. We assume that all leaves are assigned a unique id by the oracle O, and that no continuous feature is split into intervals of width smaller than ?. We may use id to refer directly to the leaf with identity id.</p><p>Correctness. Termination of the algorithm follows immediately from the fact that new queries are only added to Q when a new leaf is visited. As the number of leaves in the tree is bounded, the algorithm must terminate.</p><p>We prove by contradiction that all leaves are eventually visited. Let the depth of a node v, denote the length of the path from v to the root (the root has depth 0). For two leaves id, id , let A be their deepest common ancestor (A is the deepest node appearing on both the paths of id and id ). We denote the depth of A as ?(id, id ).</p><p>Suppose Algorithm 1 terminates without visiting all leaves, and let (id, id ) be a pair of leaves with maximal ?(id, id ), such that id was visited but id was not. Let x i be the feature that their deepest common ancestor A splits on. When id is visited, the algorithm calls LINE SEARCH or CATEGORY SPLIT on feature x i . As all leaf ids are unique and there are no intervals smaller than ?, we will discover a leaf in each sub-tree rooted at A, including the one that contains id . Thus, we visit a leaf id for which ?(id , id ) &gt; ?(id, id ), a contradiction. For the special case of boolean trees, the complexity is O(m ? d). In comparison, the algorithm of <ref type="bibr" target="#b32">[33]</ref>, that uses membership queries only, has a complexity polynomial in d and 2 ? , where ? is the tree depth. For degenerate trees, 2 ? can be exponential in m, implying that the assumption of unique leaf identities (obtained from confidence scores for instance) provides an exponential speedup over the best-known approach with class labels only. The algorithm from <ref type="bibr" target="#b32">[33]</ref> can be extended to regression trees, with a complexity polynomial in the size of the output range Y. Again, under the assumption of unique leaf identities (which could be obtained solely from the output values) we obtain a much more efficient algorithm, with a complexity independent of the output range. The Top-Down Approach. The correctness and complexity of the top-down algorithm from Section 4.2 (which uses incomplete queries), follow from a similar analysis. The main difference is that we assume that all nodes have a unique id, rather than only the leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D A Note on Improper Extraction</head><p>To extract a model f , without knowledge of the model class, a simple strategy is to extract a multilayer perceptron f with a large enough hidden layer. Indeed, feedforward networks with a single hidden layer can, in principle, closely approximate any continuous function over a bounded subset of R d <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>However, this strategy intuitively does not appear to be optimal. Even if we know that we can find a multilayer perceptron f that closely matches f , f might have a far more complex representation (more parameters) than f . Thus, tailoring the extraction to the 'simpler' model class of the target f appears more efficient. In learning theory, the problem of finding a succinct representation of some target model f is known as Occam Learning <ref type="bibr" target="#b12">[13]</ref>.</p><p>Our experiments indicate that such generic improper extraction indeed appears sub-optimal, in the context of equation-solving attacks. We train a softmax regression over the Adult data set with target "Race". The model f is defined by 530 real-valued parameters. As shown in Section 4.1.2, using only 530 queries, we extract a model f from the same model class, that closely matches f ( f and f predict the same labels on 100% of tested inputs, and produce class probabilities that differ by less than 10 -7 in TV distance). We also extracted the same model, assuming a multilayer perceptron target class. Even with 1,000 hidden nodes (this model has 111,005 parameters), and 10? more queries <ref type="bibr" target="#b4">(5,</ref><ref type="bibr">300)</ref>, the extracted model f is a weaker approximation of f (99.5% accuracy for class labels and TV distance of 10 -2 for class probabilities).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagram of ML model extraction attacks. A data owner</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training data leakage in KLR models. (a) Displays 5 of 20 training samples used as representers in a KLR model (top) and 5 of 20 extracted representers (bottom). (b) For a second model, shows the average of all 1,257 representers that the model classifies as a 3, 4, 5, 6 or 7 (top) and 5 of 10 extracted representers (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Decision tree over features Color and Size. Shows the path (thick green) to leaf id 2 on input x = {Size = 50, Color = R}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average error of extracted linear models. Results are for different extraction strategies applied to models trained on all binary data sets from Table3. The left shows R test and the right shows R unif .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Average error of extracted softmax models. Results arefor three retraining strategies applied to models trained on all multiclass data sets from Table3. The left shows R test and the right shows R unif .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of rounding on model extraction. Shows the average test error of equation-solving attacks on softmax models trained on the benchmark suite (Table3), as we vary the number of significant digits in reported class probabilities. Extraction with no rounding and with class labels only (adaptive retraining) are added for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 1 )</head><label>1</label><figDesc>0 ,. ..,w(1) h-1 in R d and thresholds ? (1) 0 ,. ..,? (1) h-1 in R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Complexity.</head><label></label><figDesc>Let m denote the number of leaves in the tree. Each leaf is visited exactly once, and for each leaf we check all d features. Suppose continuous features have range [0, b], and categorical features have arity k. For continuous features, finding one threshold takes at most log 2 ( b ? ) queries. As the total number of splits on one feature is at most m (i.e., all nodes split on the same feature), finding all thresholds uses at most m ? log 2 ( b ? ) queries. Testing a categorical feature uses k queries. The total query complexity is O(m? (d cat ? k + d cont ? m ? log( b ? )), where d cat and d cont represent respectively the number of categorical and continuous features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.11,630.00,272.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,533.00,630.00,269.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of model extraction attacks on ML services. For each target model, we report the number of prediction queries made to the ML API in an attack that extracts a 100% equivalent model. The attack time is primarily influenced by the service's prediction latency (? 100ms/query for Amazon and ? 500ms/query for BigML).</figDesc><table><row><cell cols="2">Service Model Type</cell><cell>Data set</cell><cell cols="2">Queries Time (s)</cell></row><row><cell>Amazon</cell><cell cols="2">Logistic Regression Digits Logistic Regression Adult</cell><cell>650 1,485</cell><cell>70 149</cell></row><row><cell>BigML</cell><cell>Decision Tree Decision Tree</cell><cell>German Credit Steak Survey</cell><cell>1,150 4,013</cell><cell>631 2,088</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Particularities of major MLaaS providers. 'White-box' refers to the ability to download and use a trained model locally, and 'Monetize' means that a user may charge other users for black-box access to her models. Model support for each service is obtained from available documentation. The models listed for Google's API are a projection based on the announced support of models in standard PMML format<ref type="bibr" target="#b24">[25]</ref>. Details on ML models are given in Appendix A.</figDesc><table><row><cell>model by either choosing one of many supported model</cell></row><row><cell>classes (as in BigML, Microsoft, and PredictionIO) or</cell></row><row><cell>having the service choose an appropriate model class (as</cell></row><row><cell>in Amazon and Google). Two services have also an-</cell></row><row><cell>nounced upcoming support for users to upload their own</cell></row><row><cell>trained models (Google) and their own custom learning</cell></row><row><cell>algorithms (PredictionIO). When training a model, users</cell></row><row><cell>may tune various parameters of the model or training-</cell></row><row><cell>algorithm (e.g., regularizers, tree size, learning rates) and</cell></row><row><cell>control feature-extraction and transformation methods.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Success of equation-solving attacks. Models to extract were trained on the Adult data set with multiclass target 'Race'. For each model, we report the number of unknown model parameters, the number of queries used, and the running time of the equation solver. The attack on the MLP with 11,125 queries converged after 490 epochs.</figDesc><table><row><cell>Softmax</cell><cell>530</cell><cell cols="3">265 530 100.00% 100.00% 99.96% 99.75%</cell><cell>2.6 3.1</cell></row><row><cell>OvR</cell><cell>530</cell><cell cols="3">265 530 100.00% 100.00% 99.98% 99.98%</cell><cell>2.8 3.5</cell></row><row><cell></cell><cell></cell><cell>1,112</cell><cell>98.17%</cell><cell>94.32%</cell><cell>155</cell></row><row><cell>MLP</cell><cell>2,225</cell><cell>2,225 4,450</cell><cell>98.68% 99.89%</cell><cell>97.23% 99.82%</cell><cell>168 195</cell></row><row><cell></cell><cell></cell><cell>11,125</cell><cell>99.96%</cell><cell>99.99%</cell><cell>89</cell></row></table><note><p><p><p><p>Model</p>Unknowns Queries</p>1 -R test 1 -R unif Time (s)</p>verges to a global minimum (i.e., a function f that predicts the same probabilities as f for all available samples). A similar optimization (over class labels rather than probabilities) is actually used for training logistic models. Any MLR implementation can thus easily be adapted for model extraction with equation-solving.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Data sets used for decision tree extraction. Trained trees for these data sets are available in BigML's public gallery. The last two data sets are used to train regression trees.</figDesc><table><row><cell>Data set</cell><cell># records</cell><cell># classes</cell><cell># features</cell></row><row><cell>IRS Tax Patterns</cell><cell>191,283</cell><cell>51</cell><cell>31</cell></row><row><cell>Steak Survey</cell><cell>430</cell><cell>5</cell><cell>12</cell></row><row><cell>GSS Survey</cell><cell>51,020</cell><cell>3</cell><cell>7</cell></row><row><cell>Email Importance</cell><cell>4,709</cell><cell>2</cell><cell>14</cell></row><row><cell>Email Spam</cell><cell>4,601</cell><cell>2</cell><cell>46</cell></row><row><cell>German Credit</cell><cell>1,000</cell><cell>2</cell><cell>11</cell></row><row><cell>Medical Cover Bitcoin Price</cell><cell>163,065 1,076</cell><cell>Y = R Y = R</cell><cell>13 7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Results of model extraction attacks on Amazon. OHE stands for one-hot-encoding. The reported query count is the number used to find quantile bins (at a granularity of 10 -3 ), plus those queries used for equation-solving. Amazon charges $0.0001 per prediction<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="2">OHE Binning</cell><cell>Queries</cell><cell>Time (s)</cell><cell>Price ($)</cell></row><row><cell>Circles</cell><cell>-</cell><cell>Yes</cell><cell>278</cell><cell>28</cell><cell>0.03</cell></row><row><cell>Digits</cell><cell>-</cell><cell>No</cell><cell>650</cell><cell>70</cell><cell>0.07</cell></row><row><cell>Iris</cell><cell>-</cell><cell>Yes</cell><cell>644</cell><cell>68</cell><cell>0.07</cell></row><row><cell>Adult</cell><cell>Yes</cell><cell>Yes</cell><cell>1,485</cell><cell>149</cell><cell>0.15</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We simulated victims by training models in our own accounts. We have disclosed our results to affected services in February</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2016" xml:id="foot_1"><p></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Note that for some D, it is possible that f predicts true labels better than f , yet R test ( f , f ) is large, because f does not closely match f .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>We did explore using approximations of ? , but found that the adaptive re-training techniques discussed in this section perform better.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p><ref type="bibr" target="#b3">4</ref> We do not expect retraining attacks to work well for decision trees, because of the greedy approach taken by learning algorithms. We have not evaluated extraction of trees, given class labels only, in this work.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. We thank <rs type="person">Mart?n Abadi</rs> and the anonymous reviewers for their comments. This work was supported by <rs type="funder">NSF</rs> grants <rs type="grantNumber">1330599</rs>, <rs type="grantNumber">1330308</rs>, and <rs type="grantNumber">1546033</rs>, as well as a generous gift from Microsoft.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DEfCuDR">
					<idno type="grant-number">1330599</idno>
				</org>
				<org type="funding" xml:id="_Q9ZA4kC">
					<idno type="grant-number">1330308</idno>
				</org>
				<org type="funding" xml:id="_nqcwpZh">
					<idno type="grant-number">1546033</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Web</forename><surname>Amazon</surname></persName>
		</author>
		<author>
			<persName><surname>Services</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/machine-learning" />
		<imprint>
			<date type="published" when="2016-02-10">Feb. 10, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Survey and critique of techniques for extracting rules from trained artificial neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tickle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KBS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="373" to="389" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Queries and concept learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="319" to="342" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spognardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Villani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vitali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Felici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJSN</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><surname>At&amp;t</surname></persName>
		</author>
		<author>
			<persName><surname>Cambridge</surname></persName>
		</author>
		<ptr target="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html" />
		<title level="m">The ORL database of faces</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Can machine learning be secure?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASIACCS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A technique for upper bounding the spectral norm with applications to learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bellare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="62" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learnability with respect to fixed distributions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Itai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCS</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="377" to="389" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>?rndi ?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Poisoning attacks against support vector machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bigml</surname></persName>
		</author>
		<ptr target="https://www.bigml.com" />
		<imprint>
			<date type="published" when="2016-02-10">Feb. 10, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selection of relevant features and examples in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="245" to="271" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Occam&apos;s razor. Readings in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ehrenfeucht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>War-Muth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="201" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exact learning boolean functions via the monotone theory</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Bshouty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Comp</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="146" to="153" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bucilu ?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Privacy-preserving logistic regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving generalization with active learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="201" to="221" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extracting treestructured representations of trained networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MCSS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICALP</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1322" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Privacy in pharmacogenetics: An endto-end case study of personalized Warfarin dosing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Api</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/prediction" />
		<imprint>
			<date type="published" when="2016-02-10">Feb. 10, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hickey</surname></persName>
		</author>
		<ptr target="http://fivethirtyeight.com/datalab/how-americans-like-their-steak" />
		<title level="m">How Americans Like their Steak</title>
		<imprint>
			<date type="published" when="2014-02-10">2014. Feb. 10, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adversarial machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="43" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An efficient membership-query algorithm for learning DNF with respect to the uniform distribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A practical differentially private random decision tree classifier</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pillaipakkamnatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDMW</title>
		<imprint>
			<biblScope unit="page" from="114" to="121" />
			<date type="published" when="2009">2009</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online anomaly detection under adversarial impact</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="page" from="405" to="412" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning decision trees using the Fourier spectrum</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kushilevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SICOMP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1331" to="1348" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Membership privacy: A unifying framework for privacy definitions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qardaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AND MEEK, C. Adversarial learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">AND MEEK, C. Good word attacks on statistical spam filters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CEAS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Microsoft Azure</surname></persName>
		</author>
		<ptr target="https://azure.microsoft.com/services/machine-learning" />
		<imprint>
			<date type="published" when="2016-02-10">Feb. 10, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Query strategies for evading convex-inducing classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1293" to="1332" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Paragraph: Thwarting signature learning by training maliciously</title>
		<author>
			<persName><forename type="first">J</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="81" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Numerical optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pretten-Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pas-Sos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Predictionio</surname></persName>
		</author>
		<ptr target="http://prediction.io" />
		<imprint>
			<date type="published" when="2016-02-10">Feb. 10, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning in a large function space: Privacy-preserving mechanisms for SVM learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JPC</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Antidote: understanding and defending against poisoning of anomaly detectors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Handling missing values when applying classification models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saar-Tsechansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Active learning literature survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Madison</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1310" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">General social surveys</title>
		<imprint>
			<date type="published" when="1972">1972-2012, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the hardness of evading combinations of linear classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISec</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><surname>Theano Development Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Extracting refined rules from knowledge-based neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Towell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="71" to="101" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Differentially private projected histograms: Construction and use for prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vinterbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Practical evasion of a learningbased classifier: A case study</title>
		<author>
			<persName><forename type="first">N</forename><surname>?rndi ?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="197" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Functional mechanism: regression analysis under differential privacy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Kernel logistic regression and the import vector machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Support vector machines (SVMs) perform binary classification (c = 2) by defining a maximally separating hyperplane in d-dimensional feature space. A linear SVM is a function f (x) = sign(w ? x + ? ) where &apos;sign&apos; outputs 0 for all negative inputs and 1 otherwise. Linear SVMs are not suitable for non-linearly separable data</title>
	</analytic>
	<monogr>
		<title level="m">A Some Details on Models SVMs</title>
		<imprint/>
	</monogr>
	<note>Here one uses instead kernel techniques [14</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
