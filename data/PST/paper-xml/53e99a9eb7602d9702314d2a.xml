<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compact Video Synopsis via Global Spatiotemporal Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongwei</forename><surname>Nie</surname></persName>
							<email>nieyongwei@gmail.com</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Chunxia</forename><surname>Xiao</surname></persName>
							<email>cxxiao@whu.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hanqiu</forename><surname>Sun</surname></persName>
							<email>hanqiu@cse.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>New Territories</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compact Video Synopsis via Global Spatiotemporal Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">030EE118083E996216EBA2BE95BD2EE9</idno>
					<idno type="DOI">10.1109/TVCG.2012.176</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video synopsis</term>
					<term>surveillance</term>
					<term>optimization</term>
					<term>patch relocation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video synopsis aims at providing condensed representations of video datasets that can be easily captured from digital cameras nowadays, especially for daily surveillance videos. Previous work in video synopsis usually moves active objects along the time axis, which inevitably causes collisions among the moving objects if compressed much. In this paper, we propose a novel approach for compact video synopsis using a unified spatiotemporal optimization. Our approach globally shifts moving objects in both spatial and temporal domains, which shifting objects temporally to reduce the length of the video and shifting colliding objects spatially to avoid visible collision artifacts. Furthermore, using a multi-level patch relocation method, the moving space of the original video is expanded into a compact background based on environmental content to fit with the shifted objects. The shifted objects are finally composited with the expanded moving space to obtain the high-quality video synopsis, which is more condensed while remaining free of collision artifacts. Our experimental results have shown that the compact video synopsis we produced can be browsed quickly, preserves relative spatiotemporal relationships, and avoids motion collisions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>W ITH the rapid development of the digital-media industry, the huge video datasets captured by various resources such as digital cameras, webcams, cellular phones, PDAs, and surveillance cameras, are growing at an explosive speed. It is time consuming to review entire, lengthy videos, such as those captured by surveillance cameras, to find interesting objects, since most surveillance videos contain only a limited number of important events. Thus, end users often prefer briefer, condensed representations of long video sequences, fastforwarding to important content and dynamic objects in surveillance videos. This is generally referred as video synopsis or abstraction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. In addition, it is memory intensive to store and transfer the entire captured videos while retaining less important objects; thus, video synopsis can effectively reduce memory storage for large video datasets. Furthermore, video synopsis techniques can be widely used as practical video editing toolkits, to abstract videos captured in video games, video crowd animation, and video conferences. Currently, video synopsis is an active research topic in the computer graphics and computer vision communities.</p><p>To date, there is no unified standard for measuring if an output abstracted video is a good representation of an input video, since whether a synopsis is good or bad is highly subjective and depends on the application.</p><p>In general, we outline three main objectives for our video synopsis: the spatiotemporal redundancies of the input video should be reduced as much as possible; the chronological consistency of important events should be kept; the visual artifacts, such as flickering or moving object collisions, should be avoided in the final synopsis. Efficient browsing of the condensed video synopsis is also important, especially for huge video datasets such as surveillance videos.</p><p>Many approaches have been proposed for condensing the volume of videos. Most of them are based on video frames, where image frames are treated as the basic building blocks that cannot be decomposed. In video abstraction methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, either key frames were selected according to some importance criteria or video clips with lower interest or activity were skipped. One typical approach <ref type="bibr" target="#b7">[8]</ref> used a time-lapse method to generate a summary of a very slow process, such as the growth of a flower over an entire day. These methods miss fast activities occurring in skipped frames. In order to reduce less important spaces in the video volume, Kang et al. <ref type="bibr" target="#b1">[2]</ref> extracted informative space-time video portions, and then montaged these portions together. However, in this way, visible seams may appear at the boundaries between different portions. More recently, Pritch et al. <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> proposed an object-based video synopsis approach for surveillance videos, in which objects are shifted along the time axis. Although this approach is capable of eliminating temporal free space, it produces unpleasant collisions between moving objects in the condensed video, especially for videos with narrow motion space.</p><p>In most site-seeing surveillance videos, we often observe large amounts of free space in the scene backgrounds, where no active objects move at all. Previous work <ref type="bibr" target="#b3">[4]</ref> extracted active objects from the input video and moved them compactly along the time axis, which can certainly condense videos in the temporal space. However, when there are many objects moving in the same orbit or in opposite directions, unavoidable collisions among them occur, which inevitably produces unpleasant artifacts, such as objects moving across each other. The key observation in this paper is that the free space in the video background can be utilized when condensing the video. Based on this observation, we construct larger virtual motion spaces (i.e. multi-paths) for moving objects based on the environmental context, which effectively alleviates the moving-object collisions produced in previous video synopsis methods. Thus, the condensed video looks more realistic even in shorter periods.</p><p>Our key idea in this work is to globally shift the active objects in the spatiotemporal video volume, and then synthesize a compact background constrained by the optimized object trajectories to fit the shifted objects. We propose a global spatiotemporal optimization framework that shifts active objects in the spatiotemporal space. The optimization is composed of two cost terms: the data term is used to preserve activities as much as possible and prevent objects' new trajectories from deviating far from their original ones, and the smoothness term is used to avoid object collisions and keep the spatiotemporal consistencies and relations of objects as much as possible. We develop a multilevel patch relocation method to synthesize the compact background, which further expands the movement space of shifted objects based on the environmental context, so that visual artifacts such as cars running on grassland or mutual crossings can be eliminated. Finally, we seamlessly fuse the shifted objects into the compact background to produce the final video synopsis.</p><p>Our framework presents a compact video synopsis technique via global spatiotemporal optimization. Using the synthesized compact background and simple user interactions, our approach can produce more condensed video scenes with crowded but non-colliding objects from input videos containing sparse moving objects, and can be widely applied in video summarization, video games, crowd animation design, and interactive media production. Our work makes the following two main contributions:</p><p>• We propose a novel approach for compact video synopsis in the spatiotemporal domain, which highly condenses the activity information for surveillance videos, while avoiding visual collision artifacts between moving objects; • We introduce a synthesized compact background which provides a larger virtual motion space for shifted objects using a multilevel patch relocation method in Markov Random Field (MRF) networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Here, we review the most related work in video synopsis, which can be roughly classified as frame-based video abstraction, object-based video synopsis, and synthesisbased video summarization. Previous work in patchbased image editing related to our background-synthesis work is also outlined.</p><p>Frame-based abstraction There are two basic forms of video abstracts: keyframes and video skims. They are both based on frame selection, where the frames are essential building blocks that can't be decomposed. In the keyframe-based abstraction methods, a set of salient frames are extracted from the source video. Uniform sampling is the simplest method for keyframe generation, but it may select some frames that aren't as "key" as desired. Methods for finding importance criteria to guide the selection process were proposed in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The fast-forward methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref> are another kind of keyframe-based method, which select keyframes uniformly or adaptively, but they do not preserve time coherence and may result in unrealistic views. High-level content analysis is also taken into account in keyframe selection processing <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Alternatively, video skims <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> consist of a collection of video segments extracted from the source video. These segments are then joined by a cut or a gradual effect. Although the above two kinds of methods work well in most situations, they suffer from loss of fast activities occurring in the skipped frames, and retention of large empty spaces in natural scenes from the source videos.</p><p>Object-based synopsis Kang et al. <ref type="bibr" target="#b1">[2]</ref> explicitly extracted informative space-time video portions that can be moved and stitched using first-fit and graph-cut optimizations which maximizes the amount of visual information. As optimal boundaries between portions are found, visually unpleasant seams usually appear between portions that don't match. Rav-Acha et al. <ref type="bibr" target="#b8">[9]</ref> proposed an object-based approach for video synopsis that is similar to that of Kang et al. <ref type="bibr" target="#b1">[2]</ref>. Both of these methods change the chronological order of objects, and show several actions at the same time. The difference is that the latter <ref type="bibr" target="#b8">[9]</ref> only moves objects along the time axis. Pritch et al. <ref type="bibr" target="#b9">[10]</ref> extended the latter work <ref type="bibr" target="#b8">[9]</ref> to process always-on videos captured by surveillance cameras or webcams. The more complete work is presented in <ref type="bibr" target="#b3">[4]</ref>, in which they first extracted interesting objects (tubes), then moved them along the time axis while preserving activities and local chronological orderings. It can handle always-on videos and take illumination-varying backgrounds into account. However, since it processes moving objects only in the temporal domain, this method cannot fully utilize the video space in the spatial domain and may produce collision artifacts when the synopsis is compressed much or contains more moving objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthesis-based summarization</head><p>The synthesis-based video summarization approach was proposed in <ref type="bibr" target="#b4">[5]</ref>, which defined a bidirectional similarity measure to de-cide how visually similar a summary was to the underlying source video. A good summary contains as much visual information from the source as possible, and introduces as few new visual artifacts as possible. The bestmatching patches in the source and summary are found and then go through a weighted average process to obtain the final summary. Since the nearest neighboring patches need to be computed and stored, the algorithm is time and memory intensive. Barnes et al. <ref type="bibr" target="#b17">[18]</ref> and Xiao et al. <ref type="bibr" target="#b18">[19]</ref> proposed fast patch match methods which can accelerate the nearest neighbor search to a certain extent, but the synthesis-based approach is still not scalable for long videos.</p><p>Patch-based image editing Like pixels, patches can also be used as the basis for analyzing and editing images. Features are either extracted from patches for further analysis, or the patch is directly operated on for image editing. It is often more effective to work on patches than on pixels. Kwatra et al. <ref type="bibr" target="#b19">[20]</ref> proposed a patch-based texture optimization framework which improves texture quality compared to pixel-based methods <ref type="bibr" target="#b20">[21]</ref>. Xiao et al. <ref type="bibr" target="#b21">[22]</ref> obtained effective upsampling results by combining patch-based texture synthesis and joint bilateral filter. Image and video completion methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> also work on the patch level. Cho et al. proposed a patch transformation method <ref type="bibr" target="#b25">[26]</ref> that provides image editing tools for image reorganization, object removal, image retargeting, etc. More recently, Cho et al. <ref type="bibr" target="#b26">[27]</ref> edited images by working on overlapped patches, and a patch jittering post-processing step was proposed to improve the quality of the edited image. In our work, we improve the method of <ref type="bibr" target="#b26">[27]</ref> to synthesize the compact background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the main motivation of our proposed approach. Given an input video (Fig. <ref type="figure" target="#fig_0">1a</ref>), visual motion collisions occur when shifting objects along the temporal axis (Fig. <ref type="figure" target="#fig_0">1b</ref>). Using the spatial free space observed in most site-seeing surveillance videos, we attempt to shift active objects in both the spatial and temporal domains (Fig. <ref type="figure" target="#fig_0">1c</ref>). In this fashion, we not only condense temporal free space, but also make full use of spatial free space for movements, which can effectively avoid visual artifacts, such as moving-object collisions, producing high-quality video synopsis results with scene-path context.</p><p>Our video synopsis framework is composed of four main stages. In the first stage, we analyze the input video and explicitly extract the background and moving objects out of the video. In the second stage, we shift active objects in the spatiotemporal video volume by using the global spatiotemporal optimization, which computes new positions in the synopsis for clustered objects. In the third stage, using a multilevel patch relocation method, we synthesize a compact background with the scene-path context to fit the clustered objects. Lastly, we seamlessly fuse objects into the synthesized compact background using a gradient-domain editing tool. Fig. <ref type="figure" target="#fig_2">2</ref> shows the main advantages of our proposed approach. Fig. <ref type="figure" target="#fig_2">2b</ref> shows that shifting objects only in the temporal domain leads to heavy object collisions (the circled cars, for example). Our global spatiotemporal optimization effectively eliminates these collisions (Fig. <ref type="figure" target="#fig_2">2c</ref>). Since the original video background (Fig. <ref type="figure" target="#fig_2">2d</ref>) may not be compatible with the shifted objects (cars running out of the road in Fig. <ref type="figure" target="#fig_2">2e</ref>), we compute a larger virtual motion space by synthesizing a compact background to remove these inconsistencies (Fig. <ref type="figure" target="#fig_2">2f</ref>). Finally, the shifted objects are seamlessly composited into the compact background, producing an appealing video synopsis (Fig. <ref type="figure" target="#fig_2">2g</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COMPACT VIDEO SYNOPSIS</head><p>In this section, we first analyze the input video to extract the background and moving objects. Then, we describe our global spatiotemporal optimization in detail. Finally, we present the compact background synthesis using the scene-path context, and the method to seamlessly fuse the clustered moving objects into the compact background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Video Analysis</head><p>We use a space-time volume I(x, y, t) to represent an input video I, where (x, y) is the spatial coordinate of a pixel in frame t, satisfying (1</p><formula xml:id="formula_0">x W ), (1 y H) and (1 t N ).</formula><p>With the input video, we first extract background B and interesting objects s. Typically, interesting objects are simply defined as moving objects, such as running cars or walking people. Sometimes, exceptions may be noted: not all moving objects are interesting, and not all static objects are less important. To handle such exceptions, more sophisticated techniques in pattern recognition <ref type="bibr" target="#b27">[28]</ref> can be incorporated.</p><p>Before extracting active objects and synthesizing the compact background, we first extract the background of the input video. For most site-seeing surveillance  videos, video frames change due to the entering and exiting of moving objects and due to varying illumination. The background is usually static in daily periods.</p><p>Based on this observation, we use a temporal median operator over a short period to extract the corresponding background. The median value of pixels with the same location (x, y) in a short video clip makes up the background value of the location. In our experiments, we compute a background image every minute (1800 frames for a 30 Fps video), which can alleviate the effects of varying illumination. For more complex situations, such as moving objects covering pixels over a long duration, we recommend using a shorter temporal window background estimation method <ref type="bibr" target="#b28">[29]</ref>. With the background extracted, the activity measure Θ(x, y, t) for pixel I(x, y, t) is defined as the difference between the pixel and its corresponding background value:</p><formula xml:id="formula_1">Θ(x, y, t) = I(x, y, t) -B(x, y) ,<label>(1)</label></formula><p>where B(x, y) is the pixel value at coordinate (x, y) in background B.</p><p>Next, we explicitly detect and extract interesting objects from the input video. As the objects will be later composited back into the background using Poisson Video Editing <ref type="bibr" target="#b29">[30]</ref>, precise object extraction is not necessary. In our system, we first subtract the extracted background from the original frames. Then, we construct a mask of all foreground pixels with greater absolute differences than a threshold value (10 out of 255 is used). We apply a 2D morphological dilation on all the mask frames to obtain larger moving masks. The morphological mask is a circle with a radius of 3 pixels, and we iterate the dilation three times. If needed, the background cut by Sun et al. <ref type="bibr" target="#b30">[31]</ref> can be used to precisely segment foreground objects, and is more robust but computationally more expensive. We connect all object masks together across frames to construct a 3D tube of moving object. However, when the objects move fast or overlap with each other, we perform real-time tracking <ref type="bibr" target="#b31">[32]</ref> to construct object tubes. Fig. <ref type="figure" target="#fig_3">3a</ref> shows one frame of input video, Fig. <ref type="figure" target="#fig_3">3b</ref> shows the extracted background and Fig. <ref type="figure" target="#fig_3">3c</ref> shows the extracted moving object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Global Spatiotemporal Optimization</head><p>The spatiotemporal operator V s = (s x , s y , s t ) of interesting object s, where s x and s y are spatial offsets and s t is temporal offset, shifts the object to position s in the synopsis by s = s + V s . Most objects are shifted forward along the timeline, so that the length of synopsis O, denoted by M , is much shorter than that of the input video. We compute the optimal new positions for the interesting objects by using a global spatiotemporal optimization which minimizes the following Gibbs function:</p><formula xml:id="formula_2">E(V s ) = E data (V s ) + E smooth (V s ).</formula><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>The data term is defined on single objects s i :</p><formula xml:id="formula_4">E data (V s ) = si (E a (s i ) + γE d (s i )),<label>(3)</label></formula><p>where E a encodes activity cost, and E d is spatial distance cost. The smoothness term is defined on pairs of objects s i and s j : </p><formula xml:id="formula_5">E smooth (V s ) = si,sj ∈S (αE st (s i , sj ) + βE c (s i , sj )) • δ(s i , sj ),<label>(4)</label></formula><p>where,</p><formula xml:id="formula_6">δ(s i , sj ) = 1, if si = sj , 0, otherwise. (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>The smoothness term is the sum of weighted collision cost E c and spatiotemporal consistency cost E st . α, β and γ are weights that can be tuned by users, and their default values are 2, 5, and 0.5 in our experiments. E a favors a synopsis with maximum activities by penalizing objects moving outside of the synopsis. It is zero if an object stays in the synopsis as a whole. Otherwise, it is sum of the activity values of pixels outside the synopsis.</p><formula xml:id="formula_8">E a (s i ) = (x,y,t)∈si\synopsis Θ si (x, y, t),<label>(6)</label></formula><p>where (x, y, t) ∈ si \ synopsis represents pixels belonging to si but not the synopsis, and Θ si (x, y, t) is the pixel activity value defined in Eq. ( <ref type="formula" target="#formula_1">1</ref>). E d prevents objects from being shifted far away from their original trajectories in the spatial domain; otherwise, objects may scatter anywhere in the synopsis and serious inconsistencies among them would occur. We define it as object spatial distance:</p><formula xml:id="formula_9">E d (s i ) = n si (s i,x , si,y ) 2 , (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>where n si is the number of pixels of object s i (this balances the spatial distance cost against other costs that use all pixels of an object, and not just one). In Fig. <ref type="figure" target="#fig_4">4</ref>, we illustrate the comparative results with and without using spatial distance cost E d , which shows that shifted objects do not drift far away from their original positions when using the cost. E c encodes the collision cost of all pairs of objects. For the overlapped part of two shifted objects si and sj , we define the collision cost as the sum of the products between pixel activities of two objects:</p><formula xml:id="formula_11">E c (s i , sj ) = (x,y,t)∈si∩sj Θ si (x, y, t) • Θ sj (x, y, t). (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>We see that the collision cost is zero if two objects don't collide with each other. We shift the objects in both the spatial and temporal domains, which expands the motion space for objects and effectively reduces collision artifacts. E st preserves the spatiotemporal relations of objects: (1) Chronological order should be kept, i.e., objects behind other objects should not appear in front of them. The cost of reversing the chronological order of two objects is the sum of their activities; when the chronological order is kept, the cost is zero. (2) The spatial relative locations of two objects should be preserved if they are neighboring each other. The cost of breaking such relationships is defined as the difference of their spatial offsets, which is then weighted by the original distance of the two objects. Let t f s i and t f s j be the first frames of two objects s i , s j . We compute a variable</p><formula xml:id="formula_13">u = (t f s i -t f s j ) • (t f si -t f sj ).</formula><p>Then we determine whether the chronological order of the two objects is broken or not by:</p><formula xml:id="formula_14">τ (u) = 0, if u ≥ 0, 1, else. (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>We determine whether two input objects are neighboring each other by checking if they share common frames. If so, we compute the nearest distances of the two objects for all common frames, from which we find the minimum distance d(s i , s j ); otherwise, d(s i , s j ) is set as ∞. Let χ si = (x,y,t)∈si Θ si (x, y, t) be the object's activity. Then, we define the spatiotemporal consistency cost as:</p><formula xml:id="formula_16">E st (s i , sj ) = τ (u) • (χ si + χ sj ) + (n si + n sj ) • exp(-d(s i , s j )/σ st ) • (s i,x , si,y ) -(s j,x , sj,y ) 2 , (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>where σ st determines the extent of how close the two objects are, and is set as 40 in our experiments. Fig. <ref type="figure" target="#fig_5">5</ref> shows that using our spatiotemporal consistency cost E st , the chronological order among objects is preserved. We show in Fig. <ref type="figure" target="#fig_6">6</ref> that the spatial relations among objects are preserved. δ ( si , sj ) makes Eq. ( <ref type="formula" target="#formula_3">2</ref>) regular according to the theoretic results of <ref type="bibr" target="#b32">[33]</ref>, which guarantees the equation is graph-representable. We use the Graph Cuts technique to minimize Eq. <ref type="bibr" target="#b1">(2)</ref>.</p><p>Energy Minimization We use alpha-beta swap Graph Cuts method <ref type="bibr" target="#b33">[34]</ref> to minimize the global spatiotemporal optimization given in Eq. ( <ref type="formula" target="#formula_3">2</ref>). First, we construct a graph for the synopsis where each node N i in the graph corresponds to an input object s i , and the edge E i,j connects nodes N i and N j . The cost of node N i is E a (s i )+γE d (s i ) when assigning label si to it, and the cost of edge E i,j is (αE st (s i , sj ) + βE c (s i , sj )) • δ(s i , sj ) when assigning labels si and sj to nodes N i and N j , respectively. With the constructed graph, the minimum cut of the graph minimizes Eq. ( <ref type="formula" target="#formula_3">2</ref>).</p><p>For each node N i , the number of its possible labels is the number of pixels in the synopsis W × H × M . The label number is usually too large for efficient computation. For instance, there are about 10 9 (640×480×60×30) labels for a 60-second synopsis with a resolution of 640 × 480 and FPS of 30. To speed up the computation, we sample the synopsis pixels in the spatiotemporal space using grids of size η = 20 pixels in our experiments, which results in a much smaller label set W ×H ×M/η 3 . For the same-length synopsis, the label number is 69120((60 × 30 × 640 × 480/20 3 )) now, which is still somewhat timeconsuming.</p><p>To further reduce computational cost, we approximate the global optimization by splitting it into two consecutive optimizations, which degrades the label space from 3D to 1D and 2D. We first constrain the spatial offsets s x and s y to be zero and only shift objects along the temporal axis using temporal optimization. Then, we constrain the temporal offset s t to be zero and shift objects into the spatial free space using spatial optimization. Now, for the 60-second synopsis, the spatial shift optimization has only 768(640 × 480/20 2 ) labels. However, for the temporal shift optimization, the procedure may become trapped in a local minimum. For example, one special case may occur: all the objects are inside the synopsis, and are shifted into the same label. Both the data term and smoothness term are zero in this case, but it is not the desired result. To avoid the local minima, we expand the temporal label space from M/η to n • M/η, where n is the number of nodes. In the graph construction, we set the valid label range [i • M/η, (i + 1) • M/η) (otherwise invalid range) for each node N i (0 i n). For invalid labels, the costs of node N i and edge E i,j are set to ∞. After optimization, for an object s i with label si , its temporal position is si mod M/η. For the 60-second synopsis with 15 moving objects, the temporal optimization has only 1350 (15×60×30/20) labels. Using this approximation method, we greatly speed up global spatiotemporal optimization in our system. In the global optimization, the spatiotemporal consistency cost E st keeps the spatial relations of any two neighboring objects. For the example in Fig. <ref type="figure" target="#fig_7">7a</ref>, the collisions among objects are eliminated and the relative positions among pairs of neighboring objects are preserved, even though the objects are distributed somewhat unordered in the overall view. To further refine the synopsis results, we try to keep the relative positions among multiple objects rather than between pairs of neighboring objects, using an iterative object shifting scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Iterative Object Shifting</head><p>The basic idea is that objects are clustered into several groups, and objects in a group are shifted with the same vector in the spatial domain. After energy function Eq. ( <ref type="formula" target="#formula_3">2</ref>) is minimized, we divide the shifted objects into K groups based on the calculated spatial offsets (s i,x , si,y ) using the K-Means method. The objects with similar shift vectors are assigned to the same group. For each of the groups, we compute its centra (c k,x , c k,y ), where k ∈ [1, K]. Then, we define the spatial consistency cost E SC for a shifted object si as the difference between its offset and the centra of the cluster it belongs to:</p><formula xml:id="formula_18">E SC (s i ) = n si (s i,x , si,y ) -(c k,x , c k,y ) 2 , (<label>11</label></formula><formula xml:id="formula_19">)</formula><p>where si belongs to the kth group. Finally, the data term E data (V s ) in Eq. ( <ref type="formula" target="#formula_4">3</ref>) is redefined by adding the spatial consistency cost:</p><formula xml:id="formula_20">E data (V s ) = si (E a (s i ) + γE d (s i ) + E SC (s i )),<label>(12)</label></formula><p>where the weight is set as 0.5 in our experiments. With this new data term, we minimize Eq. ( <ref type="formula" target="#formula_3">2</ref>) iteratively to obtain better global shifting of interesting objects. The iterative scheme works as follows: first, it divides objects into groups according to the spatial shifting vectors computed in the previous iteration, then it computes group centra and minimizes Eq. ( <ref type="formula" target="#formula_3">2</ref>) with the data term shown in Eq. ( <ref type="formula" target="#formula_20">12</ref>) to output new object shifting vectors. This procedure goes on about 4 or 5 times until the positions of shifting objects converges. By setting K as 2 or 3, Fig. <ref type="figure" target="#fig_7">7b</ref> and<ref type="figure" target="#fig_7">7c</ref> show that the cars are grouped into two or three queues using our iterative object shifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Compact Background Synthesis</head><p>Our global spatiotemporal optimization expands the motion space of objects, which leads to the problem of presenting a compatible background for shifted objects. This problem will not occur in scenes that are full of moving space, for example in Fig. <ref type="figure" target="#fig_4">4,</ref><ref type="figure" target="#fig_5">5,</ref><ref type="figure" target="#fig_6">6</ref>, where people can run on every athletic track. However, in other scenes, there may be not enough moving space for the shifted objects, for example in Fig. <ref type="figure" target="#fig_2">2e</ref> and Fig. <ref type="figure" target="#fig_7">7</ref>, with cars running out of road. The task in this section is to synthesize a compact background that expands the moving space, constrained by the shifted objects and by user interactions. The synthesized compact background must fit all shifted objects to avoid unrealistic artifacts. We use a patch relocation scheme to achieve this goal. We also develop a multilevel relocation method to further improve the quality of this synthesis.</p><p>Patch Relocation Inspired by patch transformation <ref type="bibr" target="#b26">[27]</ref>, we relocate existing image patches of extracted background to generate a compact background. We divide the background into grids (grid size η) and construct a MRF on the grids, where each grid is viewed as a node in the MRF. The problem is how to assign each MRF node an index x i of patch (here the grid is considered as a patch). In a good relocation of patches, adjacent patches should fit each other and the relocation should be subject to the constraints of shifted objects. With the MRF framework, the above requirements are formulated as a joint probability:</p><formula xml:id="formula_21">P (X) = i Φ i (x i ) i,j∈N (i) Ψ i,j (x i , x j ),<label>(13)</label></formula><p>where Ψ i,j (x i , x j ) is the compatibility of two patches at node i and j; Φ i (x i ) is the local evidence term used to represent the constraints. To maximize this joint probability using loopy belief propagation <ref type="bibr" target="#b34">[35]</ref>, Eq. ( <ref type="formula" target="#formula_21">13</ref>) is factorized into:</p><formula xml:id="formula_22">P (X) = i j∈N (i) p(y i |x i )p i,j (x j |x i )p(x i ),<label>(14)</label></formula><p>where for node i, N (i) are the indices of its four neighboring nodes; x i is the index of the patch assigned to node i; y i is its original patch index at location i; p i,j (x j |x i ) is the normalized compatibility between patches x i and x j with respect to the relative locations between nodes i and j, which is defined as:</p><formula xml:id="formula_23">p i,j (x j |x i ) = Ψ i,j (x i , x j ) k Ψ k,j (x k , x j ) . (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</p><p>As for local evidence, p(y i |x i ) = Φ i (x i ). Finally, p(x i ) is modeled as a uniform distribution. The compatibility term Ψ i,j (x i , x j ) is formulated as: where E seam (k, l) is the minimum energy among continuous seams between two patches k and l, calculated by dynamic programming <ref type="bibr" target="#b35">[36]</ref>. σ Ψ is fixed as 0.2 after cross validation. With the local evidence term, we determine how likely it is for a patch to be assigned to a node. For the constrained nodes, if fixing patch k for node i, we set p(y i |x i = k) = 1, and p(y i |x i = l) = 0 for any l = k. In our method, the constrained nodes are the ones occupied by shifted objects si in the synopsis. The corresponding patches occupied by object s i in the input video are set for those constrained nodes. For the unconstrained nodes, p(y i |x i ) is computed as the difference between the assigned patch l and the original one y i :</p><formula xml:id="formula_25">Ψ i,j (k, l) = exp( -E seam (k, l) σ 2 Ψ ),<label>(16)</label></formula><formula xml:id="formula_26">p(y i |x i = l) ∝ exp(- (m(y i ) -m(l)) 2 σ 2 evid ),<label>(17)</label></formula><p>where m(•) is the mean color of argument and σ evid is fixed as 0.4. Fig. <ref type="figure" target="#fig_8">8</ref> gives an example of patch relocation. We use Loopy Belief Propagation to maximize Eq. ( <ref type="formula" target="#formula_22">14</ref>) (refer to <ref type="bibr" target="#b26">[27]</ref> for more detail). The global patch relocation may sometimes retain artifacts as shown in Fig. <ref type="figure" target="#fig_8">8d</ref>. We use a local refinement technique to remove the artifacts. As illustrated in Fig. <ref type="figure" target="#fig_8">8e</ref>, the user explicitly indicates the artifact region and it is then divided into grids (the green ones) to construct a local MRF. Using the source patches (the orange ones) selected by the user input, the local patch relocation is performed on these nodes and patches. Fig. <ref type="figure" target="#fig_8">8f</ref> shows the final compact background synthesized after the local relocation step. (c) We sample the patches every grid size rather than every patch size, to supply more sufficient patch samples for good matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilevel Patch Relocation</head><p>The patch relocation is modeled on low-level vision. When synthesizing a complex compact background with strong structure or the background is densely sampled in grids, a single patch relocation process may fall into local minima easily (Fig. <ref type="figure" target="#fig_11">10c</ref>). This is due to the fact that a small patch itself can't contain much image structure. For complex scenes, we prefer a larger patch size to preserve important structures; however, serious visual seams among big patches occur (Fig. <ref type="figure" target="#fig_11">10e</ref>). To flexibly utilize different patch sizes, we develop multilevel patch relocation in our synopsis system. We first determine the compact appearance at coarser levels (larger patch sizes), and then refine the detail at finer levels. In our experiments, the patch size in the coarsest and finest levels varies from 80 to 20 pixels, respectively. The number of MRF nodes at the finer level is 4 times than that of the coarser level. In two consecutive levels, we use the result of the coarser level to initialize the BP iterative procedure of the finer one.</p><p>As we shift objects grid by grid, mismatching problems may occur when the patch size is not equal to the grid size. In Fig. <ref type="figure" target="#fig_9">9</ref>, we show a correct case in (a) and a problem case in (b). In Fig. <ref type="figure" target="#fig_9">9c</ref>, we give our solution to (b). In Fig <ref type="figure" target="#fig_9">9a</ref>, a grid is a MRF node and we sample patches from grids. If the grid and patch (and node) share a common size, the nodes of shifted object O (blue ones) can find the corresponding patches of object O (red ones). In Fig. <ref type="figure" target="#fig_9">9b</ref>, the patch (node) size is double, and each patch consists of four grids now. The mismatching problem happens because the shifted object O occupies two nodes but the original object only occupies one patch. In Fig. <ref type="figure" target="#fig_9">9c</ref>, we sample patches by an interval of a grid cell in both x and y directions. Then, we can find  two matched patches (red ones) for both blue nodes. We use this method for the coarser levels to eliminate the mismatching problem. Fig. <ref type="figure" target="#fig_11">10e</ref>, 10f and 10g give the results of our multilevel patch relocation from the coarsest to the finest levels, validating the effectiveness of our method. Finally, using the generated shifted objects and the compact background, we seamlessly compose them to produce the final video synopsis with more compact views and a shorter synopsis length. In our experiments, we used Poisson cloning <ref type="bibr" target="#b29">[30]</ref> for video composition, which worked well for our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION</head><p>We have developed our compact video synopsis system in C++, and run it on an Intel Core 2 Duo 2.1GHz CPU computer with 2 GB RAM. We have tested our approach on several surveillance video examples (frame rate of 30 Fps), and compared our results with previous methods. In Table <ref type="table" target="#tab_0">1</ref>, we give relevant experimental information and parameters for each example: the length of the input video (Input Len.), the number of interesting objects (Obj. Num.), the length of the synopsis (Synopsis Len.), the number of groups K in the iterative object shifting method, the number of labels L t and L s in the temporal and spatial optimizations, and the time T t and T s consumed by the two optimizations, respectively. We show whether the multilevel patch relocation (MPR) method and/or the local refinement (LR) scheme are used. We give the running times of <ref type="bibr" target="#b3">[4]</ref> in the last column.</p><p>We can see from the table that all the input videos are longer than 5 minutes (30 minutes maximum) and that the synopsis results are much shorter (15 seconds minimum). Our global spatiotemporal optimizations contain few enough labels, and most of them can be finished within 22 seconds. In the compact background synthesis, computing compatibility Ψ i,j (x i , x j ) between two patches consumes most of the time. The total time used for compact background synthesis is less than 29 seconds. Since our system includes both spatiotemporal optimization and compact background synthesis, we consume more time than <ref type="bibr" target="#b3">[4]</ref> for more compact and shorter video synopsis results.</p><p>In Figs. 11, 12, 13, 14 and 15, we give video synopsis results for site-seeing surveillance videos and compare our work with <ref type="bibr" target="#b3">[4]</ref>. The previous work is powerful in compressing time-scale redundancies but may produce collision artifacts if compressed much. The video synopsis examples we tested showed that our approach produces more compact and shorter synopsis results without collision artifacts. The video synopsis demos are included in the online video submission.</p><p>In Fig. <ref type="figure" target="#fig_0">11</ref>, the input video (10 minutes) shows a bridge scene where many people cross randomly. Shifting them only along the time axis as in <ref type="bibr" target="#b3">[4]</ref>, people collide with each other easily, especially when people walk in opposite directions, since the movement space is very narrow. The top row shows four frames with collisions from the synopsis (2 minutes) of <ref type="bibr" target="#b3">[4]</ref>. The collision artifacts (i.e. moving objects colliding on the bridge) make the synopsis flicker and inconsistent. The bottom row shows the results of our spatiotemporal synopsis. Our result is shorter in length (1 minute), and there are no collisions. We achieve this by expanding the movement space into free spatial domain in an environmental-path context     aware fashion. In this example, K is set to 3 in the iterative object shifting method. Fig. <ref type="figure" target="#fig_2">12</ref> shows a traffic-circle video example (3 minutes) in which cars run in and out. In this case, all the cars are moving in the same direction. The top row shows the synopsis result of <ref type="bibr" target="#b3">[4]</ref> which shifts the cars temporally only. If compressed too much in the time domain (23.3 seconds), the cars move too close to each other and collision artifacts occur among them. In our approach, the global spatiotemporal optimization shifts cars in both temporal and spatial free space, and we accordingly expand the movement space into the compact background. Our results in the second row show that collisions are avoided and our synopsis is shorter (15 seconds). For the bottom row, the parameters are set as: α = 1, β = 2.5, γ = 0.1, = 0.5 and K = 4. α and β are decreased to increase the relative weight of term E a , which ensures that the objects remain within the synopsis. γ is decreased to move objects a little farther away, which allows us to expand more into spatial free space and to obtain an even shorter synopsis (10 seconds).</p><p>In Fig. <ref type="figure" target="#fig_13">13</ref>, the input video is approximately 30 minutes. Change in shadows is apparent in the video. Our approach extracts a background image every minute. Based on these backgrounds, we synthesize time-varying compact backgrounds, which are then composited with the shifted objects in the spatiotemporal space. Thus, the shadow-variance phenomenon is preserved quite well in our final compact synopsis. Here, we show our video synopsis result, which is the same length as that of <ref type="bibr" target="#b3">[4]</ref>. However, ours contains no collisions.</p><p>In Fig. <ref type="figure" target="#fig_14">14</ref>, the input video (5 minutes) shows a narrow winding road in the woods, where people come and go very often. The top row shows the video synopsis result (24 seconds) of <ref type="bibr" target="#b3">[4]</ref> using temporal shifts only, in which collisions among people walking in opposite directions are apparent. Using our approach, the moving objects are shifted in the global spatiotemporal video volume, which effectively avoids motion collision artifacts. In the compact background, we expand the moving space to three roads by setting K to 3. The bottom row shows our synopsis result (also 24 seconds for comparison) without motion collisions.</p><p>In Fig <ref type="figure" target="#fig_15">15</ref>, the input video (10 minutes) shows a narrow lane often seen in hilly areas. Due to the side camera view, the moving people may cover parts of railings of the road. Since the extracted mask of moving people is not precise and is larger than the people themselves, it may contain extra railings. If we provide a wider lane in the compact background, people with extra railings would appear in the middle of lane, which causes visual artifacts. To avoid this kind of artifact, we relax the coefficient γ of cost term E d to 0.1, which moves the objects a little farther from their original trajectories. Accordingly, two lanes are synthesized in the compact background, which is very compatible with the shifted objects. The top row shows the video synopsis (66 seconds) of <ref type="bibr" target="#b3">[4]</ref> with serious motion collisions. The bottom row shows our shorter synopsis result (40 seconds) without any collisions.</p><p>User Study We performed a user study with 30 participants to validate the effectiveness of our spatiotemporal video synopsis method. Each time, a participant was shown two synopsis videos side by side. On the left was the result of <ref type="bibr" target="#b3">[4]</ref> and on the right was our synopsis result. Each participant browsed the five synopsis examples shown in Fig. <ref type="figure" target="#fig_0">11</ref> to Fig. <ref type="figure" target="#fig_15">15</ref>, and was asked to answer "Yes" or "No" to the following five questions for each example: (1) Do you think the synopsis on the right is interesting? (2) Do you like the collisions in the synopsis on the left? (3) Do you care more about the objects themselves than the relations between objects and the environment? (4) Can you recognize the objects in the synopsis on the left? (5) Can you recognize the objects in the synopsis on the right? Then they were shown the source video and were asked to answer this question: <ref type="bibr" target="#b5">(6)</ref> Is the synopsis on the right better than the synopsis on the left for presenting the source video?</p><p>For each question, let A ij = 1 if the i th user answered "yes" to the j th example, otherwise A ij = 0. We use the following equation to compute the rate of "Yes" responses to this question, which reflects the users' opinions: R = ( </p><p>The "Yes" rates for the six questions were 82.7%, 4%, 66.7%, 38.7%, 96% and 84%, respectively. From this testing data, we can see that most users think our work is more interesting and our results are better than that of <ref type="bibr" target="#b3">[4]</ref>.</p><p>By directly operating on the extracted objects, Pritch et al. <ref type="bibr" target="#b3">[4]</ref> find and remove redundancies in a much finer grained style than frame-based or synthesis-based synopsis methods. The algorithm used in <ref type="bibr" target="#b3">[4]</ref> is very useful for compressing surveillance videos and essentially inspires our work. Since it compresses objects only in the time scale, one of its advantages is that it does not need to adjust the background of the scene. Thus, it can process videos of complex scenes such as airports, flyovers, and crossroads. However, for objects moving on the same path, this method compacts them in a spatially narrow and temporally short space, leading to numerous overlaps among objects and making it difficult for users to understand what happened. Our method can handle surveillance videos with narrow movement space and with greater spatial free space, which can't be handled well by <ref type="bibr" target="#b3">[4]</ref>. Our work is complementary to <ref type="bibr" target="#b3">[4]</ref> and enriches the object-based synopsis methods.</p><p>Limitation When utilizing both the temporal and spatial spaces to eliminate collisions, our method may fail to produce a compact video synopsis for scenes with crowded activity in both the spatial and temporal domains. For example, our method can't handle videos full of paths with objects moving on them, since there is no spatial free space to expand into. In such situations, methods of selecting important objects and discarding undesired ones can be considered. The use of different selection methods can result in different synopses. We will further investigate these measurements and extend our synopsis framework to effectively process crowdscene videos. The second limitation is that the spatiotemporal optimization stage has no interaction with the compact background synthesis stage. As a result, even if the spatiotemporal optimization stage works well, the background synthesis may fail to produce a consistent result. Fig. <ref type="figure" target="#fig_15">15</ref> shows one example reflecting this limitation. To solve this problem, the user needs to go back to the first stage and adjust parameters to obtain better object shifting. The patch relocation in our compact background synthesis stage may fail if the original background has strong structure. Though our proposed multilevel and local patch relocation methods alleviate this problem to a certain extent, we still need a more structure-aware image editing framework in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>Video synopsis is a useful tool for summarizing long surveillance videos captured by digital cameras. In this paper, we propose a novel global video synopsis approach that shifts objects in spatiotemporal space. Our method eliminates object collisions to create shorter, more visually appealing synopses. We expand the movement space of shifted objects into the compact background in an environmentally context-aware fashion, avoiding visual artifacts such as cars running off the roads or people walking on water. The experimental results show that our proposed method is a practical and efficient video synopsis tool.</p><p>In the future, we will investigate how to use our synopsis method as a video editing tool. We will also investigate more structure-aware methods for synthesizing compact backgrounds from videos. Currently, our proposed method works on videos with static backgrounds. We will extend it to handle videos captured by moving cameras. This is a more challenging topic, due to the dynamic backgrounds, and the different types of camera motion, such as panning, zooming, and jittering, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Input video. (b) Temporal shift of objects, reducing video length by eliminating temporal free space but producing collisions among the moving objects. (c) Our global shift optimization avoids motion collisions between objects</figDesc><graphic coords="3,311.87,53.09,251.18,109.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICSThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Input video (total running time: 3 mins) with two interesting objects marked. (b) Temporal shifting [4] produces collisions if compressed too much (total running time: 23s) (two marked cars as example). (c) Objects shifted globally to avoid such artifacts. (d) The original extracted background. (e) Shifted objects not compatible with the original background. (f) The synthesized compact background. (g) Shifted objects composited into the compact background, producing an even shorter synopsis (total running time: 15s) without visual artifacts.</figDesc><graphic coords="4,199.07,163.01,118.46,93.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) One frame of input video. (b) Extracted background. (c) Extracted object.</figDesc><graphic coords="5,315.23,53.09,120.62,108.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparative results with and without using E d . (a) and (b) are two frames from the input video. (c) Using E d , the two marked objects are not shifted far away from their original trajectories and are perfectly fused into the background. (d) Without E d , the marked objects are shifted away from their original trajectories.</figDesc><graphic coords="5,315.23,175.61,120.62,108.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparative results with and without using E st . (a) The input 1200th frame. (b) The input 10280th frame. (c) Using E st , the two objects keep their chronological order in the synopsis. (d) Without E st , the blue object walks in front of the red one, which is not the case.</figDesc><graphic coords="6,52.19,175.61,120.62,108.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The relative spatial relations of objects are kept in the synopsis. (a) The 2400th frame from the input video. (b) The 100th frame from the synopsis (corresponding to (a)). (c) The 2480th frame from the input video. (d) The 180th frame from the synopsis (corresponding to (c)).</figDesc><graphic coords="6,315.23,175.61,120.62,108.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Iterative object shifts. (a) Without E SC , the shifted objects may be distributed freely. (b) With E SC , by setting K = 2, the shifted objects are clustered into 2 groups, and each of them moves with the same vector. (c) The iterative shifting result produced by setting K = 3.</figDesc><graphic coords="7,50.39,147.53,80.42,63.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Patch relocation. (a) The extracted background image is divided into grids, where the orange grids are patches. (b) One frame of shifted objects. (c) The initial compact background (the orange regions) is constructed based on the shifted objects, using them as constraints for patch relocation. (d) Patch relocation automatically produces a compact background subject to these constraints. (e) Local refinement on the manually-selected green region removes artifacts, using orange regions as source patches. (f) The final compact background in the video synopsis.</figDesc><graphic coords="8,50.39,226.49,80.42,69.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. (a) The object O is shifted to the position of O . When the patch size equals the grid size, the red patches match the four blue patches. (b) With patch size larger than grid size, the two blue patches of the shifted object can't find matched patches containing the original object.(c) We sample the patches every grid size rather than every patch size, to supply more sufficient patch samples for good matching.</figDesc><graphic coords="8,218.03,226.49,80.54,69.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICSThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Multilevel patch relocation. Three levels are used, node sizes are 80, 40, and 20. (a) Extracted background. (b) The shifted objects. (c) Using single-level patch relocation with patch size 20, the wall of the building becomes trapped in a local minimum. (d) Sampling patches for the coarser levels every 20 pixels rather than 80 (or 40). (e) Compact background on 2-th level (patch size is 80). (f) Compact background on 1-th level (patch size is 40). (g) Compact background on 0-th level (patch size is 20). (h) Compact background after local refinement.</figDesc><graphic coords="9,59.03,158.57,120.98,91.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Fig.11. Input video (10 minutes). Top: Video synopsis result of<ref type="bibr" target="#b3">[4]</ref> (2 minutes in total, and 400 th , 600 th , 2500 th , 3000 th frames are shown). Bottom: Our synopsis (1 minute in total, and 100 th , 600 th , 1200 th , 1500 th frames are shown), by setting K to 3 in the iterative object shifting.</figDesc><graphic coords="10,69.35,380.69,115.70,91.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Input video (30 minutes). Top: Video synopsis result of [4] (1 minute in total, and 212 th , 570 th , 855 th , 1477 th frames are shown). Bottom: Our synopsis (1 minute in total, and 212 th , 570 th , 855 th , 1477 th frames are shown).</figDesc><graphic coords="11,69.35,152.33,115.70,115.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Input video (5 minutes). Top: Video synopsis result of [4] (24 seconds in total, and 150 th , 215 th , 300 th frames are shown). Bottom: Our synopsis (24 seconds in total, and 150 th , 215 th , 300 th frames are shown).</figDesc><graphic coords="11,69.59,392.45,155.30,78.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. The input video (10 minutes). Top: Video synopsis result of<ref type="bibr" target="#b3">[4]</ref> (66 seconds in total, and 200 th , 600 th , 1200 th , 1800 th frames are shown). Bottom: Our synopsis (40 seconds in total, and 250 th , 500 th , 850 th , 900 th frames).</figDesc><graphic coords="11,69.35,604.25,115.70,87.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>30 i=1 5 j=1A</head><label>305</label><figDesc>ij )/150 * 100%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Experimental parameters and run-time information in video synopsis.</figDesc><table><row><cell>Examples</cell><cell>Input Len.</cell><cell>Obj. Num.</cell><cell>Synopsis Len.</cell><cell>K</cell><cell>L t</cell><cell>Ls</cell><cell>T t</cell><cell>Ts</cell><cell>MPR</cell><cell>LR</cell><cell>Run-time [4]</cell></row><row><cell>Fig. 11</cell><cell>10 minutes</cell><cell>16</cell><cell>1 minutes</cell><cell>3</cell><cell>1440</cell><cell>768</cell><cell>22 s</cell><cell>29 s</cell><cell>Yes</cell><cell>Yes</cell><cell>12 s</cell></row><row><cell>Fig. 12</cell><cell>3 minutes</cell><cell>18</cell><cell>15 s</cell><cell>2</cell><cell>396</cell><cell>720</cell><cell>15 s</cell><cell>25 s</cell><cell>No</cell><cell>Yes</cell><cell>6 s</cell></row><row><cell>Fig. 13</cell><cell>30 minutes</cell><cell>21</cell><cell>1 minutes</cell><cell>3</cell><cell>1890</cell><cell>672</cell><cell>28 s</cell><cell>26 s</cell><cell>Yes</cell><cell>Yes</cell><cell>22 s</cell></row><row><cell>Fig. 14</cell><cell>5 minutes</cell><cell>6</cell><cell>24 s</cell><cell>3</cell><cell>216</cell><cell>351</cell><cell>6 s</cell><cell>20 s</cell><cell>Yes</cell><cell>Yes</cell><cell>3 s</cell></row><row><cell>Fig. 15</cell><cell>10 minutes</cell><cell>16</cell><cell>40 s</cell><cell>2</cell><cell>960</cell><cell>768</cell><cell>19 s</cell><cell>10 s</cell><cell>Yes</cell><cell>Yes</cell><cell>10 s</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the anonymous reviewers for their valuable comments and insightful suggestions, and thank to Isaac Liao for proofreading the manuscript. This work was partly supported by the National Basic Research Program of China (No. 2012CB725303), NSFC (No. 61070081), RGC research grants (ref. 416007, 416311), UGC direct grant for research (no. 2050454, 2050485), the Open Project Program of the State Key Lab of CAD&amp;CG (Grant No. A1208), Luojia Outstanding Young Scholar Program of Wuhan University, the Project of Science and Technology Plan for Zhejiang Province (Grant No. 2012C21004), and the Fundamental Research Funds for the Central Universities. Chunxia Xiao is the corresponding author.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Yongwei Nie received the BS degree from the School of Computer, Wuhan University, in 2009. Currently, he is working toward the PHD degree at the School of Computer, Wuhan University, China. His research interests include image and video editing, and computational photography. Contact him at nieyongwei@gmail.com. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An overview of video abstraction techniques</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tretter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>HP Laboratories Palo Alto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Space-time video montage</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video abstraction: A systematic review and classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications, and Applications(TOMCCAP)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>ACM Transactions on Multimedia Computing</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nonchronological video synopsis and indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1971" to="1987" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An integrated scheme for object-based video abstraction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM international conference on Multimedia</title>
		<meeting>the eighth ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="303" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video abstract of video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="117" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computational time-lapse video</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2007">2007</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making a long video short: Dynamic video synopsis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="435" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Webcam synopsis: Peeking around the world</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shot reconstruction degree: a novel criterion for key frame selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1451" to="1457" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring video content structure for hierarchical summarization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aref</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="98" to="115" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive video fast forward</title>
		<author>
			<persName><forename type="first">N</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="327" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated video program summarization using speech transcripts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Taskiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pizlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponceleon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="775" to="791" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Multimedia</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Movie content analysis, indexing and skimming via multimodal information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Kluwer Academic Hingham, MA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Perspectives on content-based multimedia systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Replay boundary detection in mpeg compressed video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Cybernetics, 2003 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2800" to="2804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009">2009</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast exact nearest patch match for patch-based image editing and processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Visualization and Computer Graphics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Texture optimization for example-based synthesis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="795" to="802" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast texture synthesis using tree-structured vector quantization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactives techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactives techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast multi-scale joint bilateral texture upsampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="275" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Space-time video completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">120</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image completion with structure propagation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="861" to="868" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video completion and synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="341" to="353" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The patch transform and its applications to image editing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Butman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The patch transform</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1489" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition: A review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="37" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Background estimation as a labeling problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2005. ICCV 2005</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1034" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2003">2003</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Background cut</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="628" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An improved adaptive background mixture model for real-time tracking with shadow detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kaewtrakulpong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd European Workshop on Advanced Video Based Surveillance Systems</title>
		<meeting>2nd European Workshop on Advanced Video Based Surveillance Systems</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">What energy functions can be minimized via graph cuts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="147" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Understanding belief propagation and its generalizations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="236" to="239" />
		</imprint>
	</monogr>
	<note>Exploring artificial intelligence in the new millennium</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Seam carving for content-aware image resizing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007">2007</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
