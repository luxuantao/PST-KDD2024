<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<email>junweihan2010@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">N</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FFCB05CB05A416C119F9D3B1F461A25</idno>
					<idno type="DOI">10.1109/TIP.2018.2817047</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2817047, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2817047, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Saliency detection</term>
					<term>eye fixation prediction</term>
					<term>convolutional neural networks</term>
					<term>long short-term memory</term>
					<term>global context</term>
					<term>scene context</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional saliency models usually adopt hand-crafted image features and human-designed mechanisms to calculate local or global contrast. In this paper, we propose a novel computational saliency model, i.e., deep spatial contextual long-term recurrent convolutional network (DSCLRCN) to predict where people looks in natural scenes. DSCLRCN first automatically learns saliency related local features on each image location in parallel. Then, in contrast with most other deep network based saliency models which infer saliency in local contexts, DSCLRCN can mimic the cortical lateral inhibition mechanisms in human visual system to incorporate global contexts to assess the saliency of each image location by leveraging the deep spatial long short-term memory (DSLSTM) model. Moreover, we also integrate scene context modulation in DSLSTM for saliency inference, leading to a novel deep spatial contextual LSTM (DSCLSTM) model. The whole network can be trained end-to-end and works efficiently when testing. Experimental results on two benchmark datasets show that DSCLRCN can achieve state-of-the-art performance on saliency detection. Furthermore, the proposed DSCLSTM model can significantly boost the saliency detection performance by incorporating both global spatial interconnections and scene context modulation, which may uncover novel inspirations for studies on them in computational saliency models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>HEN facing visual scenes, human can quickly focus eyes on distinctive visual regions and ignore plain ones. This neural mechanism is known as visual attention and helps us quickly and efficiently observing the world. There are two forms of visual attention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. One is bottom-up saliency-driven attention, which is up to the distinctiveness of visual elements and helps people to rapidly concentrate on key points. The other one is the top-down task-driven attention and is guided by endogenous factors, such as one's prior knowledge and how people process visual tasks, and helps people to complete the tasks efficiently. In this paper, we focus on the former to predict where people look when free-viewing natural scenes.</p><p>In computer science community, researchers have developed lots of computational models to detect visual saliency, most of which follow the biological evidence that salient regions usually stand out from their surroundings and adopt the contrast mechanism to detect saliency. Contrast measures the distinctiveness of each image location with respect to a local context or global context, which involves two steps as described below. First, image representations, on which contrast inference is operated, need to be constructed. Traditional methods usually utilize various hand-designed features to represent images, e.g., intensity, color, and orientation <ref type="bibr" target="#b2">[3]</ref>, color name features <ref type="bibr" target="#b3">[4]</ref> and object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. While effective, these features are manually designed based on researcher's domain knowledge, which may be insufficient to simulate the reaction of sophisticated human visual system when facing various natural scenes. Thus, novel and more abundant feature representations are needed as basis for contrast inference.</p><p>Second, contrast inference is executed based on the extracted image representations to evaluate saliency. Most traditional methods (e.g., <ref type="bibr" target="#b2">[3]</ref>) adopt local contrast to predict saliency by assessing the difference between each image location and its local surroundings. However, cortical lateral inhibition mechanisms in human visual system suggest that neighboring similar features can inhibit each other via specific, anatomically defined interconnections, thus perceived contrast of a centrally viewed test stimulus will be mediated by peripherally viewed flanking stimuli <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>  evidences indicate that global context in visual scenes should be considered in visual attention, instead of only considering local regions in local contrast inference. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the most salient regions in pair (a) are the regions with the stuff on the table. However, when a man appears in the image in pair (b), the most salient region shifts to the man's face. If only local contrast is considered and the inner competition in the visual scene is ignored, the saliency map in pair (b) will probably highlight the region above the table, leading to false positive results. Although some previous saliency models have already taken global contrast into consideration, most of them resort to hand-designed operations or formulations to infer contrast, which may suffer from human's unthorough understanding of the visual attention mechanisms.</p><p>To deal with the previously discussed intrinsic problems, in this paper we propose a novel end-to-end model to detect saliency based on deep neural networks (DNN). As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we first adopt a deep convolutional neural network (CNN) <ref type="bibr" target="#b9">[10]</ref> to extract local image feature representations at each spatial location in parallel. By finetuning deep CNN models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> pretrained with Imagenet <ref type="bibr" target="#b12">[13]</ref> large scale dataset on the saliency detection data, CNNs can automatically learn various saliency-related features hierarchically from raw image data, e.g., color, shape, objects, faces, local contrast, etc, and obtain effective local feature maps. Then we utilize long short-term memory (LSTM) <ref type="bibr" target="#b13">[14]</ref> to model the global context. LSTM is usually used to memorize sufficient context information in time series data via its memory cell. Here we propose to adopt a deep spatial LSTM (DSLSTM) model on the obtained convolutional feature map, thus mimicking the human visual system to introduce lateral interconnections among different spatial locations. DSLSTM can learn to memorize the long-term contextual information to evaluate saliency of each image pixel, instead of being restricted in a local context as most traditional works did.</p><p>Moreover, scene context can also supply informative hints to visual saliency detection, which has not been deeply studied by most previous works. As one of the few works which studied the role of scene modulation on visual attention, Torralba et al. <ref type="bibr" target="#b14">[15]</ref> analyzed the gaze distribution over a large annotated image database, i.e., the LableMe dataset <ref type="bibr" target="#b15">[16]</ref>, and found that eye movements are highly related with scene context. For example, pedestrians are the most salient object in only 10% of the outdoor scene images, being less salient than many other objects. Tables and chairs are among the most salient objects in indoor scenes. Based on these observations, they proposed a Bayesian framework to incorporate scene context in natural search tasks. We show some intuitive examples in Figure <ref type="figure" target="#fig_0">1</ref>. As we can see, being aware of the scene context can help human to quickly focus our eyes on some scene specific important objects (e.g., traffic signs in street views as shown in (c)) or some exceptional objects (e.g., a bed in a forest as shown in (d)). Thus scene context can be seen as an extra top-down high-level semantic factor, as a supplementary to other widely studied top-down object level semantics. Different from <ref type="bibr" target="#b14">[15]</ref>, in this paper, we try to learn the modulation effect of scene context on attention of free viewing. To be specific, as shown in Figure <ref type="figure" target="#fig_1">2</ref>, we first use a scene CNN model <ref type="bibr" target="#b16">[17]</ref> to extract scene features, then we embed them as contextual information <ref type="bibr" target="#b17">[18]</ref> into the DSLSTM, obtaining a novel deep spatial contextual LSTM (DSCLSTM) model, which can simultaneously incorporate global context and scene context information to detect saliency.</p><p>The whole model can be trained end-to-end, including the local image feature extractor CNN, the scene feature extractor CNN, and the DSCLSTM, yielding a novel holistic saliency model, which we refer as the deep spatial contextual long-term recurrent convolutional network (DSCLRCN).</p><p>In summary, our novelties and contributions are threefold: 1) We propose a novel end-to-end saliency detection model, i.e., the DSCLRCN. It first learns powerful saliency-related local feature representations, then learns to simultaneously incorporate global context and scene context to infer saliency.</p><p>2) We propose a novel deep spatial contextual LSTM (DSCLSTM) model to effectively learn long-term spatial interactions and scene contextual modulation to infer image saliency. Experiments show that the proposed DSCLSTM can significantly improve saliency detection performance. This may uncover novel insights for future computational saliency models to focus on global and scene contexts.</p><p>3) The proposed DSCLRCN model achieves state-of-the-art performance on two benchmark datasets and outperforms other 14 contemporary saliency methods. Furthermore, it also works very efficiently.</p><p>The rest of this paper is organized as follows. Section II reviews some works related to our paper. Section III articulates the proposed DSCLRCN model in details. Section IV reports the experimental results on two eye fixation benchmark datasets and the ablation analysis of our model. Finally, we draw conclusion in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Traditional methods usually assess the saliency of each image location with respect to either local contexts or the global context. The former school of methods infer contrast, rarity, or distinctiveness of each image location in local contexts. As one of the earliest pioneer works, Itti et al. <ref type="bibr" target="#b2">[3]</ref> proposed the "Difference of Gaussians" (DoG) operator to compute the feature difference across Gaussian pyramids of three feature channels, i.e., color, intensity, and orientation, as local contrasts. Then the final saliency map are obtained as an average of the contrast maps. Bruce and Tsotsos <ref type="bibr" target="#b18">[19]</ref> computed bottom-up saliency as Shannon's self-information of image features learned by ICA on each local image patch. Gao et al. <ref type="bibr" target="#b19">[20]</ref> detected salient locations by maximizing the KL divergence between the feature distributions of center and surround regions in an image. Seo and Milanfar <ref type="bibr" target="#b20">[21]</ref> built "self-resemblance" maps which measure the center-surround similarity on features based on local regression kernels to assess the likelihood of saliency. Han et al. <ref type="bibr" target="#b21">[22]</ref> utilized sparse coding model and encoded center locations with dictionaries trained on surrounding locations, then the local contrast can be calculated by combining coding sparseness and residual. Judd et al. <ref type="bibr" target="#b4">[5]</ref> and Borji <ref type="bibr" target="#b5">[6]</ref> extracted low-level and top-down features at each image location and trained classifiers to decide each location to be salient or non-salient. Liang and Hu <ref type="bibr" target="#b3">[4]</ref> explored more middle-level features and combined them with object detector features to assess the saliency of each image location.</p><p>On the contrary, some other methods resort to the global context, i.e., the saliency of each image pixel is evaluated by considering the whole image. Hou and Zhang <ref type="bibr" target="#b22">[23]</ref> transformed the whole image into frequency domain first, then they extracted the spectral residual and transformed it back to spatial domain to obtain saliency map. Zhang et al. <ref type="bibr" target="#b23">[24]</ref> utilized a Bayesian framework to combine bottom-up saliency with top-down information, then overall saliency emerged as the pointwise mutual information between local image features and the search target's features when performing target searching task. Hou et al. <ref type="bibr" target="#b24">[25]</ref> proposed the "image signature", which was the sign of the Discrete Cosine Transform (DCT) of an image, as a binary and holistic image descriptor to detect salient image locations. Garcia-Diaz et al. <ref type="bibr" target="#b25">[26]</ref> proposed to extract local multioriented multiresolution features in Lab color space first, then they performed global whitening normalization on each feature map, and subsequently fused them to obtain the final saliency map.</p><p>Recently, benefitting from the great success DNNs achieved on various computer vision tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, some researchers also applied DNNs into saliency detection, including salient object detection <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> and eye fixation prediction <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>, and have achieved superior results. Here we mainly focus on eye fixation models. Shen et al. <ref type="bibr" target="#b30">[31]</ref> and Vig et al. <ref type="bibr" target="#b31">[32]</ref> used a 3-layer convolutional sparse coding model and hierarchical neuromorphic networks to learn effective image features first, respectively. Then they both adopted a linear SVM to classify each local image location to be salient or non-salient. Kümmerer et al. <ref type="bibr" target="#b32">[33]</ref> learned a softmax classifier on linearly combined multi-level features of AlexNet <ref type="bibr" target="#b26">[27]</ref> on each image location to predict eye fixations. Liu et al. <ref type="bibr" target="#b33">[34]</ref> used a multi-resolution CNN to combine multi-scale contexts and do saliency classification on each image location. <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref> all utilized fully convolutional networks (FCNs) <ref type="bibr" target="#b27">[28]</ref> based on pretrained deep networks (i.e., VGGnet <ref type="bibr" target="#b10">[11]</ref>) to infer saliency of each image location in parallel. However, all these previous works assessed saliency in local contexts due to their local features and pixel-wise classifiers <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref> or limited receptive fields in FCN based models <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>. Although some works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> tried to capture global context using convolutional layers with very large receptive field, this idea only held on a few image locations around the image center while failed on other positions due to the intrinsic property of convolutional layers.</p><p>We propose to adopt DSLSTM to construct interconnections among different image locations to incorporate long term global context. Recurrent neural networks (RNNs), including their improved variant LSTM, have shown their excellent capability to memorize long term contexts in time series data, e.g., speech recognition <ref type="bibr" target="#b40">[41]</ref> and natural language processing <ref type="bibr" target="#b41">[42]</ref>. Lately, RNNs have also been applied into computer vision tasks. Behnke <ref type="bibr" target="#b42">[43]</ref> proposed a hierarchical recurrent convolutional neural network to incorporate lateral and vertical feedback loops for resolving local ambiguities on several tasks. Donahue et al. <ref type="bibr" target="#b43">[44]</ref> proposed Long-term Recurrent Convolutional Networks (LRCN) which stacked LSTM on temporal dimension on CNN encoder features to deal with video recognition and image description tasks. Visin et al. <ref type="bibr" target="#b44">[45]</ref> proposed the ReNet model in which four recurrent neural networks swept horizontally and vertically in both directions across the image to learn context features for image classification. Bell et al. <ref type="bibr" target="#b45">[46]</ref> and Yan et al. <ref type="bibr" target="#b46">[47]</ref> applied the ReNet model on top of CNN features to integrate context information for object detection and semantic segmentation, respectively. Based on the ReNet model, in this paper we adopt DSLSTM with concatenation with deep CNNs to incorporate global context for saliency detection.</p><p>Moreover, Ghosh et al. <ref type="bibr" target="#b17">[18]</ref> proposed the Contextual LSTM (CLSTM) model to incorporate topics as contextual information into LSTM for NLP tasks. While we propose to embed scene features as contextual information into the DSLSTM model, obtaining a novel DSCLSTM model, to simultaneously incorporate global context and scene modulation for saliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DSCLRCN FOR SALIENCY DETECTION</head><p>In this section, we illuminate the proposed DSCLRCN in details for saliency detection. Specifically, as shown in Figure <ref type="figure" target="#fig_1">2</ref>, we first adopt a pretrained CNN model to extract local convolutional image features. At the same time, the preatrained Places-CNN <ref type="bibr" target="#b16">[17]</ref> model is also used to extract a scene feature vector. Then local image features and the scene vector are both normalized and fed into the DSCLSTM, which propagates the global and scene contextual information to each image location. Finally, the saliency map can be obtained by a simple convolutional layer, and the NSS loss between the upsampled saliency map and the human eye fixation locations is used as supervision to train the whole network. Below we describe each network component in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Local Image Feature Extraction using CNNs</head><p>We adopt deep CNNs pretrained on Imagenet <ref type="bibr" target="#b12">[13]</ref> dataset to extract local image features using fully convolutional architecture <ref type="bibr" target="#b27">[28]</ref>. The network is based on VGG 16-layer network <ref type="bibr" target="#b10">[11]</ref> or the ResNet-50 network <ref type="bibr" target="#b11">[12]</ref>. To preserve a relatively large size of the extracted feature map, we utilize the dialated convolution <ref type="bibr" target="#b47">[48]</ref> strategy, which supports exponential expansion of the receptive field without loss of resolution or coverage.</p><p>VGG 16-layer network consists of 13 convolutional layers with 33 convolutional kernels and 3 fully connected layers. To maintain the spatial information, we only utilize the convolutional layers, which are composed of five convolutional blocks and each of them is followed by a max-pooling layer with downsampling stride 2. We keep the layers before the forth convolutional block (conv4) as the same with the original VGG network. After that we discard pool4 and pool5 layers and adopt dilation sizes <ref type="bibr" target="#b47">[48]</ref> of 2 in conv5 layers to preserve the resolution and receptive field sizes. To enlarge the receptive filed size of the neurons in the final feature map, we add another two convolutional blocks each of which consists of two convolutional layers with 512 33 convolutional kernels, dilation sizes of 4, and ReLU <ref type="bibr" target="#b48">[49]</ref> activation function. Since the activation values after ReLU activation in the last layer are usually very large, which will make the hidden neurons in subsequent LSTM layers easily saturate and hard to train, we use the 2 -norm L layer <ref type="bibr" target="#b49">[50]</ref> to normalize the whole feature map to have standard 2 -norm L first. Specifically, considering a feature map y , we normalize it using 2 -norm L : 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ˆ. y y = y</head><p>(1)</p><p>Then we learn to re-scale it to an appropriate magnitude for the subsequent LSTM layers. The overall network structure is shown in Table <ref type="table" target="#tab_1">I</ref>. The ResNet-50 network consists of 49 convolutional layers and 1 fully connected layer. Once again, we only use the convolutional layers to extract local image features, which consist of 5 blocks of convolutional layers. The first block is just one convolutional layer with stride 2, followed by a max-pooling layer with stride 2, either. As for other 4 blocks, each of them is composed of several residual learning blocks <ref type="bibr" target="#b11">[12]</ref> and all the last 3 blocks have strides 2. Similarly, we keep the layers before the conv3 block and revise the conv4 and conv5 blocks to have strides 1 and dilation sizes of 2 and 4, respectively. Because the feature map in the last layer has relatively large channel numbers (2048), we utilize a convolutional layer with ReLU activation function to reduce the dimension to a relatively small one (we set it to 512 in this work) for easy learning of subsequent LSTM layers. Unlike the ResNet layers, we do not use batch normalization <ref type="bibr" target="#b50">[51]</ref> in this layer. Finally, we again use the 2 -norm L layer to normalize the feature map and rescale it.</p><p>We also consider multilayer features for the ResNet50 model to incorporate multiscale contexts. Specifically, we use both of the conv4 feature and conv5 feature. First, we use two convolutional layers with 512 11 convolutional kernels and ReLU activation function on top of conv4 and conv5 feature maps to reduce their feature channels. Subsequently, two 2 -norm L layers with same scale parameters are used to make the two feature maps compatible. At last, we concatenate them and use another convolutional layer with 512 11 kernels to reduce the channel numbers, thus obtaining the final feature map. The overall network structure is shown in Table <ref type="table" target="#tab_1">II</ref>.</p><p>Both of the two networks have strides of 8. Thus when we feed an image with size P Q × into the feature extractor CNNs, we can yield a convolutional feature map of size 512 8 8</p><formula xml:id="formula_0">P Q × × ,</formula><p>which will be inferred as the local feature map below.</p><p>As for the scene feature extractor CNN, we first simply resize each image to size 227 227 × and adopt the convolutional layers of Places-CNN <ref type="bibr" target="#b16">[17]</ref> model to extract convolutional features. Then we add one fully connected layer with 128 neurons and ReLU activation function on top of the pool5 feature. Finally we use another 2 -norm L layer to normalize the feature of the fully connected layer and rescale it. Thus we obtain the scene feature, which will be inputted into the subsequent DSCLSTM model with the local feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DSCLSTM for Context Incorporation</head><p>In this section we introduce the proposed DSCLSTM model which can simultaneously incorporate global context information and scene context modulation. We first briefly review the LSTM model, then we elaborate the DSLSTM model and how to embed scene context into it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Reviewing LSTM</head><p>LSTM is a variant of RNNs and was proposed by Hochreiter and Schmidhuber <ref type="bibr" target="#b13">[14]</ref> to solve the vanishing gradient problem by introducing a memory cell to keep states over long-term time series data. At each time step t, a LSTM unit has a memory cell</p><formula xml:id="formula_1">N t R ∈ c</formula><p>and the hidden state</p><formula xml:id="formula_2">N t R ∈ h</formula><p>, where N is the number of hidden units. Given the input</p><formula xml:id="formula_3">M t R ∈ x</formula><p>with input dimension M, the previous memory cell -1 t c , and the previous hidden state -1 t h , LSTM unit updates its memory cell and hidden state via four gates, namely, input</p><formula xml:id="formula_4">N t R ∈ i , forget gate N t R ∈ f , output gate N t R ∈ o</formula><p>, and input modulation gate</p><formula xml:id="formula_5">N t R ∈ g</formula><p>. The transition equations are given by: 1 ( ),</p><formula xml:id="formula_6">t xi t hi t i σ - = + + W W i x h b (2) 1 ( ), t xf t hf t f σ - = + + W W f x h b (3) 1 ( ), t xo t ho t o σ - = + + W W o x h b<label>(4) 1 (</label></formula><p>),</p><formula xml:id="formula_7">t xc t hc t c φ - = + + W W g x h b (5) 1 , t t t t t - = +   c f c i g<label>(6)</label></formula><p>( ),</p><formula xml:id="formula_8">t t t φ =  h ο c (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where σ and φ are element-wise sigmoid and hyperbolic tangent function, respectively.  represents element-wise multiplication. * W and * b are learnable weights and biases, which can be trained by backpropagation through time (BPTT) algorithm <ref type="bibr" target="#b51">[52]</ref>. Finally, we can represent the whole process as: One of the most important variants of LSTM is the Bidirectional LSTM (BLSTM) <ref type="bibr" target="#b52">[53]</ref>. BLSTM consists of two parallel LSTMs to separately scan the input data sequentially and reversely. Then the hidden states of the two LSTMs are concatenated or added as the one of the BLSTM, which captures both past and future information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) DSLSTM for Global Context Incorporation</head><p>Now we introduce the spatial LSTM (SLSTM) based on the ReNet model <ref type="bibr" target="#b44">[45]</ref> for incorporating global context into saliency detection. SLSTM takes the local feature map extracted previously as the input, then it runs four LSTMs operating in different directions over the feature map, i.e., two BLSTMs scanning horizontally and vertically, to blend the context information. Specifically, as shown in the orange box in Figure <ref type="figure" target="#fig_1">2</ref>, it first treats each pixel in each row of the feature map as a time step and runs two LSTMs in parallel to scan it left-to-right and right-to-left, thus obtaining two feature maps which consist of the hidden states of the LSTMs at each spatial location. Let us represent the input local feature map as X , the obtained hidden state feature map as H , thus at each location ( ) p,q we have ,</p><formula xml:id="formula_10">M p q R ∈ x , , N p q R ∈ h</formula><p>, where 512 M = , 1, , 8</p><formula xml:id="formula_11">P p =  , and 1, ,<label>8</label></formula><formula xml:id="formula_12">Q q = </formula><p>as mentioned in Section A. Then the updating of the cell memory , p q c and the hidden state , p q h at location ( ) p,q can be represented by: </p><formula xml:id="formula_13">          +     ×       256,1_1_1 256,3 _1_ 2 6 1024,1_1_1     ×       512,1_1_1 512,3 _1_ 2 3 2048,1_1_1     ×       [512,1_1_1] 1 2 -norm L scale:400 [512,1_1_1] 1 (MulL) [512,1_1_1] 1 (MulL) 2 -norm L scale:400 (MulL) , , ,<label>, 1 , 1 , , , , 1 , 1 ( , ) ( , , ), ( , ) ( , , ),</label></formula><p>p q p q p q p q p q p q p q p q p q p q LSTM LSTM</p><formula xml:id="formula_14">→ → → → → - - ← ← ← ← ← + + = = h c x h c h c x h c (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>where the signs → and ← represents the left-to-right and right-to-left scanning, respectively. Next the SLSTM concatenates the two feature maps → H and ← H along the channel dimension, obtaining the fused feature map 2 8 8</p><formula xml:id="formula_16">P Q N R × × ↔ ∈ H</formula><p>, which incorporates the context information from both left and right at each location.</p><p>Then, the SLSTM uses another two LSTMs to scan ↔ H from top to bottom and bottom to top, which can be represented by: , , ,</p><p>p q p q p q p q p q p q p q p q p q p q LSTM LSTM</p><formula xml:id="formula_18">↓ ↓ ↓ ↔ ↓ ↓ - - ↑ ↑ ↑ ↔ ↑ ↑ + + = = h c h h c h c h h c (<label>10</label></formula><formula xml:id="formula_19">)</formula><p>The signs ↓ and ↑ represent the top-to-bottom and bottom-to-top scanning, respectively. At last, SLSTM concatenates ↓ H and ↑ H , obtaining H  . By progressively scanning the local feature map horizontally and vertically in four directions, the information at each location can be propagated to any other locations and each location in H  contains contextual interactions from all other locations, thus long-term global contextual dependencies are incorporated in a very efficient way.</p><p>We stack two SLSTMs successively, leading to DSLSTM. The increased depth is supposed to increase the capability to learn longer-range and more complex contextual dependencies between different locations. Experiments also show that deep SLSTMs are more effective to blend contextual information in the whole image in section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Scene Context Modulation: DSCLSTM</head><p>Inspired by the CLSTM model in <ref type="bibr" target="#b17">[18]</ref> and the fact that the scene modulation effects in visual attention, we propose to embed the scene feature into the DSLSTM to integrate scene contexts in our saliency model. Specifically, <ref type="bibr" target="#b17">[18]</ref> inputted a topic vector into the traditional LSTM unit at each time step as a static input, i.e., added linear projections of the topic vector to the formulations of the four gates (Equations ( <ref type="formula" target="#formula_13">1</ref>) to ( <ref type="formula" target="#formula_6">4</ref>)). While we find that adding a static input into every time step can easily lead to poor local minimum when training the network, thus we just add the scene feature s in the first time step, where we have:</p><formula xml:id="formula_20">1 1 0 ( ), xi hi si i σ = + + + W W W i x h s b (11) 1 1 0 ( ), xf hf sf f σ = + + + W W W f x h s b (12) 1 1 0 ( ), xo ho so o σ = + + + W W W o x h s b (13) 1 1 0 ( ). xc hc sc c σ = + + + W W W g x h s b (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>We adjust the four types of LSTM in the DSLSTM, i.e., LSTM → , LSTM ← , LSTM ↓ , and LSTM ↑ accordingly to incorporate the scene contexts from their first time steps. However, benefitting from the memory cells, the scene contextual information can still be propagated to other time steps, i.e., the whole feature map, leading to the novel DSCLSTM model.</p><p>The output feature map of DSCLSTM has 2N channels and the same spatial size as the local feature map. We set 128 N = due to the limitation of the GPU memory. Now the feature at each location has simultaneously integrated the global context and scene modulation, so it is ready for saliency assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Saliency Assessment</head><p>We simply adopt a convolutional layer with 1 11 kernel and the Softmax activation function to generate the saliency map. The Softmax activation function is used to normalize the whole map, thus introducing lateral competition for saliency assessment. Because the saliency map is generated by a stride of 8, subsequently we use a deconvolutional layer with bilinear interpolation kernels <ref type="bibr" target="#b27">[28]</ref> to upsample the saliency map with a stride of 8. Thus, we can obtain a saliency map with the same size as the input image.</p><p>When training, we use the negative Normalized Scanpath Saliency (NSS) <ref type="bibr" target="#b53">[54]</ref> of the saliency map with reference to the corresponding ground truth human eye fixations as the objective function to train the network. NSS is chosen for the recommendation of <ref type="bibr" target="#b54">[55]</ref> and will be elaborated later. The whole DSCLRCN model can be trained end-to-end using back propagation algorithm <ref type="bibr" target="#b55">[56]</ref>.</p><p>When testing, we just feed each testing image into DSCLRCN and can directly yield the saliency map, being straightforward but effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluated DSCLRCN on two benchmark datasets. The first one is SALICON <ref type="bibr" target="#b56">[57]</ref>, i.e., Saliency in Context dataset, which contains 10,000 training images, 5,000 validation images, and 5,000 testing images. The images are all of size 480 640 × and selected from the MS COCO <ref type="bibr" target="#b57">[58]</ref> dataset with rich contextual information. The ground truth of eye fixations are collected by a proposed mouse-contingent multi-resolutional paradigm and have been shown to be highly similar with eye tracking data. Benefitting from the novel paradigm, this dataset, as the largest eye fixation dataset so far, can be created much efficiently and easily. The ground truth of eye fixations of the testing set are held-out and researchers are supposed to submit their results to the SALICON challenge website <ref type="foot" target="#foot_0">1</ref> or the LSUN Saliency Challenge website<ref type="foot" target="#foot_1">2</ref> to evaluate their methods.</p><p>The second dataset for evaluation is MIT300 <ref type="bibr" target="#b58">[59]</ref>. It contains 300 images with natural outdoor or indoor scenes, and has become one of the widely used benchmark datasets in recent years. The ground truth of eye fixation data are held-out and researchers can submit their models to the MIT Saliency Benchmark website <ref type="foot" target="#foot_2">3</ref> to evaluate their models. As the organizers suggested, the MIT1003 <ref type="bibr" target="#b4">[5]</ref> dataset can be used as the training and validation sets for MIT300 since they are collected with similar eye-tracking setup. This dataset contains 779 landscape images and 228 portrait images collected from Flickr and LabelMe, and the eye fixation data are collected while being viewed by 15 human subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>There exist various evaluation metrics for eye fixation prediction, including Earth Mover's Distance (EMD), Normalized Scanpath Saliency (NSS), Pearson's Correlation Coefficient (CC), Similarity (SIM), Area Under Curve (AUC), shuffled-AUC (sAUC), Kullback-Leibler divergence (KL), Information Gain (IG), etc. Bylinskii et al. <ref type="bibr" target="#b54">[55]</ref> showed that among these metrics, KL, IG, and SIM are most sensitive to false negatives, AUC metrics ignore low-valued false positives, EMD's penalty depends on spatial distance, while NSS and CC are equally affected by false positives and negatives. Thus based on their recommendation, we report CC and NSS here. Moreover, AUC and shuffled-AUC are also reported for comparison with existed models for historical reasons.</p><p>The Area Under the ROC Curve (AUC) metric is widely used to evaluate saliency models. For an image with its binary ground truth eye fixation map G F , AUC evaluates the classification performance of the computed saliency map S, where fixation points and non-fixation points in G F are considered as the positive set and negative set, respectively. Specifically, S is normalized to [0, 1] first. Then it is binarily classified into salient regions and non-salient regions by a threshold. By varying the threshold from 0 to 1, ROC curves can be obtained by plotting true positive rate vs. false positive rate. Finally, the area under the ROC curve is calculated as the AUC score. To alleviate the influence of center-bias, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b59">60]</ref> introduced sAUC, which adopts the fixation points of other images in the dataset as the negative set. Although widely used, AUC metrics are ambivalent to monotonic transformations and ignore low-valued false positives, which may be unfavorable behavior for eye fixation prediction <ref type="bibr" target="#b54">[55]</ref>.</p><p>NSS is introduced in <ref type="bibr" target="#b53">[54]</ref>, which computes the average of the normalized saliency values at eye fixation locations. As analyzed in <ref type="bibr" target="#b54">[55]</ref>, it is sensitive to false positives, relative differences in saliency across the image, and monotonic transformations. Given a saliency map S and the corresponding eye fixation map G F , NSS is calculated as:</p><formula xml:id="formula_22">1 ( , )<label>, ( ) . ( )</label></formula><formula xml:id="formula_23">F F F NSS µ σ = - = ∑ ∑ S G S G G S S S S <label>(15)</label></formula><p>CC considers the saliency map S and the corresponding ground truth saliency map G S , which is obtained by Gaussian blurring G F , as random variables and computes their Pearson's Correlation Coefficient:</p><p>( , )</p><formula xml:id="formula_24">S S S cov CC σ σ = × S G S G S G<label>( , ) . ( ) ( )</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details 1) Data Processing</head><p>For the SALICON benchmark dataset, we directly used the SALICON training and validation datasets to train and validate DSCLRCN, respectively. For the MIT300 benchmark dataset, we used 903 images from MIT1003 dataset to finetune the model trained on SALICON. Another 100 images were used for validation. Since MIT1003 contains relatively less images, we augmented the training set twice by using horizontal flipping. For simplicity, we directly resized all images to size 480 640 × and 227 227 × for the local feature extractor CNN and the scene feature extractor CNN, respectively.</p><p>When testing, we also resized testing images to size 480 640 × and 227 227 × and feedforward them through the network to obtain saliency maps. Then we resized them to the same sizes with the input images. Finally, we used small Gaussian filters to blur the saliency maps. Via validation experiments, for each image we set the standard deviation of Gaussian filters to be 0.035 min( , )</p><formula xml:id="formula_26">P Q σ =</formula><p>, and set the size of the Gaussian filters to be 4σ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Network Settings</head><p>We used stochastic gradient descent (SGD) with momentum to train the whole network. The batchsize was set to 20. For the SALICON dataset, the learning rates of the pretrained layers and other layers were set to 0.001 and 0.01, respectively. We also scaled down the learning rates by a factor of 2.5 every 500 iteration steps. The overall iteration step was set to 5,000, and we validated the trained models every 500 steps to select a best model for testing. While using MIT1003 data to finetune the model trained on SALICON, we set the learning rates for all layers to 0.001 and scaled them down every 100 iteration steps. The overall iteration step and the validation step were set to 1000 and 100, respectively. We also used momentum of 0.9 and a weight decay of 0.0005.</p><p>When using the ResNet50 model, to facilitate training, we fixed the scale and bias parameters of the BN layers, i.e., we directly used the global statistics of the original ResNet50 parameters. The initial scales of the 2 -norm L layers for the local feature map and the scene feature were set to 400 and 9, respectively. The biases of the LSTM forget gates were initialized to 1 to help to keep long-term memories. The parameters in each BLSTM, i.e., LSTM → and LSTM ← , LSTM ↓ and LSTM ↑ , were shared by considering the symmetry of images.</p><p>We implemented DSCLRCN using the Caffe <ref type="bibr" target="#b60">[61]</ref> library. The testing code was implemented using Matlab. A GTX Titan X GPU was used both in training and testing for acceleration. The running time for testing an image is 0.27s. The code will be publicly available on the author's homepage <ref type="foot" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Ablation Analysis</head><p>In this section we do ablation analysis on the SALICON validation dataset to evaluate the contribution of each model component. The results are shown in Table <ref type="table" target="#tab_2">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Influence of the Receptive Field size</head><p>The size of Receptive field (RF) determines how large the area is involved in the activation of a neuron in a CNN layer. We show the evaluation results of using conv5, conv6, and conv7 features of the VGG based feature extractor (shown in Table <ref type="table" target="#tab_1">I</ref>) to directly detect saliency in the FCN architecture without DSCLSTM, which are represented by FCN5, FCN6, and FCN7 in Table <ref type="table" target="#tab_2">III</ref>, respectively. The RF sizes of corresponding feature maps are also given. We can see that, while the RF sizes are enlarged, the saliency detection performance can be boosted by incorporating more context information. Especially, the performance gains significantly from FCN5 to FCN6, which may be attributed to FCN5's relatively too small RF size with respect to the image size (196196 vs. 480640). This indicates that saliency detection heavily relies on large contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Effectiveness of Global Context Incorporation</head><p>We directly added SLSTM layers on top of the FCN7 network to evaluate the effectiveness of incorporating the global context. As shown in Table <ref type="table" target="#tab_2">III</ref>, adding a SLSTM layer can improve the performance much, while 2 SLSTM layers (DSLSTM) bring more improvement, demonstrating that integrating global context can significantly benefit saliency detection performance. While we didn't observed more meaningful improvements by continuing to deepen SLSTM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Effectiveness of Scene Modulation</head><p>We added scene modulation to the FCN7_DSLSTM model to evaluate its effectiveness. By comparing the performance of FCN7_DSLSTM and FCN7_DSCLSTM in Table <ref type="table" target="#tab_2">III</ref>, we can see that incorporating scene context can also improve the saliency detection performance much. This indicates that scene context can also supply much informative information to saliency detection, which deserves more attention and research in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Boosting the Performance using Deeper and Multiscale Features</head><p>We show the performance using the ResNet50 based feature extractor network. From Table <ref type="table" target="#tab_2">III</ref>, we can see that, using the more powerful ResNet50 feature can further improve saliency detection performance, which is consistent with other observations on other computer vision tasks. Comparing ResNet50_DSCLSTM and ResNet50 we can see that our proposed DSCLSTM model still brings significant performance gains when using a deeper backbone network. We also show the results of using multilayer features of the ResNet50 network (shown as ResNet50_MulL_DSCLSTM in Table <ref type="table" target="#tab_2">III</ref>). We can see that, with similar conclusions in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b61">62]</ref>, integrating multiscale features can also further improve saliency detection performance.</p><p>We also tried to use multilayer features in the FCN7_DSCLSTM model but obtained worse results. This may  be attributed to that in the FCN7 based model, the layers FCN6 and FCN7 were trained from the scratch, thus adding multilayer connections may degrade the network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with state-of-the-arts</head><p>In this section, we compared the proposed DSCLRCN model on the SALICON test dataset and the MIT300 dataset with other 14 state-of-the-art saliency detection models, including GBVS <ref type="bibr" target="#b62">[63]</ref>, Judd <ref type="bibr" target="#b4">[5]</ref>, AWS <ref type="bibr" target="#b25">[26]</ref>, BMS <ref type="bibr" target="#b63">[64]</ref>, eDN <ref type="bibr" target="#b31">[32]</ref>, Mr-CNN <ref type="bibr" target="#b33">[34]</ref>, SALICON <ref type="bibr" target="#b34">[35]</ref>, DeepFix <ref type="bibr" target="#b35">[36]</ref>, Shallow Convnet <ref type="bibr" target="#b37">[38]</ref>, Deep Convnet <ref type="bibr" target="#b37">[38]</ref>, DeepGaze II <ref type="bibr" target="#b32">[33]</ref>, PDP <ref type="bibr" target="#b38">[39]</ref>, SU (Saliency Unified) <ref type="bibr" target="#b36">[37]</ref>, and ML-Net <ref type="bibr" target="#b39">[40]</ref>. It is worth noting that all the last 10 models are proposed recently and are based on DNNs. All these methods (including ours) have submitted their results to the SALICON challenge website or the MIT Saliency Benchmark challenge website, where all the saliency scores are obtained. We used the ResNet50_MulL_DSCLSTM setting as our final model due to its best performance.</p><p>We show the comparison results on the SALICON test dataset in Table <ref type="table" target="#tab_3">IV</ref>. This dataset is recently proposed, thus only some recent models provided results on this dataset, but all of them are DNN based models. We can see that DSCLRCN 1057-7149 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2817047, IEEE Transactions on Image Processing 10 outperforms all of them in terms of three metrics, i.e., AUC, NSS, and CC. Especially on NSS and CC, DSCLRCN obtains significant superiority, demonstrating its effectiveness. We notice that DeepGaze II <ref type="bibr" target="#b32">[33]</ref> achieves better sAUC scores than our method. However, sAUC primarily rewards true positives while being hard to be degraded by false positives <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b64">65]</ref>. This can usually lead to good score to very blurred/hazy saliency maps <ref type="bibr" target="#b65">[66]</ref>, which is the case of DeepGaze II.</p><p>The comparison results on the MIT300 dataset is shown in Table <ref type="table">V</ref>. We can see that in generally DSCLRCN outperforms all other previous models, especially including 9 DNN based models. Specifically, DSCLRCN achieves state-of-the-art performance in terms of AUC, and outperforms all other models on NSS and CC, indicating that DSCLRCN generates more accurate highlights and less false positives highlights. We can also see that DSCLRCN outperforms DeepFix <ref type="bibr" target="#b35">[36]</ref>, which used large convolutional kernels to integrate contextual information. This demonstrates the effectiveness of the proposed DSCLSTM model in incorporating global and scene contexts.</p><p>We also show qualitative comparison results on the MIT1003 validation set in Figure <ref type="figure" target="#fig_2">3</ref>. We can see that the saliency maps generated by DSCLRCN match the ground truth saliency maps best among all the compared models. Specifically, DSCLRCN generates more accurate detections and much less false positives compared with other models, including 4 DNN based models, i.e., ML-Net <ref type="bibr" target="#b39">[40]</ref>, Deep Convnet <ref type="bibr" target="#b37">[38]</ref>, Mr-CNN <ref type="bibr" target="#b33">[34]</ref>, and eDN <ref type="bibr" target="#b31">[32]</ref>. It is worth noting that DSCLRCN can deal with various very challenging scenarios, including cluttered scenes (rows (a), (b), (d), (f), and (k)), and scenes with no obvious salient regions (rows (e), (h), and (l)). These two scenarios are usually very challenging for other models. The former often leads to severe false positive highlights and the latter usually leads to inaccurate highlights. While benefitting from the integrated global and scene contexts, DSCLRCN can accurately detect the most salient regions in an image while ignoring other local distractions, thus obtaining much better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a novel end-to-end saliency model, i.e., DSCLRCN, to predict human eye fixation points in natural scenes. Specifically, DSCLRCN first learned various saliency related local features. Next, it leveraged the DSLSTM model to incorporate global contexts via mimicing the cortical lateral inhibition mechanisms in human visual system. Furthermore, we also proposed to integrate scene modulation in saliency detection, leading to the novel DSCLSTM model. Experimental results showed that DSCLRCN outperforms all previous saliency models on two eye fixation benchmark datasets. In the future, we can also adopt the proposed model to benefit other related tasks, such as video saliency detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b66">67]</ref> and co-saliency detection <ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The influences of global context and scene context to visual saliency. We show 4 pairs of images here. In each pair, the left one is the image stimulus, and the right one is the corresponding ground truth saliency map. Pairs (a) and (b) show the influence of the global context, and pairs (c) and (d) show the influence of the scene context.</figDesc><graphic coords="3,319.75,182.55,237.95,117.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The network architecture of the proposed DSCLRCN. First, local feature map and scene feature are extracted using pretrained CNNs. Then, a DSCLSTM model is adopted to simultaneously incorporate global context and scene context. Finally, saliency map is generated and upsampled.</figDesc><graphic coords="6,53.35,54.00,504.75,226.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative comparison results on the MIT1003 validation set. The second column shows ground truth saliency maps (GT).</figDesc><graphic coords="11,53.25,54.00,504.71,526.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ARCHITECTURE</head><label>I</label><figDesc>OF THE VGG BASED LOCAL FEATURE EXTRACTOR. CONVOLUTIONAL SETTINGS ARE GIVEN BY [CHANNEL,KERNEL_STRIDE_DILATION] LAYERS. THE POOLING SETTINGS ARE GIVEN BY [KERNEL_STRIDE].</figDesc><table><row><cell>Name</cell><cell></cell><cell>conv1</cell><cell cols="2">pool1</cell><cell></cell><cell cols="2">conv2</cell><cell>pool2</cell><cell>conv3</cell><cell>pool3</cell><cell>conv4</cell><cell>conv5</cell><cell>conv6</cell><cell>conv7</cell><cell>norm</cell></row><row><cell>Setting</cell><cell cols="2">[64,3_1_1] 2</cell><cell cols="2">[2_2]</cell><cell cols="3">[128,3_1_1] 2</cell><cell>[2_2]</cell><cell>[256,3_1_1] 3</cell><cell>[2_2]</cell><cell>[512,3_1_1] 3</cell><cell>[512,3_1_2] 3</cell><cell>[512,3_1_4] 2</cell><cell>[512,3_1_4] 2</cell><cell>2 -norm L scale:400</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="11">ARCHITECTURE OF THE RESNET50 BASED LOCAL FEATURE EXTRACTOR. CONVOLUTIONAL SETTINGS ARE GIVEN BY</cell></row><row><cell cols="13">[CHANNEL,KERNEL_STRIDE_DILATION] LAYERS. THE POOLING SETTINGS ARE GIVEN BY [KERNEL_STRIDE]. BATCH NORMALIZATION [51] IS USED IN</cell></row><row><cell></cell><cell cols="12">RESNET LAYERS (CONV1 TO CONV5). (MULL) MEANS THE LAYERS THAT ARE ONLY USED IN THE MULTILAYER CONBINATION NETWORK.</cell></row><row><cell>conv1</cell><cell></cell><cell>pool1</cell><cell></cell><cell></cell><cell>conv2</cell><cell></cell><cell></cell><cell>conv3</cell><cell></cell><cell>conv4</cell><cell></cell><cell>conv5</cell><cell>conv6</cell><cell>norm</cell><cell>conv7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">128,1_ 2 _1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">128,3 _1_1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">64,1_1_1</cell><cell></cell><cell></cell><cell cols="2">512,1_1_1</cell><cell></cell><cell></cell></row><row><cell cols="2">[64,7_2_1] 1</cell><cell>[3_2]</cell><cell>   </cell><cell cols="3">64,3 _1_1 256,1_1_1  ×   </cell><cell>3</cell><cell cols="2">128,1_1_1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">128,3 _1_1 3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">512,1_1_1</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III MODEL</head><label>III</label><figDesc>ABLATION ANALYSIS ON THE SALICON VALIDATION DATASET. THE BEST SCORE OF EACH METRIC IS SHOWN IN BOLD FACE.</figDesc><table><row><cell>Settings</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>CC</cell></row><row><cell cols="3">influence of the RF sizes</cell><cell></cell><cell></cell></row><row><cell>FCN5 (RF: 196)</cell><cell>0.789</cell><cell>0.869</cell><cell>2.960</cell><cell>0.756</cell></row><row><cell>FCN6 (RF: 340)</cell><cell>0.792</cell><cell>0.877</cell><cell>3.075</cell><cell>0.793</cell></row><row><cell>FCN7 (RF: 468)</cell><cell>0.790</cell><cell>0.878</cell><cell>3.087</cell><cell>0.795</cell></row><row><cell cols="4">effectiveness of global context incorporation</cell><cell></cell></row><row><cell>FCN7_SLSTM</cell><cell>0.79</cell><cell>0.882</cell><cell>3.143</cell><cell>0.811</cell></row><row><cell>FCN7_DSLSTM</cell><cell>0.786</cell><cell>0.883</cell><cell>3.160</cell><cell>0.816</cell></row><row><cell cols="3">effectiveness of scene modulation</cell><cell></cell><cell></cell></row><row><cell>FCN7_DSCLSTM</cell><cell>0.786</cell><cell>0.884</cell><cell>3.171</cell><cell>0.822</cell></row><row><cell cols="3">results with ResNet50 model</cell><cell></cell><cell></cell></row><row><cell>ResNet50</cell><cell>0.789</cell><cell>0.882</cell><cell>2.994</cell><cell>0.802</cell></row><row><cell>ResNet50_DSCLSTM</cell><cell>0.791</cell><cell>0.886</cell><cell>3.216</cell><cell>0.831</cell></row><row><cell>ResNet50_MulL_DSCLSTM</cell><cell>0.788</cell><cell>0.887</cell><cell>3.221</cell><cell>0.835</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>RESULTS ON THE SALICON TEST DATASET. THE BEST SCORE OF EACH METRIC IS SHOWN IN BOLD FACE.</figDesc><table><row><cell>Models</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>CC</cell></row><row><cell>Shallow Convnet [38]</cell><cell>0.658</cell><cell>0.821</cell><cell>1.663</cell><cell>0.562</cell></row><row><cell>Deep Convnet [38]</cell><cell>0.724</cell><cell>0.858</cell><cell>1.859</cell><cell>0.622</cell></row><row><cell>DeepGaze II [33]</cell><cell>0.787</cell><cell>0.867</cell><cell>1.271</cell><cell>0.479</cell></row><row><cell>ML-Net [40]</cell><cell>0.768</cell><cell>0.866</cell><cell>2.789</cell><cell>0.743</cell></row><row><cell>SU [37]</cell><cell>0.760</cell><cell>0.880</cell><cell>2.610</cell><cell>0.780</cell></row><row><cell>DSCLRCN</cell><cell>0.776</cell><cell>0.884</cell><cell>3.157</cell><cell>0.831</cell></row><row><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">COMPARISON RESULTS ON THE MIT300 DATASET. WE USE THE</cell></row><row><cell cols="5">AUC-JUDD IMPLEMENTATION AS THE AUC METRIC. THE BEST SCORE OF</cell></row><row><cell cols="4">EACH METRIC IS SHOWN IN BOLD FACE.</cell><cell></cell></row><row><cell>Models</cell><cell>sAUC</cell><cell>AUC-Judd</cell><cell>NSS</cell><cell>CC</cell></row><row><cell>GBVS [63]</cell><cell>0.63</cell><cell>0.81</cell><cell>1.24</cell><cell>0.48</cell></row><row><cell>Judd [5]</cell><cell>0.60</cell><cell>0.81</cell><cell>1.18</cell><cell>0.47</cell></row><row><cell>AWS [26]</cell><cell>0.68</cell><cell>0.74</cell><cell>1.01</cell><cell>0.37</cell></row><row><cell>BMS [64]</cell><cell>0.65</cell><cell>0.83</cell><cell>1.41</cell><cell>0.55</cell></row><row><cell>eDN [32]</cell><cell>0.62</cell><cell>0.82</cell><cell>1.14</cell><cell>0.45</cell></row><row><cell>Mr-CNN [34]</cell><cell>0.69</cell><cell>0.79</cell><cell>1.37</cell><cell>0.48</cell></row><row><cell>SALICON [35]</cell><cell>0.74</cell><cell>0.87</cell><cell>2.12</cell><cell>0.74</cell></row><row><cell>DeepFix [36]</cell><cell>0.71</cell><cell>0.87</cell><cell>2.26</cell><cell>0.78</cell></row><row><cell>Shallow Convnet [38]</cell><cell>0.63</cell><cell>0.80</cell><cell>1.47</cell><cell>0.56</cell></row><row><cell>Deep Convnet [38]</cell><cell>0.69</cell><cell>0.83</cell><cell>1.51</cell><cell>0.58</cell></row><row><cell>DeepGaze II [33]</cell><cell>0.76</cell><cell>0.87</cell><cell>1.29</cell><cell>0.51</cell></row><row><cell>PDP [39]</cell><cell>0.73</cell><cell>0.85</cell><cell>2.05</cell><cell>0.70</cell></row><row><cell>ML-Net [40]</cell><cell>0.70</cell><cell>0.85</cell><cell>2.05</cell><cell>0.67</cell></row><row><cell>DSCLRCN</cell><cell>0.72</cell><cell>0.87</cell><cell>2.35</cell><cell>0.80</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://competitions.codalab.org/competitions/3791</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://lsun.cs.princeton.edu/2016/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://saliency.mit.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://sites.google.com/site/liunian228/</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation of China under Grant 61473231 and Grant 61522207.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual attention: bottom-up versus top-down</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Egeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="850" to="R852" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advanced Deep-Learning Techniques for Salient and Category-Specific Object Detection: A Survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="100" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting eye fixations with higher-level visual features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1178" to="1189" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision</title>
		<meeting>IEEE Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boosting bottom-up and top-down visual features for saliency estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A model for inhibitory lateral interaction effects in perceived contrast</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Fullenkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1115" to="1125" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards a neurobiological theory of consciousness</title>
		<author>
			<persName><forename type="first">F</forename><surname>Crick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semin. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="263" to="275" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An oscillation-based model for the neuronal basis of attention</title>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2789" to="2802" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">766</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LabelMe: a database and web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="487" to="495" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06291</idno>
		<title level="m">Contextual LSTM (CLSTM) models for Large scale NLP tasks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv</title>
		<meeting>Adv</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the plausibility of the discriminant center-surround hypothesis for visual saliency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bottom-up saliency based on weighted sparse coding residual</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SUN: A Bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image signature: Highlighting sparse salient regions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Saliency from hierarchical adaptation through decorrelation and variance normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Fdez-Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dosil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="64" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv</title>
		<meeting>Adv</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video salient object detection via fully convolutional networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning high-level concepts by training a deep network on eye fixations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Deep Learn. Unsupervised Feature Learn</title>
		<meeting>NIPS Deep Learn. Unsupervised Feature Learn</meeting>
		<imprint>
			<publisher>Workshop</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2798" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1045</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Predicting Eye Fixations using Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision</title>
		<meeting>IEEE Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02927</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency Unified: A Deep Architecture for Simultaneous Eye Fixation Prediction and Salient Object Segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gudisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Dholakiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5781" to="5790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shallow and Deep Convolutional Networks for Saliency Prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I-Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="598" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-End Saliency Mapping via Probability Distribution Prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5753" to="5761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Deep Multi-Level Network for Saliency Prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recogn</title>
		<meeting>Int. Conf. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards End-To-End Speech Recognition with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv</title>
		<meeting>Adv</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hierarchical neural networks for image interpretation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Renet: A recurrent neural network based alternative to convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00393</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04143</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04871</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU Workshop</title>
		<meeting>ASRU Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Components of bottom-up gaze allocation in natural images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2397" to="2416" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">What do different evaluation metrics tell us about saliency models?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03605</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Distributed Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1072" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vision</title>
		<meeting>Eur. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technical Report</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visual correlates of fixation selection: effects of scale and time</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Baddeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Gilchrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="643" to="659" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visual Saliency Detection Based on Multiscale Deep CNN Features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv</title>
		<meeting>Adv</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Saliency detection: a boolean map approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision</title>
		<meeting>IEEE Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning a saliency map using fixated locations in natural scenes</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Quantitative analysis of human-model agreement in visual saliency modeling: a comparative study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Consistent video saliency using local gradient flow optimization and global refinement</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4185" to="4196" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Robust Object Co-Segmentation Using Background Prior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1639" to="1651" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Revisiting Co-Saliency Detection: A Novel Approach Based on Two-Stage Multi-View Spectral Rotation Co-clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3196" to="3209" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a self-paced multiple-instance learning framework</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
